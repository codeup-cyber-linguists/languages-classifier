,title,language,original,stars
0,IamTinashe/College-Hub,Vue,"College-Hub

Web-based accommodation service

Build Setup
# install dependencies
$ npm install

# serve with hot reload at localhost:3000
$ npm run dev

# build for production and launch server
$ npm run build
$ npm start

# generate static project
$ npm run generate
For detailed explanation on how things work, checkout Nuxt.js docs.
",2
1,EetuPe/Bounce,JavaScript,"Bounce
Bounce (or bounce 2 : the ultimate remaster (as you wish)) - Is a Javascript game created by: Miika Vuorio, Danila Karpov and Eetu Petänen. It was created in honour of the original ""Bounce"" made for Nokia phones in early 2000s.
",3
2,machsix/Super-preloader,JavaScript,"
     
Super-preloader
Document
Complete Document
Difference between verions:

Super_preloaderPlus_one_New_legacy.user.js: old version doesn't work with GM4 but doesn't use Promise at all
Super_preloaderPlus_one_New.user.js: stable version synced with Greasefork but using old javascript syntax
Super_preloaderPlus_one_ES8.user.js: version in active development

Introduction
A gm script for auto loading paginated web pages. It will join pages together based on the rules.
The script is forked from https://greasyfork.org/en/scripts/10433-super-preloaderplus-one.
Thanks for the original author swdyh && ywzhaiqi && NLF
swdyh is still actively improving rules and developping the extension version,AutoPagerize.
This userscript exists because someone needs to maintain the rules for Chinese and English users since swdyh's rules are mainly for Japanese websites. Don't be afraid of the fact that most feedbacks are in Chinese. I would like to add rules for English users if I could. Feel free to leave feedback.
Please leave feedback at Greasefork page
Development Guide
I appreciate anyone who is interested in devoting their time to the development. We currently have two verions of the code. The ES8 branch is in active development. As the name suggests, I will gradually abandon the further development of the code style code.
To take part into the development, you need to:

Be familiar with javascript, xpath/css selector and node js
Install modules by yarn
Make your modifications
Run yarn test to check bugs
Run yarn run format to format the code based on our code style
Commit and submit pull request!

Check more at Complete Document
Contributers
MachX💻 🎨 🖋 👀suchunchen💻 🖋YFdyh000💻 🎨
This project follows the all-contributors specification. Contributions of any kind welcome!
Donation

License

This program is licensed under GNU General Public License Version 3 or later.
This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
You should have received a copy of the GNU General Public License along with this program.  If not, see http://www.gnu.org/licenses/.
",17
3,dotnet/docs.tr-tr,PowerShell,"


ms.openlocfilehash
ms.sourcegitcommit
ms.translationtype
ms.contentlocale
ms.lasthandoff
ms.locfileid




0c8a16936173b1e599d018d81432ca6b73c08e53
9b552addadfb57fab0b9e7852ed4f1f1b8a42f8e
MT
tr-TR
04/23/2019
61607355



.NET belgeleri
Bu depo, .NET için kavramsal belgelerde içerir. .NET belgeleri site Bunun yanı sıra birden çok deposu oluşturulur:

Kod örnekleri ve kod parçacıkları
API başvurusu
.NET derleyici Platformu SDK başvurusu

Sorunlar ve bu depolar üçü için görevleri buraya izlenir. Büyük bir topluluk bu kaynaklara sahibiz. Sorunlara zamanında yanıt vermek için en iyi çalışmalarımız vermiyoruz. Daha fazla sınıflandırma ve sorunları çözmek için sunduğumuz yordamlar hakkında bizim ilke sorunları konu.
Biz, geliştirmek ve .NET belgeleri tamamlamak yardımcı olmak için katkılar Hoş Geldiniz. Katkıda bulunmak için bkz: projeleri .NET topluluğa katkıda bulunanlar için fikirleri. Katkıda bulunan Kılavuzu yordamlarını kullanırız yönergeler içerir. Kontrol sorunlar listesinde ilginizi çeken bir görev.
Biz, tahmin Xamarin, Mono ve Unity kullanın Bu belgeleri kazandırır.
Bu proje topluluğumuza beklenen davranışını açıklamak için katkıda bulunan Covenant tarafından tanımlanan şartları BENİMSEDİ.
Daha fazla bilgi için .NET Foundation Kullanım Şartları.
",8
4,vadxq/pushQQlove,JavaScript,"pushQQlove
weather and to remind of the time and others
dev
npm start
build to es5
npm run build
serve
npm run serve
功能

计算纪念日天数
爬取one一个每日语录
获取心中的TA当地天气信息
定时早安晚安
发送qq消息问好，推送天气等信息

how to run


配置运行环境，本项目需要node v7.6.0+版本，mongodb数据库，pm2部署工具。


进入目录，安装依赖


cd pushQQlove
npm i

配置文件，(server/config/index.js)
// mongoose path
export const dbPath = '';

// server port
export const port = 7192;

// 群id
export const group_id = 851970427

// 个人id，需加好友
export const user_id = 862235971


dbPath: mongodb
port：项目运行端口
group_id: 需要发布消息的群号
user_id： 需要发布的对象qq号
修改你需要定时提醒的服务(server/app.js)
// new timing()
let timings = new timingTask()
schedule.scheduleJob('16 58 06 * * *', () => {
  timings.init()
})

// 定时睡觉
schedule.scheduleJob('10 28 23 * * *', () => {
  timings.postMsg('缘缘，到点啦，该睡啦，晚安哟~~')
})

支持自定义文本内容，如上，第一个包含爬虫，可以自行编写代码。时间格式如下：
*    *    *    *    *    *
┬    ┬    ┬    ┬    ┬    ┬
│    │    │    │    │    |
│    │    │    │    │    └ day of week (0 - 7) (0 or 7 is Sun)
│    │    │    │    └───── month (1 - 12)
│    │    │    └────────── day of month (1 - 31)
│    │    └─────────────── hour (0 - 23)
│    └──────────────────── minute (0 - 59)
└───────────────────────── second (0 - 59, OPTIONAL)

比如：10 30 07 * * 1表示在周一7：30：10时间发送。10 30 07 * * *表示在每天7：30：10时间发送。
关于酷Q api配置问题
本项目依赖酷Q的node-sdk进行qq发送消息，目前发现图片发送根据文档发送有所出路，正待解决。
可看官方文档 https://github.com/richardchien/cqhttp-node-sdk
目录结构
pushQQlove/
   |
   ├──server/                * 主要代码目录
   │   │
   │   │──config             * 配置文件
   │   │
   │   │──controls/          * 业务逻辑主要代码
   │   │    │
   │   │    │──day           * 计算纪念日天数
   │   │    │
   │   │    │──info          * 用于保存项目和查询历史消息的api
   │   │    │
   │   │    │──spider        * 爬虫
   │   │    │
   │   │    └──timing        * 定时任务相关，包含发送qq消息相关
   │   │ 
   │   │──mongo/             * 数据库配置
   │   │ 
   │   │──route              * api路由
   │   │ 
   │   └──app                * 主入口文件
   │
   │──package.json           * 包信息
   │
   │──.babelrc               * Babel配置
   │
   └──.gitignore             * Git忽略文件配置

配置无误后，本地运行
npm start 

服务器部署,需要先安装pm2
npm i -g pm2

// 直接运行编译
pm2 start 'npm start'

// 编译后运行
npm run build
cd dist
pm2 start app.js

欢迎大家提出建议和意见，谢谢～
license：Apache Licens Version 2.0
",2
5,bajumar/Azure-Monitor-Logs,None,"My Azure Monitor Logs Repo
This repo contains everything related to Azure Monitor Logs that I have learned and created (and have time to publish) - queries, tips & tricks, etc. If I see or do something that I think will be helpful to others, I'll try to put it here.
Feedback and suggestions are always welcome.
Disclosure & Disclaimer
I am currently a Microsoft employee, however, everything that I publish in this repo is my own personal work and of my own personal opinion. Content published in this repo is not officially approved or supported by Microsoft.
",2
6,gigantum/gigantum-testing,Python,"Gigantum Testing
Automation of Gigantum testing with Selenium.
Installation
First, create and activate a Python Virtual Environment for this project.
$ python3 -m venv testenv
$ source testenv/bin/activate
$ pip3 install -r requirements.txt
Next, install the binary browser drivers, so that you can programmatically
interact with the browser.
# Web driver for Chrome/Chromium
$ brew install chromedriver

# Web driver for Firefox
$ brew install geckodriver
Starting Gigantum ""Client-Under_test""
Before running the harness, ensure the Gigantum client is installed and running
# Testing the stable build
$ pip3 install gigantum && gigantum install && gigantum start

# Testing the ""edge"" build
$ pip3 install gigantum && gigantum install -e && gigantum start -e

Usage
To run ALL tests, using regular Chrome driver.
Note, this may take a while.
# Put a valid username and password into the untracked credentials.txt
$ echo -e ""my_username\nmy_password"" > credentials.txt

# Now, run the driver!
$ python3 driver.py

To run only example tests in headless mode.
$ python3 driver.py test_examples --headless

To run ALL tests using the Firefox driver
$ python3 driver.py --firefox

Organization
The file driver.py contains the main script to prepare, execute, and clean up test runs.
The directory gigantum_tests contains Python files containing individual tests.
Tests methods must be prefaced by test_, and should use the assert method for tests.
",3
7,dequelabs/cauldron-react,JavaScript,"Cauldron React
This project is used internally by Deque Systems and is only updated when Deque needs changes for our internal use. You are free to use this project and to learn from the patterns and techniques that we used to make the widgets accessible. However we do not intend to respond to questions, feature requests, fix bugs or integrate external pull requests unless we find ourselves sitting around one day with nothing better to do. We promise, in return, not to submit questions, feature requests, bugs and pull requests to your internal projects.

Installation
$ npm install cauldron-react --save
NOTE: it is expected that you include the css from deque-pattern-library
Demo App
To document through example and make development / manual testing easy, there is a demo app which can be started by executing:
$ yarn dev
see the demo/ directory
Build
$ yarn build
NOTE: for production build run $ yarn prepack
Test
$ yarn test
or to have tests automagically re-run when files change
$ yarn --watch
Publishing
Publishing cauldron-react to the npm registry is handled by CircleCI. All (green) commits that land in the master branch will be released as a ""canary"" version (eg 1.2.3-canary.GIT_SHA) and will be available with the @next dist tag. Additionally, all (green) tags that resemble a SEMVER version will be published as stable versions (eg 1.2.3) and available with the @latest dist tag.
To install the latest canary version, do: npm install cauldron-react@next. To install the latest stable version, do npm install cauldron-react.
To publish a stable version, you'll do something like this:
# Ensure you have the latest code
$ git checkout master
$ git pull
# Run the release script
$ npm run release
# push it
$ git push --follow-tags origin master && npm publish

",4
8,c-koi/gmic-qt,C++,"G'MIC-Qt: a versatile G'MIC plugin
Purpose
G'MIC-Qt is a versatile front-end to the image processing framework
G'MIC.  It is in fact a plugin for
GIMP, Krita, and Paint.NET, as well as a standalone application.
Authors

Sébastien Fourey
David Tschumperlé (G'MIC lib & original GTK-based plugin)

Contributors

Boudewijn Rempt boud@valdyas.org (Krita compatibility layer, work in progress)
Nicholas Hayes (Paint.NET compatibility layer, work in progress)

Translators

Jan Helebrant (Czech translation)
Frank Tegtmeyer (German translation)
chroma_ghost & bazza/pixls.us (Spanish translation)
Sébastien Fourey (French translation)
Duddy Hadiwido (Indonesian translation)
Francesco Riosa (Italian translation)
iarga / pixls.us (Dutch translation)
Alex Mozheiko (Polish translation)
maxr (Portuguese translation)
Alex Mozheiko (Russian translation)
Andrex Starodubtsev (Ukrainian translation)
LinuxToy (https://twitter.com/linuxtoy) (Chinese translation)
omiya tou tokyogeometry@github (Japanese translation)

Official (pre-release) binary packages

Available at gmic.eu

Tavis CI last build status

Master branch (Linux) 
Devel branch (Linux) 

Build instructions
By default, the gimp integration plugin is built.
QMake
qmake is simple to use but only really works in an environment where bash is available.
git clone https://github.com/dtschump/gmic.git
git clone https://github.com/c-koi/gmic-qt.git
make -C gmic/src CImg.h gmic_stdlib.h
cd gmic-qt
qmake [HOST=none|gimp|krita|paintdotnet]
make
CMake
cmake works on all platforms. The first part is the same and requires make and wget to be available. If you don't have all dependencies, cmake will warn you which ones are missing. Note that the minimum cmake version is 3.1.
git clone https://github.com/dtschump/gmic.git
git clone https://github.com/c-koi/gmic-qt.git
make -C gmic/src CImg.h gmic_stdlib.h
cd gmic-qt
Then make a build directory:
mkdir build
cd build
cmake .. [-DGMIC_QT_HOST=none|gimp|krita|paintdotnet] [-DGMIC_PATH=/path/to/gmic] [-DCMAKE_BUILD_TYPE=[Debug|Release|RelwithDebInfo]
make
",44
9,GeneZharov/introversion,JavaScript,"Introversion.js
Tool for debugging JavaScript expressions. Works great with functional code.
Table of Contents

Motivation
Installation
Watchers

logV()
logF()
logM()


Timers

time(), timeEnd()
stopwatch()
timeF()
timeM()
timeRun()


Modes

Quiet mode (logV_(), v_()...)
Breakpoint mode (debV(), ...)
Guards
Mute mode (.mute)


Configuration

Default configuration (setDefaults())
Instance configuration (instance())
In-place configuration (.with())


Options

General options
Formatting options
Stack trace options
In-place options


Advanced installation

Default import
Setup in global variable
Zero-conf for Node.js


License

Motivation
Suppose you have an arrow function, and you need to know a value inside of this
function:
const fn = n => n + 1; // what's in “n”?
In order to use console.log() you'll have to rewrite this function into
multiple statements:
const fn = n => {
  console.log(n);
  return n + 1;
};
Introversion allows you to simply wrap the desired value without rewriting
anything:
const fn = n => logV(n) + 1; // log value
// ...or
const fn = logF(n => n + 1); // log every function call (arguments and return value)
React component
A real-world example for a functional React component that makes you hate
console.log().
...
renderSuggestion={item => (
  <MenuItem
    text={logV(item).name}
    onClick={_ => this.toggle(item.id)}
  />
)}
Performance
Imagine you need to check if a function call is fast enough to decide whether
you need to cache it somewhere. With Introversion you can do it by simply
wrapping the desired expression in timeRun(() => <expr>) right in JSX:
// before
<Select options={states.map(transform) /* is map() too slow? */} />

// after
<Select options={timeRun(() => states.map(transform)) /* prints 2.73ms */} />
Functional programming
Since Introversion is functional, it tries not to interfer in program's logic,
but to seamlessly proxy input and output values, so it makes it easy to debug
functional code. For example, to research a function composition for issues.
import { groupBy, omitBy, mapValues } from ""lodash/fp"";

const build = pipe([
  groupBy(o => o.key),
  logF(omitBy(x => x.length > 1)), // print what goes on the 2nd step and what comes out
  mapValues(([o]) => o.uuid)
]);
Installation
npm install introversion --save-dev
Advanced installation cases are described below (default
import, setup in global
variable, zero-conf for
Node.js)
Watchers
logV()
Alias: v() (helpful with default import: In.v())
logV() (“v” stands for “value”) merely prints an array of its arguments. The
main difference between console.log() is that the last argument is returned.
Therefore it is safe to wrap any expression in logV() without breaking your
code down.
const random = n => Math.floor(logV(Math.random()) * n) + 10;
random(1); //=> logV() [ 0.5909956243063763 ]
You can print any other values alongside with the wrapped expression. Just pass
them as arguments. Only the last argument is returned, so extra arguments won't
affect your code:
const random = n => Math.floor(logV(num, this, ""mystr"", Math.random()) * n) 10;
random(1); //=> logV [ 1, {}, 'mystr', 0.8474771121023132 ]
You can use extra arguments to distinguish different watchers from each other
in the log:
const fn = n => n > 0 ? logV(true, n * 1.25) : logV(false, n / 9);
fn(5);   //=> logV [ true, 6.25 ]
fn(-81); //=> logV [ false, -9 ]
logF()
Alias: f() (helpful with default import: In.f())
logF() (“f” stands for “function”) is designed to watch for function calls.
When a wrapped function is called, its arguments and a returned value are
logged. If a wrapped function throws an exception, that exception will be
logged and then rethrown again. A wrapped in the logF() function can be used
in the same way as an unwrapped one: all arguments, this and a returned value
will be proxied.
[1, 2].map(logF(n => 2 * n));

//=> logF()
//=> ... Params: [ 1, 0, [ 1, 2 ] ]
//=> ... Result: 2

//=> logF()
//=> ... Params: [ 2, 1, [ 1, 2 ] ]
//=> ... Result: 4
logF() can also accept additional arguments for printing just like logV()
does:
logF(""foo"", ""bar"", calculate)(1, 2, 3)

// => logF() [ ""foo"", ""bar"" ] <- extra arguments go here
// => ... Params: [ 1, 2, 3 ]
// => ... Result: 999
logM()
Alias: m() (helpful with default import: In.m())
logM() (“m” stands for “method”) is similar to logF(), but it will call
your method with correct this value. In order to use it, you need to split a
method call into an object, and a string that represents a path to the method.
A.B.C.method(5); // original call

logM(A,"".B.C.method"")(5); // wrapped method
logM(A.B,"".C.method"")(5); // ...the same
logM(A.B.C,"".method"")(5); // ...one more way to do it
The rest behavior does not differ from the logF() watcher.
Timers
time(), timeEnd()
A replacement for console.time/timeEnd() that use the most accurate source
of time available:

performance.now()
console.time/timeEnd()
Date.now()

time(); // start the timer
calculateEverything();
timeEnd(); // stop the timer

//=> timeEnd() 203 ms
Just like console timing methods, these functions accept an optional name for a
new timer. timeEnd() may also accept additional arguments for printing
alongside with the ellapsed time (not available with format: false and
console methods as a time source).
time(""label""); // start the timer named ""label""
calculateEverything();
timeEnd(""foo"", ""bar"", ""label""); // stop the timer named ""label""

//=> timeEnd() [ 'foo', 'bar', 'label' ] 203 ms
stopwatch()
When you have a sequence of actions, it is inconvenient to wrap every action in
time()...timeEnd(). In this case stopwatch api is more helpful.

stopwatch() — initially starts the timer.
lap([...args]) — prints the ellapsed time since the previous
stopwatch/lap(). Also prints optional arguments and starts a new timer
for the next lap.

stopwatch();

createTables();
lap(""created""); //=> lap() [ 'created' ] 15 ms

const rows = queryRows();
lap(""foobar"", rows, ""queried""); //=> lap() [ 'foobar', [], 'queried' ] 107 ms

populateState(rows);
lap(""populated""); //=> lap() [ 'populated' ] 768 ms
timeF()
You can wrap any function with timeF(). The result will be a function with
the same behavior as a wrapped one, but additionally it will print its
execution time of its synchronous code.
array.map(timeF(iterator));

//=> timeF() 4 ms
//=> timeF() 9 ms
//=> timeF() 1 ms
Optionally you can pass any arguments for printing:
array.map(timeF(""foo"", ""bar"", iterator));

//=> timeF() [ 'foo', 'bar' ] 4 ms
//=> timeF() [ 'foo', 'bar' ] 9 ms
//=> timeF() [ 'foo', 'bar' ] 1 ms
timeM()
Like timeF() but for methods.
// original method call
array.map(n => this.iterator(n));

// wrapped method call
array.map(n => timeM(this,"".iterator"")(n));

//=> timeM() 4 ms
//=> timeM() 9 ms
//=> timeM() 1 ms
Optionally you can pass any arguments for printing:
array.map(n => timeM(""foo"", ""bar"", this,"".iterator"")(n));

//=> timeM() [ 'foo', 'bar' ] 4 ms
//=> timeM() [ 'foo', 'bar' ] 9 ms
//=> timeM() [ 'foo', 'bar' ] 1 ms
timeRun()
Sometimes you suspect that some expression may be calculated for too long. In
this case it is convenient to wrap this expression into timeRun(() => <expr>)
that will print the ellapsed time.
// original expression
data = [calculate(src), readState()];

// wrapped expression
data = timeRun(() => [calculate(src), readState()]);

//=> timeRun() 349 ms
Optionally you can pass any arguments for printing:
data = timeRun(""data"", src, () => [calculate(src), readState()]);

//=> timeRun() [ 'data', ""DATABASE"" ] 349 ms
Modes
Quiet Mode

logV_(), alias: v_()
logF_(), alias: f_()
logM_(), alias: m_()

Sometimes you are not interested in a wrapped value itself, but you need to
know, that it was calculated. For example, in React Native an attempt to log an
event object may hang the application. Or maybe you are interested only in
printing additional arguments. For these cases, there are alternative quiet
mode watchers that don't log wrapped value itself but log all additional
arguments.
const fn = logF_(""Invoked!"", n => n + 1);
fn(2); //=> logF_() [ 'Invoked!' ]
Breakpoint Mode

debV()
debF()
debM()

Instead of printing data these functions create a breakpoint using debugger
statement. It can help to look around and walk through the call stack. An
underscore in function names symbolizes a pause in program execution.
Guards
Sometimes a watcher can produce too many outputs if it is called for too many
times. You may want to suppress excess outputs. Perhaps you need only the first
one or first ten outputs. In this case the in-place “guard” option may help.
More about the in-place configuration is described
below.
for (let i = 0; i < 1000; i++) {
  logV.with({ guard: 1 })(i); // prints only once
}
Guards need some way to distinguish one call from another to keep track of the
amount of executed calls. So if you have more than one call with a guard, you
need to explicitly identify a call with “id”"" option:
for (let i = 0; i < 1000; i++) {
  logV.with({ id: 1, guard: 1 })(i); // prints only once
  logV.with({ id: 2, guard: 10 })(arr[i]); // prints for the first 10 times
}
Mute Mode
Imagine, you have a function covered with unit tests. And 1 of 30 tests fails.
For debugging reasons, it's important to know a value deep inside of that
function. But if you log that value each time it is evaluated for every unit
test, there would be hundreds of log entries. In this case, the mute mode comes
to the rescue.
You can use a muted watcher, that is available for any watcher under the method
called mute() (e.g. logV.mute(), logV_.mute(), debV.mute(),
logF.mute(), ...). Muted watcher doesn't produce any logs or breakpoints
unless you explicitly unmute it (in the failed unit test for instance).

unmuteF(fn) — unmute everything during this function execution
unmuteRun(() => <expr>) — runs passed function and unmutes everything while
it is running
unmute() — to unmute all the muted functions
mute() — to mute everything again

Example:
// module.js
function action(x) {
  // ... big and complicated function
  const y = x * 8;
  return y ^ Math.PI;
}

// module.spec.js
describe(""action()"", () => {
  // ... lots of unit tests
  it(""should perform a complex calculation"", () => {
    const res = action(2);
    expect(res).toBe(16);
  });
});
First we need to wrap a desired expression in a muted watcher:
// module.js
function action(x) {
  ...
  return logV.mute(y) ^ Math.PI;
}
Then we need to unmute a muted watcher at the desired moment (in the failed
unit test in this case):
// module.spec.js

// unmute a function
const res = unmuteF(action)(2);

// or unmute an expression
const res = unmuteRun(() => action(2));

// or unmute anything in a low level imperative style
unmute();
const res = action(2);
mute();
Configuration
Default Configuration
You can pass any number of default options as object properties:
setDefaults({
  format: false,
  log: (...xs) => Reactotron.log(xs),
  warn: (...xs) => Reactotron.warn(xs)
});
Instance configuration
You can have many instances of Introversions with different configurations:
const InR = instance({
  format: false,
  log: (...xs) => Reactotron.log(xs),
  warn: (...xs) => Reactotron.warn(xs)
});

const InX = instance({
  format: false,
  log: (...xs) => xscript.response.write(xs.join("" "") + ""\n""),
  warn (...xs) => xscript.response.write(xs.join("" "") + ""\n"")
})
In-place configuration
Most functions have method with() for setting temporary local configuration
options only for this call.
logV.with({ depth: Infinity })(myobj);
Options
General Options


log
A function that accepts any number of any arguments and prints them to the
log.
Default: (...xs) => console.log(...xs)
Examples:
(...xs) => Reactotron.log(xs)
(...xs) => xscript.response.write(xs.join("" "") + ""\n"")


warn
A function that accepts any number of any arguments and prints them as
warnings to the log.
Default: (...xs) => console.warn(...xs)
Example:
(...xs) => Reactotron.warn(xs)


timer

""auto"" — use the most accurate source of time available
""performance"" — use performance.now()
""console"" — use console.time/timeEnd()
""date"" — use Date.now()
() => number — custom user function that returns time in milliseconds

To offer protection against timing attacks and fingerprinting, the
precision of Date.now() might get rounded depending on the environment.
So consider use of “repeat” option to increase preciseness in this case
(see below).
Default: ""auto""


clone

""auto"" — clone all the values before printing if DevTools are detected
true / false — whether to deeply clone all values for printing

Default: ""auto""


errorHandling

""warn"" — output errors as warnings and try to fallback on default behavior
""throw"" — always throw an exception on error

Default: ""warn""


devTools
For some options it is important if DevTools are connected to the program.
Introversion tries to detect DevTools with a test output to the log. To
skip it, you can explicitly specify presense of DevTools with this option.
Or you can explicitly specify all the options that depends on DevTools
(currently these are “clone”, “format”, “formatErrors”).

""auto"" — detect DevTools with a test output to the log
true / false — whether DevTools are connected

Default: ""auto""


dev
If true Introversion utilities will additionally print a configuration
object they are using and a stack trace including the detected user code
position in it that is useful for configuring “stackTraceShift” option.
Default: false


Formatting Options


format

""auto"" — detect the environment
true — optimized for browsers and sophisticated tools like DevTools
false — optimized for text output, e.g. to a terminal by Node.js

When “format” is enabled:

stringifies printed objects in a pretty way
Only single log() call is made with a single formatted string as an
argument
empty line after each output

Not formatted output:
logF() at myfunc (index.js:10:23)
... Params: [ 1, 0, [ 1, 2, 3 ] ]
... Result: 1

Formatted output:
logF() at myfunc (index.js:10:23)
--- Params ---
[ 1, 0, [ 1, 2, 3 ] ]
--- Result ---
1

Default: ""auto""


formatErrors
Similar to the “format” option, but for errors and warnings.

""auto"" — detect the environment
true — optimized for browsers and sophisticated tools like DevTools
false — optimized for text output, e.g. to a terminal by Node.js

Not formatted output:
Introversion Warning

Unknown option stakcTrace

Formatted output:
▒  Introversion
▒
▒  Unknown option stakcTrace
▒
▒  at validateConf (introversion.js:780:7)
▒  at (introversion.js:898:31)
▒  at Object.<anonymous> (index.js:35:4)
▒  at Module._compile (loader.js:723:30)
▒  at Object.Module._extensions..js (loader.js:734:10)
▒  at Module.load (loader.js:620:32)
▒  at tryModuleLoad (loader.js:560:12)

Default: ""auto""


highlight
If true, Introversion will try to highlight the output for terminals.
Default: ""auto""


inspectOptions
Options that will be proxied to the node's util.inspect()
Default: ""auto""


precision
Number of digits after the point for the time measured by
timers.
Default: 2


Stack Trace Options


stacktrace
При распечатке в лог Интроверсия умеет выводить имя функции, из которой она
была вызвана, имя файла, номер строки и столбца. Что именно выводить
настраивается этой опцией с помощью массива ключевых слов.

""auto"" — detect the environment
Array<""func"" | ""file"" | ""line"" | ""col"">
true — print everything, shorthand for [""func"", ""file"", ""line"", ""col""]
false — print nothing, shorthand for []

Default: ""auto""


stackTraceAsync

true — распечатка будет происходит асинхронно. При этом будет use
source maps and guess anonymous functions. It can make network requests
for source maps and wait for its completion. Но из-за асинхронности вывод
может выглядеть непоследовательным.
false — synchronous behavior, won't use source maps or guess anonymous
functions.
auto — try to use asynchronous behavior if available.

Default: ""auto""


stackTraceShift
Итроверсия знает на какой глубине в стеке вызовов должен находиться
пользовательский код, который её вызывал. Однако, в некоторых окружениях,
модуль интроверсии может быть обёрнут ещё в какой-то код. Например, React
Native увеличивает стэк вызовов не 1. Из-за Интроверсия может ошибиться с
положением пользовательского кода в stack trace. Эта опция позволяет
исправить эту ошибку.
logV.with({ dev: true })
// ...
// --- Dev: stacktrace ---
//  0  — at getTrace (introversion.js:989:21)
//  1  — at logVal (introversion.js:1048:3)
//  2  — at (introversion.js:1414:12)
//  3  — at (introversion.js:869:12)
// [4] — at Object.<anonymous> (2.js:5:25)
//  5  — at Module._compile (loader.js:723:30)
//  6  — at Object.Module._extensions..js (loader.js:734:10)
//  7  — at Module.load (loader.js:620:32)
You can see that Introversion suppose that user call was in the 4th
position, but actually it was in the 5th. So you can set stackTraceShift:  1 to correct this.
Default: ""auto""


In-Place Options


id
Makes a watcher/timer unique. It helps Introversion to distinguish
watchers/timers from each other. This option is required in order to use
“guard” options.
ID can be of any type, but if you are using console as a time source, then
the value will be internally converted to a string since it is required by
the console spec.


guard
Sets how may times a watcher or a timer will be functional. After the
number of calls exceedes, the call will act just like an original
value/function/method without any additional behavior like printing or
debugging.
See guards for a complete description.
Default: Infinity
const fn = logF.with({ id: 0, guard: 1 })(n => n % 2); // prints only once
[1, 2, 3, 4, 5].map(fn);

//=> logF()
//=> ... Params: [ 1, 0, [ 1, 2, 3, 4, 5 ] ]
//=> ... Result: 1


repeat
The number of times to repeat the measure in order to increase preciseness.
Especially useful with Date.now() as a time source, because its precision
is rounded.
The value can be either a number or a string with a special suffix (""K"",
""M"", ""G"") for big numbers. For example:

""5K"" stands for 5,000
""1.5M"" stands for 1,500,000
""10G"" stands for 10,000,000,000

Default: 1
timeRun.with({ repeat: 5000 })(mergeDatabases);
// ...or
timeRun.with({ repeat: ""5K"" })(mergeDatabases);

//=> timeRun() 0.113 ms


Advanced installation
Default import
Introversion has a default export:
import In from ""introversion"";

In.v(""foobar"");
In this case short aliasesv(), f(), m() and their quiet alternatives
v_(), f_(), m_() are especially helpful.
ImportJS
If you use ImportJS and want it to
automatically add Introversion's as a default import, like in the example
above, then set the desired alias in the configuration file.
// .importjs.js

module.exports = {
  aliases: { In: ""introversion"" }
};
Setup in global variable
Sometimes, it is convenient to setup Introversion in global scope. In order to
do this you can import the following script in your main file:
// src/globals.js

import In from ""introversion"";

In.setDefaults({...}); // if necessary
window.In = In; // for browser
global.In = In; // for node
Jest
If you are using Introversion in Jest unit tests:
// package.json

""jest"": {
  ""setupFiles"": [""<rootDir>/src/globals.js""]
}
Flow
If you are using Introversion with Flow, then you'll have
to specify type of the global variable with a libdef file:
// flow-typed/introversion.js

declare var In: any;
If you want to specify the shape of the API, then you can copypaste it from the
libdef
script.
Zero-conf for Node.js
Introversion can work with Node.js without need to initialize and write
imports. To make API available in scripts you need to run node with -r  introversion/init option, that will write API into the global variable In.
node -r introversion/init myfile.js
// myfile.js
In.v(""working without initialization!"");
Introversion initialized this way can be configured with environment variables:

INTROVERSION_NAME — the name for the global variable (In by default)
INTROVERSION_CONF — js object with the configuration
INTROVERSION_CONF_FILE — path to the CommonJS module that exports the
configuration object

Examples:
INTROVERSION_NAME='I' node -r introversion/init myfile.js
INTROVERSION_CONF='{ stackTrace: false }' node -r introversion/init myfile.js
INTROVERSION_CONF_FILE=~/.introversion-conf.js node -r introversion/init myfile.js
You can go further and create an alias for node with initialized Introversion
for debugging small scripts that you don't want to over bloat with extra code.
You can put these commands in ~/.bashrc, ~/.zshrc, etc. Introversion should
be installed globally in this case.
npm install -g introversion

alias nodein='node -r introversion/init'
alias babel-nodein='babel-node -r introversion/init'
alias ts-nodein='ts-node -r introversion/init'
...
License
MIT
",8
10,dem-net/DEM.Net,C#,"
DEM.Net
Digital Elevation Model library in C#

Elevation queries (point, polylines, heightmap, GPX)
3D export (glTF, STL)
Imagery (MapBox, OSM, Stamen) : textured 3D models and normal maps
No setup
Automatic DEM file download from openTopography.org
Fast and optimized queries

See samples here

Supported formats and datasets
Input

GeoTIFF (JAXA AW3D, and any GeoTIFF)
HGT (Nasa SRTM)

Output

glTF

Current dev status

All incoming features are listed in the project board here : https://github.com/xfischer/DEM.Net/projects/1.
Feel free to suggest any idea you'd like to see covered here in the issues : https://github.com/xfischer/DEM.Net/issues.

SampleApp
(Work in progress)
SampleApp is a Console App used for test purposes, full of samples. It's pretty messy and lacks documentation but names are self explanatory.
How do I use the API ?
Raster operations

Use elevationService.DownloadMissingFiles(DEMDataSet.AW3D30, <bbox>) to download and generate metadata for a given dataset.
Supported datasets : SRTM GL1 and GL3 (HGT files), AWD30 (GeoTIFF)
Use new RasterService().GenerateReport(DEMDataSet.AW3D30, <bounding box>) to download only necessary tiles using remote VRT file.
Use rasterService.GenerateFileMetadata(<path to file>, DEMFileFormat.GEOTIFF, false, false) to generate metada for an arbitrary file.
Use RasterService.GenerateDirectoryMetadata(samplePath);to generate metadata files for your raster tiles.
These metadata files will be used as an index when querying Digital Elevation Model data.

Elevation operations

GetLineGeometryElevation
GetPointElevation

glTF export

glTFService can generate triangulated MeshPrimitives from height maps
Export to .gtlf or .glb

Sample data

Rasters from http://www.opentopography.org
Dataset used is ""ALOS World 3D - 30m"" : http://opentopo.sdsc.edu/lidar?format=sd&platform=Satellite%20Data&collector=JAXA
For development and tests, files covering France were used.
Not used yet but worth mentionning :
For sea bed elevation : ETOPO1 Global Relief Model https://www.ngdc.noaa.gov/mgg/global/global.html

Acknowledgements / Sources

https://github.com/stefangordon/GeoTiffSharp from @stefangordon which provided a good starting point.
Pedro Sousa : http://build-failed.blogspot.fr/2014/12/processing-geotiff-files-in-net-without.html for good explanations.
Mathieu Leplatre for http://blog.mathieu-leplatre.info/drape-lines-on-a-dem-with-postgis.html
Andy9FromSpace : HGT file reader in https://github.com/Andy9FromSpace/map-elevation

Third party code and librairies

glTF : glTF2Loader and AssetGenerator : https://github.com/KhronosGroup/glTF
Tiff support : https://github.com/BitMiracle/libtiff.net
Serialization : https://github.com/neuecc/ZeroFormatter and https://github.com/JamesNK/Newtonsoft.Json
System.Numerics.Vectors for Vector support
GPX reader from dlg.krakow.pl

",27
11,damng/hackernews-rss-with-inlined-content,Python,"hackernews-rss-inlined-content
Loads the hackerness rss and inlines the contents of the pages. Chrome with Selenium loads the page, dom-distiller makes the contents like they're in firefox's reader mode, and the resulting html is served as the entry description. The 300 or so entries become about 5mb. PDFs and things that yield no usible text preview will remain as they were on the old feed.
I invoke it as:
  xvfb-run python main.py

I used xvfb-run instead of headless mode because extensions are not supported in headless. One directory up, in ""../dat"", is a chrome user profile data directory that is copied for each instance of the browser run and deleted when finished. You can initialize it and add whatever extensions you want. The resulting rss file is then commited/pushed on here and served via gitpages.
This is hack level code.
The feed is available at https://damng.github.io/hackernews-rss-with-inlined-content/output.rss
If you find this useful, don't give me anything. Instead, go to http://templeos.org, donate to the TempleOS project and become a Templar. Dontate to the Brain and Behavior Research Foundation (https://www.bbrfoundation.org/).
",10
12,NOAA-GFDL/MOM6,Fortran,"


MOM6
This is the MOM6 source code.
Where to find information
Start at the MOM6-examples wiki which has installation instructions.
Source code documentation is hosted on read the docs.
What files are what
The top level directory structure groups source code and input files as follow:



File/directory
Purpose




LICENSE.md
A copy of the Gnu lesser general public license, version 3.


README.md
This file with basic pointers to more information.


src/
Contains the source code for MOM6 that is always compiled.


config_src/
Contains optional source code depending on mode and configuration such as dynamic-memory versus static, ocean-only versus coupled.


pkg/
Contains third party (non-MOM6 or FMS) code that is compiled into MOM6.


docs/
Workspace for generated documentation.



Disclaimer
The United States Department of Commerce (DOC) GitHub project code is provided
on an ""as is"" basis and the user assumes responsibility for its use. DOC has
relinquished control of the information and no longer has responsibility to
protect the integrity, confidentiality, or availability of the information. Any
claims against the Department of Commerce stemming from the use of its GitHub
project will be governed by all applicable Federal law. Any reference to
specific commercial products, processes, or services by service mark,
trademark, manufacturer, or otherwise, does not constitute or imply their
endorsement, recommendation or favoring by the Department of Commerce. The
Department of Commerce seal and logo, or the seal and logo of a DOC bureau,
shall not be used in any manner to imply endorsement of any commercial product
or activity by DOC or the United States Government.
This project code is made available through GitHub but is managed by NOAA-GFDL
at https://gitlab.gfdl.noaa.gov.
",60
13,dotnet/docs.fr-fr,PowerShell,"Documentation de .NET
Ce référentiel contient la documentation conceptuelle de .NET. Le site de la documentation de .NET repose sur plusieurs référentiels en plus de celui-ci :

Exemples et extraits de code
Informations de référence sur les API
Informations de référence sur le kit SDK .NET Compiler Platform

Le suivi des problèmes et des tâches de ces trois référentiels se trouve ici. Nous avons une grande communauté qui utilise ces ressources. Nous nous efforçons de répondre aux problèmes dans les meilleurs délais. Vous pouvez en savoir plus sur nos procédures de classification et de résolution des problèmes dans notre rubrique Politique des problèmes.
Nous portons un grand intérêt aux contributions qui nous aident à améliorer et à compléter la documentation de .NET. Pour contribuer, consultez les Projets pour les contributeurs de la communauté .NET et partagez vos idées. Le Guide de contribution contient des instructions sur les procédures que nous utilisons. Vous pouvez aussi consulter la liste des problèmes relatifs aux tâches qui vous intéressent.
Nous prévoyons que Xamarin, Mono et Unity feront également partie de cette documentation.
Ce projet a adopté le code de conduite défini dans la charte des contributeurs afin que le comportement à observer dans notre communauté soit clair.
Pour plus d’informations, consultez le code de conduite de .NET Foundation.
",5
14,coveo/tgf,Go,"TGF



A Terragrunt frontend that allow execution of Terragrunt/Terraform through Docker.
Table of content:

Description
Configuration
Invocation arguments
Images
Usage
Development

Description
TGF is a small utility used to launch a Docker image and automatically map the current folder, your HOME folder and your current environment
variables to the underlying container.
By default, TGF is used as a frontend for terragrunt, but it could also be used to run different endpoints.
Why use TGF
Using TGF ensure that all your users are using the same set of tools to run infrastructure configuration even if they are working on different environments (linux, Microsoft Windows, Mac OSX, etc).
Terraform is very sensitive to the version used and if one user update to a newer version, the state files will be marked with the latest version and
all other user will have to update their Terraform version to the latest used one.
Also, tools such as AWS CLI are updated on a regular basis and people don't tend to update their version regularly, resulting in many different version
among your users. If someone make a script calling a new feature of the AWS api, that script may break when executed by another user that has an
outdated version.
Installation
On mac and linux:
You can run the get-latest-tgf.sh script to check if you have the latest version of tgf installed and install it if needed:
curl https://raw.githubusercontent.com/coveo/tgf/master/get-latest-tgf.sh | bash

PS: The GITHUB_TOKEN environement variable can be set to make authenticated requests to github.

On Windows with Powershell:
[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12
Invoke-WebRequest https://github.com/coveo/tgf/releases/download/v1.20.2/tgf_1.20.2_windows_64-bits.zip -OutFile tgf.zip
Configuration
TGF has multiple levels of configuration. It first looks through the AWS parameter store
under /default/tgf using your current AWS CLI configuration if any. There it tries to find parameters called config-location (example: bucket.s3.amazonaws.com/foo) and config-paths (example: my-file.json:my-second-file.json, default: TGFConfig). If it finds config-location, it fetches its config from that path using the go-getter library. Otherwise, it looks directly in SSM for configuration keys (ex: /default/tgf/logging-level).
TGF then looks for a file named .tgf.config or tgf.user.config in the current working folder (and recursively in any parent folders) to get its parameters. These configuration files overwrite the remote configurations.
Your configuration file could be expressed in  YAML or JSON
Example of YAML configuration file:
docker-refresh: 1h
logging-level: notice
Example of JSON configuration file:
{
  ""docker-refresh"": ""1h"",
  ""logging-level"": ""notice""
}
Configuration keys



Key
Description
Default value




docker-image
Identify the docker image  to use
coveo/tgf


docker-image-version
Identify the image version



docker-image-tag
Identify the image tag (could specify specialized version such as k8s, full)
latest


docker-image-build
List of Dockerfile instructions to customize the specified docker image)



docker-image-build-folder
Folder where the docker build command should be executed



docker-refresh
Delay before checking if a newer version of the docker image is available
1h (1 hour)


docker-options
Additional options to supply to the Docker command



logging-level
Terragrunt logging level (only apply to Terragrunt entry point).Critical (0), Error (1), Warning (2), Notice (3), Info (4), Debug (5), Full (6)
Notice


entry-point
The program that will be automatically launched when the docker starts
terragrunt


tgf-recommended-version
The minimal tgf version recommended in your context  (should not be placed in .tgf.config file)
no default


recommended-image
The tgf image recommended in your context (should not be placed in .tgf.config file)
no default


environment
Allows temporary addition of environment variables
no default


run-before
Script that is executed before the actual command
no default


run-after
Script that is executed after the actual command
no default


alias
Allows to set short aliases for long commandsmy_command: ""--ri --with-docker-mount --image=my-image --image-version=my-tag -E my-script.py""
no default



Note: The key names are not case sensitive
Configuration section
It is possible to specify configuration elements that only apply on specific os.
Example of HCL configuration file:
docker-refresh: 1h
logging-level: notice
windows:
  logging-level: debug
linux:
  docker-refresh: 2h




section
Description




windows
Configuration that is applied only on Windows systems


linux
Configuration that is applied only on Linux systems


darwin
Configuration that is applied only on OSX systems


ix
Configuration that is applied only on Linux or OSX systems



TGF Invocation
> tgf -H
usage: tgf [<flags>]

DESCRIPTION: TGF (terragrunt frontend) is a Docker frontend for terragrunt/terraform. It automatically maps your current
folder, your HOME folder, your TEMP folder as well of most environment variables to the docker process. You can add -D to your command to get the
exact docker command that is generated.

It then looks in your current folder and all its parents to find a file named '.tgf.config' to retrieve the default configuration. If not all
configurable values are satisfied and you have an AWS configuration, it will then try to retrieve the missing elements from the AWS Parameter Store
under the key '/default/tgf'.

Configurable values are: docker-image, docker-image-version, docker-image-tag, docker-image-build, docker-image-build-folder,
docker-image-build-tag, logging-level, entry-point, docker-refresh, docker-options, recommended-image-version, required-image-version,
tgf-recommended-version, environment, run-before, run-after, alias.

You can get the full documentation at https://github.com/coveo/tgf/blob/master/README.md and check for new version at
https://github.com/coveo/tgf/releases/latest.

Any docker image could be used, but TGF specialized images could be found at: https://hub.docker.com/r/coveo/tgf/tags.

Terragrunt documentation could be found at https://github.com/coveo/terragrunt/blob/master/README.md (Coveo fork)

Terraform documentation could be found at https://www.terraform.io/docs/index.html.

IMPORTANT: Most of the tgf command line arguments are in uppercase to avoid potential conflict with the underlying command. If any of the tgf
arguments conflicts with an argument of the desired entry point, you must place that argument after -- to ensure that they are not interpreted by
tgf and are passed to the entry point. Any non conflicting argument will be passed to the entry point wherever it is located on the invocation
arguments.

  tgf ls -- -D   # Avoid -D to be interpreted by tgf as --debug-docker

It is also possible to specify additional arguments through environment variable TGF_ARGS or enable debugging mode through TGF_DEBUG.

VERSION: 1.20.2

AUTHOR: Coveo

Flags:
  -H, --tgf-help                 Show context-sensitive help (also try --help-man).
      --all-versions             Get versions of TGF & all others underlying utilities (alias --av)
      --current-version          Get current version information (alias --cv)
  -D, --debug-docker             Print the docker command issued
  -F, --flush-cache              Invoke terragrunt with --terragrunt-update-source to flush the cache
      --get-image-name           Just return the resulting image name (alias --gi)
      --interactive              On by default, use --no-interactive or --no-it to disable launching Docker in interactive mode or set
                                 TGF_INTERACTIVE to 0 or false
      --local-image              If set, TGF will not pull the image when refreshing (alias --li)
      --no-docker-build          Disable docker build instructions configured in the config files (alias --nb)
      --no-home                  Disable the mapping of the home directory (alias --nh) or set TGF_NO_HOME
      --no-temp                  Disable the mapping of the temp directory (alias --nt) or set TGF_NO_TEMP
      --no-aws                   Disable use of AWS to get configuration (alias --na) or set TGF_NO_AWS
      --prune                    Remove all previous versions of the targeted image
      --refresh-image            Force a refresh of the docker image (alias --ri)
      --with-current-user        Runs the docker command with the current user, using the --user arg (alias --cu)
      --with-docker-mount        Mounts the docker socket to the image so the host's docker api is usable (alias --wd)
      --config-files=<files>     Set the files to look for (default: TGFConfig) or set TGF_CONFIG_FILES
      --config-location=<path>   Set the configuration location or set TGF_CONFIG_LOCATION
      --docker-arg=<opt> ...     Supply extra argument to Docker (alias --da)
  -E, --entrypoint=terragrunt    Override the entry point for docker
      --ignore-user-config       Ignore all tgf.user.config files (alias --iuc)
      --image=coveo/tgf          Use the specified image instead of the default one
      --image-version=version    Use a different version of docker image instead of the default one (alias --iv)
  -L, --logging-level=<level>    Set the logging level (critical=0, error=1, warning=2, notice=3, info=4, debug=5, full=6) or set TGF_LOGGING_LEVEL
      --mount-point=MOUNT-POINT  Specify a mount point for the current folder --mp)
  -P, --profile=PROFILE          Set the AWS profile configuration to use
      --ps-path=<path>           Parameter Store path used to find AWS common configuration shared by a team or set TGF_SSM_PATH
  -T, --tag=latest               Use a different tag of docker image instead of the default one

Example:
> tgf --current-version
tgf v1.18.1
Returns the current version of the tgf tool
> tgf -- --version
terragrunt version v1.2.0
Returns the version of the default entry point (i.e. Terragrunt), the --version located after the -- instructs tgf to pass this argument
to the desired entry point
> tgf -E terraform -- --version
Terraform v0.11.8
Returns the version of Terraform since we specified the entry point to be terraform.
Default Docker images
Base image: coveo/tgf.base (based on Alpine)

Terraform
Terragrunt
Go Template
Shells & tools

sh
openssl



Default image: coveo/tgf (based on Alpine)
All tools included in coveo/tgf:base plus:

Python (2 and 3)
Ruby
AWS CLI
jq
Terraforming
Tflint
Terraform-docs
Terraform Quantum Provider
Shells

bash
zsh
fish


Tools & editors

vim
nano
zip
git
mercurial



AWS provider specialized image: coveo/tgf:aws (based on Alpine)
All tools included in coveo/tgf plus:

terraform-provider-aws (fork)

Kubernetes tools (based on Alpine)
All tools included in coveo/tgf:aws plus:

kubectl
helm

Full image: coveo/tgf:full (based on Ubuntu)
All tools included in the other images plus:

AWS Tools for Powershell
Oh My ZSH
Shells

powershell



Usage
As Terragrunt front-end
> tgf plan
Invoke terragrunt plan (which will invoke terraform plan) after doing the terragrunt relative configurations.
> tgf apply -var env=dev
Invoke terragrunt apply (which will invoke terraform apply) after doing the terragrunt relative configurations. You can pass any arguments
that are supported by terraform.
> tgf plan-all
Invoke terragrunt plan-all (which will invoke terraform plan on the current folder and all sub folders). Terragrunt allows xxx-all operations to be
executed according to dependencies that are defined by the dependencies statements.
Other usages
> tgf -e aws s3 ls
Invoke AWS CLI as entry point and list all s3 buckets
> tgf -e fish
Start a shell fish in the current folder
> tgf -e my_command -i my_image:latest
Invokes my_command in your own docker image. As you can see, you can do whatever you need to with tgf. It is not restricted to only the pre-packaged
Docker images, you can use it to run any program in any Docker images. Your imagination is your limit.
Development
Build are automatically launched on tagging.
Tags with format image-0.0.0 automatically launch a Docker images build that are available through Docker Hub.
Tags with format v0.0.0 automatically launch a new release on Github for the TGF executable.
",43
15,dataquestio/solutions,Jupyter Notebook,"Dataquest Project Solutions
This repository is a series of notebooks that show solutions for the projects at Dataquest.io.
Of course, there are always going to be multiple ways to solve any one problem, so these notebooks just show one possible solution.

Guided Project: Explore U.S. Births
Guided Project: Customizing Data Visualizations
Guided Project: Star Wars survey
Guided Project: Police killings
Guided Project: Visualizing Pixar's Roller Coaster
Guided Project: Using Jupyter Notebook
Guided Project: Analyzing movie reviews
Guided Project: Winning Jeopardy
Guided Project: Predicting board game reviews
Guided Project: Predicting bike rentals
Guided Project: Preparing data for SQLite
Guided Project: Creating relations in SQLite
Guided Project: Analyzing NYC High School Data
Guided Project: Visualizing Earnings Based On College Majors
Guided Project: Exploring Gun Deaths in the US
Guided Project: Analyzing Thanksgiving Dinner
Guided Project: Analyzing Wikipedia Pages
Guided Project: Analyzing Stock Prices
Guided Project: Creating A Kaggle Workflow
Guided Project: Analyzing Startup Fundraising Deals from Crunchbase
Guided Project: Predicting House Sale Prices
Guided Project: Answering Business Questions using SQL
Guided Project: Designing and Creating a Database
Guided Project: Investigating Fandango's Movie Rating System
Guided Project: Forest Fires Data
Guided Project: NYC Schools Perceptions
Guided Project: Clean and Analyze Employee Exit Surveys

",396
16,bazelbuild/rules_jvm_external,Python,"rules_jvm_external
Transitive Maven artifact resolver as a repository rule.

Features

WORKSPACE configuration
JAR, AAR, source JARs
Custom Maven repositories
Private Maven repositories with HTTP Basic Authentication
Artifact version resolution with Coursier
Reuse artifacts from a central cache
Versionless target labels for simpler dependency management
Ability to declare multiple sets of versioned artifacts
Supported on Windows, macOS, Linux

Get the latest release
here.
Usage
List the top-level Maven artifacts and servers in the WORKSPACE:
load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"")

RULES_JVM_EXTERNAL_TAG = ""2.1""
RULES_JVM_EXTERNAL_SHA = ""515ee5265387b88e4547b34a57393d2bcb1101314bcc5360ec7a482792556f42""

http_archive(
    name = ""rules_jvm_external"",
    strip_prefix = ""rules_jvm_external-%s"" % RULES_JVM_EXTERNAL_TAG,
    sha256 = RULES_JVM_EXTERNAL_SHA,
    url = ""https://github.com/bazelbuild/rules_jvm_external/archive/%s.zip"" % RULES_JVM_EXTERNAL_TAG,
)

load(""@rules_jvm_external//:defs.bzl"", ""maven_install"")

maven_install(
    artifacts = [
        ""junit:junit:4.12"",
        ""androidx.test.espresso:espresso-core:3.1.1"",
    ],
    repositories = [
        # Private repositories are supported through HTTP Basic auth
        ""http://username:password@localhost:8081/artifactory/my-repository"",
        ""https://jcenter.bintray.com/"",
        ""https://maven.google.com"",
        ""https://repo1.maven.org/maven2"",
    ],
)
and use them directly in the BUILD file by specifying the versionless target alias label:
android_library(
    name = ""test_deps"",
    exports = [
        ""@maven//:androidx_test_espresso_espresso_core"",
        ""@maven//:junit_junit"",
    ],
)
Generated targets
For the junit:junit example, using bazel query @maven//:all --output=build, we can see that the rule generated these targets:
alias(
  name = ""junit_junit_4_12"",
  actual = ""@maven//:junit_junit"",
)

jvm_import(
  name = ""junit_junit"",
  jars = [""@maven//:https/repo1.maven.org/maven2/junit/junit/4.12/junit-4.12.jar""],
  srcjar = ""@maven//:https/repo1.maven.org/maven2/junit/junit/4.12/junit-4.12-sources.jar"",
  deps = [""@maven//:org_hamcrest_hamcrest_core""],
  tags = [""maven_coordinates=junit:junit:4.12""],
)

jvm_import(
  name = ""org_hamcrest_hamcrest_core"",
  jars = [""@maven//:https/repo1.maven.org/maven2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar""],
  srcjar = ""@maven//:https/repo1.maven.org/maven2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3-sources.jar"",
  deps = [],
  tags = [""maven_coordinates=org.hamcrest:hamcrest.library:1.3""],
)
The generated tags attribute value also contains the original coordinates of
the artifact, which integrates with rules like bazel-common's
pom_file
for generating POM files. See the pom_file_generation
example for more information.
API Reference
You can find the complete API reference at docs/api.md.
Advanced usage
Fetch source JARs
To download the source JAR alongside the main artifact JAR, set fetch_sources = True in maven_install:
maven_install(
    artifacts = [
        # ...
    ],
    repositories = [
        # ...
    ],
    fetch_sources = True,
)
Checksum verification
Artifact resolution will fail if a SHA-1 or MD5 checksum file for the
artifact is missing in the repository. To disable this behavior, set
fail_on_missing_checksum = False in maven_install:
maven_install(
    artifacts = [
        # ...
    ],
    repositories = [
        # ...
    ],
    fail_on_missing_checksum = False,
)
Using a persistent artifact cache
To download artifacts into a shared and persistent directory in your home
directory, set use_unsafe_shared_cache = True in maven_install.
maven_install(
    artifacts = [
        # ...
    ],
    repositories = [
        # ...
    ],
    use_unsafe_shared_cache = True,
)
This is not safe as Bazel is currently not able to detect changes in the
shared cache. For example, if an artifact is deleted from the shared cache,
Bazel will not re-run the repository rule automatically.
The default value of use_unsafe_shared_cache is False. This means that Bazel
will create independent caches for each maven_install repository, located at
$(bazel info output_base)/external/@repository_name/v1.
artifact helper macro
The artifact macro translates the artifact's group:artifact coordinates to
the label of the versionless target. This target is an
alias that
points to the java_import/aar_import target in the @maven repository,
which includes the transitive dependencies specified in the top level artifact's
POM file.
For example, @maven//:junit_junit is equivalent to artifact(""junit:junit"").
To use it, add the load statement to the top of your BUILD file:
load(""@rules_jvm_external//:defs.bzl"", ""artifact"")
Note that usage of this macro makes BUILD file refactoring with tools like
buildozer more difficult, because the macro hides the actual target label at
the syntax level.
Multiple maven_install declarations for isolated artifact version trees
If your WORKSPACE contains several projects that use different versions of the
same artifact, you can specify multiple maven_install declarations in the
WORKSPACE, with a unique repository name for each of them.
For example, if you want to use the JRE version of Guava for a server app, and
the Android version for an Android app, you can specify two maven_install
declarations:
maven_install(
    name = ""server_app"",
    artifacts = [
        ""com.google.guava:guava:27.0-jre"",
    ],
    repositories = [
        ""https://repo1.maven.org/maven2"",
    ],
)

maven_install(
    name = ""android_app"",
    artifacts = [
        ""com.google.guava:guava:27.0-android"",
    ],
    repositories = [
        ""https://repo1.maven.org/maven2"",
    ],
)
This way, rules_jvm_external will invoke coursier to resolve artifact versions for
both repositories independent of each other. Coursier will fail if it encounters
version conflicts that it cannot resolve. The two Guava targets can then be used
in BUILD files like so:
java_binary(
    name = ""my_server_app"",
    srcs = ...
    deps = [
        # a versionless alias to @server_app//:com_google_guava_guava_27_0_jre
        ""@server_app//:com_google_guava_guava"",
    ]
)

android_binary(
    name = ""my_android_app"",
    srcs = ...
    deps = [
        # a versionless alias to @android_app//:com_google_guava_guava_27_0_android
        ""@android_app//:com_google_guava_guava"",
    ]
)
Detailed dependency information specifications
Although you can always give a dependency as a Maven coordinate string,
occasionally special handling is required in the form of additional directives
to properly situate the artifact in the dependency tree. For example, a given
artifact may need to have one of its dependencies excluded to prevent a
conflict.
This situation is provided for by allowing the artifact to be specified as a map
containing all of the required information. This map can express more
information than the coordinate strings can, so internally the coordinate
strings are parsed into the artifact map with default values for the additional
items. To assist in generating the maps, you can pull in the file specs.bzl
alongside defs.bzl and import the maven struct, which provides several
helper functions to assist in creating these maps. An example:
load(""@rules_jvm_external//:defs.bzl"", ""artifact"")
load(""@rules_jvm_external//:specs.bzl"", ""maven"")

maven_install(
    artifacts = [
        maven.artifact(
            group = ""com.google.guava"",
            artifact = ""guava"",
            version = ""27.0-android"",
            exclusions = [
                ...
            ]
        ),
        ""junit:junit:4.12"",
        ...
    ],
    repositories = [
        maven.repository(
            ""https://some.private.maven.re/po"",
            user = ""johndoe"",
            password = ""example-password""
        ),
        ""https://repo1.maven.org/maven2"",
        ...
    ],
)
Artifact exclusion
If you want to exclude an artifact from the transitive closure of a top level
artifact, specify its group-id:artifact-id in the exclusions attribute of
the maven.artifact helper:
load(""@rules_jvm_external//:specs.bzl"", ""maven"")

maven_install(
    artifacts = [
        maven.artifact(
            group = ""com.google.guava"",
            artifact = ""guava"",
            version = ""27.0-jre"",
            exclusions = [
                maven.exclusion(
                    group = ""org.codehaus.mojo"",
                    artifact = ""animal-sniffer-annotations""
                ),
                ""com.google.j2objc:j2objc-annotations"",
            ]
        ),
        # ...
    ],
    repositories = [
        # ...
    ],
)
You can specify the exclusion using either the maven.exclusion helper or the
group-id:artifact-id string directly.
Compile-only dependencies
If you want to mark certain artifacts as compile-only dependencies, use the
neverlink attribute in the maven.artifact helper:
load(""@rules_jvm_external//:specs.bzl"", ""maven"")

maven_install(
    artifacts = [
        maven.artifact(""com.squareup"", ""javapoet"", ""1.11.0"", neverlink = True),
    ],
    # ...
)
This instructs rules_jvm_external to mark the generated target for
com.squareup:javapoet with the neverlink = True attribute, making the
artifact available only for compilation and not at runtime.
Proxies
As with other Bazel repository rules, the standard http_proxy, https_proxy
and no_proxy environment variables (and their uppercase counterparts) are
supported.
Demo
You can find demos in the examples/ directory.
",46
17,rstudio/tensorflow,R,"TensorFlow for R
  
TensorFlow™ is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API.
The TensorFlow API is composed of a set of Python modules that enable constructing and executing TensorFlow graphs. The tensorflow package provides access to the complete TensorFlow API from within R.
Installation
To get started, install the tensorflow R package from GitHub as follows:
devtools::install_github(""rstudio/tensorflow"")
Then, use the install_tensorflow() function to install TensorFlow:
library(tensorflow)
install_tensorflow()
You can confirm that the installation succeeded with:
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
This will provide you with a default installation of TensorFlow suitable for getting started with the tensorflow R package. See the article on installation to learn about more advanced options, including installing a version of TensorFlow that takes advantage of Nvidia GPUs if you have the correct CUDA libraries installed.
Documentation
See the package website for additional details on using the TensorFlow API from R: https://tensorflow.rstudio.com
See the TensorFlow API reference for details on all of the modules, classes, and functions within the API: https://www.tensorflow.org/api_docs/python/index.html
The tensorflow package provides code completion and inline help for the TensorFlow API when running within the RStudio IDE. In order to take advantage of these features you should also install the Current Release of RStudio.
",1132
18,googleapis/google-api-nodejs-client,TypeScript,"
Google APIs Node.js Client






Node.js client library for using Google APIs. Support for authorization and authentication with OAuth 2.0, API Keys and JWT tokens is included.

Google APIs
Getting started

Installation
First example
Samples
API Reference


Authentication and authorization

OAuth2 client
Using API keys
Service <--> Service authentication
Setting global or service-level auth


Usage

Specifying request body
Media uploads
Request Options
Using a Proxy
Supported APIs
TypeScript


License
Contributing
Questions/problems?

Google APIs
The full list of supported APIs can be found here. The API endpoints are automatically generated, so if the API is not in the list, it is currently not supported by this API client library.
Supported APIs are listed on the Google APIs Explorer.
Working with Google Cloud Platform APIs?
If you're working with Google Cloud Platform APIs such as Datastore, Cloud Storage or Pub/Sub, consider using the @google-cloud client libraries: single purpose idiomatic Node.js clients for Google Cloud Platform services.
Support and maintenance
These client libraries are officially supported by Google. However, these libraries are considered complete and are in maintenance mode. This means that we will address critical bugs and security issues but will not add any new features. For Google Cloud Platform APIs, we recommend using google-cloud-node which is under active development.
This library supports the maintenance LTS, active LTS, and current release of node.js.  See the node.js release schedule for more information.
Getting started
Installation
This library is distributed on npm. In order to add it as a dependency, run the following command:
$ npm install googleapis
First example
This is a very simple example. This creates a Blogger client and retrieves the details of a blog given the blog Id:
const {google} = require('googleapis');

// Each API may support multiple version. With this sample, we're getting
// v3 of the blogger API, and using an API key to authenticate.
const blogger = google.blogger({
  version: 'v3',
  auth: 'YOUR API KEY'
});

const params = {
  blogId: 3213900
};

// get the blog details
blogger.blogs.get(params, (err, res) => {
  if (err) {
    console.error(err);
    throw err;
  }
  console.log(`The blog url is ${res.data.url}`);
});
Instead of using callbacks you can also use promises!
blogger.blogs.get(params)
  .then(res => {
    console.log(`The blog url is ${res.data.url}`);
  })
  .catch(error => {
    console.error(error);
  });
Or async/await:
async function runSample() {
  const res = await blogger.blogs.get(params);
  console.log(`The blog url is ${res.data.url}`);
}
runSample().catch(console.error);
Samples
There are a lot of samples 🤗  If you're trying to figure out how to use an API ... look there first! If there's a sample you need missing, feel free to file an issue.
API Reference
This library has a full set of API Reference Documentation. This documentation is auto-generated, and the location may change.
Authentication and authorization
There are three primary ways to authenticate to Google APIs. Some service support all authentication methods, other may only support one or two.


OAuth2 - This allows you to make API calls on behalf of a given user.  In this model, the user visits your application, signs in with their Google account, and provides your application with authorization against a set of scopes.  Learn more.


Service <--> Service - In this model, your application talks directly to Google APIs using a Service Account.  It's useful when you have a backend application that will talk directly to Google APIs from the backend. Learn more.


API Key - With an API key, you can access your service from a client or the server.  Typically less secure, this is only available on a small subset of services with limited scopes.  Learn more.


To learn more about the authentication client, see the Google Auth Library.
OAuth2 client
This client comes with an OAuth2 client that allows you to retrieve an access token, refresh it, and retry the request seamlessly. The basics of Google's OAuth2 implementation is explained on Google Authorization and Authentication documentation.
In the following examples, you may need a CLIENT_ID, CLIENT_SECRET and REDIRECT_URL. You can find these pieces of information by going to the Developer Console, clicking your project --> APIs & auth --> credentials.
For more information about OAuth2 and how it works, see here.
A complete sample application that authorizes and authenticates with the OAuth2 client is available at samples/oauth2.js.
Generating an authentication URL
To ask for permissions from a user to retrieve an access token, you redirect them to a consent page. To create a consent page URL:
const {google} = require('googleapis');

const oauth2Client = new google.auth.OAuth2(
  YOUR_CLIENT_ID,
  YOUR_CLIENT_SECRET,
  YOUR_REDIRECT_URL
);

// generate a url that asks permissions for Blogger and Google Calendar scopes
const scopes = [
  'https://www.googleapis.com/auth/blogger',
  'https://www.googleapis.com/auth/calendar'
];

const url = oauth2Client.generateAuthUrl({
  // 'online' (default) or 'offline' (gets refresh_token)
  access_type: 'offline',

  // If you only need one scope you can pass it as a string
  scope: scopes
});
IMPORTANT NOTE - The refresh_token is only returned on the first authorization. More details here.
Retrieve authorization code
Once a user has given permissions on the consent page, Google will redirect the page to the redirect URL you have provided with a code query parameter.
GET /oauthcallback?code={authorizationCode}

Retrieve access token
With the code returned, you can ask for an access token as shown below:
// This will provide an object with the access_token and refresh_token.
// Save these somewhere safe so they can be used at a later time.
const {tokens} = await oauth2Client.getToken(code)
oauth2Client.setCredentials(tokens);
With the credentials set on your OAuth2 client - you're ready to go!
Handling refresh tokens
Access tokens expire. This library will automatically use a refresh token to obtain a new access token if it is about to expire. An easy way to make sure you always store the most recent tokens is to use the tokens event:
oauth2Client.on('tokens', (tokens) => {
  if (tokens.refresh_token) {
    // store the refresh_token in my database!
    console.log(tokens.refresh_token);
  }
  console.log(tokens.access_token);
});
To set the refresh_token at a later time, you can use the setCredentials method:
oauth2Client.setCredentials({
  refresh_token: `STORED_REFRESH_TOKEN`
});
Once the client has a refresh token, access tokens will be acquired and refreshed automatically in the next call to the API.
Using API keys
You may need to send an API key with the request you are going to make. The following uses an API key to make a request to the Blogger API service to retrieve a blog's name, url, and its total amount of posts:
const {google} = require('googleapis');
const blogger = google.blogger_v3({
  version: 'v3',
  auth: 'YOUR_API_KEY' // specify your API key here
});

const params = {
  blogId: 3213900
};

async function main(params) {
  const res = await blogger.blogs.get({blogId: params.blogId});
  console.log(`${res.data.name} has ${res.data.posts.totalItems} posts! The blog url is ${res.data.url}`)
};

main().catch(console.error);
To learn more about API keys, please see the documentation.
Service to Service Authentication
Rather than manually creating an OAuth2 client, JWT client, or Compute client, the auth library can create the correct credential type for you, depending upon the environment your code is running under.
For example, a JWT auth client will be created when your code is running on your local developer machine, and a Compute client will be created when the same code is running on a configured instance of Google Compute Engine.
The code below shows how to retrieve a default credential type, depending upon the runtime environment. The createScopedRequired must be called to determine when you need to pass in the scopes manually, and when they have been set for you automatically based on the configured runtime environment.
const {google} = require('googleapis');
const compute = google.compute('v1');

async function main () {
  // This method looks for the GCLOUD_PROJECT and GOOGLE_APPLICATION_CREDENTIALS
  // environment variables.
  const auth = await google.auth.getClient({
    // Scopes can be specified either as an array or as a single, space-delimited string.
    scopes: ['https://www.googleapis.com/auth/compute']
  });

  // obtain the current project Id
  const project = await google.auth.getProjectId();

  // Fetch the list of GCE zones within a project.
  const res = await compute.zones.list({ project, auth });
  console.log(res.data);
}

main().catch(console.error);
Setting global or service-level auth
You can set the auth as a global or service-level option so you don't need to specify it every request. For example, you can set auth as a global option:
const {google} = require('googleapis');

const oauth2Client = new google.auth.OAuth2(
  YOUR_CLIENT_ID,
  YOUR_CLIENT_SECRET,
  YOUR_REDIRECT_URL
);

// set auth as a global default
google.options({
  auth: oauth2Client
});
Instead of setting the option globally, you can also set the authentication client at the service-level:
const {google} = require('googleapis');
const oauth2Client = new google.auth.OAuth2(
  YOUR_CLIENT_ID,
  YOUR_CLIENT_SECRET,
  YOUR_REDIRECT_URL
);

const drive = google.drive({
  version: 'v2',
  auth: oauth2Client
});
See the Options section for more information.
Usage
Specifying request body
The body of the request is specified in the requestBody parameter object of the request. The body is specified as a JavaScript object with key/value pairs. For example, this sample creates a watcher that posts notifications to a Google Cloud Pub/Sub topic when emails are sent to a gmail account:
const res = await gmail.users.watch({
  userId: 'me',
  requestBody: {
    // Replace with `projects/${PROJECT_ID}/topics/${TOPIC_NAME}`
    topicName: `projects/el-gato/topics/gmail`
  }
});
console.log(res.data);
Media uploads
This client supports multipart media uploads. The resource parameters are specified in the requestBody parameter object, and the media itself is specified in the media.body parameter with mime-type specified in media.mimeType.
This example uploads a plain text file to Google Drive with the title ""Test"" and contents ""Hello World"".
const drive = google.drive({
  version: 'v3',
  auth: oauth2Client
});

const res = await drive.files.create({
  requestBody: {
    name: 'Test',
    mimeType: 'text/plain'
  },
  media: {
    mimeType: 'text/plain',
    body: 'Hello World'
  }
});
You can also upload media by specifying media.body as a Readable stream. This can allow you to upload very large files that cannot fit into memory.
const fs = require('fs');

const drive = google.drive({
  version: 'v3',
  auth: oauth2Client
});

async function main() {
  const res = await drive.files.create({
    requestBody: {
      name: 'testimage.png',
      mimeType: 'image/png'
    },
    media: {
      mimeType: 'image/png',
      body: fs.createReadStream('awesome.png')
    }
  });
  console.log(res.data);
}

main().catch(console.error);
For more examples of creation and modification requests with media attachments, take a look at the samples/drive/upload.js sample.
Request Options
For more fine-tuned control over how your API calls are made, we provide you with the ability to specify additional options that can be applied directly to the 'gaxios' object used in this library to make network calls to the API.
You may specify additional options either in the global google object or on a service client basis.  The options you specify are attached to the gaxios object so whatever gaxios supports, this library supports. You may also specify global or per-service request parameters that will be attached to all API calls you make.
A full list of supported options can be [found here][requestopts].
Global options
You can choose default options that will be sent with each request. These options will be used for every service instantiated by the google client. In this example, the timeout property of GaxiosOptions will be set for every request:
const {google} = require('googleapis');
google.options({
  // All requests made with this object will use these settings unless overridden.
  timeout: 1000,
  auth: auth
});
You can also modify the parameters sent with each request:
const {google} = require('googleapis');
google.options({
  // All requests from all services will contain the above query parameter
  // unless overridden either in a service client or in individual API calls.
  params: {
    quotaUser: 'user123@example.com'
  }
});
Service-client options
You can also specify options when creating a service client.
const blogger = google.blogger({
  version: 'v3',
  // All requests made with this object will use the specified auth.
  auth: 'API KEY';
});
By doing this, every API call made with this service client will use 'API KEY' to authenticate.
Note: Created clients are immutable so you must create a new one if you want to specify different options.
Similar to the examples above, you can also modify the parameters used for every call of a given service:
const blogger = google.blogger({
  version: 'v3',
  // All requests made with this service client will contain the
  // blogId query parameter unless overridden in individual API calls.
  params: {
    blogId: 3213900
  }
});

// Calls with this drive client will NOT contain the blogId query parameter.
const drive = google.drive('v3');
...

Request-level options
You can specify an auth object to be used per request. Each request also inherits the options specified at the service level and global level.
For example:
const {google} = require('googleapis');
const bigquery = google.bigquery('v2');

async function main() {

  // This method looks for the GCLOUD_PROJECT and GOOGLE_APPLICATION_CREDENTIALS
  // environment variables.
  const client = await google.auth.getClient({
    scopes: ['https://www.googleapis.com/auth/cloud-platform']
  });

  const projectId = await google.auth.getProjectId();

  const request = {
    projectId,
    datasetId: '<YOUR_DATASET_ID>',

    // This is a ""request-level"" option
    auth: client
  };

  const res = await bigquery.datasets.delete(request);
  console.log(res.data);

}

main().catch(console.error);
You can also override gaxios options per request, such as url, method, and encoding.
For example:
const res = await drive.files.export({
  fileId: 'asxKJod9s79', // A Google Doc
  mimeType: 'application/pdf'
}, {
  // Make sure we get the binary data
  encoding: null
});
Using a Proxy
You can use the following environment variables to proxy HTTP and HTTPS requests:

HTTP_PROXY / http_proxy
HTTPS_PROXY / https_proxy

When HTTP_PROXY / http_proxy are set, they will be used to proxy non-SSL requests that do not have an explicit proxy configuration option present. Similarly, HTTPS_PROXY / https_proxy will be respected for SSL requests that do not have an explicit proxy configuration option. It is valid to define a proxy in one of the environment variables, but then override it for a specific request, using the proxy configuration option.
Getting Supported APIs
You can programatically obtain the list of supported APIs, and all available versions:
const {google} = require('googleapis');
const apis = google.getSupportedAPIs();
This will return an object with the API name as object property names, and an array of version strings as the object values;
TypeScript
This library is written in TypeScript, and provides types out of the box. All classes and interfaces generated for each API are exported under the ${apiName}_${version} namespace.  For example, the Drive v3 API types are all available from the drive_v3 namespace:
import { drive_v3 } from 'googleapis';
Release Notes & Breaking Changes
You can find a detailed list of breaking changes and new features in our Release Notes. If you've used this library before 25.x, see our Release Notes to learn about migrating your code from 24.x.x to 25.x.x. It's pretty easy :)
License
This library is licensed under Apache 2.0. Full license text is available in LICENSE.
Contributing
We love contributions! Before submitting a Pull Request, it's always good to start with a new issue first. To learn more, see CONTRIBUTING.
Questions/problems?

Ask your development related questions on Stackoverflow.
If you've found an bug/issue, please file it on GitHub.

",7443
19,bazelbuild/rules_jvm_external,Python,"rules_jvm_external
Transitive Maven artifact resolver as a repository rule.

Features

WORKSPACE configuration
JAR, AAR, source JARs
Custom Maven repositories
Private Maven repositories with HTTP Basic Authentication
Artifact version resolution with Coursier
Reuse artifacts from a central cache
Versionless target labels for simpler dependency management
Ability to declare multiple sets of versioned artifacts
Supported on Windows, macOS, Linux

Get the latest release
here.
Usage
List the top-level Maven artifacts and servers in the WORKSPACE:
load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"")

RULES_JVM_EXTERNAL_TAG = ""2.1""
RULES_JVM_EXTERNAL_SHA = ""515ee5265387b88e4547b34a57393d2bcb1101314bcc5360ec7a482792556f42""

http_archive(
    name = ""rules_jvm_external"",
    strip_prefix = ""rules_jvm_external-%s"" % RULES_JVM_EXTERNAL_TAG,
    sha256 = RULES_JVM_EXTERNAL_SHA,
    url = ""https://github.com/bazelbuild/rules_jvm_external/archive/%s.zip"" % RULES_JVM_EXTERNAL_TAG,
)

load(""@rules_jvm_external//:defs.bzl"", ""maven_install"")

maven_install(
    artifacts = [
        ""junit:junit:4.12"",
        ""androidx.test.espresso:espresso-core:3.1.1"",
    ],
    repositories = [
        # Private repositories are supported through HTTP Basic auth
        ""http://username:password@localhost:8081/artifactory/my-repository"",
        ""https://jcenter.bintray.com/"",
        ""https://maven.google.com"",
        ""https://repo1.maven.org/maven2"",
    ],
)
and use them directly in the BUILD file by specifying the versionless target alias label:
android_library(
    name = ""test_deps"",
    exports = [
        ""@maven//:androidx_test_espresso_espresso_core"",
        ""@maven//:junit_junit"",
    ],
)
Generated targets
For the junit:junit example, using bazel query @maven//:all --output=build, we can see that the rule generated these targets:
alias(
  name = ""junit_junit_4_12"",
  actual = ""@maven//:junit_junit"",
)

jvm_import(
  name = ""junit_junit"",
  jars = [""@maven//:https/repo1.maven.org/maven2/junit/junit/4.12/junit-4.12.jar""],
  srcjar = ""@maven//:https/repo1.maven.org/maven2/junit/junit/4.12/junit-4.12-sources.jar"",
  deps = [""@maven//:org_hamcrest_hamcrest_core""],
  tags = [""maven_coordinates=junit:junit:4.12""],
)

jvm_import(
  name = ""org_hamcrest_hamcrest_core"",
  jars = [""@maven//:https/repo1.maven.org/maven2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar""],
  srcjar = ""@maven//:https/repo1.maven.org/maven2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3-sources.jar"",
  deps = [],
  tags = [""maven_coordinates=org.hamcrest:hamcrest.library:1.3""],
)
The generated tags attribute value also contains the original coordinates of
the artifact, which integrates with rules like bazel-common's
pom_file
for generating POM files. See the pom_file_generation
example for more information.
API Reference
You can find the complete API reference at docs/api.md.
Advanced usage
Fetch source JARs
To download the source JAR alongside the main artifact JAR, set fetch_sources = True in maven_install:
maven_install(
    artifacts = [
        # ...
    ],
    repositories = [
        # ...
    ],
    fetch_sources = True,
)
Checksum verification
Artifact resolution will fail if a SHA-1 or MD5 checksum file for the
artifact is missing in the repository. To disable this behavior, set
fail_on_missing_checksum = False in maven_install:
maven_install(
    artifacts = [
        # ...
    ],
    repositories = [
        # ...
    ],
    fail_on_missing_checksum = False,
)
Using a persistent artifact cache
To download artifacts into a shared and persistent directory in your home
directory, set use_unsafe_shared_cache = True in maven_install.
maven_install(
    artifacts = [
        # ...
    ],
    repositories = [
        # ...
    ],
    use_unsafe_shared_cache = True,
)
This is not safe as Bazel is currently not able to detect changes in the
shared cache. For example, if an artifact is deleted from the shared cache,
Bazel will not re-run the repository rule automatically.
The default value of use_unsafe_shared_cache is False. This means that Bazel
will create independent caches for each maven_install repository, located at
$(bazel info output_base)/external/@repository_name/v1.
artifact helper macro
The artifact macro translates the artifact's group:artifact coordinates to
the label of the versionless target. This target is an
alias that
points to the java_import/aar_import target in the @maven repository,
which includes the transitive dependencies specified in the top level artifact's
POM file.
For example, @maven//:junit_junit is equivalent to artifact(""junit:junit"").
To use it, add the load statement to the top of your BUILD file:
load(""@rules_jvm_external//:defs.bzl"", ""artifact"")
Note that usage of this macro makes BUILD file refactoring with tools like
buildozer more difficult, because the macro hides the actual target label at
the syntax level.
Multiple maven_install declarations for isolated artifact version trees
If your WORKSPACE contains several projects that use different versions of the
same artifact, you can specify multiple maven_install declarations in the
WORKSPACE, with a unique repository name for each of them.
For example, if you want to use the JRE version of Guava for a server app, and
the Android version for an Android app, you can specify two maven_install
declarations:
maven_install(
    name = ""server_app"",
    artifacts = [
        ""com.google.guava:guava:27.0-jre"",
    ],
    repositories = [
        ""https://repo1.maven.org/maven2"",
    ],
)

maven_install(
    name = ""android_app"",
    artifacts = [
        ""com.google.guava:guava:27.0-android"",
    ],
    repositories = [
        ""https://repo1.maven.org/maven2"",
    ],
)
This way, rules_jvm_external will invoke coursier to resolve artifact versions for
both repositories independent of each other. Coursier will fail if it encounters
version conflicts that it cannot resolve. The two Guava targets can then be used
in BUILD files like so:
java_binary(
    name = ""my_server_app"",
    srcs = ...
    deps = [
        # a versionless alias to @server_app//:com_google_guava_guava_27_0_jre
        ""@server_app//:com_google_guava_guava"",
    ]
)

android_binary(
    name = ""my_android_app"",
    srcs = ...
    deps = [
        # a versionless alias to @android_app//:com_google_guava_guava_27_0_android
        ""@android_app//:com_google_guava_guava"",
    ]
)
Detailed dependency information specifications
Although you can always give a dependency as a Maven coordinate string,
occasionally special handling is required in the form of additional directives
to properly situate the artifact in the dependency tree. For example, a given
artifact may need to have one of its dependencies excluded to prevent a
conflict.
This situation is provided for by allowing the artifact to be specified as a map
containing all of the required information. This map can express more
information than the coordinate strings can, so internally the coordinate
strings are parsed into the artifact map with default values for the additional
items. To assist in generating the maps, you can pull in the file specs.bzl
alongside defs.bzl and import the maven struct, which provides several
helper functions to assist in creating these maps. An example:
load(""@rules_jvm_external//:defs.bzl"", ""artifact"")
load(""@rules_jvm_external//:specs.bzl"", ""maven"")

maven_install(
    artifacts = [
        maven.artifact(
            group = ""com.google.guava"",
            artifact = ""guava"",
            version = ""27.0-android"",
            exclusions = [
                ...
            ]
        ),
        ""junit:junit:4.12"",
        ...
    ],
    repositories = [
        maven.repository(
            ""https://some.private.maven.re/po"",
            user = ""johndoe"",
            password = ""example-password""
        ),
        ""https://repo1.maven.org/maven2"",
        ...
    ],
)
Artifact exclusion
If you want to exclude an artifact from the transitive closure of a top level
artifact, specify its group-id:artifact-id in the exclusions attribute of
the maven.artifact helper:
load(""@rules_jvm_external//:specs.bzl"", ""maven"")

maven_install(
    artifacts = [
        maven.artifact(
            group = ""com.google.guava"",
            artifact = ""guava"",
            version = ""27.0-jre"",
            exclusions = [
                maven.exclusion(
                    group = ""org.codehaus.mojo"",
                    artifact = ""animal-sniffer-annotations""
                ),
                ""com.google.j2objc:j2objc-annotations"",
            ]
        ),
        # ...
    ],
    repositories = [
        # ...
    ],
)
You can specify the exclusion using either the maven.exclusion helper or the
group-id:artifact-id string directly.
Compile-only dependencies
If you want to mark certain artifacts as compile-only dependencies, use the
neverlink attribute in the maven.artifact helper:
load(""@rules_jvm_external//:specs.bzl"", ""maven"")

maven_install(
    artifacts = [
        maven.artifact(""com.squareup"", ""javapoet"", ""1.11.0"", neverlink = True),
    ],
    # ...
)
This instructs rules_jvm_external to mark the generated target for
com.squareup:javapoet with the neverlink = True attribute, making the
artifact available only for compilation and not at runtime.
Proxies
As with other Bazel repository rules, the standard http_proxy, https_proxy
and no_proxy environment variables (and their uppercase counterparts) are
supported.
Demo
You can find demos in the examples/ directory.
",46
20,rstudio/tensorflow,R,"TensorFlow for R
  
TensorFlow™ is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API.
The TensorFlow API is composed of a set of Python modules that enable constructing and executing TensorFlow graphs. The tensorflow package provides access to the complete TensorFlow API from within R.
Installation
To get started, install the tensorflow R package from GitHub as follows:
devtools::install_github(""rstudio/tensorflow"")
Then, use the install_tensorflow() function to install TensorFlow:
library(tensorflow)
install_tensorflow()
You can confirm that the installation succeeded with:
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
This will provide you with a default installation of TensorFlow suitable for getting started with the tensorflow R package. See the article on installation to learn about more advanced options, including installing a version of TensorFlow that takes advantage of Nvidia GPUs if you have the correct CUDA libraries installed.
Documentation
See the package website for additional details on using the TensorFlow API from R: https://tensorflow.rstudio.com
See the TensorFlow API reference for details on all of the modules, classes, and functions within the API: https://www.tensorflow.org/api_docs/python/index.html
The tensorflow package provides code completion and inline help for the TensorFlow API when running within the RStudio IDE. In order to take advantage of these features you should also install the Current Release of RStudio.
",1132
21,googleapis/google-api-nodejs-client,TypeScript,"
Google APIs Node.js Client






Node.js client library for using Google APIs. Support for authorization and authentication with OAuth 2.0, API Keys and JWT tokens is included.

Google APIs
Getting started

Installation
First example
Samples
API Reference


Authentication and authorization

OAuth2 client
Using API keys
Service <--> Service authentication
Setting global or service-level auth


Usage

Specifying request body
Media uploads
Request Options
Using a Proxy
Supported APIs
TypeScript


License
Contributing
Questions/problems?

Google APIs
The full list of supported APIs can be found here. The API endpoints are automatically generated, so if the API is not in the list, it is currently not supported by this API client library.
Supported APIs are listed on the Google APIs Explorer.
Working with Google Cloud Platform APIs?
If you're working with Google Cloud Platform APIs such as Datastore, Cloud Storage or Pub/Sub, consider using the @google-cloud client libraries: single purpose idiomatic Node.js clients for Google Cloud Platform services.
Support and maintenance
These client libraries are officially supported by Google. However, these libraries are considered complete and are in maintenance mode. This means that we will address critical bugs and security issues but will not add any new features. For Google Cloud Platform APIs, we recommend using google-cloud-node which is under active development.
This library supports the maintenance LTS, active LTS, and current release of node.js.  See the node.js release schedule for more information.
Getting started
Installation
This library is distributed on npm. In order to add it as a dependency, run the following command:
$ npm install googleapis
First example
This is a very simple example. This creates a Blogger client and retrieves the details of a blog given the blog Id:
const {google} = require('googleapis');

// Each API may support multiple version. With this sample, we're getting
// v3 of the blogger API, and using an API key to authenticate.
const blogger = google.blogger({
  version: 'v3',
  auth: 'YOUR API KEY'
});

const params = {
  blogId: 3213900
};

// get the blog details
blogger.blogs.get(params, (err, res) => {
  if (err) {
    console.error(err);
    throw err;
  }
  console.log(`The blog url is ${res.data.url}`);
});
Instead of using callbacks you can also use promises!
blogger.blogs.get(params)
  .then(res => {
    console.log(`The blog url is ${res.data.url}`);
  })
  .catch(error => {
    console.error(error);
  });
Or async/await:
async function runSample() {
  const res = await blogger.blogs.get(params);
  console.log(`The blog url is ${res.data.url}`);
}
runSample().catch(console.error);
Samples
There are a lot of samples 🤗  If you're trying to figure out how to use an API ... look there first! If there's a sample you need missing, feel free to file an issue.
API Reference
This library has a full set of API Reference Documentation. This documentation is auto-generated, and the location may change.
Authentication and authorization
There are three primary ways to authenticate to Google APIs. Some service support all authentication methods, other may only support one or two.


OAuth2 - This allows you to make API calls on behalf of a given user.  In this model, the user visits your application, signs in with their Google account, and provides your application with authorization against a set of scopes.  Learn more.


Service <--> Service - In this model, your application talks directly to Google APIs using a Service Account.  It's useful when you have a backend application that will talk directly to Google APIs from the backend. Learn more.


API Key - With an API key, you can access your service from a client or the server.  Typically less secure, this is only available on a small subset of services with limited scopes.  Learn more.


To learn more about the authentication client, see the Google Auth Library.
OAuth2 client
This client comes with an OAuth2 client that allows you to retrieve an access token, refresh it, and retry the request seamlessly. The basics of Google's OAuth2 implementation is explained on Google Authorization and Authentication documentation.
In the following examples, you may need a CLIENT_ID, CLIENT_SECRET and REDIRECT_URL. You can find these pieces of information by going to the Developer Console, clicking your project --> APIs & auth --> credentials.
For more information about OAuth2 and how it works, see here.
A complete sample application that authorizes and authenticates with the OAuth2 client is available at samples/oauth2.js.
Generating an authentication URL
To ask for permissions from a user to retrieve an access token, you redirect them to a consent page. To create a consent page URL:
const {google} = require('googleapis');

const oauth2Client = new google.auth.OAuth2(
  YOUR_CLIENT_ID,
  YOUR_CLIENT_SECRET,
  YOUR_REDIRECT_URL
);

// generate a url that asks permissions for Blogger and Google Calendar scopes
const scopes = [
  'https://www.googleapis.com/auth/blogger',
  'https://www.googleapis.com/auth/calendar'
];

const url = oauth2Client.generateAuthUrl({
  // 'online' (default) or 'offline' (gets refresh_token)
  access_type: 'offline',

  // If you only need one scope you can pass it as a string
  scope: scopes
});
IMPORTANT NOTE - The refresh_token is only returned on the first authorization. More details here.
Retrieve authorization code
Once a user has given permissions on the consent page, Google will redirect the page to the redirect URL you have provided with a code query parameter.
GET /oauthcallback?code={authorizationCode}

Retrieve access token
With the code returned, you can ask for an access token as shown below:
// This will provide an object with the access_token and refresh_token.
// Save these somewhere safe so they can be used at a later time.
const {tokens} = await oauth2Client.getToken(code)
oauth2Client.setCredentials(tokens);
With the credentials set on your OAuth2 client - you're ready to go!
Handling refresh tokens
Access tokens expire. This library will automatically use a refresh token to obtain a new access token if it is about to expire. An easy way to make sure you always store the most recent tokens is to use the tokens event:
oauth2Client.on('tokens', (tokens) => {
  if (tokens.refresh_token) {
    // store the refresh_token in my database!
    console.log(tokens.refresh_token);
  }
  console.log(tokens.access_token);
});
To set the refresh_token at a later time, you can use the setCredentials method:
oauth2Client.setCredentials({
  refresh_token: `STORED_REFRESH_TOKEN`
});
Once the client has a refresh token, access tokens will be acquired and refreshed automatically in the next call to the API.
Using API keys
You may need to send an API key with the request you are going to make. The following uses an API key to make a request to the Blogger API service to retrieve a blog's name, url, and its total amount of posts:
const {google} = require('googleapis');
const blogger = google.blogger_v3({
  version: 'v3',
  auth: 'YOUR_API_KEY' // specify your API key here
});

const params = {
  blogId: 3213900
};

async function main(params) {
  const res = await blogger.blogs.get({blogId: params.blogId});
  console.log(`${res.data.name} has ${res.data.posts.totalItems} posts! The blog url is ${res.data.url}`)
};

main().catch(console.error);
To learn more about API keys, please see the documentation.
Service to Service Authentication
Rather than manually creating an OAuth2 client, JWT client, or Compute client, the auth library can create the correct credential type for you, depending upon the environment your code is running under.
For example, a JWT auth client will be created when your code is running on your local developer machine, and a Compute client will be created when the same code is running on a configured instance of Google Compute Engine.
The code below shows how to retrieve a default credential type, depending upon the runtime environment. The createScopedRequired must be called to determine when you need to pass in the scopes manually, and when they have been set for you automatically based on the configured runtime environment.
const {google} = require('googleapis');
const compute = google.compute('v1');

async function main () {
  // This method looks for the GCLOUD_PROJECT and GOOGLE_APPLICATION_CREDENTIALS
  // environment variables.
  const auth = await google.auth.getClient({
    // Scopes can be specified either as an array or as a single, space-delimited string.
    scopes: ['https://www.googleapis.com/auth/compute']
  });

  // obtain the current project Id
  const project = await google.auth.getProjectId();

  // Fetch the list of GCE zones within a project.
  const res = await compute.zones.list({ project, auth });
  console.log(res.data);
}

main().catch(console.error);
Setting global or service-level auth
You can set the auth as a global or service-level option so you don't need to specify it every request. For example, you can set auth as a global option:
const {google} = require('googleapis');

const oauth2Client = new google.auth.OAuth2(
  YOUR_CLIENT_ID,
  YOUR_CLIENT_SECRET,
  YOUR_REDIRECT_URL
);

// set auth as a global default
google.options({
  auth: oauth2Client
});
Instead of setting the option globally, you can also set the authentication client at the service-level:
const {google} = require('googleapis');
const oauth2Client = new google.auth.OAuth2(
  YOUR_CLIENT_ID,
  YOUR_CLIENT_SECRET,
  YOUR_REDIRECT_URL
);

const drive = google.drive({
  version: 'v2',
  auth: oauth2Client
});
See the Options section for more information.
Usage
Specifying request body
The body of the request is specified in the requestBody parameter object of the request. The body is specified as a JavaScript object with key/value pairs. For example, this sample creates a watcher that posts notifications to a Google Cloud Pub/Sub topic when emails are sent to a gmail account:
const res = await gmail.users.watch({
  userId: 'me',
  requestBody: {
    // Replace with `projects/${PROJECT_ID}/topics/${TOPIC_NAME}`
    topicName: `projects/el-gato/topics/gmail`
  }
});
console.log(res.data);
Media uploads
This client supports multipart media uploads. The resource parameters are specified in the requestBody parameter object, and the media itself is specified in the media.body parameter with mime-type specified in media.mimeType.
This example uploads a plain text file to Google Drive with the title ""Test"" and contents ""Hello World"".
const drive = google.drive({
  version: 'v3',
  auth: oauth2Client
});

const res = await drive.files.create({
  requestBody: {
    name: 'Test',
    mimeType: 'text/plain'
  },
  media: {
    mimeType: 'text/plain',
    body: 'Hello World'
  }
});
You can also upload media by specifying media.body as a Readable stream. This can allow you to upload very large files that cannot fit into memory.
const fs = require('fs');

const drive = google.drive({
  version: 'v3',
  auth: oauth2Client
});

async function main() {
  const res = await drive.files.create({
    requestBody: {
      name: 'testimage.png',
      mimeType: 'image/png'
    },
    media: {
      mimeType: 'image/png',
      body: fs.createReadStream('awesome.png')
    }
  });
  console.log(res.data);
}

main().catch(console.error);
For more examples of creation and modification requests with media attachments, take a look at the samples/drive/upload.js sample.
Request Options
For more fine-tuned control over how your API calls are made, we provide you with the ability to specify additional options that can be applied directly to the 'gaxios' object used in this library to make network calls to the API.
You may specify additional options either in the global google object or on a service client basis.  The options you specify are attached to the gaxios object so whatever gaxios supports, this library supports. You may also specify global or per-service request parameters that will be attached to all API calls you make.
A full list of supported options can be [found here][requestopts].
Global options
You can choose default options that will be sent with each request. These options will be used for every service instantiated by the google client. In this example, the timeout property of GaxiosOptions will be set for every request:
const {google} = require('googleapis');
google.options({
  // All requests made with this object will use these settings unless overridden.
  timeout: 1000,
  auth: auth
});
You can also modify the parameters sent with each request:
const {google} = require('googleapis');
google.options({
  // All requests from all services will contain the above query parameter
  // unless overridden either in a service client or in individual API calls.
  params: {
    quotaUser: 'user123@example.com'
  }
});
Service-client options
You can also specify options when creating a service client.
const blogger = google.blogger({
  version: 'v3',
  // All requests made with this object will use the specified auth.
  auth: 'API KEY';
});
By doing this, every API call made with this service client will use 'API KEY' to authenticate.
Note: Created clients are immutable so you must create a new one if you want to specify different options.
Similar to the examples above, you can also modify the parameters used for every call of a given service:
const blogger = google.blogger({
  version: 'v3',
  // All requests made with this service client will contain the
  // blogId query parameter unless overridden in individual API calls.
  params: {
    blogId: 3213900
  }
});

// Calls with this drive client will NOT contain the blogId query parameter.
const drive = google.drive('v3');
...

Request-level options
You can specify an auth object to be used per request. Each request also inherits the options specified at the service level and global level.
For example:
const {google} = require('googleapis');
const bigquery = google.bigquery('v2');

async function main() {

  // This method looks for the GCLOUD_PROJECT and GOOGLE_APPLICATION_CREDENTIALS
  // environment variables.
  const client = await google.auth.getClient({
    scopes: ['https://www.googleapis.com/auth/cloud-platform']
  });

  const projectId = await google.auth.getProjectId();

  const request = {
    projectId,
    datasetId: '<YOUR_DATASET_ID>',

    // This is a ""request-level"" option
    auth: client
  };

  const res = await bigquery.datasets.delete(request);
  console.log(res.data);

}

main().catch(console.error);
You can also override gaxios options per request, such as url, method, and encoding.
For example:
const res = await drive.files.export({
  fileId: 'asxKJod9s79', // A Google Doc
  mimeType: 'application/pdf'
}, {
  // Make sure we get the binary data
  encoding: null
});
Using a Proxy
You can use the following environment variables to proxy HTTP and HTTPS requests:

HTTP_PROXY / http_proxy
HTTPS_PROXY / https_proxy

When HTTP_PROXY / http_proxy are set, they will be used to proxy non-SSL requests that do not have an explicit proxy configuration option present. Similarly, HTTPS_PROXY / https_proxy will be respected for SSL requests that do not have an explicit proxy configuration option. It is valid to define a proxy in one of the environment variables, but then override it for a specific request, using the proxy configuration option.
Getting Supported APIs
You can programatically obtain the list of supported APIs, and all available versions:
const {google} = require('googleapis');
const apis = google.getSupportedAPIs();
This will return an object with the API name as object property names, and an array of version strings as the object values;
TypeScript
This library is written in TypeScript, and provides types out of the box. All classes and interfaces generated for each API are exported under the ${apiName}_${version} namespace.  For example, the Drive v3 API types are all available from the drive_v3 namespace:
import { drive_v3 } from 'googleapis';
Release Notes & Breaking Changes
You can find a detailed list of breaking changes and new features in our Release Notes. If you've used this library before 25.x, see our Release Notes to learn about migrating your code from 24.x.x to 25.x.x. It's pretty easy :)
License
This library is licensed under Apache 2.0. Full license text is available in LICENSE.
Contributing
We love contributions! Before submitting a Pull Request, it's always good to start with a new issue first. To learn more, see CONTRIBUTING.
Questions/problems?

Ask your development related questions on Stackoverflow.
If you've found an bug/issue, please file it on GitHub.

",7443
22,oracle/graal,Java,"GraalVM

GraalVM is a universal virtual machine for running applications written in JavaScript, Python, Ruby, R, JVM-based languages like Java, Scala, Clojure, Kotlin, and LLVM-based languages such as C and C++.
The project website at https://www.graalvm.org describes how to get started, how to stay connected, and how to contribute.
Repository Structure
The GraalVM main source repository includes the following components:


GraalVM SDK contains long term supported APIs of GraalVM.


Graal compiler written in Java that supports both dynamic and static compilation and can integrate with
the Java HotSpot VM or run standalone.


Truffle language implementation framework for creating languages and instrumentations for GraalVM.


Tools contains a set of tools for GraalVM languages
implemented with the instrumentation framework.


Substrate VM framework that allows ahead-of-time (AOT)
compilation of Java applications under closed-world assumption into executable
images or shared objects.


Sulong is an engine for running LLVM bitcode on GraalVM.


TRegex is an implementation of regular expressions which leverages GraalVM for efficient compilation of automata.


VM includes the components to build a modular GraalVM image.


Reporting Vulnerabilities
Please report security vulnerabilities not via GitHub issues or the public mailing lists, but via the process outlined at Reporting Vulnerabilities guide.
Related Repositories
GraalVM allows running of following languages which are being developed and tested in related repositories with GraalVM core to run on top of it using Truffle and Graal compiler. These are:

Graal JavaScript - JavaScript (ECMAScript 2019 compatible) and Node.js 10.15.2
FastR - R Language 3.5.1
GraalPython - Python 3.7
TruffleRuby - Ruby Programming Language 2.6.2
SimpleLanguage - A simple demonstration language for the GraalVM.

License
Each GraalVM component is licensed:

Truffle and its dependency GraalVM SDK are licensed under the Universal Permissive License.
Tools project is licensed under the GPL 2 with Classpath exception.
TRegex project is licensed under the GPL 2 with Classpath exception.
The Graal compiler is licensed under the GPL 2 with Classpath exception.
Substrate VM is licensed under the GPL 2 with Classpath exception.
Sulong is licensed under 3-clause BSD.
VM is licensed under the GPL 2 with Classpath exception.

",8910
23,sphinx-gallery/sphinx-gallery,Python,"Sphinx-Gallery


A Sphinx extension that builds an HTML version of any Python
script and puts it into an examples gallery.


Who uses Sphinx-Gallery

Sphinx-Gallery
Scikit-learn
Nilearn
MNE-python
PyStruct
GIMLi
Nestle
pyRiemann
scikit-image
Astropy
SunPy
PySurfer
Matplotlib Examples and Tutorials
PyTorch tutorials
Cartopy


Installation

Install via pip
You can do a direct install via pip by using:
$ pip install sphinx-gallery
Sphinx-Gallery will not manage its dependencies when installing, thus
you are required to install them manually. Our minimal dependencies
are:

Sphinx >= 1.5 (1.8 recommended)
Matplotlib
Pillow

Sphinx-Gallery has also support for packages like:

Seaborn
Mayavi


Install as a developer
You can get the latest development source from our Github repository. You need
setuptools installed in your system to install Sphinx-Gallery.
Additional to the dependencies listed above you will need to install pytest
and pytest-coverage for testing.
To install everything do:
$ git clone https://github.com/sphinx-gallery/sphinx-gallery
$ cd sphinx-gallery
$ pip install -r requirements.txt
$ pip install pytest pytest-coverage
$ pip install -e .
In addition, you will need the following dependencies to build the
sphinx-gallery documentation:

Scipy
Seaborn

",147
24,artsy/palette,TypeScript,"@artsy/palette   
Artsy's Design System
Meta

Point People: @damassi, @zephraph, @l2succes

What is Palette?
Palette is a collection of primitive, product-agnostic elements that help encapsulate Artsy's look and feel at base level. This project is intended to be used
across our digital product portfolio.
Does my component belong in Palette?
If the component applies to Artsy as a brand and can/will be used across multiple digital products, then Palette is a great place for it. If it's highly product
specific then it's best to leave the component where it's used. We can always move things later!
If the above guidance still doesn't give you a good sense of what to do, please join the bi-weekly design systems sync.
How to contribute
If you'd like to add a new component to Palette please create an issue using the component spec template. That'll give both design and engineering a chance
to peek at the proposal and provide feedback before moving forward.
Local development
In the project root run the following:
$ yarn start
$ open http://localhost:8000/

Deployment process
Commits and Deployments
Palette uses auto-release to automatically release on every PR. Every PR should have a label that matches one of the following

Version: Trivial
Version: Patch
Version: Minor
Version: Major

Major, minor, and patch will cause a new release to be generated. Use major for breaking changes, minor for new non-breaking features,
and patch for bug fixes. Trivial will not cause a release and should be used when updating documentation or non-project code.
If you don't want to release on a particular PR but the changes aren't trivial then use the Skip Release tag along side the appropriate version tag.
",62
25,googleapis/google-cloud-ruby,Ruby,"Google Cloud Ruby Client
Idiomatic Ruby client for Google Cloud Platform
services.




Homepage
API documentation
google-cloud on RubyGems

This client supports the following Google Cloud Platform services at a General
Availability (GA) quality level:

BigQuery (GA)
Cloud Datastore (GA)
Cloud Key Management Service (GA)
Stackdriver Logging (GA)
Cloud Spanner API (GA)
Cloud Storage (GA)
Cloud Translation API (GA)
Cloud Video Intelligence API (GA)

This client supports the following Google Cloud Platform services at a
Beta quality level:

Cloud Asset (Beta)
BigQuery Data Transfer (Beta)
Cloud Bigtable (Beta)
Stackdriver Debugger (Beta)
Stackdriver Error Reporting (Beta)
Cloud Firestore (Beta)
Cloud Pub/Sub (Beta)
Stackdriver Monitoring API (Beta)
Stackdriver Trace (Beta)

This client supports the following Google Cloud Platform services at an
Alpha quality level:

Container Engine (Alpha)
Cloud Dataproc (Alpha)
Data Loss Prevention (Alpha)
Dialogflow API (Alpha)
Cloud DNS (Alpha)
Cloud Natural Language API (Alpha)
Cloud OS Login (Alpha)
Phishing Protection (Alpha)
Recaptcha Enterprise (Alpha)
Cloud Redis (Alpha)
Cloud Resource Manager (Alpha)
Cloud Scheduler (Alpha)
Cloud Security Center (Alpha)
Cloud Speech API (Alpha)
Cloud Talent Solutions API (Alpha)
Cloud Tasks API (Alpha)
Cloud Text-To-Speech API (Alpha)
Cloud Vision API (Alpha)

The support for each service is distributed as a separate gem. However, for your
convenience, the google-cloud gem lets you install the entire collection.
If you need support for other Google APIs, check out the Google API Ruby Client
library.
Quick Start
$ gem install google-cloud
The google-cloud gem shown above provides all of the individual service gems
in the google-cloud-ruby project, making it easy to explore Google Cloud
Platform. To avoid unnecessary dependencies, you can also install the service
gems independently.
Authentication
In general, the google-cloud-ruby library uses Service
Account
credentials to connect to Google Cloud services. When running on Compute Engine
the credentials will be discovered automatically. When running on other
environments, the Service Account credentials can be specified by providing the
path to the JSON
keyfile for
the account (or the JSON itself) in environment variables. Additionally, Cloud
SDK credentials can also be discovered automatically, but this is only
recommended during development.
General instructions, environment variables, and configuration options are
covered in the general Authentication
guide
for the google-cloud umbrella package. Specific instructions and environment
variables for each individual service are linked from the README documents
listed below for each service.
The preview examples below demonstrate how to provide the Project ID and
Credentials JSON file path directly in code.
Cloud Asset API (Beta)

google-cloud-asset README
google-cloud-asset API documentation
google-cloud-asset on RubyGems

Quick Start
$ gem install google-cloud-asset
BigQuery (GA)

google-cloud-bigquery README
google-cloud-bigquery API documentation
google-cloud-bigquery on RubyGems
Google BigQuery documentation

Quick Start
$ gem install google-cloud-bigquery
Preview
require ""google/cloud/bigquery""

bigquery = Google::Cloud::Bigquery.new
dataset = bigquery.create_dataset ""my_dataset""

table = dataset.create_table ""my_table"" do |t|
  t.name = ""My Table""
  t.description = ""A description of my table.""
  t.schema do |s|
    s.string ""first_name"", mode: :required
    s.string ""last_name"", mode: :required
    s.integer ""age"", mode: :required
  end
end

# Load data into the table from Google Cloud Storage
table.load ""gs://my-bucket/file-name.csv""

# Run a query
data = dataset.query ""SELECT first_name FROM my_table""

data.each do |row|
  puts row[:first_name]
end
BigQuery Data Transfer API (Beta)

google-bigquery-data_transfer README
google-bigquery-data_transfer API documentation
google-bigquery-data_transfer on RubyGems
Google BigQuery Data Transfer documentation

Quick Start
$ gem install google-cloud-bigquery-data_transfer
Preview
require ""google/cloud/bigquery/data_transfer""

data_transfer_service_client = Google::Cloud::Bigquery::DataTransfer.new
formatted_parent = Google::Cloud::Bigquery::DataTransfer::V1::DataTransferServiceClient.project_path(project_id)

# Iterate over all results.
data_transfer_service_client.list_data_sources(formatted_parent).each do |element|
  # Process element.
end

# Or iterate over results one page at a time.
data_transfer_service_client.list_data_sources(formatted_parent).each_page do |page|
  # Process each page at a time.
  page.each do |element|
    # Process element.
  end
end
Cloud Bigtable (Beta)

google-cloud-bigtable README
google-cloud-bigtable API documentation
google-cloud-bigtable on RubyGems
Cloud Bigtable documentation

Quick Start
$ gem install google-cloud-bigtable
Preview
require ""google/cloud/bigtable""

bigtable = Google::Cloud::Bigtable.new

table = bigtable.table(""my-instance"", ""my-table"")

entry = table.new_mutation_entry(""user-1"")
entry.set_cell(
  ""cf-1"",
  ""field-1"",
  ""XYZ"",
  timestamp: Time.now.to_i * 1000 # Time stamp in milli seconds.
).delete_cells(""cf2"", ""field02"")

table.mutate_row(entry)
Cloud Datastore (GA)

google-cloud-datastore README
google-cloud-datastore API documentation
google-cloud-datastore on RubyGems
Google Cloud Datastore documentation

Follow the activation instructions to use the Google Cloud Datastore API with your project.
Quick Start
$ gem install google-cloud-datastore
Preview
require ""google/cloud/datastore""

datastore = Google::Cloud::Datastore.new(
  project_id: ""my-todo-project"",
  credentials: ""/path/to/keyfile.json""
)

# Create a new task to demo datastore
task = datastore.entity ""Task"", ""sampleTask"" do |t|
  t[""type""] = ""Personal""
  t[""done""] = false
  t[""priority""] = 4
  t[""description""] = ""Learn Cloud Datastore""
end

# Save the new task
datastore.save task

# Run a query for all completed tasks
query = datastore.query(""Task"").
  where(""done"", ""="", false)
tasks = datastore.run query
Stackdriver Debugger (Beta)

google-cloud-debugger README
google-cloud-debugger instrumentation documentation
google-cloud-debugger on RubyGems
Stackdriver Debugger documentation

Quick Start
$ gem install google-cloud-debugger

Preview
require ""google/cloud/debugger""

debugger = Google::Cloud::Debugger.new
debugger.start
Cloud DNS (Alpha)

google-cloud-dns README
google-cloud-dns API documentation
google-cloud-dns on RubyGems
Google Cloud DNS documentation

Quick Start
$ gem install google-cloud-dns
Preview
require ""google/cloud/dns""

dns = Google::Cloud::Dns.new

# Retrieve a zone
zone = dns.zone ""example-com""

# Update records in the zone
change = zone.update do |tx|
  tx.add     ""www"", ""A"",  86400, ""1.2.3.4""
  tx.remove  ""example.com."", ""TXT""
  tx.replace ""example.com."", ""MX"", 86400, [""10 mail1.example.com."",
                                           ""20 mail2.example.com.""]
  tx.modify ""www.example.com."", ""CNAME"" do |r|
    r.ttl = 86400 # only change the TTL
  end
end

Container Engine (Alpha)

google-cloud-container README
google-cloud-container API documentation
google-cloud-container on RubyGems
Container Engine documentation

Quick Start
$ gem install google-cloud-container
Preview
require ""google/cloud/container""

cluster_manager_client = Google::Cloud::Container.new
project_id_2 = project_id
zone = ""us-central1-a""
response = cluster_manager_client.list_clusters(project_id_2, zone)
Cloud Dataproc (Alpha)

google-cloud-dataproc README
google-cloud-dataproc API documentation
google-cloud-dataproc on RubyGems
Google Cloud Dataproc documentation

Quick Start
$ gem install google-cloud-dataproc
Preview
require ""google/cloud/dataproc""

cluster_controller_client = Google::Cloud::Dataproc::ClusterController.new
project_id_2 = project_id
region = ""global""

# Iterate over all results.
cluster_controller_client.list_clusters(project_id_2, region).each do |element|
  # Process element.
end

# Or iterate over results one page at a time.
cluster_controller_client.list_clusters(project_id_2, region).each_page do |page|
  # Process each page at a time.
  page.each do |element|
    # Process element.
  end
end
Data Loss Prevention (Alpha)

google-cloud-dlp README
google-cloud-dlp API documentation
google-cloud-dlp on RubyGems
Data Loss Prevention documentation

Quick Start
$ gem install google-cloud-dlp
Preview
require ""google/cloud/dlp""

dlp_service_client = Google::Cloud::Dlp.new
min_likelihood = :POSSIBLE
inspect_config = { min_likelihood: min_likelihood }
type = ""text/plain""
value = ""my phone number is 215-512-1212""
items_element = { type: type, value: value }
items = [items_element]
response = dlp_service_client.inspect_content(inspect_config, items)
Dialogflow API (Alpha)

google-cloud-dialogflow README
google-cloud-dialogflow API documentation
google-cloud-dialogflow on RubyGems
Dialogflow API documentation

Quick Start
$ gem install google-cloud-dialogflow
Stackdriver Error Reporting (Beta)

google-cloud-error_reporting README
google-cloud-error_reporting instrumentation documentation
google-cloud-error_reporting on RubyGems
Stackdriver Error Reporting documentation

Quick Start
$ gem install google-cloud-error_reporting
Preview
require ""google/cloud/error_reporting""

# Report an exception
begin
  fail ""Boom!""
rescue => exception
  Google::Cloud::ErrorReporting.report exception
end
Cloud Firestore (Beta)

google-cloud-firestore README
google-cloud-firestore API documentation
google-cloud-firestore on RubyGems
Google Cloud Firestore documentation

Quick Start
$ gem install google-cloud-firestore
Preview
require ""google/cloud/firestore""

firestore = Google::Cloud::Firestore.new(
  project_id: ""my-project"",
  credentials: ""/path/to/keyfile.json""
)

city = firestore.col(""cities"").doc(""SF"")
city.set({ name: ""San Francisco"",
           state: ""CA"",
           country: ""USA"",
           capital: false,
           population: 860000 })

firestore.transaction do |tx|
  new_population = tx.get(city).data[:population] + 1
  tx.update(city, { population: new_population })
end
Cloud Key Management Service (GA)

google-cloud-kms README
google-cloud-kms API documentation
google-cloud-kms on RubyGems
Google Cloud KMS documentation

Quick Start
$ gem install google-cloud-kms
Preview
require ""google/cloud/kms""

# Create a client for a project and given credentials
kms = Google::Cloud::Kms.new credentials: ""/path/to/keyfile.json""

# Where to create key rings
key_ring_parent = kms.class.location_path ""my-project"", ""us-central1""

# Create a new key ring
key_ring = kms.create_key_ring key_ring_parent, ""my-ring"", {}
puts ""Created at #{Time.new key_ring.create_time.seconds}""

# Iterate over created key rings
kms.list_key_rings(key_ring_parent).each do |key_ring|
  puts ""Found ring called #{key_ring.name}""
end
Stackdriver Logging (GA)

google-cloud-logging README
google-cloud-logging API documentation
google-cloud-logging on RubyGems
Stackdriver Logging documentation

Quick Start
$ gem install google-cloud-logging
Preview
require ""google/cloud/logging""

logging = Google::Cloud::Logging.new

# List all log entries
logging.entries.each do |e|
  puts ""[#{e.timestamp}] #{e.log_name} #{e.payload.inspect}""
end

# List only entries from a single log
entries = logging.entries filter: ""log:syslog""

# Write a log entry
entry = logging.entry
entry.payload = ""Job started.""
entry.log_name = ""my_app_log""
entry.resource.type = ""gae_app""
entry.resource.labels[:module_id] = ""1""
entry.resource.labels[:version_id] = ""20150925t173233""

logging.write_entries entry
Cloud Natural Language API (Alpha)

google-cloud-language README
google-cloud-language API documentation
google-cloud-language on RubyGems
Google Cloud Natural Language API documentation

Quick Start
$ gem install google-cloud-language
Preview
require ""google/cloud/language""

language = Google::Cloud::Language.new(
  project_id: ""my-todo-project"",
  credentials: ""/path/to/keyfile.json""
)

content = ""Star Wars is a great movie. The Death Star is fearsome.""
document = language.document content
annotation = document.annotate

annotation.entities.count #=> 3
annotation.sentiment.score #=> 0.10000000149011612
annotation.sentiment.magnitude #=> 1.100000023841858
annotation.sentences.count #=> 2
annotation.tokens.count #=> 13
Cloud OS Login (Alpha)

google-cloud-os_login README
google-cloud-os_login API documentation
google-cloud-os_login on RubyGems
Google Cloud DNS documentation

Quick Start
$ gem install google-cloud-os_login
Phishing Protection (Alpha)

google-cloud-phishing_protection README
google-cloud-phishing_protection API documentation
google-cloud-phishing_protection on RubyGems
Phishing Protection documentation

Quick Start
$ gem install google-cloud-phishing_protection
Cloud Pub/Sub (Beta)

google-cloud-pubsub README
google-cloud-pubsub API documentation
google-cloud-pubsub on RubyGems
Google Cloud Pub/Sub documentation

Quick Start
$ gem install google-cloud-pubsub
Preview
require ""google/cloud/pubsub""

pubsub = Google::Cloud::Pubsub.new(
  project_id: ""my-todo-project"",
  credentials: ""/path/to/keyfile.json""
)

# Retrieve a topic
topic = pubsub.topic ""my-topic""

# Publish a new message
msg = topic.publish ""new-message""

# Retrieve a subscription
sub = pubsub.subscription ""my-topic-sub""

# Create a subscriber to listen for available messages
subscriber = sub.listen do |received_message|
  # process message
  received_message.acknowledge!
end

# Start background threads that will call the block passed to listen.
subscriber.start

# Shut down the subscriber when ready to stop receiving messages.
subscriber.stop.wait!
Recaptcha Enterprise (Alpha)

google-cloud-recaptcha_enterprise README
google-cloud-recaptcha_enterprise API documentation
google-cloud-recaptcha_enterprise on RubyGems
Recaptcha Enterprise documentation

Quick Start
$ gem install google-cloud-recaptcha_enterprise
Cloud Redis API (Alpha)

google-cloud-redis README
google-cloud-redis API documentation
google-cloud-redis on RubyGems
Cloud Redis API documentation

Quick Start
$ gem install google-cloud-redis
Cloud Resource Manager (Alpha)

google-cloud-resource_manager README
google-cloud-resource_manager API documentation
google-cloud-resource_manager on RubyGems
Google Cloud Resource Manager documentation

Quick Start
$ gem install google-cloud-resource_manager
Preview
require ""google/cloud/resource_manager""

resource_manager = Google::Cloud::ResourceManager.new

# List all projects
resource_manager.projects.each do |project|
  puts projects.project_id
end

# Label a project as production
project = resource_manager.project ""tokyo-rain-123""
project.update do |p|
  p.labels[""env""] = ""production""
end

# List only projects with the ""production"" label
projects = resource_manager.projects filter: ""labels.env:production""
Stackdriver Trace (Beta)

google-cloud-trace README
google-cloud-trace instrumentation documentation
google-cloud-trace on RubyGems
Stackdriver Trace documentation

Quick Start
$ gem install google-cloud-trace
Preview
require ""google/cloud/trace""

trace = Google::Cloud::Trace.new

result_set = trace.list_traces Time.now - 3600, Time.now
result_set.each do |trace_record|
  puts ""Retrieved trace ID: #{trace_record.trace_id}""
end
Cloud Spanner API (GA)

google-cloud-spanner README
google-cloud-spanner API documentation
google-cloud-spanner on RubyGems
Google Cloud Speech API documentation

Quick Start
$ gem install google-cloud-spanner
Preview
require ""google/cloud/spanner""

spanner = Google::Cloud::Spanner.new

db = spanner.client ""my-instance"", ""my-database""

db.transaction do |tx|
  results = tx.execute ""SELECT * FROM users""

  results.rows.each do |row|
    puts ""User #{row[:id]} is #{row[:name]}""
  end
end
Cloud Speech API (Alpha)

google-cloud-speech README
google-cloud-speech API documentation
google-cloud-speech on RubyGems
Google Cloud Speech API documentation

Quick Start
$ gem install google-cloud-speech
Preview
require ""google/cloud/speech""

speech = Google::Cloud::Speech.new

audio = speech.audio ""path/to/audio.raw"",
                     encoding: :raw, sample_rate: 16000
results = audio.recognize

result = results.first
result.transcript #=> ""how old is the Brooklyn Bridge""
result.confidence #=> 0.9826789498329163
Cloud Scheduler (Alpha)

Client Library Documentation
Product Documentation

Quick Start
In order to use this library, you first need to go through the following
steps:

Select or create a Cloud Platform project.
Enable billing for your project.
Enable the Cloud Scheduler API.
Setup Authentication.

Installation
$ gem install google-cloud-scheduler

Next Steps

Read the Client Library Documentation for Cloud Scheduler API
to see other available methods on the client.
Read the Cloud Scheduler API Product documentation
to learn more about the product and see How-to Guides.
View this repository's main README
to see the full list of Cloud APIs that we cover.

Enabling Logging
To enable logging for this library, set the logger for the underlying gRPC library.
The logger that you set may be a Ruby stdlib Logger as shown below,
or a Google::Cloud::Logging::Logger
that will write logs to Stackdriver Logging. See grpc/logconfig.rb
and the gRPC spec_helper.rb for additional information.
Configuring a Ruby stdlib logger:
require ""logger""

module MyLogger
  LOGGER = Logger.new $stderr, level: Logger::WARN
  def logger
    LOGGER
  end
end

# Define a gRPC module-level logger method before grpc/logconfig.rb loads.
module GRPC
  extend MyLogger
end
Cloud Security Center API (Alpha)

google-cloud-security_center README
google-cloud-security_center API documentation
google-cloud-security_center on RubyGems
Google Cloud Security Center API documentation

Quick Start
$ gem install google-cloud-security_center
Cloud Storage (GA)

google-cloud-storage README
google-cloud-storage API documentation
google-cloud-storage on RubyGems
Google Cloud Storage documentation

Quick Start
$ gem install google-cloud-storage
Preview
require ""google/cloud/storage""

storage = Google::Cloud::Storage.new(
  project_id: ""my-todo-project"",
  credentials: ""/path/to/keyfile.json""
)

bucket = storage.bucket ""task-attachments""

file = bucket.file ""path/to/my-file.ext""

# Download the file to the local file system
file.download ""/tasks/attachments/#{file.name}""

# Copy the file to a backup bucket
backup = storage.bucket ""task-attachment-backups""
file.copy backup, file.name
Cloud Talent Solutions API (Alpha)

google-cloud-talent README
google-cloud-talent API documentation
google-cloud-talent on RubyGems
Google Cloud Talent Solutions documentation

Quick Start
$ gem install google-cloud-talent
Preview
 require ""google/cloud/talent""

 require ""google/cloud/talent""
 job_service_client = Google::Cloud::Talent::JobService.new(version: :v4beta1)
 formatted_parent = job_service_client.project_path(""[PROJECT]"")

 # TODO: Initialize `filter`:
 filter = ''
 # Iterate over all results.
 job_service_client.list_jobs(formatted_parent, filter).each do |element|
   # Process element.
 end

 # Or iterate over results one page at a time.
 job_service_client.list_jobs(formatted_parent, filter).each_page do |page|
   # Process each page at a time.
   page.each do |element|
     # Process element.
   end
 end
Cloud Tasks API (Alpha)

google-cloud-tasks README
google-cloud-tasks API documentation
google-cloud-tasks on RubyGems

Quick Start
$ gem install google-cloud-tasks
Preview
 require ""google/cloud/tasks/v2beta2""

 cloud_tasks_client = Google::Cloud::Tasks::V2beta2.new
 formatted_parent = Google::Cloud::Tasks::V2beta2::CloudTasksClient.location_path(""[PROJECT]"", ""[LOCATION]"")

 # Iterate over all results.
 cloud_tasks_client.list_queues(formatted_parent).each do |element|
   # Process element.
 end

 # Or iterate over results one page at a time.
 cloud_tasks_client.list_queues(formatted_parent).each_page do |page|
   # Process each page at a time.
   page.each do |element|
     # Process element.
   end
 end
Cloud Text To Speech API (Alpha)
Quick Start
$ gem install google-cloud-text_to_speech
Preview
require ""google/cloud/text_to_speech/v1""

text_to_speech_client = Google::Cloud::TextToSpeech::V1.new

input = {text: ""Hello, world!""}
voice = {language_code: ""en-US""}
audio_config = {audio_encoding: Google::Cloud::Texttospeech::V1::AudioEncoding::MP3}
response = text_to_speech_client.synthesize_speech input, voice, audio_config
File.open ""hello.mp3"", ""w"" do |file|
  file.write response.audio_content
end
Cloud Translation API (GA)

google-cloud-translate README
google-cloud-translate API documentation
google-cloud-translate on RubyGems
Google Cloud Translation API documentation

Quick Start
$ gem install google-cloud-translate
Preview
require ""google/cloud/translate""

translate = Google::Cloud::Translate.new

translation = translate.translate ""Hello world!"", to: ""la""

puts translation #=> Salve mundi!

translation.from #=> ""en""
translation.origin #=> ""Hello world!""
translation.to #=> ""la""
translation.text #=> ""Salve mundi!""
Cloud Vision API (Alpha)

google-cloud-vision README
google-cloud-vision API documentation
google-cloud-vision on RubyGems
Google Cloud Vision API documentation

Quick Start
$ gem install google-cloud-vision
Preview
require ""google/cloud/vision""

image_annotator_client = Google::Cloud::Vision::ImageAnnotator.new
gcs_image_uri = ""gs://gapic-toolkit/President_Barack_Obama.jpg""
source = { gcs_image_uri: gcs_image_uri }
image = { source: source }
type = :FACE_DETECTION
features_element = { type: type }
features = [features_element]
requests_element = { image: image, features: features }
requests = [requests_element]
response = image_annotator_client.batch_annotate_images(requests)
Stackdriver Monitoring API (Beta)

google-cloud-monitoring README
google-cloud-monitoring API documentation
google-cloud-monitoring on RubyGems
Google Cloud Monitoring API documentation

Quick Start
$ gem install google-cloud-monitoring
Preview
 require ""google/cloud/monitoring/v3""

 MetricServiceClient = Google::Cloud::Monitoring::V3::MetricServiceClient

 metric_service_client = MetricServiceClient.new
 formatted_name = MetricServiceClient.project_path(project_id)

 # Iterate over all results.
 metric_service_client.list_monitored_resource_descriptors(formatted_name).each do |element|
   # Process element.
 end

 # Or iterate over results one page at a time.
 metric_service_client.list_monitored_resource_descriptors(formatted_name).each_page do |page|
   # Process each page at a time.
   page.each do |element|
     # Process element.
   end
 end
Cloud Video Intelligence API (GA)

google-cloud-video_intelligence README
google-cloud-video_intelligence API documentation
google-cloud-video_intelligence on RubyGems
Google Cloud Video Intelligence API documentation

Quick Start
$ gem install google-cloud-video_intelligence
Preview
 require ""google/cloud/video_intelligence/v1beta2""

 video_intelligence_service_client = Google::Cloud::VideoIntelligence.new
 input_uri = ""gs://cloud-ml-sandbox/video/chicago.mp4""
 features_element = :LABEL_DETECTION
 features = [features_element]

 # Register a callback during the method call.
 operation = video_intelligence_service_client.annotate_video(input_uri: input_uri, features: features) do |op|
   raise op.results.message if op.error?
   op_results = op.results
   # Process the results.

   metadata = op.metadata
   # Process the metadata.
 end

 # Or use the return value to register a callback.
 operation.on_done do |op|
   raise op.results.message if op.error?
   op_results = op.results
   # Process the results.

   metadata = op.metadata
   # Process the metadata.
 end

 # Manually reload the operation.
 operation.reload!

 # Or block until the operation completes, triggering callbacks on
 # completion.
 operation.wait_until_done!
Supported Ruby Versions
These libraries are currently supported on Ruby 2.3+.
Google provides official support for Ruby versions that are actively supported
by Ruby Core—that is, Ruby versions that are either in normal maintenance or
in security maintenance, and not end of life. Currently, this means Ruby 2.3
and later. Older versions of Ruby may still work, but are unsupported and not
recommended. See https://www.ruby-lang.org/en/downloads/branches/ for details
about the Ruby support schedule.
Versioning
This library follows Semantic Versioning.
Please note it is currently under active development. Any release versioned 0.x.y is subject to backwards incompatible changes at any time.
GA: Libraries defined at the GA (general availability) quality level are stable. The code surface will not change in backwards-incompatible ways unless absolutely necessary (e.g. because of critical security issues) or with an extensive deprecation period. Issues and requests against GA libraries are addressed with the highest priority.
Please note that the auto-generated portions of the GA libraries (the ones in modules such as v1 or v2) are considered to be of Beta quality, even if the libraries that wrap them are GA.
Beta: Libraries defined at a Beta quality level are expected to be mostly stable and we're working towards their release candidate. We will address issues and requests with a higher priority.
Alpha: Libraries defined at an Alpha quality level are still a work-in-progress and are more likely to get backwards-incompatible updates.
Contributing
Contributions to this library are always welcome and highly encouraged.
See CONTRIBUTING for more information on how to get started.
Please note that this project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms. See Code of Conduct for more information.
License
This library is licensed under Apache 2.0. Full license text is
available in LICENSE.
Support
Please report bugs at the project on Github.
Don't hesitate to ask questions about the client or APIs on StackOverflow.
",797
26,rakudo/rakudo,Perl 6,"Rakudo Perl 6
This is Rakudo Perl 6, a Perl 6 compiler for the MoarVM and JVM.
Rakudo Perl 6 is Copyright © 2008-2019, The Perl Foundation. Rakudo Perl 6
is distributed under the terms of the Artistic License 2.0. For more
details, see the full text of the license in the file LICENSE.
This directory contains only the Rakudo Perl 6 compiler itself; it
does not contain any of the modules, documentation, or other items
that would normally come with a full Perl 6 distribution.  If you're
after more than just the bare compiler, please download the latest
Rakudo Star package.
Note that different backends implement slightly different sets of
features. For a high-level overview of implemented and missing features,
please visit the features page on perl6.org.
Recent changes and feature additions are documented in the docs/ChangeLog
text file.
To receive important notifications from the core developer team, please
subscribe to the p6lert service using the RSS feed,
twitter, or the p6lert commandline script.
Building and Installing Rakudo
 
See the INSTALL.txt file for detailed prerequisites and build and
installation instructions.
The general process for building is running perl Configure.pl with
the desired configuration options (common options listed below), and
then running make or make install. Optionally, you may run
make spectest to test your build on Roast,
the Official Perl 6 test suite. To update the test suite, run
make spectest_update.
Installation of Rakudo simply requires building and running make install.
Note that this step is necessary for running Rakudo from outside the build
directory. But don't worry, it installs locally by default, so you don't need
any administrator privileges for carrying out this step.
Configuring Rakudo to run on MoarVM
To automatically download, build, and install a fresh MoarVM and NQP, run:
$ perl Configure.pl --gen-moar --gen-nqp --backends=moar

Please be aware, that this will install MoarVM and NQP into your given
--prefix before Configure.pl exits.
Alternatively, feel free to git clone https://github.com/perl6/nqp and
https://github.com/MoarVM/MoarVM manually and install them individually.
Configuration flags can be passed to MoarVM's Configure.pl using the
--moar-option flag. For example, if you wish to use Clang when GCC is the
default compiler selected for your OS, use the --compiler flag:
$ perl Configure.pl --gen-moar --moar-option='--compiler=clang' \
    --gen-nqp --backends=moar

If the compiler you want to use isn't known by MoarVM or you have multiple
versions of the same compiler installed, the --cc flag can be used to pass its
exact binary:
$ perl Configure.pl --gen-moar --moar-option='--cc=egcc' \
    --gen-nqp --backends=moar

Custom optimization and debugging levels may also be passed through:
$ perl Configure.pl --gen-moar --moar-option='--optimize=0 --debug=3' \
    --gen-nqp --backends=moar

For more information on how MoarVM can be configured, view MoarVM's
Configure.pl.
Configuring Rakudo to run on the JVM
Note that to run Rakudo on JVM, JDK 1.8 must be installed. To automatically
download, build, and install a fresh NQP, run:
$ perl Configure.pl --gen-nqp --backends=jvm

If you get a java.lang.OutOfMemoryError: Java heap space error building
rakudo on the JVM, you may need to modify your NQP runner to limit memory
use. e.g. edit the nqp-j / nqp-j.bat executable (found wherever you installed to, or in the
install/bin directory) to include -Xms500m -Xmx3g as options passed to java.
Alternatively, you can set JAVA_OPTS env var; e.g.
export JAVA_OPTS=""-Xmx51200000000""
Please be aware, that this will install NQP into your given --prefix
before Configure.pl exits.
Alternatively, feel free to git clone https://github.com/perl6/nqp manually
and install it individually.
Multiple backends at the same time
By supplying combinations of backends to the --backends flag, you
can get two or three backends built in the same prefix. The first
backend you supply in the list is the one that gets the perl6 name
as a symlink, and all backends are installed separately as
perl6-m or perl6-j for Rakudo on
MoarVM, or JVM respectively.
The format for the --backends flag is:
$ perl Configure.pl --backends=moar,jvm
$ perl Configure.pl --backends=ALL

Testing
Run the full spectest:
$ make spectest   # <== takes a LONG time!!

To run a single test, one must use make because of the tooling required to
run the spectests.  For example:
$ make t/spec/S12-traits/parameterized.t

Run all tests in one S* directory with a sh script. One example:
$ cat run-tests.sh
#!/bin/sh

# specify the desired directory:
D='t/spec/S26-documentation'

# collect the individual files
F=$(ls $D/*t)

# and run them
for f in $F
do
    echo ""Testing file '$f'""
    make $f
done
echo ""All tests in dir '$D' have been run.""

That can be written as a one-liner:
for f in $(ls t/spec/S26-documentation/*t); do make ""$f""; done

Where to get help or answers to questions
There are several mailing lists, IRC channels, and wikis available with
help for Perl 6 and Rakudo. Figuring out the right one to use
is often the biggest battle. Here are some rough guidelines:
The central hub for Perl 6 information is perl6.org.
This is always a good starting point.
If you have a question about Perl 6 syntax or the right way to approach
a problem using Perl 6, you probably want the “perl6-users@perl.org”
mailing list or the irc.freenode.net/#perl6 IRC
channel. The perl6-users
list is primarily for the people who want to use Perl 6 to write
programs, so newbie questions are welcomed there.  Newbie questions
are also welcome on the #perl6 channel; the Rakudo and Perl 6
development teams tend to hang out there and are generally glad
to help.  You can follow @perl6org
and on Twitter, there's a Perl 6 news aggregator at
Planet Perl 6.
Questions about NQP can also be posted to the #perl6 IRC channel.
For questions about MoarVM, you can join #moarvm on freenode.
Reporting bugs
See https://rakudo.org/bugs
Submitting patches
If you have a patch that fixes a bug or adds a new feature, please
create a pull request using github's pull request infrastructure.
See our contribution guidelines for more information.
Line editing and tab completion
If you would like simple history and tab completion in the perl6 executable,
you need to install the Linenoise module.  The recommended way to install
Linenoise is via zef:
$ zef install Linenoise

An alternative is to use a third-party program such as rlwrap.
AUTHOR
Jonathan Worthington is the current pumpking for Rakudo Perl 6.
See CREDITS for the many people that have contributed
to the development of the Rakudo compiler.
",1118
27,jdayllon/contratacionestado,None,"contratacionestado
El objetivo del proyecto es generar un dataset sobre contratación del sector público para facilitar la reutilización y el análisis de la información que genera este tipo de contratación
Sobre la información recogida


La información en el repositorio procede de las páginas web de los organismos:

Junta de Andalucía (http://www.juntadeandalucia.es)
Generalitat de Catalunya. (https://contractaciopublica.gencat.cat/)



Las transformaciones aplicadas buscan facilitar el proceso de la información o enriquecer la información ofrecida por las plataformas anteriores.


La estructura de los datos se está adaptando a un modelo inspirado en CODICE 2.1 / UBL
** Más información en: https://contrataciondelestado.es/wps/portal/codice


Propiedad Intelectual
Junta de Andalucía
Tal y como se cita en el aviso legal del Portal de la Junta de Andalucía:
La Junta de Andalucía promueve el libre uso y reutilización de los textos disponibles en el presente Portal sobre los que ostenta derechos de propiedad intelectual. Dichos textos están disponibles a través de una licencia-tipo Creative Commons Reconocimiento 3.0.
De modo general, la Junta de Andalucía te autoriza a:

Copiar, redistribuir y comunicar públicamente los textos del Portal.
Hacer un uso comercial de los contenidos.
Generar obras derivadas.

He impone las siguientes obligaciones:

Reconocer explícitamente la fuente de información Identificada en el presente texto.
Incluir la misma obligación de reconocimiento en los términos de licencia de cualquier producto derivado que haga uso de esta información. * Por lo que usuarios de esta información deben tener presente esta obligación *
No desnaturalizar el sentido de la información reproducida.
Evitar cualquier rasgo de presentación que sugiera que la Junta de Andalucía apoya o promueve el uso que se hace de la información difundida. En ningún caso está permitida la reproducción de logotipos, escudos, símbolos y marcas identificativas de la Junta de Andalucía sin autorización expresa de la institución.

Generalitat de Catalunya
TODO
",2
28,googleapis/google-api-nodejs-client,TypeScript,"
Google APIs Node.js Client






Node.js client library for using Google APIs. Support for authorization and authentication with OAuth 2.0, API Keys and JWT tokens is included.

Google APIs
Getting started

Installation
First example
Samples
API Reference


Authentication and authorization

OAuth2 client
Using API keys
Service <--> Service authentication
Setting global or service-level auth


Usage

Specifying request body
Media uploads
Request Options
Using a Proxy
Supported APIs
TypeScript


License
Contributing
Questions/problems?

Google APIs
The full list of supported APIs can be found here. The API endpoints are automatically generated, so if the API is not in the list, it is currently not supported by this API client library.
Supported APIs are listed on the Google APIs Explorer.
Working with Google Cloud Platform APIs?
If you're working with Google Cloud Platform APIs such as Datastore, Cloud Storage or Pub/Sub, consider using the @google-cloud client libraries: single purpose idiomatic Node.js clients for Google Cloud Platform services.
Support and maintenance
These client libraries are officially supported by Google. However, these libraries are considered complete and are in maintenance mode. This means that we will address critical bugs and security issues but will not add any new features. For Google Cloud Platform APIs, we recommend using google-cloud-node which is under active development.
This library supports the maintenance LTS, active LTS, and current release of node.js.  See the node.js release schedule for more information.
Getting started
Installation
This library is distributed on npm. In order to add it as a dependency, run the following command:
$ npm install googleapis
First example
This is a very simple example. This creates a Blogger client and retrieves the details of a blog given the blog Id:
const {google} = require('googleapis');

// Each API may support multiple version. With this sample, we're getting
// v3 of the blogger API, and using an API key to authenticate.
const blogger = google.blogger({
  version: 'v3',
  auth: 'YOUR API KEY'
});

const params = {
  blogId: 3213900
};

// get the blog details
blogger.blogs.get(params, (err, res) => {
  if (err) {
    console.error(err);
    throw err;
  }
  console.log(`The blog url is ${res.data.url}`);
});
Instead of using callbacks you can also use promises!
blogger.blogs.get(params)
  .then(res => {
    console.log(`The blog url is ${res.data.url}`);
  })
  .catch(error => {
    console.error(error);
  });
Or async/await:
async function runSample() {
  const res = await blogger.blogs.get(params);
  console.log(`The blog url is ${res.data.url}`);
}
runSample().catch(console.error);
Samples
There are a lot of samples 🤗  If you're trying to figure out how to use an API ... look there first! If there's a sample you need missing, feel free to file an issue.
API Reference
This library has a full set of API Reference Documentation. This documentation is auto-generated, and the location may change.
Authentication and authorization
There are three primary ways to authenticate to Google APIs. Some service support all authentication methods, other may only support one or two.


OAuth2 - This allows you to make API calls on behalf of a given user.  In this model, the user visits your application, signs in with their Google account, and provides your application with authorization against a set of scopes.  Learn more.


Service <--> Service - In this model, your application talks directly to Google APIs using a Service Account.  It's useful when you have a backend application that will talk directly to Google APIs from the backend. Learn more.


API Key - With an API key, you can access your service from a client or the server.  Typically less secure, this is only available on a small subset of services with limited scopes.  Learn more.


To learn more about the authentication client, see the Google Auth Library.
OAuth2 client
This client comes with an OAuth2 client that allows you to retrieve an access token, refresh it, and retry the request seamlessly. The basics of Google's OAuth2 implementation is explained on Google Authorization and Authentication documentation.
In the following examples, you may need a CLIENT_ID, CLIENT_SECRET and REDIRECT_URL. You can find these pieces of information by going to the Developer Console, clicking your project --> APIs & auth --> credentials.
For more information about OAuth2 and how it works, see here.
A complete sample application that authorizes and authenticates with the OAuth2 client is available at samples/oauth2.js.
Generating an authentication URL
To ask for permissions from a user to retrieve an access token, you redirect them to a consent page. To create a consent page URL:
const {google} = require('googleapis');

const oauth2Client = new google.auth.OAuth2(
  YOUR_CLIENT_ID,
  YOUR_CLIENT_SECRET,
  YOUR_REDIRECT_URL
);

// generate a url that asks permissions for Blogger and Google Calendar scopes
const scopes = [
  'https://www.googleapis.com/auth/blogger',
  'https://www.googleapis.com/auth/calendar'
];

const url = oauth2Client.generateAuthUrl({
  // 'online' (default) or 'offline' (gets refresh_token)
  access_type: 'offline',

  // If you only need one scope you can pass it as a string
  scope: scopes
});
IMPORTANT NOTE - The refresh_token is only returned on the first authorization. More details here.
Retrieve authorization code
Once a user has given permissions on the consent page, Google will redirect the page to the redirect URL you have provided with a code query parameter.
GET /oauthcallback?code={authorizationCode}

Retrieve access token
With the code returned, you can ask for an access token as shown below:
// This will provide an object with the access_token and refresh_token.
// Save these somewhere safe so they can be used at a later time.
const {tokens} = await oauth2Client.getToken(code)
oauth2Client.setCredentials(tokens);
With the credentials set on your OAuth2 client - you're ready to go!
Handling refresh tokens
Access tokens expire. This library will automatically use a refresh token to obtain a new access token if it is about to expire. An easy way to make sure you always store the most recent tokens is to use the tokens event:
oauth2Client.on('tokens', (tokens) => {
  if (tokens.refresh_token) {
    // store the refresh_token in my database!
    console.log(tokens.refresh_token);
  }
  console.log(tokens.access_token);
});
To set the refresh_token at a later time, you can use the setCredentials method:
oauth2Client.setCredentials({
  refresh_token: `STORED_REFRESH_TOKEN`
});
Once the client has a refresh token, access tokens will be acquired and refreshed automatically in the next call to the API.
Using API keys
You may need to send an API key with the request you are going to make. The following uses an API key to make a request to the Blogger API service to retrieve a blog's name, url, and its total amount of posts:
const {google} = require('googleapis');
const blogger = google.blogger_v3({
  version: 'v3',
  auth: 'YOUR_API_KEY' // specify your API key here
});

const params = {
  blogId: 3213900
};

async function main(params) {
  const res = await blogger.blogs.get({blogId: params.blogId});
  console.log(`${res.data.name} has ${res.data.posts.totalItems} posts! The blog url is ${res.data.url}`)
};

main().catch(console.error);
To learn more about API keys, please see the documentation.
Service to Service Authentication
Rather than manually creating an OAuth2 client, JWT client, or Compute client, the auth library can create the correct credential type for you, depending upon the environment your code is running under.
For example, a JWT auth client will be created when your code is running on your local developer machine, and a Compute client will be created when the same code is running on a configured instance of Google Compute Engine.
The code below shows how to retrieve a default credential type, depending upon the runtime environment. The createScopedRequired must be called to determine when you need to pass in the scopes manually, and when they have been set for you automatically based on the configured runtime environment.
const {google} = require('googleapis');
const compute = google.compute('v1');

async function main () {
  // This method looks for the GCLOUD_PROJECT and GOOGLE_APPLICATION_CREDENTIALS
  // environment variables.
  const auth = await google.auth.getClient({
    // Scopes can be specified either as an array or as a single, space-delimited string.
    scopes: ['https://www.googleapis.com/auth/compute']
  });

  // obtain the current project Id
  const project = await google.auth.getProjectId();

  // Fetch the list of GCE zones within a project.
  const res = await compute.zones.list({ project, auth });
  console.log(res.data);
}

main().catch(console.error);
Setting global or service-level auth
You can set the auth as a global or service-level option so you don't need to specify it every request. For example, you can set auth as a global option:
const {google} = require('googleapis');

const oauth2Client = new google.auth.OAuth2(
  YOUR_CLIENT_ID,
  YOUR_CLIENT_SECRET,
  YOUR_REDIRECT_URL
);

// set auth as a global default
google.options({
  auth: oauth2Client
});
Instead of setting the option globally, you can also set the authentication client at the service-level:
const {google} = require('googleapis');
const oauth2Client = new google.auth.OAuth2(
  YOUR_CLIENT_ID,
  YOUR_CLIENT_SECRET,
  YOUR_REDIRECT_URL
);

const drive = google.drive({
  version: 'v2',
  auth: oauth2Client
});
See the Options section for more information.
Usage
Specifying request body
The body of the request is specified in the requestBody parameter object of the request. The body is specified as a JavaScript object with key/value pairs. For example, this sample creates a watcher that posts notifications to a Google Cloud Pub/Sub topic when emails are sent to a gmail account:
const res = await gmail.users.watch({
  userId: 'me',
  requestBody: {
    // Replace with `projects/${PROJECT_ID}/topics/${TOPIC_NAME}`
    topicName: `projects/el-gato/topics/gmail`
  }
});
console.log(res.data);
Media uploads
This client supports multipart media uploads. The resource parameters are specified in the requestBody parameter object, and the media itself is specified in the media.body parameter with mime-type specified in media.mimeType.
This example uploads a plain text file to Google Drive with the title ""Test"" and contents ""Hello World"".
const drive = google.drive({
  version: 'v3',
  auth: oauth2Client
});

const res = await drive.files.create({
  requestBody: {
    name: 'Test',
    mimeType: 'text/plain'
  },
  media: {
    mimeType: 'text/plain',
    body: 'Hello World'
  }
});
You can also upload media by specifying media.body as a Readable stream. This can allow you to upload very large files that cannot fit into memory.
const fs = require('fs');

const drive = google.drive({
  version: 'v3',
  auth: oauth2Client
});

async function main() {
  const res = await drive.files.create({
    requestBody: {
      name: 'testimage.png',
      mimeType: 'image/png'
    },
    media: {
      mimeType: 'image/png',
      body: fs.createReadStream('awesome.png')
    }
  });
  console.log(res.data);
}

main().catch(console.error);
For more examples of creation and modification requests with media attachments, take a look at the samples/drive/upload.js sample.
Request Options
For more fine-tuned control over how your API calls are made, we provide you with the ability to specify additional options that can be applied directly to the 'gaxios' object used in this library to make network calls to the API.
You may specify additional options either in the global google object or on a service client basis.  The options you specify are attached to the gaxios object so whatever gaxios supports, this library supports. You may also specify global or per-service request parameters that will be attached to all API calls you make.
A full list of supported options can be [found here][requestopts].
Global options
You can choose default options that will be sent with each request. These options will be used for every service instantiated by the google client. In this example, the timeout property of GaxiosOptions will be set for every request:
const {google} = require('googleapis');
google.options({
  // All requests made with this object will use these settings unless overridden.
  timeout: 1000,
  auth: auth
});
You can also modify the parameters sent with each request:
const {google} = require('googleapis');
google.options({
  // All requests from all services will contain the above query parameter
  // unless overridden either in a service client or in individual API calls.
  params: {
    quotaUser: 'user123@example.com'
  }
});
Service-client options
You can also specify options when creating a service client.
const blogger = google.blogger({
  version: 'v3',
  // All requests made with this object will use the specified auth.
  auth: 'API KEY';
});
By doing this, every API call made with this service client will use 'API KEY' to authenticate.
Note: Created clients are immutable so you must create a new one if you want to specify different options.
Similar to the examples above, you can also modify the parameters used for every call of a given service:
const blogger = google.blogger({
  version: 'v3',
  // All requests made with this service client will contain the
  // blogId query parameter unless overridden in individual API calls.
  params: {
    blogId: 3213900
  }
});

// Calls with this drive client will NOT contain the blogId query parameter.
const drive = google.drive('v3');
...

Request-level options
You can specify an auth object to be used per request. Each request also inherits the options specified at the service level and global level.
For example:
const {google} = require('googleapis');
const bigquery = google.bigquery('v2');

async function main() {

  // This method looks for the GCLOUD_PROJECT and GOOGLE_APPLICATION_CREDENTIALS
  // environment variables.
  const client = await google.auth.getClient({
    scopes: ['https://www.googleapis.com/auth/cloud-platform']
  });

  const projectId = await google.auth.getProjectId();

  const request = {
    projectId,
    datasetId: '<YOUR_DATASET_ID>',

    // This is a ""request-level"" option
    auth: client
  };

  const res = await bigquery.datasets.delete(request);
  console.log(res.data);

}

main().catch(console.error);
You can also override gaxios options per request, such as url, method, and encoding.
For example:
const res = await drive.files.export({
  fileId: 'asxKJod9s79', // A Google Doc
  mimeType: 'application/pdf'
}, {
  // Make sure we get the binary data
  encoding: null
});
Using a Proxy
You can use the following environment variables to proxy HTTP and HTTPS requests:

HTTP_PROXY / http_proxy
HTTPS_PROXY / https_proxy

When HTTP_PROXY / http_proxy are set, they will be used to proxy non-SSL requests that do not have an explicit proxy configuration option present. Similarly, HTTPS_PROXY / https_proxy will be respected for SSL requests that do not have an explicit proxy configuration option. It is valid to define a proxy in one of the environment variables, but then override it for a specific request, using the proxy configuration option.
Getting Supported APIs
You can programatically obtain the list of supported APIs, and all available versions:
const {google} = require('googleapis');
const apis = google.getSupportedAPIs();
This will return an object with the API name as object property names, and an array of version strings as the object values;
TypeScript
This library is written in TypeScript, and provides types out of the box. All classes and interfaces generated for each API are exported under the ${apiName}_${version} namespace.  For example, the Drive v3 API types are all available from the drive_v3 namespace:
import { drive_v3 } from 'googleapis';
Release Notes & Breaking Changes
You can find a detailed list of breaking changes and new features in our Release Notes. If you've used this library before 25.x, see our Release Notes to learn about migrating your code from 24.x.x to 25.x.x. It's pretty easy :)
License
This library is licensed under Apache 2.0. Full license text is available in LICENSE.
Contributing
We love contributions! Before submitting a Pull Request, it's always good to start with a new issue first. To learn more, see CONTRIBUTING.
Questions/problems?

Ask your development related questions on Stackoverflow.
If you've found an bug/issue, please file it on GitHub.

",7443
29,oracle/graal,Java,"GraalVM

GraalVM is a universal virtual machine for running applications written in JavaScript, Python, Ruby, R, JVM-based languages like Java, Scala, Clojure, Kotlin, and LLVM-based languages such as C and C++.
The project website at https://www.graalvm.org describes how to get started, how to stay connected, and how to contribute.
Repository Structure
The GraalVM main source repository includes the following components:


GraalVM SDK contains long term supported APIs of GraalVM.


Graal compiler written in Java that supports both dynamic and static compilation and can integrate with
the Java HotSpot VM or run standalone.


Truffle language implementation framework for creating languages and instrumentations for GraalVM.


Tools contains a set of tools for GraalVM languages
implemented with the instrumentation framework.


Substrate VM framework that allows ahead-of-time (AOT)
compilation of Java applications under closed-world assumption into executable
images or shared objects.


Sulong is an engine for running LLVM bitcode on GraalVM.


TRegex is an implementation of regular expressions which leverages GraalVM for efficient compilation of automata.


VM includes the components to build a modular GraalVM image.


Reporting Vulnerabilities
Please report security vulnerabilities not via GitHub issues or the public mailing lists, but via the process outlined at Reporting Vulnerabilities guide.
Related Repositories
GraalVM allows running of following languages which are being developed and tested in related repositories with GraalVM core to run on top of it using Truffle and Graal compiler. These are:

Graal JavaScript - JavaScript (ECMAScript 2019 compatible) and Node.js 10.15.2
FastR - R Language 3.5.1
GraalPython - Python 3.7
TruffleRuby - Ruby Programming Language 2.6.2
SimpleLanguage - A simple demonstration language for the GraalVM.

License
Each GraalVM component is licensed:

Truffle and its dependency GraalVM SDK are licensed under the Universal Permissive License.
Tools project is licensed under the GPL 2 with Classpath exception.
TRegex project is licensed under the GPL 2 with Classpath exception.
The Graal compiler is licensed under the GPL 2 with Classpath exception.
Substrate VM is licensed under the GPL 2 with Classpath exception.
Sulong is licensed under 3-clause BSD.
VM is licensed under the GPL 2 with Classpath exception.

",8910
30,sphinx-gallery/sphinx-gallery,Python,"Sphinx-Gallery


A Sphinx extension that builds an HTML version of any Python
script and puts it into an examples gallery.


Who uses Sphinx-Gallery

Sphinx-Gallery
Scikit-learn
Nilearn
MNE-python
PyStruct
GIMLi
Nestle
pyRiemann
scikit-image
Astropy
SunPy
PySurfer
Matplotlib Examples and Tutorials
PyTorch tutorials
Cartopy


Installation

Install via pip
You can do a direct install via pip by using:
$ pip install sphinx-gallery
Sphinx-Gallery will not manage its dependencies when installing, thus
you are required to install them manually. Our minimal dependencies
are:

Sphinx >= 1.5 (1.8 recommended)
Matplotlib
Pillow

Sphinx-Gallery has also support for packages like:

Seaborn
Mayavi


Install as a developer
You can get the latest development source from our Github repository. You need
setuptools installed in your system to install Sphinx-Gallery.
Additional to the dependencies listed above you will need to install pytest
and pytest-coverage for testing.
To install everything do:
$ git clone https://github.com/sphinx-gallery/sphinx-gallery
$ cd sphinx-gallery
$ pip install -r requirements.txt
$ pip install pytest pytest-coverage
$ pip install -e .
In addition, you will need the following dependencies to build the
sphinx-gallery documentation:

Scipy
Seaborn

",147
31,artsy/palette,TypeScript,"@artsy/palette   
Artsy's Design System
Meta

Point People: @damassi, @zephraph, @l2succes

What is Palette?
Palette is a collection of primitive, product-agnostic elements that help encapsulate Artsy's look and feel at base level. This project is intended to be used
across our digital product portfolio.
Does my component belong in Palette?
If the component applies to Artsy as a brand and can/will be used across multiple digital products, then Palette is a great place for it. If it's highly product
specific then it's best to leave the component where it's used. We can always move things later!
If the above guidance still doesn't give you a good sense of what to do, please join the bi-weekly design systems sync.
How to contribute
If you'd like to add a new component to Palette please create an issue using the component spec template. That'll give both design and engineering a chance
to peek at the proposal and provide feedback before moving forward.
Local development
In the project root run the following:
$ yarn start
$ open http://localhost:8000/

Deployment process
Commits and Deployments
Palette uses auto-release to automatically release on every PR. Every PR should have a label that matches one of the following

Version: Trivial
Version: Patch
Version: Minor
Version: Major

Major, minor, and patch will cause a new release to be generated. Use major for breaking changes, minor for new non-breaking features,
and patch for bug fixes. Trivial will not cause a release and should be used when updating documentation or non-project code.
If you don't want to release on a particular PR but the changes aren't trivial then use the Skip Release tag along side the appropriate version tag.
",62
32,rakudo/rakudo,Perl 6,"Rakudo Perl 6
This is Rakudo Perl 6, a Perl 6 compiler for the MoarVM and JVM.
Rakudo Perl 6 is Copyright © 2008-2019, The Perl Foundation. Rakudo Perl 6
is distributed under the terms of the Artistic License 2.0. For more
details, see the full text of the license in the file LICENSE.
This directory contains only the Rakudo Perl 6 compiler itself; it
does not contain any of the modules, documentation, or other items
that would normally come with a full Perl 6 distribution.  If you're
after more than just the bare compiler, please download the latest
Rakudo Star package.
Note that different backends implement slightly different sets of
features. For a high-level overview of implemented and missing features,
please visit the features page on perl6.org.
Recent changes and feature additions are documented in the docs/ChangeLog
text file.
To receive important notifications from the core developer team, please
subscribe to the p6lert service using the RSS feed,
twitter, or the p6lert commandline script.
Building and Installing Rakudo
 
See the INSTALL.txt file for detailed prerequisites and build and
installation instructions.
The general process for building is running perl Configure.pl with
the desired configuration options (common options listed below), and
then running make or make install. Optionally, you may run
make spectest to test your build on Roast,
the Official Perl 6 test suite. To update the test suite, run
make spectest_update.
Installation of Rakudo simply requires building and running make install.
Note that this step is necessary for running Rakudo from outside the build
directory. But don't worry, it installs locally by default, so you don't need
any administrator privileges for carrying out this step.
Configuring Rakudo to run on MoarVM
To automatically download, build, and install a fresh MoarVM and NQP, run:
$ perl Configure.pl --gen-moar --gen-nqp --backends=moar

Please be aware, that this will install MoarVM and NQP into your given
--prefix before Configure.pl exits.
Alternatively, feel free to git clone https://github.com/perl6/nqp and
https://github.com/MoarVM/MoarVM manually and install them individually.
Configuration flags can be passed to MoarVM's Configure.pl using the
--moar-option flag. For example, if you wish to use Clang when GCC is the
default compiler selected for your OS, use the --compiler flag:
$ perl Configure.pl --gen-moar --moar-option='--compiler=clang' \
    --gen-nqp --backends=moar

If the compiler you want to use isn't known by MoarVM or you have multiple
versions of the same compiler installed, the --cc flag can be used to pass its
exact binary:
$ perl Configure.pl --gen-moar --moar-option='--cc=egcc' \
    --gen-nqp --backends=moar

Custom optimization and debugging levels may also be passed through:
$ perl Configure.pl --gen-moar --moar-option='--optimize=0 --debug=3' \
    --gen-nqp --backends=moar

For more information on how MoarVM can be configured, view MoarVM's
Configure.pl.
Configuring Rakudo to run on the JVM
Note that to run Rakudo on JVM, JDK 1.8 must be installed. To automatically
download, build, and install a fresh NQP, run:
$ perl Configure.pl --gen-nqp --backends=jvm

If you get a java.lang.OutOfMemoryError: Java heap space error building
rakudo on the JVM, you may need to modify your NQP runner to limit memory
use. e.g. edit the nqp-j / nqp-j.bat executable (found wherever you installed to, or in the
install/bin directory) to include -Xms500m -Xmx3g as options passed to java.
Alternatively, you can set JAVA_OPTS env var; e.g.
export JAVA_OPTS=""-Xmx51200000000""
Please be aware, that this will install NQP into your given --prefix
before Configure.pl exits.
Alternatively, feel free to git clone https://github.com/perl6/nqp manually
and install it individually.
Multiple backends at the same time
By supplying combinations of backends to the --backends flag, you
can get two or three backends built in the same prefix. The first
backend you supply in the list is the one that gets the perl6 name
as a symlink, and all backends are installed separately as
perl6-m or perl6-j for Rakudo on
MoarVM, or JVM respectively.
The format for the --backends flag is:
$ perl Configure.pl --backends=moar,jvm
$ perl Configure.pl --backends=ALL

Testing
Run the full spectest:
$ make spectest   # <== takes a LONG time!!

To run a single test, one must use make because of the tooling required to
run the spectests.  For example:
$ make t/spec/S12-traits/parameterized.t

Run all tests in one S* directory with a sh script. One example:
$ cat run-tests.sh
#!/bin/sh

# specify the desired directory:
D='t/spec/S26-documentation'

# collect the individual files
F=$(ls $D/*t)

# and run them
for f in $F
do
    echo ""Testing file '$f'""
    make $f
done
echo ""All tests in dir '$D' have been run.""

That can be written as a one-liner:
for f in $(ls t/spec/S26-documentation/*t); do make ""$f""; done

Where to get help or answers to questions
There are several mailing lists, IRC channels, and wikis available with
help for Perl 6 and Rakudo. Figuring out the right one to use
is often the biggest battle. Here are some rough guidelines:
The central hub for Perl 6 information is perl6.org.
This is always a good starting point.
If you have a question about Perl 6 syntax or the right way to approach
a problem using Perl 6, you probably want the “perl6-users@perl.org”
mailing list or the irc.freenode.net/#perl6 IRC
channel. The perl6-users
list is primarily for the people who want to use Perl 6 to write
programs, so newbie questions are welcomed there.  Newbie questions
are also welcome on the #perl6 channel; the Rakudo and Perl 6
development teams tend to hang out there and are generally glad
to help.  You can follow @perl6org
and on Twitter, there's a Perl 6 news aggregator at
Planet Perl 6.
Questions about NQP can also be posted to the #perl6 IRC channel.
For questions about MoarVM, you can join #moarvm on freenode.
Reporting bugs
See https://rakudo.org/bugs
Submitting patches
If you have a patch that fixes a bug or adds a new feature, please
create a pull request using github's pull request infrastructure.
See our contribution guidelines for more information.
Line editing and tab completion
If you would like simple history and tab completion in the perl6 executable,
you need to install the Linenoise module.  The recommended way to install
Linenoise is via zef:
$ zef install Linenoise

An alternative is to use a third-party program such as rlwrap.
AUTHOR
Jonathan Worthington is the current pumpking for Rakudo Perl 6.
See CREDITS for the many people that have contributed
to the development of the Rakudo compiler.
",1118
33,jdayllon/contratacionestado,None,"contratacionestado
El objetivo del proyecto es generar un dataset sobre contratación del sector público para facilitar la reutilización y el análisis de la información que genera este tipo de contratación
Sobre la información recogida


La información en el repositorio procede de las páginas web de los organismos:

Junta de Andalucía (http://www.juntadeandalucia.es)
Generalitat de Catalunya. (https://contractaciopublica.gencat.cat/)



Las transformaciones aplicadas buscan facilitar el proceso de la información o enriquecer la información ofrecida por las plataformas anteriores.


La estructura de los datos se está adaptando a un modelo inspirado en CODICE 2.1 / UBL
** Más información en: https://contrataciondelestado.es/wps/portal/codice


Propiedad Intelectual
Junta de Andalucía
Tal y como se cita en el aviso legal del Portal de la Junta de Andalucía:
La Junta de Andalucía promueve el libre uso y reutilización de los textos disponibles en el presente Portal sobre los que ostenta derechos de propiedad intelectual. Dichos textos están disponibles a través de una licencia-tipo Creative Commons Reconocimiento 3.0.
De modo general, la Junta de Andalucía te autoriza a:

Copiar, redistribuir y comunicar públicamente los textos del Portal.
Hacer un uso comercial de los contenidos.
Generar obras derivadas.

He impone las siguientes obligaciones:

Reconocer explícitamente la fuente de información Identificada en el presente texto.
Incluir la misma obligación de reconocimiento en los términos de licencia de cualquier producto derivado que haga uso de esta información. * Por lo que usuarios de esta información deben tener presente esta obligación *
No desnaturalizar el sentido de la información reproducida.
Evitar cualquier rasgo de presentación que sugiera que la Junta de Andalucía apoya o promueve el uso que se hace de la información difundida. En ningún caso está permitida la reproducción de logotipos, escudos, símbolos y marcas identificativas de la Junta de Andalucía sin autorización expresa de la institución.

Generalitat de Catalunya
TODO
",2
34,dotnet/docs.pl-pl,PowerShell,"


ms.openlocfilehash
ms.sourcegitcommit
ms.translationtype
ms.contentlocale
ms.lasthandoff
ms.locfileid




0c8a16936173b1e599d018d81432ca6b73c08e53
9b552addadfb57fab0b9e7852ed4f1f1b8a42f8e
MT
pl-PL
04/23/2019
61607352



.NET Docs
To repozytorium zawiera dokumentacji koncepcyjnego dla platformy .NET. Witrynie dokumentacji .NET składa się z wieloma repozytoriami, oprócz tego:

Przykłady kodu i fragmentów kodu
Dokumentacja interfejsu API
Odwołanie do zestawu SDK platformy kompilatora .NET

Zagadnienia i zadania dla wszystkich trzech tych repozytoriów są śledzone w tym miejscu. Mamy duże społeczności przy użyciu tych zasobów. Ułatwiamy naszych wszelkich starań, aby reagować na problemy w odpowiednim czasie. Możesz dowiedzieć się więcej o naszych procedury dotyczące klasyfikacji i rozwiązywanie problemów z naszych problemy z zasadami tematu.
Chętnie poznamy wkładów, aby pomóc nam w ulepszeniu ukończenia dokumentacji platformy .NET. Aby współtworzyć, zobacz projektów dla platformy .NET, uczestnicy społeczności zapoznać się z pomysłami. Współtworzenia przewodnik zawiera instrukcje na procedurach używamy. Lub sprawdź listę problemów zadań, które Cię interesują.
Przewidujemy, że Xamarin, Mono i Unity spowoduje również użycie tej dokumentacji.
Ten projekt przyjęła Kodeks postępowania definicją Przymierzem współautorem, aby wyjaśnić, oczekiwane zachowanie w naszej społeczności.
Aby uzyskać więcej informacji, zobacz .NET Foundation Kodeks postępowania.
",5
35,jcreinhold/synthtorch,Python,"synthtorch






This package contains deep neural network-based (pytorch) modules to synthesize magnetic resonance (MR) and computed
tomography (CT) brain images. Synthesis is the procedure of learning the transformation that takes a specific contrast image to another estimate contrast.
For example, given a set of T1-weighted (T1-w) and T2-weighted (T2-w) images, we can learn the function that maps the intensities of the
T1-w image to match that of the T2-w image via a UNet or other deep neural network architecture. In this package, we supply
the framework and several models for this type of synthesis. See the Relevant Papers section (at the bottom of
the README) for a non-exhaustive list of some papers relevant to the work in this package.
We also support a non-DNN-based synthesis package called synthit.
There is also a seperate package to gather quality metrics of the synthesis result called synthqc.
** Note that this is an alpha release. If you have feedback or problems, please submit an issue (it is very appreciated) **
This package was developed by Jacob Reinhold and the other students and researchers of the
Image Analysis and Communication Lab (IACL).
Link to main Gitlab Repository
Requirements

matplotlib
nibabel >= 2.3.1
niftidataset >= 0.1.4
numpy >= 1.15.4
pillow >= 5.3.0
torch >= 1.0.0
torchvision >= 0.2.1

Installation
pip install git+git://github.com/jcreinhold/synthtorch.git

Tutorial
5 minute Overview
Jupyter Notebook example
In addition to the above small tutorial and example notebook, there is consolidated documentation here.
Singularity
You can build a singularity image from the docker image hosted on dockerhub or through singularity-hub via the following command:
singularity pull shub://jcreinhold/synthtorch:latest

Test Package
Unit tests can be run from the main directory as follows:
nosetests -v tests

Citation
If you use the synthtorch package in an academic paper, please use the following citation:
@misc{reinhold2019,
    author       = {Jacob Reinhold},
    title        = {{synthtorch}},
    year         = 2019,
    doi          = {10.5281/zenodo.2669612},
    version      = {0.3.2},
    publisher    = {Zenodo},
    url          = {https://doi.org/10.5281/zenodo.2669612}
}

Relevant Papers
[1] C. Zhao, A. Carass, J. Lee, Y. He, and J. L. Prince, “Whole Brain Segmentation and Labeling from CT Using Synthetic MR Images,” in MICCAI MLMI, vol. 10541, pp. 291–298, 2017.
",6
36,rakudo/rakudo,Perl 6,"Rakudo Perl 6
This is Rakudo Perl 6, a Perl 6 compiler for the MoarVM and JVM.
Rakudo Perl 6 is Copyright © 2008-2019, The Perl Foundation. Rakudo Perl 6
is distributed under the terms of the Artistic License 2.0. For more
details, see the full text of the license in the file LICENSE.
This directory contains only the Rakudo Perl 6 compiler itself; it
does not contain any of the modules, documentation, or other items
that would normally come with a full Perl 6 distribution.  If you're
after more than just the bare compiler, please download the latest
Rakudo Star package.
Note that different backends implement slightly different sets of
features. For a high-level overview of implemented and missing features,
please visit the features page on perl6.org.
Recent changes and feature additions are documented in the docs/ChangeLog
text file.
To receive important notifications from the core developer team, please
subscribe to the p6lert service using the RSS feed,
twitter, or the p6lert commandline script.
Building and Installing Rakudo
 
See the INSTALL.txt file for detailed prerequisites and build and
installation instructions.
The general process for building is running perl Configure.pl with
the desired configuration options (common options listed below), and
then running make or make install. Optionally, you may run
make spectest to test your build on Roast,
the Official Perl 6 test suite. To update the test suite, run
make spectest_update.
Installation of Rakudo simply requires building and running make install.
Note that this step is necessary for running Rakudo from outside the build
directory. But don't worry, it installs locally by default, so you don't need
any administrator privileges for carrying out this step.
Configuring Rakudo to run on MoarVM
To automatically download, build, and install a fresh MoarVM and NQP, run:
$ perl Configure.pl --gen-moar --gen-nqp --backends=moar

Please be aware, that this will install MoarVM and NQP into your given
--prefix before Configure.pl exits.
Alternatively, feel free to git clone https://github.com/perl6/nqp and
https://github.com/MoarVM/MoarVM manually and install them individually.
Configuration flags can be passed to MoarVM's Configure.pl using the
--moar-option flag. For example, if you wish to use Clang when GCC is the
default compiler selected for your OS, use the --compiler flag:
$ perl Configure.pl --gen-moar --moar-option='--compiler=clang' \
    --gen-nqp --backends=moar

If the compiler you want to use isn't known by MoarVM or you have multiple
versions of the same compiler installed, the --cc flag can be used to pass its
exact binary:
$ perl Configure.pl --gen-moar --moar-option='--cc=egcc' \
    --gen-nqp --backends=moar

Custom optimization and debugging levels may also be passed through:
$ perl Configure.pl --gen-moar --moar-option='--optimize=0 --debug=3' \
    --gen-nqp --backends=moar

For more information on how MoarVM can be configured, view MoarVM's
Configure.pl.
Configuring Rakudo to run on the JVM
Note that to run Rakudo on JVM, JDK 1.8 must be installed. To automatically
download, build, and install a fresh NQP, run:
$ perl Configure.pl --gen-nqp --backends=jvm

If you get a java.lang.OutOfMemoryError: Java heap space error building
rakudo on the JVM, you may need to modify your NQP runner to limit memory
use. e.g. edit the nqp-j / nqp-j.bat executable (found wherever you installed to, or in the
install/bin directory) to include -Xms500m -Xmx3g as options passed to java.
Alternatively, you can set JAVA_OPTS env var; e.g.
export JAVA_OPTS=""-Xmx51200000000""
Please be aware, that this will install NQP into your given --prefix
before Configure.pl exits.
Alternatively, feel free to git clone https://github.com/perl6/nqp manually
and install it individually.
Multiple backends at the same time
By supplying combinations of backends to the --backends flag, you
can get two or three backends built in the same prefix. The first
backend you supply in the list is the one that gets the perl6 name
as a symlink, and all backends are installed separately as
perl6-m or perl6-j for Rakudo on
MoarVM, or JVM respectively.
The format for the --backends flag is:
$ perl Configure.pl --backends=moar,jvm
$ perl Configure.pl --backends=ALL

Testing
Run the full spectest:
$ make spectest   # <== takes a LONG time!!

To run a single test, one must use make because of the tooling required to
run the spectests.  For example:
$ make t/spec/S12-traits/parameterized.t

Run all tests in one S* directory with a sh script. One example:
$ cat run-tests.sh
#!/bin/sh

# specify the desired directory:
D='t/spec/S26-documentation'

# collect the individual files
F=$(ls $D/*t)

# and run them
for f in $F
do
    echo ""Testing file '$f'""
    make $f
done
echo ""All tests in dir '$D' have been run.""

That can be written as a one-liner:
for f in $(ls t/spec/S26-documentation/*t); do make ""$f""; done

Where to get help or answers to questions
There are several mailing lists, IRC channels, and wikis available with
help for Perl 6 and Rakudo. Figuring out the right one to use
is often the biggest battle. Here are some rough guidelines:
The central hub for Perl 6 information is perl6.org.
This is always a good starting point.
If you have a question about Perl 6 syntax or the right way to approach
a problem using Perl 6, you probably want the “perl6-users@perl.org”
mailing list or the irc.freenode.net/#perl6 IRC
channel. The perl6-users
list is primarily for the people who want to use Perl 6 to write
programs, so newbie questions are welcomed there.  Newbie questions
are also welcome on the #perl6 channel; the Rakudo and Perl 6
development teams tend to hang out there and are generally glad
to help.  You can follow @perl6org
and on Twitter, there's a Perl 6 news aggregator at
Planet Perl 6.
Questions about NQP can also be posted to the #perl6 IRC channel.
For questions about MoarVM, you can join #moarvm on freenode.
Reporting bugs
See https://rakudo.org/bugs
Submitting patches
If you have a patch that fixes a bug or adds a new feature, please
create a pull request using github's pull request infrastructure.
See our contribution guidelines for more information.
Line editing and tab completion
If you would like simple history and tab completion in the perl6 executable,
you need to install the Linenoise module.  The recommended way to install
Linenoise is via zef:
$ zef install Linenoise

An alternative is to use a third-party program such as rlwrap.
AUTHOR
Jonathan Worthington is the current pumpking for Rakudo Perl 6.
See CREDITS for the many people that have contributed
to the development of the Rakudo compiler.
",1118
37,jdayllon/contratacionestado,None,"contratacionestado
El objetivo del proyecto es generar un dataset sobre contratación del sector público para facilitar la reutilización y el análisis de la información que genera este tipo de contratación
Sobre la información recogida


La información en el repositorio procede de las páginas web de los organismos:

Junta de Andalucía (http://www.juntadeandalucia.es)
Generalitat de Catalunya. (https://contractaciopublica.gencat.cat/)



Las transformaciones aplicadas buscan facilitar el proceso de la información o enriquecer la información ofrecida por las plataformas anteriores.


La estructura de los datos se está adaptando a un modelo inspirado en CODICE 2.1 / UBL
** Más información en: https://contrataciondelestado.es/wps/portal/codice


Propiedad Intelectual
Junta de Andalucía
Tal y como se cita en el aviso legal del Portal de la Junta de Andalucía:
La Junta de Andalucía promueve el libre uso y reutilización de los textos disponibles en el presente Portal sobre los que ostenta derechos de propiedad intelectual. Dichos textos están disponibles a través de una licencia-tipo Creative Commons Reconocimiento 3.0.
De modo general, la Junta de Andalucía te autoriza a:

Copiar, redistribuir y comunicar públicamente los textos del Portal.
Hacer un uso comercial de los contenidos.
Generar obras derivadas.

He impone las siguientes obligaciones:

Reconocer explícitamente la fuente de información Identificada en el presente texto.
Incluir la misma obligación de reconocimiento en los términos de licencia de cualquier producto derivado que haga uso de esta información. * Por lo que usuarios de esta información deben tener presente esta obligación *
No desnaturalizar el sentido de la información reproducida.
Evitar cualquier rasgo de presentación que sugiera que la Junta de Andalucía apoya o promueve el uso que se hace de la información difundida. En ningún caso está permitida la reproducción de logotipos, escudos, símbolos y marcas identificativas de la Junta de Andalucía sin autorización expresa de la institución.

Generalitat de Catalunya
TODO
",2
38,dotnet/docs.pl-pl,PowerShell,"


ms.openlocfilehash
ms.sourcegitcommit
ms.translationtype
ms.contentlocale
ms.lasthandoff
ms.locfileid




0c8a16936173b1e599d018d81432ca6b73c08e53
9b552addadfb57fab0b9e7852ed4f1f1b8a42f8e
MT
pl-PL
04/23/2019
61607352



.NET Docs
To repozytorium zawiera dokumentacji koncepcyjnego dla platformy .NET. Witrynie dokumentacji .NET składa się z wieloma repozytoriami, oprócz tego:

Przykłady kodu i fragmentów kodu
Dokumentacja interfejsu API
Odwołanie do zestawu SDK platformy kompilatora .NET

Zagadnienia i zadania dla wszystkich trzech tych repozytoriów są śledzone w tym miejscu. Mamy duże społeczności przy użyciu tych zasobów. Ułatwiamy naszych wszelkich starań, aby reagować na problemy w odpowiednim czasie. Możesz dowiedzieć się więcej o naszych procedury dotyczące klasyfikacji i rozwiązywanie problemów z naszych problemy z zasadami tematu.
Chętnie poznamy wkładów, aby pomóc nam w ulepszeniu ukończenia dokumentacji platformy .NET. Aby współtworzyć, zobacz projektów dla platformy .NET, uczestnicy społeczności zapoznać się z pomysłami. Współtworzenia przewodnik zawiera instrukcje na procedurach używamy. Lub sprawdź listę problemów zadań, które Cię interesują.
Przewidujemy, że Xamarin, Mono i Unity spowoduje również użycie tej dokumentacji.
Ten projekt przyjęła Kodeks postępowania definicją Przymierzem współautorem, aby wyjaśnić, oczekiwane zachowanie w naszej społeczności.
Aby uzyskać więcej informacji, zobacz .NET Foundation Kodeks postępowania.
",5
39,jcreinhold/synthtorch,Python,"synthtorch






This package contains deep neural network-based (pytorch) modules to synthesize magnetic resonance (MR) and computed
tomography (CT) brain images. Synthesis is the procedure of learning the transformation that takes a specific contrast image to another estimate contrast.
For example, given a set of T1-weighted (T1-w) and T2-weighted (T2-w) images, we can learn the function that maps the intensities of the
T1-w image to match that of the T2-w image via a UNet or other deep neural network architecture. In this package, we supply
the framework and several models for this type of synthesis. See the Relevant Papers section (at the bottom of
the README) for a non-exhaustive list of some papers relevant to the work in this package.
We also support a non-DNN-based synthesis package called synthit.
There is also a seperate package to gather quality metrics of the synthesis result called synthqc.
** Note that this is an alpha release. If you have feedback or problems, please submit an issue (it is very appreciated) **
This package was developed by Jacob Reinhold and the other students and researchers of the
Image Analysis and Communication Lab (IACL).
Link to main Gitlab Repository
Requirements

matplotlib
nibabel >= 2.3.1
niftidataset >= 0.1.4
numpy >= 1.15.4
pillow >= 5.3.0
torch >= 1.0.0
torchvision >= 0.2.1

Installation
pip install git+git://github.com/jcreinhold/synthtorch.git

Tutorial
5 minute Overview
Jupyter Notebook example
In addition to the above small tutorial and example notebook, there is consolidated documentation here.
Singularity
You can build a singularity image from the docker image hosted on dockerhub or through singularity-hub via the following command:
singularity pull shub://jcreinhold/synthtorch:latest

Test Package
Unit tests can be run from the main directory as follows:
nosetests -v tests

Citation
If you use the synthtorch package in an academic paper, please use the following citation:
@misc{reinhold2019,
    author       = {Jacob Reinhold},
    title        = {{synthtorch}},
    year         = 2019,
    doi          = {10.5281/zenodo.2669612},
    version      = {0.3.2},
    publisher    = {Zenodo},
    url          = {https://doi.org/10.5281/zenodo.2669612}
}

Relevant Papers
[1] C. Zhao, A. Carass, J. Lee, Y. He, and J. L. Prince, “Whole Brain Segmentation and Labeling from CT Using Synthetic MR Images,” in MICCAI MLMI, vol. 10541, pp. 291–298, 2017.
",6
40,gocd/gocd,Java,"GoCD

This is the main repository for GoCD - a continuous delivery server. GoCD helps you automate and streamline the build-test-release cycle for worry-free, continuous delivery of your product.
To quickly build your first pipeline while learning key GoCD concepts, visit our Intro to GoCD guide.
Development Setup
This is a Java/JRuby on Rails project. Here is the guide to setup your development environment.
Contributing
We'd love it if you contributed to GoCD. For information on contributing to this project, please see our contributor's guide.
A lot of useful information like links to user documentation, design documentation, mailing lists etc. can be found in the resources section.
License
GoCD is an open source project, sponsored by ThoughtWorks Inc. under the Apache License, Version 2.0.
",4904
41,mulesoft/mule-esb-maven-tools,Java,"Mule ESB Maven Tools
The Mule ESB Maven Tools allow the development of Mule applications based on Maven tooling. This kit includes archetypes for building regular Mule applications, Mule domains and Mule domain bundles.
Maven Configuration
For it to work you need to add some entries to your settings.xml file.
First, add a new profile with the following repositories and pluginRepositories:
 <profiles>
     ...
     <profile>
        <id>mule-extra-repos</id>
        <activation>
            <activeByDefault>true</activeByDefault>
        </activation>
        <repositories>
            <repository>
                <id>mule-public</id>
                <url> https://repository.mulesoft.org/nexus/content/repositories/public </url>
            </repository>
        </repositories>
        <pluginRepositories>
            <pluginRepository>
                <id>mule-public</id>
                <url> https://repository.mulesoft.org/nexus/content/repositories/public </url>
            </pluginRepository>
        </pluginRepositories>
     </profile>
     ...
 </profiles>

Finally, as a pluginGroup you should add:
 <pluginGroups>
    ...
    <pluginGroup>org.mule.tools</pluginGroup>
    ...
 </pluginGroups>

Once you have your app or domain you can use -Dmule.home to specify the path to your Mule instance where they should be installed. Alternatively, $MULE_HOME will be used.
Creating a Mule Application
Creating a mule application using the mule archetype project is extremely easy. Just invoke it as follows:
 mvn archetype:generate -DarchetypeGroupId=org.mule.tools.maven -DarchetypeArtifactId=maven-archetype-mule-app \
-DarchetypeVersion=1.1 -DgroupId=org.mycompany.app -DartifactId=mule-app -Dversion=1.0-SNAPSHOT \
-DmuleVersion=3.7.0 -Dpackage=org.mycompany.app -Dtransports=http,jms,vm,file,ftp -Dmodules=db,xml,jersey,json,ws

In case you want your application to belong to a mule domain then you can add the domain specification parameters:
 mvn archetype:generate -DarchetypeGroupId=org.mule.tools.maven -DarchetypeArtifactId=maven-archetype-mule-app \
-DarchetypeVersion=1.1 -DgroupId=org.mycompany.app -DartifactId=mule-app -Dversion=1.0-SNAPSHOT \
-DmuleVersion=3.7.0 -Dpackage=org.mycompany.app -Dtransports=http,jms,vm,file,ftp -Dmodules=db,xml,jersey,json,ws \
-DdomainGroupId=org.mycompany.domain -DdomainArtifactId=mule-domain -DdomainVersion=1.0-SNAPSHOT

Archetype Parameters:



parameter
description
default




archetypeGroupId
The group Id of the archetype
This value must ALWAYS by org.mule.tools.maven


archetypeArtifactId
The artifact Id of the archetype
This value must ALWAYS mule-archetype-project


archetypeVersion
The version of the archetype. This value can change as we release new versions of the archetype. Always use the latest non-SNAPSHOT version available.



groupId
The group Id of the application you are creating. A good value would be the reverse name of your company domain name, like: com.mycompany.app or org.mycompany.app



artifactId
The artifact Id of the application you are creating.



version
The version of your application. Usually 1.0-SNAPSHOT.
1.0-SNAPSHOT


muleVersion
The version of the mule runtime you are going to use. Mule 2.2.x is no longer supported
3.7.0


transports
A comma separated list of the transport you are going to use within your application.
http,jms,vm,file,ftp


modules
A comma separated list of the modules you are going to use within your application.
db,xml,jersey,json,ws


EE
A flag to import the EE counterpart of the transports/modules you are using.
false


domainGroupId
The group Id of the domain that applications belongs to.
empty


domainArtifactId
The artifact Id of the domain that applications belongs to.
empty


domainVersion
The version of the domain that applications belongs to.
empty



Creating a Mule Domain
A mule application can belong to a domain group. The domain allows sharing of resources such as connectors or libraries between applications.
To create a domain execute:
 mvn archetype:generate -DarchetypeGroupId=org.mule.tools.maven -DarchetypeArtifactId=maven-archetype-mule-domain \
-DarchetypeVersion=1.1 -DgroupId=org.mycompany.domain -DartifactId=mule-domain -Dversion=1.0-SNAPSHOT \
-Dpackage=org.mycompany.domain

Archetype Parameters:



parameter
description
default




archetypeGroupId
The group Id of the archetype
This value must ALWAYS by org.mule.tools.maven


archetypeArtifactId
The artifact Id of the archetype
This value must ALWAYS by maven-archetype-mule-domain


archetypeVersion
The version of the archetype. This value can change as we release new versions of the archetype. Always use the latest non-SNAPSHOT version available.



groupId
The group Id of the domain you are creating. A good value would be the reverse name of your company domain name, like: com.mycompany.domain or org.mycompany.domain



artifactId
The artifact Id of the domain you are creating.



version
The version of your application. Usually 1.0-SNAPSHOT. Your domain name, when deployed to mule, will be artifactId-version
1.0-SNAPSHOT


EE
A flag to import the EE counterpart of the domain namespace.
false



Create a Complete Mule Domain Project
Several mule applications will make use of a particular mule domain for sharing resources. So it make perfect sense to aggregate all the applications and
the domain in a single project.
To create a maven project with the domain and all the applications that will be deployed using that domain you can use the mule domain bundle archetype:
 mvn archetype:generate -DarchetypeGroupId=org.mule.tools.maven -DarchetypeArtifactId=maven-archetype-mule-domain-bundle \
-DarchetypeVersion=1.1 -DgroupId=com.mycompany -DartifactId=mule-project -Dversion=1.0-SNAPSHOT \
-Dpackage=com.mycompany

This command will create a maven multi-module project with the following modules:

domain: This project is exactly as any project created with mule domain archetype. The artifact id for this project is ${artifactId}-domain. In this case would be mule-api-domain.
apps: This project is a bundle project for the mule applications that belong to this domain. Create here the mule applications using the mule applications archetype.
domain-bundle: This project is creates a bundle artifact with the domain plus the applications. This bundle project can be deployed as any domain and will also deploy the domain applications.

If you're using the EE distribution, you should add the EE flag:
 mvn archetype:generate -DarchetypeGroupId=org.mule.tools.maven -DarchetypeArtifactId=maven-archetype-mule-domain-bundle \
-DarchetypeVersion=1.1 -DgroupId=com.mycompany -DartifactId=mule-project -Dversion=1.0-SNAPSHOT \
-Dpackage=com.mycompany -DEE=true




parameter
description
default




archetypeGroupId
The group Id of the archetype
This value must ALWAYS by org.mule.tools.maven


archetypeArtifactId
The artifact Id of the archetype
This value must ALWAYS by maven-archetype-mule-domain-bundle


archetypeVersion
The version of the archetype. This value can change as we release new versions of the archetype. Always use the latest non-SNAPSHOT version available.



groupId
The group Id of the domain bundle project you are creating. A good value would be the reverse name of your company domain name, like: com.mycompany or org.mycompany



artifactId
The artifact Id of the domain bundle you are creating. Try to not include the domain word in it.



version
The version of your domain bundle. Usually 1.0-SNAPSHOT. Your domain name, when deployed to mule, will be artifactId-version
1.0-SNAPSHOT


package
Required by maven archetype but not used.



EE
A flag to import the EE counterpart of the domain namespace.
false



",11
42,fsprojects/Fabulous,F#,"F# Functional App Development, using Elmish and Xamarin.Forms
   
Never write a ViewModel class again! Conquer the world with clean dynamic UIs!
This library allows you to use the ultra-simple Model-View-Update architecture to build applications for iOS, Android, Mac, WPF and more. It is a variation of elmish, an Elm architecture implemented in F#. Elmish was originally written for Fable applications, however it is used here for applications using Xamarin.Forms.


Getting started


Documentation Guide


Roadmap


Contributor guide


Release Notes


Contributing
Please contribute to this library through issue reports, pull requests, code reviews and discussion.
Credits
This library is inspired by Elmish.WPF, Elmish.Forms and elmish, written by et1975. This project technically has no tie to Fable, which is an F# to JavaScript transpiler that is great.
",356
43,dotnet/docs.zh-tw,PowerShell,".NET Docs
此存放庫包含 .NET 的概念文件。 .NET 文件網站使用了多個存放庫作為建置基礎，但不包括：

程式碼範例及程式碼片段
API 參考
.NET Compiler Platform SDK 參考

這三個存放庫的問題及工作會在此處追蹤。 這些資源的使用者眾多。 我們會盡可能地即時回應各項問題。 請參閱我們的問題原則主題，了解更多有關於我們如何分類及解決問題的程序。
歡迎您參與協助我們改進及完成 .NET 文件。若要參與，請參閱 .NET 社群參與者專案進行了解。 參與指南包含所用程序的指示。 除此之外，您也可以查看問題清單，從中尋找您關注的工作。
我們期望 Xamarin、Mono 及 Unity 也能使用此文件。
此專案採納了 Contributor Covenant 定義的行為守則來規範社群中的行為。
如需詳細資訊，請參閱 .NET Foundation Code of Conduct (.NET Foundation 行為守則)。
",21
44,dotnet/docs.pl-pl,PowerShell,"


ms.openlocfilehash
ms.sourcegitcommit
ms.translationtype
ms.contentlocale
ms.lasthandoff
ms.locfileid




0c8a16936173b1e599d018d81432ca6b73c08e53
9b552addadfb57fab0b9e7852ed4f1f1b8a42f8e
MT
pl-PL
04/23/2019
61607352



.NET Docs
To repozytorium zawiera dokumentacji koncepcyjnego dla platformy .NET. Witrynie dokumentacji .NET składa się z wieloma repozytoriami, oprócz tego:

Przykłady kodu i fragmentów kodu
Dokumentacja interfejsu API
Odwołanie do zestawu SDK platformy kompilatora .NET

Zagadnienia i zadania dla wszystkich trzech tych repozytoriów są śledzone w tym miejscu. Mamy duże społeczności przy użyciu tych zasobów. Ułatwiamy naszych wszelkich starań, aby reagować na problemy w odpowiednim czasie. Możesz dowiedzieć się więcej o naszych procedury dotyczące klasyfikacji i rozwiązywanie problemów z naszych problemy z zasadami tematu.
Chętnie poznamy wkładów, aby pomóc nam w ulepszeniu ukończenia dokumentacji platformy .NET. Aby współtworzyć, zobacz projektów dla platformy .NET, uczestnicy społeczności zapoznać się z pomysłami. Współtworzenia przewodnik zawiera instrukcje na procedurach używamy. Lub sprawdź listę problemów zadań, które Cię interesują.
Przewidujemy, że Xamarin, Mono i Unity spowoduje również użycie tej dokumentacji.
Ten projekt przyjęła Kodeks postępowania definicją Przymierzem współautorem, aby wyjaśnić, oczekiwane zachowanie w naszej społeczności.
Aby uzyskać więcej informacji, zobacz .NET Foundation Kodeks postępowania.
",5
45,jcreinhold/synthtorch,Python,"synthtorch






This package contains deep neural network-based (pytorch) modules to synthesize magnetic resonance (MR) and computed
tomography (CT) brain images. Synthesis is the procedure of learning the transformation that takes a specific contrast image to another estimate contrast.
For example, given a set of T1-weighted (T1-w) and T2-weighted (T2-w) images, we can learn the function that maps the intensities of the
T1-w image to match that of the T2-w image via a UNet or other deep neural network architecture. In this package, we supply
the framework and several models for this type of synthesis. See the Relevant Papers section (at the bottom of
the README) for a non-exhaustive list of some papers relevant to the work in this package.
We also support a non-DNN-based synthesis package called synthit.
There is also a seperate package to gather quality metrics of the synthesis result called synthqc.
** Note that this is an alpha release. If you have feedback or problems, please submit an issue (it is very appreciated) **
This package was developed by Jacob Reinhold and the other students and researchers of the
Image Analysis and Communication Lab (IACL).
Link to main Gitlab Repository
Requirements

matplotlib
nibabel >= 2.3.1
niftidataset >= 0.1.4
numpy >= 1.15.4
pillow >= 5.3.0
torch >= 1.0.0
torchvision >= 0.2.1

Installation
pip install git+git://github.com/jcreinhold/synthtorch.git

Tutorial
5 minute Overview
Jupyter Notebook example
In addition to the above small tutorial and example notebook, there is consolidated documentation here.
Singularity
You can build a singularity image from the docker image hosted on dockerhub or through singularity-hub via the following command:
singularity pull shub://jcreinhold/synthtorch:latest

Test Package
Unit tests can be run from the main directory as follows:
nosetests -v tests

Citation
If you use the synthtorch package in an academic paper, please use the following citation:
@misc{reinhold2019,
    author       = {Jacob Reinhold},
    title        = {{synthtorch}},
    year         = 2019,
    doi          = {10.5281/zenodo.2669612},
    version      = {0.3.2},
    publisher    = {Zenodo},
    url          = {https://doi.org/10.5281/zenodo.2669612}
}

Relevant Papers
[1] C. Zhao, A. Carass, J. Lee, Y. He, and J. L. Prince, “Whole Brain Segmentation and Labeling from CT Using Synthetic MR Images,” in MICCAI MLMI, vol. 10541, pp. 291–298, 2017.
",6
46,gocd/gocd,Java,"GoCD

This is the main repository for GoCD - a continuous delivery server. GoCD helps you automate and streamline the build-test-release cycle for worry-free, continuous delivery of your product.
To quickly build your first pipeline while learning key GoCD concepts, visit our Intro to GoCD guide.
Development Setup
This is a Java/JRuby on Rails project. Here is the guide to setup your development environment.
Contributing
We'd love it if you contributed to GoCD. For information on contributing to this project, please see our contributor's guide.
A lot of useful information like links to user documentation, design documentation, mailing lists etc. can be found in the resources section.
License
GoCD is an open source project, sponsored by ThoughtWorks Inc. under the Apache License, Version 2.0.
",4904
47,mulesoft/mule-esb-maven-tools,Java,"Mule ESB Maven Tools
The Mule ESB Maven Tools allow the development of Mule applications based on Maven tooling. This kit includes archetypes for building regular Mule applications, Mule domains and Mule domain bundles.
Maven Configuration
For it to work you need to add some entries to your settings.xml file.
First, add a new profile with the following repositories and pluginRepositories:
 <profiles>
     ...
     <profile>
        <id>mule-extra-repos</id>
        <activation>
            <activeByDefault>true</activeByDefault>
        </activation>
        <repositories>
            <repository>
                <id>mule-public</id>
                <url> https://repository.mulesoft.org/nexus/content/repositories/public </url>
            </repository>
        </repositories>
        <pluginRepositories>
            <pluginRepository>
                <id>mule-public</id>
                <url> https://repository.mulesoft.org/nexus/content/repositories/public </url>
            </pluginRepository>
        </pluginRepositories>
     </profile>
     ...
 </profiles>

Finally, as a pluginGroup you should add:
 <pluginGroups>
    ...
    <pluginGroup>org.mule.tools</pluginGroup>
    ...
 </pluginGroups>

Once you have your app or domain you can use -Dmule.home to specify the path to your Mule instance where they should be installed. Alternatively, $MULE_HOME will be used.
Creating a Mule Application
Creating a mule application using the mule archetype project is extremely easy. Just invoke it as follows:
 mvn archetype:generate -DarchetypeGroupId=org.mule.tools.maven -DarchetypeArtifactId=maven-archetype-mule-app \
-DarchetypeVersion=1.1 -DgroupId=org.mycompany.app -DartifactId=mule-app -Dversion=1.0-SNAPSHOT \
-DmuleVersion=3.7.0 -Dpackage=org.mycompany.app -Dtransports=http,jms,vm,file,ftp -Dmodules=db,xml,jersey,json,ws

In case you want your application to belong to a mule domain then you can add the domain specification parameters:
 mvn archetype:generate -DarchetypeGroupId=org.mule.tools.maven -DarchetypeArtifactId=maven-archetype-mule-app \
-DarchetypeVersion=1.1 -DgroupId=org.mycompany.app -DartifactId=mule-app -Dversion=1.0-SNAPSHOT \
-DmuleVersion=3.7.0 -Dpackage=org.mycompany.app -Dtransports=http,jms,vm,file,ftp -Dmodules=db,xml,jersey,json,ws \
-DdomainGroupId=org.mycompany.domain -DdomainArtifactId=mule-domain -DdomainVersion=1.0-SNAPSHOT

Archetype Parameters:



parameter
description
default




archetypeGroupId
The group Id of the archetype
This value must ALWAYS by org.mule.tools.maven


archetypeArtifactId
The artifact Id of the archetype
This value must ALWAYS mule-archetype-project


archetypeVersion
The version of the archetype. This value can change as we release new versions of the archetype. Always use the latest non-SNAPSHOT version available.



groupId
The group Id of the application you are creating. A good value would be the reverse name of your company domain name, like: com.mycompany.app or org.mycompany.app



artifactId
The artifact Id of the application you are creating.



version
The version of your application. Usually 1.0-SNAPSHOT.
1.0-SNAPSHOT


muleVersion
The version of the mule runtime you are going to use. Mule 2.2.x is no longer supported
3.7.0


transports
A comma separated list of the transport you are going to use within your application.
http,jms,vm,file,ftp


modules
A comma separated list of the modules you are going to use within your application.
db,xml,jersey,json,ws


EE
A flag to import the EE counterpart of the transports/modules you are using.
false


domainGroupId
The group Id of the domain that applications belongs to.
empty


domainArtifactId
The artifact Id of the domain that applications belongs to.
empty


domainVersion
The version of the domain that applications belongs to.
empty



Creating a Mule Domain
A mule application can belong to a domain group. The domain allows sharing of resources such as connectors or libraries between applications.
To create a domain execute:
 mvn archetype:generate -DarchetypeGroupId=org.mule.tools.maven -DarchetypeArtifactId=maven-archetype-mule-domain \
-DarchetypeVersion=1.1 -DgroupId=org.mycompany.domain -DartifactId=mule-domain -Dversion=1.0-SNAPSHOT \
-Dpackage=org.mycompany.domain

Archetype Parameters:



parameter
description
default




archetypeGroupId
The group Id of the archetype
This value must ALWAYS by org.mule.tools.maven


archetypeArtifactId
The artifact Id of the archetype
This value must ALWAYS by maven-archetype-mule-domain


archetypeVersion
The version of the archetype. This value can change as we release new versions of the archetype. Always use the latest non-SNAPSHOT version available.



groupId
The group Id of the domain you are creating. A good value would be the reverse name of your company domain name, like: com.mycompany.domain or org.mycompany.domain



artifactId
The artifact Id of the domain you are creating.



version
The version of your application. Usually 1.0-SNAPSHOT. Your domain name, when deployed to mule, will be artifactId-version
1.0-SNAPSHOT


EE
A flag to import the EE counterpart of the domain namespace.
false



Create a Complete Mule Domain Project
Several mule applications will make use of a particular mule domain for sharing resources. So it make perfect sense to aggregate all the applications and
the domain in a single project.
To create a maven project with the domain and all the applications that will be deployed using that domain you can use the mule domain bundle archetype:
 mvn archetype:generate -DarchetypeGroupId=org.mule.tools.maven -DarchetypeArtifactId=maven-archetype-mule-domain-bundle \
-DarchetypeVersion=1.1 -DgroupId=com.mycompany -DartifactId=mule-project -Dversion=1.0-SNAPSHOT \
-Dpackage=com.mycompany

This command will create a maven multi-module project with the following modules:

domain: This project is exactly as any project created with mule domain archetype. The artifact id for this project is ${artifactId}-domain. In this case would be mule-api-domain.
apps: This project is a bundle project for the mule applications that belong to this domain. Create here the mule applications using the mule applications archetype.
domain-bundle: This project is creates a bundle artifact with the domain plus the applications. This bundle project can be deployed as any domain and will also deploy the domain applications.

If you're using the EE distribution, you should add the EE flag:
 mvn archetype:generate -DarchetypeGroupId=org.mule.tools.maven -DarchetypeArtifactId=maven-archetype-mule-domain-bundle \
-DarchetypeVersion=1.1 -DgroupId=com.mycompany -DartifactId=mule-project -Dversion=1.0-SNAPSHOT \
-Dpackage=com.mycompany -DEE=true




parameter
description
default




archetypeGroupId
The group Id of the archetype
This value must ALWAYS by org.mule.tools.maven


archetypeArtifactId
The artifact Id of the archetype
This value must ALWAYS by maven-archetype-mule-domain-bundle


archetypeVersion
The version of the archetype. This value can change as we release new versions of the archetype. Always use the latest non-SNAPSHOT version available.



groupId
The group Id of the domain bundle project you are creating. A good value would be the reverse name of your company domain name, like: com.mycompany or org.mycompany



artifactId
The artifact Id of the domain bundle you are creating. Try to not include the domain word in it.



version
The version of your domain bundle. Usually 1.0-SNAPSHOT. Your domain name, when deployed to mule, will be artifactId-version
1.0-SNAPSHOT


package
Required by maven archetype but not used.



EE
A flag to import the EE counterpart of the domain namespace.
false



",11
48,fsprojects/Fabulous,F#,"F# Functional App Development, using Elmish and Xamarin.Forms
   
Never write a ViewModel class again! Conquer the world with clean dynamic UIs!
This library allows you to use the ultra-simple Model-View-Update architecture to build applications for iOS, Android, Mac, WPF and more. It is a variation of elmish, an Elm architecture implemented in F#. Elmish was originally written for Fable applications, however it is used here for applications using Xamarin.Forms.


Getting started


Documentation Guide


Roadmap


Contributor guide


Release Notes


Contributing
Please contribute to this library through issue reports, pull requests, code reviews and discussion.
Credits
This library is inspired by Elmish.WPF, Elmish.Forms and elmish, written by et1975. This project technically has no tie to Fable, which is an F# to JavaScript transpiler that is great.
",356
49,dotnet/docs.zh-tw,PowerShell,".NET Docs
此存放庫包含 .NET 的概念文件。 .NET 文件網站使用了多個存放庫作為建置基礎，但不包括：

程式碼範例及程式碼片段
API 參考
.NET Compiler Platform SDK 參考

這三個存放庫的問題及工作會在此處追蹤。 這些資源的使用者眾多。 我們會盡可能地即時回應各項問題。 請參閱我們的問題原則主題，了解更多有關於我們如何分類及解決問題的程序。
歡迎您參與協助我們改進及完成 .NET 文件。若要參與，請參閱 .NET 社群參與者專案進行了解。 參與指南包含所用程序的指示。 除此之外，您也可以查看問題清單，從中尋找您關注的工作。
我們期望 Xamarin、Mono 及 Unity 也能使用此文件。
此專案採納了 Contributor Covenant 定義的行為守則來規範社群中的行為。
如需詳細資訊，請參閱 .NET Foundation Code of Conduct (.NET Foundation 行為守則)。
",21
50,litespeedtech/ls-cloud-image,Shell,"ls-cloud-image
Build Your Own Image
To build free, high-performance WordPress, Cyberpanel, etc. images:

Choose the Long Term Support version for the base image, e.g. Ubuntu 18.04. and launch it
Install setup script
Install a launch script
Build Image
Test functionality (Optional)

More details
Launch an Existing Image
Look here if you are interested in launching existing images from popular cloud platforms
",2
51,ConAntares/Algorithms,Python,"Algorithms







Repository for Algorithms.
Julia is the main Language.
Thank you for visiting.
My QQ Group: 436192270
",3
52,cloudfoundry/buildpacks-ci,HTML,"Introduction
This contains the configuration for the Cloud Foundry Buildpacks team Concourse deployment.
Pipelines

dependency-builds: build binaries for Cloud Foundry buildpacks
buildpacks: test and release all of the buildpacks
edge-shared: deploy CF Deployment environment
brats: run BRATS against the master branch of buildpacks
buildpack-verification: generate static site for buildpack verification
buildpacks-ci: testing tasks for correct usage, rebuild CI docker images
cf-release: deployment of latest buildpacks to
cf-release develop
gems-and-extensions: gems and extensions that support buildpack development and deployment
notifications: monitor upstream sources for
changes and notify on Slack
cflinuxfs2: test and release Cloud Foundry cflinuxfs2

Concourse State
Jobs and tasks in the buildpacks-ci repository store state in public-buildpacks-ci-robots. See repository README for details.
Commands and recipes
Updating all the Pipelines
./bin/update-pipelines
Debugging the build
fly intercept -j $JOB_NAME -t task -n $TASK_NAME
Clearing the git resources
fly intercept -c $RESOURCE_NAME rm -rf /tmp/git-resource-repo-cache
To build a new version of a binary


Check out the binary-builds branch


Edit the YAML file appropriate for the build (e.g. ruby-builds.yml)


Find the version number and package SHA256 of the new binary. For many binaries, the project website provides the SHA256 along with the release (for example, jruby.org/download provides the SHA256 along with each JRuby release). For others (such as Godep), you download the .tar.gz file and run shasum -a 256 <tar_file> to obtain the SHA256.


Add any number of versions and their checksums to the array, e.g.
ruby:
- version: 2.2.2
  sha256: 5ffc0f317e429e6b29d4a98ac521c3ce65481bfd22a8cf845fa02a7b113d9b44


git commit -am 'Build ruby 2.2.2' && git push


Build should automatically kick off at
https://buildpacks.ci.cf-app.com/pipelines/binary-builder and silently
upload a binary to the pivotal-buildpacks bucket under
dependencies/,
e.g. https://pivotal-buildpacks.s3.amazonaws.com/dependencies/ruby/ruby-2.2.2-linux-x64.tgz
Note that the array is a stack, which will be emptied as the build
succeeds in packaging successive versions.
Running the Test Suite
If you are running the full test suite, some of the integration tests are dependent on the Lastpass CLI and correctly targeting the fly CLI.
To login to the Lastpass CLI:
lpass login $USERNAME
You will then be prompted for your Lastpass password and Google Authenticator Code.
To login to the Fly CLI and target the buildpacks CI:
fly -t buildpacks login
You will be prompted to select either the Github or Basic Auth authentication methods.
After these are set up, you will be able to run the test suite via:
rspec
Making Changes to Build Scripts
When you want to change how a binary gets built, there are two places you may need to make changes. All binaries are built by the dependency-builds pipeline, and you may need to change the task that builds them. For many binaries, the dependency-builds pipeline runs recipes from the binary-builder repo; for those binaries, you will usually need to change the recipe rather than the concourse task.
For the list of currently supported binaries, check out our dependency-builds pipeline.
The concourse task that orchestrates the building is buildpacks-ci/tasks/build-binary-new/builder.rb; many of the recipes are in binary-builder.
To test these changes locally, you can execute the concourse task for it, but point to local changes. For instance:
$ cd buildpacks-ci
$ STACK=cflinuxfs2 fly -t buildpacks e -c tasks/build-binary-new/build.yml -j dependency-builds/build-r-3.4.X -i buildpacks-ci=.

For binaries that use recipes in binary-builder, you can also test in Docker. For instance:
$ docker run -w /binary-builder -v `pwd`:/binary-builder -it cloudfoundry/cflinuxfs2:ruby-2.2.4 ./bin/binary-builder --name=ruby --version=2.2.3 --md5=150a5efc5f5d8a8011f30aa2594a7654
$ ls
ruby-2.2.3-linux-x64.tgz

Buildpack Repositories Guide
buildpacks-ci pipelines and tasks refer to many other repositories. These repos are where the buildpack team and others develop buildpacks and related artifacts.
Officially-supported Buildpacks
Each officially-supported buildpack has a develop and a master branch.
Active development happens on develop. Despite our best efforts, develop will sometimes be unstable and is not production-ready.
Our release branch is master. This is stable and only updated with new buildpack releases.

binary-buildpack
go-buildpack
nodejs-buildpack
php-buildpack
python-buildpack
ruby-buildpack
dotnet-core-buildpack
staticfile-buildpack

Tooling for Development and Runtime

buildpack-packager   Builds cached and uncached buildpacks
machete           Buildpack integration testing framework.
compile-extensions Suite of utility scripts used in buildpacks at runtime
libbuildpack Library used for writing buildpacks in Golang
binary-builder           Builds binaries against specified rootfs
cflinuxfs2 Tooling to build cflinuxfs2 root file system (""rootfs"") for CF
brats Buildpack Runtime Acceptance Test Suite, a collection of smoke tests

BOSH Releases
BOSH releases are used in the assembly of cf-release.

cflinuxfs2-release
go-buildpack-release
ruby-buildpack-release
python-buildpack-release
php-buildpack-release
nodejs-buildpack-release
staticfile-buildpack-release
binary-buildpack-release
java-offline-buildpack-release
java-buildpack-release
dotnet-core-buildpack-release

Experimental or unsupported
Buildpacks
These buildpacks are possible candidates for promotion, or experimental architecture explorations.

hwc-buildpack
hwc-buildpack-release

Tools

concourse-filter Redacts credentials from Concourse logs
new_version_resource Concourse resource to track dependency versions by scraping webpages

Private Repos
Some repositories are private for historical or security reasons. We list them for completeness.

deployments-buildpacks See repository README.
buildpacks-ci-robots See repository README.
cflinuxfs2-nc See repository README.
cflinuxfs2-nc-release See repository README.

",33
53,jcreinhold/synthtorch,Python,"synthtorch






This package contains deep neural network-based (pytorch) modules to synthesize magnetic resonance (MR) and computed
tomography (CT) brain images. Synthesis is the procedure of learning the transformation that takes a specific contrast image to another estimate contrast.
For example, given a set of T1-weighted (T1-w) and T2-weighted (T2-w) images, we can learn the function that maps the intensities of the
T1-w image to match that of the T2-w image via a UNet or other deep neural network architecture. In this package, we supply
the framework and several models for this type of synthesis. See the Relevant Papers section (at the bottom of
the README) for a non-exhaustive list of some papers relevant to the work in this package.
We also support a non-DNN-based synthesis package called synthit.
There is also a seperate package to gather quality metrics of the synthesis result called synthqc.
** Note that this is an alpha release. If you have feedback or problems, please submit an issue (it is very appreciated) **
This package was developed by Jacob Reinhold and the other students and researchers of the
Image Analysis and Communication Lab (IACL).
Link to main Gitlab Repository
Requirements

matplotlib
nibabel >= 2.3.1
niftidataset >= 0.1.4
numpy >= 1.15.4
pillow >= 5.3.0
torch >= 1.0.0
torchvision >= 0.2.1

Installation
pip install git+git://github.com/jcreinhold/synthtorch.git

Tutorial
5 minute Overview
Jupyter Notebook example
In addition to the above small tutorial and example notebook, there is consolidated documentation here.
Singularity
You can build a singularity image from the docker image hosted on dockerhub or through singularity-hub via the following command:
singularity pull shub://jcreinhold/synthtorch:latest

Test Package
Unit tests can be run from the main directory as follows:
nosetests -v tests

Citation
If you use the synthtorch package in an academic paper, please use the following citation:
@misc{reinhold2019,
    author       = {Jacob Reinhold},
    title        = {{synthtorch}},
    year         = 2019,
    doi          = {10.5281/zenodo.2669612},
    version      = {0.3.2},
    publisher    = {Zenodo},
    url          = {https://doi.org/10.5281/zenodo.2669612}
}

Relevant Papers
[1] C. Zhao, A. Carass, J. Lee, Y. He, and J. L. Prince, “Whole Brain Segmentation and Labeling from CT Using Synthetic MR Images,” in MICCAI MLMI, vol. 10541, pp. 291–298, 2017.
",6
54,gocd/gocd,Java,"GoCD

This is the main repository for GoCD - a continuous delivery server. GoCD helps you automate and streamline the build-test-release cycle for worry-free, continuous delivery of your product.
To quickly build your first pipeline while learning key GoCD concepts, visit our Intro to GoCD guide.
Development Setup
This is a Java/JRuby on Rails project. Here is the guide to setup your development environment.
Contributing
We'd love it if you contributed to GoCD. For information on contributing to this project, please see our contributor's guide.
A lot of useful information like links to user documentation, design documentation, mailing lists etc. can be found in the resources section.
License
GoCD is an open source project, sponsored by ThoughtWorks Inc. under the Apache License, Version 2.0.
",4904
55,mulesoft/mule-esb-maven-tools,Java,"Mule ESB Maven Tools
The Mule ESB Maven Tools allow the development of Mule applications based on Maven tooling. This kit includes archetypes for building regular Mule applications, Mule domains and Mule domain bundles.
Maven Configuration
For it to work you need to add some entries to your settings.xml file.
First, add a new profile with the following repositories and pluginRepositories:
 <profiles>
     ...
     <profile>
        <id>mule-extra-repos</id>
        <activation>
            <activeByDefault>true</activeByDefault>
        </activation>
        <repositories>
            <repository>
                <id>mule-public</id>
                <url> https://repository.mulesoft.org/nexus/content/repositories/public </url>
            </repository>
        </repositories>
        <pluginRepositories>
            <pluginRepository>
                <id>mule-public</id>
                <url> https://repository.mulesoft.org/nexus/content/repositories/public </url>
            </pluginRepository>
        </pluginRepositories>
     </profile>
     ...
 </profiles>

Finally, as a pluginGroup you should add:
 <pluginGroups>
    ...
    <pluginGroup>org.mule.tools</pluginGroup>
    ...
 </pluginGroups>

Once you have your app or domain you can use -Dmule.home to specify the path to your Mule instance where they should be installed. Alternatively, $MULE_HOME will be used.
Creating a Mule Application
Creating a mule application using the mule archetype project is extremely easy. Just invoke it as follows:
 mvn archetype:generate -DarchetypeGroupId=org.mule.tools.maven -DarchetypeArtifactId=maven-archetype-mule-app \
-DarchetypeVersion=1.1 -DgroupId=org.mycompany.app -DartifactId=mule-app -Dversion=1.0-SNAPSHOT \
-DmuleVersion=3.7.0 -Dpackage=org.mycompany.app -Dtransports=http,jms,vm,file,ftp -Dmodules=db,xml,jersey,json,ws

In case you want your application to belong to a mule domain then you can add the domain specification parameters:
 mvn archetype:generate -DarchetypeGroupId=org.mule.tools.maven -DarchetypeArtifactId=maven-archetype-mule-app \
-DarchetypeVersion=1.1 -DgroupId=org.mycompany.app -DartifactId=mule-app -Dversion=1.0-SNAPSHOT \
-DmuleVersion=3.7.0 -Dpackage=org.mycompany.app -Dtransports=http,jms,vm,file,ftp -Dmodules=db,xml,jersey,json,ws \
-DdomainGroupId=org.mycompany.domain -DdomainArtifactId=mule-domain -DdomainVersion=1.0-SNAPSHOT

Archetype Parameters:



parameter
description
default




archetypeGroupId
The group Id of the archetype
This value must ALWAYS by org.mule.tools.maven


archetypeArtifactId
The artifact Id of the archetype
This value must ALWAYS mule-archetype-project


archetypeVersion
The version of the archetype. This value can change as we release new versions of the archetype. Always use the latest non-SNAPSHOT version available.



groupId
The group Id of the application you are creating. A good value would be the reverse name of your company domain name, like: com.mycompany.app or org.mycompany.app



artifactId
The artifact Id of the application you are creating.



version
The version of your application. Usually 1.0-SNAPSHOT.
1.0-SNAPSHOT


muleVersion
The version of the mule runtime you are going to use. Mule 2.2.x is no longer supported
3.7.0


transports
A comma separated list of the transport you are going to use within your application.
http,jms,vm,file,ftp


modules
A comma separated list of the modules you are going to use within your application.
db,xml,jersey,json,ws


EE
A flag to import the EE counterpart of the transports/modules you are using.
false


domainGroupId
The group Id of the domain that applications belongs to.
empty


domainArtifactId
The artifact Id of the domain that applications belongs to.
empty


domainVersion
The version of the domain that applications belongs to.
empty



Creating a Mule Domain
A mule application can belong to a domain group. The domain allows sharing of resources such as connectors or libraries between applications.
To create a domain execute:
 mvn archetype:generate -DarchetypeGroupId=org.mule.tools.maven -DarchetypeArtifactId=maven-archetype-mule-domain \
-DarchetypeVersion=1.1 -DgroupId=org.mycompany.domain -DartifactId=mule-domain -Dversion=1.0-SNAPSHOT \
-Dpackage=org.mycompany.domain

Archetype Parameters:



parameter
description
default




archetypeGroupId
The group Id of the archetype
This value must ALWAYS by org.mule.tools.maven


archetypeArtifactId
The artifact Id of the archetype
This value must ALWAYS by maven-archetype-mule-domain


archetypeVersion
The version of the archetype. This value can change as we release new versions of the archetype. Always use the latest non-SNAPSHOT version available.



groupId
The group Id of the domain you are creating. A good value would be the reverse name of your company domain name, like: com.mycompany.domain or org.mycompany.domain



artifactId
The artifact Id of the domain you are creating.



version
The version of your application. Usually 1.0-SNAPSHOT. Your domain name, when deployed to mule, will be artifactId-version
1.0-SNAPSHOT


EE
A flag to import the EE counterpart of the domain namespace.
false



Create a Complete Mule Domain Project
Several mule applications will make use of a particular mule domain for sharing resources. So it make perfect sense to aggregate all the applications and
the domain in a single project.
To create a maven project with the domain and all the applications that will be deployed using that domain you can use the mule domain bundle archetype:
 mvn archetype:generate -DarchetypeGroupId=org.mule.tools.maven -DarchetypeArtifactId=maven-archetype-mule-domain-bundle \
-DarchetypeVersion=1.1 -DgroupId=com.mycompany -DartifactId=mule-project -Dversion=1.0-SNAPSHOT \
-Dpackage=com.mycompany

This command will create a maven multi-module project with the following modules:

domain: This project is exactly as any project created with mule domain archetype. The artifact id for this project is ${artifactId}-domain. In this case would be mule-api-domain.
apps: This project is a bundle project for the mule applications that belong to this domain. Create here the mule applications using the mule applications archetype.
domain-bundle: This project is creates a bundle artifact with the domain plus the applications. This bundle project can be deployed as any domain and will also deploy the domain applications.

If you're using the EE distribution, you should add the EE flag:
 mvn archetype:generate -DarchetypeGroupId=org.mule.tools.maven -DarchetypeArtifactId=maven-archetype-mule-domain-bundle \
-DarchetypeVersion=1.1 -DgroupId=com.mycompany -DartifactId=mule-project -Dversion=1.0-SNAPSHOT \
-Dpackage=com.mycompany -DEE=true




parameter
description
default




archetypeGroupId
The group Id of the archetype
This value must ALWAYS by org.mule.tools.maven


archetypeArtifactId
The artifact Id of the archetype
This value must ALWAYS by maven-archetype-mule-domain-bundle


archetypeVersion
The version of the archetype. This value can change as we release new versions of the archetype. Always use the latest non-SNAPSHOT version available.



groupId
The group Id of the domain bundle project you are creating. A good value would be the reverse name of your company domain name, like: com.mycompany or org.mycompany



artifactId
The artifact Id of the domain bundle you are creating. Try to not include the domain word in it.



version
The version of your domain bundle. Usually 1.0-SNAPSHOT. Your domain name, when deployed to mule, will be artifactId-version
1.0-SNAPSHOT


package
Required by maven archetype but not used.



EE
A flag to import the EE counterpart of the domain namespace.
false



",11
56,fsprojects/Fabulous,F#,"F# Functional App Development, using Elmish and Xamarin.Forms
   
Never write a ViewModel class again! Conquer the world with clean dynamic UIs!
This library allows you to use the ultra-simple Model-View-Update architecture to build applications for iOS, Android, Mac, WPF and more. It is a variation of elmish, an Elm architecture implemented in F#. Elmish was originally written for Fable applications, however it is used here for applications using Xamarin.Forms.


Getting started


Documentation Guide


Roadmap


Contributor guide


Release Notes


Contributing
Please contribute to this library through issue reports, pull requests, code reviews and discussion.
Credits
This library is inspired by Elmish.WPF, Elmish.Forms and elmish, written by et1975. This project technically has no tie to Fable, which is an F# to JavaScript transpiler that is great.
",356
57,dotnet/docs.zh-tw,PowerShell,".NET Docs
此存放庫包含 .NET 的概念文件。 .NET 文件網站使用了多個存放庫作為建置基礎，但不包括：

程式碼範例及程式碼片段
API 參考
.NET Compiler Platform SDK 參考

這三個存放庫的問題及工作會在此處追蹤。 這些資源的使用者眾多。 我們會盡可能地即時回應各項問題。 請參閱我們的問題原則主題，了解更多有關於我們如何分類及解決問題的程序。
歡迎您參與協助我們改進及完成 .NET 文件。若要參與，請參閱 .NET 社群參與者專案進行了解。 參與指南包含所用程序的指示。 除此之外，您也可以查看問題清單，從中尋找您關注的工作。
我們期望 Xamarin、Mono 及 Unity 也能使用此文件。
此專案採納了 Contributor Covenant 定義的行為守則來規範社群中的行為。
如需詳細資訊，請參閱 .NET Foundation Code of Conduct (.NET Foundation 行為守則)。
",21
58,litespeedtech/ls-cloud-image,Shell,"ls-cloud-image
Build Your Own Image
To build free, high-performance WordPress, Cyberpanel, etc. images:

Choose the Long Term Support version for the base image, e.g. Ubuntu 18.04. and launch it
Install setup script
Install a launch script
Build Image
Test functionality (Optional)

More details
Launch an Existing Image
Look here if you are interested in launching existing images from popular cloud platforms
",2
59,ConAntares/Algorithms,Python,"Algorithms







Repository for Algorithms.
Julia is the main Language.
Thank you for visiting.
My QQ Group: 436192270
",3
60,cloudfoundry/buildpacks-ci,HTML,"Introduction
This contains the configuration for the Cloud Foundry Buildpacks team Concourse deployment.
Pipelines

dependency-builds: build binaries for Cloud Foundry buildpacks
buildpacks: test and release all of the buildpacks
edge-shared: deploy CF Deployment environment
brats: run BRATS against the master branch of buildpacks
buildpack-verification: generate static site for buildpack verification
buildpacks-ci: testing tasks for correct usage, rebuild CI docker images
cf-release: deployment of latest buildpacks to
cf-release develop
gems-and-extensions: gems and extensions that support buildpack development and deployment
notifications: monitor upstream sources for
changes and notify on Slack
cflinuxfs2: test and release Cloud Foundry cflinuxfs2

Concourse State
Jobs and tasks in the buildpacks-ci repository store state in public-buildpacks-ci-robots. See repository README for details.
Commands and recipes
Updating all the Pipelines
./bin/update-pipelines
Debugging the build
fly intercept -j $JOB_NAME -t task -n $TASK_NAME
Clearing the git resources
fly intercept -c $RESOURCE_NAME rm -rf /tmp/git-resource-repo-cache
To build a new version of a binary


Check out the binary-builds branch


Edit the YAML file appropriate for the build (e.g. ruby-builds.yml)


Find the version number and package SHA256 of the new binary. For many binaries, the project website provides the SHA256 along with the release (for example, jruby.org/download provides the SHA256 along with each JRuby release). For others (such as Godep), you download the .tar.gz file and run shasum -a 256 <tar_file> to obtain the SHA256.


Add any number of versions and their checksums to the array, e.g.
ruby:
- version: 2.2.2
  sha256: 5ffc0f317e429e6b29d4a98ac521c3ce65481bfd22a8cf845fa02a7b113d9b44


git commit -am 'Build ruby 2.2.2' && git push


Build should automatically kick off at
https://buildpacks.ci.cf-app.com/pipelines/binary-builder and silently
upload a binary to the pivotal-buildpacks bucket under
dependencies/,
e.g. https://pivotal-buildpacks.s3.amazonaws.com/dependencies/ruby/ruby-2.2.2-linux-x64.tgz
Note that the array is a stack, which will be emptied as the build
succeeds in packaging successive versions.
Running the Test Suite
If you are running the full test suite, some of the integration tests are dependent on the Lastpass CLI and correctly targeting the fly CLI.
To login to the Lastpass CLI:
lpass login $USERNAME
You will then be prompted for your Lastpass password and Google Authenticator Code.
To login to the Fly CLI and target the buildpacks CI:
fly -t buildpacks login
You will be prompted to select either the Github or Basic Auth authentication methods.
After these are set up, you will be able to run the test suite via:
rspec
Making Changes to Build Scripts
When you want to change how a binary gets built, there are two places you may need to make changes. All binaries are built by the dependency-builds pipeline, and you may need to change the task that builds them. For many binaries, the dependency-builds pipeline runs recipes from the binary-builder repo; for those binaries, you will usually need to change the recipe rather than the concourse task.
For the list of currently supported binaries, check out our dependency-builds pipeline.
The concourse task that orchestrates the building is buildpacks-ci/tasks/build-binary-new/builder.rb; many of the recipes are in binary-builder.
To test these changes locally, you can execute the concourse task for it, but point to local changes. For instance:
$ cd buildpacks-ci
$ STACK=cflinuxfs2 fly -t buildpacks e -c tasks/build-binary-new/build.yml -j dependency-builds/build-r-3.4.X -i buildpacks-ci=.

For binaries that use recipes in binary-builder, you can also test in Docker. For instance:
$ docker run -w /binary-builder -v `pwd`:/binary-builder -it cloudfoundry/cflinuxfs2:ruby-2.2.4 ./bin/binary-builder --name=ruby --version=2.2.3 --md5=150a5efc5f5d8a8011f30aa2594a7654
$ ls
ruby-2.2.3-linux-x64.tgz

Buildpack Repositories Guide
buildpacks-ci pipelines and tasks refer to many other repositories. These repos are where the buildpack team and others develop buildpacks and related artifacts.
Officially-supported Buildpacks
Each officially-supported buildpack has a develop and a master branch.
Active development happens on develop. Despite our best efforts, develop will sometimes be unstable and is not production-ready.
Our release branch is master. This is stable and only updated with new buildpack releases.

binary-buildpack
go-buildpack
nodejs-buildpack
php-buildpack
python-buildpack
ruby-buildpack
dotnet-core-buildpack
staticfile-buildpack

Tooling for Development and Runtime

buildpack-packager   Builds cached and uncached buildpacks
machete           Buildpack integration testing framework.
compile-extensions Suite of utility scripts used in buildpacks at runtime
libbuildpack Library used for writing buildpacks in Golang
binary-builder           Builds binaries against specified rootfs
cflinuxfs2 Tooling to build cflinuxfs2 root file system (""rootfs"") for CF
brats Buildpack Runtime Acceptance Test Suite, a collection of smoke tests

BOSH Releases
BOSH releases are used in the assembly of cf-release.

cflinuxfs2-release
go-buildpack-release
ruby-buildpack-release
python-buildpack-release
php-buildpack-release
nodejs-buildpack-release
staticfile-buildpack-release
binary-buildpack-release
java-offline-buildpack-release
java-buildpack-release
dotnet-core-buildpack-release

Experimental or unsupported
Buildpacks
These buildpacks are possible candidates for promotion, or experimental architecture explorations.

hwc-buildpack
hwc-buildpack-release

Tools

concourse-filter Redacts credentials from Concourse logs
new_version_resource Concourse resource to track dependency versions by scraping webpages

Private Repos
Some repositories are private for historical or security reasons. We list them for completeness.

deployments-buildpacks See repository README.
buildpacks-ci-robots See repository README.
cflinuxfs2-nc See repository README.
cflinuxfs2-nc-release See repository README.

",33
61,Xeverous/filter_spirit,C++,"Filter Spirit
Advanced item filter generator for Path of Exile that uses it's own DSL and online item price APIs. Basically a separate pseudo-programming language that lets you write item filters using item price data from poe.ninja or poe.watch available at generation time. Create multiple filter variants and refresh whenever you want to always be up to date with market prices.
The project is under construction, message /u/Xeverous on reddit or Xeverous#2151 on Discord if you are interested in it's early development or have any questions/suggestions/whatever. You can also open an issue.
documentation and tutorial
Browse doc directory. Files are in Markdown so you can read them online on GitHub.
Note that the tool is in its early development so things may chage. Suggestions welcome.
overview
Core features:

static typing safety
generation-time error checking
language elements dedicated to elimination of code duplication:

ability to define constants: const red = RGB(255, 0, 0)
statements work top-down like in regular filters but they can be nested to inherit conditions and override actions


dynamic rules: BaseType $divination(10, 100) pulls data from API to list div cards worth 10-100c at generation time, refresh whenever you want - your filter is always up-to-date with market prices
support for filter variants (eg color-blind, strict, uber-strict, Animate Weapon support etc) - generate multiple filters with different flavours from a single source file

There are 2 intended usage scenarios:

Someone writes a filter using Filer Spirit templates and publishes generated variants that are ready to be immediately loaded by the game client.
Someone writes a filter template and shares it to other people who have downloaded Filter Spirit - they can:

(re)generate actual filter any time, making it always up to date with market prices
edit the config to tweak generated variants
edit the source to change styles (eg colors, sounds)
edit the source to change filter structure



example code
There is a full example filter template source in the repository although the syntax is a subject to change.
const color_white  = RGB(255, 255, 255, 255)
const color_hammer = RGB(162,  85,   0) # (default opacity)

BaseType [""Gavel"", ""Stone Hammer"", ""Rock Breaker""] {
	SetTextColor color_white
	SetBackgroundColor color_hammer
	# above BaseType conditon will be inherited by all nested blocks
	# above actions will be inherited and can be overriden by nested blocks

	Rarity normal {
		Show # show normal hammers
	}

	Rarity magic
	Quality >= 12 {
		Show # show magic hammers with 12+ quality
	}

	Rarity rare
	Quality >= 16 {
		Show # show rare hammers with 16+ quality
	}

	# hide any other hammers
	# actually discouraged as you might lose RGB recipe material for later blocks
	Hide
}

Class ""Divination Card"" {
	SetTextColor color_divination

	# cards that you always want to pickup
	BaseType [""The Void"", ""The Cartographer"", ""Chaotic Disposition""] {
		SetAlertSound alert_divination_stack_1
		SetBeam beam_divination
		Show
	}

	# 100c+
	BaseType $divination(100, 999999) {
		SetTextColor color_black
		SetBorderColor color_divination
		SetBackgroundColor color_divination
		SetFontSize font_max
		SetAlertSound alert_divination_best
		SetBeam beam_divination
		SetMinimapIcon MinimapIcon(0, blue, square)
		Show
	}

	# 10 - 100c
	BaseType $divination(10, 100) {
		SetBorderColor color_divination
		SetFontSize font_mid_divinaton
		SetAlertSound alert_divination_mid
		SetBeam beam_divination
		SetMinimapIcon MinimapIcon(1, blue, square)
		Show
	}

	# if you really hate these cards
	BaseType [""Rain of Chaos"", ""Carrion Crow""] {
		Hide
	}
}

build dependencies

C++17 compiler (<filesystem> not required)
Boost 1.70 OR older with Spirit headers updated to 1.70; this project uses:

spirit (using bleeding-edge X3 parser)
fusion
optional
variant
program_options
date_time
beast
asio
system
unit_test_framework (only if you build tests)


nlohmann/json
OpenSSL (preferably 1.1)

bolded dependencies require linking, all dependencies are exposed as targets in CMake script
building
standard CMake based build
mkdir build
cd build
cmake .. -DCMAKE_BUILD_TYPE=Release [your_options...]

If you are using make don't forget to add -j (parallel jobs) to add 100% increased build speed per additional core.
licensing
LICENSE file in the main directory of the repository applies to any file, unless otherwise specified.
Contact me if you are interested in work on or using this project but need or require a more permissive license.
",2
62,dotnet/docs.zh-tw,PowerShell,".NET Docs
此存放庫包含 .NET 的概念文件。 .NET 文件網站使用了多個存放庫作為建置基礎，但不包括：

程式碼範例及程式碼片段
API 參考
.NET Compiler Platform SDK 參考

這三個存放庫的問題及工作會在此處追蹤。 這些資源的使用者眾多。 我們會盡可能地即時回應各項問題。 請參閱我們的問題原則主題，了解更多有關於我們如何分類及解決問題的程序。
歡迎您參與協助我們改進及完成 .NET 文件。若要參與，請參閱 .NET 社群參與者專案進行了解。 參與指南包含所用程序的指示。 除此之外，您也可以查看問題清單，從中尋找您關注的工作。
我們期望 Xamarin、Mono 及 Unity 也能使用此文件。
此專案採納了 Contributor Covenant 定義的行為守則來規範社群中的行為。
如需詳細資訊，請參閱 .NET Foundation Code of Conduct (.NET Foundation 行為守則)。
",21
63,litespeedtech/ls-cloud-image,Shell,"ls-cloud-image
Build Your Own Image
To build free, high-performance WordPress, Cyberpanel, etc. images:

Choose the Long Term Support version for the base image, e.g. Ubuntu 18.04. and launch it
Install setup script
Install a launch script
Build Image
Test functionality (Optional)

More details
Launch an Existing Image
Look here if you are interested in launching existing images from popular cloud platforms
",2
64,ConAntares/Algorithms,Python,"Algorithms







Repository for Algorithms.
Julia is the main Language.
Thank you for visiting.
My QQ Group: 436192270
",3
65,cloudfoundry/buildpacks-ci,HTML,"Introduction
This contains the configuration for the Cloud Foundry Buildpacks team Concourse deployment.
Pipelines

dependency-builds: build binaries for Cloud Foundry buildpacks
buildpacks: test and release all of the buildpacks
edge-shared: deploy CF Deployment environment
brats: run BRATS against the master branch of buildpacks
buildpack-verification: generate static site for buildpack verification
buildpacks-ci: testing tasks for correct usage, rebuild CI docker images
cf-release: deployment of latest buildpacks to
cf-release develop
gems-and-extensions: gems and extensions that support buildpack development and deployment
notifications: monitor upstream sources for
changes and notify on Slack
cflinuxfs2: test and release Cloud Foundry cflinuxfs2

Concourse State
Jobs and tasks in the buildpacks-ci repository store state in public-buildpacks-ci-robots. See repository README for details.
Commands and recipes
Updating all the Pipelines
./bin/update-pipelines
Debugging the build
fly intercept -j $JOB_NAME -t task -n $TASK_NAME
Clearing the git resources
fly intercept -c $RESOURCE_NAME rm -rf /tmp/git-resource-repo-cache
To build a new version of a binary


Check out the binary-builds branch


Edit the YAML file appropriate for the build (e.g. ruby-builds.yml)


Find the version number and package SHA256 of the new binary. For many binaries, the project website provides the SHA256 along with the release (for example, jruby.org/download provides the SHA256 along with each JRuby release). For others (such as Godep), you download the .tar.gz file and run shasum -a 256 <tar_file> to obtain the SHA256.


Add any number of versions and their checksums to the array, e.g.
ruby:
- version: 2.2.2
  sha256: 5ffc0f317e429e6b29d4a98ac521c3ce65481bfd22a8cf845fa02a7b113d9b44


git commit -am 'Build ruby 2.2.2' && git push


Build should automatically kick off at
https://buildpacks.ci.cf-app.com/pipelines/binary-builder and silently
upload a binary to the pivotal-buildpacks bucket under
dependencies/,
e.g. https://pivotal-buildpacks.s3.amazonaws.com/dependencies/ruby/ruby-2.2.2-linux-x64.tgz
Note that the array is a stack, which will be emptied as the build
succeeds in packaging successive versions.
Running the Test Suite
If you are running the full test suite, some of the integration tests are dependent on the Lastpass CLI and correctly targeting the fly CLI.
To login to the Lastpass CLI:
lpass login $USERNAME
You will then be prompted for your Lastpass password and Google Authenticator Code.
To login to the Fly CLI and target the buildpacks CI:
fly -t buildpacks login
You will be prompted to select either the Github or Basic Auth authentication methods.
After these are set up, you will be able to run the test suite via:
rspec
Making Changes to Build Scripts
When you want to change how a binary gets built, there are two places you may need to make changes. All binaries are built by the dependency-builds pipeline, and you may need to change the task that builds them. For many binaries, the dependency-builds pipeline runs recipes from the binary-builder repo; for those binaries, you will usually need to change the recipe rather than the concourse task.
For the list of currently supported binaries, check out our dependency-builds pipeline.
The concourse task that orchestrates the building is buildpacks-ci/tasks/build-binary-new/builder.rb; many of the recipes are in binary-builder.
To test these changes locally, you can execute the concourse task for it, but point to local changes. For instance:
$ cd buildpacks-ci
$ STACK=cflinuxfs2 fly -t buildpacks e -c tasks/build-binary-new/build.yml -j dependency-builds/build-r-3.4.X -i buildpacks-ci=.

For binaries that use recipes in binary-builder, you can also test in Docker. For instance:
$ docker run -w /binary-builder -v `pwd`:/binary-builder -it cloudfoundry/cflinuxfs2:ruby-2.2.4 ./bin/binary-builder --name=ruby --version=2.2.3 --md5=150a5efc5f5d8a8011f30aa2594a7654
$ ls
ruby-2.2.3-linux-x64.tgz

Buildpack Repositories Guide
buildpacks-ci pipelines and tasks refer to many other repositories. These repos are where the buildpack team and others develop buildpacks and related artifacts.
Officially-supported Buildpacks
Each officially-supported buildpack has a develop and a master branch.
Active development happens on develop. Despite our best efforts, develop will sometimes be unstable and is not production-ready.
Our release branch is master. This is stable and only updated with new buildpack releases.

binary-buildpack
go-buildpack
nodejs-buildpack
php-buildpack
python-buildpack
ruby-buildpack
dotnet-core-buildpack
staticfile-buildpack

Tooling for Development and Runtime

buildpack-packager   Builds cached and uncached buildpacks
machete           Buildpack integration testing framework.
compile-extensions Suite of utility scripts used in buildpacks at runtime
libbuildpack Library used for writing buildpacks in Golang
binary-builder           Builds binaries against specified rootfs
cflinuxfs2 Tooling to build cflinuxfs2 root file system (""rootfs"") for CF
brats Buildpack Runtime Acceptance Test Suite, a collection of smoke tests

BOSH Releases
BOSH releases are used in the assembly of cf-release.

cflinuxfs2-release
go-buildpack-release
ruby-buildpack-release
python-buildpack-release
php-buildpack-release
nodejs-buildpack-release
staticfile-buildpack-release
binary-buildpack-release
java-offline-buildpack-release
java-buildpack-release
dotnet-core-buildpack-release

Experimental or unsupported
Buildpacks
These buildpacks are possible candidates for promotion, or experimental architecture explorations.

hwc-buildpack
hwc-buildpack-release

Tools

concourse-filter Redacts credentials from Concourse logs
new_version_resource Concourse resource to track dependency versions by scraping webpages

Private Repos
Some repositories are private for historical or security reasons. We list them for completeness.

deployments-buildpacks See repository README.
buildpacks-ci-robots See repository README.
cflinuxfs2-nc See repository README.
cflinuxfs2-nc-release See repository README.

",33
66,Xeverous/filter_spirit,C++,"Filter Spirit
Advanced item filter generator for Path of Exile that uses it's own DSL and online item price APIs. Basically a separate pseudo-programming language that lets you write item filters using item price data from poe.ninja or poe.watch available at generation time. Create multiple filter variants and refresh whenever you want to always be up to date with market prices.
The project is under construction, message /u/Xeverous on reddit or Xeverous#2151 on Discord if you are interested in it's early development or have any questions/suggestions/whatever. You can also open an issue.
documentation and tutorial
Browse doc directory. Files are in Markdown so you can read them online on GitHub.
Note that the tool is in its early development so things may chage. Suggestions welcome.
overview
Core features:

static typing safety
generation-time error checking
language elements dedicated to elimination of code duplication:

ability to define constants: const red = RGB(255, 0, 0)
statements work top-down like in regular filters but they can be nested to inherit conditions and override actions


dynamic rules: BaseType $divination(10, 100) pulls data from API to list div cards worth 10-100c at generation time, refresh whenever you want - your filter is always up-to-date with market prices
support for filter variants (eg color-blind, strict, uber-strict, Animate Weapon support etc) - generate multiple filters with different flavours from a single source file

There are 2 intended usage scenarios:

Someone writes a filter using Filer Spirit templates and publishes generated variants that are ready to be immediately loaded by the game client.
Someone writes a filter template and shares it to other people who have downloaded Filter Spirit - they can:

(re)generate actual filter any time, making it always up to date with market prices
edit the config to tweak generated variants
edit the source to change styles (eg colors, sounds)
edit the source to change filter structure



example code
There is a full example filter template source in the repository although the syntax is a subject to change.
const color_white  = RGB(255, 255, 255, 255)
const color_hammer = RGB(162,  85,   0) # (default opacity)

BaseType [""Gavel"", ""Stone Hammer"", ""Rock Breaker""] {
	SetTextColor color_white
	SetBackgroundColor color_hammer
	# above BaseType conditon will be inherited by all nested blocks
	# above actions will be inherited and can be overriden by nested blocks

	Rarity normal {
		Show # show normal hammers
	}

	Rarity magic
	Quality >= 12 {
		Show # show magic hammers with 12+ quality
	}

	Rarity rare
	Quality >= 16 {
		Show # show rare hammers with 16+ quality
	}

	# hide any other hammers
	# actually discouraged as you might lose RGB recipe material for later blocks
	Hide
}

Class ""Divination Card"" {
	SetTextColor color_divination

	# cards that you always want to pickup
	BaseType [""The Void"", ""The Cartographer"", ""Chaotic Disposition""] {
		SetAlertSound alert_divination_stack_1
		SetBeam beam_divination
		Show
	}

	# 100c+
	BaseType $divination(100, 999999) {
		SetTextColor color_black
		SetBorderColor color_divination
		SetBackgroundColor color_divination
		SetFontSize font_max
		SetAlertSound alert_divination_best
		SetBeam beam_divination
		SetMinimapIcon MinimapIcon(0, blue, square)
		Show
	}

	# 10 - 100c
	BaseType $divination(10, 100) {
		SetBorderColor color_divination
		SetFontSize font_mid_divinaton
		SetAlertSound alert_divination_mid
		SetBeam beam_divination
		SetMinimapIcon MinimapIcon(1, blue, square)
		Show
	}

	# if you really hate these cards
	BaseType [""Rain of Chaos"", ""Carrion Crow""] {
		Hide
	}
}

build dependencies

C++17 compiler (<filesystem> not required)
Boost 1.70 OR older with Spirit headers updated to 1.70; this project uses:

spirit (using bleeding-edge X3 parser)
fusion
optional
variant
program_options
date_time
beast
asio
system
unit_test_framework (only if you build tests)


nlohmann/json
OpenSSL (preferably 1.1)

bolded dependencies require linking, all dependencies are exposed as targets in CMake script
building
standard CMake based build
mkdir build
cd build
cmake .. -DCMAKE_BUILD_TYPE=Release [your_options...]

If you are using make don't forget to add -j (parallel jobs) to add 100% increased build speed per additional core.
licensing
LICENSE file in the main directory of the repository applies to any file, unless otherwise specified.
Contact me if you are interested in work on or using this project but need or require a more permissive license.
",2
67,microsoft/clrmd,C#,"Microsoft.Diagnostics.Runtime

Microsoft.Diagnostics.Runtime.dll (nicknamed ""CLR MD"") is a process and crash
dump introspection library. This allows you to write tools and debugger plugins
which can do thing similar to SOS and PSSCOR.
For more details, take a look at the documentation and samples.
FAQ
Please see the FAQ for more information.
Tutorials
Here you will find a step by step walkthrough on how to use the CLR MD API.
These tutorials are meant to be read and worked through in linear order to teach
you the surface area of the API and what you can do with it.


Getting Started - A brief introduction
to the API and how to create a CLRRuntime instance.


The CLRRuntime Object - Basic operations
like enumerating AppDomains, Threads, the Finalizer Queue, etc.


Walking the Heap - Walking objects on
the GC heap, working with types in CLR MD.


Types and Fields in CLRMD - More
information about dealing with types and fields in CLRMD.


Machine Code in CLRMD - Getting access to
the native code produced by the JIT or NGEN


",515
68,REDasmOrg/REDasm-Library,C++,"LibREDasm
REDasm Core Library
",20
69,rusefi/rusefi,C,"rusEFI
A GPL open-source DIY ECU
current binaries are always available at http://rusefi.com/build_server/


Cloning the repository
Important note - we now use submodules:
git submodule update --init
What do we have here?

Firmware Source code for open source engine control unit for stm32 chips
Hardware KiCAD files for all our PCBs
rusEfi console rusEfi own naive tuning software
Simulator win32 or posix version of firmware allows to explore without any hardware
Unit Tests Unit tests of firmware pieces
Misc tools Misc development utilities
misc/Jenkins Jenkins Continues Integration scripts
Work in progress status

External Links

Wiki: http://rusefi.com
Forum http://rusefi.com/forum
Documentation is now moving to https://github.com/rusefi/rusefi_documentation
Doxygen documentation is available at http://rusefi.com/docs/html
General source code Q&A is at http://rusefi.com/forum/viewtopic.php?f=5&t=10
Facebook https://www.facebook.com/rusEfiECU
YouTube: https://www.youtube.com/user/rusefi
Patreon https://www.patreon.com/rusefi

Building the Code
See https://rusefi.com/forum/viewtopic.php?f=5&t=9
Release Notes



Release date
Revision
Details




04/25/2019
r17317
bugfix #775: electrical noise reboot during settings change causes with full tune loss


04/07/2019
r17098
improvement #714: TLE8888 Enable Outputs SPI integration


02/27/2019
r16886
bugfix #698: concurrency defect with single timer executor initialization


02/23/2019
r16857
improvement #688: better support for 144 and 176 pin packages


02/05/2019
r16713
improvement #631: ChibiOS 18


01/11/2019
r16346
bugfix #663: SPI fixes for CJ125 for stm32 errata STM32f405/7/15/17


12/09/2018
r16057
electronic throttle body control is now BETA version


08/19/2018
r15811
bugfix #604: no interpolation outside of the table


01/29/2018
r15514
improvement #215: CJ125 wideband contoller


01/23/2018
r15442
improvement #463: ChibiOS 17 / 4


01/07/2018
r14952
usability & minor bugfix #532 idle defaults are broken


12/17/2017
r14892
bugfix #513: console: erase on older chips does not work


11/26/2017
r14789
bugfix #500: set_rpn_expression command is broken


11/23/2017
r14779
bugfix #497: engine does not start with SD card


11/19/2017
r14766
improvement #496: console flasher to support older boards


11/19/2017
r14760
improvement #495: incomppatible change - larger settings area


11/06/2017
r14733
improvement #487: single-point injection mode


10/26/2017
r14700
improvement #385: narrow band to wide band conversion table


08/31/2017

improvement #442: ADC_VCC should be configurable


07/24/2017
r14513
bugfix #307: TS bench test blinks three times


07/13/2017
r14476
bugfix #461: SAXParserException on console start-up due to damaged settings.xml file


07/09/2017
r14473
improvement: IAC solenoid frequencty changeable on the fly


06/19/2017
r14393
bugfix: pinMode, milMode


05/27/2017
r14221
improvement: ochGetCommand to support offset and count


05/05/2017
r13974
bugfix #404: 36/1 FATAL error: angle range trgSync


05/03/2017
r13967
improvement: ChibiOS 3.2


04/06/2017
r13759
major improvement #72: ChibiOS 3.1


03/26/2017
r13330
super annoying bug #336 fixed


03/20/2017
r13233
improvements #375 & #376: hard FPU mode & migrating to fresh version of arm gcc


03/19/2017
r13223
bugfix #374: persistent configuration starts to touch firmware - settings would need to be reloaded


03/09/2017
r13146
bugfix #370: duty cycle vs error code 6050


03/06/2017
r13123
bugfix #363: 2/1 skipped wheel is a corner-case


03/05/2017
r13108
bugfix #363: trigger front only processing


02/22/2017
r12980
bugfix: false error message in case of single coil or simultaneous injection


02/22/2017
r12973
unused property 'custom Use Rise Edge' removed


02/22/2017
r12972
protocol signature changed to 'v0.02' (this would happen from time to time to ensure version match between console and TS project


02/20/2017
r12939
https://github.com/rusefi/rusefi is now the official primary repository


02/18/2017
r11565
improvement: level1 default brown out https://sourceforge.net/p/rusefi/tickets/354/


02/18/2017
r11554
bugfix: rusEfi console program/erase buttons fixed with ST-LINK 2.1


12/09/2016
r10991
bugfixes, bugfixes, bugfixes & releave 1.0


02/15/2016
r9600
automatic warm-up fuel correction, performance improvements


08/31/2015
r8725
acceleration fuel correction, console improvements


07/06/2015
r8554
more CAN, better console, bugfixes, performance


04/26/2015
r7938
vehicle speed sensor, stepper idle valve, lots of improvements


11/13/2014
r5327
ChibiOS/RT 2.6.6, better cold start logic, bugfixes, bugfixes, bugfixes...


06/12/2014
r3477
Precise event scheduling, protocol selection


03/26/2014
r2413
C++, refactoring & improvements


02/23/2013
r1777
trigger is now configurable via TunerStudio


01/30/2014
r1309
true trigger angles, VBatt signal


01/12/2014
r1007
refactoring, refactoring & refactoring


12/19/2013
r605
ignition control & a little bit of CAN bus


11/08/2013
r100
Tuner Studio fuel map tuning, fuel pump control


10/14/2013
r39
USB serial bug, missing IAR files, self-containted Makefile


10/13/2013
r33
IAR project file


10/04/2013
r26
Patched ChibiOS/RT 2.6.1 sources are now included


09/23/2013
r20
Tuner Studio integration, configuraton persistence


08/30/2013
r14
initial documentation & refactoring. tunerstudio integration


08/03/2013
r13
wideband O2 input, better idling algorithm, serial-over-USB


07/05/2013
r10
Second CKP, sequential injection.


06/19/2013
r9
Initial version - batch injection & ignition with advance table lookup.



Cloning the repository
Important note - we now use submodules:
git submodule update --init
",124
70,nss-day-cohort-30/bangazon-api-whimsical-whooping-cranes,C#,"Building the Bangazon Platform API
Welcome, new Bangazonians!
Your job is to build out a .NET Web API that makes each resource in the Bangazon ERD available to application developers throughout the entire company.

Products
Product types
Customers
Orders
Payment types
Employees
Computers
Training programs
Departments


Pro tip: You do not need to make a Controller for the join tables, because those aren't resources.

Your product owner will provide you with a prioritized backlog of features for you to work on over the development sprint. The first version of the API will be completely open since we have not determined which authentication method we want to use yet.
The only restriction on the API is that only requests from the www.bangazon.com domain should be allowed. Requests from that domain should be able to access every resource, and perform any operation a resource.
Plan
First, you need to plan. Your team needs to come to a consensus about the Bangazon ERD design. Once you feel you have consensus, you must get it approved by your manager before you begin writing code for the API.
Modeling
Next, you need to author the Models needed for your API. Make sure that each model has the approprate foreign key relationship defined on it, either with a custom type or an List<T> to store many related things. The boilerplate code shows you one example - the relationship between Order and OrderProduct, which is 1 -> ∞. For every OrderId, it can be stored in the OrderProduct table many times.
Database Management
You will be using the Official Bangazon SQL file to create your database. Create the database using SSMS, create a new SQL script for that database, copy the contents of the SQL file into your script, and then execute it.
Controllers
Now it's time to build the controllers that handle GET, POST, PUT, and DELETE operations on each resource. Make sure you read, and understand, the requirements in the issue tickets to you can use  SQL to return the correct data structure to client requests.
Test Classes
Each feature ticket your team will work on for this sprint has testing requirements. This boilerplate solution has a testing project includes with some starter code. You must make sure that all tests pass before you submit a PR.
",3
71,nao-romasaga/nao-romasaga.github.io,HTML,"nao-romasaga.github.io
test
",2
72,sphereio/sphere-node-cli,JavaScript,"
Node.js CLI





The next generation Command-Line-Interface for SPHERE.IO.
Table of Contents

Features
Requirements
Usage
Credentials
Docker

Examples


Commands

sphere-import


Contributing

Features

import of stock, product, price, category, discount, order, customer, productType, discountCode, state, customObject
Docker support
Custom plugin

Requirements
Make sure you have installed all of the following prerequisites on your development machine:

Git - Download & Install Git. MacOS and Linux machines typically have this already installed.
Node.js - Download & Install Node.js and the npm package manager. Make sure to get the latest active LTS version. You could also use a Node.js version manager such as n or nvm.


If you were using the old ruby CLI make sure to uninstall it first.

Usage
$ npm install -g sphere-node-cli

# show general help
$ sphere -h

# show help for a command (e.g.: import)
$ sphere help <cmd>

The CLI is still under development but already provides a bunch of commands.
The idea behind it is to operate as a proxy for the different libraries that are used underneath. For example the import command will stream chunks from a given JSON file and pass them to the related library that will handled the rest.
Credentials
The CLI has a lookup mechanism to load SPHERE.IO project credentials.
If you specify a -p, --project option, the CLI will try to load the credentials for that project from the following locations:
./.sphere-project-credentials
./.sphere-project-credentials.json
~/.sphere-project-credentials
~/.sphere-project-credentials.json
/etc/sphere-project-credentials
/etc/sphere-project-credentials.json

There are 2 supported formats: csv and json.

csv: project_key:client_id:client_secret
json: { ""project_key"": { ""client_id"": """", ""client_secret"": """" } }

If no -p, --project option is provided, the CLI tries to read the credentials from ENV variables:
export SPHERE_PROJECT_KEY=""""
export SPHERE_CLIENT_ID=""""
export SPHERE_CLIENT_SECRET=""""

Docker

You need to have a working docker client! The Docker Toolbox is an installer to quickly and easily install and setup a Docker environment on your computer. Available for both Windows and Mac, the Toolbox installs Docker Client, Machine, Compose, Kitematic and VirtualBox.
Examples
Show help
docker run \
sphereio/sphere-node-cli -h
Import a product (host folder /sample_dir/ mounted as docker volume)
docker run \
-e SPHERE_PROJECT_KEY=<KEY>
-e SPHERE_CLIENT_ID=<ID>
-e SPHERE_CLIENT_SECRET=<SECRET>
-v /sample_dir/:/sample_dir/ \
sphereio/sphere-node-cli
import -p my-project-key -t product -f /sample_dir/products.json'
You can also set an alias for repeated calls:
alias sphere='docker run \
-v /etc/sphere-project-credentials.json:/etc/sphere-project-credentials.json \
sphereio/sphere-node-cli'
Commands
The CLI has git-like sub-commands which can be invoked as sphere <cmd>.
Current available commands:

import (stock, product, price, category, discount, order, customer, productType, discountCode, state, customObject)

Commands expects at least a -t, --type option which may vary for each command.
sphere-import
Imports a resource type by streaming the input JSON file.
$ sphere import -p my-project-key -t product \
  -f sample_dir/products.json \
  -c '{""errorDir"": ""./productErrors""}'
The input must be a valid JSON following a specific schema (import-type-key is the plural form of the type option, e.g.: products, stocks, etc.).
{
  ""$schema"": ""http://json-schema.org/draft-04/schema#"",
  ""title"": ""SPHERE.IO CLI import schema"",
  ""type"": ""object"",
  ""properties"": {
    ""<import-type-key>"": {
      ""type"": ""array"",
      ""items"": {
        ""$ref"": ""https://github.com/sphereio/sphere-json-schemas/tree/master/schema""
      }
    }
  },
  ""additionalProperties"": false,
  ""required"": [""<import-type-key>""]
}

If you don't provide a file to read from, the CLI listens from stdin so you can i.e. pipe in something.

Each import type might have / expect some extra specific configuration. In that case you have to refer to the related documentation.

Product import
Product Type import
Price import
Category import
Product Discount import
Customer import
Stock import
Order import
Discount Code import
State import
Custom Object import

Contributing
See Contribution guidelines
",3
73,dsherret/code-block-writer,TypeScript,"code-block-writer




Code writer that assists with formatting and visualizing blocks of JavaScript or TypeScript code.
npm install --save code-block-writer

Example
import CodeBlockWriter from ""code-block-writer"";

const writer = new CodeBlockWriter({
    // optional options
    newLine: ""\r\n"",         // default: ""\n""
    indentNumberOfSpaces: 2, // default: 4
    useTabs: false,          // default: false
    useSingleQuote: true     // default: false
});
const className = ""MyClass"";

writer.write(`class ${className} extends OtherClass`).block(() => {
    writer.writeLine(`@MyDecorator(1, 2)`);
    writer.write(`myMethod(myParam: any)`).block(() => {
        writer.write(""return this.post("").quote(""myArgument"").write("");"");
    });
});

console.log(writer.toString());
Outputs (using ""\r\n"" for newlines):
class MyClass extends OtherClass {
  @MyDecorator(1, 2)
  myMethod(myParam: any) {
    return this.post('myArgument');
  }
}
de```

## Methods

* `block(block?: () => void)` - Indents all the code written within and surrounds it in braces.
* `inlineBlock(block?: () => void)` - Same as block, but doesn't add a space before the first brace and doesn't add a newline at the end.
* `getLength()` - Get the current number of characters.
* `writeLine(text: string)` - Writes some text and adds a newline.
* `newLine()` - Writes a newline.
* `newLineIfLastNot()` - Writes a newline if what was written last wasn't a newline.
* `blankLine()` - Writes a blank line. Does not allow consecutive blank lines.
* `blankLineIfLastNot()` - Writes a blank line if what was written last wasn't a blank line.
* `quote()` - Writes a quote character.
* `quote(text: string)` - Writes text surrounded in quotes.
* `indent(times?: number)` - Indents the current line. Optionally indents multiple times when providing a number.
* `indentBlock(block?: () => void)` - Indents a block of code.
* `space(times?: number)` - Writes a space. Optionally writes multiple spaces when providing a number.
* `spaceIfLastNot()` - Writes a space if the last was not a space.
* `tab(times?: number)` - Writes a tab. Optionally writes multiple tabs when providing a number.
* `tabIfLastNot()` - Writes a tab if the last was not a tab.
* `write(text: string)` - Writes some text.
* `conditionalNewLine(condition: boolean)` - Writes a newline if the condition is matched.
* `conditionalBlankLine(condition: boolean)` - Writes a blank line if the condition is matched.
* `conditionalWrite(condition: boolean, text: string)` - Writes if the condition is matched.
* `conditionalWrite(condition: boolean, textFunc: () => string)` - Writes if the condition is matched.
* `conditionalWriteLine(condition: boolean, text: string)` - Writes some text and adds a newline if the condition is matched.
* `conditionalWriteLine(condition: boolean, textFunc: () => string)` - Writes some text and adds a newline if the condition is matched.
* `setIndentationLevel(indentationLevel: number)` - Sets the current indentation level.
* `setIndentationLevel(whitespaceText: string)` - Sets the current indentation level based on the provided whitespace text.
* `withIndentationLevel(indentationLevel: number, action: () => void)` - Sets the indentation level within the provided action.
* `withIndentationLevel(whitespaceText: string, action: () => void)` - Sets the indentation level based on the provided whitespace text within the action.
* `getIndentationLevel()` - Gets the current indentation level.
* `queueIndentationLevel(indentationLevel: number)` - Queues an indentation level to be used once a new line is written.
* `queueIndentationLevel(whitespaceText: string)` - Queues an indentation level to be used once a new line is written based on the provided whitespace text.
* `withHangingIndentation(action: () => void)` - Writes the code within the action with hanging indentation.
* `closeComment()` - Writes text to exit a comment if in a comment.
* `isInComment()` - Gets if the writer is currently in a comment.
* `isAtStartOfFirstLineOfBlock()` - Gets if the writer is currently at the start of the first line of the text, block, or indentation block.
* `isOnFirstLineOfBlock()` - Gets if the writer is currently on the first line of the text, block, or indentation block.
* `isInString()` - Gets if the writer is currently in a string.
* `isLastNewLine()` - Gets if the writer last wrote a newline.
* `isLastBlankLine()` - Gets if the writer last wrote a blank line.
* `isLastSpace()` - Gets if the writer last wrote a space.
* `isLastTab()` - Gets if the writer last wrote a tab.
* `getLastChar()` - Gets the last character written.
* `getOptions()` - Gets the writer options.
* `toString()` - Gets the string.

## Other Features

* Does not indent within strings.
* Escapes newlines within double and single quotes created with `.quote(text)`.

## C# Version

See [CodeBlockWriterSharp](https://github.com/dsherret/CodeBlockWriterSharp).

",16
74,microsoft/react-native-windows,C#," React Native for Windows 

  Build native Windows apps with React.
















React Native enables you to build world-class application experiences on native platforms using a consistent developer experience based on JavaScript and React. The focus of React Native is on developer efficiency across all the platforms you care about - learn once, write anywhere. Facebook uses React Native in multiple production apps and will continue investing in React Native.
This repository adds support for the Windows 10 SDK, which allows you to build apps for all devices supported by Windows 10 including PCs, tablets, 2-in-1s, Xbox, Mixed reality devices etc.
Status
We are in the process of re-implementing react-native-windows in C++, for better performance, and to better align with the shared C++ react-native core as it evolves. This effort is currently taking place in the vnext subdirectory within this GitHub repo. You can read more about this effort here. We are tracking progress on this work through Projects and Issues on this GitHub repo.
The ""current"" subdirectory holds the earlier C# implementation for react-native-windows.
Our intent is to provide a compatibility layer that will support existing apps, view managers, and native modules written in C# with minimal breaking changes.
We anticipate that there will be increased activity on the repository related to this refactoring effort, but that overall effort allocated to the existing architecture will be minimized. Please continue to report issues as you encounter them, but be sure to use the correct template for issues related to the existing react-native-windows package (version 0.57 and earlier).
Quick Links

Getting Started
Getting Help
Documentation
Examples
Extending React Native
Opening Issues
Contributing
License
Code of Conduct

Introduction
See the official React Native website for an introduction to React Native.
System Requirements

You can run React Native Windows UWP apps only on Windows 10 devices

React Native Windows -current supports Windows 10 SDK > 14393 and Windows 8.1
React Native Windows -vnext currently supports Windows 10 SDK > 16299


You can run React Native Windows WPF apps using the current implementation on Windows 7-10 so long as .NET 4.6 is installed on the end user's machine
Download Visual Studio 2017 Community or Greater. (Visual Studio 2015 support has been deprecated.)

You will need to start Visual Studio after it is installed to do some final setup before it can be used to build or run your React Native Windows application



Note: Development on React Native Windows itself currently requires Visual Studio 2017. It is not supported with VS Code, but we will gladly accept pull requests to enable a great developer experience in those environments.
Getting Started
Want to develop a React Native Windows app? Head over to our Getting Started Guide - current.
You can also begin to prototype or try out the vnext version under development by checking out the Getting Started Guide - vnext
Getting Help

Chat with us on Reactiflux in #react-native-platforms
If it turns out that you may have found a bug, please open an issue

Documentation
React Native already has great documentation, and we're working to ensure the React Native Windows is part of that documentation story. Check out the React documentation for further details about the React API in general.
For information on parity status with Android and iOS, including details on implemented and missing components and modules, along with related issues for missing features from partial implementations, go here for the current react-native-windows implementation. We are working on publishing a similar parity status for vnext shortly.
Extending React Native

Looking for a component? JS.coach
Fellow developers write and publish React Native modules to npm and open source them on GitHub.
Third party modules may be missing Windows support, so reach out to the project owners directly.
Making modules helps grow the React Native ecosystem and community. We recommend writing modules for your use cases and sharing them on npm.
You should almost never need to modify or fork React Native Windows itself (the ReactNative and ReactNative.Shared project/sources) to create custom controls. The extension points should already exist to do just about anything!
Read the guides on Native Modules for Windows and Native UI Components for Windows if you are interested in extending native functionality. Note: Guides for vnext coming soon.

Opening Issues
If you encounter a bug with the React Native Windows plugin, we would like to hear about it. Search the existing issues and try to make sure your problem doesn’t already exist before opening a new issue. It’s helpful if you include the version of Windows, React Native, React Native Windows plugin, and device family (i.e., mobile, desktop, Xbox, etc.) you’re using. Please include a stack trace and reduced repro case when appropriate, too.
Please use either the current vs. vnext issue template as appropriate.
The GitHub issues are intended for bug reports and feature requests. For help and questions with using the React Native Windows plugin please make use of the resources listed in the Getting Help section.
Contributing
Make sure to install the system requirements. If you just want to get started with developing your own app, read Getting Started with App Development. You only need to interact with npm to use for your app development.
For more information about contributing PRs, see :

Contribution Guidelines for current react-native-windows
Contribution Guidelines for vnext react-native-windows

Good First Task and help wanted are great starting points for PRs.
Examples

Using the CLI in the Getting Started sections will set you up with a sample React-Native-Windows app that you can begin editing.
If you're looking for sample code, just browse the RNTester folder in the GitHub web UI

License
The React Native Windows plugin, including modifications to the original Facebook source code, and all newly contributed code is provided under the MIT License. Portions of the React Native Windows plugin derived from React Native are copyright Facebook.
Code of Conduct
This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.
",8496
75,ApptivaAG/apptiva-webpage,JavaScript,"Apptiva Webpage
This repo contains the website of Apptiva AG
Prerequisites

Node (I recommend using v8.2.0 or higher)
Gatsby CLI

Getting Started (Recommended)
Access Locally
$ git clone https://github.com/[GITHUB_USERNAME]/[REPO_NAME].git
$ cd [REPO_NAME]
$ npm i
$ npm start

To test the CMS locally, you'll need run a production build of the site:
$ npm run build
$ npm run serve

Push update
Just make your changes locally, test and git commit/push. A new version will be built automatically.
Debugging
Windows users might encounter node-gyp errors when trying to npm install.
To resolve, make sure that you have both Python 2.7 and the Visual C++ build environment installed.
npm config set python python2.7
npm install --global --production windows-build-tools

Full details here
",2
76,mgree/smoosh,OCaml,"
Smoosh (the Symbolic, Mechanized, Observable, Operational SHell) is a formalization of the POSIX shell standard; Smoosh is one part of Michael Greenberg's broader project on the POSIX shell.
Smoosh is written in a mix of Lem and OCaml, using libdash to parse shell code.
Fetch the submodules

The best way to clone this repository is via git clone --recurse-submodules https://github.com/mgree/smoosh.
If you didn't use --recursive or --recurse-submodules, before trying anything, run: git submodule update --init --recursive

If you don't load the git submodules, the libdash and Lem references won't resolve properly---the directories will be empty!
Whenever you pull, remember to run git submodule update to make sure that the submodules are at the correct versions.
How to build and test it

Run: ./build.sh

Building in more detail

Run: docker build -t smoosh .

To build by hand, you should more or less follow the steps in the Dockerfile, adapting to your system. (For example, on OS X, you'll probably want to install directly to /usr/local.)
Testing in more detail

To run the test suite after building, run: docker build -t smoosh-test -f Dockerfile.test . && docker run smoosh-test
To explore the built image, run: docker run -it smoosh

To test by hand, there are three sets of relevant tests: the libdash tests (in libdash/test), the unit tests for symbolic smoosh (in src), and the shell test suite (in tests). All three directories have Makefiles with appropriate test targets, so you can test both by running the following:
make -C libdash/test test
make -C src/ test
make -C tests test

You can do so by running docker run -it smoosh to get an interactive environment.
How to use the web interface

After building the smoosh image, build the web image: docker build -t smoosh-web -f Dockerfile.web .
To run the web image docker run -p 80:2080 --name smoosh-web -t smoosh-web and go to http://localhost/.

",12
77,tlambert03/FPbase,JavaScript,"
FPbase: The Fluorescent Protein Database




https://www.fpbase.org
Installation for local development

Clone repo and cd into directory

    $ git clone https://github.com/tlambert03/FPbase.git
    $ cd FPbase


Create/activate environment with pipenv/virtualenv/conda (python 3 required)
Install python requirements for local development

    $ pip install -r requirements/local.txt


Install Node.js & npm  (homebrew: brew install node)
Install frontend requirements and gulpjs

    $ npm install
    $ npm install gulp-cli -g


Install a local postgreSQL database (for mac: postgres.app)
Create database, and apply migrations

    $ createdb fpbase
    $ python manage.py migrate


If desired, load sample data (this may eventually break if the database schema changes enough).

    $ python manage.py loaddata fixtures/testdata.json.gz


Compile assets and start server:

    $ gulp

How to cite FPbase
If you have used FPbase in a publication, or are referencing an FPbase protein collection or microscope in your methods, please cite the following paper:
Lambert, TJ (2019) FPbase: a community-editable fluorescent protein database. Nature Methods. doi: 10.1038/s41592-019-0352-8
Contributing
If you would like to contribute to the website directly (for instance, to add a feature or fix an error), please branch off of develop and submit a pull request.
If you have data that you would like to contribute to the database, please do not do that here.  All data can be submitted directly on the website:
Submit a fluorescent protein
Submit spectral information
Thank you to these providers for supporting open source projects!






",11
78,occamLab/dongle_cane_ios_game,Swift,"Musical Cane Game
Last updated : May 10, 2019 by Team OM-ega
Background
This project has been initiated and maintained by Paul's students (Occam Lab, TAD). Eric Jerman from Perkins School for the Blind is our primary partner for this project. He works with students at Perkins School for the Blind to help them develop their orientation and mobility skills.
Orientation and Mobility refers to a set of skills that consists of knowing where you are in an environment, understanding how to navigate to a place of interest, and how to move about safely within an environment.While these skills are vital to the independence of Eric’s students, their motivation to practice these skills is often low.  In contrast, other elements of the Perkins curriculum (e.g., music therapy) tend to be met with more excitement and commitment.  Eric is interested in developing strategies for making his sessions more engaging and effective through the use of the concept of gamified learning, whereby elements of game design are brought to an educational context.
Introduction
Musical Cane Game gamifies the orientation and mobility learning experience and helps an O&M instructor by

Motivating students with sounds, music, and beep noises
Rewarding students while preventing them from ""cheating""
Providing real-time feedback of a student's cane sweep through the app screen
Enabling customized support for each individual student (cane length, sweep range, skill level)
Introducing cool features like ""Shepard's Grip""

How to Play
This guide is for the O&M instructor who runs the training sessions with students who play this game.
To play our musical cane game,
On the cane
Attach the bluetooth dongle to the cane using a zip tie. Orientation does not matter.
Take a paper clip, and unbend one end of it. Stick the end into the slot on the rounded side of the dongle. There is a switch inside that you must toggle. If you see a blue light on the dongle, that means the device is on! Remember to turn it back off when you are done.


On the App


Form the start screen, hit the menu navigation bar in the top left.


Select Manage Profiles


Create a new profile for the student by clicking Create Profile in the top right


Touch 'Edit'


Select the student's favorite music, sounds, and beep noises from Apple Music.
You need to put sound files into Apple Music if you don't have any
(The app only selects music files from Apple Music)


Set custom values for the student

Beep Count : Number of Beeps needed for the reward music to play
Cane Length (inches) : Length between the tip of the cane and the student's grip
Sweep Range (inches): Chord Length (straight line from left end to right end) of the arc the cane makes
Skill Level : Determines the error tolerance (from 1 being super loose to 5 being super strict)



Touch Save in the top right.


Go to the side navigation and choose the game mode you would like to play. The student's name and other relevent information should appear on the screen when the mode is selected.
There are three different modes available.

Sound Mode

Speaking mode: Phone says the number of sweeps
Beep mode: Phone plays the selected beep sound on each successful sweep


Music Mode: Phone plays music while there is successful sweeping



Touch 'Start' in the top right. With sound turned on, you should hear Connected and then start sweeping


Let the student start sweeping, there should be a progress bar indicating their progress on the current sweep. When students are in green zone, it means they could change directions and the sweep would be counted as valid.

Remember that the app only measures the length the student is sweeping, not the orientation of the cane to the student.
If the green range is not representative of where you want them to be sweeping, consider changing the sweep range (for wider/shorter sweeps) or the user level (for more tolerence for bad sweeps).



When the reward music plays or you want to end the game, Touch Stop in the top right.


Contribution Guide (for developers)
Dev Environment Setup
The code is written in Swift 4.
When working on the app, open the workspace on Xcode.
Run pod install in a terminal to install the pods locally.
(It uses an external library called SQLite.swift to manage the database for profile data).
Documentation
Code documentation (Jazzy) is available in docs/. The master branch automatically generates a documentation website based on docs/
To generate the documentation, you need to first install Jazzy.
Run [sudo] gem install jazzy.
Once the installation is done,
Run jazzy --min-acl internal to build the documentation.
This command generates the website under /docs, and github deploys master/docs on every push.
Follow this tutorial to learn how to add inline comments in markdown style so that Jazzy can parse them automatically.
Future Dev Directions

Tracking of a student's progress over multiple games (Generate daily, weekly, monthly development report)
Beacons Mode (expansion of the game into learning spaces around)
Detect Cane Velocity
Refactor the code base (add more inline documentation in Jazzy Style)
Fully support Apple Voiceover

",3
79,benjaminlefevre/auto-plate-number,Kotlin,"auto-plate-number    
This project is composed of 3 parts:

a library auto-plate-number-library providing several ways to retrieve vehicle from its plate number
a CLI to execute in shell the service provided by the previous library (CLI is a Spring Boot application)
an Android app GUI invoking the same service

Command Line Interface (CLI) usage
java -jar plate-number-cli/build/libs/plate-number-cli.jar <plate number> --country <countryCode>
Currently, only fr (France), gb( United Kingdom), it (Italia) and es (Spain) are supported

Android app : Auto Plaque (fr) / Auto Plate (en) / Auto Targa (it) / Auto Place (pt) / Auto Matricula (es)
Google Play Store Link

Language and libraries used

Kotlin over JVM
Android SDK 28.x
Spring Boot and Spring Web
commons-io, commons-lang3
android-mail, android-activation
design android libs like cardview and so on...

Dependency graph
CLI app

Android app

",4
80,nilvon9wo/Xappe,Apex,"Xappe
Salesforce Apex framework solutions for Trigger Handlers and SObject Selectors.
A new trigger handling framework with a minimalist, heavily OOP design.
The selector framework is heavily derivative of Apex Commons, but with the following modifications:

Clean code.
Concise classes and methods.
Minimalist dependencies, all explicitly injected whenever possible.
An OOP approach to generating the SOQL WHERE clause.

",2
81,tlambert03/FPbase,JavaScript,"
FPbase: The Fluorescent Protein Database




https://www.fpbase.org
Installation for local development

Clone repo and cd into directory

    $ git clone https://github.com/tlambert03/FPbase.git
    $ cd FPbase


Create/activate environment with pipenv/virtualenv/conda (python 3 required)
Install python requirements for local development

    $ pip install -r requirements/local.txt


Install Node.js & npm  (homebrew: brew install node)
Install frontend requirements and gulpjs

    $ npm install
    $ npm install gulp-cli -g


Install a local postgreSQL database (for mac: postgres.app)
Create database, and apply migrations

    $ createdb fpbase
    $ python manage.py migrate


If desired, load sample data (this may eventually break if the database schema changes enough).

    $ python manage.py loaddata fixtures/testdata.json.gz


Compile assets and start server:

    $ gulp

How to cite FPbase
If you have used FPbase in a publication, or are referencing an FPbase protein collection or microscope in your methods, please cite the following paper:
Lambert, TJ (2019) FPbase: a community-editable fluorescent protein database. Nature Methods. doi: 10.1038/s41592-019-0352-8
Contributing
If you would like to contribute to the website directly (for instance, to add a feature or fix an error), please branch off of develop and submit a pull request.
If you have data that you would like to contribute to the database, please do not do that here.  All data can be submitted directly on the website:
Submit a fluorescent protein
Submit spectral information
Thank you to these providers for supporting open source projects!






",11
82,occamLab/dongle_cane_ios_game,Swift,"Musical Cane Game
Last updated : May 10, 2019 by Team OM-ega
Background
This project has been initiated and maintained by Paul's students (Occam Lab, TAD). Eric Jerman from Perkins School for the Blind is our primary partner for this project. He works with students at Perkins School for the Blind to help them develop their orientation and mobility skills.
Orientation and Mobility refers to a set of skills that consists of knowing where you are in an environment, understanding how to navigate to a place of interest, and how to move about safely within an environment.While these skills are vital to the independence of Eric’s students, their motivation to practice these skills is often low.  In contrast, other elements of the Perkins curriculum (e.g., music therapy) tend to be met with more excitement and commitment.  Eric is interested in developing strategies for making his sessions more engaging and effective through the use of the concept of gamified learning, whereby elements of game design are brought to an educational context.
Introduction
Musical Cane Game gamifies the orientation and mobility learning experience and helps an O&M instructor by

Motivating students with sounds, music, and beep noises
Rewarding students while preventing them from ""cheating""
Providing real-time feedback of a student's cane sweep through the app screen
Enabling customized support for each individual student (cane length, sweep range, skill level)
Introducing cool features like ""Shepard's Grip""

How to Play
This guide is for the O&M instructor who runs the training sessions with students who play this game.
To play our musical cane game,
On the cane
Attach the bluetooth dongle to the cane using a zip tie. Orientation does not matter.
Take a paper clip, and unbend one end of it. Stick the end into the slot on the rounded side of the dongle. There is a switch inside that you must toggle. If you see a blue light on the dongle, that means the device is on! Remember to turn it back off when you are done.


On the App


Form the start screen, hit the menu navigation bar in the top left.


Select Manage Profiles


Create a new profile for the student by clicking Create Profile in the top right


Touch 'Edit'


Select the student's favorite music, sounds, and beep noises from Apple Music.
You need to put sound files into Apple Music if you don't have any
(The app only selects music files from Apple Music)


Set custom values for the student

Beep Count : Number of Beeps needed for the reward music to play
Cane Length (inches) : Length between the tip of the cane and the student's grip
Sweep Range (inches): Chord Length (straight line from left end to right end) of the arc the cane makes
Skill Level : Determines the error tolerance (from 1 being super loose to 5 being super strict)



Touch Save in the top right.


Go to the side navigation and choose the game mode you would like to play. The student's name and other relevent information should appear on the screen when the mode is selected.
There are three different modes available.

Sound Mode

Speaking mode: Phone says the number of sweeps
Beep mode: Phone plays the selected beep sound on each successful sweep


Music Mode: Phone plays music while there is successful sweeping



Touch 'Start' in the top right. With sound turned on, you should hear Connected and then start sweeping


Let the student start sweeping, there should be a progress bar indicating their progress on the current sweep. When students are in green zone, it means they could change directions and the sweep would be counted as valid.

Remember that the app only measures the length the student is sweeping, not the orientation of the cane to the student.
If the green range is not representative of where you want them to be sweeping, consider changing the sweep range (for wider/shorter sweeps) or the user level (for more tolerence for bad sweeps).



When the reward music plays or you want to end the game, Touch Stop in the top right.


Contribution Guide (for developers)
Dev Environment Setup
The code is written in Swift 4.
When working on the app, open the workspace on Xcode.
Run pod install in a terminal to install the pods locally.
(It uses an external library called SQLite.swift to manage the database for profile data).
Documentation
Code documentation (Jazzy) is available in docs/. The master branch automatically generates a documentation website based on docs/
To generate the documentation, you need to first install Jazzy.
Run [sudo] gem install jazzy.
Once the installation is done,
Run jazzy --min-acl internal to build the documentation.
This command generates the website under /docs, and github deploys master/docs on every push.
Follow this tutorial to learn how to add inline comments in markdown style so that Jazzy can parse them automatically.
Future Dev Directions

Tracking of a student's progress over multiple games (Generate daily, weekly, monthly development report)
Beacons Mode (expansion of the game into learning spaces around)
Detect Cane Velocity
Refactor the code base (add more inline documentation in Jazzy Style)
Fully support Apple Voiceover

",3
83,benjaminlefevre/auto-plate-number,Kotlin,"auto-plate-number    
This project is composed of 3 parts:

a library auto-plate-number-library providing several ways to retrieve vehicle from its plate number
a CLI to execute in shell the service provided by the previous library (CLI is a Spring Boot application)
an Android app GUI invoking the same service

Command Line Interface (CLI) usage
java -jar plate-number-cli/build/libs/plate-number-cli.jar <plate number> --country <countryCode>
Currently, only fr (France), gb( United Kingdom), it (Italia) and es (Spain) are supported

Android app : Auto Plaque (fr) / Auto Plate (en) / Auto Targa (it) / Auto Place (pt) / Auto Matricula (es)
Google Play Store Link

Language and libraries used

Kotlin over JVM
Android SDK 28.x
Spring Boot and Spring Web
commons-io, commons-lang3
android-mail, android-activation
design android libs like cardview and so on...

Dependency graph
CLI app

Android app

",4
84,nilvon9wo/Xappe,Apex,"Xappe
Salesforce Apex framework solutions for Trigger Handlers and SObject Selectors.
A new trigger handling framework with a minimalist, heavily OOP design.
The selector framework is heavily derivative of Apex Commons, but with the following modifications:

Clean code.
Concise classes and methods.
Minimalist dependencies, all explicitly injected whenever possible.
An OOP approach to generating the SOQL WHERE clause.

",2
85,ben-laufer/DMRichR,R,"DMRichR
A workflow for the statistical analysis and visualization of differentially methylated regions (DMRs) of CpG count matrices (Bismark cytosine reports) from the CpG_Me pipeline.
Table of Contents

DMR Approach and Interpretation
Installation
The Design Matrix and Covariates
Input

Generic Example
UC Davis Example


Output
Citation
Acknowledgements

DMR Approach and Interpretation
The main statistical approach applied by the executable script located in the exec folder is dmrseq::dmrseq(), which identifies DMRs in a two step approach:

DMR Detection: The differences in CpG methylation for the effect of interest are pooled and smoothed to give CpG sites with higher coverage a higher weight, and candidate DMRs with a difference between groups are assembled.
Statistical Analysis: A region statistic for each DMR, which is comparable across the genome, is estimated via the application of a generalized least squares (GLS) regression model with a nested autoregressive correlated error structure for the effect of interest. Then, permutation testing of a pooled null distribution enables the identification of significant DMRs This approach accounts for both inter-individual and inter-CpG variability across the entire genome.

The main estimate of a difference in methylation between groups is not a fold change but rather a beta coefficient, which is representative of the average effect size; however, it is on the scale of the arcsine transformed differences and must be divided by π (3.14) to be similar to the mean methylation difference over a DMR, which is provided in the percentDifference column. Since the testing is permutation based, it provides empirical p-values as well as FDR corrected q-values.
One of the key differences between dmrseq and other DMR identification packages, like bsseq, is that dmrseq is performing statistical testing on the DMRs themselves rather than testing for differences in single CpGs that are then assembled into DMRs like bsseq::dmrFinder() does. This unique approach helps with controlling the false discovery rate and testing the correlated nature of CpG sites in a regulatory region, while also enabling complex experimental designs. However, since dmrseq::dmrseq() does not provide individual smoothed methylation values, bsseq::BSmooth() is utlized to generate individual smoothed methylation values from the DMRs. Therefore, while the DMRs themselves are adjusted for covariates, the indvidual smoothed methylation values for these DMRs are not adjusted for covaraites.
You can also read my general summary of the drmseq approach on EpiGenie.
Example DMR

Each dot represents the methylation level of an individual CpG in a single sample, where the size of the dot is representative of coverage. The lines represent smoothed methylation levels for each sample, either control (blue) or DS (red). Genic and CpG annotations are shown below the plot.
Installation
No manual installation of R packages is required, since the required packages and updates will occur automatically upon running the executable script located in the exec folder. However, the package does require Bioconductor 3.8, which you can install or update to using:
if (!requireNamespace(""BiocManager"", quietly = TRUE))
install.packages(""BiocManager"")
BiocManager::install(""BiocInstaller"", version = ""3.8"")

Additionally, if you are interested in creating your own workflow as opposed to using the executable script, you can download the package using:
BiocManager::install(c(""remotes"", ""ben-laufer/DMRichR""))
The Design Matrix and Covariates
This script requires a basic design matrix to identify the groups and covariates, which should be named sample_info.xlsx and contain header columns to identify the factor. It is important to have the label for the experimental samples start with a letter in the alphabet that comes after the one used for control samples in order to obtain results for experimental vs. control rather than control vs. experimental. You can select which specific samples to analyze from the working directory through the design matrix, where pattern matching of the sample name will only select bismark cytosine report files with a matching name before the first underscore, which also means that sample names should not contain underscores. Within the script, covariates can be selected for adjustment. There are two different ways to adjust for covariates: directly adjust values or balance permutations.



Name
Diagnosis
Age
Sex




SRR3537014
Idiopathic_ASD
14
M


SRR3536981
Control
42
F



Input
Before running the executable, ensure you have the following project directory tree structure for the Bismark cytosine reports and design matrix:
├── Project
│   ├── cytosine_reports
│   │   ├── sample1_bismark_bt2.deduplicated.bismark.cov.gz.CpG_report.txt.gz
│   │   ├── sample2_bismark_bt2.deduplicated.bismark.cov.gz.CpG_report.txt.gz
│   │   ├── sample_info.xlsx

This workflow requires the following variables:

-g --genome Select either: hg38, mm10, rn6, or rheMac8.
-x --coverage CpG coverage cutoff for all samples, 1x is the default and minimum value.
-s --perGroup Percent of samples per a group for CpG coverage cutoff, values range from 0 to 1, 1 (100%) is the default.
-m --minCpGs Minimum number of CpGs for a DMR, 5 is default.
-p --maxPerms Number of permutations for DMR and block analyses, 10 is default.
-o --cutoff The cutoff value for the single CpG coefficient utilized to discover testable background regions, values range from 0 to 1, 0.05 (5%) is the default.
-t --testCovariate Covariate to test for significant differences between experimental and control, i.e. Diagnosis.
-a --adjustCovariate Adjust covariates that are continuous or contain two or more factor groups, i.e. ""Age"". More than one covariate can be adjusted for using single brackets and the ; delimiter, i.e. 'BMI;Smoking'
-m --matchCovariate Covariate to balance permutations, which is meant for two-group factor covariates in small sample sizes in order to prevent extremely unbalanced permutations. Only one covariate two-group factor can be balanced, i.e. Sex. Note: This will not work for larger sample sizes (> 500,000 permutations) and is not needed for them as the odds of sampling an extremely unbalanced permutation for a covariate decreases with increasing sample size.
-c --cores The number of cores to use, 20 is recommended but you can go as low as 1, 8 is the default.

Generic Example
Below is an example of how to execute the main R script (DM.R) in the exec folder on command line. This should be called from the working directory that contains the cytosine reports.
call=""Rscript \
--vanilla \
/share/lasallelab/programs/DMRichR/DM.R \
--genome hg38 \
--coverage 1 \
--perGroup '1' \
--minCpGs 5 \
--maxPerms 10 \
--cutoff '0.05' \
--testCovariate Diagnosis \
--adjustCovariate 'BMI;Smoking' \
--matchCovariate Sex \
--cores 20""

echo $call
eval $call

UC Davis Example
If you are using the Barbera cluster at UC Davis, the following commands can be used to execute DM.R from your login node (i.e. epigenerate), where htop should be called first to make sure the whole node is available. This should be called from the working directory that contains the cytosine reports and not from within a screen.
module load R

call=""nohup \
Rscript \
--vanilla \
/share/lasallelab/programs/DMRichR/DM.R \
--genome hg38 \
--coverage 1 \
--perGroup '1' \
--minCpGs 5 \
--maxPerms 10 \
--cutoff '0.05' \
--testCovariate Diagnosis \
--adjustCovariate 'BMI;Smoking' \
--matchCovariate Sex \
--cores 60 \
> DMRichR.log 2>&1 &""

echo $call
eval $call 
echo $! > save_pid.txt

You can then check on the job using tail -f DMRichR.log and ⌃ Control + c to exit the log view.
You can cancel the job from the project directory using cat save_pid.txt | xargs kill. You can also check your running jobs using ps -ef | grep , which should be followed by your username i.e. ps -ef | grep blaufer. Finally, if you still see leftover processes in htop, you can cancel all your processes using pkill -u, which should be followed by your username i.e. pkill -u blaufer.
Alternatively, the executable can also be submitted to the cluster using the shell script via sbatch DM.R.sh.
Output
This workflow provides the following files:

CpG methylation and coverage value distribution plots
DMRs and background regions
Individual smoothed methylation values for DMRs, background regions, and windows/bins
Smoothed global and chromosomal methylation values and statistics
Heatmap and region plots of individual smoothed methylation values for DMRs
PCA plots of 20 Kb windows (all genomes) and CpG island windows (hg38, mm10, and rn6)
Gene ontology and pathway enrichments (enrichr for all genomes, GREAT for hg38 and mm10)
Gene region and CpG annotations and plots (hg38, mm10, or rn6)
Manhattan and Q-Qplots
Blocks of methylation and background blocks

Citation
If you use DMRichR in published research please cite the following 3 articles:
Laufer BI, Hwang H, Vogel Ciernia A, Mordaunt CE, LaSalle JM. Whole genome bisulfite sequencing of Down syndrome brain reveals regional DNA hypermethylation and novel disease insights. Epigenetics, 2019. doi: 10.1080/15592294.2019.1609867
Korthauer K, Chakraborty S, Benjamini Y, and Irizarry RA. Detection and accurate false discovery rate control of differentially methylated regions from whole genome bisulfite sequencing. Biostatistics, 2018. doi: 10.1093/biostatistics/kxy007
Hansen KD, Langmead B, Irizarry RA. BSmooth: from whole genome bisulfite sequencing reads to differentially methylated regions. Genome Biology, 2012. doi: 10.1186/gb-2012-13-10-r83
Acknowledgements
This workflow is primarily based on the dmrseq and bsseq bioconductor packages. I would like to thank Keegan Korthauer, the creator of dmrseq, for helpful conceptual advice in establishing and optimizing this workflow. I would like to thank Matt Settles from the UC Davis Bioinformatics Core for advice on creating an R package and use of the tidyverse and also for help with the UC Davis example. I would like to thank Rochelle Coulson for a script that was developed into the PCA function. I would also like to thank Blythe Durbin-Johnson and Annie Vogel Ciernia for statistical consulting that enabled the global and chromosomal methylation statistics. I would like to thank Nikhil Joshi from the UC Davis Bioinformatics Core for troubleshooting of a resource issue. Finally, I would like to thank Ian Korf for invaluable discussions related to the bioinformatic approaches utilized in this repository.
",3
86,IamTinashe/College-Hub,Vue,"College-Hub

Web-based accommodation service

Build Setup
# install dependencies
$ npm install

# serve with hot reload at localhost:3000
$ npm run dev

# build for production and launch server
$ npm run build
$ npm start

# generate static project
$ npm run generate
For detailed explanation on how things work, checkout Nuxt.js docs.
",2
87,EetuPe/Bounce,JavaScript,"Bounce
Bounce (or bounce 2 : the ultimate remaster (as you wish)) - Is a Javascript game created by: Miika Vuorio, Danila Karpov and Eetu Petänen. It was created in honour of the original ""Bounce"" made for Nokia phones in early 2000s.
",3
88,pharo-project/pharo,Smalltalk,"Pharo
This repository contains sources of the Pharo language. Pharo is a pure object-oriented programming language and a powerful environment, focused on simplicity and immediate feedback (think IDE and OS rolled into one).

Download Pharo
To download the Pharo stable version for your platform, please visit:

http://pharo.org/download

Virtual machine
This repository contains only sources of the Pharo image. The virtual machine is served by a separate repository:

https://github.com/OpenSmalltalk/opensmalltalk-vm

Automated Builds
This repository is being built on a Jenkins server and uploaded to files.pharo.org.

Latest build - 64bit
Latest build - 32bit

The minimal image contains the basic Pharo packages without the graphical user interface. It is useful as a base for server-side applications deployment.

Minimal image latest build - 64bit
Minimal image latest build - 32bit

Bootstrapping Pharo from sources
To bootstrap a new Pharo image you need the latest stable version of Pharo. For more information about bootstrapping, refer to guillep/PharoBootstrap.
The bootstrapping can be done using the following script:
BUILD_NUMBER=42 BOOTSTRAP_ARCH=32 bash ./bootstrap/scripts/bootstrap.sh
This will generate and archive images at various stages of the bootstrap process up to the full image in Pharo7.0-32bit-hhhhhhh.zip where hhhhhhh is the identifying hash.
Additional information on the stages of the bootstrap and how to snapshot during the process are provided as comments in bootstrap.sh.
Tip: You can set BOOTSTRAP_REPOSITORY and BOOTSTRAP_CACHE environment variables to do the bootstrap outside of the source repository.
File format
This source code repository is exported in Tonel format. In this format, packages are represented as directories and each class is inside a single file.
How to contribute
Pharo is an opensource project very friendly to contributions of the users. See the document CONTRIBUTING how you can help to improve Pharo.
",287
89,trueos/trueos,C,"
TrueOS Source:
This is the top level TrueOS source directory. It is more or less a fork
of FreeBSD.
TrueOS Differences:
In what ways does TrueOS differ from stock FreeBSD you may be wondering?
Read on for a list of the distinctions in no particular order:


Bi-Annual Release Cycle - TrueOS follows traditional FreeBSD HEAD and cuts new releases
on a 6 months schedule which includes OS and Ports.


GitHub - TrueOS uses Git/GitHub as the ""Source of truth"", Pull-Requests welcome!


CI - Our Jenkins build cluster is constantly building new versions of TrueOS for testing.


dhcpcd - dhcpcd has been integrated directly into base,
allowing more advanced IPv4 and IPv6 network configuration


OpenRC - This replaces the legacy rc.d scripts with
OpenRC's init.d subsystem, allowing faster boots, as well as a host of other service improvements.


Package Base - TrueOS is installed and based on using Packages for the Base OS.


Root NSS Certs - Since it really is a bummer to not be able to use HTTPS out of box...


Custom Installer - TrueOS includes its own pc-sysinstall installation system, along with
text-based front-end. This allows a wide variety of ZFS-based installation options, as well
as scriptability.


JSON Build Manifest - TrueOS supports a customizable JSON manifest for building. This allows TrueOS to run poudriere and assemble installation images for a variety of use-cases.


Single repo - Base packages and ports now share a single repository which allows base packages to depend upon ports, such as jq for -devel packages


More as they come...


Build Instructions:
The following instructions may be used to generate TrueOS installation
images:
make buildworld buildkernel
make packages
cd release && make iso

If you want to re-use the base packages and just re-run port builds, you may do so with:
make ports

To change the default manifest from CI to something else, set the TRUEOS_MANIFEST variable to the full path. I.E:
# setenv TRUEOS_MANIFEST /usr/src/release/manifests/trueos-master.json
# make buildworld buildkernel
 ...

Using Poudriere:
Since TrueOS uses an external toolchain, an additional step is required before using stock Poudriere. This is to install the base pkg environment and boot-strap the external compiler. To create a Poudriere jail on TrueOS, run the following command assuming you have poudriere-trueos installed:
poudriere jail -c -j trueos -m trueos -v 13.0

Checkout the portstree with git.
poudriere ports -c -p default -m git -U https://github.com/trueos/trueos-ports -B trueos-master

and to update jail to the latest trueos snapshot you can use
poudriere jail -u -j trueos

",55
90,machsix/Super-preloader,JavaScript,"
     
Super-preloader
Document
Complete Document
Difference between verions:

Super_preloaderPlus_one_New_legacy.user.js: old version doesn't work with GM4 but doesn't use Promise at all
Super_preloaderPlus_one_New.user.js: stable version synced with Greasefork but using old javascript syntax
Super_preloaderPlus_one_ES8.user.js: version in active development

Introduction
A gm script for auto loading paginated web pages. It will join pages together based on the rules.
The script is forked from https://greasyfork.org/en/scripts/10433-super-preloaderplus-one.
Thanks for the original author swdyh && ywzhaiqi && NLF
swdyh is still actively improving rules and developping the extension version,AutoPagerize.
This userscript exists because someone needs to maintain the rules for Chinese and English users since swdyh's rules are mainly for Japanese websites. Don't be afraid of the fact that most feedbacks are in Chinese. I would like to add rules for English users if I could. Feel free to leave feedback.
Please leave feedback at Greasefork page
Development Guide
I appreciate anyone who is interested in devoting their time to the development. We currently have two verions of the code. The ES8 branch is in active development. As the name suggests, I will gradually abandon the further development of the code style code.
To take part into the development, you need to:

Be familiar with javascript, xpath/css selector and node js
Install modules by yarn
Make your modifications
Run yarn test to check bugs
Run yarn run format to format the code based on our code style
Commit and submit pull request!

Check more at Complete Document
Contributers
MachX💻 🎨 🖋 👀suchunchen💻 🖋YFdyh000💻 🎨
This project follows the all-contributors specification. Contributions of any kind welcome!
Donation

License

This program is licensed under GNU General Public License Version 3 or later.
This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
You should have received a copy of the GNU General Public License along with this program.  If not, see http://www.gnu.org/licenses/.
",17
91,ben-laufer/DMRichR,R,"DMRichR
A workflow for the statistical analysis and visualization of differentially methylated regions (DMRs) of CpG count matrices (Bismark cytosine reports) from the CpG_Me pipeline.
Table of Contents

DMR Approach and Interpretation
Installation
The Design Matrix and Covariates
Input

Generic Example
UC Davis Example


Output
Citation
Acknowledgements

DMR Approach and Interpretation
The main statistical approach applied by the executable script located in the exec folder is dmrseq::dmrseq(), which identifies DMRs in a two step approach:

DMR Detection: The differences in CpG methylation for the effect of interest are pooled and smoothed to give CpG sites with higher coverage a higher weight, and candidate DMRs with a difference between groups are assembled.
Statistical Analysis: A region statistic for each DMR, which is comparable across the genome, is estimated via the application of a generalized least squares (GLS) regression model with a nested autoregressive correlated error structure for the effect of interest. Then, permutation testing of a pooled null distribution enables the identification of significant DMRs This approach accounts for both inter-individual and inter-CpG variability across the entire genome.

The main estimate of a difference in methylation between groups is not a fold change but rather a beta coefficient, which is representative of the average effect size; however, it is on the scale of the arcsine transformed differences and must be divided by π (3.14) to be similar to the mean methylation difference over a DMR, which is provided in the percentDifference column. Since the testing is permutation based, it provides empirical p-values as well as FDR corrected q-values.
One of the key differences between dmrseq and other DMR identification packages, like bsseq, is that dmrseq is performing statistical testing on the DMRs themselves rather than testing for differences in single CpGs that are then assembled into DMRs like bsseq::dmrFinder() does. This unique approach helps with controlling the false discovery rate and testing the correlated nature of CpG sites in a regulatory region, while also enabling complex experimental designs. However, since dmrseq::dmrseq() does not provide individual smoothed methylation values, bsseq::BSmooth() is utlized to generate individual smoothed methylation values from the DMRs. Therefore, while the DMRs themselves are adjusted for covariates, the indvidual smoothed methylation values for these DMRs are not adjusted for covaraites.
You can also read my general summary of the drmseq approach on EpiGenie.
Example DMR

Each dot represents the methylation level of an individual CpG in a single sample, where the size of the dot is representative of coverage. The lines represent smoothed methylation levels for each sample, either control (blue) or DS (red). Genic and CpG annotations are shown below the plot.
Installation
No manual installation of R packages is required, since the required packages and updates will occur automatically upon running the executable script located in the exec folder. However, the package does require Bioconductor 3.8, which you can install or update to using:
if (!requireNamespace(""BiocManager"", quietly = TRUE))
install.packages(""BiocManager"")
BiocManager::install(""BiocInstaller"", version = ""3.8"")

Additionally, if you are interested in creating your own workflow as opposed to using the executable script, you can download the package using:
BiocManager::install(c(""remotes"", ""ben-laufer/DMRichR""))
The Design Matrix and Covariates
This script requires a basic design matrix to identify the groups and covariates, which should be named sample_info.xlsx and contain header columns to identify the factor. It is important to have the label for the experimental samples start with a letter in the alphabet that comes after the one used for control samples in order to obtain results for experimental vs. control rather than control vs. experimental. You can select which specific samples to analyze from the working directory through the design matrix, where pattern matching of the sample name will only select bismark cytosine report files with a matching name before the first underscore, which also means that sample names should not contain underscores. Within the script, covariates can be selected for adjustment. There are two different ways to adjust for covariates: directly adjust values or balance permutations.



Name
Diagnosis
Age
Sex




SRR3537014
Idiopathic_ASD
14
M


SRR3536981
Control
42
F



Input
Before running the executable, ensure you have the following project directory tree structure for the Bismark cytosine reports and design matrix:
├── Project
│   ├── cytosine_reports
│   │   ├── sample1_bismark_bt2.deduplicated.bismark.cov.gz.CpG_report.txt.gz
│   │   ├── sample2_bismark_bt2.deduplicated.bismark.cov.gz.CpG_report.txt.gz
│   │   ├── sample_info.xlsx

This workflow requires the following variables:

-g --genome Select either: hg38, mm10, rn6, or rheMac8.
-x --coverage CpG coverage cutoff for all samples, 1x is the default and minimum value.
-s --perGroup Percent of samples per a group for CpG coverage cutoff, values range from 0 to 1, 1 (100%) is the default.
-m --minCpGs Minimum number of CpGs for a DMR, 5 is default.
-p --maxPerms Number of permutations for DMR and block analyses, 10 is default.
-o --cutoff The cutoff value for the single CpG coefficient utilized to discover testable background regions, values range from 0 to 1, 0.05 (5%) is the default.
-t --testCovariate Covariate to test for significant differences between experimental and control, i.e. Diagnosis.
-a --adjustCovariate Adjust covariates that are continuous or contain two or more factor groups, i.e. ""Age"". More than one covariate can be adjusted for using single brackets and the ; delimiter, i.e. 'BMI;Smoking'
-m --matchCovariate Covariate to balance permutations, which is meant for two-group factor covariates in small sample sizes in order to prevent extremely unbalanced permutations. Only one covariate two-group factor can be balanced, i.e. Sex. Note: This will not work for larger sample sizes (> 500,000 permutations) and is not needed for them as the odds of sampling an extremely unbalanced permutation for a covariate decreases with increasing sample size.
-c --cores The number of cores to use, 20 is recommended but you can go as low as 1, 8 is the default.

Generic Example
Below is an example of how to execute the main R script (DM.R) in the exec folder on command line. This should be called from the working directory that contains the cytosine reports.
call=""Rscript \
--vanilla \
/share/lasallelab/programs/DMRichR/DM.R \
--genome hg38 \
--coverage 1 \
--perGroup '1' \
--minCpGs 5 \
--maxPerms 10 \
--cutoff '0.05' \
--testCovariate Diagnosis \
--adjustCovariate 'BMI;Smoking' \
--matchCovariate Sex \
--cores 20""

echo $call
eval $call

UC Davis Example
If you are using the Barbera cluster at UC Davis, the following commands can be used to execute DM.R from your login node (i.e. epigenerate), where htop should be called first to make sure the whole node is available. This should be called from the working directory that contains the cytosine reports and not from within a screen.
module load R

call=""nohup \
Rscript \
--vanilla \
/share/lasallelab/programs/DMRichR/DM.R \
--genome hg38 \
--coverage 1 \
--perGroup '1' \
--minCpGs 5 \
--maxPerms 10 \
--cutoff '0.05' \
--testCovariate Diagnosis \
--adjustCovariate 'BMI;Smoking' \
--matchCovariate Sex \
--cores 60 \
> DMRichR.log 2>&1 &""

echo $call
eval $call 
echo $! > save_pid.txt

You can then check on the job using tail -f DMRichR.log and ⌃ Control + c to exit the log view.
You can cancel the job from the project directory using cat save_pid.txt | xargs kill. You can also check your running jobs using ps -ef | grep , which should be followed by your username i.e. ps -ef | grep blaufer. Finally, if you still see leftover processes in htop, you can cancel all your processes using pkill -u, which should be followed by your username i.e. pkill -u blaufer.
Alternatively, the executable can also be submitted to the cluster using the shell script via sbatch DM.R.sh.
Output
This workflow provides the following files:

CpG methylation and coverage value distribution plots
DMRs and background regions
Individual smoothed methylation values for DMRs, background regions, and windows/bins
Smoothed global and chromosomal methylation values and statistics
Heatmap and region plots of individual smoothed methylation values for DMRs
PCA plots of 20 Kb windows (all genomes) and CpG island windows (hg38, mm10, and rn6)
Gene ontology and pathway enrichments (enrichr for all genomes, GREAT for hg38 and mm10)
Gene region and CpG annotations and plots (hg38, mm10, or rn6)
Manhattan and Q-Qplots
Blocks of methylation and background blocks

Citation
If you use DMRichR in published research please cite the following 3 articles:
Laufer BI, Hwang H, Vogel Ciernia A, Mordaunt CE, LaSalle JM. Whole genome bisulfite sequencing of Down syndrome brain reveals regional DNA hypermethylation and novel disease insights. Epigenetics, 2019. doi: 10.1080/15592294.2019.1609867
Korthauer K, Chakraborty S, Benjamini Y, and Irizarry RA. Detection and accurate false discovery rate control of differentially methylated regions from whole genome bisulfite sequencing. Biostatistics, 2018. doi: 10.1093/biostatistics/kxy007
Hansen KD, Langmead B, Irizarry RA. BSmooth: from whole genome bisulfite sequencing reads to differentially methylated regions. Genome Biology, 2012. doi: 10.1186/gb-2012-13-10-r83
Acknowledgements
This workflow is primarily based on the dmrseq and bsseq bioconductor packages. I would like to thank Keegan Korthauer, the creator of dmrseq, for helpful conceptual advice in establishing and optimizing this workflow. I would like to thank Matt Settles from the UC Davis Bioinformatics Core for advice on creating an R package and use of the tidyverse and also for help with the UC Davis example. I would like to thank Rochelle Coulson for a script that was developed into the PCA function. I would also like to thank Blythe Durbin-Johnson and Annie Vogel Ciernia for statistical consulting that enabled the global and chromosomal methylation statistics. I would like to thank Nikhil Joshi from the UC Davis Bioinformatics Core for troubleshooting of a resource issue. Finally, I would like to thank Ian Korf for invaluable discussions related to the bioinformatic approaches utilized in this repository.
",3
92,IamTinashe/College-Hub,Vue,"College-Hub

Web-based accommodation service

Build Setup
# install dependencies
$ npm install

# serve with hot reload at localhost:3000
$ npm run dev

# build for production and launch server
$ npm run build
$ npm start

# generate static project
$ npm run generate
For detailed explanation on how things work, checkout Nuxt.js docs.
",2
93,EetuPe/Bounce,JavaScript,"Bounce
Bounce (or bounce 2 : the ultimate remaster (as you wish)) - Is a Javascript game created by: Miika Vuorio, Danila Karpov and Eetu Petänen. It was created in honour of the original ""Bounce"" made for Nokia phones in early 2000s.
",3
94,pharo-project/pharo,Smalltalk,"Pharo
This repository contains sources of the Pharo language. Pharo is a pure object-oriented programming language and a powerful environment, focused on simplicity and immediate feedback (think IDE and OS rolled into one).

Download Pharo
To download the Pharo stable version for your platform, please visit:

http://pharo.org/download

Virtual machine
This repository contains only sources of the Pharo image. The virtual machine is served by a separate repository:

https://github.com/OpenSmalltalk/opensmalltalk-vm

Automated Builds
This repository is being built on a Jenkins server and uploaded to files.pharo.org.

Latest build - 64bit
Latest build - 32bit

The minimal image contains the basic Pharo packages without the graphical user interface. It is useful as a base for server-side applications deployment.

Minimal image latest build - 64bit
Minimal image latest build - 32bit

Bootstrapping Pharo from sources
To bootstrap a new Pharo image you need the latest stable version of Pharo. For more information about bootstrapping, refer to guillep/PharoBootstrap.
The bootstrapping can be done using the following script:
BUILD_NUMBER=42 BOOTSTRAP_ARCH=32 bash ./bootstrap/scripts/bootstrap.sh
This will generate and archive images at various stages of the bootstrap process up to the full image in Pharo7.0-32bit-hhhhhhh.zip where hhhhhhh is the identifying hash.
Additional information on the stages of the bootstrap and how to snapshot during the process are provided as comments in bootstrap.sh.
Tip: You can set BOOTSTRAP_REPOSITORY and BOOTSTRAP_CACHE environment variables to do the bootstrap outside of the source repository.
File format
This source code repository is exported in Tonel format. In this format, packages are represented as directories and each class is inside a single file.
How to contribute
Pharo is an opensource project very friendly to contributions of the users. See the document CONTRIBUTING how you can help to improve Pharo.
",287
95,trueos/trueos,C,"
TrueOS Source:
This is the top level TrueOS source directory. It is more or less a fork
of FreeBSD.
TrueOS Differences:
In what ways does TrueOS differ from stock FreeBSD you may be wondering?
Read on for a list of the distinctions in no particular order:


Bi-Annual Release Cycle - TrueOS follows traditional FreeBSD HEAD and cuts new releases
on a 6 months schedule which includes OS and Ports.


GitHub - TrueOS uses Git/GitHub as the ""Source of truth"", Pull-Requests welcome!


CI - Our Jenkins build cluster is constantly building new versions of TrueOS for testing.


dhcpcd - dhcpcd has been integrated directly into base,
allowing more advanced IPv4 and IPv6 network configuration


OpenRC - This replaces the legacy rc.d scripts with
OpenRC's init.d subsystem, allowing faster boots, as well as a host of other service improvements.


Package Base - TrueOS is installed and based on using Packages for the Base OS.


Root NSS Certs - Since it really is a bummer to not be able to use HTTPS out of box...


Custom Installer - TrueOS includes its own pc-sysinstall installation system, along with
text-based front-end. This allows a wide variety of ZFS-based installation options, as well
as scriptability.


JSON Build Manifest - TrueOS supports a customizable JSON manifest for building. This allows TrueOS to run poudriere and assemble installation images for a variety of use-cases.


Single repo - Base packages and ports now share a single repository which allows base packages to depend upon ports, such as jq for -devel packages


More as they come...


Build Instructions:
The following instructions may be used to generate TrueOS installation
images:
make buildworld buildkernel
make packages
cd release && make iso

If you want to re-use the base packages and just re-run port builds, you may do so with:
make ports

To change the default manifest from CI to something else, set the TRUEOS_MANIFEST variable to the full path. I.E:
# setenv TRUEOS_MANIFEST /usr/src/release/manifests/trueos-master.json
# make buildworld buildkernel
 ...

Using Poudriere:
Since TrueOS uses an external toolchain, an additional step is required before using stock Poudriere. This is to install the base pkg environment and boot-strap the external compiler. To create a Poudriere jail on TrueOS, run the following command assuming you have poudriere-trueos installed:
poudriere jail -c -j trueos -m trueos -v 13.0

Checkout the portstree with git.
poudriere ports -c -p default -m git -U https://github.com/trueos/trueos-ports -B trueos-master

and to update jail to the latest trueos snapshot you can use
poudriere jail -u -j trueos

",55
96,machsix/Super-preloader,JavaScript,"
     
Super-preloader
Document
Complete Document
Difference between verions:

Super_preloaderPlus_one_New_legacy.user.js: old version doesn't work with GM4 but doesn't use Promise at all
Super_preloaderPlus_one_New.user.js: stable version synced with Greasefork but using old javascript syntax
Super_preloaderPlus_one_ES8.user.js: version in active development

Introduction
A gm script for auto loading paginated web pages. It will join pages together based on the rules.
The script is forked from https://greasyfork.org/en/scripts/10433-super-preloaderplus-one.
Thanks for the original author swdyh && ywzhaiqi && NLF
swdyh is still actively improving rules and developping the extension version,AutoPagerize.
This userscript exists because someone needs to maintain the rules for Chinese and English users since swdyh's rules are mainly for Japanese websites. Don't be afraid of the fact that most feedbacks are in Chinese. I would like to add rules for English users if I could. Feel free to leave feedback.
Please leave feedback at Greasefork page
Development Guide
I appreciate anyone who is interested in devoting their time to the development. We currently have two verions of the code. The ES8 branch is in active development. As the name suggests, I will gradually abandon the further development of the code style code.
To take part into the development, you need to:

Be familiar with javascript, xpath/css selector and node js
Install modules by yarn
Make your modifications
Run yarn test to check bugs
Run yarn run format to format the code based on our code style
Commit and submit pull request!

Check more at Complete Document
Contributers
MachX💻 🎨 🖋 👀suchunchen💻 🖋YFdyh000💻 🎨
This project follows the all-contributors specification. Contributions of any kind welcome!
Donation

License

This program is licensed under GNU General Public License Version 3 or later.
This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
You should have received a copy of the GNU General Public License along with this program.  If not, see http://www.gnu.org/licenses/.
",17
97,dotnet/docs.tr-tr,PowerShell,"


ms.openlocfilehash
ms.sourcegitcommit
ms.translationtype
ms.contentlocale
ms.lasthandoff
ms.locfileid




0c8a16936173b1e599d018d81432ca6b73c08e53
9b552addadfb57fab0b9e7852ed4f1f1b8a42f8e
MT
tr-TR
04/23/2019
61607355



.NET belgeleri
Bu depo, .NET için kavramsal belgelerde içerir. .NET belgeleri site Bunun yanı sıra birden çok deposu oluşturulur:

Kod örnekleri ve kod parçacıkları
API başvurusu
.NET derleyici Platformu SDK başvurusu

Sorunlar ve bu depolar üçü için görevleri buraya izlenir. Büyük bir topluluk bu kaynaklara sahibiz. Sorunlara zamanında yanıt vermek için en iyi çalışmalarımız vermiyoruz. Daha fazla sınıflandırma ve sorunları çözmek için sunduğumuz yordamlar hakkında bizim ilke sorunları konu.
Biz, geliştirmek ve .NET belgeleri tamamlamak yardımcı olmak için katkılar Hoş Geldiniz. Katkıda bulunmak için bkz: projeleri .NET topluluğa katkıda bulunanlar için fikirleri. Katkıda bulunan Kılavuzu yordamlarını kullanırız yönergeler içerir. Kontrol sorunlar listesinde ilginizi çeken bir görev.
Biz, tahmin Xamarin, Mono ve Unity kullanın Bu belgeleri kazandırır.
Bu proje topluluğumuza beklenen davranışını açıklamak için katkıda bulunan Covenant tarafından tanımlanan şartları BENİMSEDİ.
Daha fazla bilgi için .NET Foundation Kullanım Şartları.
",8
98,vadxq/pushQQlove,JavaScript,"pushQQlove
weather and to remind of the time and others
dev
npm start
build to es5
npm run build
serve
npm run serve
功能

计算纪念日天数
爬取one一个每日语录
获取心中的TA当地天气信息
定时早安晚安
发送qq消息问好，推送天气等信息

how to run


配置运行环境，本项目需要node v7.6.0+版本，mongodb数据库，pm2部署工具。


进入目录，安装依赖


cd pushQQlove
npm i

配置文件，(server/config/index.js)
// mongoose path
export const dbPath = '';

// server port
export const port = 7192;

// 群id
export const group_id = 851970427

// 个人id，需加好友
export const user_id = 862235971


dbPath: mongodb
port：项目运行端口
group_id: 需要发布消息的群号
user_id： 需要发布的对象qq号
修改你需要定时提醒的服务(server/app.js)
// new timing()
let timings = new timingTask()
schedule.scheduleJob('16 58 06 * * *', () => {
  timings.init()
})

// 定时睡觉
schedule.scheduleJob('10 28 23 * * *', () => {
  timings.postMsg('缘缘，到点啦，该睡啦，晚安哟~~')
})

支持自定义文本内容，如上，第一个包含爬虫，可以自行编写代码。时间格式如下：
*    *    *    *    *    *
┬    ┬    ┬    ┬    ┬    ┬
│    │    │    │    │    |
│    │    │    │    │    └ day of week (0 - 7) (0 or 7 is Sun)
│    │    │    │    └───── month (1 - 12)
│    │    │    └────────── day of month (1 - 31)
│    │    └─────────────── hour (0 - 23)
│    └──────────────────── minute (0 - 59)
└───────────────────────── second (0 - 59, OPTIONAL)

比如：10 30 07 * * 1表示在周一7：30：10时间发送。10 30 07 * * *表示在每天7：30：10时间发送。
关于酷Q api配置问题
本项目依赖酷Q的node-sdk进行qq发送消息，目前发现图片发送根据文档发送有所出路，正待解决。
可看官方文档 https://github.com/richardchien/cqhttp-node-sdk
目录结构
pushQQlove/
   |
   ├──server/                * 主要代码目录
   │   │
   │   │──config             * 配置文件
   │   │
   │   │──controls/          * 业务逻辑主要代码
   │   │    │
   │   │    │──day           * 计算纪念日天数
   │   │    │
   │   │    │──info          * 用于保存项目和查询历史消息的api
   │   │    │
   │   │    │──spider        * 爬虫
   │   │    │
   │   │    └──timing        * 定时任务相关，包含发送qq消息相关
   │   │ 
   │   │──mongo/             * 数据库配置
   │   │ 
   │   │──route              * api路由
   │   │ 
   │   └──app                * 主入口文件
   │
   │──package.json           * 包信息
   │
   │──.babelrc               * Babel配置
   │
   └──.gitignore             * Git忽略文件配置

配置无误后，本地运行
npm start 

服务器部署,需要先安装pm2
npm i -g pm2

// 直接运行编译
pm2 start 'npm start'

// 编译后运行
npm run build
cd dist
pm2 start app.js

欢迎大家提出建议和意见，谢谢～
license：Apache Licens Version 2.0
",2
99,bajumar/Azure-Monitor-Logs,None,"My Azure Monitor Logs Repo
This repo contains everything related to Azure Monitor Logs that I have learned and created (and have time to publish) - queries, tips & tricks, etc. If I see or do something that I think will be helpful to others, I'll try to put it here.
Feedback and suggestions are always welcome.
Disclosure & Disclaimer
I am currently a Microsoft employee, however, everything that I publish in this repo is my own personal work and of my own personal opinion. Content published in this repo is not officially approved or supported by Microsoft.
",2
100,gigantum/gigantum-testing,Python,"Gigantum Testing
Automation of Gigantum testing with Selenium.
Installation
First, create and activate a Python Virtual Environment for this project.
$ python3 -m venv testenv
$ source testenv/bin/activate
$ pip3 install -r requirements.txt
Next, install the binary browser drivers, so that you can programmatically
interact with the browser.
# Web driver for Chrome/Chromium
$ brew install chromedriver

# Web driver for Firefox
$ brew install geckodriver
Starting Gigantum ""Client-Under_test""
Before running the harness, ensure the Gigantum client is installed and running
# Testing the stable build
$ pip3 install gigantum && gigantum install && gigantum start

# Testing the ""edge"" build
$ pip3 install gigantum && gigantum install -e && gigantum start -e

Usage
To run ALL tests, using regular Chrome driver.
Note, this may take a while.
# Put a valid username and password into the untracked credentials.txt
$ echo -e ""my_username\nmy_password"" > credentials.txt

# Now, run the driver!
$ python3 driver.py

To run only example tests in headless mode.
$ python3 driver.py test_examples --headless

To run ALL tests using the Firefox driver
$ python3 driver.py --firefox

Organization
The file driver.py contains the main script to prepare, execute, and clean up test runs.
The directory gigantum_tests contains Python files containing individual tests.
Tests methods must be prefaced by test_, and should use the assert method for tests.
",3
101,vadxq/pushQQlove,JavaScript,"pushQQlove
weather and to remind of the time and others
dev
npm start
build to es5
npm run build
serve
npm run serve
功能

计算纪念日天数
爬取one一个每日语录
获取心中的TA当地天气信息
定时早安晚安
发送qq消息问好，推送天气等信息

how to run


配置运行环境，本项目需要node v7.6.0+版本，mongodb数据库，pm2部署工具。


进入目录，安装依赖


cd pushQQlove
npm i

配置文件，(server/config/index.js)
// mongoose path
export const dbPath = '';

// server port
export const port = 7192;

// 群id
export const group_id = 851970427

// 个人id，需加好友
export const user_id = 862235971


dbPath: mongodb
port：项目运行端口
group_id: 需要发布消息的群号
user_id： 需要发布的对象qq号
修改你需要定时提醒的服务(server/app.js)
// new timing()
let timings = new timingTask()
schedule.scheduleJob('16 58 06 * * *', () => {
  timings.init()
})

// 定时睡觉
schedule.scheduleJob('10 28 23 * * *', () => {
  timings.postMsg('缘缘，到点啦，该睡啦，晚安哟~~')
})

支持自定义文本内容，如上，第一个包含爬虫，可以自行编写代码。时间格式如下：
*    *    *    *    *    *
┬    ┬    ┬    ┬    ┬    ┬
│    │    │    │    │    |
│    │    │    │    │    └ day of week (0 - 7) (0 or 7 is Sun)
│    │    │    │    └───── month (1 - 12)
│    │    │    └────────── day of month (1 - 31)
│    │    └─────────────── hour (0 - 23)
│    └──────────────────── minute (0 - 59)
└───────────────────────── second (0 - 59, OPTIONAL)

比如：10 30 07 * * 1表示在周一7：30：10时间发送。10 30 07 * * *表示在每天7：30：10时间发送。
关于酷Q api配置问题
本项目依赖酷Q的node-sdk进行qq发送消息，目前发现图片发送根据文档发送有所出路，正待解决。
可看官方文档 https://github.com/richardchien/cqhttp-node-sdk
目录结构
pushQQlove/
   |
   ├──server/                * 主要代码目录
   │   │
   │   │──config             * 配置文件
   │   │
   │   │──controls/          * 业务逻辑主要代码
   │   │    │
   │   │    │──day           * 计算纪念日天数
   │   │    │
   │   │    │──info          * 用于保存项目和查询历史消息的api
   │   │    │
   │   │    │──spider        * 爬虫
   │   │    │
   │   │    └──timing        * 定时任务相关，包含发送qq消息相关
   │   │ 
   │   │──mongo/             * 数据库配置
   │   │ 
   │   │──route              * api路由
   │   │ 
   │   └──app                * 主入口文件
   │
   │──package.json           * 包信息
   │
   │──.babelrc               * Babel配置
   │
   └──.gitignore             * Git忽略文件配置

配置无误后，本地运行
npm start 

服务器部署,需要先安装pm2
npm i -g pm2

// 直接运行编译
pm2 start 'npm start'

// 编译后运行
npm run build
cd dist
pm2 start app.js

欢迎大家提出建议和意见，谢谢～
license：Apache Licens Version 2.0
",2
102,bajumar/Azure-Monitor-Logs,None,"My Azure Monitor Logs Repo
This repo contains everything related to Azure Monitor Logs that I have learned and created (and have time to publish) - queries, tips & tricks, etc. If I see or do something that I think will be helpful to others, I'll try to put it here.
Feedback and suggestions are always welcome.
Disclosure & Disclaimer
I am currently a Microsoft employee, however, everything that I publish in this repo is my own personal work and of my own personal opinion. Content published in this repo is not officially approved or supported by Microsoft.
",2
103,gigantum/gigantum-testing,Python,"Gigantum Testing
Automation of Gigantum testing with Selenium.
Installation
First, create and activate a Python Virtual Environment for this project.
$ python3 -m venv testenv
$ source testenv/bin/activate
$ pip3 install -r requirements.txt
Next, install the binary browser drivers, so that you can programmatically
interact with the browser.
# Web driver for Chrome/Chromium
$ brew install chromedriver

# Web driver for Firefox
$ brew install geckodriver
Starting Gigantum ""Client-Under_test""
Before running the harness, ensure the Gigantum client is installed and running
# Testing the stable build
$ pip3 install gigantum && gigantum install && gigantum start

# Testing the ""edge"" build
$ pip3 install gigantum && gigantum install -e && gigantum start -e

Usage
To run ALL tests, using regular Chrome driver.
Note, this may take a while.
# Put a valid username and password into the untracked credentials.txt
$ echo -e ""my_username\nmy_password"" > credentials.txt

# Now, run the driver!
$ python3 driver.py

To run only example tests in headless mode.
$ python3 driver.py test_examples --headless

To run ALL tests using the Firefox driver
$ python3 driver.py --firefox

Organization
The file driver.py contains the main script to prepare, execute, and clean up test runs.
The directory gigantum_tests contains Python files containing individual tests.
Tests methods must be prefaced by test_, and should use the assert method for tests.
",3
104,dequelabs/cauldron-react,JavaScript,"Cauldron React
This project is used internally by Deque Systems and is only updated when Deque needs changes for our internal use. You are free to use this project and to learn from the patterns and techniques that we used to make the widgets accessible. However we do not intend to respond to questions, feature requests, fix bugs or integrate external pull requests unless we find ourselves sitting around one day with nothing better to do. We promise, in return, not to submit questions, feature requests, bugs and pull requests to your internal projects.

Installation
$ npm install cauldron-react --save
NOTE: it is expected that you include the css from deque-pattern-library
Demo App
To document through example and make development / manual testing easy, there is a demo app which can be started by executing:
$ yarn dev
see the demo/ directory
Build
$ yarn build
NOTE: for production build run $ yarn prepack
Test
$ yarn test
or to have tests automagically re-run when files change
$ yarn --watch
Publishing
Publishing cauldron-react to the npm registry is handled by CircleCI. All (green) commits that land in the master branch will be released as a ""canary"" version (eg 1.2.3-canary.GIT_SHA) and will be available with the @next dist tag. Additionally, all (green) tags that resemble a SEMVER version will be published as stable versions (eg 1.2.3) and available with the @latest dist tag.
To install the latest canary version, do: npm install cauldron-react@next. To install the latest stable version, do npm install cauldron-react.
To publish a stable version, you'll do something like this:
# Ensure you have the latest code
$ git checkout master
$ git pull
# Run the release script
$ npm run release
# push it
$ git push --follow-tags origin master && npm publish

",4
105,c-koi/gmic-qt,C++,"G'MIC-Qt: a versatile G'MIC plugin
Purpose
G'MIC-Qt is a versatile front-end to the image processing framework
G'MIC.  It is in fact a plugin for
GIMP, Krita, and Paint.NET, as well as a standalone application.
Authors

Sébastien Fourey
David Tschumperlé (G'MIC lib & original GTK-based plugin)

Contributors

Boudewijn Rempt boud@valdyas.org (Krita compatibility layer, work in progress)
Nicholas Hayes (Paint.NET compatibility layer, work in progress)

Translators

Jan Helebrant (Czech translation)
Frank Tegtmeyer (German translation)
chroma_ghost & bazza/pixls.us (Spanish translation)
Sébastien Fourey (French translation)
Duddy Hadiwido (Indonesian translation)
Francesco Riosa (Italian translation)
iarga / pixls.us (Dutch translation)
Alex Mozheiko (Polish translation)
maxr (Portuguese translation)
Alex Mozheiko (Russian translation)
Andrex Starodubtsev (Ukrainian translation)
LinuxToy (https://twitter.com/linuxtoy) (Chinese translation)
omiya tou tokyogeometry@github (Japanese translation)

Official (pre-release) binary packages

Available at gmic.eu

Tavis CI last build status

Master branch (Linux) 
Devel branch (Linux) 

Build instructions
By default, the gimp integration plugin is built.
QMake
qmake is simple to use but only really works in an environment where bash is available.
git clone https://github.com/dtschump/gmic.git
git clone https://github.com/c-koi/gmic-qt.git
make -C gmic/src CImg.h gmic_stdlib.h
cd gmic-qt
qmake [HOST=none|gimp|krita|paintdotnet]
make
CMake
cmake works on all platforms. The first part is the same and requires make and wget to be available. If you don't have all dependencies, cmake will warn you which ones are missing. Note that the minimum cmake version is 3.1.
git clone https://github.com/dtschump/gmic.git
git clone https://github.com/c-koi/gmic-qt.git
make -C gmic/src CImg.h gmic_stdlib.h
cd gmic-qt
Then make a build directory:
mkdir build
cd build
cmake .. [-DGMIC_QT_HOST=none|gimp|krita|paintdotnet] [-DGMIC_PATH=/path/to/gmic] [-DCMAKE_BUILD_TYPE=[Debug|Release|RelwithDebInfo]
make
",44
106,alpinelinux/aports,Shell,"Alpine Linux aports repository
This repository contains the APKBUILD files for each and every
Alpine Linux package, along with the required patches and scripts,
if any.
It also contains some extra files and directories related to testing
(and therefore, building) those packages on GitHub (via Travis and Drone.io).
If you want to contribute, please read the
contributor guide
and feel free to either submit a git patch on the Alpine aports
mailing list (alpine-aports@lists.alpinelinux.org), or to submit a
pull request on GitHub.
Git Hooks
You can find some useful git hooks in the .githooks directory.
To use them, run the following command after cloning this repository:
git config --local core.hooksPath .githooks
",340
107,GeneZharov/introversion,JavaScript,"Introversion.js
Tool for debugging JavaScript expressions. Works great with functional code.
Table of Contents

Motivation
Installation
Watchers

logV()
logF()
logM()


Timers

time(), timeEnd()
stopwatch()
timeF()
timeM()
timeRun()


Modes

Quiet mode (logV_(), v_()...)
Breakpoint mode (debV(), ...)
Guards
Mute mode (.mute)


Configuration

Default configuration (setDefaults())
Instance configuration (instance())
In-place configuration (.with())


Options

General options
Formatting options
Stack trace options
In-place options


Advanced installation

Default import
Setup in global variable
Zero-conf for Node.js


License

Motivation
Suppose you have an arrow function, and you need to know a value inside of this
function:
const fn = n => n + 1; // what's in “n”?
In order to use console.log() you'll have to rewrite this function into
multiple statements:
const fn = n => {
  console.log(n);
  return n + 1;
};
Introversion allows you to simply wrap the desired value without rewriting
anything:
const fn = n => logV(n) + 1; // log value
// ...or
const fn = logF(n => n + 1); // log every function call (arguments and return value)
React component
A real-world example for a functional React component that makes you hate
console.log().
...
renderSuggestion={item => (
  <MenuItem
    text={logV(item).name}
    onClick={_ => this.toggle(item.id)}
  />
)}
Performance
Imagine you need to check if a function call is fast enough to decide whether
you need to cache it somewhere. With Introversion you can do it by simply
wrapping the desired expression in timeRun(() => <expr>) right in JSX:
// before
<Select options={states.map(transform) /* is map() too slow? */} />

// after
<Select options={timeRun(() => states.map(transform)) /* prints 2.73ms */} />
Functional programming
Since Introversion is functional, it tries not to interfer in program's logic,
but to seamlessly proxy input and output values, so it makes it easy to debug
functional code. For example, to research a function composition for issues.
import { groupBy, omitBy, mapValues } from ""lodash/fp"";

const build = pipe([
  groupBy(o => o.key),
  logF(omitBy(x => x.length > 1)), // print what goes on the 2nd step and what comes out
  mapValues(([o]) => o.uuid)
]);
Installation
npm install introversion --save-dev
Advanced installation cases are described below (default
import, setup in global
variable, zero-conf for
Node.js)
Watchers
logV()
Alias: v() (helpful with default import: In.v())
logV() (“v” stands for “value”) merely prints an array of its arguments. The
main difference between console.log() is that the last argument is returned.
Therefore it is safe to wrap any expression in logV() without breaking your
code down.
const random = n => Math.floor(logV(Math.random()) * n) + 10;
random(1); //=> logV() [ 0.5909956243063763 ]
You can print any other values alongside with the wrapped expression. Just pass
them as arguments. Only the last argument is returned, so extra arguments won't
affect your code:
const random = n => Math.floor(logV(num, this, ""mystr"", Math.random()) * n) 10;
random(1); //=> logV [ 1, {}, 'mystr', 0.8474771121023132 ]
You can use extra arguments to distinguish different watchers from each other
in the log:
const fn = n => n > 0 ? logV(true, n * 1.25) : logV(false, n / 9);
fn(5);   //=> logV [ true, 6.25 ]
fn(-81); //=> logV [ false, -9 ]
logF()
Alias: f() (helpful with default import: In.f())
logF() (“f” stands for “function”) is designed to watch for function calls.
When a wrapped function is called, its arguments and a returned value are
logged. If a wrapped function throws an exception, that exception will be
logged and then rethrown again. A wrapped in the logF() function can be used
in the same way as an unwrapped one: all arguments, this and a returned value
will be proxied.
[1, 2].map(logF(n => 2 * n));

//=> logF()
//=> ... Params: [ 1, 0, [ 1, 2 ] ]
//=> ... Result: 2

//=> logF()
//=> ... Params: [ 2, 1, [ 1, 2 ] ]
//=> ... Result: 4
logF() can also accept additional arguments for printing just like logV()
does:
logF(""foo"", ""bar"", calculate)(1, 2, 3)

// => logF() [ ""foo"", ""bar"" ] <- extra arguments go here
// => ... Params: [ 1, 2, 3 ]
// => ... Result: 999
logM()
Alias: m() (helpful with default import: In.m())
logM() (“m” stands for “method”) is similar to logF(), but it will call
your method with correct this value. In order to use it, you need to split a
method call into an object, and a string that represents a path to the method.
A.B.C.method(5); // original call

logM(A,"".B.C.method"")(5); // wrapped method
logM(A.B,"".C.method"")(5); // ...the same
logM(A.B.C,"".method"")(5); // ...one more way to do it
The rest behavior does not differ from the logF() watcher.
Timers
time(), timeEnd()
A replacement for console.time/timeEnd() that use the most accurate source
of time available:

performance.now()
console.time/timeEnd()
Date.now()

time(); // start the timer
calculateEverything();
timeEnd(); // stop the timer

//=> timeEnd() 203 ms
Just like console timing methods, these functions accept an optional name for a
new timer. timeEnd() may also accept additional arguments for printing
alongside with the ellapsed time (not available with format: false and
console methods as a time source).
time(""label""); // start the timer named ""label""
calculateEverything();
timeEnd(""foo"", ""bar"", ""label""); // stop the timer named ""label""

//=> timeEnd() [ 'foo', 'bar', 'label' ] 203 ms
stopwatch()
When you have a sequence of actions, it is inconvenient to wrap every action in
time()...timeEnd(). In this case stopwatch api is more helpful.

stopwatch() — initially starts the timer.
lap([...args]) — prints the ellapsed time since the previous
stopwatch/lap(). Also prints optional arguments and starts a new timer
for the next lap.

stopwatch();

createTables();
lap(""created""); //=> lap() [ 'created' ] 15 ms

const rows = queryRows();
lap(""foobar"", rows, ""queried""); //=> lap() [ 'foobar', [], 'queried' ] 107 ms

populateState(rows);
lap(""populated""); //=> lap() [ 'populated' ] 768 ms
timeF()
You can wrap any function with timeF(). The result will be a function with
the same behavior as a wrapped one, but additionally it will print its
execution time of its synchronous code.
array.map(timeF(iterator));

//=> timeF() 4 ms
//=> timeF() 9 ms
//=> timeF() 1 ms
Optionally you can pass any arguments for printing:
array.map(timeF(""foo"", ""bar"", iterator));

//=> timeF() [ 'foo', 'bar' ] 4 ms
//=> timeF() [ 'foo', 'bar' ] 9 ms
//=> timeF() [ 'foo', 'bar' ] 1 ms
timeM()
Like timeF() but for methods.
// original method call
array.map(n => this.iterator(n));

// wrapped method call
array.map(n => timeM(this,"".iterator"")(n));

//=> timeM() 4 ms
//=> timeM() 9 ms
//=> timeM() 1 ms
Optionally you can pass any arguments for printing:
array.map(n => timeM(""foo"", ""bar"", this,"".iterator"")(n));

//=> timeM() [ 'foo', 'bar' ] 4 ms
//=> timeM() [ 'foo', 'bar' ] 9 ms
//=> timeM() [ 'foo', 'bar' ] 1 ms
timeRun()
Sometimes you suspect that some expression may be calculated for too long. In
this case it is convenient to wrap this expression into timeRun(() => <expr>)
that will print the ellapsed time.
// original expression
data = [calculate(src), readState()];

// wrapped expression
data = timeRun(() => [calculate(src), readState()]);

//=> timeRun() 349 ms
Optionally you can pass any arguments for printing:
data = timeRun(""data"", src, () => [calculate(src), readState()]);

//=> timeRun() [ 'data', ""DATABASE"" ] 349 ms
Modes
Quiet Mode

logV_(), alias: v_()
logF_(), alias: f_()
logM_(), alias: m_()

Sometimes you are not interested in a wrapped value itself, but you need to
know, that it was calculated. For example, in React Native an attempt to log an
event object may hang the application. Or maybe you are interested only in
printing additional arguments. For these cases, there are alternative quiet
mode watchers that don't log wrapped value itself but log all additional
arguments.
const fn = logF_(""Invoked!"", n => n + 1);
fn(2); //=> logF_() [ 'Invoked!' ]
Breakpoint Mode

debV()
debF()
debM()

Instead of printing data these functions create a breakpoint using debugger
statement. It can help to look around and walk through the call stack. An
underscore in function names symbolizes a pause in program execution.
Guards
Sometimes a watcher can produce too many outputs if it is called for too many
times. You may want to suppress excess outputs. Perhaps you need only the first
one or first ten outputs. In this case the in-place “guard” option may help.
More about the in-place configuration is described
below.
for (let i = 0; i < 1000; i++) {
  logV.with({ guard: 1 })(i); // prints only once
}
Guards need some way to distinguish one call from another to keep track of the
amount of executed calls. So if you have more than one call with a guard, you
need to explicitly identify a call with “id”"" option:
for (let i = 0; i < 1000; i++) {
  logV.with({ id: 1, guard: 1 })(i); // prints only once
  logV.with({ id: 2, guard: 10 })(arr[i]); // prints for the first 10 times
}
Mute Mode
Imagine, you have a function covered with unit tests. And 1 of 30 tests fails.
For debugging reasons, it's important to know a value deep inside of that
function. But if you log that value each time it is evaluated for every unit
test, there would be hundreds of log entries. In this case, the mute mode comes
to the rescue.
You can use a muted watcher, that is available for any watcher under the method
called mute() (e.g. logV.mute(), logV_.mute(), debV.mute(),
logF.mute(), ...). Muted watcher doesn't produce any logs or breakpoints
unless you explicitly unmute it (in the failed unit test for instance).

unmuteF(fn) — unmute everything during this function execution
unmuteRun(() => <expr>) — runs passed function and unmutes everything while
it is running
unmute() — to unmute all the muted functions
mute() — to mute everything again

Example:
// module.js
function action(x) {
  // ... big and complicated function
  const y = x * 8;
  return y ^ Math.PI;
}

// module.spec.js
describe(""action()"", () => {
  // ... lots of unit tests
  it(""should perform a complex calculation"", () => {
    const res = action(2);
    expect(res).toBe(16);
  });
});
First we need to wrap a desired expression in a muted watcher:
// module.js
function action(x) {
  ...
  return logV.mute(y) ^ Math.PI;
}
Then we need to unmute a muted watcher at the desired moment (in the failed
unit test in this case):
// module.spec.js

// unmute a function
const res = unmuteF(action)(2);

// or unmute an expression
const res = unmuteRun(() => action(2));

// or unmute anything in a low level imperative style
unmute();
const res = action(2);
mute();
Configuration
Default Configuration
You can pass any number of default options as object properties:
setDefaults({
  format: false,
  log: (...xs) => Reactotron.log(xs),
  warn: (...xs) => Reactotron.warn(xs)
});
Instance configuration
You can have many instances of Introversions with different configurations:
const InR = instance({
  format: false,
  log: (...xs) => Reactotron.log(xs),
  warn: (...xs) => Reactotron.warn(xs)
});

const InX = instance({
  format: false,
  log: (...xs) => xscript.response.write(xs.join("" "") + ""\n""),
  warn (...xs) => xscript.response.write(xs.join("" "") + ""\n"")
})
In-place configuration
Most functions have method with() for setting temporary local configuration
options only for this call.
logV.with({ depth: Infinity })(myobj);
Options
General Options


log
A function that accepts any number of any arguments and prints them to the
log.
Default: (...xs) => console.log(...xs)
Examples:
(...xs) => Reactotron.log(xs)
(...xs) => xscript.response.write(xs.join("" "") + ""\n"")


warn
A function that accepts any number of any arguments and prints them as
warnings to the log.
Default: (...xs) => console.warn(...xs)
Example:
(...xs) => Reactotron.warn(xs)


timer

""auto"" — use the most accurate source of time available
""performance"" — use performance.now()
""console"" — use console.time/timeEnd()
""date"" — use Date.now()
() => number — custom user function that returns time in milliseconds

To offer protection against timing attacks and fingerprinting, the
precision of Date.now() might get rounded depending on the environment.
So consider use of “repeat” option to increase preciseness in this case
(see below).
Default: ""auto""


clone

""auto"" — clone all the values before printing if DevTools are detected
true / false — whether to deeply clone all values for printing

Default: ""auto""


errorHandling

""warn"" — output errors as warnings and try to fallback on default behavior
""throw"" — always throw an exception on error

Default: ""warn""


devTools
For some options it is important if DevTools are connected to the program.
Introversion tries to detect DevTools with a test output to the log. To
skip it, you can explicitly specify presense of DevTools with this option.
Or you can explicitly specify all the options that depends on DevTools
(currently these are “clone”, “format”, “formatErrors”).

""auto"" — detect DevTools with a test output to the log
true / false — whether DevTools are connected

Default: ""auto""


dev
If true Introversion utilities will additionally print a configuration
object they are using and a stack trace including the detected user code
position in it that is useful for configuring “stackTraceShift” option.
Default: false


Formatting Options


format

""auto"" — detect the environment
true — optimized for browsers and sophisticated tools like DevTools
false — optimized for text output, e.g. to a terminal by Node.js

When “format” is enabled:

stringifies printed objects in a pretty way
Only single log() call is made with a single formatted string as an
argument
empty line after each output

Not formatted output:
logF() at myfunc (index.js:10:23)
... Params: [ 1, 0, [ 1, 2, 3 ] ]
... Result: 1

Formatted output:
logF() at myfunc (index.js:10:23)
--- Params ---
[ 1, 0, [ 1, 2, 3 ] ]
--- Result ---
1

Default: ""auto""


formatErrors
Similar to the “format” option, but for errors and warnings.

""auto"" — detect the environment
true — optimized for browsers and sophisticated tools like DevTools
false — optimized for text output, e.g. to a terminal by Node.js

Not formatted output:
Introversion Warning

Unknown option stakcTrace

Formatted output:
▒  Introversion
▒
▒  Unknown option stakcTrace
▒
▒  at validateConf (introversion.js:780:7)
▒  at (introversion.js:898:31)
▒  at Object.<anonymous> (index.js:35:4)
▒  at Module._compile (loader.js:723:30)
▒  at Object.Module._extensions..js (loader.js:734:10)
▒  at Module.load (loader.js:620:32)
▒  at tryModuleLoad (loader.js:560:12)

Default: ""auto""


highlight
If true, Introversion will try to highlight the output for terminals.
Default: ""auto""


inspectOptions
Options that will be proxied to the node's util.inspect()
Default: ""auto""


precision
Number of digits after the point for the time measured by
timers.
Default: 2


Stack Trace Options


stacktrace
При распечатке в лог Интроверсия умеет выводить имя функции, из которой она
была вызвана, имя файла, номер строки и столбца. Что именно выводить
настраивается этой опцией с помощью массива ключевых слов.

""auto"" — detect the environment
Array<""func"" | ""file"" | ""line"" | ""col"">
true — print everything, shorthand for [""func"", ""file"", ""line"", ""col""]
false — print nothing, shorthand for []

Default: ""auto""


stackTraceAsync

true — распечатка будет происходит асинхронно. При этом будет use
source maps and guess anonymous functions. It can make network requests
for source maps and wait for its completion. Но из-за асинхронности вывод
может выглядеть непоследовательным.
false — synchronous behavior, won't use source maps or guess anonymous
functions.
auto — try to use asynchronous behavior if available.

Default: ""auto""


stackTraceShift
Итроверсия знает на какой глубине в стеке вызовов должен находиться
пользовательский код, который её вызывал. Однако, в некоторых окружениях,
модуль интроверсии может быть обёрнут ещё в какой-то код. Например, React
Native увеличивает стэк вызовов не 1. Из-за Интроверсия может ошибиться с
положением пользовательского кода в stack trace. Эта опция позволяет
исправить эту ошибку.
logV.with({ dev: true })
// ...
// --- Dev: stacktrace ---
//  0  — at getTrace (introversion.js:989:21)
//  1  — at logVal (introversion.js:1048:3)
//  2  — at (introversion.js:1414:12)
//  3  — at (introversion.js:869:12)
// [4] — at Object.<anonymous> (2.js:5:25)
//  5  — at Module._compile (loader.js:723:30)
//  6  — at Object.Module._extensions..js (loader.js:734:10)
//  7  — at Module.load (loader.js:620:32)
You can see that Introversion suppose that user call was in the 4th
position, but actually it was in the 5th. So you can set stackTraceShift:  1 to correct this.
Default: ""auto""


In-Place Options


id
Makes a watcher/timer unique. It helps Introversion to distinguish
watchers/timers from each other. This option is required in order to use
“guard” options.
ID can be of any type, but if you are using console as a time source, then
the value will be internally converted to a string since it is required by
the console spec.


guard
Sets how may times a watcher or a timer will be functional. After the
number of calls exceedes, the call will act just like an original
value/function/method without any additional behavior like printing or
debugging.
See guards for a complete description.
Default: Infinity
const fn = logF.with({ id: 0, guard: 1 })(n => n % 2); // prints only once
[1, 2, 3, 4, 5].map(fn);

//=> logF()
//=> ... Params: [ 1, 0, [ 1, 2, 3, 4, 5 ] ]
//=> ... Result: 1


repeat
The number of times to repeat the measure in order to increase preciseness.
Especially useful with Date.now() as a time source, because its precision
is rounded.
The value can be either a number or a string with a special suffix (""K"",
""M"", ""G"") for big numbers. For example:

""5K"" stands for 5,000
""1.5M"" stands for 1,500,000
""10G"" stands for 10,000,000,000

Default: 1
timeRun.with({ repeat: 5000 })(mergeDatabases);
// ...or
timeRun.with({ repeat: ""5K"" })(mergeDatabases);

//=> timeRun() 0.113 ms


Advanced installation
Default import
Introversion has a default export:
import In from ""introversion"";

In.v(""foobar"");
In this case short aliasesv(), f(), m() and their quiet alternatives
v_(), f_(), m_() are especially helpful.
ImportJS
If you use ImportJS and want it to
automatically add Introversion's as a default import, like in the example
above, then set the desired alias in the configuration file.
// .importjs.js

module.exports = {
  aliases: { In: ""introversion"" }
};
Setup in global variable
Sometimes, it is convenient to setup Introversion in global scope. In order to
do this you can import the following script in your main file:
// src/globals.js

import In from ""introversion"";

In.setDefaults({...}); // if necessary
window.In = In; // for browser
global.In = In; // for node
Jest
If you are using Introversion in Jest unit tests:
// package.json

""jest"": {
  ""setupFiles"": [""<rootDir>/src/globals.js""]
}
Flow
If you are using Introversion with Flow, then you'll have
to specify type of the global variable with a libdef file:
// flow-typed/introversion.js

declare var In: any;
If you want to specify the shape of the API, then you can copypaste it from the
libdef
script.
Zero-conf for Node.js
Introversion can work with Node.js without need to initialize and write
imports. To make API available in scripts you need to run node with -r  introversion/init option, that will write API into the global variable In.
node -r introversion/init myfile.js
// myfile.js
In.v(""working without initialization!"");
Introversion initialized this way can be configured with environment variables:

INTROVERSION_NAME — the name for the global variable (In by default)
INTROVERSION_CONF — js object with the configuration
INTROVERSION_CONF_FILE — path to the CommonJS module that exports the
configuration object

Examples:
INTROVERSION_NAME='I' node -r introversion/init myfile.js
INTROVERSION_CONF='{ stackTrace: false }' node -r introversion/init myfile.js
INTROVERSION_CONF_FILE=~/.introversion-conf.js node -r introversion/init myfile.js
You can go further and create an alias for node with initialized Introversion
for debugging small scripts that you don't want to over bloat with extra code.
You can put these commands in ~/.bashrc, ~/.zshrc, etc. Introversion should
be installed globally in this case.
npm install -g introversion

alias nodein='node -r introversion/init'
alias babel-nodein='babel-node -r introversion/init'
alias ts-nodein='ts-node -r introversion/init'
...
License
MIT
",8
108,dOrgTech/DAOcreator,TypeScript,"DAOcreator
Wizard for setting up your own DAOstack DAO.
Interested?
The Contributor's Guide can help!
Code? Nah.
You may be intersted in some of our other Repositories. Get acquainted with the Vision, or sift through our Research.
Let's Chat!
Riot Chat.
",7
109,pytorch/pytorch.github.io,CSS,"pytorch.org site
https://pytorch.org
A static website built in Jekyll and Bootstrap for PyTorch, and its tutorials and documentation.
Prerequisites
Install the following packages before attempting to setup the project:

rbenv
ruby-build
nvm

On OSX, you can use:
brew install rbenv ruby-build nvm

Setup
Install required Ruby version:
#### You only need to run these commands if you are missing the needed Ruby version.

rbenv install `cat .ruby-version`
gem install bundler -v 1.16.3
rbenv rehash

####

bundle install
rbenv rehash

Install required Node version
nvm install
nvm use

Install Yarn
brew install yarn --ignore-dependencies
yarn install

Local Development
To run the website locally for development:
make serve

Then navigate to localhost:4000.
Note the serve task is contained in a Makefile in the root directory. We are using make as an alternative to the standard jekyll serve as we want to run yarn, which is not included in Jekyll by default.
Building the Static Site
To build the static website from source:
make build

This will build the static site at ./_site. This directory is not tracked in git.
Deployments
The website is hosted on Github Pages at https://pytorch.org.
To deploy changes, merge your latest code into the sites branch. A build will be automatically built and committed to the master branch via a CircleCI job.
To view the status of the build visit https://circleci.com/gh/pytorch/pytorch.github.io.
",76
110,pnpm/pnpm,TypeScript,"
pnpm

Fast, disk space efficient package manager








Features:

Fast. As fast as npm and Yarn.
Efficient. One version of a package is saved only ever once on a disk.
Great for multi-package repositories (a.k.a. monorepos). See the recursive commands.
Strict. A package can access only dependencies that are specified in its package.json.
Deterministic. Has a lockfile called pnpm-lock.yaml.
Works everywhere. Works on Windows, Linux and OS X.
Aliases. Install different versions of the same package or import it using a different name.

Like this project? Let people know with a tweet.
Table of Contents

Background
Install
Usage
Benchmark
Support
Contributors

Background
pnpm uses hard links and symlinks to save one version of a module only ever once on a disk.
When using npm or Yarn for example, if you have 100 projects using the same version
of lodash, you will have 100 copies of lodash on disk. With pnpm, lodash will be saved in a
single place on the disk and a hard link will put it into the node_modules where it should
be installed.
As a result, you save gigabytes of space on your disk and you have a lot faster installations!
If you'd like more details about the unique node_modules structure that pnpm creates and
why it works fine with the Node.js ecosystem, read this small article: Flat node_modules is not the only way.
Install
Using a standalone script:
curl -L https://unpkg.com/@pnpm/self-installer | node

Via npm:
npm install -g pnpm

Once you first installed pnpm, you can upgrade it using pnpm:
pnpm install -g pnpm


Do you wanna use pnpm on CI servers? See: Continuous Integration.

Usage
pnpm CLI
Just use pnpm in place of npm. For instance, to install run:
pnpm install lodash

For more advanced usage, read pnpm CLI on our website.
For using the programmatic API, use pnpm's engine: supi.
pnpx CLI
npm has a great package runner called npx.
pnpm offers the same tool via the pnpx command. The only difference is that pnpx uses pnpm for installing packages.
The following command installs a temporary create-react-app and calls it,
without polluting global installs or requiring more than one step!
pnpx create-react-app my-cool-new-app

Benchmark
pnpm is as fast as npm and Yarn. See all benchmarks here.
Benchmarks on a React app:

Support

Frequently Asked Questions
Stack Overflow
Gitter chat
Twitter
Awesome list

Contributors
This project exists thanks to all the people who contribute. [Contribute].

Backers
Thank you to all our backers! 🙏 [Become a backer]

Sponsors
Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor]










License
MIT
",5805
111,alpinelinux/aports,Shell,"Alpine Linux aports repository
This repository contains the APKBUILD files for each and every
Alpine Linux package, along with the required patches and scripts,
if any.
It also contains some extra files and directories related to testing
(and therefore, building) those packages on GitHub (via Travis and Drone.io).
If you want to contribute, please read the
contributor guide
and feel free to either submit a git patch on the Alpine aports
mailing list (alpine-aports@lists.alpinelinux.org), or to submit a
pull request on GitHub.
Git Hooks
You can find some useful git hooks in the .githooks directory.
To use them, run the following command after cloning this repository:
git config --local core.hooksPath .githooks
",340
112,GeneZharov/introversion,JavaScript,"Introversion.js
Tool for debugging JavaScript expressions. Works great with functional code.
Table of Contents

Motivation
Installation
Watchers

logV()
logF()
logM()


Timers

time(), timeEnd()
stopwatch()
timeF()
timeM()
timeRun()


Modes

Quiet mode (logV_(), v_()...)
Breakpoint mode (debV(), ...)
Guards
Mute mode (.mute)


Configuration

Default configuration (setDefaults())
Instance configuration (instance())
In-place configuration (.with())


Options

General options
Formatting options
Stack trace options
In-place options


Advanced installation

Default import
Setup in global variable
Zero-conf for Node.js


License

Motivation
Suppose you have an arrow function, and you need to know a value inside of this
function:
const fn = n => n + 1; // what's in “n”?
In order to use console.log() you'll have to rewrite this function into
multiple statements:
const fn = n => {
  console.log(n);
  return n + 1;
};
Introversion allows you to simply wrap the desired value without rewriting
anything:
const fn = n => logV(n) + 1; // log value
// ...or
const fn = logF(n => n + 1); // log every function call (arguments and return value)
React component
A real-world example for a functional React component that makes you hate
console.log().
...
renderSuggestion={item => (
  <MenuItem
    text={logV(item).name}
    onClick={_ => this.toggle(item.id)}
  />
)}
Performance
Imagine you need to check if a function call is fast enough to decide whether
you need to cache it somewhere. With Introversion you can do it by simply
wrapping the desired expression in timeRun(() => <expr>) right in JSX:
// before
<Select options={states.map(transform) /* is map() too slow? */} />

// after
<Select options={timeRun(() => states.map(transform)) /* prints 2.73ms */} />
Functional programming
Since Introversion is functional, it tries not to interfer in program's logic,
but to seamlessly proxy input and output values, so it makes it easy to debug
functional code. For example, to research a function composition for issues.
import { groupBy, omitBy, mapValues } from ""lodash/fp"";

const build = pipe([
  groupBy(o => o.key),
  logF(omitBy(x => x.length > 1)), // print what goes on the 2nd step and what comes out
  mapValues(([o]) => o.uuid)
]);
Installation
npm install introversion --save-dev
Advanced installation cases are described below (default
import, setup in global
variable, zero-conf for
Node.js)
Watchers
logV()
Alias: v() (helpful with default import: In.v())
logV() (“v” stands for “value”) merely prints an array of its arguments. The
main difference between console.log() is that the last argument is returned.
Therefore it is safe to wrap any expression in logV() without breaking your
code down.
const random = n => Math.floor(logV(Math.random()) * n) + 10;
random(1); //=> logV() [ 0.5909956243063763 ]
You can print any other values alongside with the wrapped expression. Just pass
them as arguments. Only the last argument is returned, so extra arguments won't
affect your code:
const random = n => Math.floor(logV(num, this, ""mystr"", Math.random()) * n) 10;
random(1); //=> logV [ 1, {}, 'mystr', 0.8474771121023132 ]
You can use extra arguments to distinguish different watchers from each other
in the log:
const fn = n => n > 0 ? logV(true, n * 1.25) : logV(false, n / 9);
fn(5);   //=> logV [ true, 6.25 ]
fn(-81); //=> logV [ false, -9 ]
logF()
Alias: f() (helpful with default import: In.f())
logF() (“f” stands for “function”) is designed to watch for function calls.
When a wrapped function is called, its arguments and a returned value are
logged. If a wrapped function throws an exception, that exception will be
logged and then rethrown again. A wrapped in the logF() function can be used
in the same way as an unwrapped one: all arguments, this and a returned value
will be proxied.
[1, 2].map(logF(n => 2 * n));

//=> logF()
//=> ... Params: [ 1, 0, [ 1, 2 ] ]
//=> ... Result: 2

//=> logF()
//=> ... Params: [ 2, 1, [ 1, 2 ] ]
//=> ... Result: 4
logF() can also accept additional arguments for printing just like logV()
does:
logF(""foo"", ""bar"", calculate)(1, 2, 3)

// => logF() [ ""foo"", ""bar"" ] <- extra arguments go here
// => ... Params: [ 1, 2, 3 ]
// => ... Result: 999
logM()
Alias: m() (helpful with default import: In.m())
logM() (“m” stands for “method”) is similar to logF(), but it will call
your method with correct this value. In order to use it, you need to split a
method call into an object, and a string that represents a path to the method.
A.B.C.method(5); // original call

logM(A,"".B.C.method"")(5); // wrapped method
logM(A.B,"".C.method"")(5); // ...the same
logM(A.B.C,"".method"")(5); // ...one more way to do it
The rest behavior does not differ from the logF() watcher.
Timers
time(), timeEnd()
A replacement for console.time/timeEnd() that use the most accurate source
of time available:

performance.now()
console.time/timeEnd()
Date.now()

time(); // start the timer
calculateEverything();
timeEnd(); // stop the timer

//=> timeEnd() 203 ms
Just like console timing methods, these functions accept an optional name for a
new timer. timeEnd() may also accept additional arguments for printing
alongside with the ellapsed time (not available with format: false and
console methods as a time source).
time(""label""); // start the timer named ""label""
calculateEverything();
timeEnd(""foo"", ""bar"", ""label""); // stop the timer named ""label""

//=> timeEnd() [ 'foo', 'bar', 'label' ] 203 ms
stopwatch()
When you have a sequence of actions, it is inconvenient to wrap every action in
time()...timeEnd(). In this case stopwatch api is more helpful.

stopwatch() — initially starts the timer.
lap([...args]) — prints the ellapsed time since the previous
stopwatch/lap(). Also prints optional arguments and starts a new timer
for the next lap.

stopwatch();

createTables();
lap(""created""); //=> lap() [ 'created' ] 15 ms

const rows = queryRows();
lap(""foobar"", rows, ""queried""); //=> lap() [ 'foobar', [], 'queried' ] 107 ms

populateState(rows);
lap(""populated""); //=> lap() [ 'populated' ] 768 ms
timeF()
You can wrap any function with timeF(). The result will be a function with
the same behavior as a wrapped one, but additionally it will print its
execution time of its synchronous code.
array.map(timeF(iterator));

//=> timeF() 4 ms
//=> timeF() 9 ms
//=> timeF() 1 ms
Optionally you can pass any arguments for printing:
array.map(timeF(""foo"", ""bar"", iterator));

//=> timeF() [ 'foo', 'bar' ] 4 ms
//=> timeF() [ 'foo', 'bar' ] 9 ms
//=> timeF() [ 'foo', 'bar' ] 1 ms
timeM()
Like timeF() but for methods.
// original method call
array.map(n => this.iterator(n));

// wrapped method call
array.map(n => timeM(this,"".iterator"")(n));

//=> timeM() 4 ms
//=> timeM() 9 ms
//=> timeM() 1 ms
Optionally you can pass any arguments for printing:
array.map(n => timeM(""foo"", ""bar"", this,"".iterator"")(n));

//=> timeM() [ 'foo', 'bar' ] 4 ms
//=> timeM() [ 'foo', 'bar' ] 9 ms
//=> timeM() [ 'foo', 'bar' ] 1 ms
timeRun()
Sometimes you suspect that some expression may be calculated for too long. In
this case it is convenient to wrap this expression into timeRun(() => <expr>)
that will print the ellapsed time.
// original expression
data = [calculate(src), readState()];

// wrapped expression
data = timeRun(() => [calculate(src), readState()]);

//=> timeRun() 349 ms
Optionally you can pass any arguments for printing:
data = timeRun(""data"", src, () => [calculate(src), readState()]);

//=> timeRun() [ 'data', ""DATABASE"" ] 349 ms
Modes
Quiet Mode

logV_(), alias: v_()
logF_(), alias: f_()
logM_(), alias: m_()

Sometimes you are not interested in a wrapped value itself, but you need to
know, that it was calculated. For example, in React Native an attempt to log an
event object may hang the application. Or maybe you are interested only in
printing additional arguments. For these cases, there are alternative quiet
mode watchers that don't log wrapped value itself but log all additional
arguments.
const fn = logF_(""Invoked!"", n => n + 1);
fn(2); //=> logF_() [ 'Invoked!' ]
Breakpoint Mode

debV()
debF()
debM()

Instead of printing data these functions create a breakpoint using debugger
statement. It can help to look around and walk through the call stack. An
underscore in function names symbolizes a pause in program execution.
Guards
Sometimes a watcher can produce too many outputs if it is called for too many
times. You may want to suppress excess outputs. Perhaps you need only the first
one or first ten outputs. In this case the in-place “guard” option may help.
More about the in-place configuration is described
below.
for (let i = 0; i < 1000; i++) {
  logV.with({ guard: 1 })(i); // prints only once
}
Guards need some way to distinguish one call from another to keep track of the
amount of executed calls. So if you have more than one call with a guard, you
need to explicitly identify a call with “id”"" option:
for (let i = 0; i < 1000; i++) {
  logV.with({ id: 1, guard: 1 })(i); // prints only once
  logV.with({ id: 2, guard: 10 })(arr[i]); // prints for the first 10 times
}
Mute Mode
Imagine, you have a function covered with unit tests. And 1 of 30 tests fails.
For debugging reasons, it's important to know a value deep inside of that
function. But if you log that value each time it is evaluated for every unit
test, there would be hundreds of log entries. In this case, the mute mode comes
to the rescue.
You can use a muted watcher, that is available for any watcher under the method
called mute() (e.g. logV.mute(), logV_.mute(), debV.mute(),
logF.mute(), ...). Muted watcher doesn't produce any logs or breakpoints
unless you explicitly unmute it (in the failed unit test for instance).

unmuteF(fn) — unmute everything during this function execution
unmuteRun(() => <expr>) — runs passed function and unmutes everything while
it is running
unmute() — to unmute all the muted functions
mute() — to mute everything again

Example:
// module.js
function action(x) {
  // ... big and complicated function
  const y = x * 8;
  return y ^ Math.PI;
}

// module.spec.js
describe(""action()"", () => {
  // ... lots of unit tests
  it(""should perform a complex calculation"", () => {
    const res = action(2);
    expect(res).toBe(16);
  });
});
First we need to wrap a desired expression in a muted watcher:
// module.js
function action(x) {
  ...
  return logV.mute(y) ^ Math.PI;
}
Then we need to unmute a muted watcher at the desired moment (in the failed
unit test in this case):
// module.spec.js

// unmute a function
const res = unmuteF(action)(2);

// or unmute an expression
const res = unmuteRun(() => action(2));

// or unmute anything in a low level imperative style
unmute();
const res = action(2);
mute();
Configuration
Default Configuration
You can pass any number of default options as object properties:
setDefaults({
  format: false,
  log: (...xs) => Reactotron.log(xs),
  warn: (...xs) => Reactotron.warn(xs)
});
Instance configuration
You can have many instances of Introversions with different configurations:
const InR = instance({
  format: false,
  log: (...xs) => Reactotron.log(xs),
  warn: (...xs) => Reactotron.warn(xs)
});

const InX = instance({
  format: false,
  log: (...xs) => xscript.response.write(xs.join("" "") + ""\n""),
  warn (...xs) => xscript.response.write(xs.join("" "") + ""\n"")
})
In-place configuration
Most functions have method with() for setting temporary local configuration
options only for this call.
logV.with({ depth: Infinity })(myobj);
Options
General Options


log
A function that accepts any number of any arguments and prints them to the
log.
Default: (...xs) => console.log(...xs)
Examples:
(...xs) => Reactotron.log(xs)
(...xs) => xscript.response.write(xs.join("" "") + ""\n"")


warn
A function that accepts any number of any arguments and prints them as
warnings to the log.
Default: (...xs) => console.warn(...xs)
Example:
(...xs) => Reactotron.warn(xs)


timer

""auto"" — use the most accurate source of time available
""performance"" — use performance.now()
""console"" — use console.time/timeEnd()
""date"" — use Date.now()
() => number — custom user function that returns time in milliseconds

To offer protection against timing attacks and fingerprinting, the
precision of Date.now() might get rounded depending on the environment.
So consider use of “repeat” option to increase preciseness in this case
(see below).
Default: ""auto""


clone

""auto"" — clone all the values before printing if DevTools are detected
true / false — whether to deeply clone all values for printing

Default: ""auto""


errorHandling

""warn"" — output errors as warnings and try to fallback on default behavior
""throw"" — always throw an exception on error

Default: ""warn""


devTools
For some options it is important if DevTools are connected to the program.
Introversion tries to detect DevTools with a test output to the log. To
skip it, you can explicitly specify presense of DevTools with this option.
Or you can explicitly specify all the options that depends on DevTools
(currently these are “clone”, “format”, “formatErrors”).

""auto"" — detect DevTools with a test output to the log
true / false — whether DevTools are connected

Default: ""auto""


dev
If true Introversion utilities will additionally print a configuration
object they are using and a stack trace including the detected user code
position in it that is useful for configuring “stackTraceShift” option.
Default: false


Formatting Options


format

""auto"" — detect the environment
true — optimized for browsers and sophisticated tools like DevTools
false — optimized for text output, e.g. to a terminal by Node.js

When “format” is enabled:

stringifies printed objects in a pretty way
Only single log() call is made with a single formatted string as an
argument
empty line after each output

Not formatted output:
logF() at myfunc (index.js:10:23)
... Params: [ 1, 0, [ 1, 2, 3 ] ]
... Result: 1

Formatted output:
logF() at myfunc (index.js:10:23)
--- Params ---
[ 1, 0, [ 1, 2, 3 ] ]
--- Result ---
1

Default: ""auto""


formatErrors
Similar to the “format” option, but for errors and warnings.

""auto"" — detect the environment
true — optimized for browsers and sophisticated tools like DevTools
false — optimized for text output, e.g. to a terminal by Node.js

Not formatted output:
Introversion Warning

Unknown option stakcTrace

Formatted output:
▒  Introversion
▒
▒  Unknown option stakcTrace
▒
▒  at validateConf (introversion.js:780:7)
▒  at (introversion.js:898:31)
▒  at Object.<anonymous> (index.js:35:4)
▒  at Module._compile (loader.js:723:30)
▒  at Object.Module._extensions..js (loader.js:734:10)
▒  at Module.load (loader.js:620:32)
▒  at tryModuleLoad (loader.js:560:12)

Default: ""auto""


highlight
If true, Introversion will try to highlight the output for terminals.
Default: ""auto""


inspectOptions
Options that will be proxied to the node's util.inspect()
Default: ""auto""


precision
Number of digits after the point for the time measured by
timers.
Default: 2


Stack Trace Options


stacktrace
При распечатке в лог Интроверсия умеет выводить имя функции, из которой она
была вызвана, имя файла, номер строки и столбца. Что именно выводить
настраивается этой опцией с помощью массива ключевых слов.

""auto"" — detect the environment
Array<""func"" | ""file"" | ""line"" | ""col"">
true — print everything, shorthand for [""func"", ""file"", ""line"", ""col""]
false — print nothing, shorthand for []

Default: ""auto""


stackTraceAsync

true — распечатка будет происходит асинхронно. При этом будет use
source maps and guess anonymous functions. It can make network requests
for source maps and wait for its completion. Но из-за асинхронности вывод
может выглядеть непоследовательным.
false — synchronous behavior, won't use source maps or guess anonymous
functions.
auto — try to use asynchronous behavior if available.

Default: ""auto""


stackTraceShift
Итроверсия знает на какой глубине в стеке вызовов должен находиться
пользовательский код, который её вызывал. Однако, в некоторых окружениях,
модуль интроверсии может быть обёрнут ещё в какой-то код. Например, React
Native увеличивает стэк вызовов не 1. Из-за Интроверсия может ошибиться с
положением пользовательского кода в stack trace. Эта опция позволяет
исправить эту ошибку.
logV.with({ dev: true })
// ...
// --- Dev: stacktrace ---
//  0  — at getTrace (introversion.js:989:21)
//  1  — at logVal (introversion.js:1048:3)
//  2  — at (introversion.js:1414:12)
//  3  — at (introversion.js:869:12)
// [4] — at Object.<anonymous> (2.js:5:25)
//  5  — at Module._compile (loader.js:723:30)
//  6  — at Object.Module._extensions..js (loader.js:734:10)
//  7  — at Module.load (loader.js:620:32)
You can see that Introversion suppose that user call was in the 4th
position, but actually it was in the 5th. So you can set stackTraceShift:  1 to correct this.
Default: ""auto""


In-Place Options


id
Makes a watcher/timer unique. It helps Introversion to distinguish
watchers/timers from each other. This option is required in order to use
“guard” options.
ID can be of any type, but if you are using console as a time source, then
the value will be internally converted to a string since it is required by
the console spec.


guard
Sets how may times a watcher or a timer will be functional. After the
number of calls exceedes, the call will act just like an original
value/function/method without any additional behavior like printing or
debugging.
See guards for a complete description.
Default: Infinity
const fn = logF.with({ id: 0, guard: 1 })(n => n % 2); // prints only once
[1, 2, 3, 4, 5].map(fn);

//=> logF()
//=> ... Params: [ 1, 0, [ 1, 2, 3, 4, 5 ] ]
//=> ... Result: 1


repeat
The number of times to repeat the measure in order to increase preciseness.
Especially useful with Date.now() as a time source, because its precision
is rounded.
The value can be either a number or a string with a special suffix (""K"",
""M"", ""G"") for big numbers. For example:

""5K"" stands for 5,000
""1.5M"" stands for 1,500,000
""10G"" stands for 10,000,000,000

Default: 1
timeRun.with({ repeat: 5000 })(mergeDatabases);
// ...or
timeRun.with({ repeat: ""5K"" })(mergeDatabases);

//=> timeRun() 0.113 ms


Advanced installation
Default import
Introversion has a default export:
import In from ""introversion"";

In.v(""foobar"");
In this case short aliasesv(), f(), m() and their quiet alternatives
v_(), f_(), m_() are especially helpful.
ImportJS
If you use ImportJS and want it to
automatically add Introversion's as a default import, like in the example
above, then set the desired alias in the configuration file.
// .importjs.js

module.exports = {
  aliases: { In: ""introversion"" }
};
Setup in global variable
Sometimes, it is convenient to setup Introversion in global scope. In order to
do this you can import the following script in your main file:
// src/globals.js

import In from ""introversion"";

In.setDefaults({...}); // if necessary
window.In = In; // for browser
global.In = In; // for node
Jest
If you are using Introversion in Jest unit tests:
// package.json

""jest"": {
  ""setupFiles"": [""<rootDir>/src/globals.js""]
}
Flow
If you are using Introversion with Flow, then you'll have
to specify type of the global variable with a libdef file:
// flow-typed/introversion.js

declare var In: any;
If you want to specify the shape of the API, then you can copypaste it from the
libdef
script.
Zero-conf for Node.js
Introversion can work with Node.js without need to initialize and write
imports. To make API available in scripts you need to run node with -r  introversion/init option, that will write API into the global variable In.
node -r introversion/init myfile.js
// myfile.js
In.v(""working without initialization!"");
Introversion initialized this way can be configured with environment variables:

INTROVERSION_NAME — the name for the global variable (In by default)
INTROVERSION_CONF — js object with the configuration
INTROVERSION_CONF_FILE — path to the CommonJS module that exports the
configuration object

Examples:
INTROVERSION_NAME='I' node -r introversion/init myfile.js
INTROVERSION_CONF='{ stackTrace: false }' node -r introversion/init myfile.js
INTROVERSION_CONF_FILE=~/.introversion-conf.js node -r introversion/init myfile.js
You can go further and create an alias for node with initialized Introversion
for debugging small scripts that you don't want to over bloat with extra code.
You can put these commands in ~/.bashrc, ~/.zshrc, etc. Introversion should
be installed globally in this case.
npm install -g introversion

alias nodein='node -r introversion/init'
alias babel-nodein='babel-node -r introversion/init'
alias ts-nodein='ts-node -r introversion/init'
...
License
MIT
",8
113,dOrgTech/DAOcreator,TypeScript,"DAOcreator
Wizard for setting up your own DAOstack DAO.
Interested?
The Contributor's Guide can help!
Code? Nah.
You may be intersted in some of our other Repositories. Get acquainted with the Vision, or sift through our Research.
Let's Chat!
Riot Chat.
",7
114,pytorch/pytorch.github.io,CSS,"pytorch.org site
https://pytorch.org
A static website built in Jekyll and Bootstrap for PyTorch, and its tutorials and documentation.
Prerequisites
Install the following packages before attempting to setup the project:

rbenv
ruby-build
nvm

On OSX, you can use:
brew install rbenv ruby-build nvm

Setup
Install required Ruby version:
#### You only need to run these commands if you are missing the needed Ruby version.

rbenv install `cat .ruby-version`
gem install bundler -v 1.16.3
rbenv rehash

####

bundle install
rbenv rehash

Install required Node version
nvm install
nvm use

Install Yarn
brew install yarn --ignore-dependencies
yarn install

Local Development
To run the website locally for development:
make serve

Then navigate to localhost:4000.
Note the serve task is contained in a Makefile in the root directory. We are using make as an alternative to the standard jekyll serve as we want to run yarn, which is not included in Jekyll by default.
Building the Static Site
To build the static website from source:
make build

This will build the static site at ./_site. This directory is not tracked in git.
Deployments
The website is hosted on Github Pages at https://pytorch.org.
To deploy changes, merge your latest code into the sites branch. A build will be automatically built and committed to the master branch via a CircleCI job.
To view the status of the build visit https://circleci.com/gh/pytorch/pytorch.github.io.
",76
115,pnpm/pnpm,TypeScript,"
pnpm

Fast, disk space efficient package manager








Features:

Fast. As fast as npm and Yarn.
Efficient. One version of a package is saved only ever once on a disk.
Great for multi-package repositories (a.k.a. monorepos). See the recursive commands.
Strict. A package can access only dependencies that are specified in its package.json.
Deterministic. Has a lockfile called pnpm-lock.yaml.
Works everywhere. Works on Windows, Linux and OS X.
Aliases. Install different versions of the same package or import it using a different name.

Like this project? Let people know with a tweet.
Table of Contents

Background
Install
Usage
Benchmark
Support
Contributors

Background
pnpm uses hard links and symlinks to save one version of a module only ever once on a disk.
When using npm or Yarn for example, if you have 100 projects using the same version
of lodash, you will have 100 copies of lodash on disk. With pnpm, lodash will be saved in a
single place on the disk and a hard link will put it into the node_modules where it should
be installed.
As a result, you save gigabytes of space on your disk and you have a lot faster installations!
If you'd like more details about the unique node_modules structure that pnpm creates and
why it works fine with the Node.js ecosystem, read this small article: Flat node_modules is not the only way.
Install
Using a standalone script:
curl -L https://unpkg.com/@pnpm/self-installer | node

Via npm:
npm install -g pnpm

Once you first installed pnpm, you can upgrade it using pnpm:
pnpm install -g pnpm


Do you wanna use pnpm on CI servers? See: Continuous Integration.

Usage
pnpm CLI
Just use pnpm in place of npm. For instance, to install run:
pnpm install lodash

For more advanced usage, read pnpm CLI on our website.
For using the programmatic API, use pnpm's engine: supi.
pnpx CLI
npm has a great package runner called npx.
pnpm offers the same tool via the pnpx command. The only difference is that pnpx uses pnpm for installing packages.
The following command installs a temporary create-react-app and calls it,
without polluting global installs or requiring more than one step!
pnpx create-react-app my-cool-new-app

Benchmark
pnpm is as fast as npm and Yarn. See all benchmarks here.
Benchmarks on a React app:

Support

Frequently Asked Questions
Stack Overflow
Gitter chat
Twitter
Awesome list

Contributors
This project exists thanks to all the people who contribute. [Contribute].

Backers
Thank you to all our backers! 🙏 [Become a backer]

Sponsors
Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor]










License
MIT
",5805
116,nikitavoloboev/knowledge,None,"My Knowledge Wiki 📚
This is my personal wiki where I share everything I know about this world in form of an online GitBook.
If this is your first time visiting this wiki, take a look here as it describes this wiki, its structure and goals in a lot of detail.
Using the wiki well
You can quickly search the contents of this wiki above or you can explore the tree view to the left.
You can access any entry from this wiki super fast using Alfred My Mind workflow.
Aside from this wiki, you can also explore Learn Anything for things that you can learn about.
Other things I wrote and shared
I share my knowledge in longer form by writing articles and making YouTube videos.
I also love writing code that solves various problems I have and I share it all on GitHub.
Here you can find all the things I made and shared thus far.
Make your own wiki
You can view other similar to this, continuously updated wikis, here.
Don't be afraid to create one of your own and share what you know with the world.
Contributing
If you found a mistake anywhere in this wiki, I would appreciate your help. You can quickly find any entry you wish to edit by searching for the topic and then making the changes.
I also appreciate any ideas you have on how I can improve this wiki.
Thank you
You can support me on Patreon or look into other projects I shared.
 
",573
117,Kronuz/Xapiand,C++,"Xapiand

A RESTful Search Engine
Xapiand is A Modern Highly Available Distributed RESTful Search and Storage
Engine built for the Cloud and with Data Locality in mind. It takes JSON
(or MessagePack) documents and indexes them efficiently for later retrieval.
Official site is at https://kronuz.io/Xapiand
License
MIT
",297
118,abpframework/abp,C#,"ABP

This project is the next generation of the ASP.NET Boilerplate web application framework. See the announcement.
See the official web site (abp.io) for more information.
Status
This project is in very early preview stage and it's not suggested to use it in a real project.
Documentation
See the documentation.
How to Build

Run the build-all.ps1. It will build all the solutions in this repository.

Development
Pre Requirements

Visual Studio 2017 15.9.0+

Framework
Framework solution is located under the framework folder. It has no external dependency. Just open Volo.Abp.sln by Visual Studio and start the development.
Modules/Templates
Modules and Templates have their own solutions and have local references to the framework. Unfortunately, Visual Studio has some problems with local references to projects those are out of the solution. As a workaround, you should follow the steps below in order to start developing a module/template:

Disable ""Automatically check for missing packages during build in Visual Studio"" in the Visual Studio options.



When you open a solution, first run dotnet restore in the root folder of the solution.
When you change a dependency of a project (or any of the dependencies of your projects change their dependencies), run dotnet restore again.

Contribution
ABP is an open source platform. Check the contribution guide if you want to contribute to the project.
",1540
119,iterami/Guides.htm,HTML,"iterami/Guides.htm

Contributing: https://github.com/iterami/Docs.htm/blob/gh-pages/CONTRIBUTING.md
Requires:

iterami/common



",2
120,nikitavoloboev/knowledge,None,"My Knowledge Wiki 📚
This is my personal wiki where I share everything I know about this world in form of an online GitBook.
If this is your first time visiting this wiki, take a look here as it describes this wiki, its structure and goals in a lot of detail.
Using the wiki well
You can quickly search the contents of this wiki above or you can explore the tree view to the left.
You can access any entry from this wiki super fast using Alfred My Mind workflow.
Aside from this wiki, you can also explore Learn Anything for things that you can learn about.
Other things I wrote and shared
I share my knowledge in longer form by writing articles and making YouTube videos.
I also love writing code that solves various problems I have and I share it all on GitHub.
Here you can find all the things I made and shared thus far.
Make your own wiki
You can view other similar to this, continuously updated wikis, here.
Don't be afraid to create one of your own and share what you know with the world.
Contributing
If you found a mistake anywhere in this wiki, I would appreciate your help. You can quickly find any entry you wish to edit by searching for the topic and then making the changes.
I also appreciate any ideas you have on how I can improve this wiki.
Thank you
You can support me on Patreon or look into other projects I shared.
 
",573
121,abpframework/abp,C#,"ABP

This project is the next generation of the ASP.NET Boilerplate web application framework. See the announcement.
See the official web site (abp.io) for more information.
Status
This project is in very early preview stage and it's not suggested to use it in a real project.
Documentation
See the documentation.
How to Build

Run the build-all.ps1. It will build all the solutions in this repository.

Development
Pre Requirements

Visual Studio 2017 15.9.0+

Framework
Framework solution is located under the framework folder. It has no external dependency. Just open Volo.Abp.sln by Visual Studio and start the development.
Modules/Templates
Modules and Templates have their own solutions and have local references to the framework. Unfortunately, Visual Studio has some problems with local references to projects those are out of the solution. As a workaround, you should follow the steps below in order to start developing a module/template:

Disable ""Automatically check for missing packages during build in Visual Studio"" in the Visual Studio options.



When you open a solution, first run dotnet restore in the root folder of the solution.
When you change a dependency of a project (or any of the dependencies of your projects change their dependencies), run dotnet restore again.

Contribution
ABP is an open source platform. Check the contribution guide if you want to contribute to the project.
",1540
122,iterami/Guides.htm,HTML,"iterami/Guides.htm

Contributing: https://github.com/iterami/Docs.htm/blob/gh-pages/CONTRIBUTING.md
Requires:

iterami/common



",2
123,dotnet/docs.zh-cn,PowerShell,".NET Docs
此存储库包含适用于 .NET 的概念文档。 .NET 文档站点建立在多个存储库的基础上，还包括下面这个：

代码示例和代码片段
API 参考
.NET Compiler Platform SDK 参考

可在此处跟踪所有这三个存储库的问题和任务。 我们有一个大型社区在使用这些资源。 我们将最大程度地及时答复问题。 你可在问题政策主题中详细了解问题归类和解决的过程。
欢迎大家积极参与，帮助我们改进并完成 .NET 文档。若要参与，请参阅 .NET 社区参与者项目，了解相关信息。 参与指南提供所涉及过程的说明。 或者，查看问题列表，了解你感兴趣的任务。
我们预计，Xamarin、Mono 和 Unity 也都将使用此文档。
本项目采用由“参与者协议”定义的行为准则来明确社区中的预期行为。
有关详细信息，请参阅 .NET 基础行为准则。
",265
124,OpenMx/OpenMx,C,"OpenMx






OpenMx is a Structural Equation Modeling
package that encourages users to treat model specifications as something to be generated
and manipulated programmatically.
Example models which OpenMx can fit include everything from confirmatory factor,
through multiple group, mixture distribution, categorical threshold,
modern test theory, differential equations, state space, and many others.
Models may be specified as RAM or LISREL paths, or directly in matrix algebra.
Fit functions include ML (summary and full information) and WLS.
The package is on CRAN, so the easiest way to install is just:
install.packages(""OpenMx"")
Website: https://openmx.ssri.psu.edu (with forums, example models, documentation)
Development versions
Developers commit to the master branch.  Intrepid users are encouraged to install the master branch.
On Mac OS, this can be installed as a binary via travis:
install.packages(""https://vipbg.vcu.edu/vipbg/OpenMx2/software/bin/macosx/travis/OpenMx_latest.tgz"")

The stable branch can be considered our current alpha release.
The stable branch is updated automatically when all models/passing
and models/nightly tests pass along with make cran-check.
",30
125,OfficeDev/microsoft-teams-sample-auth-node,TypeScript,"


topic
products
languages
extensions




sample



Microsoft Teams
Office 365







TypeScript







contentType
createdDate




samples
2/8/2018 5:06:47 PM







Microsoft Teams Authentication Sample
This sample demonstrates authentication in Microsoft Teams apps.
There is a version of this app running on Microsoft Azure that you can try yourself. Download the AuthBot.zip app package and then upload it into Microsoft Teams. Then start a chat with @authbot.
Getting started
Start by following the setup instructions in the Microsoft Teams Sample (Node.JS), under Steps to see the full app in Microsoft Teams, applying it to the code in this sample. The instructions in that project walk you through the following steps:

Set up a tunneling service such as ngrok.
Register a bot in Microsoft Bot Framework.
Configure the app so it runs as the registered bot.
Create an app manifest (follow the ""Manual"" instructions) and sideload the app into Microsoft Teams.

Setup
To be able to use an identity provider, first you have to register your application.
Changing app settings
This project uses the config package. The default configuration is in config\default.json.

Environment variable overrides are defined in config\custom-environment-variables.json. You can set these environment variables when running node. If you are using Visual Studio Code, you can set these in your launch.json file.
Alternatively, you can specify local modifications in config\local.json.

The instructions below assume that you're using environment variables to configure the app, and will specify the name of the variable to set.
Using Azure AD
Registering a bot with the Microsoft Bot Framework automatically creates a corresponding Azure AD application with the same name and ID.

Go to the Application Registration Portal and sign in with the same account that you used to register your bot.
Find your application in the list and click on the name to edit.
Click on ""Add platform"", choose ""Web"", then add the following redirect URLs:

https://<your_ngrok_url>/auth/azureADv1/callback
https://<your_ngrok_url>/tab/simple-end
https://<your_ngrok_url>/tab/silent-end


Scroll to the bottom of the page and click on ""Save"".
The bot uses MICROSOFT_APP_ID and MICROSOFT_APP_PASSWORD, so these should already be set. No further changes needed!

Using LinkedIn

Follow the instructions in Step 1 — Configuring your LinkedIn application to create and configure a LinkedIn application for OAuth 2.
In ""Authorized Redirect URLs"", add https://<your_ngrok_url>/auth/linkedIn/callback.
Note your app's ""Client ID"" and ""Client Secret"".
Set the environment variables (or equivalent config) LINKEDIN_CLIENT_ID = <your_client_id>, and LINKEDIN_CLIENT_SECRET = <your_client_secret>.

Using Google

Obtain OAuth2 client credentials from the Google API Console. Enable access to the Google People API.
In ""Authorized redirect URLs"", add https://<your_ngrok_url>/auth/google/callback.
Note your app's ""Client ID"" and ""Client Secret"".
Set the environment variables (or equivalent config) GOOGLE_CLIENT_ID = <your_client_id>, and GOOGLE_CLIENT_SECRET = <your_client_secret>.

Bot authentication flow


The user sends a message to the bot.
The bot determines if the user needs to sign in.

In the example, the bot stores the access token in its user data store. It asks the user to log in if it doesn't have a validated token for the selected identity provider. (View code)


The bot constructs the URL to the start page of the auth flow, and sends a card to the user with a signin action. (View code)

Like other application auth flows in Teams, the start page must be on a domain that's in your validDomains list, and on the same domain as the post-login redirect page.


When the user clicks on the button, Teams opens a popup window and navigates it to the start page.
The start page redirects the user to the identity provider's authorize endpoint. (View code)
On the provider's site, the user signs in and grants access to the bot.
The provider takes the user to the bot's OAuth redirect page, with an authorization code.
The bot redeems the authorization code for an access token, and provisionally associates the token with the user that initiated the signin flow.

In the example, the bot uses information in the OAuth state parameter to determine the id of the user that started the signin process. Before proceeding, it checks state against the expected value, to detect forged requests. (View code)
IMPORTANT: The bot puts the token in user's data store, but it is marked as ""pending validation"". The token is not used while in this state. The user has to ""complete the loop"" first by sending a verification code in Teams. This is to ensure that the user who authorized the bot with the identity provider is the same user who is chatting in Teams. This guards against ""man-in-the-middle"" attacks. (View code)


The OAuth callback renders a page that calls notifySuccess(""<verification code>""). (View code)
Teams closes the popup and sends the string given to notifySuccess() back to the bot. The bot receives an invoke message with name =  signin/verifyState.
The bot checks the incoming verification code against the code stored in the user's provisional token. (View code)
If they match, the bot marks the token as validated and ready for use. Otherwise, the auth flow fails, and the bot deletes the provisional token.

Security notes

The verification code mechanism prevents a potential ""man in the middle"" attack by requiring evidence that the user who authorized the bot in the browser is the same person as the user who is chatting with the bot. Don't remove the need for a verification code without understanding what it is protecting against, and weighing the risk against your use case and threat model.
Don't use the signin/verifyState message to pass sensitive data (e.g., access tokens) directly to your bot in plaintext. The state value should not be usable without additional information that's available only to your bot.
The Teams app sends the signin/verifyState invoke message in a way that's equivalent to the user typing a message to your bot. This means that although the user information in the message is not falsifiable, a malicious user can tamper with the payload, or send additional invoke messages that were not initiated by your app.
Store your users’ access tokens in such a way that they are encrypted at rest, especially if you are also storing refresh tokens. Consider, based on your use case and threat model, how often to rotate the encryption key. (The sample uses an in-memory store for simplicity; do not do this in your production app!)
If you are using OAuth, remember that the state parameter in the authentication request must contain a unique session token to prevent request forgery attacks. The sample uses a randomly-generated GUID.

Mobile clients
As of February 2018, the Microsoft Teams mobile clients do not fully support the signin action protocol:

If the URL provided to the signin action has a fallbackUrl query string parameter, Teams will launch that URL in the browser.
Otherwise, Teams will show an error saying that the action is not yet supported on mobile.

In the example, the mobile signin flow works the same way as on desktop, until the point where the OAuth callback page tries to send the verification code back to the bot. The bot sets the fallbackUrl query string parameter to be the same as the original url to the auth start page, so that the user goes to the same page on all platforms. (View code)
When the OAuth callback runs in a mobile browser, the call to notifySuccess() will fail silently because it's not running inside the Teams client. The window will not close and the bot won't get the verification code. To handle this case, the page has a timer that checks if it's still open after 5 seconds. If so, it asks the user to manually send the verification code via chat. The bot code is able to receive the verification code from either the signin/verifyState callback or a chat message. (View code)
To limit signing in to web and desktop clients only, you can either omit the fallbackUrl parameter, or point it to an error page that asks the user to sign in with Teams on web or desktop.
When the Teams mobile clients support the complete signin protocol, including passing the verification code via notifySuccess(), they will launch the auth start page in a popup window and ignore fallbackUrl.
",18
126,google-research/morph-net,Python,"MorphNet: Fast & Simple Resource-Constrained Learning of Deep Network Structure
[TOC]
What is MorphNet?
MorphNet is a method for learning deep network structure during training. The
key principle is continuous relaxation of the network-structure learning
problem. Specifically, activation sparsity is induced by adding regularizers
that target the consumption of specific resources such as FLOPs or model size.
When the regularizer loss is added to the training loss and their sum is
minimized via stochastic gradient descent or a similar optimizer, the learning
problem becomes a constrained optimization of the structure of the network,
under the constraint represented by the regularizer. The method was first
introduced in our CVPR 2018, paper ""MorphNet: Fast & Simple Resource-Constrained Learning of
Deep Network Structure"". A overview of the
approach as well as new device-specific latency regularizers were prestend in
GTC 2019
[slides, recording: YouTube, GTC on-demand].
In short, the MorphNet regularizer pushes weights down, and once they are small
enough, the corresponding output channels are marked for removal from the
network.
Usage
Suppose you have a working convolutional neural network for image classification
but want to shrink the model to satisfy some constraints (e.g., memory,
latency). Given an existing model (the “seed network”) and a target criterion,
MorphNet will propose a new model by adjusting the number of output channels in
each convolution layer.
Note that MorphNet does not change the topology of the network -- the proposed
model will have the same number of layers and connectivity pattern as the seed
network.
To use MorphNet, you must:


Choose a regularizer from morphnet.network_regularizers. The choice is
based on

your target cost (e.g., FLOPs, latency)
your network architecture: use Gamma regularizer if the seed network
has BatchNorm; use GroupLasso otherwise.

Note: If you use BatchNorm, you must enable the scale parameters (“gamma
variables”), i.e., by setting scale=True if you are using
tf.keras.layers.BatchNormalization.


Initialize the regularizer with a threshold and the output ops of your model
(e.g., logits for classification).
MorphNet regularizer crawls your graph starting from the output ops, and
applies regularization to some of the ops it encounters. It uses the
threshold to determine which output channels can be eliminated.


Add the regularization term to your loss.
As always, regularization loss must be scaled. We recommend to search for
the scaling hyperparameter (regularization strength) along a logarithmic
scale spanning a few orders of magnitude around 1/(initial cost). For
example, if the seed network starts with 1e9 FLOPs, explore regularization
strength around 1e-9.
Note: MorphNet does not currently add the regularization loss to the
tf.GraphKeys.REGULARIZATION_LOSSES collection; this choice is subject to
revision.
Note: Do not confuse get_regularization_term() (the loss you should add to
your training) with get_cost() (the estimated cost of the network if the
proposed structure is applied).


Train the model.
Note: We recommend using a fixed learning rate (no decay) for this step,
though this is not strictly necessary.


Save the proposed model structure with the StructureExporter.
The exported files are in JSON format. Note that as the training progresses,
the proposed model structure will change. There are no specific guidelines
on the stopping time, although you would likely want to wait for the
regularization loss (reported via summaries) to stabilize.


(Optional) Create summary ops to monitor the training progress through
TensorBoard.


Modify your model using the StructureExporter output.


Retrain the model from scratch without the MorphNet regularizer.
Note: Use the standard values for all hyperparameters (such as the learning
rate schedule).


(Optional) Uniformly expand the network to adjust the accuracy vs. cost
trade-off as desired. Alternatively, this step can be performed before
the structure learning step.


We refer to the first round of training as structure learning and the second
round as retraining.
To summarize, the key hyperparameters for MorphNet are:

Regularization strength
Alive threshold

Note that the regularizer type is not a hyperparameter because it's uniquely
determined by the metric of interest (FLOPs, latency) and the presence of
BatchNorm.
Regularizer Types
Regularizer classes can be found under network_regularizers/ directory. They
are named by the algorithm they use and the target cost they attempt to
minimize. For example, GammaFlopsRegularizer uses the batch norm gamma in
order to regularize the FLOP cost.
Regularizer Algorithms

GroupLasso is designed for models without batch norm.
Gamma is designed for
models with batch norm; it requires that batch norm scale is enabled.

Regularizer Target Costs

Flops targets the FLOP count of the inference network.
Model Size targets the number of weights of the network.
Latency optimizes for the estimated inference latency of the network, based
on the specific hardware characteristics.

Example: Adding a FLOPs Regularizer
The example below demonstrates how to use MorphNet to reduce the number of FLOPs
in your model.
from morph_net.network_regularizers import flop_regularizer
from morph_net.tools import structure_exporter

logits = build_model()

network_regularizer = flop_regularizer.GammaFlopsRegularizer(
    [logits.op], gamma_threshold=1e-3)
regularization_strength = 1e-10
regularizer_loss = (network_regularizer.get_regularization_term() * regularization_strength)

model_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits)

optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)

train_op = optimizer.minimize(model_loss + regularizer_loss)
You should monitor the progress of structure learning training via Tensorboard.
In particular, you should consider adding a summary that computes the current
MorphNet regularization loss and the cost if the currently proposed structure is
adopted.
tf.summary.scalar('RegularizationLoss', regularizer_loss)
tf.summary.scalar(network_regularizer.cost_name, network_regularizer.get_cost())

Larger values of regularization_strength will converge to smaller effective
FLOP count. If regularization_strength is large enough, the FLOP count will
collapse to zero. Conversely, if it is small enough, the FLOP count will remain
at its initial value and the network structure will not vary. The
regularization_strength parameter is your knob to control where you want to be
on the price-performance curve. The gamma_threshold parameter is used for
determining when an activation is alive.
Extracting the Architecture Learned by MorphNet
During training, you should save a JSON file that contains the learned structure
of the network, that is the count of activations in a given layer kept alive (as
opposed to removed) by MorphNet.
exporter = structure_exporter.StructureExporter(
    network_regularizer.op_regularizer_manager)

with tf.Session() as sess:
  tf.global_variables_initializer().run()
  for step in range(max_steps):
    _, structure_exporter_tensors = sess.run([train_op, exporter.tensors])
    if (step % 1000 == 0):
      exporter.populate_tensor_values(structure_exporter_tensors)
      exporter.create_file_and_save_alive_counts(train_dir, step)
Misc
Contact: morphnet@google.com
Maintainers

Elad Eban, github: eladeban
Andrew Poon
Yair Movshovitz-Attias, github: yairmov
Max Moroz

Contributors

Ariel Gordon, github: gariel-google.

",695
127,phugnguyen/SamuraiCourses,JavaScript,"Samurai Courses
Samurai Courses, a Ninja Courses clone, is a course scheduling platform that allows students to plan their schedules without having to worry about which courses will conflict with each other.
Live Demo
Technologies

Backend: Ruby on Rails/ActiveRecord/PostreSQL
Frontend: React/Redux

Features

BCrypt encryption for passwords and session tokens
Fast schedule generator

Dashboard & Schedules
Snapshots
Select School and Add Courses
Select Different Course Offerings
Generate Schedules Without Conflict
This README would normally document whatever steps are necessary to get the
application up and running.
Things you may want to cover:


Ruby version


System dependencies


Configuration


Database creation


Database initialization


How to run the test suite


Services (job queues, cache servers, search engines, etc.)


Deployment instructions


...


NOTES - STYLE BEFORE EVERYTHING
AUTH - ERRORS

AUTH

modal!!


Choose theme/ color theme

run by ryan before executing


MVP 1

layout -> choosing school -> choosing term


MVP 2

dropdown selection classes
BONUS: sections - focus on main classes


MVP 3

generate schedules


MVP 4

generate schedule views


BONNUS

saved schedules



*SCHEMA


times table


course can have many times


scrape real data


courses table enrolled bonus

current enrolled
max enrolled



calculate on the backend
*send back up to schedule slice of state


schedules

SAVED boolean



Sample State

preview slice of state for schedules



backend routes

nest courses under a school
/api/courses


required params: school term subject number





frontend routes

bonus front end routes for sharing



",3
128,google-research/morph-net,Python,"MorphNet: Fast & Simple Resource-Constrained Learning of Deep Network Structure
[TOC]
What is MorphNet?
MorphNet is a method for learning deep network structure during training. The
key principle is continuous relaxation of the network-structure learning
problem. Specifically, activation sparsity is induced by adding regularizers
that target the consumption of specific resources such as FLOPs or model size.
When the regularizer loss is added to the training loss and their sum is
minimized via stochastic gradient descent or a similar optimizer, the learning
problem becomes a constrained optimization of the structure of the network,
under the constraint represented by the regularizer. The method was first
introduced in our CVPR 2018, paper ""MorphNet: Fast & Simple Resource-Constrained Learning of
Deep Network Structure"". A overview of the
approach as well as new device-specific latency regularizers were prestend in
GTC 2019
[slides, recording: YouTube, GTC on-demand].
In short, the MorphNet regularizer pushes weights down, and once they are small
enough, the corresponding output channels are marked for removal from the
network.
Usage
Suppose you have a working convolutional neural network for image classification
but want to shrink the model to satisfy some constraints (e.g., memory,
latency). Given an existing model (the “seed network”) and a target criterion,
MorphNet will propose a new model by adjusting the number of output channels in
each convolution layer.
Note that MorphNet does not change the topology of the network -- the proposed
model will have the same number of layers and connectivity pattern as the seed
network.
To use MorphNet, you must:


Choose a regularizer from morphnet.network_regularizers. The choice is
based on

your target cost (e.g., FLOPs, latency)
your network architecture: use Gamma regularizer if the seed network
has BatchNorm; use GroupLasso otherwise.

Note: If you use BatchNorm, you must enable the scale parameters (“gamma
variables”), i.e., by setting scale=True if you are using
tf.keras.layers.BatchNormalization.


Initialize the regularizer with a threshold and the output ops of your model
(e.g., logits for classification).
MorphNet regularizer crawls your graph starting from the output ops, and
applies regularization to some of the ops it encounters. It uses the
threshold to determine which output channels can be eliminated.


Add the regularization term to your loss.
As always, regularization loss must be scaled. We recommend to search for
the scaling hyperparameter (regularization strength) along a logarithmic
scale spanning a few orders of magnitude around 1/(initial cost). For
example, if the seed network starts with 1e9 FLOPs, explore regularization
strength around 1e-9.
Note: MorphNet does not currently add the regularization loss to the
tf.GraphKeys.REGULARIZATION_LOSSES collection; this choice is subject to
revision.
Note: Do not confuse get_regularization_term() (the loss you should add to
your training) with get_cost() (the estimated cost of the network if the
proposed structure is applied).


Train the model.
Note: We recommend using a fixed learning rate (no decay) for this step,
though this is not strictly necessary.


Save the proposed model structure with the StructureExporter.
The exported files are in JSON format. Note that as the training progresses,
the proposed model structure will change. There are no specific guidelines
on the stopping time, although you would likely want to wait for the
regularization loss (reported via summaries) to stabilize.


(Optional) Create summary ops to monitor the training progress through
TensorBoard.


Modify your model using the StructureExporter output.


Retrain the model from scratch without the MorphNet regularizer.
Note: Use the standard values for all hyperparameters (such as the learning
rate schedule).


(Optional) Uniformly expand the network to adjust the accuracy vs. cost
trade-off as desired. Alternatively, this step can be performed before
the structure learning step.


We refer to the first round of training as structure learning and the second
round as retraining.
To summarize, the key hyperparameters for MorphNet are:

Regularization strength
Alive threshold

Note that the regularizer type is not a hyperparameter because it's uniquely
determined by the metric of interest (FLOPs, latency) and the presence of
BatchNorm.
Regularizer Types
Regularizer classes can be found under network_regularizers/ directory. They
are named by the algorithm they use and the target cost they attempt to
minimize. For example, GammaFlopsRegularizer uses the batch norm gamma in
order to regularize the FLOP cost.
Regularizer Algorithms

GroupLasso is designed for models without batch norm.
Gamma is designed for
models with batch norm; it requires that batch norm scale is enabled.

Regularizer Target Costs

Flops targets the FLOP count of the inference network.
Model Size targets the number of weights of the network.
Latency optimizes for the estimated inference latency of the network, based
on the specific hardware characteristics.

Example: Adding a FLOPs Regularizer
The example below demonstrates how to use MorphNet to reduce the number of FLOPs
in your model.
from morph_net.network_regularizers import flop_regularizer
from morph_net.tools import structure_exporter

logits = build_model()

network_regularizer = flop_regularizer.GammaFlopsRegularizer(
    [logits.op], gamma_threshold=1e-3)
regularization_strength = 1e-10
regularizer_loss = (network_regularizer.get_regularization_term() * regularization_strength)

model_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits)

optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)

train_op = optimizer.minimize(model_loss + regularizer_loss)
You should monitor the progress of structure learning training via Tensorboard.
In particular, you should consider adding a summary that computes the current
MorphNet regularization loss and the cost if the currently proposed structure is
adopted.
tf.summary.scalar('RegularizationLoss', regularizer_loss)
tf.summary.scalar(network_regularizer.cost_name, network_regularizer.get_cost())

Larger values of regularization_strength will converge to smaller effective
FLOP count. If regularization_strength is large enough, the FLOP count will
collapse to zero. Conversely, if it is small enough, the FLOP count will remain
at its initial value and the network structure will not vary. The
regularization_strength parameter is your knob to control where you want to be
on the price-performance curve. The gamma_threshold parameter is used for
determining when an activation is alive.
Extracting the Architecture Learned by MorphNet
During training, you should save a JSON file that contains the learned structure
of the network, that is the count of activations in a given layer kept alive (as
opposed to removed) by MorphNet.
exporter = structure_exporter.StructureExporter(
    network_regularizer.op_regularizer_manager)

with tf.Session() as sess:
  tf.global_variables_initializer().run()
  for step in range(max_steps):
    _, structure_exporter_tensors = sess.run([train_op, exporter.tensors])
    if (step % 1000 == 0):
      exporter.populate_tensor_values(structure_exporter_tensors)
      exporter.create_file_and_save_alive_counts(train_dir, step)
Misc
Contact: morphnet@google.com
Maintainers

Elad Eban, github: eladeban
Andrew Poon
Yair Movshovitz-Attias, github: yairmov
Max Moroz

Contributors

Ariel Gordon, github: gariel-google.

",695
129,phugnguyen/SamuraiCourses,JavaScript,"Samurai Courses
Samurai Courses, a Ninja Courses clone, is a course scheduling platform that allows students to plan their schedules without having to worry about which courses will conflict with each other.
Live Demo
Technologies

Backend: Ruby on Rails/ActiveRecord/PostreSQL
Frontend: React/Redux

Features

BCrypt encryption for passwords and session tokens
Fast schedule generator

Dashboard & Schedules
Snapshots
Select School and Add Courses
Select Different Course Offerings
Generate Schedules Without Conflict
This README would normally document whatever steps are necessary to get the
application up and running.
Things you may want to cover:


Ruby version


System dependencies


Configuration


Database creation


Database initialization


How to run the test suite


Services (job queues, cache servers, search engines, etc.)


Deployment instructions


...


NOTES - STYLE BEFORE EVERYTHING
AUTH - ERRORS

AUTH

modal!!


Choose theme/ color theme

run by ryan before executing


MVP 1

layout -> choosing school -> choosing term


MVP 2

dropdown selection classes
BONUS: sections - focus on main classes


MVP 3

generate schedules


MVP 4

generate schedule views


BONNUS

saved schedules



*SCHEMA


times table


course can have many times


scrape real data


courses table enrolled bonus

current enrolled
max enrolled



calculate on the backend
*send back up to schedule slice of state


schedules

SAVED boolean



Sample State

preview slice of state for schedules



backend routes

nest courses under a school
/api/courses


required params: school term subject number





frontend routes

bonus front end routes for sharing



",3
130,dotnet/docs.ja-jp,PowerShell,".NET ドキュメント
このリポジトリには、.NET の概念に関するドキュメントが含まれています。 .NET ドキュメント サイトは次のものに加え、複数のリポジトリから構築されています。

コード サンプルとスニペット
API リファレンス
.NET Compiler Platform SDK リファレンス

これら 3 つすべてのリポジトリの問題とタスクがここに記載されています。 これらのリソースを使用した大規模なコミュニティがあります。 問題に適切なタイミングで対処するように最善を努めています。 問題のポリシーに関するトピックに、問題を分類して解決するための手順を詳しく説明しています。
.NET ドキュメントの改善と完成に役立つご協力をお待ちしています。ご投稿いただく場合は、.NET コミュニティの共同作成者用のプロジェクトでさまざまなアイデアをご覧ください。 共同作成ガイドに、使用されている手順に関する説明があります。 または、関心のあるタスクの問題リストをご確認ください。
Xamarin、Mono および Unity でもこのドキュメントが使用される予定です。
このプロジェクトでは、当コミュニティで期待される動作を明らかにするために共同作成者契約で定義された倫理規定を採用しています。
詳細については、「.NET Foundation Code of Conduct」(.NET Foundation の倫理規定) を参照してください。
",28
131,noyainrain/listling,JavaScript,"Open Listling
Web app for collaboratively composing lists.
You can give it a try on Listling.
Requirements
The following software is required and must be set up on your system:

Python >= 3.5
Node.js >= 8.0
Redis >= 2.8

Open Listling should work on any POSIX system.
Installing dependencies
To install the dependencies for Open Listling, type:
make deps
Running Open Listling
To run Open Listling, type:
python3 -m listling
Browser support
Open Listling supports the latest version of popular browsers (i.e. Chrome, Edge, Firefox and
Safari; see http://caniuse.com/ ).
Deprecation policy
Features marked as deprecated are removed after a period of six months.
Community
Join the chat at
#listling on freenode.
Contributors

Sven James <sven.jms AT gmail.com>

Copyright (C) 2018 Open Listling contributors
",10
132,ESW1234/esw1234.github.io,HTML,"esw1234.github.io
",2
133,scorzy/IdleSpace,TypeScript,"IdleSpace

This is an idle game about space combat. I made it to try web workers and some Angular Matrial CDK, that's why there are drag and drop and per battle is handled per ship.
Links:

Game: https://github.com/scorzy/IdleSpace
Kongregate: https://www.kongregate.com/games/scorzy88/idle-space
Discord: https://discord.gg/CGEfHGe
Battle code: https://github.com/scorzy/IdleSpace/blob/master/src/app/battle.service.ts

This project was generated with Angular CLI version 7.2.2.
Development server
Run ng serve for a dev server. Navigate to http://localhost:4200/. The app will automatically reload if you change any of the source files.
Build
Run ng build to build the project. The build artifacts will be stored in the dist/ directory. Use the --prod flag for a production build.
Running unit tests
Run ng test to execute the unit tests via Karma.
",12
134,ethereum/lahja,Python,"Lahja

Documentation hosted by ReadTheDocs
DISCLAIMER: This is alpha state software. Expect bugs.
Lahja is a generic multi process event bus implementation written in Python 3.6+ to enable lightweight inter-process communication, based on non-blocking asyncio.
What is this for?
Lahja is tailored around one primary use case: enabling multi process Python applications to communicate through events between processes in a non-blocking
asyncio fashion.
Key facts:

non-blocking APIs based on asyncio
lightweight and simple (e.g no IPC pipes etc)
easy multicasting of events (one event, many independent receivers)
easy event routing (e.g route event X only to process group Y)
multiple consuming APIs to adapt to different use cases and styles

TODOs

Filter support (e.g. only subscribe to EventX from origin y)
Push boundaries (don't push this into process x)
Testing
Performance analysis

Developer Setup
If you would like to hack on lahja, please check out the
Ethereum Development Tactical Manual
for information on how we do:

Testing
Pull Requests
Code Style
Documentation

Development Environment Setup
You can set up your dev environment with:
git clone https://github.com/cburgdorf/lahja
cd lahja
virtualenv -p python3 venv
. venv/bin/activate
pip install -e .[dev]
Testing Setup
During development, you might like to have tests run on every file save.
Show flake8 errors on file change:
# Test flake8
when-changed -v -s -r -1 lahja/ tests/ -c ""clear; flake8 lahja tests && echo 'flake8 success' || echo 'error'""
Run multi-process tests in one command, but without color:
# in the project root:
pytest --numprocesses=4 --looponfail --maxfail=1
# the same thing, succinctly:
pytest -n 4 -f --maxfail=1
Run in one thread, with color and desktop notifications:
cd venv
ptw --onfail ""notify-send -t 5000 'Test failure ⚠⚠⚠⚠⚠' 'python 3 test on lahja failed'"" ../tests ../lahja
Release setup
For Debian-like systems:
apt install pandoc

To release a new version:
make release bump=$$VERSION_PART_TO_BUMP$$
How to bumpversion
The version format for this repo is {major}.{minor}.{patch} for stable, and
{major}.{minor}.{patch}-{stage}.{devnum} for unstable (stage can be alpha or beta).
To issue the next version in line, specify which part to bump,
like make release bump=minor or make release bump=devnum.
If you are in a beta version, make release bump=stage will switch to a stable.
To issue an unstable version when the current version is stable, specify the
new version explicitly, like make release bump=""--new-version 4.0.0-alpha.1 devnum""
",19
135,octavore/homebrew-tools,Ruby,"Tools
This repo simplifies installation of my homegrown tools, such as:

delta, a diff tool with a browser-based GUI.
lightproxy, a lightweight local proxy.

Installing
# (optional) brew update
brew install octavore/tools/delta
brew install octavore/tools/lightproxy

",2
136,dotnet/docs.ja-jp,PowerShell,".NET ドキュメント
このリポジトリには、.NET の概念に関するドキュメントが含まれています。 .NET ドキュメント サイトは次のものに加え、複数のリポジトリから構築されています。

コード サンプルとスニペット
API リファレンス
.NET Compiler Platform SDK リファレンス

これら 3 つすべてのリポジトリの問題とタスクがここに記載されています。 これらのリソースを使用した大規模なコミュニティがあります。 問題に適切なタイミングで対処するように最善を努めています。 問題のポリシーに関するトピックに、問題を分類して解決するための手順を詳しく説明しています。
.NET ドキュメントの改善と完成に役立つご協力をお待ちしています。ご投稿いただく場合は、.NET コミュニティの共同作成者用のプロジェクトでさまざまなアイデアをご覧ください。 共同作成ガイドに、使用されている手順に関する説明があります。 または、関心のあるタスクの問題リストをご確認ください。
Xamarin、Mono および Unity でもこのドキュメントが使用される予定です。
このプロジェクトでは、当コミュニティで期待される動作を明らかにするために共同作成者契約で定義された倫理規定を採用しています。
詳細については、「.NET Foundation Code of Conduct」(.NET Foundation の倫理規定) を参照してください。
",28
137,noyainrain/listling,JavaScript,"Open Listling
Web app for collaboratively composing lists.
You can give it a try on Listling.
Requirements
The following software is required and must be set up on your system:

Python >= 3.5
Node.js >= 8.0
Redis >= 2.8

Open Listling should work on any POSIX system.
Installing dependencies
To install the dependencies for Open Listling, type:
make deps
Running Open Listling
To run Open Listling, type:
python3 -m listling
Browser support
Open Listling supports the latest version of popular browsers (i.e. Chrome, Edge, Firefox and
Safari; see http://caniuse.com/ ).
Deprecation policy
Features marked as deprecated are removed after a period of six months.
Community
Join the chat at
#listling on freenode.
Contributors

Sven James <sven.jms AT gmail.com>

Copyright (C) 2018 Open Listling contributors
",10
138,ESW1234/esw1234.github.io,HTML,"esw1234.github.io
",2
139,scorzy/IdleSpace,TypeScript,"IdleSpace

This is an idle game about space combat. I made it to try web workers and some Angular Matrial CDK, that's why there are drag and drop and per battle is handled per ship.
Links:

Game: https://github.com/scorzy/IdleSpace
Kongregate: https://www.kongregate.com/games/scorzy88/idle-space
Discord: https://discord.gg/CGEfHGe
Battle code: https://github.com/scorzy/IdleSpace/blob/master/src/app/battle.service.ts

This project was generated with Angular CLI version 7.2.2.
Development server
Run ng serve for a dev server. Navigate to http://localhost:4200/. The app will automatically reload if you change any of the source files.
Build
Run ng build to build the project. The build artifacts will be stored in the dist/ directory. Use the --prod flag for a production build.
Running unit tests
Run ng test to execute the unit tests via Karma.
",12
140,ethereum/lahja,Python,"Lahja

Documentation hosted by ReadTheDocs
DISCLAIMER: This is alpha state software. Expect bugs.
Lahja is a generic multi process event bus implementation written in Python 3.6+ to enable lightweight inter-process communication, based on non-blocking asyncio.
What is this for?
Lahja is tailored around one primary use case: enabling multi process Python applications to communicate through events between processes in a non-blocking
asyncio fashion.
Key facts:

non-blocking APIs based on asyncio
lightweight and simple (e.g no IPC pipes etc)
easy multicasting of events (one event, many independent receivers)
easy event routing (e.g route event X only to process group Y)
multiple consuming APIs to adapt to different use cases and styles

TODOs

Filter support (e.g. only subscribe to EventX from origin y)
Push boundaries (don't push this into process x)
Testing
Performance analysis

Developer Setup
If you would like to hack on lahja, please check out the
Ethereum Development Tactical Manual
for information on how we do:

Testing
Pull Requests
Code Style
Documentation

Development Environment Setup
You can set up your dev environment with:
git clone https://github.com/cburgdorf/lahja
cd lahja
virtualenv -p python3 venv
. venv/bin/activate
pip install -e .[dev]
Testing Setup
During development, you might like to have tests run on every file save.
Show flake8 errors on file change:
# Test flake8
when-changed -v -s -r -1 lahja/ tests/ -c ""clear; flake8 lahja tests && echo 'flake8 success' || echo 'error'""
Run multi-process tests in one command, but without color:
# in the project root:
pytest --numprocesses=4 --looponfail --maxfail=1
# the same thing, succinctly:
pytest -n 4 -f --maxfail=1
Run in one thread, with color and desktop notifications:
cd venv
ptw --onfail ""notify-send -t 5000 'Test failure ⚠⚠⚠⚠⚠' 'python 3 test on lahja failed'"" ../tests ../lahja
Release setup
For Debian-like systems:
apt install pandoc

To release a new version:
make release bump=$$VERSION_PART_TO_BUMP$$
How to bumpversion
The version format for this repo is {major}.{minor}.{patch} for stable, and
{major}.{minor}.{patch}-{stage}.{devnum} for unstable (stage can be alpha or beta).
To issue the next version in line, specify which part to bump,
like make release bump=minor or make release bump=devnum.
If you are in a beta version, make release bump=stage will switch to a stable.
To issue an unstable version when the current version is stable, specify the
new version explicitly, like make release bump=""--new-version 4.0.0-alpha.1 devnum""
",19
141,octavore/homebrew-tools,Ruby,"Tools
This repo simplifies installation of my homegrown tools, such as:

delta, a diff tool with a browser-based GUI.
lightproxy, a lightweight local proxy.

Installing
# (optional) brew update
brew install octavore/tools/delta
brew install octavore/tools/lightproxy

",2
142,google/TensorNetwork,Python,"TensorNetwork

A tensor network wrapper for TensorFlow.
Installation
pip3 install tensornetwork

Note: The following examples assume a TensorFlow v2 interface
(in TF 1.13 or higher, run tf.enable_v2_behavior() after
importing tensorflow) but should also work with eager mode
(tf.enable_eager_execution()). The actual library does work
under graph mode, but documentation is limited.
Basic Example
Here, we build a simple 2 node contraction.
import numpy as np
import tensorflow as tf
tf.enable_v2_behavior()
import tensornetwork

# Create the network
net = tensornetwork.TensorNetwork()
# Add the nodes
a = net.add_node(np.ones((10,), dtype=np.float32)) 
# Can use either np.array or tf.Tensor and can even mix them!
b = net.add_node(tf.ones((10,)))
edge = net.connect(a[0], b[0])
final_node = net.contract(edge)
print(final_node.get_tensor().numpy()) # Should print 10.0
Node and Edge names.
You can optionally name your nodes/edges. This can be useful for debugging,
as all error messages will print the name of the broken edge/node.
net = tensornetwork.TensorNetwork()
node = net.add_node(np.eye(2), name=""Identity Matrix"")
print(""Name of node: {}"".format(node.name))
edge = net.connect(node[0], node[1], name=""Trace Edge"")
print(""Name of the edge: {}"".format(edge.name))
# Adding name to a contraction will add the name to the new edge created.
final_result = net.contract(edge, name=""Trace Of Identity"")
print(""Name of new node after contraction: {}"".format(final_result.name))
Named axes.
To make remembering what an axis does easier, you can optionally name a node's axes.
net = tensornetwork.TensorNetwork()
a = net.add_node(np.zeros((2, 2)), axis_names=[""alpha"", ""beta""])
edge = net.connect(a[""beta""], a[""alpha""])
Edge reordering.
To assert that your result's axes are in the correct order, you can reorder a node at any time during computation.
net = tensornetwork.TensorNetwork()
a = net.add_node(np.zeros((1, 2, 3)))
e1 = a[0]
e2 = a[1]
e3 = a[2]
a.reorder_edges([e3, e1, e2])
# If you already know the axis values, you can equivalently do
# a.reorder_axes([2, 0, 1])
print(a.tensor.shape) # Should print (3, 1, 2)
NCON interface.
For a more compact specification of a tensor network and its contraction, there is ncon(). For example:
from tensornetwork import ncon
a = tf.random_normal((2,2))
b = tf.random_normal((2,2))
c = ncon([a,b], [(-1,0),(0,-2)])
print(tf.norm(tf.matmul(a,b) - c)) # Should be zero
It is also possible to generate a TensorNetwork:
from tensornetwork import ncon_network
a = tf.random_normal((2,2))
b = tf.random_normal((2,2))
net, e_con, e_out = ncon_network([a,b], [(-1,0),(0,-2)])
for e in e_con:
    n = net.contract(e) # Contract edges in order
n.reorder_edges(e_out) # Permute final tensor as necessary
print(tf.norm(tf.matmul(a,b) - n.get_tensor()))
TensorNetwork is not an official Google product. Copyright 2019 The TensorNetwork Developers.
",56
143,machine-learning-apps/Issue-Label-Bot,CSS," 

Code for: ""How to automate tasks on GitHub with machine learning for fun and profit""
Table of Contents

Issue-Label Bot

Important links
Files


Running This Code

Environment Variables
Run Locally
Deploy As A Service


Contributing

Roadmap
References


Disclaimers

Original Authors: @hamelsmu, @inc0, @jlewi
Issue Label Bot
Install this app from the GitHub marketplace
A GitHub App powered by machine learning, written in python.  A discussion of the motivation for building this app is described in this blog post.
When an issue is opened, the bot predicts if the label should be a: feature request, bug or question and applies a label automatically if appropriate. Here is a screenshot of the bot in action:



More examples can be viewed on our app's homepage.  It should be noted that the bot may not apply any label in circumstances where the prediction is uncertain.  See the disclaimers section for more caveats.
Important links

Issue Label Bot homepage. Provides a way to view example predictions as well as other information regarding this bot.
GitHub App page for Issue Label Bot, where you can install the app. See disclaimers below before installing.

Files

/notebooks: contains notebooks on how to train the model and interact with the GitHub api uing a python client.
/flask_app: code for a flask app that listens for GitHub issue events and responds with predictions.  This is the main application that the user will interact with.
/argo: the code in this directory relates to the construction of Argo ML Pipelines for training and deploying ML workflows.
/deployment: This directory contains files that are helpful in deploying the app.

Dockerfile this is the definition of the container that is used to run the flask app.  The build for this container is hosted on DockerHub at hamelsmu/mlapp.
heroku.yml: this is used for deploying to Heroku.
*.yaml: these files relate to a Kubernetees deployment.



Running This Code
Prerequisites
To utilize the code in this repository, you will need to register a GitHub App of your own and install this app on your desired repositories and store authentication secrets.
First, walk through the prerequisites section of this getting started guide except The Ruby programming language"" section as we will be using python instead as the client that interfaces with the GitHub api.
Second, setup your development environment. Make sure you create a Webhook secret, even though this step is optional.
Next, setup a postgres database.  You can do this for free on Heroku.    Detailed instructions (stolen shamelessly from here):

Navigate to https://www.heroku.com/, and create an account if you don’t already have one.
On Heroku’s Dashboard, click “New” and choose “Create new app.”
Give your app a name, and click “Create app.”
On your app’s “Overview” page, click the “Configure Add-ons” button.
In the “Add-ons” section of the page, type in and select “Heroku Postgres.”
Choose the “Hobby Dev - Free” plan, which will give you access to a free PostgreSQL database that will support up to 10,000 rows of data. Click “Provision.”
Now, click the “Heroku Postgres :: Database” link.
You should now be on your database’s overview page. Click on 8 “Settings”, and then “View Credentials.” This is the information you’ll need to log into your database.

Finally, you need to create environment variables for all the secrets, which is described below.
Environment Variables

PRIVATE_KEY:  this is the private key you use to authenticate as an app with the GitHub api.
WEBHOOK_SECRET: this is used to verify that payloads received by your app are actually from GitHub.  This is described here.
DATABASE_URL: this is the URL that contains the login information for your POSTGRESQL database, usually in the form: postgres://<username>:<password>@<url>:5432/<database_name>
APP_ID: this is a unique identifier provided to you by GitHub when you register your app.
FLASK_ENV: this is usually set to either production or development.  You will want to use deployment for local testing.
PORT: this is the port your app will be serving on.  Note that if you are deploying to Heroku, Heroku will override this variable with their own value when building your app.  For local development, you will want this to match the port Smee is serving to.
APP_URL: this is the url for the homepage of your app that is provided to users as a link in issue comments.  You can set this to an arbitrary value for local development.

Note: If you are using zsh, the dotenv plugin can be useful for managing environment variables.
Run Locally


Install Dependencies: Install requirements.txt into a virtual environment.  If you are using pipenv install the necessary dependencies from Pipfile.lock by typing pipenv install in the root of this repository.


Run the flask app: run python flask_app/app.py from the root of this repository.  For this to work, you must correctly set the environment variables as described in the Environment Variables section.


Optional - Run app as docker container.  A Docker container that serves Issue-Label Bot can be built with the command bash script/bootstrap from the root of this repository.  This script builds a Docker image named hamelsmu/mlapp, which is also available on Dockerhub.  If you desire to run the Docker container locally for testing, you must pass the necessary environment variables to the Docker container at runtime, as well as expose necessary ports for the app. See the References section for more resources on using Docker.


Deploy As A Service
The assets in this repo allow you to deploy to Heroku (easier) or a Kubernetees cluster (more advanced).
In Heroku, secrets can be passed in as configuration variables.  Furthermore, this documentation describes how you can set secrets in Kubernetees.  Make sure you set the environment variable FLASK_ENV to production if you are going to deploy the app publicly.
Contributing
We welcome all forms of contributions.  We are especially interested in the following:

Bug fixes
Enhancements or additional features
Improvements to the model, or expansion of the dataset(s) used for training.

Roadmap
The authors of this project are interested in adding the following features in the near future:

Constructing better labels and negative samples of items that do not belong in the label set to drive further improvements.
Using the tools from fastai to explore:

State of the art architectures, such as Multi-Head Attention
Utilizing transfer learning to predict unique labels and/or enhance accuracy.


Using GitHub Actions to trigger automated deploys of this code.
Model pipeline orchestration on Argo pipelines.

References


The code in this repo and associated tutorial(s) assume familiarity with Docker. This blog post offers a gentle introduction to Docker for data scientists.


Need inspiration for other data products you can build using machine learning and public GitHub datasets?  See these examples:

GitHub issue summarization and recommendation.
Natural language semantic code search.



Excellent course on flask: HarvardX CS50 Web.


MOOCs by fastai for machine learning and deep learning.


Disclaimers
Issue-Label Bot is for educational and demonstration purposes only.  Our goal was to provide a minimal working example for the community with the least amount of complexity as possible. Therefore, we believe the model demonstrated has great room from improvement.  Furthermore, this app only works on public repositories and will do nothing if installed on a private repo.
",84
144,botfront/botfront,JavaScript,"




Botfront
An Open Source GUI platform to build chatbots with Rasa. Get started

Getting started
This quick tutorial will get you started in minutes
Getting help

Read the documentation
Get answers on the Spectrum community

License
Botfront is AGPLv3 licensed
",6
145,ESW1234/esw1234.github.io,HTML,"esw1234.github.io
",2
146,scorzy/IdleSpace,TypeScript,"IdleSpace

This is an idle game about space combat. I made it to try web workers and some Angular Matrial CDK, that's why there are drag and drop and per battle is handled per ship.
Links:

Game: https://github.com/scorzy/IdleSpace
Kongregate: https://www.kongregate.com/games/scorzy88/idle-space
Discord: https://discord.gg/CGEfHGe
Battle code: https://github.com/scorzy/IdleSpace/blob/master/src/app/battle.service.ts

This project was generated with Angular CLI version 7.2.2.
Development server
Run ng serve for a dev server. Navigate to http://localhost:4200/. The app will automatically reload if you change any of the source files.
Build
Run ng build to build the project. The build artifacts will be stored in the dist/ directory. Use the --prod flag for a production build.
Running unit tests
Run ng test to execute the unit tests via Karma.
",12
147,ethereum/lahja,Python,"Lahja

Documentation hosted by ReadTheDocs
DISCLAIMER: This is alpha state software. Expect bugs.
Lahja is a generic multi process event bus implementation written in Python 3.6+ to enable lightweight inter-process communication, based on non-blocking asyncio.
What is this for?
Lahja is tailored around one primary use case: enabling multi process Python applications to communicate through events between processes in a non-blocking
asyncio fashion.
Key facts:

non-blocking APIs based on asyncio
lightweight and simple (e.g no IPC pipes etc)
easy multicasting of events (one event, many independent receivers)
easy event routing (e.g route event X only to process group Y)
multiple consuming APIs to adapt to different use cases and styles

TODOs

Filter support (e.g. only subscribe to EventX from origin y)
Push boundaries (don't push this into process x)
Testing
Performance analysis

Developer Setup
If you would like to hack on lahja, please check out the
Ethereum Development Tactical Manual
for information on how we do:

Testing
Pull Requests
Code Style
Documentation

Development Environment Setup
You can set up your dev environment with:
git clone https://github.com/cburgdorf/lahja
cd lahja
virtualenv -p python3 venv
. venv/bin/activate
pip install -e .[dev]
Testing Setup
During development, you might like to have tests run on every file save.
Show flake8 errors on file change:
# Test flake8
when-changed -v -s -r -1 lahja/ tests/ -c ""clear; flake8 lahja tests && echo 'flake8 success' || echo 'error'""
Run multi-process tests in one command, but without color:
# in the project root:
pytest --numprocesses=4 --looponfail --maxfail=1
# the same thing, succinctly:
pytest -n 4 -f --maxfail=1
Run in one thread, with color and desktop notifications:
cd venv
ptw --onfail ""notify-send -t 5000 'Test failure ⚠⚠⚠⚠⚠' 'python 3 test on lahja failed'"" ../tests ../lahja
Release setup
For Debian-like systems:
apt install pandoc

To release a new version:
make release bump=$$VERSION_PART_TO_BUMP$$
How to bumpversion
The version format for this repo is {major}.{minor}.{patch} for stable, and
{major}.{minor}.{patch}-{stage}.{devnum} for unstable (stage can be alpha or beta).
To issue the next version in line, specify which part to bump,
like make release bump=minor or make release bump=devnum.
If you are in a beta version, make release bump=stage will switch to a stable.
To issue an unstable version when the current version is stable, specify the
new version explicitly, like make release bump=""--new-version 4.0.0-alpha.1 devnum""
",19
148,octavore/homebrew-tools,Ruby,"Tools
This repo simplifies installation of my homegrown tools, such as:

delta, a diff tool with a browser-based GUI.
lightproxy, a lightweight local proxy.

Installing
# (optional) brew update
brew install octavore/tools/delta
brew install octavore/tools/lightproxy

",2
149,google/TensorNetwork,Python,"TensorNetwork

A tensor network wrapper for TensorFlow.
Installation
pip3 install tensornetwork

Note: The following examples assume a TensorFlow v2 interface
(in TF 1.13 or higher, run tf.enable_v2_behavior() after
importing tensorflow) but should also work with eager mode
(tf.enable_eager_execution()). The actual library does work
under graph mode, but documentation is limited.
Basic Example
Here, we build a simple 2 node contraction.
import numpy as np
import tensorflow as tf
tf.enable_v2_behavior()
import tensornetwork

# Create the network
net = tensornetwork.TensorNetwork()
# Add the nodes
a = net.add_node(np.ones((10,), dtype=np.float32)) 
# Can use either np.array or tf.Tensor and can even mix them!
b = net.add_node(tf.ones((10,)))
edge = net.connect(a[0], b[0])
final_node = net.contract(edge)
print(final_node.get_tensor().numpy()) # Should print 10.0
Node and Edge names.
You can optionally name your nodes/edges. This can be useful for debugging,
as all error messages will print the name of the broken edge/node.
net = tensornetwork.TensorNetwork()
node = net.add_node(np.eye(2), name=""Identity Matrix"")
print(""Name of node: {}"".format(node.name))
edge = net.connect(node[0], node[1], name=""Trace Edge"")
print(""Name of the edge: {}"".format(edge.name))
# Adding name to a contraction will add the name to the new edge created.
final_result = net.contract(edge, name=""Trace Of Identity"")
print(""Name of new node after contraction: {}"".format(final_result.name))
Named axes.
To make remembering what an axis does easier, you can optionally name a node's axes.
net = tensornetwork.TensorNetwork()
a = net.add_node(np.zeros((2, 2)), axis_names=[""alpha"", ""beta""])
edge = net.connect(a[""beta""], a[""alpha""])
Edge reordering.
To assert that your result's axes are in the correct order, you can reorder a node at any time during computation.
net = tensornetwork.TensorNetwork()
a = net.add_node(np.zeros((1, 2, 3)))
e1 = a[0]
e2 = a[1]
e3 = a[2]
a.reorder_edges([e3, e1, e2])
# If you already know the axis values, you can equivalently do
# a.reorder_axes([2, 0, 1])
print(a.tensor.shape) # Should print (3, 1, 2)
NCON interface.
For a more compact specification of a tensor network and its contraction, there is ncon(). For example:
from tensornetwork import ncon
a = tf.random_normal((2,2))
b = tf.random_normal((2,2))
c = ncon([a,b], [(-1,0),(0,-2)])
print(tf.norm(tf.matmul(a,b) - c)) # Should be zero
It is also possible to generate a TensorNetwork:
from tensornetwork import ncon_network
a = tf.random_normal((2,2))
b = tf.random_normal((2,2))
net, e_con, e_out = ncon_network([a,b], [(-1,0),(0,-2)])
for e in e_con:
    n = net.contract(e) # Contract edges in order
n.reorder_edges(e_out) # Permute final tensor as necessary
print(tf.norm(tf.matmul(a,b) - n.get_tensor()))
TensorNetwork is not an official Google product. Copyright 2019 The TensorNetwork Developers.
",56
150,machine-learning-apps/Issue-Label-Bot,CSS," 

Code for: ""How to automate tasks on GitHub with machine learning for fun and profit""
Table of Contents

Issue-Label Bot

Important links
Files


Running This Code

Environment Variables
Run Locally
Deploy As A Service


Contributing

Roadmap
References


Disclaimers

Original Authors: @hamelsmu, @inc0, @jlewi
Issue Label Bot
Install this app from the GitHub marketplace
A GitHub App powered by machine learning, written in python.  A discussion of the motivation for building this app is described in this blog post.
When an issue is opened, the bot predicts if the label should be a: feature request, bug or question and applies a label automatically if appropriate. Here is a screenshot of the bot in action:



More examples can be viewed on our app's homepage.  It should be noted that the bot may not apply any label in circumstances where the prediction is uncertain.  See the disclaimers section for more caveats.
Important links

Issue Label Bot homepage. Provides a way to view example predictions as well as other information regarding this bot.
GitHub App page for Issue Label Bot, where you can install the app. See disclaimers below before installing.

Files

/notebooks: contains notebooks on how to train the model and interact with the GitHub api uing a python client.
/flask_app: code for a flask app that listens for GitHub issue events and responds with predictions.  This is the main application that the user will interact with.
/argo: the code in this directory relates to the construction of Argo ML Pipelines for training and deploying ML workflows.
/deployment: This directory contains files that are helpful in deploying the app.

Dockerfile this is the definition of the container that is used to run the flask app.  The build for this container is hosted on DockerHub at hamelsmu/mlapp.
heroku.yml: this is used for deploying to Heroku.
*.yaml: these files relate to a Kubernetees deployment.



Running This Code
Prerequisites
To utilize the code in this repository, you will need to register a GitHub App of your own and install this app on your desired repositories and store authentication secrets.
First, walk through the prerequisites section of this getting started guide except The Ruby programming language"" section as we will be using python instead as the client that interfaces with the GitHub api.
Second, setup your development environment. Make sure you create a Webhook secret, even though this step is optional.
Next, setup a postgres database.  You can do this for free on Heroku.    Detailed instructions (stolen shamelessly from here):

Navigate to https://www.heroku.com/, and create an account if you don’t already have one.
On Heroku’s Dashboard, click “New” and choose “Create new app.”
Give your app a name, and click “Create app.”
On your app’s “Overview” page, click the “Configure Add-ons” button.
In the “Add-ons” section of the page, type in and select “Heroku Postgres.”
Choose the “Hobby Dev - Free” plan, which will give you access to a free PostgreSQL database that will support up to 10,000 rows of data. Click “Provision.”
Now, click the “Heroku Postgres :: Database” link.
You should now be on your database’s overview page. Click on 8 “Settings”, and then “View Credentials.” This is the information you’ll need to log into your database.

Finally, you need to create environment variables for all the secrets, which is described below.
Environment Variables

PRIVATE_KEY:  this is the private key you use to authenticate as an app with the GitHub api.
WEBHOOK_SECRET: this is used to verify that payloads received by your app are actually from GitHub.  This is described here.
DATABASE_URL: this is the URL that contains the login information for your POSTGRESQL database, usually in the form: postgres://<username>:<password>@<url>:5432/<database_name>
APP_ID: this is a unique identifier provided to you by GitHub when you register your app.
FLASK_ENV: this is usually set to either production or development.  You will want to use deployment for local testing.
PORT: this is the port your app will be serving on.  Note that if you are deploying to Heroku, Heroku will override this variable with their own value when building your app.  For local development, you will want this to match the port Smee is serving to.
APP_URL: this is the url for the homepage of your app that is provided to users as a link in issue comments.  You can set this to an arbitrary value for local development.

Note: If you are using zsh, the dotenv plugin can be useful for managing environment variables.
Run Locally


Install Dependencies: Install requirements.txt into a virtual environment.  If you are using pipenv install the necessary dependencies from Pipfile.lock by typing pipenv install in the root of this repository.


Run the flask app: run python flask_app/app.py from the root of this repository.  For this to work, you must correctly set the environment variables as described in the Environment Variables section.


Optional - Run app as docker container.  A Docker container that serves Issue-Label Bot can be built with the command bash script/bootstrap from the root of this repository.  This script builds a Docker image named hamelsmu/mlapp, which is also available on Dockerhub.  If you desire to run the Docker container locally for testing, you must pass the necessary environment variables to the Docker container at runtime, as well as expose necessary ports for the app. See the References section for more resources on using Docker.


Deploy As A Service
The assets in this repo allow you to deploy to Heroku (easier) or a Kubernetees cluster (more advanced).
In Heroku, secrets can be passed in as configuration variables.  Furthermore, this documentation describes how you can set secrets in Kubernetees.  Make sure you set the environment variable FLASK_ENV to production if you are going to deploy the app publicly.
Contributing
We welcome all forms of contributions.  We are especially interested in the following:

Bug fixes
Enhancements or additional features
Improvements to the model, or expansion of the dataset(s) used for training.

Roadmap
The authors of this project are interested in adding the following features in the near future:

Constructing better labels and negative samples of items that do not belong in the label set to drive further improvements.
Using the tools from fastai to explore:

State of the art architectures, such as Multi-Head Attention
Utilizing transfer learning to predict unique labels and/or enhance accuracy.


Using GitHub Actions to trigger automated deploys of this code.
Model pipeline orchestration on Argo pipelines.

References


The code in this repo and associated tutorial(s) assume familiarity with Docker. This blog post offers a gentle introduction to Docker for data scientists.


Need inspiration for other data products you can build using machine learning and public GitHub datasets?  See these examples:

GitHub issue summarization and recommendation.
Natural language semantic code search.



Excellent course on flask: HarvardX CS50 Web.


MOOCs by fastai for machine learning and deep learning.


Disclaimers
Issue-Label Bot is for educational and demonstration purposes only.  Our goal was to provide a minimal working example for the community with the least amount of complexity as possible. Therefore, we believe the model demonstrated has great room from improvement.  Furthermore, this app only works on public repositories and will do nothing if installed on a private repo.
",84
151,botfront/botfront,JavaScript,"




Botfront
An Open Source GUI platform to build chatbots with Rasa. Get started

Getting started
This quick tutorial will get you started in minutes
Getting help

Read the documentation
Get answers on the Spectrum community

License
Botfront is AGPLv3 licensed
",6
152,conda/conda-package-handling,Python,"conda-package-handling
Create and extract conda packages of various formats
usage: cph [-h] {extract,x,create,c,transmute,t} ...

optional arguments:
  -h, --help            show this help message and exit

subcommands:
  {extract,x,create,c,transmute,t}
    extract (x)         extract package contents
    create (c)          bundle files into a package
    transmute (t)       convert from one package type to another

cph is an abstraction of conda package handling and a tool for extracting,
creating, and converting between formats.
At the time of writing, the standard conda package format is a .tar.bz2 file.
That will need to be maintained for quite a long time, thanks to the long tail
of people using old conda versions. There is a new conda format, described at
https://docs.google.com/document/d/1HGKsbg_j69rKXPihhpCb1kNQSE8Iy3yOsUU2x68x8uw/edit?usp=sharing.
This new format is designed to have much faster metadata access and utilize more
modern compression algorithms, while also facilitating package signing without
adding sidecar files.
For example, one can transmute (convert) the old package format to the new one with cph:
cph transmute mkl-2018.0.3-1.tar.bz2 .conda

One can then test the speed of extracting the old file and the new one:
$ time cph extract mkl-2018.0.3-1.tar.bz2 --dest mkl-a
cph extract mkl-2018.0.3-1.tar.bz2 --dest mkl-a  18.16s user 0.59s system 98% cpu 19.015 total
$ time cph extract mkl-2018.0.3-1.conda --dest mkl-b
cph extract mkl-2018.0.3-1.conda --dest mkl-b  1.41s user 0.65s system 87% cpu 2.365 total

The new package format is an indexed, uncompressed zip file that contains
tarfiles that can use any compression supported by libarchive. The info metadata
about packages is separated into its own tarfile from the rest of the package
contents. By doing this, we can extract only the metadata, for speeding up
operations like indexing.
$ time cph extract mkl-2018.0.3-1.conda --dest mkl-b --info
cph extract mkl-2018.0.3-1.conda --dest mkl-b --info  0.21s user 0.07s system 98% cpu 0.284 total```

Package creation is primarily something that conda-build uses, as cph does
absolutely nothing to create metadata that makes a conda package useful.
However, you may consider it useful to abuse cph's package creation as a way of
utilizing newer compression formats.
cph create /path/to/some/dir my-cute-archive.conda

Again, this would not necessarily create a valid conda package, unless the
directory being archived contained all the metadata in an ""info"" directory that
a standard conda package needs. The .conda file it creates, however, uses all
the nice new compression formats, though, and you could use cph on some other
computer to extract it.
Releasing
Conda-package-handling releases may be performed via the rever command <https://regro.github.io/rever-docs/>_.
Rever is configured to perform the activities for a typical conda-build release.
To cut a release, simply run rever <X.Y.Z> where <X.Y.Z> is the
release number that you want bump to. For example, rever 1.2.3.  However,
it is always good idea to make sure that the you have permissions everywhere
to actually perform the release.  So it is customary to run rever check before
the release, just to make sure.  The standard workflow is thus::
 rever check
rever 1.2.3

If for some reason a release fails partway through, or you want to claw back a
release that you have made, rever allows you to undo activities. If you find yourself
in this pickle, you can pass the --undo option a comma-separated list of
activities you'd like to undo.  For example::
 rever --undo tag,changelog,authors 1.2.3

Happy releasing!
",4
153,ValeevGroup/tiledarray,C++,"


Synopsis
TiledArray is a scalable, block-sparse tensor framework for rapid composition of high-performance tensor arithmetic, appearing for example in many-body quantum mechanics. It allows users to compose tensor expressions of arbitrary complexity in native C++ code that closely resembles the standard mathematical notation. The framework is designed to scale from a single multicore computer to a massive distributed-memory multiprocessor.
TiledArray is built on top of MADNESS parallel runtime (MADWorld), part of MADNESS numerical calculus framework.
TiledArray is a work in progress. Its development has been possible thanks to generous support from the U.S. National Science Foundation, the Alfred P. Sloan Foundation, the Camille and Henry Dreyfus Foundation, and the Department of Energy.
Design Goals

General-purpose arithmetic on dense and block-sparse tensors;
High-level (math-like) composition as well as full access to low-level data and algorithms, both from C++
Massive shared- and distributed-memory parallelism
Deeply-reusable framework: everything can be customized, from tile types (e.g. to support on-disk or compute-on-the-fly tensors) to how the structure of sparse tensors is described.

Example Code
The following example expressions are written in C++ with TiledArray. TiledArray use the Einstein summation convention when evaluating tensor expressions.


Matrix-matrix multiplication
C(""m,n"") = 2.0 * A(""m,k"") * B(""k,n"");



Matrix-vector multiplication
C(""n"") = A(""k"") * B(""k,n"");



A more complex tensor expression
E(""m,n"") = 2.0 * A(""m,k"") * B(""k,n"") + C(""k,n"") * D(""k,m"");



The following application is a minimal example of a distributed-memory matrix multiplcation.
#include <tiledarray.h>

int main(int argc, char** argv) {
  // Initialize the parallel runtime
  TA::World& world = TA::initialize(argc, argv);
  
  // Construct a 2D tiled range structure that defines
  // the tiling of an array. Each dimension contains
  // 10 tiles.
  TA::TiledRange trange = 
      { { 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100 },
        { 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100 } };
  
  // Construct and fill the argument arrays with data
  TA::TArrayD A(world, trange);
  TA::TArrayD B(world, trange);
  A.fill_local(3.0);
  B.fill_local(2.0);
  
  // Construct the (empty) result array.
  TA::TArrayD C;
  
  // Perform a distributed matrix multiplication
  C(""i,j"") = A(""i,k"") * B(""k,j"");
  
  // Tear down the parallel runtime. 
  TA::finalize();
  return 0;
}

Performance
Parallel performance of TiledArray for multiplication of dense square matrices on Mira, an IBM BlueGene/Q supercomputer at Argonne National Laboratory, compared with that of Cyclops Tensor Framework and ScaLAPACK:

This figure was obtained with the help of an award from the Department of Energy INCITE program.
Excellent parallel scalability is also possible for much more complicated expressions than just a single GEMM, as demonstrated below for the coupled-cluster singles and doubles (CCSD) wave function solver. Parallel speed-up of 1 iteration of CCSD solver for uracil trimer in 6-31G* AO basis was measured on ""BlueRidge"" cluster at Virginia Tech (wall time on one 16-core node = 1290 sec):

This figure was obtained with the help of an allocation from Advanced Research Computing at Virginia Tech.
Installing TiledArray
Dependencies

C++ compiler with C++14 support - Compilers that have been tested include:

GCC 4.9 and later,
Clang 3.4 and later,
Apple Clang 5.0 and later, and
Intel C/C++ Compiler 17 and later.


Cmake 3.9 or later
Eigen - Version 3.3 and later, available from [http://eigen.tuxfamily.org]. Will be downloaded automatically, if missing.
BTAS - master branch, available from [http://github.com/BTAS/BTAS]. Will be downloaded automatically, if missing.
MADNESS - While it is possible to compile MADNESS separately, we recommend that MADNESS is downloaded and compiled automatically as part of TiledArray. Compilation of MADNESS requires the following additional prerequisites (see the MADNESS GitHub page for details):

Pthreads
MPI-2 or MPI-3 library - Open-MPI, MPICH, MVAPICH, and Intel MPI have been tested. Support for MPI_THREAD_MULTIPLE must be enabled in come of these libraries at configuration time.
LAPACK and BLAS - Serial (sequential, or 1-thread) versions of these libraries is recommended. If you have to use threaded version of these libraries, to avoid poor performance (or even errors) due to non-interoperable threading runtimes it is recommended to configure these libraries to use single thread at runtime before entering the block of TiledArray code.
Intel Threading Building Blocks (optional, but strongly recommended) Version 4.3 Update 5 or later.


Boost - Version 1.33.0 or later, used for for unit tests only. Will be downloaded automatically, if missing.
Doxygen (optional) - Used to generate for documentation only. We strongly recommend to use the most recent version of Doxygen to produce the most accurate documentation.

Build
TiledArray includes several tool chain files for common platforms. These files contain system specific settings that have been tested on the various platforms. We recommend using one of these
$ git clone https://github.com/ValeevGroup/TiledArray.git tiledarray
$ cd tiledarray
$ mkdir build
$ cd build
$ cmake \
    -D CMAKE_INSTALL_PREFIX=/path/to/tiledarray/install \
    -D CMAKE_TOOLCHAIN_FILE=../cmake/toolchains/osx-clang-mpi-accelerate.cmake \
    ..
$ cmake --build .
$ cmake --build . --target install

Common CMake cache variables that you may want to define include:
Compiler Variables
-D CMAKE_C_COMPILER=/path/to/bin/cc
-D CMAKE_CXX_COMPILER=/path/to/bin/c++
-D MPI_C_COMPILER=/path/to/bin/mpicc
-D MPI_CXX_COMPILER=/path/to/bin/mpicxx
Option Variables
-D CMAKE_BUILD_TYPE=(Release|Debug|RelWithDebInfo)
-D BUILD_SHARED_LIBS=(TRUE|FALSE)
-D CMAKE_CXX_STANDARD=(14|17|20)
-D TA_ERROR=(none|throw|assert)
Note that currently BUILD_SHARED_LIBS=TRUE is only supported for single-process runs due to the limitations of the MADWorld runtime.
Library Variables
-D MADNESS_ROOT_DIR=/path/to/madness/root/dir
-D TBB_ROOT_DIR=/path/to/tbb/root/dir
-D LAPACK_LIBRARIES=(semicolon seperated list of LAPACK libraries)
-D BLAS_LIBRARIES=(semicolon seperated list of BLAS libraries)
-D BLA_STATIC=(TRUE|FALSE)
-D INTEGER4=(TRUE|FALSE)
-D EIGEN3_INCLUDE_DIR=/path/to/eigen3/include
BLA_STATIC indicates static LAPACK and BLAS libraries will be perferred. INTEGER4 indicated the Fortran integer width used by BLAS and LAPACK; if TRUE (the default), the integer size is integer*4, otherwise integer*8 is used.
Expert Variables

Note, when configuring TiledArray, CMake will download and build MADNESS, Eigen, and Boost if they are not found on the system. Boost will only be installed if unit testing is enabled. This behavior can be disable with -D TA_EXPERT=TRUE.
To enable tracing of MADNESS tasks add -D TA_TRACE_TASKS=ON

Developers
TiledArray is developed by the Valeev Group at Virginia Tech.
License
TiledArray is freely available under the terms of the GPL v3+ licence. See the the included LICENSE file for details. If you are interested in using TiledArray under different licensing terms, please contact us.
How to Cite
Cite TiledArray as

""TiledArray: A general-purpose scalable block-sparse tensor framework"", Justus A. Calvin and Edward F. Valeev, https://github.com/valeevgroup/tiledarray .

Inner workings of TiledArray are partially described in the following publications:

Justus A. Calvin, Cannada A. Lewis, and Edward F. Valeev, ""Scalable Task-Based Algorithm for Multiplication of Block-Rank-Sparse Matrices."", Proceedings of the 5th Workshop on Irregular Applications: Architectures and Algorithms, http://dx.doi.org/10.1145/2833179.2833186.
Justus A. Calvin and Edward F. Valeev, ""Task-Based Algorithm for Matrix Multiplication: A Step Towards Block-Sparse Tensor Computing."" http://arxiv.org/abs/1504.05046 .

The MADNESS parallel runtime is described in the following publication:

Robert J. Harrison, Gregory Beylkin, Florian A. Bischoff, Justus A. Calvin, George I. Fann, Jacob Fosso-Tande, Diego Galindo, Jeff R. Hammond, Rebecca Hartman-Baker, Judith C. Hill, Jun Jia, Jakob S. Kottmann, M-J. Yvonne Ou, Junchen Pei, Laura E. Ratcliff, Matthew G. Reuter, Adam C. Richie-Halford, Nichols A. Romero, Hideo Sekino, William A. Shelton, Bryan E. Sundahl, W. Scott Thornton, Edward F. Valeev, Álvaro Vázquez-Mayagoitia, Nicholas Vence, Takeshi Yanai, and Yukina Yokoi, ""madness: A Multiresolution, Adaptive Numerical Environment for Scientific Simulation."", SIAM J Sci Comput 38, S123-S142 (2016), http://dx.doi.org/10.1137/15M1026171 .

Acknowledgements
Development of TiledArray is made possible by past and present contributions from the National Science Foundation (awards CHE-0847295, CHE-0741927, OCI-1047696, CHE-1362655, ACI-1450262, ACI-1550456), the Alfred P. Sloan Foundation, the Camille and Henry Dreyfus Foundation, the Department of Energy Exascale Computing Project (NWChemEx subproject), and the Department of Energy INCITE Program.
",86
154,Nycto89/CSCE-4901.001-Team-Captain-Jack-Sparrow,Objective-C,"CSCE-4901.001-Team-Captain-Jack-Sparrow
CSCE 4901 Capstone Project Repository
The End Stage Renal Disease Network of North Texas wishes to provide better education
and accessibility for their treatment options to their patients. Most hospitals and dialysis
centers rarely give enough information for people to understand their options and patients have
very little knowledge of how their kidneys even work. This application will help in that aspect
as well as giving patients the resources to find nearby clinics and setting up appointments for
their respective treatments.
Trello: https://trello.com/b/lLNcTyGM/management-board
Chris Cox
Olivia Haynes
Justin Hicks
Kevin Morales
Jackson Pfeffer
",3
