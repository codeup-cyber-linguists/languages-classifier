title,language,original,stars
NetBSD/src,None,"NetBSD
NetBSD is a free, fast, secure, and highly portable Unix-like Open
Source operating system.  It is available for a wide range of
platforms, from large-scale servers
and powerful desktop systems to handheld and embedded devices.
Building
You can cross-build NetBSD from most UNIX-like operating systems.
To build for amd64 (x86_64), in the src directory:
./build.sh -U -u -j4 -m amd64 -O ~/obj release

Additional build information available in the BUILDING file.
Binaries

Daily builds
Releases

Testing
On a running NetBSD system:
cd /usr/tests; atf-run | atf-report

Troubleshooting

Send bugs and patches via web form.
Subscribe to the mailing lists.
The netbsd-users list is a good choice for many problems; watch current-users if you follow the bleeding edge of NetBSD-current.
Join the community IRC channel #netbsd @ freenode.

Latest sources
To fetch the main CVS repository:
cvs -d anoncvs@anoncvs.NetBSD.org:/cvsroot checkout -P src

To work in the Git mirror, which is updated every few hours from CVS:
git clone https://github.com/NetBSD/src.git

Additional Links

The NetBSD Guide
NetBSD manual pages
NetBSD Cross-Reference

",157
morozov-group/magento2-similar-products,PHP,"magento2-similar-products

Magento 2 Similarity extension which provides connectivity with Similarity Engine.
Demo store

Automated Upsells for every product Demo | Production
Visually Similar products for specified product /catalogsearch/advanced/result/?similar=PRODUCT_ID
Visually Similar products within same category category.html?similar=PRODUCT_ID Demo
CMS Widget to put similar products any where for specified PRODUCT_ID.
(Near future) Category filling assistant, for some special events or campaigns.

Installation
Simple installation via composer.
compose require morozov-group/magento2-similar-products

Go to configuration enter your email, and we'll take care of everything else.
You will receive email once we are ready to serve similar products recommendations.
Then you can proceed with customizations.
Contributions and new ideas
You are welcome to post tickets and pull requests.
",5
u-simon/springCloudDemo,Java,"springCloudDemo
",2
AMReX-Codes/amrex,C++,"
License
AMReX Copyright (c) 2017, The Regents of the University of California,
through Lawrence Berkeley National Laboratory and the Alliance for
Sustainable Energy, LLC., through National Renewable Energy Laboratory
(subject to receipt of any required approvals from the U.S. Dept. of
Energy).  All rights reserved.
If you have questions about your rights to use or distribute this
software, please contact Berkeley Lab's Innovation & Partnerships
Office at IPO@lbl.gov.
NOTICE.  This Software was developed under funding from the
U.S. Department of Energy and the U.S. Government consequently retains
certain rights. As such, the U.S. Government has been granted for
itself and others acting on its behalf a paid-up, nonexclusive,
irrevocable, worldwide license in the Software to reproduce,
distribute copies to the public, prepare derivative works, and perform
publicly and display publicly, and to permit other to do so.
License for AMReX can be found at LICENSE.
Development Model
Development generally follows the following ideas:


New features are committed to the development branch.
Nightly regression testing is used to ensure that no answers
change (or if they do, that the changes were expected).
If a change is critical, we can cherry-pick the commit from
development to master.


Bug fixes, questions and contributions of new features are welcome!


Bugs should be reported through GitHub issues


We suggest asking questions through GitHub issues as well


Any contributions of new features that have the potential
to change answers should be done via pull requests.
A pull request should be generated from your fork of
amrex and target the development branch.
If there are a number of small commits making up the PR, we may
wish to squash commits upon merge to have a clean history.
Please ensure that your PR title and first post are descriptive,
since these will be used for a squashed commit message.
Please note the following:
If you choose to make contributions to the code
then you hereby grant a non-exclusive, royalty-free perpetual license
to install, use, modify, prepare derivative works,
incorporate into other computer software,
distribute, and sublicense such enhancements or derivative works
thereof, in binary and source code form.




On the first workday of each month, we perform a merge of
development into master.  For this merge to take place, we
need to be passing the regression tests.
To accommodate this need, we close the merge window into
development a few days before the merge day.  While the merge
window is closed, only bug fixes should be pushed into
development.  Once the merge from development -> master is
done, the merge window reopens.


Core Developers
People who make a number of substantive contributions will be named
""core developers"" of AMReX.  The criteria for becoming a core
developer are flexible, but generally involve one of the following:


100 non-trivial commits to amrex/Src/ and/or


addition of a new algorithm / module  and/or


substantial input into the code design process or testing


If a core developer is inactive for multiple years, we may reassess their
status as a core developer.
The current list of core developers is: Ann Almgren (LBNL), Vince Beckner, John Bell (LBNL), Johannes Blaschke (LBNL), Cy Chan (LBNL), Marcus Day (LBNL), Brian Friesen (NERSC), Kevin Gott (NERSC), Daniel Graves (LBNL), Max Katz (NVIDIA), Andrew Myers (LBNL), Tan Nguyen (LBNL), Andrew Nonaka (LBNL), Michele Rosso (LBNL), Sam Williams (LBNL), Weiqun Zhang (LBNL), Michael Zingale (Stonybrook University).
",130
joeynmt/joeynmt,Python,"   Joey NMT

Goal and Purpose
Joey NMT framework is developed for educational purposes.
It aims to be a clean and minimalistic code base to help novices
pursuing the understanding of the following questions.

How to implement classic NMT architectures (RNN and Transformer) in PyTorch?
What are the building blocks of these architectures and how do they interact?
How to modify these blocks (e.g. deeper, wider, ...)?
How to modify the training procedure (e.g. add a regularizer)?

In contrast to other NMT frameworks, we will not aim for
state-of-the-art results or speed through engineering or training tricks
since this often goes in hand with an increase in code complexity
and a decrease in readability.
However, Joey NMT re-implements baselines from major publications.
Contributors
Joey NMT is developed by Joost Bastings (University of Amsterdam) and Julia Kreutzer (Heidelberg University).
Features
We aim to implement the following features (aka the minimalist toolkit of NMT):

Recurrent Encoder-Decoder with GRUs or LSTMs
Transformer Encoder-Decoder
Attention Types: MLP, Dot, Multi-Head, Bilinear
Word-, BPE- and character-based input handling
BLEU, ChrF evaluation
Beam search with length penalty and greedy decoding
Customizable initialization
Attention visualization
Learning curve plotting

[Work in progress: Transformer, Multi-Head and Dot still missing.]
Coding
In order to keep the code clean and readable, we make use of:

Style checks: pylint with (mostly) PEP8 conventions, see .pylintrc.
Typing: Every function has documented input types.
Docstrings: Every function, class and module has docstrings describing their purpose and usage.
Unittests: Every module has unit tests, defined in test/unit/.
Travis CI runs the tests and pylint on every push to ensure the repository stays clean.

Installation
Joey NMT is built on PyTorch and torchtext for Python >= 3.5.

Clone this repository:
git clone https://github.com/joeynmt/joeynmt.git
Install the requirements:
cd joeynmt
pip3 install -r requirements.txt (you might want to add --user for a local installation).
Install joeynmt:
python3 setup.py install
Run the unit tests:
python3 -m unittest

Usage
For details, follow the tutorial in the docs.
Data Preparation
Parallel Data
For training a translation model, you need parallel data, i.e. a collection of source sentences and reference translations that are aligned sentence-by-sentence and stored in two files,
such that each line in the reference file is the translation of the same line in the source file.
Pre-processing
Before training a model on it, parallel data is most commonly filtered by length ratio, tokenized and true- or lowercased.
The Moses toolkit provides a set of useful scripts for this purpose.
In addition, you might want to build the NMT model not on the basis of words, but rather sub-words or characters (the level in JoeyNMT configurations).
Currently, JoeyNMT supports the byte-pair-encodings (BPE) format by subword-nmt.
Configuration
Experiments are specified in configuration files, in simple YAML format. You can find examples in the configs directory.
small.yaml contains a detailed explanation of configuration options.
Most importantly, the configuration contains the description of the model architecture (e.g. number of hidden units in the encoder RNN),
paths to the training, development and test data, and the training hyperparameters (learning rate, validation frequency etc.).
Training
Start
For training, run
python3 -m joeynmt train configs/small.yaml.
This will train a model on the training data specified in the config (here: small.yaml),
validate on validation data,
and store model parameters, vocabularies, validation outputs and a small number of attention plots in the model_dir (also specified in config).
Note that pre-processing like tokenization or BPE-ing is not included in training, but has to be done manually before.
Tip: Be careful not to overwrite models, set overwrite: False in the model configuration.
Validations
The validations.txt file in the model directory reports the validation results at every validation point.
Models are saved whenever a new best validation score is reached, in batch_no.ckpt, where batch_no is the number of batches the model has been trained on so far.
best.ckpt links to the checkpoint that has so far achieved the best validation score.
Visualization
JoeyNMT uses TensorboardX to visualize training and validation curves and attention matrices during training.
Launch Tensorboard with tensorboard --logdir model_dir/tensorboard (or python -m tensorboard.main ...) and then open the url (default: localhost:6006) with a browser.
For a stand-alone plot, run python3 scripts/plot_validation.py model_dir --plot_values bleu PPL --output_path my_plot.pdf to plot curves of validation BLEU and PPL.
CPU vs. GPU
For training on a GPU, set use_cuda in the config file to True. This requires the installation of required CUDA libraries.
Translating
There's 3 options for testing what the model has learned.
Whatever data you feed the model for translating, make sure it is properly pre-processed, just as you pre-processed the training data, e.g. tokenized and split into subwords (if working with BPEs).
1. Test Set Evaluation
For testing and evaluating on your parallel test/dev set, run
python3 -m joeynmt test configs/small.yaml --output_path out.
This will generate translations for validation and test set (as specified in the configuration) in out.[dev|test]
with the latest/best model in the model_dir (or a specific checkpoint set with load_model).
It will also evaluate the outputs with eval_metric.
If --output_path is not specified, it will not store the translation, and only do the evaluation and print the results.
2. File Translation
In order to translate the contents of a file not contained in the configuration (here my_input.txt), simply run
python3 -m joeynmt translate configs/small.yaml < my_input.txt > out.
The translations will be written to stdout or alternatively--output_path if specified.
3. Interactive
If you just want try a few examples, run
python3 -m joeynmt translate configs/small.yaml
and you'll be prompted to type input sentences that JoeyNMT will then translate with the model specified in the configuration.
Documentation and Tutorial
The docs include an overview of the NMT implementation, a walk-through tutorial for building, training, tuning, testing and inspecting an NMT system, the API documentation and FAQs.
Benchmarks
Benchmarks on small models trained on GPU/CPU on standard data sets are reported here.

IWSLT15 En-Vi, word-based
IWSLT14 De-En, 32000 joint BPE, word-based
WMT17 En-De and Lv-En, 32000 joint BPE

IWSLT English-Vietnamese
We compare against Tensorflow NMT on the IWSLT15 En-Vi data set as preprocessed by Stanford.
You can download the data with scripts/get_iwslt15_envi.sh, and then use configs/iwslt_envi_luong.yaml to replicate the experiment.



Systems
tst2012 (dev)
test2013 (test)




TF NMT (greedy)
23.2
25.5


TF NMT (beam=10)
23.8
26.1


Joey NMT (greedy)
23.2
25.8


Joey NMT (beam=10, alpha=1.0)
23.8
26.5


(Luong & Manning, 2015)
-
23.3



We also compare against xnmt which uses different hyperparameters, so we use a different configuration for Joey NMT too: configs/iwslt_envi_xnmt.yaml.



Systems
tst2012 (dev)
test2013 (test)




xnmt (beam=5)
25.0
27.3


Joey NMT (greedy)
24.6
27.4


Joey NMT (beam=5, alpha=1.0)
24.9
27.7



IWSLT  German-English
We compare against the baseline scores reported in (Wiseman & Rush, 2016) (W&R),
(Bahdanau et al., 2017) (B17) with tokenized, lowercased BLEU (using sacrebleu).
Ẁe compare a word-based model of the same size and vocabulary as in W&R and B17.
The script to obtain and pre-process the data is the one published with W&R.
Use configs/iwslt_deen_bahdanau.yaml for training the model.
On a K40-GPU word-level training took <1h, beam search decoding for both dev and test <2min.



Systems
level
dev
test
#params




W&R (greedy)
word
-
22.53



W&R (beam=10)
word
-
23.87



B17 (greedy)
word
-
25.82



B17 (beam=10)
word
-
27.56



Joey NMT (greedy)
word
28.41
26.68
22.05M


Joey NMT (beam=10, alpha=1.0)
word
28.96
27.03
22.05M



On CPU (use_cuda: False):
(approx 8-10x slower: 8h for training, beam search decoding for both dev and test 19min, greedy decoding 5min)



Systems
level
dev
test
#params




Joey NMT (greedy)
word
28.35
26.46
22.05M


Joey NMT (beam=10, alpha=1.0)
word
28.85
27.06
22.05M



In addition, we compare to a BPE-based GRU model with 32k (Groundhog style).
Use scripts/get_iwslt14_bpe.sh to pre-process the data and configs/iwslt14_deen_bpe.yaml to train the model.
This model is available for download here.



Systems
level
dev
test
#params




Joey NMT (greedy)
bpe
27.57

60.69M


Joey NMT (beam=5, alpha=1.0)
bpe
28.55
27.34
60.69M



WMT 17 English-German and Latvian-English
We compare against the results for recurrent BPE-based models that were reported in the Sockeye paper.
We only consider the Groundhog setting here, where toolkits are used out-of-the-box for creating a Groundhog-like model (1 layer, LSTMs, MLP attention).
The data is pre-processed as described in the paper (code).
Postprocessing is done with Moses' detokenizer, evaluation with sacrebleu.
Note that the scores reported for other models might not reflect the current state of the code, but the state at the time of the Sockeye evaluation.
Please also consider the difference in number of parameters despite ""the same"" setup: our models are the smallest in numbers of parameters.
English-German
Groundhog setting: configs/wmt_ende_default.yaml  with encoder rnn=500, lr=0.0003, init_hidden=""bridge"".



Systems
level
dev
test
#params




Sockeye (beam=5)
bpe
-
23.18
87.83M


OpenNMT-Py (beam=5)
bpe
-
18.66
87.62M


Joey NMT (beam=5)
bpe
24.33
23.45
86.37M



The Joey NMT model was trained for 4 days (14 epochs).
Latvian-English
Groundhog setting: configs/wmt_lven_default.yaml with encoder rnn=500, lr=0.0003, init_hidden=""bridge"".



Systems
level
dev
test
#params




Sockeye (beam=5)
bpe
-
14.40
?


OpenNMT-Py (beam=5)
bpe
-
9.98
?


Joey NMT (beam=5)
bpe
12.09
8.75
64.52M



Contributing
Since this codebase is supposed to stay clean and minimalistic, contributions addressing the following are welcome:

Code correctness
Code cleanliness
Documentation quality
Speed or memory improvements
resolving issues

Code extending the functionalities beyond the basics will most likely not end up in the master branch, but we're curions to learn what you used Joey for.
Use-cases and Projects
Here we'll collect projects and repositories that are based on Joey. If you used Joey for a project, publication or built some code on top of it, let us know and we'll link it here.
Projects:

TBD

Contact
Please leave an issue if you have questions or issues with the code.
For general questions, email us at joeynmt <at> gmail.com.
Naming
Joeys are infant marsupials.
",46
JingningShi/MtreeRing,R,"MtreeRing
Authors: Jingning Shi, Wei Xiang
License: GPL3






MtreeRing is a tool for automatically measuring tree-ring width using image processing techniques.
Installation
Install the stable version from CRAN
install.packages(""MtreeRing"")
or the development version from GitHub
# install.packages(""devtools"")
devtools::install_github(""JingningShi/MtreeRing"")
Ring-width measurement
1. Read an image
library(MtreeRing)
## Read and plot a tree ring image
img.name <- system.file(""001.png"", package = ""MtreeRing"")
t1 <- ring_read(img = img.name, dpi = 1200, plot = TRUE)
ring_read supports commonly used image formats, including png, tiff, jpg and bmp.
2. Detect ring borders
After plotting the image, the automatic detection of ring borders can be performed using three alternative methods: (1) watershed algorithm; (2) Canny edge detector; (3) a linear detection algorithm from R package measuRing.
## Split a long core sample into 2 pieces to
## get better display performance and use the
## watershed algorithm to detect ring borders:
t2 <- ring_detect(ring.data = t1, seg = 2, method = 'watershed')

Figure 1. The automatic detection of ring borders
3. Calculate ring-width series
If all ring borders are correctly identified, you can generate a ring-width series in data frame format. Use write.rwl to export the ring-width series to an rwl file.
rw.df <- ring_calculate(ring.data = t2, seriesID = ""940220"")
library(dplR) # A dendrochronological analysis package
fn <- tempfile(fileext="".rwl"")
write.rwl(rwl.df = rw.df, fname = fn, format = ""tucson"")
Shiny application
If you are not familiar with R and its command line interface, the shiny-based app is a good alternative.
MtreeRing::ring_app_launch()
This command allows to run a Shiny-based application within the system's default web browser. The app provides a beginner-friendly graphical interface and supports more flexible mouse-based interactions.
The dashboard has three components: a header, sidebar and body, like this

A workflow for the Shiny app can be found in the package vignette. Most steps are demonstrated with a gif to make the workflow more understandable.
vignette('app-MtreeRing')
Ring width correction
If an increment borer is used to extract samples, it is well known that the auger sometimes fails to traverse the pith of the sampled tree but passes through one side of the pith at a certain distance. Tangent lines of rings close to the pith are therefore not perpendicular to the horizontal path, which may lead to considerable errors in ring widths.
Under such conditions, you can create two paths by setting the argument incline = TRUE, or by ticking the checkbox ""Inclined tree rings"". See this example.

The line segment connecting two dots on the same ring should match the tangent of a tree ring border. The corrected ring width is estimated from the distance between adjacent rings and orientation of ring borders.
Code of conduct
Please note that the 'MtreeRing' project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.
",2
yongzhuo/nlp_xiaojiang,Python,"nlp_xiaojiang
AugmentText
- 回译（效果比较好）
- EDA（同义词替换、插入、交换和删除）（效果还行）
- HMM-marko（质量较差）
- syntax（依存句法、句法、语法书）（简单句还可）
- seq2seq（深度学习同义句生成，效果不理想，seq2seq代码大都是 [https://github.com/qhduan/just_another_seq2seq] 的，效果不理想）

ChatBot
- 检索式ChatBot
    - 像ES那样直接检索(如使用fuzzywuzzy)，只能字面匹配
    - 构造句向量，检索问答库，能够检索有同义词的句子
- 生成式ChatBot（todo）
    - seq2seq
    - GAN

ClassificationText
- bert+bi-lstm(keras) approach 0.78~0.79% acc of Weizhong Bank Intelligent Customer Service Question Matching Competition

FeatureProject
- bert句向量、文本相似度
    - bert/extract_keras_bert_feature.py:提取bert句向量特征
    - bert/tet_bert_keras_sim.py:测试bert句向量cosin相似度
- normalization_util指的是数据归一化
    - 0-1归一化处理
    - 均值归一化
    - sig归一化处理
- sim feature（ML）
    - distance_text_or_vec:各种计算文本、向量距离等
    - distance_vec_TS_SS：TS_SS计算词向量距离
    - cut_td_idf：将小黄鸡语料和gossip结合
    - sentence_sim_feature：计算两个文本的相似度或者距离，例如qq（问题和问题），或者qa（问题和答案）

run(可以在win10下,pycharm下运行)

1.创建tf-idf文件等（运行2需要先跑1）:
python cut_td_idf.py
2.计算两个句子间的各种相似度，先计算一个预定义的，然后可输入自定义的（先跑1）:
python sentence_sim_feature.py
3.chatbot_1跑起来(fuzzy检索-没)（独立）：
python chatbot_fuzzy.py
4.chatbot_2跑起来(句向量检索-词)（独立）：
python chatbot_sentence_vec_by_word.py
5.chatbot_3跑起来(句向量检索-字)（独立）：
python chatbot_sentence_vec_by_char.py
6.数据增强（eda)：                     python enhance_eda.py
7.数据增强（marko）:                   python enhance_marko.py
8.数据增强（translate_account）:       python translate_tencent_secret.py
9.数据增强（translate_tools）:         python translate_translate.py
10.数据增强（translate_web）:          python translate_google.py
11.数据增强（augment_seq2seq）:        先跑 python extract_char_webank.py生成数据，
再跑 python train_char_anti.py
然后跑 python predict_char_anti.py
12.特征计算(bert)（提取特征、计算相似度）:
run extract_keras_bert_feature.py run tet_bert_keras_sim.py

Data
- chinese_L-12_H-768_A-12（谷歌预训练好的模型）
   github项目中只是上传部分数据，需要的前往链接: https://pan.baidu.com/s/1I3vydhmFEQ9nuPG2fDou8Q 提取码: rket
   解压后就可以啦
- chinese_vector
    github项目中只是上传部分数据，需要的前往链接: https://pan.baidu.com/s/1I3vydhmFEQ9nuPG2fDou8Q 提取码: rket
    - 截取的部分word2vec训练词向量（自己需要下载全效果才会好）
    - w2v_model_wiki_char.vec、w2v_model_wiki_word.vec都只有部分
- corpus
    github项目中只是上传部分数据，需要的前往链接: https://pan.baidu.com/s/1I3vydhmFEQ9nuPG2fDou8Q 提取码: rket
    - webank(train、dev、test)
    - 小黄鸡和gossip问答预料（数据没清洗）,chicken_and_gossip.txt
    - 微众银行和支付宝文本相似度竞赛数据， sim_webank.csv
- sentence_vec_encode_char
    - 1.txt（字向量生成的前100000句向量）
- sentence_vec_encode_word
    - 1.txt（词向量生成的前100000句向量）
- tf_idf（chicken_and_gossip.txt生成的tf-idf）

requestments.txt
- python_Levenshtei
    - 调用Levenshtein，我的python是3.6，
    - 打开其源文件: https://www.lfd.uci.edu/~gohlke/pythonlibs/
    - 查找python_Levenshtein-0.12.0-cp36-cp36m-win_amd64.whl下载即可
- pyemd
- pyhanlp
    - 下好依赖JPype1-0.6.3-cp36-cp36m-win_amd64.whl

参考/感谢

eda_chinese：https://github.com/zhanlaoban/eda_nlp_for_Chinese
主谓宾提取器：https://github.com/hankcs/MainPartExtractor
HMM生成句子：https://github.com/takeToDreamLand/SentenceGenerate_byMarkov
同义词等：https://github.com/fighting41love/funNLP/tree/master/data/
小牛翻译：http://www.niutrans.com/index.html

其他资料

bert(keras):https://github.com/CyberZHG/keras-bert
NLP数据增强汇总:https://github.com/quincyliang/nlp-data-augmentation
知乎NLP数据增强话题:https://www.zhihu.com/question/305256736/answer/550873100
chatbot_seq2seq_seqGan（比较好用）：https://github.com/qhduan/just_another_seq2seq
自己动手做聊天机器人教程: https://github.com/warmheartli/ChatBotCourse

",19
alexherbo2/site,JavaScript,"Site
Configuration | Theme | Builds | Contributing

Personal web site built with Hugo.

",2
opengeospatial/geotiff,HTML,"geotiff
The key folder where the asciidoc specification is developed is GeoTIFF_Standard cf. https://github.com/opengeospatial/geotiff/tree/master/GeoTIFF_Standard/standard
Other files of interest are:

geotiff_standard.html
geotiff_standard.pdf

Other folders are just for traceability, capturing the works done between 2014 and 2018.
",6
dawoudt/JustWatchAPI,Python,"JustWatchAPI

JustWatch.com Python 3 API
Install
python3 -m pip install JustWatch
How To
search for an item
from justwatch import JustWatch

just_watch = JustWatch(country='US')

results = just_watch.search_for_item(query='the matrix')
or search for combination of genres
just_watch = JustWatch(genres=['act', 'scf', 'hrr'])

results_by_genres = just_watch.search_for_item()
or maybe search by provider
just_watch = JustWatch()

results_by_providers = just_watch.search_for_item(providers=['nfx', 'stn'])
or possibly a combination of the above
just_watch = JustWatch()

results_by_multiple = just_watch.search_for_item(
    providers=['nfx', 'stn'], 
    content_types=['movie'], 
    monetization_types=['free'])
get list of genres and codes
just_watch = JustWatch(country='GB')
genre_details = just_watch.get_genres()

get list of providers for a country
just_watch = JustWatch(country='DE')
provider_details = just_watch.get_providers()

get further details on a movie or tv program
Based on title id found in previous search
just_watch = JustWatch(country='GB')
megamind = just_watch.get_title(title_id=103561)
dark = just_watch.get_title(title_id=55668, content_type='show')

You can query for title IDs
just_watch = JustWatch(country='GB')
the_matrix = just_watch.get_title_id(query='the matrix')

{'The Matrix': 10, 'The Matrix Revisited': 30701, ...}

get further defails on a specific season of a tv program
season_id can be found in the response from get_title of a tv program
just_watch = JustWatch(country='GB')
hannibal_season2 = just_watch.get_season(season_id=20236)

get country specific certification details
just_watch = JustWatch(country='GB')
certs = just_watch.get_certifications()

content_type can be specified but (for GB at least) setting to 'show' gives less detail than the default of 'movie'
get cinema details
Setting ""monetization_types"" to ""cinema"" and possibly setting nationwide_cinema_releases_only = True will return a list of potential showings.
just_watch = JustWatch(country='GB')
cinema_showings = just_watch.search_for_item(monetization_types='cinema')

Then based on title_id obtained from that search
cinema_times = just_watch.get_cinema_times(title_id=this_title_id,
                                           date='2018-03-24',
                                           latitude=51.5287718,
                                           longitude=-0.2416809,
                                           radius=20000)
This will return details of all the showings in the area.  Details of all the cinemas in the area can be obtained by a call to get_cinema_details().  This takes the same latitutde, longitude and radius parameters as get_cinema_times(), and if a call has already been made they'll be reused.
local_cinemas = just_watch.get_cinema_details()
You can then join the data from the two calls by joining 'cinema_id' from get_cinema_times() with 'id' from get_cinema_details()
get upcoming cinema details
Call get_upcoming_cinema() with number of weeks forward or back and whether you only require national releases
showings_last_week = just_watch.get_upcoming_cinema(weeks_offset=-1, nationwide_cinema_releases_only=True)
showings_three_weeks = just_watch.get_upcoming_cinema(weeks_offset=3, nationwide_cinema_releases_only=False)
Note: Default country is AU
Read api_payload.txt for more information
Contributions
Contributions are welcome!
Please write unit tests for any new functionality :)
",70
apple/swift-source-compat-suite,Python,"Swift Source Compatibility Suite
Source compatibility is a strong goal for future Swift releases. To aid in this
goal, a community owned source compatibility test suite serves to regression
test changes to the compiler against a (gradually increasing) corpus of Swift
source code. Projects added to this test suite are periodically built against
the latest development versions of Swift as part of Swift's continuous
integration system, allowing Swift compiler developers to
understand the compatibility impact their changes have on real-world Swift
projects.
Current List of Projects
The current list of projects can be viewed on Swift.org.
Adding Projects
The Swift source compatibility test suite is community driven, meaning that open
source Swift project owners are encouraged to submit their projects that meet
the acceptance criteria for inclusion in the test suite. Projects added to the
suite serve as general source compatibility tests and are afforded greater
protection against unintentional source breakage in future Swift releases.
Acceptance Criteria
To be accepted into the Swift source compatibility test suite, a project must:

Target Linux, macOS, or iOS/tvOS/watchOS device
Be an Xcode or Swift Package Manager project (Carthage and CocoaPods are currently unsupported but are being explored to be supported in the future)
Support building on either Linux or macOS
Be contained in a publicly accessible git repository
Maintain a project branch that builds against Swift 3.0 compatibility mode
and passes any unit tests
Have maintainers who will commit to resolve issues in a timely manner
Be compatible with the latest GM/Beta versions of Xcode and swiftpm
Add value not already included in the suite
Be licensed with one of the following permissive licenses:

BSD
MIT
Apache License, version 2.0
Eclipse Public License
Mozilla Public License (MPL) 1.1
MPL 2.0
CDDL



Note: Linux compatibility testing in continuous integration is not available
yet, but Linux projects are being accepted now.
Adding a Project
To add a project meeting the acceptance criteria to the suite, perform the
following steps:

Ensure the project builds successfully at a chosen commit against
Swift 3.0 GM
Create a pull request against the source compatibility suite
repository,
modifying projects.json to include a reference to the project being added
to the test suite.

The project index is a JSON file that contains a list of repositories containing
Xcode and/or Swift Package Manager target actions.
To add a new Swift Package Manager project, use the following template:
{
  ""repository"": ""Git"",
  ""url"": ""https://github.com/example/project.git"",
  ""path"": ""project"",
  ""branch"": ""master"",
  ""maintainer"": ""email@example.com"",
  ""compatibility"": [
    {
      ""version"": ""3.0"",
      ""commit"": ""195cd8cde2bb717242b3081f9c367ccd0a2f0121""
    }
  ],
  ""platforms"": [
    ""Darwin""
  ],
  ""actions"": [
    {
      ""action"": ""BuildSwiftPackage"",
      ""configuration"": ""release""
    },
    {
      ""action"": ""TestSwiftPackage""
    }
  ]
}
The compatibility field contains a list of version dictionaries, each
containing a Swift version and a commit. Commits are checked out before
building a project in the associated Swift version compatibility mode. The
Swift version is the earliest version of Swift known to compile the project at
the given commit. The goal is to have multiple commits at different points in a
project's history that are compatible with all supported Swift version
compatibility modes.
The platforms field specifies the platforms that can be used to build the
project. Linux and Darwin can currently be specified.
If tests aren't supported, remove the test action entry.
To add a new Swift Xcode workspace, use the following template:
{
  ""repository"": ""Git"",
  ""url"": ""https://github.com/example/project.git"",
  ""path"": ""project"",
  ""branch"": ""master"",
  ""maintainer"": ""email@example.com"",
  ""compatibility"": [
    {
      ""version"": ""3.0"",
      ""commit"": ""195cd8cde2bb717242b3081f9c367ccd0a2f0121""
    }
  ],
  ""platforms"": [
    ""Darwin""
  ],
  ""actions"": [
    {
      ""action"": ""BuildXcodeWorkspaceScheme"",
      ""workspace"": ""project.xcworkspace"",
      ""scheme"": ""project OSX"",
      ""destination"": ""platform=macOS"",
      ""configuration"": ""Release""
    },
    {
      ""action"": ""BuildXcodeWorkspaceScheme"",
      ""workspace"": ""project.xcworkspace"",
      ""scheme"": ""project iOS"",
      ""destination"": ""generic/platform=iOS"",
      ""configuration"": ""Release""
    },
    {
      ""action"": ""BuildXcodeWorkspaceScheme"",
      ""workspace"": ""project.xcworkspace"",
      ""scheme"": ""project tvOS"",
      ""destination"": ""generic/platform=tvOS"",
      ""configuration"": ""Release""
    },
    {
      ""action"": ""BuildXcodeWorkspaceScheme"",
      ""workspace"": ""project.xcworkspace"",
      ""scheme"": ""project watchOS"",
      ""destination"": ""generic/platform=watchOS"",
      ""configuration"": ""Release""
    },
    {
      ""action"": ""TestXcodeWorkspaceScheme"",
      ""workspace"": ""project.xcworkspace"",
      ""scheme"": ""project OSX"",
      ""destination"": ""platform=macOS""
    },
    {
      ""action"": ""TestXcodeWorkspaceScheme"",
      ""workspace"": ""project.xcworkspace"",
      ""scheme"": ""project iOS"",
      ""destination"": ""platform=iOS Simulator,name=iPhone 7""
    },
    {
      ""action"": ""TestXcodeWorkspaceScheme"",
      ""workspace"": ""project.xcworkspace"",
      ""scheme"": ""project tvOS"",
      ""destination"": ""platform=tvOS Simulator,name=Apple TV 1080p""
    }
  ]
}
To add a new Swift Xcode project, use the following template:
{
  ""repository"": ""Git"",
  ""url"": ""https://github.com/example/project.git"",
  ""path"": ""project"",
  ""branch"": ""master"",
  ""maintainer"": ""email@example.com"",
  ""compatibility"": [
    {
      ""version"": ""3.0"",
      ""commit"": ""195cd8cde2bb717242b3081f9c367ccd0a2f0121""
    }
  ],
  ""platforms"": [
    ""Darwin""
  ],
  ""actions"": [
    {
      ""action"": ""BuildXcodeProjectTarget"",
      ""project"": ""project.xcodeproj"",
      ""target"": ""project"",
      ""destination"": ""generic/platform=iOS"",
      ""configuration"": ""Release""
    }
  ]
}
After adding a new project to the index, ensure it builds successfully at the
pinned commits against the specified versions of Swift. In the examples,
the commits are specified as being compatible with Swift 3.0, which is included
in Xcode 8.0.
# Select Xcode 8.0 GM
sudo xcode-select -s /Applications/Xcode.app
# Build project at pinned commit against selected Xcode
./project_precommit_check project-path-field --earliest-compatible-swift-version 3.0
On Linux, you can build against the Swift 3.0 release toolchain:
curl -O https://swift.org/builds/swift-3.0-release/ubuntu1510/swift-3.0-RELEASE/swift-3.0-RELEASE-ubuntu15.10.tar.gz
tar xzvf swift-3.0-RELEASE-ubuntu15.10.tar.gz
./project_precommit_check project-path-field --earliest-compatible-swift-version 3.0 --swiftc swift-3.0-RELEASE-ubuntu15.10/usr/bin/swiftc
Maintaining Projects
In the event that Swift introduces a change that breaks source compatibility
with a project (e.g., a compiler bug fix that fixes wrong behavior in the
compiler), project maintainers are expected to update their projects and submit
a new pull request with the updated commit hash within two weeks of being
notified. Otherwise, unmaintained projects may be removed from the project
index.
Pull Request Testing
Pull request testing against the Swift source compatibility suite can be
executed by commenting with @swift-ci Please test source compatibility in a
Swift pull request.
Building Projects
To build all projects against a specified Swift compiler locally, use the
runner.py utility as shown below.
./runner.py --swift-branch master --projects projects.json --include-actions 'action.startswith(""Build"")' --swiftc path/to/swiftc
Use the --include-repos flag to build a specific project.
./runner.py --swift-branch master --projects projects.json --include-actions 'action.startswith(""Build"")' --include-repos 'path == ""Alamofire""' --swiftc path/to/swiftc
By default, build output is redirected to per-action .log files in the current
working directory. To change this behavior to output build results to standard
out, use the --verbose flag.
Marking actions as expected failures
When an action is expected to fail for an extended period of time, it's
important to mark the action as an expected failure to make new failures more
visible.
To mark an action as an expected failure, add an xfail entry for the correct
Swift version and branch to the failing actions, associating each with a link
to a JIRA reporting the relevant failure. The following is an example of an
action that's XFAIL'd when building against Swift master branch in 3.0
compatibility mode.
{
  ""repository"": ""Git"",
  ""url"": ""https://github.com/example/project.git"",
  ""path"": ""project"",
  ""branch"": ""master"",
  ""maintainer"": ""email@example.com"",
  ""compatibility"": [
    {
      ""version"": ""3.0"",
      ""commit"": ""195cd8cde2bb717242b3081f9c367ccd0a2f0121""
    }
  ],
  ""platforms"": [
    ""Darwin""
  ],
  ""actions"": [
    {
      ""action"": ""BuildXcodeProjectTarget"",
      ""project"": ""project.xcodeproj"",
      ""target"": ""project"",
      ""destination"": ""generic/platform=iOS"",
      ""configuration"": ""Release"",
      ""xfail"": {
        ""compatibility"": {
          ""3.0"": {
            ""branch"": {
              ""master"": ""https://bugs.swift.org/browse/SR-9999""
            }
          }
        }
      }
    }
  ]
}
Additional Swift branches and versions can be added to XFAIL different
configurations.
",197
MaksimRudnev/LittleHelpers,R,"LittleHelpers
Continuously updated collection of little helpers (tm) that facilitates my life in analyzing data (mostly comparative datasets) with R.
Use devtools::install_github(""maksimrudnev/LittleHelpers"") to install.
Overview

Multilevel helpers
Multigroup helpers
Tools for labelled data and Rstudio viewer
Pipe helpers
Values, Schwartz, ESS
Miscellaneous

Multilevel helpers
Explore multilevel data:

cor_within prints and plots individual correlations within each group.
cor_between computes means and shows group-level correlation between two variables.
scatter_means_ci Computes means by group and plots on scatterplot against each other (shows country-level correlations).
graph_means_ci Plots means by group.
stacked_bar Computes proportions cross-table and plots them in a nice way, returns ggplot object, so any further +theme(), +scale_x(), etc. codes can be added.

Recode multilevel data:

aggr_and_merge helps to create group-level variables from individual-level variables and merge them back to the data.frame on the go.
grand_center Quick grand-mean centering.
group_center Quick group-mean centering.

Summarize and visualize multilevel regressions:

good_table Large function that creates customizable coefficients tables using multiple lmer models; outputs in Rstudio viewer.
potential_interactions Exploratory. If you have no idea what cross-level interactions to look for. Computes pairwise tests of all the possible interactions in the lmer() model, or simply shows correlations between random effects and group-level variables.
random_interaction Plots cross-level interactions for lmer()-fitted models. Customizable. Can automatically choose real moderator values close to mean+-(2)SD.
random_plot Plots random effects from lmer()-fitted models.

Compute extra stats for multilevel regressions:

explained_variance.merMod Computes psudo-R-square for two-level regressions fitted with lmer().
vif_mer Compute variance inflation factor for multilevel regressions fitted with lmer().

Multigroup helpers


lavTestScore_clean Wrapper around lavaan::lavTestScore(), merging parameter labels with parameters and groups names and adding stars. Useful when you decide with between-group contraints might be relaxed.


mgcfa_diagnose Print comprehensible output to diagnose problems with MGCFA models.


mi_test Series of measurement invariance tests, analoigous to semTools::measurementInvariance().


See also Measurement invariance explorer - Shiny App


Pipe helpers
Branching/ramifying pipes
Imagine you need to create a list with means, correlations, and regression results. And you like to do it in one single pipe. In general, it is not possible, and you'll have to start a second pipe, probably doing some redundant computations.
Three little functions that allow for branching pipes. It is against Hadley's idea, as pipes are in principle linear, and in general I agree, but sometimes it would be comfy to ramify pipes away. It overcomes native magrittr %T>% by allowing more than one step after cutting the pipe.

ramify Saves current result into temporary object .buf and identifies a point in the pipe where branching will happen. Argument is an id of a ramification.
branch Starts a new branch from the ramify point. (branch(1) can be omitted, as ramify creates the first branch. Second argument is a family of branches, or parent branch. By default it uses the last parent branch created by the last used ramify.
harvest Returns contents of all the branches as a list.

Example that allows it:
data.frame(a=1:5, b=1/(1+exp(6:10)) ) %>%
  ramify(1) %>%
    branch(1) %>% colMeans %>% 
    branch(2) %>% lm(a ~ b, .) %>% broom::tidy(.) %>% 
    branch(3) %>% cor %>%
      ramify(2) %>%
        branch(1) %>% round(2) %>%
        branch(2) %>% psych::fisherz(.) %>%
      harvest(2) %>%
  harvest

Save'n'go & Append'n'go
savengo is ridiculously  simple but very useful function that saves objects from a middle of your pipe and passes the same object to further elements of the pipe. It allows more efficient debugging and less confusing code, in which you don't have to interrupt your pipe every time you need to save an output.
Its sister function appendngo appends an intermediary product to an existing list or a vector.
By analogy, one can create whatever storing function they need.
## Example 1
#Saves intermediary result as an object called intermediate.result

final.result <- dt %>% dplyr::filter(score<.5) %>%
                        savengo(""intermediate.result"") %>% 
                        dplyr::filter(estimated<0)
  
## Example 2
#Saves intermediary result as a first element of existing list myExistingList

final.result <- dt %>% dplyr::filter(score<.5) %>%
                        appendngo(myExistingList, after=0) %>% 
                        dplyr::filter(estimated<0)

Tools for labelled data and Rstudio viewer
Know the labels:

label_book Creates a codebook for data.frames with labels.

Make use of labels:

cor_table Prints ready-to-publish correlation tables with significance stars.
crosstab Simple cross-tabulation with labels.

Get rid of labels:

drop_labs Drops labels if you don't need them.
untibble Get rid of tibble and get clean data.frame.
lab_to_fac Converts labelled variables to factors.

Make use of Rstudio viewer:

df_to_viewer Puts any data.frame to RStudio viewer. Also works with models and anything that can be passed through stargazer.

Values, Schwartz, ESS

values list of value labels.
download_ess Download European Social Survey data
schwartz_circle Draw Schwartz circle and more with three simple functions: add_circle, add_radius, and add_label.
ess_values Computes 2, 4, or 10 value indices as they are measured in ESS.

Miscellaneous

reverse Recodes variable in reverse order. Works with labels.
replace_by_table Useful for recoding when matching tables are alsready specified in a table. Particularly useful for translation.
mean_se_lower_upper Simply mean, SE, upper and lower 95% CI.
verb Simply prints its arguments.
rename Renames variables in data.frame without bullshittery.
theme_mr Clean theme for ggplot.

News

plef

",3
alexherbo2/configuration.chrome,JavaScript,"Chrome – Configuration

Completed extension for a Dead simple Kakoune support for Chrome.

Installation
make
Open chrome://extensions in your browser, enable Developer mode then Load unpacked to select the extension directory.

",3
ryanelandt/PressureFieldContact.jl,Julia,"PressureFieldContact.jl



This module implements the elastic foundation-themed contact model for rigid body dynamics described in this short video.
This paper describes the method in greater detail.
UNDER CONSTRUCTION: See the latest documentation for installation instructions, a quick-start guide and summary of how the different pieces of this method work.
Summary
The surface of bodies are represented with a triangular mesh.
The compliant portion of bodies is represented with a tetrahedral mesh.
The easiest way to get started is to run the boxes.jl example in the test directory.
More examples will come soon.
Friction Models
This package can model friction using either a regularized Coulomb friction model or a bristle friction model.
A regularized Coulomb friction model is simple at the cost of allowing creep (i.e. a block will slide down a ramp irrespective of slope).
The bristle friction model adds extra state variables that model deformation.
This friction model allows objects to have zero tangential velocity even when applied tangential forces are not zero.
Try both to see which is best for your application.
",2
Genivia/ugrep,C++,"ugrep: universal grep
Offers powerful pre-defined search patterns and quick options to selectively
search source code files efficiently in large directory trees.
ugrep uses RE/flex for
high-performance regex matching, which is 100 times faster than the GNU C
POSIX.2 regex library used by GNU grep and 10 times faster than PCRE2 and RE2.
Because RE/flex is a streaming regex matcher, ugrep scans files more
efficiently with options like -o, permitting pattern matches that span
multiple lines instead of searching per line as with other grep utilities.
ugrep makes it easy to search source code.  It is the only grep tool that
allows you to define negative patterns to ""zap"" parts in files you want to
skip.  This removes many false positives.  For example to find exact matches of
main in C/C++ source code while skipping strings and comments that may have a
match with main in them:
ugrep -r -o -tc,c++ -n -w 'main' -f patterns/c/zap_strings -f patterns/c/zap_comments myprojects

where -r is recursive search, -o for multi-line matches (since strings and
comments may span multiple lines), -tc,c++ searches C and C++ source code
files only, -n shows line numbers in the output, -w matches exact words
(for example, mainly won't be matched), the -f options specify two
pre-defined patterns to match and ignore strings and comments in the input.
ugrep searches source code files by file name extension and other criteria
using option -t so specify the type of files to search recursively in a
directory tree with option -r, e.g. -r -tc++.
ugrep includes a growing database of
patterns with common
search patterns to use with option -f.  So you don't need to memorize complex
regex patterns for common search criteria.  Environment variable GREP_PATH
can be set to point to your own directory with patterns that option -f uses
to read your pattern files.
ugrep offers options that are compatible with the
GNU grep and BSD grep
utilities, and can be used as a more powerful replacement of these.
ugrep matches Unicode patterns.  The regular expression syntax is POSIX ERE
compliant, extended with Unicode character classes, lazy quantifiers, and
negative patterns to skip unwanted pattern matches to produce more precise
results.
ugrep searches UTF-encoded input when UTF BOM
(byte order mark) are present
and ASCII and UTF-8 when no UTF BOM is present.  Option --file-format permits
many other file formats to be searched, such as ISO-8859-1, EBCDIC, and code
pages 437, 850, 858, 1250 to 1258.
ugrep regex patterns are converted to
DFAs for fast
matching.  Rare and pathelogical cases are known to exist that may increase the
initial running time for DFA construction.  The resulting DFAs still yield
significant speedups to search large files.
ugrep is portable and compiles with MSVC++ to run on Windows.
ugrep is free BSD-3 source
code and does not include any GNU or BSD grep open source code or algorithms.
ugrep is built entirely on the RE/flex open source library and Rich Salz'
free and open wildmat source code for glob matching with options --include
and --exclude.
ugrep is evolving and more features will be added.  You can help!  We love
your feedback (issues) and contributions (pull requests) ❤️
Speed
Initial performance results look promising.  For example, searching for all
matches of syntactically-valid variants of #include ""..."" in the directory
tree from the Qt 5.9.2 root, restricted to .h, .hpp, and .cpp files only:
time egrep -r -o '#[ \t]*include[ \t]+""[^""]+""' --include='*.h' --include='*.hpp' --include='*.cpp' . >& /dev/null
3.630u 0.274s 0:03.90 100.0%    0+0k 0+0io 0pf+0w

time ugrep -r -o '#[ \t]*include[ \t]+""[^""]+""' -Oh,hpp,cpp . >& /dev/null
0.837u 0.185s 0:01.02 99.0%     0+0k 0+0io 0pf+0w

Unoptimized, ugrep is already 3 times faster than BSD egrep (ugrep was
compiled with clang 9.0.0 -O2, and this test was run on a 2.9 GHz Intel Core
i7, 16 GB 2133 MHz LPDDR3 machine).
Dependencies
https://github.com/Genivia/RE-flex
Installation
First install RE/flex from https://github.com/Genivia/RE-flex then download
ugrep from https://github.com/Genivia/ugrep and execute:
$ ./configure; make

This builds ugrep in the src directory.  You can tell which version it is
with:
$ src/ugrep -V
ugrep 1.1.0 x86_64-apple-darwin16.7.0

Optionally, install the ugrep utility and the ugrep manual page:
$ sudo make install
$ ugrep -V
ugrep 1.1.0 x86_64-apple-darwin16.7.0

Examples
Searching source code
To search for the identifier main as a word (-w) recursively (-r) in
directory myproject, showing the matching line (-n) and column (-k)
numbers next to the lines matched:
ugrep -r -n -k -w 'main' myproject

But this search query also finds main in strings and comment blocks.  With
ugrep we can use ""negative patterns"" of the form (?^...) to ignore
unwanted matches in C/C++ quoted strings and comment blocks.  Because strings
and comment blocks may span multiple lines, we should use -o:
ugrep -r -o -nkw 'main' '(?^""(\\.|\\\r?\n|[^\\\n""])*""|//.*|/\*([^*]|(\*+[^*/]))*\*+\/)' myproject

This is a lot of work to type in correctly!  If you are like me, I'm lazy and
don't want to spend time fiddling with regex patterns when I am working on
something more important.  There is an easier way by using ugrep's
pre-defined patterns (-f):
ugrep -r -o -nkw 'main' -f patterns/c/zap_strings -f patterns/c/zap_comments myproject

This query also searches through other files than C/C++ source code, like
READMEs, Makefiles, and so on.  So let's refine this query by selecting C/C++
files only using option -tc,c++:
ugrep -r -o -tc,c++ -nkw 'main' -f patterns/c/zap_strings -f patterns/c/zap_comments myproject

As another example, we may want to search for word FIXME in C/C++ comment
blocks.  To do so we can first select the comment blocks with ugrep's
pre-defined c/comments pattern AND THEN select lines with FIXME using a
pipe:
ugrep -r -o -tc,c++ -nk -f patterns/c/comments myproject | ugrep -w 'FIXME'

Filtering results this way with pipes is generally easier than using AND-OR
logic that some search tools use.  This approach follows the Unix spirit to
keep utilities simple and use them in combination for more complex tasks.
Say we want to produce a sorted list of all identifiers found in Java source
code while skipping strings and comments:
ugrep -r -o -tjava -f patterns/java/names -f patterns/java/zap_strings -f patterns/java/zap_comments myproject | sort -u

This matches Java Unicode identifiers using the regex
\p{JavaIdentifierStart}\p{JavaIdentifierPart}* defined in
patterns/java/names.
With traditional grep and grep-like tools it takes great effort to recursively
search for the C/C++ source file that defines function qsort, requiring
something like this:
ugrep -r --include='*.c' --include='*.cpp' '^([ \t]*[[:word:]:*&]+)+[ \t]+qsort[ \t]*\([^;\n]+$' myproject

Fortunately, with ugrep we can simply select all function definitions in
files with extension .c or .cpp by using option -Oc,cpp and by using a
pre-defined pattern function_defs to produce all function definitions.  Then
we select the one we want:
ugrep -r -o -Oc,cpp -nk -f patterns/c/function_defs myproject | ugrep 'qsort'

Note that we could have used -tc,c++ to select C/C++ files, but this also
includes header files when we want to only search .c and .cpp files.  To
display the list of file name extensions searched for all available options for
-t use:
ugrep -tlist

We can also skip files and directories from being searched that are defined in
.gitignore.  To do so we use --exclude-from to specify a file with files
and directories (declared as glob patterns) to ignore:
ugrep -r -tc++ --color --exclude-from='.gitignore' -f patterns/c++/defines .

While searching C++ files (-tc++) in the current directory (.)for #define
lines (-f patterns/c++/defines), this query skips file config.h and other
files and directories declared in .gitignore.
To highlight matches when pushed through a chain of pipes we should use
--color=always:
ugrep -r -tc++ --color=always --exclude-from='.gitignore' -f patterns/c++/defines . | ugrep -w 'Foo.*'

To list all files in a GitHub project directory that are not ignored by
.gitignore:
ugrep -r -l '' --exclude-from='.gitignore' .

Where -l (files with matches) lists the files specified in .gitignore
matched by the empty pattern '', which is typically used to match any
non-empty file (as per POSIX.1 compliance).
Note that the complement of --exclude is not --include, so we cannot
reliably list the files that are ignored with --include-from='.gitignore'.
Only files explicitly specified with --include and directories explicitly
specified with --include-dir are visited.  The --include-from from lists
globs that are considered both files and directories to add to --include and
--include-dir, respectively.  This means that when a directory or directory
path is not explicitly listed in this file then it will not be visited using
--include-from.
Using Unicode
To display lines with Unicode words in places.txt:
ugrep '\w+' places.txt

To produce a sorted list of all ASCII words in places.txt:
ugrep '[[:word:]]+' places.txt

To display all lines containing laughing face emojis in birthday.txt:
ugrep '[😀-😏]' birthday.txt

Likewise, we can use the following for the same results:
ugrep '[\x{1F600}-\x{1F60F}]' birthday.txt

To display lines containing the names Gödel (or Goedel), Escher, or Bach:
ugrep 'G(ö|oe)del|Escher|Bach' GEB.txt wiki.txt

To display lines that do not contain the names Gödel (or Goedel), Escher, or
Bach we use option -v (invert match):
ugrep -v 'G(ö|oe)del|Escher|Bach' GEB.txt wiki.txt

To count the number of lines containing the names Gödel (or Goedel), Escher, or
Bach we use option -c:
ugrep -c 'G(ö|oe)del|Escher|Bach' GEB.txt wiki.txt

To count the total number of occurrences of the names Gödel (or Goedel),
Escher, or Bach we use options -c and -g (don't group matches on the same
line):
ugrep -c -g 'G(ö|oe)del|Escher|Bach' GEB.txt wiki.txt

To check if myfile contains any non-ASCII Unicode characters we use pattern
[^[:ascii:]] (not ASCII) and option -q (quick) that only sets the ugrep
exit status to 0 (success) or 1 (failure):
ugrep -q '[^[:ascii:]]' myfile && echo ""contains Unicode""

To check if a file has any invalid Unicode characters:
ugrep -q '[^\p{Unicode}--[\xFFFD]]' myfile && echo ""contains invalid Unicode""

In this example we included the Unicode code point U+FFFD as an error for
illustrative purposes, because it is often used to flag invalid UTF encodings.
To search for lorem in lower or upper case (option -i case insensitive) in
a UTF-16 file (with UTF-16 BOM), while color-highlighting the matches:
ugrep --color -i -w 'lorem' utf16lorem.txt

When utf16lorem.txt has no UTF-16 BOM we can specify UTF-16 file encoding:
ugrep --file-format=UTF-16 -i -w 'lorem' utf16lorem.txt

Man page
UGREP(1)                         User Commands                        UGREP(1)



NAME
       ugrep -- universal file pattern searcher

SYNOPSIS
       ugrep [OPTIONS] [-A NUM] [-B NUM] [-C[NUM]] [PATTERN] [-e PATTERN]
             [-f FILE] [--file-type=TYPES] [--file-format=ENCODING]
             [--colour[=WHEN]|--color[=WHEN]] [--label[=LABEL]] [FILE ...]

DESCRIPTION
       The  ugrep utility searches any given input files, selecting lines that
       match one or more patterns.  By default, a  pattern  matches  an  input
       line  if  the  regular expression (RE) in the pattern matches the input
       line without its trailing newline.  An empty expression  matches  every
       line.   Each  input  line  that matches at least one of the patterns is
       written to the standard output.

       The ugrep utility normalizes Unicode input, so ugrep  can  be  used  to
       search  for  Unicode  patterns  in text files encoded in UTF-8, UTF-16,
       UTF-32 by detecting UTF BOM in the input.  When no UTF BOM is detected,
       ugrep  searches  for  Unicode  patterns  in UTF-8 input, which includes
       ASCII input.  ugrep searches input files encoded in ISO-8859-1, EBCDIC,
       CP-437,  CP-850, CP-858, CP-1250 to CP-1258 when the file encoding for-
       mat is specified with option --file-format.

       The following options are available:

       -A NUM, --after-context=NUM
              Print NUM  lines  of  trailing  context  after  matching  lines.
              Places a --group-separator between contiguous groups of matches.
              See also the -B and -C options.

       -B NUM, --before-context=NUM
              Print NUM  lines  of  leading  context  before  matching  lines.
              Places a --group-separator between contiguous groups of matches.
              See also the -A and -C options.

       -b, --byte-offset
              The offset in bytes of a matched line is displayed in  front  of
              the respective matched line.  With option -g displays the offset
              in bytes of each pattern matched.

       -C[NUM], --context[=NUM]
              Print NUM lines of leading and trailing context surrounding each
              match.  The default is 2 and is equivalent to -A 2 -B 2.  Places
              a --group-separator between contiguous groups of matches.  Note:
              no  whitespace may be given between the option and its argument.

       -c, --count
              Only a count of selected lines is written  to  standard  output.
              When used with option -g, counts the number of patterns matched.
              With option -v, counts the number of non-matching lines.

       --colour[=WHEN], --color[=WHEN]
              Mark up the matching text with  the  expression  stored  in  the
              GREP_COLOR  or  GREP_COLORS  environment variable.  The possible
              values of WHEN can be `never', `always' or `auto'.

       -D ACTION, --devices=ACTION
              If an input file is a device, FIFO  or  socket,  use  ACTION  to
              process  it.   By  default,  ACTION  is `read', which means that
              devices are read just as if they were ordinary files.  If ACTION
              is `skip', devices are silently skipped.

       -d ACTION, --directories=ACTION
              If  an  input file is a directory, use ACTION to process it.  By
              default, ACTION is `read', i.e., read  directories  just  as  if
              they  were  ordinary  files.  If ACTION is `skip', silently skip
              directories.  If ACTION is `recurse', read all files under  each
              directory,  recursively,  following  symbolic links only if they
              are on the command line.  This is equivalent to the  -r  option.
              If  ACTION  is  `dereference-recurse', read all files under each
              directory,  recursively,  following  symbolic  links.   This  is
              equivalent to the -R option.

       -E, --extended-regexp
              Interpret  patterns as extended regular expressions (EREs). This
              is the default.

       -e PATTERN, --regexp=PATTERN
              Specify a PATTERN used during the search of the input: an  input
              line  is  selected  if it matches any of the specified patterns.
              This option is most useful when multiple -e options are used  to
              specify  multiple  patterns,  when  a pattern begins with a dash
              (`-'), or to specify a pattern after option -f.

       --exclude=GLOB
              Skip files whose name matches GLOB (using wildcard matching).  A
              glob  can  use  *,  ?,  and [...] as wildcards, and \ to quote a
              wildcard or backslash character literally.  If GLOB contains  /,
              full  pathnames  are  matched.  Otherwise basenames are matched.
              Note that --exclude patterns take priority over  --include  pat-
              terns.  This option may be repeated.

       --exclude-dir=GLOB
              Exclude  directories  whose  name  matches  GLOB  from recursive
              searches.  If GLOB contains /, full pathnames are matched.  Oth-
              erwise  basenames are matched.  Note that --exclude-dir patterns
              take priority over --include-dir patterns.  This option  may  be
              repeated.

       --exclude-from=FILE
              Read  the  globs  from FILE and skip files and directories whose
              name matches one or more globs (as if specified by --exclude and
              --exclude-dir).   Lines  starting  with a `#' and empty lines in
              FILE ignored. This option may be repeated.

       -F, --fixed-strings
              Interpret pattern as a set of fixed strings, separated  by  new-
              lines,  any  of  which  is  to be matched.  This forces ugrep to
              behave as fgrep but less efficiently.

       -f FILE, --file=FILE
              Read one or more newline-separated patterns  from  FILE.   Empty
              pattern  lines  in  the file are not processed.  Options -F, -w,
              and -x do not apply to FILE patterns.  If FILE does  not  exist,
              uses the GREP_PATH environment variable to attempt to open FILE.
              This option may be repeated.

       -G, --basic-regexp
              Interpret pattern as a  basic  regular  expression  (i.e.  force
              ugrep to behave as traditional grep).

       -g, --no-group
              Do  not  group  pattern  matches  on the same line.  Display the
              matched line again for each additional pattern match, using  `+'
              as the field separator for each additional line.

       --group-separator=SEP
              Use SEP as a group separator for context options -A, -B, and -C.
              By default SEP is a double hyphen (`--').

       -H, --with-filename
              Always print the  filename  with  output  lines.   This  is  the
              default when there is more than one file to search.

       -h, --no-filename
              Never print filenames with output lines.

       --help Print a help message.

       -i, --ignore-case
              Perform   case   insensitive   matching.   This  option  applies
              case-insensitive matching of ASCII characters in the input.   By
              default, ugrep is case sensitive.

       --include=GLOB
              Search only files whose name matches GLOB (using wildcard match-
              ing).  A glob can use *, ?, and [...] as  wildcards,  and  \  to
              quote a wildcard or backslash character literally.  If GLOB con-
              tains /, file pathnames are matched.  Otherwise  file  basenames
              are  matched.   Note  that --exclude patterns take priority over
              --include patterns.  This option may be repeated.

       --include-dir=GLOB
              Only directories whose name matches GLOB are included in  recur-
              sive  searches.  If GLOB contains /, full pathnames are matched.
              Otherwise basenames are matched.  Note that  --exclude-dir  pat-
              terns  take  priority  over --include-dir patterns.  This option
              may be repeated.

       --include-from=FILE
              Read the globs from FILE and search only files  and  directories
              whose  name  matches  one  or  more  globs  (as  if specified by
              --include and --include-dir).  Lines starting  with  a  `#'  and
              empty lines in FILE are ignored.  This option may be repeated.

       -k, --column-number
              The  column number of a matched pattern is displayed in front of
              the respective matched line, starting at  column  1.   Tabs  are
              expanded when columns are counted.

       -L, --files-without-match
              Only  the names of files not containing selected lines are writ-
              ten to standard output.  Pathnames  are  listed  once  per  file
              searched.   If  the  standard  input  is  searched,  the  string
              ``(standard input)'' is written.

       -l, --files-with-matches
              Only the names of files containing selected lines are written to
              standard  output.   ugrep  will only search a file until a match
              has been found,  making  searches  potentially  less  expensive.
              Pathnames  are  listed  once per file searched.  If the standard
              input is searched, the string ``(standard input)'' is written.

       --label[=LABEL]
              Displays the LABEL value when input is read from standard  input
              where a file name would normally be printed in the output.  This
              option applies to options -H, -L, and -l.

       --line-buffered
              Force output to be line buffered.  By default,  output  is  line
              buffered  when  standard output is a terminal and block buffered
              otherwise.

       -m NUM, --max-count=NUM
              Stop reading the input after NUM matches.

       -N, --only-line-number
              The line number of the match in the file is output without  dis-
              playing  the  match.   The line number counter is reset for each
              file processed.

       -n, --line-number
              Each output line is preceded by its relative line number in  the
              file,  starting at line 1.  The line number counter is reset for
              each file processed.

       --no-group-separator
              Removes the group separator line from  the  output  for  context
              options -A, -B, and -C.

       -O EXTENSIONS, --file-extensions=EXTENSIONS
              Search only files whose file name extensions match the specified
              comma-separated list of file name EXTENSIONS.   This  option  is
              the same as specifying --include='*.ext' for each extension name
              `ext' in the EXTENSIONS list.  This option may be repeated.

       -o, --only-matching
              Prints only the matching part of the lines.   Allows  a  pattern
              match  to  span  multiple  lines.   Line  numbers for multi-line
              matches are displayed with option -n, using  `|'  as  the  field
              separator for each additional line matched by the pattern.  Con-
              text options -A, -B, and -C are disabled.

       -P, --perl-regexp
              Interpret PATTERN as a Perl regular expression.  This feature is
              not yet available.

       -p, --no-dereference
              If -R is specified, no symbolic links are followed.  This is the
              default.

       -q, --quiet, --silent
              Quiet mode: suppress normal output.  ugrep will  only  search  a
              file  until  a match has been found, making searches potentially
              less expensive.  Allows a pattern match to span multiple  lines.

       -R, --dereference-recursive
              Recursively  read  all  files  under each directory.  Follow all
              symbolic links, unlike -r.

       -r, --recursive
              Recursively read all files under each directory, following  sym-
              bolic links only if they are on the command line.

       -S, --dereference
              If  -R  is  specified,  all  symbolic  links  are followed.  The
              default is not to follow symbolic links.

       -s, --no-messages
              Silent mode.  Nonexistent and unreadable files are ignored (i.e.
              their error messages are suppressed).

       -T, --initial-tab
              Add  a  tab space to separate the file name, line number, column
              number, and byte offset with the matched line.

       -t TYPES, --file-type=TYPES
              Search only files of TYPES, which is a comma-separated  list  of
              file  types.   Each  file  type is associated with a set of file
              name extensions to search.  This option may  be  repeated.   The
              possible  values  of  type  can  be  (use  -t  list to display a
              detailed list): `actionscript',  `ada',  `asm',  `asp',  `aspx',
              `autoconf',  `automake',  `awk', `basic', `batch', `bison', `c',
              `c++', `clojure',  `csharp',  `css',  `csv',  `dart',  `delphi',
              `elixir',   `erlang',   `fortran',  `go',  `groovy',  `haskell',
              `html', `jade', `java', `javascript',  `json',  `jsp',  `julia',
              `kotlin',  `less', `lex', `lisp', `lua', `m4', `make', `matlab',
              `objc', `objcpp', `ocaml', `parrot',  `pascal',  `perl',  `php',
              `prolog',   `python',   `R',  `rst',  `ruby',  `rust',  `scala',
              `scheme', `shell', `smalltalk', `sql',  `swift',  `tcl',  `tex',
              `text',  `tt',  `typescript',  `verilog',  `vhdl', `vim', `xml',
              `yacc', `yaml'

       --tabs=NUM
              Set the tab size to NUM to expand tabs for option -k.  The value
              of NUM may be 1, 2, 4, or 8.

       -V, --version
              Display version information and exit.

       -v, --invert-match
              Selected  lines are those not matching any of the specified pat-
              terns.

       -w, --word-regexp
              The pattern or -e patterns are searched for as  a  word  (as  if
              surrounded by `\<' and `\>').

       -X, --free-space
              Spacing (blanks and tabs) in regular expressions are ignored.

       -x, --line-regexp
              Only  input lines selected against the entire pattern or -e pat-
              terns are considered to be matching lines (as if surrounded by ^
              and $).

       -Y ENCODING, --file-format=ENCODING
              The  input file format.  The possible values of ENCODING can be:
              `binary', `ISO-8859-1', `ASCII',  `EBCDIC',  `UTF-8',  `UTF-16',
              `UTF-16BE',   `UTF-16LE',   `UTF-32',   `UTF-32BE',  `UTF-32LE',
              `CP437',  `CP850',  `CP1250',  `CP1251',   `CP1252',   `CP1253',
              `CP1254', `CP1255', `CP1256', `CP1257', `CP1258'

       -y     Equivalent to -i.  Obsoleted.

       -Z, --null
              Prints a zero-byte after the file name.

       -z SEP, --separator=SEP
              Use  SEP as field separator between file name, line number, col-
              umn number, byte offset, and the matched line.  The default is a
              colon (`:').

       The  regular expression pattern syntax is an extended form of the POSIX
       ERE syntax.  For an overview of the syntax see README.md or visit:

              https://github.com/Genivia/ugrep

       Note that `.' matches any non-newline character.   Matching  a  newline
       character  is  not possible in line-buffered mode.  Pattern matches may
       span multiple lines in block-buffered mode, which is enabled by one  of
       the options -c, -o, or -q (unless combined with option -v).

       If  no  file arguments are specified, or if `-' is specified, the stan-
       dard input is used.

EXIT STATUS
       The ugrep utility exits with one of the following values:

       0      One or more lines were selected.

       1      No lines were selected.

       >1     An error occurred.

GLOBBING
       Globbing is used by options --include,  --include-dir,  --include-from,
       --exclude,  --exclude-dir,  --exclude-from to match pathnames and base-
       names.  Globbing supports gitignore syntax and the corresponding match-
       ing  rules.  When a glob contains a path separator `/', the pathname is
       matched.  Otherwise the basename of a file  or  directory  is  matched.
       For   example,  *.h  matches  foo.h  and  bar/foo.h.   bar/*.h  matches
       bar/foo.h but not foo.h and not bar/bar/foo.h.  Use a  leading  `/'  to
       force /*.h to match foo.h but not bar/foo.h.

       Syntax:

       **/    Matches zero or more directories.

       /**    When at the end of a glob, matches everything after the /.

       *      Matches anything except a /.

       /      When  used at the begin of a glob, matches if pathname has no /.

       ?      Matches any character except a /.

       [a-z]  Matches one character in the selected range of characters.

       [^a-z] Matches one character not in the selected range of characters.

       [!a-z] Matches one character not in the selected range of characters.

       \?     Matches a ? (or any character after the backslash).

       Examples:


       **/a   Matches a, x/a, x/y/a,       but not b, x/b.

       a/**/b Matches a/b, a/x/b, a/x/y/b, but not x/a/b, a/b/x

       a/**   Matches a/x, a/y, a/x/y,     but not b/x

       a/*/b  Matches a/x/b, a/y/b,        but not a/x/y/b

       /a     Matches a,                   but not x/a

       /*     Matches a, b,                but not x/a, x/b

       a?b    Matches axb, ayb,            but not a, b, ab

       a[xy]b Matches axb, ayb             but not a, b, azb

       a[a-z]b
              Matches aab, abb, acb, azb,  but not a, b, a3b, aAb, aZb

       a[^xy]b
              Matches aab, abb, acb, azb,  but not a, b, axb, ayb

       a[^a-z]b
              Matches a3b, aAb, aZb        but not a, b, aab, abb, acb, azb

       Lines in the --exclude-from and --include-from files are  ignored  when
       empty  or  start  with  a `#'.  The prefix `!' to a glob in such a file
       negates the pattern match, i.e.  matching  files  are  excluded  except
       files  matching the globs prefixed with `!' in the --exclude-from file.

ENVIRONMENT
       GREP_PATH
              May be used to specify a file path to pattern files.   The  file
              path  is used by option -f to open a pattern file, when the file
              specified with option -f cannot be opened.

       GREP_COLOR
              May be used to specify ANSI SGR parameters to highlight  matches
              when  option --color is used, e.g. 1;35;40 shows pattern matches
              in bold magenta text on a black background.

       GREP_COLORS
              May be used to specify ANSI SGR parameters to highlight  matches
              and  other attributes when option --color is used.  Its value is
              a colon-separated list of ANSI SGR parameters that  defaults  to
              mt=1;31:sl=:cx=:fn=35:ln=32:cn=32:bn=32:se=36.   The  mt=,  ms=,
              and  mc=  capabilities  of  GREP_COLORS   have   priority   over
              GREP_COLOR.

GREP_COLORS
       sl=    SGR substring for selected lines.

       cx=    SGR substring for context lines.

       rv     Swaps the sl= and cx= capabilities when -v is specified.

       mt=    SGR substring for matching text in any matching line.

       ms=    SGR  substring  for  matching text in a selected line.  The sub-
              string mt= by default.

       mc=    SGR substring for matching text in a  context  line.   The  sub-
              string mt= by default.

       fn=    SGR substring for file names.

       ln=    SGR substring for line numbers.

       cn=    SGR substring for column numbers.

       bn=    SGR substring for byte offsets.

       se=    SGR substring for separators.

EXAMPLES
       To find all occurrences of the word `patricia' in a file:

              $ ugrep -w 'patricia' myfile

       To  count the number of lines containing the word `patricia' or `Patri-
       cia` in a file:

              $ ugrep -cw '[Pp]atricia' myfile

       To count the total number of times the word  `patricia'  or  `Patricia`
       occur in a file:

              $ ugrep -cgw '[Pp]atricia' myfile

       To list all Unicode words in a file:

              $ ugrep -o '\w+' myfile

       To list all ASCII words in a file:

              $ ugrep -o '[[:word:]]+' myfile

       To  list  all  laughing  face  emojis  (Unicode  code points U+1F600 to
       U+1F60F) in a file:

              $ ugrep -o '[\x{1F600}-\x{1F60F}]' myfile

       To check if a file contains any non-ASCII (i.e. Unicode) characters:

              $ ugrep -q '[^[:ascii:]]' myfile && echo ""contains Unicode""

       To list all C/C++ comments in a file displaying their line  and  column
       numbers using options -n and -k, and option -o that allows for matching
       patterns across multiple lines:

              $ ugrep -nko -e '//.*' -e '/\*([^*]|(\*+[^*/]))*\*+\/' myfile

       The same search, but using pre-defined patterns:

              $ ugrep -nko -f patterns/c_comments myfile

       To list the lines that need fixing in a C/C++ source  file  by  looking
       for  the word FIXME while skipping any FIXME in quoted strings by using
       a negative pattern `(?^X)' to ignore quoted strings:

              $ ugrep -no -e 'FIXME' -e '(?^""(\\.|\\\r?\n|[^\\\n""])*"")' myfile

BUGS
       Report bugs at:

              https://github.com/Genivia/ugrep/issues


LICENSE
       ugrep  is  released under the BSD-3 license.  All parts of the software
       have reasonable copyright terms permitting free  redistribution.   This
       includes the ability to reuse all or parts of the ugrep source tree.

SEE ALSO
       grep(1).



ugrep 1.1.0                      May 11, 2019                         UGREP(1)

ugrep versus other ""greps""

ugrep supports ""negative patterns"" to skip parts of the input that should
not be matched, such as skipping strings and comments when searching for
identifiers in source code.
When one or more of the options -q (quiet), -o (only matching), -N
(only line number), -l (file with match), or -L (files without match) is
used, ugrep performs an even faster streaming-based search of the input
file instead of reading the input line-by-line as other grep tools do.  This
allows matching patterns that include newlines (\n), i.e. a match can span
multiple lines.  This is not possible with other grep-like tools.
New option -k, --column-number with ugrep to display the column
number, taking tab spacing into account by expanding tabs, as specified by
option --tabs.
New option -g, --no-group to not group matches per line.  This option
displays a matched input line again for each additional pattern match.  This
option is particularly useful with option -c to report the total number of
pattern matches per file instead of the number of lines matched per file.
When option -b is used with option -o or with option -g, ugrep
displays the exact byte offset of the pattern match instead of the byte
offset of the start of the matched line as grep reports.  Reporting exact
byte offsets is now possible with grep.
ugrep regular expression patterns are more expressive than GNU grep and
BSD grep and support Unicode pattern matching, see further below.  Extended
regular expression syntax is the default (i.e.  option -E, as egrep).
ugrep always assumes UTF-8 locale to support Unicode, e.g.
LANG=en_US.UTF-8, wheras grep is locale-sensitive.
BSD grep (e.g. on Mac OS X) has bugs and limitations that ugrep fixes,
e.g.  options -r versus -R, support for GREP_COLORS, and more.

For future updates

Skip hidden files and directories, e.g. dot files and Windows hidden files.
However, skipping dot files and directories can already be done with
--exclude='.*' and --exclude-dir='.*', respectively.  Windows hidden
files are defined by their attributes returned by GetFileAttributesA.
Pattern ^$ does not match empty lines, because RE/flex find() does not
permit empty matches.  This can be fixed in RE/flex, but requires some work
and testing to avoid infinite find() loops on an empty match that does not
advance the input cursor.
Back-references are not supported.  This will likely not be supported soon
with the RE/flex library.  We could use Boost.Regex for this (using RE/flex
BoostMatcher class), which is faster than PCRE2 but slower than RE/flex
Matcher class.  With Boost.Regex we can also support Perl-like matching
as an option.
There are reported cases where lazy quantifiers misbehave when used in
negative patterns, so it is best to avoid them unless the patterns are
simple.
Not locale-sensitive, e.g. LC_COLLATE currently has no effect.

Pattern syntax
A pattern is an extended set of regular expressions, with nested sub-expression
patterns φ and ψ:



Pattern
Matches




x
matches the character x, where x is not a special character


.
matches any single character except newline (unless in dotall mode)


\.
matches . (dot), special characters are escaped with a backslash


\n
matches a newline, others are \a (BEL), \b (BS), \t (HT), \v (VT), \f (FF), and \r (CR)


\0
matches the NUL character


\cX
matches the control character X mod 32 (e.g. \cA is \x01)


\0177
matches an 8-bit character with octal value 177


\x7f
matches an 8-bit character with hexadecimal value 7f


\x{3B1}
matches Unicode character U+03B1, i.e. α


\u{3B1}
matches Unicode character U+03B1, i.e. α


\p{C}
matches a character in category C


\Q...\E
matches the quoted content between \Q and \E literally


[abc]
matches one of a, b, or c


[0-9]
matches a digit 0 to 9


[^0-9]
matches any character except a digit


φ?
matches φ zero or one time (optional)


φ*
matches φ zero or more times (repetition)


φ+
matches φ one or more times (repetition)


φ{2,5}
matches φ two to five times (repetition)


φ{2,}
matches φ at least two times (repetition)


φ{2}
matches φ exactly two times (repetition)


φ??
matches φ zero or once as needed (lazy optional)


φ*?
matches φ a minimum number of times as needed (lazy repetition)


φ+?
matches φ a minimum number of times at least once as needed (lazy repetition)


φ{2,5}?
matches φ two to five times as needed (lazy repetition)


φ{2,}?
matches φ at least two times or more as needed (lazy repetition)


φψ
matches φ then matches ψ (concatenation)


φ⎮ψ
matches φ or matches ψ (alternation)


(φ)
matches φ as a group


(?:φ)
matches φ as a group without capture


(?=φ)
matches φ without consuming it, i.e. lookahead (top-level φ, not for sub-patterns φ)


(?^φ)
matches φ and ignore it to continue matching (top-level φ, not for sub-patterns φ)


^φ
matches φ at the start of input or start of a line (top-level φ, not for sub-patterns φ)


φ$
matches φ at the end of input or end of a line (top-level φ, not for sub-patterns φ)


\Aφ
matches φ at the start of input (requires option -o) (top-level φ, not for sub-patterns φ)


φ\z
matches φ at the end of input (requires option -o) (top-level φ, not for sub-patterns φ)


\bφ
matches φ starting at a word boundary (top-level φ, not for sub-patterns φ)


φ\b
matches φ ending at a word boundary (top-level φ, not for sub-patterns φ)


\Bφ
matches φ starting at a non-word boundary (top-level φ, not for sub-patterns φ)


φ\B
matches φ ending at a non-word boundary (note: top-level regex pattern only)


\<φ
matches φ that starts a word (top-level φ, not for sub-patterns φ)


\>φ
matches φ that starts a non-word (top-level φ, not for sub-patterns φ)


φ\<
matches φ that ends a non-word (top-level φ, not for sub-patterns φ)


φ\>
matches φ that ends a word (top-level φ, not for sub-patterns φ)


\i
matches an indent


\j
matches a dedent


(?i:φ)
matches φ ignoring case


(?s:φ)
. (dot) in φ matches newline


(?x:φ)
ignore all whitespace and comments in φ


(?#:X)
all of X is skipped as a comment



The order of precedence for composing larger patterns from sub-patterns is as
follows, from high to low precedence:

Characters, character classes (bracket expressions), escapes, quotation
Grouping (φ), (?:φ), (?=φ), and inline modifiers (?imsux:φ)
Quantifiers ?, *, +, {n,m}
Concatenation φψ
Anchoring ^, $, \<, \>, \b, \B, \A, \z
Alternation φ|ψ
Global modifiers (?imsux)φ

POSIX and Unicode character classes
Character classes in bracket lists represent sets of characters.  Sets can be
inverted, subtracted, intersected, and merged:



Pattern
Matches




[a-zA-Z]
matches a letter


[^a-zA-Z]
matches a non-letter (character class inversion)


[a-z−−[aeiou]]
matches a consonant (character class subtraction)


[a-z&&[^aeiou]]
matches a consonant (character class intersection)


[a-z⎮⎮[A-Z]]
matches a letter (character class union)



Bracket lists cannot be empty, so [] and [^] are invalid.  In fact, the
first character after the bracket is always part of the list.  So [][] is a
list that matches a ] and a [, [^][] is a list that matches anything but
] and [, and [-^] is a list that matches a - and a ^.
POSIX and Unicode character categories



POSIX form
POSIX category
Matches




[:ascii:]
\p{ASCII}
matches any ASCII character


[:space:]
\p{Space}
matches a white space character [ \t\n\v\f\r]


[:xdigit:]
\p{Xdigit}
matches a hex digit [0-9A-Fa-f]


[:cntrl:]
\p{Cntrl}
matches a control character [\x00-\0x1f\x7f]


[:print:]
\p{Print}
matches a printable character [\x20-\x7e]


[:alnum:]
\p{Alnum}
matches a alphanumeric character [0-9A-Za-z]


[:alpha:]
\p{Alpha}
matches a letter [A-Za-z]


[:blank:]
\p{Blank}, \h
matches a blank [ \t]


[:digit:]
\p{Digit}, \d
matches a digit [0-9]


[:graph:]
\p{Graph}
matches a visible character [\x21-\x7e]


[:lower:]

matches a lower case letter [a-z]


[:punct:]
\p{Punct}
matches a punctuation character [\x21-\x2f\x3a-\x40\x5b-\x60\x7b-\x7e]


[:upper:]

matches an upper case letter [A-Z]


[:word:]

matches a word character [0-9A-Za-z_]


[:^blank:]
\H
matches a non-blank character [^ \t]


[:^digit:]
\D
matches a non-digit [^0-9]



The POSIX form can only be used in bracket lists, for example
[[:lower:][:digit:]] matches an ASCII lower case letter or a digit.
You can also use the capitalized \P{C} form that has the same meaning as
\p{^C}, which matches any character except characters in the class C.
For example, \P{ASCII} is the same as \p{^ASCII} which is the same as
[^[:ascii:]].  A word of caution: because POSIX character categories only
cover ASCII, [[:^ascii]] is empty and invalid to use.  By contrast,
[^[:ascii]] is a Unicode character class that excludes the ASCII character
category.



Unicode category
Matches




.
matches any single Unicode character except newline


\X
matches any ISO-8859-1 or Unicode character


\R
matches a Unicode line break


\s, \p{Zs}
matches a white space character with Unicode sub-propert Zs


\l, \p{Ll}
matches a lower case letter with Unicode sub-property Ll


\u, \p{Lu}
matches an upper case letter with Unicode sub-property Lu


\w, \p{Word}
matches a Unicode word character with property L, Nd, or Pc


\p{Unicode}
matches any Unicode character (U+0000 to U+10FFFF minus U+D800 to U+DFFF)


\p{ASCII}
matches an ASCII character U+0000 to U+007F)


\p{Non_ASCII_Unicode}
matches a non-ASCII character U+0080 to U+10FFFF minus U+D800 to U+DFFF)


\p{Letter}
matches a character with Unicode property Letter


\p{Mark}
matches a character with Unicode property Mark


\p{Separator}
matches a character with Unicode property Separator


\p{Symbol}
matches a character with Unicode property Symbol


\p{Number}
matches a character with Unicode property Number


\p{Punctuation}
matches a character with Unicode property Punctuation


\p{Other}
matches a character with Unicode property Other


\p{Lowercase_Letter}, \p{Ll}
matches a character with Unicode sub-property Ll


\p{Uppercase_Letter}, \p{Lu}
matches a character with Unicode sub-property Lu


\p{Titlecase_Letter}, \p{Lt}
matches a character with Unicode sub-property Lt


\p{Modifier_Letter}, \p{Lm}
matches a character with Unicode sub-property Lm


\p{Other_Letter}, \p{Lo}
matches a character with Unicode sub-property Lo


\p{Non_Spacing_Mark}, \p{Mn}
matches a character with Unicode sub-property Mn


\p{Spacing_Combining_Mark}, \p{Mc}
matches a character with Unicode sub-property Mc


\p{Enclosing_Mark}, \p{Me}
matches a character with Unicode sub-property Me


\p{Space_Separator}, \p{Zs}
matches a character with Unicode sub-property Zs


\p{Line_Separator}, \p{Zl}
matches a character with Unicode sub-property Zl


\p{Paragraph_Separator}, \p{Zp}
matches a character with Unicode sub-property Zp


\p{Math_Symbol}, \p{Sm}
matches a character with Unicode sub-property Sm


\p{Currency_Symbol}, \p{Sc}
matches a character with Unicode sub-property Sc


\p{Modifier_Symbol}, \p{Sk}
matches a character with Unicode sub-property Sk


\p{Other_Symbol}, \p{So}
matches a character with Unicode sub-property So


\p{Decimal_Digit_Number}, \p{Nd}
matches a character with Unicode sub-property Nd


\p{Letter_Number}, \p{Nl}
matches a character with Unicode sub-property Nl


\p{Other_Number}, \p{No}
matches a character with Unicode sub-property No


\p{Dash_Punctuation}, \p{Pd}
matches a character with Unicode sub-property Pd


\p{Open_Punctuation}, \p{Ps}
matches a character with Unicode sub-property Ps


\p{Close_Punctuation}, \p{Pe}
matches a character with Unicode sub-property Pe


\p{Initial_Punctuation}, \p{Pi}
matches a character with Unicode sub-property Pi


\p{Final_Punctuation}, \p{Pf}
matches a character with Unicode sub-property Pf


\p{Connector_Punctuation}, \p{Pc}
matches a character with Unicode sub-property Pc


\p{Other_Punctuation}, \p{Po}
matches a character with Unicode sub-property Po


\p{Control}, \p{Cc}
matches a character with Unicode sub-property Cc


\p{Format}, \p{Cf}
matches a character with Unicode sub-property Cf


\p{UnicodeIdentifierStart}
matches a character in the Unicode IdentifierStart class


\p{UnicodeIdentifierPart}
matches a character in the Unicode IdentifierPart class


\p{IdentifierIgnorable}
matches a character in the IdentifierIgnorable class


\p{JavaIdentifierStart}
matches a character in the Java IdentifierStart class


\p{JavaIdentifierPart}
matches a character in the Java IdentifierPart class


\p{CsIdentifierStart}
matches a character in the C# IdentifierStart class


\p{CsIdentifierPart}
matches a character in the C# IdentifierPart class


\p{PythonIdentifierStart}
matches a character in the Python IdentifierStart class


\p{PythonIdentifierPart}
matches a character in the Python IdentifierPart class



To specify a Unicode block as a category use \p{IsBlockName} with a Unicode
BlockName.
To specify a Unicode language script, use \p{Language} with a Unicode
Language.
Unicode language script character classes differ from the Unicode blocks that
have a similar name.  For example, the \p{Greek} class represents Greek and
Coptic letters and differs from the Unicode block \p{IsGreek} that spans a
specific Unicode block of Greek and Coptic characters only, which also includes
unassigned characters.
",4
gochain-io/web3,Go,"Web3
Simple command line tool for interacting with web3 enabled blockchains - GoChain, Ethereum, etc.
This repository also exports the backing golang package web3.

Install web3
Quick one line install:
curl -LSs https://raw.githubusercontent.com/gochain-io/web3/master/install.sh | sh
Install Docker (optional) - While not required for all commands, many of the developer commands require Docker so we recommend installing it.
More options
Quickstart
If you just plan to read from the blockchain, you do not need any GO tokens and you do not need to set your PRIVATE_KEY. If you plan to deploy contracts or write anything to the blockchain, you'll need tokens and you'll need to set your PRIVATE_KEY for the account that has those tokens.
Pick a network to use
a) Run a local node
Run this command to start a local node. It will print 10 addresses with keys upon starting that you can use to deploy and interact.
web3 start
export WEB3_NETWORK=localhost
b) Use the GoChain testnet
export WEB3_NETWORK=testnet
To do any write operations, get yourself some GO testnet tokens so you can deploy and interact with your contract.
c) Use the GoChain mainnet or another web3 network
export WEB3_NETWORK=gochain
You'll need mainnet GO for this which you can buy on various exchanges.
You can also point this to other web3 based networks such as Ethereum. Ethereum is supported by default and you
can use one of the following: ethereum or ropsten.
Set Private Key (optional)
Required if you plan to deploy or write transactions.
export WEB3_PRIVATE_KEY=0x...
Deploy a contract
Copy contracts/hello.sol into your current directory.
Then:
web3 contract build hello.sol
web3 contract deploy Hello.bin
This will return a contract address, copy it and use below.
Read from a contract
Let's call a read function (which is free):
web3 contract call --address 0xCONTRACT_ADDRESS --abi Hello.abi --function hello
That should return: [Hello World].
Write to a contract
Now let's change the name:
web3 contract call --address 0xCONTRACT_ADDRESS --abi Hello.abi --function setName ""Johnny""
And call the hello function again to see if the name changed:
web3 contract call --address 0xCONTRACT_ADDRESS --abi Hello.abi --function hello
Now it should return [Hello Johnny]
💥
Troubleshooting
If it doesn't return Hello Johnny, you can check the logs and receipt with:
web3 rc TX_HASH
Testing
To automate testing using web3 CLI, enable the JSON format flag with --format json. This will
return easily parseable results for your tests. Eg:
web3 --format json contract call --address 0xCONTRACT_ADDRESS --abi Hello.abi --function hello
And you'll get a JSON response like this:
{
  ""response"": [
    ""Hello"",
    ""World""
  ]
}
Deploying an Upgradeable Contract
The web3 tool comes with built-in support for deploying contracts that can be
upgraded later. To deploy an upgradeable contract, simply specify the
--upgradeable flag while deploying. From our Hello example above:
web3 contract deploy --upgradeable Hello.bin
This will return the contract address. Let's set the contract address environment variable so you can use it throughout the rest of this
tutorial (alternatively you can pass in the --address CONTRACT_ADDRESS flag on all the commands).
export WEB3_ADDRESS=0xCONTRACT_ADDRESS
Internally, deploying an upgradeable contract will actually deploy two separate contracts:

Your original Hello contract.
A proxy contract for redirecting calls and storage.

The returned contract address is the address of your proxy. To see the contract
address that your proxy is pointing to, you can use the target command in
the CLI:
web3 contract target
One caveat to using upgradeable contracts is that their constructors will not
execute. To get around this, we will have to initialize our contract with an
initial call to setName:
web3 contract call --abi Hello.abi --function setName ""World""
Now we can interact with our upgradeable contract just like a normal contract:
web3 contract call --abi Hello.abi --function hello
# returns: [Hello World]
Alright, so we have a working contract. Let's upgrade it!
Upgrading the contract
We can now deploy a different contract (without the upgradeable flag) and
redirect our upgradeable contract to point to that new contract.
Copy contracts/goodbye.sol into your current directory
and build and deploy it:
web3 contract build goodbye.sol
web3 contract deploy Goodbye.bin
Using the new Goodbye contract address, we can upgrade our previous contract
using the contract upgrade command:
web3 contract upgrade --to 0xGOODBYE_CONTRACT_ADDRESS
We can see that our proxy contract now points to this new contract by
calling the hello function again:
web3 contract call --abi Hello.abi --function hello
# returns: [Goodbye World]
Note that contracts can only be upgraded by the account that created them.
Pausing and resuming a contract
Upgradeable contracts also include the ability to pause & resume execution.
This can be useful if you discover a bug in your contract and you wish to cease
operation until you can upgrade to a fixed version.
Pausing a contract is simple:
web3 contract pause
Wait a minute for the transaction to go through, then try to use the contract again and it will fail:
web3 contract call --abi Hello.abi --function hello
# returns: ERROR: Cannot call the contract: abi: unmarshalling empty output
Contracts can be upgraded while they are paused. To execute any other contract functions, you
will need to first resume operation:
web3 contract resume
List of available commands
Global parameters
$NETWORK as env variable or -network as command parameter - the name of the network. Available networks are:

gochain (default)
testnet
ethereum
ropsten
localhost

$RPC_URL as env variable or -rpc-url as command parameter - The network RPC URL (ie http://localhost:8545)
-verbose as command parameter - Verbose logging
Show information about a block
web3 block BLOCK_ID
Parameters:

BLOCK_ID - id of a block (omit for latest)

Show information about a transaction
web3 transaction TX_HASH
Parameters:

TX_HASH - hash of a transaction

Show information about an address
web3 transaction ADDRESS_HASH
Parameters:

ADDRESS_HASH - hash of the address

Build a smart contract
web3 contract build FILENAME.sol --solc-version SOLC_VERSION
Parameters:

FILENAME - the name of the .sol file, eg: hello.sol
SOLC_VERSION - the version of the solc compiler

Deploy a smart contract to a network
web3 contract deploy FILENAME.bin
Parameters:

FILENAME - the name of the .bin
$WEB3_PRIVATE_KEY as env variable or -private-key as command parameter - the private key of the wallet

Call a function of a deployed contract
web3 contract call --amount AMOUNT --address CONTRACT_ADDRESS --abi CONTRACT_ABI_FILE --function FUNCTION_NAME FUNCTION_PARAMETERS
or using bundled abi files
web3 contract call --amount AMOUNT --address CONTRACT_ADDRESS --abi erc20|erc721 --function FUNCTION_NAME FUNCTION_PARAMETERS
Parameters:

CONTRACT_ADDRESS - the address of the deployed contract
CONTRACT_ABI_FILE - the abi file of the deployed contract (take into account that there are some bundled abi files like erc20 and erc721 so you could use them without downloading or compiling them)
FUNCTION_NAME - the name of the function you want to call
FUNCTION_PARAMETERS - the list of the function parameters
AMOUNT - amount of wei to be send with transaction (require only for paid transact functions)
$WEB3_PRIVATE_KEY as env variable or -private-key as command parameter - the private key of the wallet

List functions in an ABI
web3 contract list --abi CONTRACT_ABI_FILE
Parameters:

CONTRACT_ABI_FILE - the abi file of the compiled contract

Transfer amount to the address
web3 send --to RECIPIENT_ADDRESS AMOUNT
Parameters:

RECIPIENT_ADDRESS - the address of the recepient
AMOUNT - the amount that should be send in the transaction ie - 1go (allowed units: go,eth,nanogo,gwei,attogo,wei)

Generate common contracts - ERC20, ERC721, etc
web3 generate contract [erc20/erc721] --name ""TEST Tokens"" --symbol ""TEST""
See web3 generate contract --help for more information.
Generate ABI bindings
web3 generate code --abi CONTRACT_ABI_FILE --out OUT_FILENAME --lang [go|objc|java] --pkg PGK_NAME
See web3 generate code --help for more information.
Parameters:

CONTRACT_ABI_FILE - the abi file of the compiled contract
OUT_FILENAME - the output file
PGK_NAME - package name

More installation options
Install a specific version
You can use the script to install a specific version:
curl -LSs https://raw.githubusercontent.com/gochain-io/web3/master/install.sh | sh -s v0.0.9
Install using the Go language
go install github.com/gochain-io/web3/cmd/web3
Build from source
Clone this repo:
git clone https://github.com/gochain-io/web3
cd web3
make build
./web3 help
",12
gochain-io/web3,Go,"Web3
Simple command line tool for interacting with web3 enabled blockchains - GoChain, Ethereum, etc.
This repository also exports the backing golang package web3.

Install web3
Quick one line install:
curl -LSs https://raw.githubusercontent.com/gochain-io/web3/master/install.sh | sh
Install Docker (optional) - While not required for all commands, many of the developer commands require Docker so we recommend installing it.
More options
Quickstart
If you just plan to read from the blockchain, you do not need any GO tokens and you do not need to set your PRIVATE_KEY. If you plan to deploy contracts or write anything to the blockchain, you'll need tokens and you'll need to set your PRIVATE_KEY for the account that has those tokens.
Pick a network to use
a) Run a local node
Run this command to start a local node. It will print 10 addresses with keys upon starting that you can use to deploy and interact.
web3 start
export WEB3_NETWORK=localhost
b) Use the GoChain testnet
export WEB3_NETWORK=testnet
To do any write operations, get yourself some GO testnet tokens so you can deploy and interact with your contract.
c) Use the GoChain mainnet or another web3 network
export WEB3_NETWORK=gochain
You'll need mainnet GO for this which you can buy on various exchanges.
You can also point this to other web3 based networks such as Ethereum. Ethereum is supported by default and you
can use one of the following: ethereum or ropsten.
Set Private Key (optional)
Required if you plan to deploy or write transactions.
export WEB3_PRIVATE_KEY=0x...
Deploy a contract
Copy contracts/hello.sol into your current directory.
Then:
web3 contract build hello.sol
web3 contract deploy Hello.bin
This will return a contract address, copy it and use below.
Read from a contract
Let's call a read function (which is free):
web3 contract call --address 0xCONTRACT_ADDRESS --abi Hello.abi --function hello
That should return: [Hello World].
Write to a contract
Now let's change the name:
web3 contract call --address 0xCONTRACT_ADDRESS --abi Hello.abi --function setName ""Johnny""
And call the hello function again to see if the name changed:
web3 contract call --address 0xCONTRACT_ADDRESS --abi Hello.abi --function hello
Now it should return [Hello Johnny]
💥
Troubleshooting
If it doesn't return Hello Johnny, you can check the logs and receipt with:
web3 rc TX_HASH
Testing
To automate testing using web3 CLI, enable the JSON format flag with --format json. This will
return easily parseable results for your tests. Eg:
web3 --format json contract call --address 0xCONTRACT_ADDRESS --abi Hello.abi --function hello
And you'll get a JSON response like this:
{
  ""response"": [
    ""Hello"",
    ""World""
  ]
}
Deploying an Upgradeable Contract
The web3 tool comes with built-in support for deploying contracts that can be
upgraded later. To deploy an upgradeable contract, simply specify the
--upgradeable flag while deploying. From our Hello example above:
web3 contract deploy --upgradeable Hello.bin
This will return the contract address. Let's set the contract address environment variable so you can use it throughout the rest of this
tutorial (alternatively you can pass in the --address CONTRACT_ADDRESS flag on all the commands).
export WEB3_ADDRESS=0xCONTRACT_ADDRESS
Internally, deploying an upgradeable contract will actually deploy two separate contracts:

Your original Hello contract.
A proxy contract for redirecting calls and storage.

The returned contract address is the address of your proxy. To see the contract
address that your proxy is pointing to, you can use the target command in
the CLI:
web3 contract target
One caveat to using upgradeable contracts is that their constructors will not
execute. To get around this, we will have to initialize our contract with an
initial call to setName:
web3 contract call --abi Hello.abi --function setName ""World""
Now we can interact with our upgradeable contract just like a normal contract:
web3 contract call --abi Hello.abi --function hello
# returns: [Hello World]
Alright, so we have a working contract. Let's upgrade it!
Upgrading the contract
We can now deploy a different contract (without the upgradeable flag) and
redirect our upgradeable contract to point to that new contract.
Copy contracts/goodbye.sol into your current directory
and build and deploy it:
web3 contract build goodbye.sol
web3 contract deploy Goodbye.bin
Using the new Goodbye contract address, we can upgrade our previous contract
using the contract upgrade command:
web3 contract upgrade --to 0xGOODBYE_CONTRACT_ADDRESS
We can see that our proxy contract now points to this new contract by
calling the hello function again:
web3 contract call --abi Hello.abi --function hello
# returns: [Goodbye World]
Note that contracts can only be upgraded by the account that created them.
Pausing and resuming a contract
Upgradeable contracts also include the ability to pause & resume execution.
This can be useful if you discover a bug in your contract and you wish to cease
operation until you can upgrade to a fixed version.
Pausing a contract is simple:
web3 contract pause
Wait a minute for the transaction to go through, then try to use the contract again and it will fail:
web3 contract call --abi Hello.abi --function hello
# returns: ERROR: Cannot call the contract: abi: unmarshalling empty output
Contracts can be upgraded while they are paused. To execute any other contract functions, you
will need to first resume operation:
web3 contract resume
List of available commands
Global parameters
$NETWORK as env variable or -network as command parameter - the name of the network. Available networks are:

gochain (default)
testnet
ethereum
ropsten
localhost

$RPC_URL as env variable or -rpc-url as command parameter - The network RPC URL (ie http://localhost:8545)
-verbose as command parameter - Verbose logging
Show information about a block
web3 block BLOCK_ID
Parameters:

BLOCK_ID - id of a block (omit for latest)

Show information about a transaction
web3 transaction TX_HASH
Parameters:

TX_HASH - hash of a transaction

Show information about an address
web3 transaction ADDRESS_HASH
Parameters:

ADDRESS_HASH - hash of the address

Build a smart contract
web3 contract build FILENAME.sol --solc-version SOLC_VERSION
Parameters:

FILENAME - the name of the .sol file, eg: hello.sol
SOLC_VERSION - the version of the solc compiler

Deploy a smart contract to a network
web3 contract deploy FILENAME.bin
Parameters:

FILENAME - the name of the .bin
$WEB3_PRIVATE_KEY as env variable or -private-key as command parameter - the private key of the wallet

Call a function of a deployed contract
web3 contract call --amount AMOUNT --address CONTRACT_ADDRESS --abi CONTRACT_ABI_FILE --function FUNCTION_NAME FUNCTION_PARAMETERS
or using bundled abi files
web3 contract call --amount AMOUNT --address CONTRACT_ADDRESS --abi erc20|erc721 --function FUNCTION_NAME FUNCTION_PARAMETERS
Parameters:

CONTRACT_ADDRESS - the address of the deployed contract
CONTRACT_ABI_FILE - the abi file of the deployed contract (take into account that there are some bundled abi files like erc20 and erc721 so you could use them without downloading or compiling them)
FUNCTION_NAME - the name of the function you want to call
FUNCTION_PARAMETERS - the list of the function parameters
AMOUNT - amount of wei to be send with transaction (require only for paid transact functions)
$WEB3_PRIVATE_KEY as env variable or -private-key as command parameter - the private key of the wallet

List functions in an ABI
web3 contract list --abi CONTRACT_ABI_FILE
Parameters:

CONTRACT_ABI_FILE - the abi file of the compiled contract

Transfer amount to the address
web3 send --to RECIPIENT_ADDRESS AMOUNT
Parameters:

RECIPIENT_ADDRESS - the address of the recepient
AMOUNT - the amount that should be send in the transaction ie - 1go (allowed units: go,eth,nanogo,gwei,attogo,wei)

Generate common contracts - ERC20, ERC721, etc
web3 generate contract [erc20/erc721] --name ""TEST Tokens"" --symbol ""TEST""
See web3 generate contract --help for more information.
Generate ABI bindings
web3 generate code --abi CONTRACT_ABI_FILE --out OUT_FILENAME --lang [go|objc|java] --pkg PGK_NAME
See web3 generate code --help for more information.
Parameters:

CONTRACT_ABI_FILE - the abi file of the compiled contract
OUT_FILENAME - the output file
PGK_NAME - package name

More installation options
Install a specific version
You can use the script to install a specific version:
curl -LSs https://raw.githubusercontent.com/gochain-io/web3/master/install.sh | sh -s v0.0.9
Install using the Go language
go install github.com/gochain-io/web3/cmd/web3
Build from source
Clone this repo:
git clone https://github.com/gochain-io/web3
cd web3
make build
./web3 help
",12
NetBSD/src,None,"NetBSD
NetBSD is a free, fast, secure, and highly portable Unix-like Open
Source operating system.  It is available for a wide range of
platforms, from large-scale servers
and powerful desktop systems to handheld and embedded devices.
Building
You can cross-build NetBSD from most UNIX-like operating systems.
To build for amd64 (x86_64), in the src directory:
./build.sh -U -u -j4 -m amd64 -O ~/obj release

Additional build information available in the BUILDING file.
Binaries

Daily builds
Releases

Testing
On a running NetBSD system:
cd /usr/tests; atf-run | atf-report

Troubleshooting

Send bugs and patches via web form.
Subscribe to the mailing lists.
The netbsd-users list is a good choice for many problems; watch current-users if you follow the bleeding edge of NetBSD-current.
Join the community IRC channel #netbsd @ freenode.

Latest sources
To fetch the main CVS repository:
cvs -d anoncvs@anoncvs.NetBSD.org:/cvsroot checkout -P src

To work in the Git mirror, which is updated every few hours from CVS:
git clone https://github.com/NetBSD/src.git

Additional Links

The NetBSD Guide
NetBSD manual pages
NetBSD Cross-Reference

",157
morozov-group/magento2-similar-products,PHP,"magento2-similar-products

Magento 2 Similarity extension which provides connectivity with Similarity Engine.
Demo store

Automated Upsells for every product Demo | Production
Visually Similar products for specified product /catalogsearch/advanced/result/?similar=PRODUCT_ID
Visually Similar products within same category category.html?similar=PRODUCT_ID Demo
CMS Widget to put similar products any where for specified PRODUCT_ID.
(Near future) Category filling assistant, for some special events or campaigns.

Installation
Simple installation via composer.
compose require morozov-group/magento2-similar-products

Go to configuration enter your email, and we'll take care of everything else.
You will receive email once we are ready to serve similar products recommendations.
Then you can proceed with customizations.
Contributions and new ideas
You are welcome to post tickets and pull requests.
",5
u-simon/springCloudDemo,Java,"springCloudDemo
",2
AMReX-Codes/amrex,C++,"
License
AMReX Copyright (c) 2017, The Regents of the University of California,
through Lawrence Berkeley National Laboratory and the Alliance for
Sustainable Energy, LLC., through National Renewable Energy Laboratory
(subject to receipt of any required approvals from the U.S. Dept. of
Energy).  All rights reserved.
If you have questions about your rights to use or distribute this
software, please contact Berkeley Lab's Innovation & Partnerships
Office at IPO@lbl.gov.
NOTICE.  This Software was developed under funding from the
U.S. Department of Energy and the U.S. Government consequently retains
certain rights. As such, the U.S. Government has been granted for
itself and others acting on its behalf a paid-up, nonexclusive,
irrevocable, worldwide license in the Software to reproduce,
distribute copies to the public, prepare derivative works, and perform
publicly and display publicly, and to permit other to do so.
License for AMReX can be found at LICENSE.
Development Model
Development generally follows the following ideas:


New features are committed to the development branch.
Nightly regression testing is used to ensure that no answers
change (or if they do, that the changes were expected).
If a change is critical, we can cherry-pick the commit from
development to master.


Bug fixes, questions and contributions of new features are welcome!


Bugs should be reported through GitHub issues


We suggest asking questions through GitHub issues as well


Any contributions of new features that have the potential
to change answers should be done via pull requests.
A pull request should be generated from your fork of
amrex and target the development branch.
If there are a number of small commits making up the PR, we may
wish to squash commits upon merge to have a clean history.
Please ensure that your PR title and first post are descriptive,
since these will be used for a squashed commit message.
Please note the following:
If you choose to make contributions to the code
then you hereby grant a non-exclusive, royalty-free perpetual license
to install, use, modify, prepare derivative works,
incorporate into other computer software,
distribute, and sublicense such enhancements or derivative works
thereof, in binary and source code form.




On the first workday of each month, we perform a merge of
development into master.  For this merge to take place, we
need to be passing the regression tests.
To accommodate this need, we close the merge window into
development a few days before the merge day.  While the merge
window is closed, only bug fixes should be pushed into
development.  Once the merge from development -> master is
done, the merge window reopens.


Core Developers
People who make a number of substantive contributions will be named
""core developers"" of AMReX.  The criteria for becoming a core
developer are flexible, but generally involve one of the following:


100 non-trivial commits to amrex/Src/ and/or


addition of a new algorithm / module  and/or


substantial input into the code design process or testing


If a core developer is inactive for multiple years, we may reassess their
status as a core developer.
The current list of core developers is: Ann Almgren (LBNL), Vince Beckner, John Bell (LBNL), Johannes Blaschke (LBNL), Cy Chan (LBNL), Marcus Day (LBNL), Brian Friesen (NERSC), Kevin Gott (NERSC), Daniel Graves (LBNL), Max Katz (NVIDIA), Andrew Myers (LBNL), Tan Nguyen (LBNL), Andrew Nonaka (LBNL), Michele Rosso (LBNL), Sam Williams (LBNL), Weiqun Zhang (LBNL), Michael Zingale (Stonybrook University).
",130
joeynmt/joeynmt,Python,"   Joey NMT

Goal and Purpose
Joey NMT framework is developed for educational purposes.
It aims to be a clean and minimalistic code base to help novices
pursuing the understanding of the following questions.

How to implement classic NMT architectures (RNN and Transformer) in PyTorch?
What are the building blocks of these architectures and how do they interact?
How to modify these blocks (e.g. deeper, wider, ...)?
How to modify the training procedure (e.g. add a regularizer)?

In contrast to other NMT frameworks, we will not aim for
state-of-the-art results or speed through engineering or training tricks
since this often goes in hand with an increase in code complexity
and a decrease in readability.
However, Joey NMT re-implements baselines from major publications.
Contributors
Joey NMT is developed by Joost Bastings (University of Amsterdam) and Julia Kreutzer (Heidelberg University).
Features
We aim to implement the following features (aka the minimalist toolkit of NMT):

Recurrent Encoder-Decoder with GRUs or LSTMs
Transformer Encoder-Decoder
Attention Types: MLP, Dot, Multi-Head, Bilinear
Word-, BPE- and character-based input handling
BLEU, ChrF evaluation
Beam search with length penalty and greedy decoding
Customizable initialization
Attention visualization
Learning curve plotting

[Work in progress: Transformer, Multi-Head and Dot still missing.]
Coding
In order to keep the code clean and readable, we make use of:

Style checks: pylint with (mostly) PEP8 conventions, see .pylintrc.
Typing: Every function has documented input types.
Docstrings: Every function, class and module has docstrings describing their purpose and usage.
Unittests: Every module has unit tests, defined in test/unit/.
Travis CI runs the tests and pylint on every push to ensure the repository stays clean.

Installation
Joey NMT is built on PyTorch and torchtext for Python >= 3.5.

Clone this repository:
git clone https://github.com/joeynmt/joeynmt.git
Install the requirements:
cd joeynmt
pip3 install -r requirements.txt (you might want to add --user for a local installation).
Install joeynmt:
python3 setup.py install
Run the unit tests:
python3 -m unittest

Usage
For details, follow the tutorial in the docs.
Data Preparation
Parallel Data
For training a translation model, you need parallel data, i.e. a collection of source sentences and reference translations that are aligned sentence-by-sentence and stored in two files,
such that each line in the reference file is the translation of the same line in the source file.
Pre-processing
Before training a model on it, parallel data is most commonly filtered by length ratio, tokenized and true- or lowercased.
The Moses toolkit provides a set of useful scripts for this purpose.
In addition, you might want to build the NMT model not on the basis of words, but rather sub-words or characters (the level in JoeyNMT configurations).
Currently, JoeyNMT supports the byte-pair-encodings (BPE) format by subword-nmt.
Configuration
Experiments are specified in configuration files, in simple YAML format. You can find examples in the configs directory.
small.yaml contains a detailed explanation of configuration options.
Most importantly, the configuration contains the description of the model architecture (e.g. number of hidden units in the encoder RNN),
paths to the training, development and test data, and the training hyperparameters (learning rate, validation frequency etc.).
Training
Start
For training, run
python3 -m joeynmt train configs/small.yaml.
This will train a model on the training data specified in the config (here: small.yaml),
validate on validation data,
and store model parameters, vocabularies, validation outputs and a small number of attention plots in the model_dir (also specified in config).
Note that pre-processing like tokenization or BPE-ing is not included in training, but has to be done manually before.
Tip: Be careful not to overwrite models, set overwrite: False in the model configuration.
Validations
The validations.txt file in the model directory reports the validation results at every validation point.
Models are saved whenever a new best validation score is reached, in batch_no.ckpt, where batch_no is the number of batches the model has been trained on so far.
best.ckpt links to the checkpoint that has so far achieved the best validation score.
Visualization
JoeyNMT uses TensorboardX to visualize training and validation curves and attention matrices during training.
Launch Tensorboard with tensorboard --logdir model_dir/tensorboard (or python -m tensorboard.main ...) and then open the url (default: localhost:6006) with a browser.
For a stand-alone plot, run python3 scripts/plot_validation.py model_dir --plot_values bleu PPL --output_path my_plot.pdf to plot curves of validation BLEU and PPL.
CPU vs. GPU
For training on a GPU, set use_cuda in the config file to True. This requires the installation of required CUDA libraries.
Translating
There's 3 options for testing what the model has learned.
Whatever data you feed the model for translating, make sure it is properly pre-processed, just as you pre-processed the training data, e.g. tokenized and split into subwords (if working with BPEs).
1. Test Set Evaluation
For testing and evaluating on your parallel test/dev set, run
python3 -m joeynmt test configs/small.yaml --output_path out.
This will generate translations for validation and test set (as specified in the configuration) in out.[dev|test]
with the latest/best model in the model_dir (or a specific checkpoint set with load_model).
It will also evaluate the outputs with eval_metric.
If --output_path is not specified, it will not store the translation, and only do the evaluation and print the results.
2. File Translation
In order to translate the contents of a file not contained in the configuration (here my_input.txt), simply run
python3 -m joeynmt translate configs/small.yaml < my_input.txt > out.
The translations will be written to stdout or alternatively--output_path if specified.
3. Interactive
If you just want try a few examples, run
python3 -m joeynmt translate configs/small.yaml
and you'll be prompted to type input sentences that JoeyNMT will then translate with the model specified in the configuration.
Documentation and Tutorial
The docs include an overview of the NMT implementation, a walk-through tutorial for building, training, tuning, testing and inspecting an NMT system, the API documentation and FAQs.
Benchmarks
Benchmarks on small models trained on GPU/CPU on standard data sets are reported here.

IWSLT15 En-Vi, word-based
IWSLT14 De-En, 32000 joint BPE, word-based
WMT17 En-De and Lv-En, 32000 joint BPE

IWSLT English-Vietnamese
We compare against Tensorflow NMT on the IWSLT15 En-Vi data set as preprocessed by Stanford.
You can download the data with scripts/get_iwslt15_envi.sh, and then use configs/iwslt_envi_luong.yaml to replicate the experiment.



Systems
tst2012 (dev)
test2013 (test)




TF NMT (greedy)
23.2
25.5


TF NMT (beam=10)
23.8
26.1


Joey NMT (greedy)
23.2
25.8


Joey NMT (beam=10, alpha=1.0)
23.8
26.5


(Luong & Manning, 2015)
-
23.3



We also compare against xnmt which uses different hyperparameters, so we use a different configuration for Joey NMT too: configs/iwslt_envi_xnmt.yaml.



Systems
tst2012 (dev)
test2013 (test)




xnmt (beam=5)
25.0
27.3


Joey NMT (greedy)
24.6
27.4


Joey NMT (beam=5, alpha=1.0)
24.9
27.7



IWSLT  German-English
We compare against the baseline scores reported in (Wiseman & Rush, 2016) (W&R),
(Bahdanau et al., 2017) (B17) with tokenized, lowercased BLEU (using sacrebleu).
Ẁe compare a word-based model of the same size and vocabulary as in W&R and B17.
The script to obtain and pre-process the data is the one published with W&R.
Use configs/iwslt_deen_bahdanau.yaml for training the model.
On a K40-GPU word-level training took <1h, beam search decoding for both dev and test <2min.



Systems
level
dev
test
#params




W&R (greedy)
word
-
22.53



W&R (beam=10)
word
-
23.87



B17 (greedy)
word
-
25.82



B17 (beam=10)
word
-
27.56



Joey NMT (greedy)
word
28.41
26.68
22.05M


Joey NMT (beam=10, alpha=1.0)
word
28.96
27.03
22.05M



On CPU (use_cuda: False):
(approx 8-10x slower: 8h for training, beam search decoding for both dev and test 19min, greedy decoding 5min)



Systems
level
dev
test
#params




Joey NMT (greedy)
word
28.35
26.46
22.05M


Joey NMT (beam=10, alpha=1.0)
word
28.85
27.06
22.05M



In addition, we compare to a BPE-based GRU model with 32k (Groundhog style).
Use scripts/get_iwslt14_bpe.sh to pre-process the data and configs/iwslt14_deen_bpe.yaml to train the model.
This model is available for download here.



Systems
level
dev
test
#params




Joey NMT (greedy)
bpe
27.57

60.69M


Joey NMT (beam=5, alpha=1.0)
bpe
28.55
27.34
60.69M



WMT 17 English-German and Latvian-English
We compare against the results for recurrent BPE-based models that were reported in the Sockeye paper.
We only consider the Groundhog setting here, where toolkits are used out-of-the-box for creating a Groundhog-like model (1 layer, LSTMs, MLP attention).
The data is pre-processed as described in the paper (code).
Postprocessing is done with Moses' detokenizer, evaluation with sacrebleu.
Note that the scores reported for other models might not reflect the current state of the code, but the state at the time of the Sockeye evaluation.
Please also consider the difference in number of parameters despite ""the same"" setup: our models are the smallest in numbers of parameters.
English-German
Groundhog setting: configs/wmt_ende_default.yaml  with encoder rnn=500, lr=0.0003, init_hidden=""bridge"".



Systems
level
dev
test
#params




Sockeye (beam=5)
bpe
-
23.18
87.83M


OpenNMT-Py (beam=5)
bpe
-
18.66
87.62M


Joey NMT (beam=5)
bpe
24.33
23.45
86.37M



The Joey NMT model was trained for 4 days (14 epochs).
Latvian-English
Groundhog setting: configs/wmt_lven_default.yaml with encoder rnn=500, lr=0.0003, init_hidden=""bridge"".



Systems
level
dev
test
#params




Sockeye (beam=5)
bpe
-
14.40
?


OpenNMT-Py (beam=5)
bpe
-
9.98
?


Joey NMT (beam=5)
bpe
12.09
8.75
64.52M



Contributing
Since this codebase is supposed to stay clean and minimalistic, contributions addressing the following are welcome:

Code correctness
Code cleanliness
Documentation quality
Speed or memory improvements
resolving issues

Code extending the functionalities beyond the basics will most likely not end up in the master branch, but we're curions to learn what you used Joey for.
Use-cases and Projects
Here we'll collect projects and repositories that are based on Joey. If you used Joey for a project, publication or built some code on top of it, let us know and we'll link it here.
Projects:

TBD

Contact
Please leave an issue if you have questions or issues with the code.
For general questions, email us at joeynmt <at> gmail.com.
Naming
Joeys are infant marsupials.
",46
JingningShi/MtreeRing,R,"MtreeRing
Authors: Jingning Shi, Wei Xiang
License: GPL3






MtreeRing is a tool for automatically measuring tree-ring width using image processing techniques.
Installation
Install the stable version from CRAN
install.packages(""MtreeRing"")
or the development version from GitHub
# install.packages(""devtools"")
devtools::install_github(""JingningShi/MtreeRing"")
Ring-width measurement
1. Read an image
library(MtreeRing)
## Read and plot a tree ring image
img.name <- system.file(""001.png"", package = ""MtreeRing"")
t1 <- ring_read(img = img.name, dpi = 1200, plot = TRUE)
ring_read supports commonly used image formats, including png, tiff, jpg and bmp.
2. Detect ring borders
After plotting the image, the automatic detection of ring borders can be performed using three alternative methods: (1) watershed algorithm; (2) Canny edge detector; (3) a linear detection algorithm from R package measuRing.
## Split a long core sample into 2 pieces to
## get better display performance and use the
## watershed algorithm to detect ring borders:
t2 <- ring_detect(ring.data = t1, seg = 2, method = 'watershed')

Figure 1. The automatic detection of ring borders
3. Calculate ring-width series
If all ring borders are correctly identified, you can generate a ring-width series in data frame format. Use write.rwl to export the ring-width series to an rwl file.
rw.df <- ring_calculate(ring.data = t2, seriesID = ""940220"")
library(dplR) # A dendrochronological analysis package
fn <- tempfile(fileext="".rwl"")
write.rwl(rwl.df = rw.df, fname = fn, format = ""tucson"")
Shiny application
If you are not familiar with R and its command line interface, the shiny-based app is a good alternative.
MtreeRing::ring_app_launch()
This command allows to run a Shiny-based application within the system's default web browser. The app provides a beginner-friendly graphical interface and supports more flexible mouse-based interactions.
The dashboard has three components: a header, sidebar and body, like this

A workflow for the Shiny app can be found in the package vignette. Most steps are demonstrated with a gif to make the workflow more understandable.
vignette('app-MtreeRing')
Ring width correction
If an increment borer is used to extract samples, it is well known that the auger sometimes fails to traverse the pith of the sampled tree but passes through one side of the pith at a certain distance. Tangent lines of rings close to the pith are therefore not perpendicular to the horizontal path, which may lead to considerable errors in ring widths.
Under such conditions, you can create two paths by setting the argument incline = TRUE, or by ticking the checkbox ""Inclined tree rings"". See this example.

The line segment connecting two dots on the same ring should match the tangent of a tree ring border. The corrected ring width is estimated from the distance between adjacent rings and orientation of ring borders.
Code of conduct
Please note that the 'MtreeRing' project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.
",2
yongzhuo/nlp_xiaojiang,Python,"nlp_xiaojiang
AugmentText
- 回译（效果比较好）
- EDA（同义词替换、插入、交换和删除）（效果还行）
- HMM-marko（质量较差）
- syntax（依存句法、句法、语法书）（简单句还可）
- seq2seq（深度学习同义句生成，效果不理想，seq2seq代码大都是 [https://github.com/qhduan/just_another_seq2seq] 的，效果不理想）

ChatBot
- 检索式ChatBot
    - 像ES那样直接检索(如使用fuzzywuzzy)，只能字面匹配
    - 构造句向量，检索问答库，能够检索有同义词的句子
- 生成式ChatBot（todo）
    - seq2seq
    - GAN

ClassificationText
- bert+bi-lstm(keras) approach 0.78~0.79% acc of Weizhong Bank Intelligent Customer Service Question Matching Competition

FeatureProject
- bert句向量、文本相似度
    - bert/extract_keras_bert_feature.py:提取bert句向量特征
    - bert/tet_bert_keras_sim.py:测试bert句向量cosin相似度
- normalization_util指的是数据归一化
    - 0-1归一化处理
    - 均值归一化
    - sig归一化处理
- sim feature（ML）
    - distance_text_or_vec:各种计算文本、向量距离等
    - distance_vec_TS_SS：TS_SS计算词向量距离
    - cut_td_idf：将小黄鸡语料和gossip结合
    - sentence_sim_feature：计算两个文本的相似度或者距离，例如qq（问题和问题），或者qa（问题和答案）

run(可以在win10下,pycharm下运行)

1.创建tf-idf文件等（运行2需要先跑1）:
python cut_td_idf.py
2.计算两个句子间的各种相似度，先计算一个预定义的，然后可输入自定义的（先跑1）:
python sentence_sim_feature.py
3.chatbot_1跑起来(fuzzy检索-没)（独立）：
python chatbot_fuzzy.py
4.chatbot_2跑起来(句向量检索-词)（独立）：
python chatbot_sentence_vec_by_word.py
5.chatbot_3跑起来(句向量检索-字)（独立）：
python chatbot_sentence_vec_by_char.py
6.数据增强（eda)：                     python enhance_eda.py
7.数据增强（marko）:                   python enhance_marko.py
8.数据增强（translate_account）:       python translate_tencent_secret.py
9.数据增强（translate_tools）:         python translate_translate.py
10.数据增强（translate_web）:          python translate_google.py
11.数据增强（augment_seq2seq）:        先跑 python extract_char_webank.py生成数据，
再跑 python train_char_anti.py
然后跑 python predict_char_anti.py
12.特征计算(bert)（提取特征、计算相似度）:
run extract_keras_bert_feature.py run tet_bert_keras_sim.py

Data
- chinese_L-12_H-768_A-12（谷歌预训练好的模型）
   github项目中只是上传部分数据，需要的前往链接: https://pan.baidu.com/s/1I3vydhmFEQ9nuPG2fDou8Q 提取码: rket
   解压后就可以啦
- chinese_vector
    github项目中只是上传部分数据，需要的前往链接: https://pan.baidu.com/s/1I3vydhmFEQ9nuPG2fDou8Q 提取码: rket
    - 截取的部分word2vec训练词向量（自己需要下载全效果才会好）
    - w2v_model_wiki_char.vec、w2v_model_wiki_word.vec都只有部分
- corpus
    github项目中只是上传部分数据，需要的前往链接: https://pan.baidu.com/s/1I3vydhmFEQ9nuPG2fDou8Q 提取码: rket
    - webank(train、dev、test)
    - 小黄鸡和gossip问答预料（数据没清洗）,chicken_and_gossip.txt
    - 微众银行和支付宝文本相似度竞赛数据， sim_webank.csv
- sentence_vec_encode_char
    - 1.txt（字向量生成的前100000句向量）
- sentence_vec_encode_word
    - 1.txt（词向量生成的前100000句向量）
- tf_idf（chicken_and_gossip.txt生成的tf-idf）

requestments.txt
- python_Levenshtei
    - 调用Levenshtein，我的python是3.6，
    - 打开其源文件: https://www.lfd.uci.edu/~gohlke/pythonlibs/
    - 查找python_Levenshtein-0.12.0-cp36-cp36m-win_amd64.whl下载即可
- pyemd
- pyhanlp
    - 下好依赖JPype1-0.6.3-cp36-cp36m-win_amd64.whl

参考/感谢

eda_chinese：https://github.com/zhanlaoban/eda_nlp_for_Chinese
主谓宾提取器：https://github.com/hankcs/MainPartExtractor
HMM生成句子：https://github.com/takeToDreamLand/SentenceGenerate_byMarkov
同义词等：https://github.com/fighting41love/funNLP/tree/master/data/
小牛翻译：http://www.niutrans.com/index.html

其他资料

bert(keras):https://github.com/CyberZHG/keras-bert
NLP数据增强汇总:https://github.com/quincyliang/nlp-data-augmentation
知乎NLP数据增强话题:https://www.zhihu.com/question/305256736/answer/550873100
chatbot_seq2seq_seqGan（比较好用）：https://github.com/qhduan/just_another_seq2seq
自己动手做聊天机器人教程: https://github.com/warmheartli/ChatBotCourse

",19
flexbooru/flexbooru,Kotlin,"Flexbooru
A booru client for Android, support Danbooru, Moebooru, Gelbooru, Sankaku, etc.







Translate
Click on this link and you can translate this app into your language.
Downlad
 or Github Releases
Screenshot
  
Thanks to

OkHttp: An HTTP+HTTP/2 client for Android and Java applications.
Retrofit: Type-safe HTTP client for Android and Java by Square.
Gson: A Java serialization/deserialization library to convert Java Objects into JSON and back.
TikXml: Modern XML Parser for Android.
Glide: An image loading and caching library for Android focused on smooth scrolling.
Picasso: A powerful image downloading and caching library for Android.
MaterialDrawer: A drawer with material 2 design.
SimpleMenuPreference: A preference displaying a simple menu, originally implemented by RikkaW. On pre-Lollipop devices it falls back to a ListPreference as the older devices can't handle elevation and animation properly introduced in API 21.
FlexboxLayout: A library project which brings the similar capabilities of CSS Flexible Box Layout Module to Android.
PhotoView: Implementation of ImageView for Android that supports zooming, by various touch gestures.
SubsamplingScaleImageView: Highly configurable, easily extendable deep zoom view for displaying huge images without loss of detail. Perfect for photo galleries, maps, building plans etc.
ExoPlayer: An application level media player for Android.
Kodein-DI: A very simple and yet very useful dependency retrieval container. It is very easy to use and configure.
KotlinCoroutineAdapter: A Retrofit 2 CallAdapter.Factory for Kotlin coroutine's Deferred.
Muzei: A live wallpaper that gently refreshes your home screen each day with famous works of art. It also recedes into the background, blurring and dimming artwork to keep your icons and widgets in the spotlight. Simply double touch the wallpaper or open the Muzei app to enjoy and explore the artwork in its full glory.

",134
xwings/tuya,Python,"all about reversing, exploit, ctf and misc
顾名思义 涂鸦
@mail: kj _ xandora _ net
",14
mcidasv/mcidasv,Java,"This is the main trunk for SSEC's McIDAS-V Project.
Things of Interest
.                                   
├── build-customized.xml            * User-customizable Ant build file.
├── build.xml                       * Main Ant build file.
├── docs                            
│   ├── javadoc                     * API documentation for developers.
│   └── userguide                   * Project documentation.
├── edu                             
│   └── wisc                        
│       └── ssec                    
│           ├── mcidas              
│           └── mcidasv             * General managers and main application 
│               │                     code should go here, e.g., ViewManager, 
│               │                     McIDASV.java.
│               ├── chooser         * Data choosers should go here.
│               ├── control         * Display controls should go here.
│               ├── data            * Datasources should go here.
│               ├── display         * Displays code should go here.
│               ├── images          * DEPRECATED: please use appropriate
│               │                     directory within ""resources"".
│               ├── jython          * Linear Combination Jython Interpreter.
│               ├── monitors        * Monitor the state of a McIDAS-V session.
│               ├── probes          * Data probes.
│               ├── resources       * Non-code resources required by 
│               │   │                 McIDAS-V should reside here. Things
│               │   │                 like RBI, XML, and images.
│               │   └── python      * Jython library code.
│               ├── servermanager   * Handles local and remote ADDE datasets.
│               ├── startupmanager  * Manage McIDAS-V startup options.
│               ├── supportform     * Submit McIDAS-V support requests.
│               ├── ui              * UI related classes here, e.g., UIManager.
│               └── util            * Utility classes can go here.
|
├── lib                             * McIDAS-V dependencies (other than VisAD/IDV).
|   |
|   ├── linux-amd64                 * 64-bit Linux dependencies.
|   ├── linux-i586                  * 32-bit Linux dependencies.
|   ├── macosx                      * OS X dependencies
|   ├── share                       * Platform independent dependencies. This is
|   |                                 where most JAR files will end up.
|   ├── windows-amd64               * 64-bit Windows dependencies.
|   └── windows-i586                * 32-bit Windows dependencies.
|
├── release                         * Files used by install4J.
├── tools                           
│   ├── apidocs                     
│   └── external                    
│       ├── orphan_icon_finder      
│       ├── pluginfeed              
│       └── supportreq              
└── ucar                            

Running McIDAS-V
Assuming you've cloned the repo, and have installed Java 8+:
ant jar.runlarge

Building a new release
Make sure IDV is up to date, then build the ""dist"" target:
ant dist

Run Install4J and build the installer packages.
Nightlies
A cron job that builds the ""nightly"" target runs daily at 4am:
ant nightly

There is a separate script on the webserver that pulls the completed build.
Acknowledgements
YourKit is kindly supporting open source projects with its full-featured Java
Profiler. YourKit, LLC is the creator of innovative and intelligent tools for
profiling Java and .NET applications. Take a look at YourKit's leading
software products: YourKit Java Profiler and YourKit .NET Profiler.
",3
Shougo/denite.nvim,Python,"denite.nvim

About

Denite is a dark powered plugin for Neovim/Vim to unite all interfaces.
It can replace many features or plugins with its interface.
It is like a fuzzy finder, but is more generic.
You can extend the interface and create the sources.
Some things you can do with it include:


Opening files


Switching buffers


Inserting the value of a register


Changing current directory


Searching for a string


Unite.vim was meant to be like Helm for Vim.
But the implementation is ugly and it's very slow.
Denite resolves Unite's problems. Here are some of its benefits:


Theoretically faster because the main process is executed by Python


Theoretically more stable because no other processes can be performed when
it runs.


The implementation is simpler than unite


Has greater potential to implement new features


Python3 is easier to work with than Vimscript


There are a lot of useful tools to keep code simple (linter, tester, etc...)
in Python3.


Unite is officially obsolete, minor bugs (or even major bugs) are
not fixed anymore


Requirements
Denite requires Neovim 0.3.0+ or Vim 8.0+ with if_python3.
If :echo has(""python3"") returns 1, then you're done.
Note: You need to install Python3.6.1+.
For neovim:
You must install ""pynvim"" module with pip
pip3 install --user pynvim

If you want to read the Neovim-python/python3 interface install documentation,
you should read :help provider-python.
For Vim8:
Please install nvim-yarp plugin for Vim8.
https://github.com/roxma/nvim-yarp
Please install vim-hug-neovim-rpc plugin for Vim8.
https://github.com/roxma/vim-hug-neovim-rpc
You must install ""pynvim"" module with pip
pip3 install --user pynvim

For Windows users

Install Vim from Vim Win32 Installer
releases
Download Python latest embeddable zip
file and copy the all files in
the zip file to the folder where you installed Vim.

Note: You need to do 1. and 2. with the common-arch files (x86 or x64).
Screenshots




",1337
fate0/proxylist,HTML,"proxylist

proxylist, generate by fate0/getproxy project in every 15 minute
",444
bcgov/dbcrss,Python,"
dbcrss
DataBC Application Feeds Service
Visualizations
DataBC Web Services
https://uptime.apps.gov.bc.ca 
Purpose
Service Status Page
License
Copyright 2016 Province of British Columbia

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

",2
sunsettrack4/xmltv_epg,None,"About the easyEPG project
This is the free platform to download EPG data for your TV setup.
All XMLTV files are compatible with tvHeadend, KODI and other platforms supporting XMLTV files.
Support my work

Paypal Donation Link

Get help

Forum: Kodinerds
Email: sunsettrack4@gmail.com

Download links
Unified EPG list

Zattoo DE+CH

Current supported sources

Zattoo DE
Zattoo CH
Wilmaa CH
Horizon DE
TVPlayer UK

",20
koalaverse/vip,HTML,"vip: Variable Importance Plots 







Overview
vip is an R package for constructing variable importance
plots (VIPs). VIPs are part of a larger framework referred to as
interpretable machine learning (IML), which includes (but not limited
to): partial dependence plots (PDPs) and individual conditional
expectation (ICE) curves. While PDPs and ICE curves (available in the R
package pdp) help visualize
feature effects, VIPs help visualize feature impact (either locally or
globally). An in-progress, but comprehensive, overview of IML can be
found here: https://github.com/christophM/interpretable-ml-book.
Installation
# The easiest way to get vip is to install it from CRAN:
install.packages(""vip"")

# Alternatively, you can install the development version from GitHub:
if (!requireNamespace(""devtools"")) {
  install.packages(""devtools"")
}
devtools::install_github(""koalaverse/vip"")
For details and example usage, visit the vip package
website.
",47
silvernoo/ac-rss,Python,"AC-RSS
订阅以下地址
https://raw.githubusercontent.com/silvernoo/ac-rss/master/rrs/feed_110.xml
https://raw.githubusercontent.com/silvernoo/ac-rss/master/rrs/feed_164.xml
https://raw.githubusercontent.com/silvernoo/ac-rss/master/rrs/feed_184.xml
https://raw.githubusercontent.com/silvernoo/ac-rss/master/rrs/feed_73.xml
https://raw.githubusercontent.com/silvernoo/ac-rss/master/rrs/feed_74.xml
https://raw.githubusercontent.com/silvernoo/ac-rss/master/rrs/feed_75.xml
",17
CreatCodeBuild/brutal-algorithm-class,JavaScript,"brutal-algorithm-class
我所教的课《最强算法班》代码笔记
",6
timi-liuliang/echo,C++,"


Echo
Introduction
Echo is a new game engine, which used more industry standard of nowadays for game development. The new design concept makes the engine simplicity to use. but more powerful.
Examples
Documentation
Download



Description
Files
Previous




Editor [Win32]
echo-setup-2018.10.09.exe
old version


Editor [Mac OS X 10.7 or later]
echo.dmg



Examples
echo-examples-master.zip




Build
[Windows Editor]

Install [Visual Studio 2017] [CMake] [Python 3]...
EnterFolder ""${echo_root_path}/build/editor/windows""
Double click ""cmake.bat""
Double click ""echo.sln""

[Windows App]
Features
Easy Concept
Scene manager is easy, No Entiy, No GameObject, No Component, No Prefab. Only Node and NodeTree.
Highly Efficient Workflow

Multi-Platform Support
iOS Android Html5 Windows Mac Linux Steam
New Industry Standard Support
gLtf2.0, Vulkan, Pbr, Real time ray tracing.
2D And 3D Seamless Transition  
Every node can be 2d or 3d. The core difference is the camera and the unit the node use. So you can just switch a node to 2d or 3d easily.
Easy To Program
Mostly, you'll use Lua as your main programming language. and also you can use c++ directly. the design of node tree makes the Lua logic code more easy to write. and the embedded Lua editor and embedded document help you write code just in the echo editor.
Except for Lua, You can also choose use embedded Scratch as the main development language. Which is a type of visual script inspired by MIT.  In the echo, Scratch is based on Lua, when running the app, It'll convert to Lua, Make sure it's good both at code merge and running efficiency.
If you really like other types of script language, you can tell us, or you can support it by modifying the c++ code directly.

Configurable Module
Most of the engine's Functionality was implemented by configurable modules. that means when you release your app, you can just choose the module you really need. which makes your app have smaller size and more efficiency running speed.
Animate Everything
With Timeline, You can animate everything. You can not only animate any Object's(Node, Setting, Res) any property. But also you can call any Object's any function.
Open Source
Echo is licensed under MIT license. You can just do what you want as your wish.
",84
notadd/ng-notadd,TypeScript,"




ng-notadd
Medium-Background solution based on Angular7 Material2
中文说明
Technology stack

Typescript
Angular
Material2
rxjs
Graphql

RELATED LINKS
DEMO
ng-notadd-mock-server
Quick start
clone & run mock-server
    git clone https://github.com/notadd/ng-notadd-mock-server.git
    
    cd ng-notadd-mock-server
    
    npm install
    npm start
clone & run ng-notadd
    git clone https://github.com/notadd/ng-notadd.git
     
    cd ng-notadd
     
    npm install
    npm start
    # or use ng cli
    ng serve
Roadmap
0.18.0

 component phone end compatible
 firebase (not available domestically) or other alternative support

0.19.0

 DIY Dashboard
 JSON generates a simple dashboard

0.20.0

 2K/4K screen adaptation

0.21.0

 built-in permission component
 Preliminary e2e unit test

1.0

 Perfect unit testing
 Overall fine tuning

1.1

 websocket support

1.2

 Support electron to build desktop apps

2.0

 Enterprise-Class custom forms
 Enterprise-Class form system
 Enterprise window/Pop window

Follow-up

 Excel online editing
 Word online editing

",246
SmallChi/JT808,C#,"JT808协议

前提条件

掌握进制转换：二进制转十六进制；
掌握BCD编码、Hex编码；
掌握各种位移、异或；
掌握常用反射；
掌握快速ctrl+c、ctrl+v；
掌握以上装逼技能，就可以开始搬砖了。

JT808数据结构解析
数据包[JT808Package]



头标识
数据头
数据体
校验码
尾标识




Begin
JT808Header
JT808Bodies
CheckCode
End


7E
-
-
-
7E



数据头[JT808Header]



消息ID
消息体属性
终端手机号
消息流水号




MsgId
JT808HeaderMessageBodyProperty
TerminalPhoneNo
MsgNum



数据头-消息体属性[JT808HeaderMessageBodyProperty]



是否分包
加密标识
消息体长度
消息总包数
包序号




IsPackge
Encrypt
DataLength
PackgeCount
PackageIndex



消息体属性[JT808Bodies]

根据对应消息ID：MsgId

注意：数据内容(除去头和尾标识)进行转义判断
转义规则如下:

若数据内容中有出现字符 0x7e 的，需替换为字符 0x7d 紧跟字符 0x02;
若数据内容中有出现字符 0x7d 的，需替换为字符 0x7d 紧跟字符 0x01;

反转义的原因：确认JT808协议的TCP消息边界。
举个栗子1
1.组包：

MsgId 0x0200:位置信息汇报


JT808Package jT808Package = new JT808Package();

jT808Package.Header = new JT808Header
{
    MsgId = Enums.JT808MsgId.位置信息汇报,
    MsgNum = 126,
    TerminalPhoneNo = ""123456789012""
};

JT808_0x0200 jT808_0x0200 = new JT808_0x0200();
jT808_0x0200.AlarmFlag = 1;
jT808_0x0200.Altitude = 40;
jT808_0x0200.GPSTime = DateTime.Parse(""2018-10-15 10:10:10"");
jT808_0x0200.Lat = 12222222;
jT808_0x0200.Lng = 132444444;
jT808_0x0200.Speed = 60;
jT808_0x0200.Direction = 0;
jT808_0x0200.StatusFlag = 2;
jT808_0x0200.JT808LocationAttachData = new Dictionary<byte, JT808_0x0200_BodyBase>();

jT808_0x0200.JT808LocationAttachData.Add(JT808_0x0200_BodyBase.AttachId0x01, new JT808_0x0200_0x01
{
    Mileage = 100
});

jT808_0x0200.JT808LocationAttachData.Add(JT808_0x0200_BodyBase.AttachId0x02, new JT808_0x0200_0x02
{
    Oil = 125
});

jT808Package.Bodies = jT808_0x0200;

byte[] data = JT808Serializer.Serialize(jT808Package);

var hex = data.ToHexString();

// 输出结果Hex：
// 7E 02 00 00 26 12 34 56 78 90 12 00 7D 02 00 00 00 01 00 00 00 02 00 BA 7F 0E 07 E4 F1 1C 00 28 00 3C 00 00 18 10 15 10 10 10 01 04 00 00 00 64 02 02 00 7D 01 13 7E

2.手动解包：
1.原包：
7E 02 00 00 26 12 34 56 78 90 12 00 (7D 02) 00 00 00 01 00 00 00 02 00 BA 7F 0E 07 E4 F1 1C 00 28 00 3C 00 00 18 10 15 10 10 10 01 04 00 00 00 64 02 02 00 (7D 01) 13 7E

2.进行反转义
7D 02 ->7E
7D 01 ->7D
反转义后
7E 02 00 00 26 12 34 56 78 90 12 00 7E 00 00 00 01 00 00 00 02 00 BA 7F 0E 07 E4 F1 1C 00 28 00 3C 00 00 18 10 15 10 10 10 01 04 00 00 00 64 02 02 00 7D 13 7E

3.拆解
7E                  --头标识
02 00               --数据头->消息ID
00 26               --数据头->消息体属性
12 34 56 78 90 12   --数据头->终端手机号
00 7E               --数据头->消息流水号
00 00 00 01         --消息体->报警标志
00 00 00 02         --消息体->状态位标志
00 BA 7F 0E         --消息体->纬度
07 E4 F1 1C         --消息体->经度
00 28               --消息体->海拔高度
00 3C               --消息体->速度
00 00               --消息体->方向
18 10 15 10 10 10   --消息体->GPS时间
01                  --消息体->附加信息->里程
04                  --消息体->附加信息->长度
00 00 00 64         --消息体->附加信息->数据
02                  --消息体->附加信息->油量
02                  --消息体->附加信息->长度
00 7D               --消息体->附加信息->数据
13                  --检验码
7E                  --尾标识

3.程序解包：
//1.转成byte数组
byte[] bytes = ""7E 02 00 00 26 12 34 56 78 90 12 00 7D 02 00 00 00 01 00 00 00 02 00 BA 7F 0E 07 E4 F1 1C 00 28 00 3C 00 00 18 10 15 10 10 10 01 04 00 00 00 64 02 02 00 7D 01 13 7E"".ToHexBytes();

//2.将数组反序列化
var jT808Package = JT808Serializer.Deserialize<JT808Package>(bytes);

//3.数据包头
Assert.Equal(Enums.JT808MsgId.位置信息汇报, jT808Package.Header.MsgId);
Assert.Equal(38, jT808Package.Header.MessageBodyProperty.DataLength);
Assert.Equal(126, jT808Package.Header.MsgNum);
Assert.Equal(""123456789012"", jT808Package.Header.TerminalPhoneNo);
Assert.False(jT808Package.Header.MessageBodyProperty.IsPackge);
Assert.Equal(0, jT808Package.Header.MessageBodyProperty.PackageIndex);
Assert.Equal(0, jT808Package.Header.MessageBodyProperty.PackgeCount);
Assert.Equal(JT808EncryptMethod.None, jT808Package.Header.MessageBodyProperty.Encrypt);

//4.数据包体
JT808_0x0200 jT808_0x0200 = (JT808_0x0200)jT808Package.Bodies;
Assert.Equal((uint)1, jT808_0x0200.AlarmFlag);
Assert.Equal((uint)40, jT808_0x0200.Altitude);
Assert.Equal(DateTime.Parse(""2018-10-15 10:10:10""), jT808_0x0200.GPSTime);
Assert.Equal(12222222, jT808_0x0200.Lat);
Assert.Equal(132444444, jT808_0x0200.Lng);
Assert.Equal(60, jT808_0x0200.Speed);
Assert.Equal(0, jT808_0x0200.Direction);
Assert.Equal((uint)2, jT808_0x0200.StatusFlag);
//4.1.附加信息1
Assert.Equal(100, ((JT808_0x0200_0x01)jT808_0x0200.JT808LocationAttachData[JT808_0x0200_BodyBase.AttachId0x01]).Mileage);
//4.2.附加信息2
Assert.Equal(125, ((JT808_0x0200_0x02)jT808_0x0200.JT808LocationAttachData[JT808_0x0200_BodyBase.AttachId0x02]).Oil);

举个栗子2
// 使用消息Id的扩展方法创建JT808Package包
JT808Package jT808Package = Enums.JT808MsgId.位置信息汇报.Create(""123456789012"",
    new JT808_0x0200 {
        AlarmFlag = 1,
        Altitude = 40,
        GPSTime = DateTime.Parse(""2018-10-15 10:10:10""),
        Lat = 12222222,
        Lng = 132444444,
        Speed = 60,
        Direction = 0,
        StatusFlag = 2,
        JT808LocationAttachData = new Dictionary<byte, JT808LocationAttachBase>
        {
            { JT808_0x0200_BodyBase.AttachId0x01,new JT808_0x0200_0x01{Mileage = 100}},
            { JT808_0x0200_BodyBase.AttachId0x02,new JT808_0x0200_0x02{Oil = 125}}
        }
});

byte[] data = JT808Serializer.Serialize(jT808Package);

var hex = data.ToHexString();
//输出结果Hex：
//7E 02 00 00 26 12 34 56 78 90 12 00 01 00 00 00 01 00 00 00 02 00 BA 7F 0E 07 E4 F1 1C 00 28 00 3C 00 00 18 10 15 10 10 10 01 04 00 00 00 64 02 02 00 7D 01 6C 7E

举个栗子3
// 全局配置
JT808GlobalConfig.Instance
    // 注册自定义位置附加信息
    .Register_0x0200_Attach(0x06)
    //.SetMsgSNDistributed(//todo 实现IMsgSNDistributed消息流水号)
    // 注册自定义数据上行透传信息
    //.Register_0x0900_Ext<>(//todo 继承自JT808_0x0900_BodyBase类)
    // 注册自定义数据下行透传信息
    //.Register_0x8900_Ext<>(//todo 继承自JT808_0x8900_BodyBase类)
    // 跳过校验码验证
    .SetSkipCRCCode(true);

举个栗子4
遇到的问题-多设备多协议的自定义位置附加信息
场景：
一个设备厂商对应多个设备类型，不同设备类型可能存在相同的自定义位置附加信息Id，导致自定义附加信息Id冲突，无法解析。
解决方式：
1.凡是解析自定义附加信息Id协议的，先进行分割存储，然后在根据外部的设备类型进行统一处理;
2.可以根据设备类型做个工厂，解耦对公共序列化器的依赖。
可以参考DemoTest的Demo5

要是哪位大佬还有其他的解决方式，请您告知我下，谢谢您了。

举个栗子5
遇到的问题-多媒体数据上传进行分包处理
场景:
设备在上传多媒体数据的时候，由于数据比较多，一次上传不了，所以采用分包方式处理。
解决方式：


第一包数据上来采用平常的方式去解析数据；


当N包数据上来，采用统一分包消息体去接收数据，最后在合并成一条。



普及知识点：一般行业分包是按256的整数倍，太多不行，太少也不行，必须刚刚好。

可以参考DemoTest的Demo6
NuGet安装



Package Name
Version
Downloads




Install-Package JT808




Install-Package JT808.Extensions.DependencyInjection





使用BenchmarkDotNet性能测试报告（只是玩玩，不能当真）
BenchmarkDotNet=v0.11.3, OS=Windows 10.0.17134.472 (1803/April2018Update/Redstone4)
Intel Core i7-8700K CPU 3.70GHz (Coffee Lake), 1 CPU, 12 logical and 6 physical cores
  [Host]     : .NET Framework 4.7.2 (CLR 4.0.30319.42000), 64bit RyuJIT-v4.7.3260.0
  Job-FVMQGI : .NET Framework 4.7.2 (CLR 4.0.30319.42000), 64bit RyuJIT-v4.7.3260.0
  Job-LGLQDK : .NET Core 2.2.1 (CoreCLR 4.6.27207.03, CoreFX 4.6.27207.03), 64bit RyuJIT

Platform=AnyCpu  Runtime=Clr  Server=False  




Method
Toolchain
N
Mean
Error
StdDev
Gen 0/1k Op
Gen 1/1k Op
Gen 2/1k Op
Allocated Memory/Op




0x0200Serialize
Default
100
3.199 ms
0.0417 ms
0.0390 ms
74.2188
-
-
475.79 KB


0x0200Deserialize
Default
100
3.007 ms
0.0379 ms
0.0355 ms
78.1250
-
-
501.57 KB


0x0200Serialize
.NET Core 2.2
100
2.507 ms
0.0157 ms
0.0139 ms
66.4063
-
-
424.22 KB


0x0200Deserialize
.NET Core 2.2
100
2.423 ms
0.0483 ms
0.0645 ms
70.3125
-
-
442.97 KB


0x0200Serialize
Default
10000
317.658 ms
5.1248 ms
4.7937 ms
7000.0000
-
-
47584.41 KB


0x0200Deserialize
Default
10000
302.344 ms
5.7195 ms
5.6174 ms
8000.0000
-
-
50160.67 KB


0x0200Serialize
.NET Core 2.2
10000
251.612 ms
3.2408 ms
2.7062 ms
6000.0000
-
-
42421.88 KB


0x0200Deserialize
.NET Core 2.2
10000
234.793 ms
2.5215 ms
2.2352 ms
7000.0000
-
-
44296.88 KB


0x0200Serialize
Default
100000
3,228.227 ms
26.2990 ms
23.3134 ms
77000.0000
-
-
475789.2 KB


0x0200Deserialize
Default
100000
2,999.779 ms
27.0077 ms
23.9416 ms
81000.0000
-
-
501566.45 KB


0x0200Serialize
.NET Core 2.2
100000
2,541.111 ms
20.8916 ms
18.5199 ms
69000.0000
-
-
424218.75 KB


0x0200Deserialize
.NET Core 2.2
100000
2,350.114 ms
11.5168 ms
10.2093 ms
72000.0000
-
-
442968.75 KB



JT808终端通讯协议消息对照表



序号
消息ID
完成情况
测试情况
消息体名称




1
0x0001
√
√
终端通用应答


2
0x8001
√
√
平台通用应答


3
0x0002
√
√
终端心跳


4
0x8003
√
√
补传分包请求


5
0x0100
√
√
终端注册


6
0x8100
√
√
终端注册应答


7
0x0003
√
√
终端注销


8
0x0102
√
√
终端鉴权


9
0x8103
√
√
设置终端参数


10
0x8104
√
√
查询终端参数


11
0x0104
√
√
查询终端参数应答


12
0x8105
√
√
终端控制


13
0x8106
√
√
查询指定终端参数


14
0x8107
√
消息体为空
查询终端属性


15
0x0107
√
√
查询终端属性应答


16
0x8108
√
√
下发终端升级包


17
0x0108
√
√
终端升级结果通知


18
0x0200
√
√
位置信息汇报


19
0x8201
√
√
位置信息查询


20
0x0201
√
√
位置信息查询应答


21
0x8202
√
√
临时位置跟踪控制


22
0x8203
√
√
人工确认报警消息


23
0x8300
√
√
文本信息下发


24
0x8301
√
√
事件设置


25
0x0301
√
√
事件报告


26
0x8302
√
√
提问下发


27
0x0302
√
√
提问应答


28
0x8303
√
√
信息点播菜单设置


29
0x0303
√
√
信息点播/取消


30
0x8304
√
√
信息服务


31
0x8400
√
√
电话回拨


32
0x8401
√
√
设置电话本


33
0x8500
√
√
车辆控制


34
0x0500
√
√
车辆控制应答


35
0x8600
√
√
设置圆形区域


36
0x8601
√
√
删除圆形区域


37
0x8602
√
√
设置矩形区域


38
0x8603
√
√
删除矩形区域


39
0x8604
√
√
设置多边形区域


40
0x8605
√
√
删除多边形区域


41
0x8606
√
√
设置路线


42
0x8607
√
√
删除路线


43
0x8700
x
下个版本
行驶记录仪数据采集命令


44
0x0700
x
下个版本
行驶记录仪数据上传


45
0x8701
x
下个版本
行驶记录仪参数下传命令


46
0x0701
√
√
电子运单上报


47
0x0702
√
√
驾驶员身份信息采集上报


48
0x8702
√
消息体为空
上报驾驶员身份信息请求


49
0x0704
√
√
定位数据批量上传


50
0x0705
√
√
CAN 总线数据上传


51
0x0800
√
√
多媒体事件信息上传


52
0x0801
√
√
多媒体数据上传


53
0x8800
√
√
多媒体数据上传应答


54
0x8801
√
√
摄像头立即拍摄命令


55
0x0805
√
√
摄像头立即拍摄命令应答


56
0x8802
√
√
存储多媒体数据检索


57
0x0802
√
√
存储多媒体数据检索应答


58
0x8803
√
√
存储多媒体数据上传


59
0x8804
√
√
录音开始命令


60
0x8805
√
√
单条存储多媒体数据检索上传命令


61
0x8900
√
√
数据下行透传


62
0x0900
√
√
数据上行透传


63
0x0901
√
√
数据压缩上报


64
0x8A00
√
√
平台 RSA 公钥


65
0x0A00
√
√
终端 RSA 公钥


66
0x8F00~0x8FFF
保留
保留
平台下行消息保留


67
0x0F00~0x0FFF
保留
保留
终端上行消息保留



",45
SmallChi/JT808,C#,"JT808协议

前提条件

掌握进制转换：二进制转十六进制；
掌握BCD编码、Hex编码；
掌握各种位移、异或；
掌握常用反射；
掌握快速ctrl+c、ctrl+v；
掌握以上装逼技能，就可以开始搬砖了。

JT808数据结构解析
数据包[JT808Package]



头标识
数据头
数据体
校验码
尾标识




Begin
JT808Header
JT808Bodies
CheckCode
End


7E
-
-
-
7E



数据头[JT808Header]



消息ID
消息体属性
终端手机号
消息流水号




MsgId
JT808HeaderMessageBodyProperty
TerminalPhoneNo
MsgNum



数据头-消息体属性[JT808HeaderMessageBodyProperty]



是否分包
加密标识
消息体长度
消息总包数
包序号




IsPackge
Encrypt
DataLength
PackgeCount
PackageIndex



消息体属性[JT808Bodies]

根据对应消息ID：MsgId

注意：数据内容(除去头和尾标识)进行转义判断
转义规则如下:

若数据内容中有出现字符 0x7e 的，需替换为字符 0x7d 紧跟字符 0x02;
若数据内容中有出现字符 0x7d 的，需替换为字符 0x7d 紧跟字符 0x01;

反转义的原因：确认JT808协议的TCP消息边界。
举个栗子1
1.组包：

MsgId 0x0200:位置信息汇报


JT808Package jT808Package = new JT808Package();

jT808Package.Header = new JT808Header
{
    MsgId = Enums.JT808MsgId.位置信息汇报,
    MsgNum = 126,
    TerminalPhoneNo = ""123456789012""
};

JT808_0x0200 jT808_0x0200 = new JT808_0x0200();
jT808_0x0200.AlarmFlag = 1;
jT808_0x0200.Altitude = 40;
jT808_0x0200.GPSTime = DateTime.Parse(""2018-10-15 10:10:10"");
jT808_0x0200.Lat = 12222222;
jT808_0x0200.Lng = 132444444;
jT808_0x0200.Speed = 60;
jT808_0x0200.Direction = 0;
jT808_0x0200.StatusFlag = 2;
jT808_0x0200.JT808LocationAttachData = new Dictionary<byte, JT808_0x0200_BodyBase>();

jT808_0x0200.JT808LocationAttachData.Add(JT808_0x0200_BodyBase.AttachId0x01, new JT808_0x0200_0x01
{
    Mileage = 100
});

jT808_0x0200.JT808LocationAttachData.Add(JT808_0x0200_BodyBase.AttachId0x02, new JT808_0x0200_0x02
{
    Oil = 125
});

jT808Package.Bodies = jT808_0x0200;

byte[] data = JT808Serializer.Serialize(jT808Package);

var hex = data.ToHexString();

// 输出结果Hex：
// 7E 02 00 00 26 12 34 56 78 90 12 00 7D 02 00 00 00 01 00 00 00 02 00 BA 7F 0E 07 E4 F1 1C 00 28 00 3C 00 00 18 10 15 10 10 10 01 04 00 00 00 64 02 02 00 7D 01 13 7E

2.手动解包：
1.原包：
7E 02 00 00 26 12 34 56 78 90 12 00 (7D 02) 00 00 00 01 00 00 00 02 00 BA 7F 0E 07 E4 F1 1C 00 28 00 3C 00 00 18 10 15 10 10 10 01 04 00 00 00 64 02 02 00 (7D 01) 13 7E

2.进行反转义
7D 02 ->7E
7D 01 ->7D
反转义后
7E 02 00 00 26 12 34 56 78 90 12 00 7E 00 00 00 01 00 00 00 02 00 BA 7F 0E 07 E4 F1 1C 00 28 00 3C 00 00 18 10 15 10 10 10 01 04 00 00 00 64 02 02 00 7D 13 7E

3.拆解
7E                  --头标识
02 00               --数据头->消息ID
00 26               --数据头->消息体属性
12 34 56 78 90 12   --数据头->终端手机号
00 7E               --数据头->消息流水号
00 00 00 01         --消息体->报警标志
00 00 00 02         --消息体->状态位标志
00 BA 7F 0E         --消息体->纬度
07 E4 F1 1C         --消息体->经度
00 28               --消息体->海拔高度
00 3C               --消息体->速度
00 00               --消息体->方向
18 10 15 10 10 10   --消息体->GPS时间
01                  --消息体->附加信息->里程
04                  --消息体->附加信息->长度
00 00 00 64         --消息体->附加信息->数据
02                  --消息体->附加信息->油量
02                  --消息体->附加信息->长度
00 7D               --消息体->附加信息->数据
13                  --检验码
7E                  --尾标识

3.程序解包：
//1.转成byte数组
byte[] bytes = ""7E 02 00 00 26 12 34 56 78 90 12 00 7D 02 00 00 00 01 00 00 00 02 00 BA 7F 0E 07 E4 F1 1C 00 28 00 3C 00 00 18 10 15 10 10 10 01 04 00 00 00 64 02 02 00 7D 01 13 7E"".ToHexBytes();

//2.将数组反序列化
var jT808Package = JT808Serializer.Deserialize<JT808Package>(bytes);

//3.数据包头
Assert.Equal(Enums.JT808MsgId.位置信息汇报, jT808Package.Header.MsgId);
Assert.Equal(38, jT808Package.Header.MessageBodyProperty.DataLength);
Assert.Equal(126, jT808Package.Header.MsgNum);
Assert.Equal(""123456789012"", jT808Package.Header.TerminalPhoneNo);
Assert.False(jT808Package.Header.MessageBodyProperty.IsPackge);
Assert.Equal(0, jT808Package.Header.MessageBodyProperty.PackageIndex);
Assert.Equal(0, jT808Package.Header.MessageBodyProperty.PackgeCount);
Assert.Equal(JT808EncryptMethod.None, jT808Package.Header.MessageBodyProperty.Encrypt);

//4.数据包体
JT808_0x0200 jT808_0x0200 = (JT808_0x0200)jT808Package.Bodies;
Assert.Equal((uint)1, jT808_0x0200.AlarmFlag);
Assert.Equal((uint)40, jT808_0x0200.Altitude);
Assert.Equal(DateTime.Parse(""2018-10-15 10:10:10""), jT808_0x0200.GPSTime);
Assert.Equal(12222222, jT808_0x0200.Lat);
Assert.Equal(132444444, jT808_0x0200.Lng);
Assert.Equal(60, jT808_0x0200.Speed);
Assert.Equal(0, jT808_0x0200.Direction);
Assert.Equal((uint)2, jT808_0x0200.StatusFlag);
//4.1.附加信息1
Assert.Equal(100, ((JT808_0x0200_0x01)jT808_0x0200.JT808LocationAttachData[JT808_0x0200_BodyBase.AttachId0x01]).Mileage);
//4.2.附加信息2
Assert.Equal(125, ((JT808_0x0200_0x02)jT808_0x0200.JT808LocationAttachData[JT808_0x0200_BodyBase.AttachId0x02]).Oil);

举个栗子2
// 使用消息Id的扩展方法创建JT808Package包
JT808Package jT808Package = Enums.JT808MsgId.位置信息汇报.Create(""123456789012"",
    new JT808_0x0200 {
        AlarmFlag = 1,
        Altitude = 40,
        GPSTime = DateTime.Parse(""2018-10-15 10:10:10""),
        Lat = 12222222,
        Lng = 132444444,
        Speed = 60,
        Direction = 0,
        StatusFlag = 2,
        JT808LocationAttachData = new Dictionary<byte, JT808LocationAttachBase>
        {
            { JT808_0x0200_BodyBase.AttachId0x01,new JT808_0x0200_0x01{Mileage = 100}},
            { JT808_0x0200_BodyBase.AttachId0x02,new JT808_0x0200_0x02{Oil = 125}}
        }
});

byte[] data = JT808Serializer.Serialize(jT808Package);

var hex = data.ToHexString();
//输出结果Hex：
//7E 02 00 00 26 12 34 56 78 90 12 00 01 00 00 00 01 00 00 00 02 00 BA 7F 0E 07 E4 F1 1C 00 28 00 3C 00 00 18 10 15 10 10 10 01 04 00 00 00 64 02 02 00 7D 01 6C 7E

举个栗子3
// 全局配置
JT808GlobalConfig.Instance
    // 注册自定义位置附加信息
    .Register_0x0200_Attach(0x06)
    //.SetMsgSNDistributed(//todo 实现IMsgSNDistributed消息流水号)
    // 注册自定义数据上行透传信息
    //.Register_0x0900_Ext<>(//todo 继承自JT808_0x0900_BodyBase类)
    // 注册自定义数据下行透传信息
    //.Register_0x8900_Ext<>(//todo 继承自JT808_0x8900_BodyBase类)
    // 跳过校验码验证
    .SetSkipCRCCode(true);

举个栗子4
遇到的问题-多设备多协议的自定义位置附加信息
场景：
一个设备厂商对应多个设备类型，不同设备类型可能存在相同的自定义位置附加信息Id，导致自定义附加信息Id冲突，无法解析。
解决方式：
1.凡是解析自定义附加信息Id协议的，先进行分割存储，然后在根据外部的设备类型进行统一处理;
2.可以根据设备类型做个工厂，解耦对公共序列化器的依赖。
可以参考DemoTest的Demo5

要是哪位大佬还有其他的解决方式，请您告知我下，谢谢您了。

举个栗子5
遇到的问题-多媒体数据上传进行分包处理
场景:
设备在上传多媒体数据的时候，由于数据比较多，一次上传不了，所以采用分包方式处理。
解决方式：


第一包数据上来采用平常的方式去解析数据；


当N包数据上来，采用统一分包消息体去接收数据，最后在合并成一条。



普及知识点：一般行业分包是按256的整数倍，太多不行，太少也不行，必须刚刚好。

可以参考DemoTest的Demo6
NuGet安装



Package Name
Version
Downloads




Install-Package JT808




Install-Package JT808.Extensions.DependencyInjection





使用BenchmarkDotNet性能测试报告（只是玩玩，不能当真）
BenchmarkDotNet=v0.11.3, OS=Windows 10.0.17134.472 (1803/April2018Update/Redstone4)
Intel Core i7-8700K CPU 3.70GHz (Coffee Lake), 1 CPU, 12 logical and 6 physical cores
  [Host]     : .NET Framework 4.7.2 (CLR 4.0.30319.42000), 64bit RyuJIT-v4.7.3260.0
  Job-FVMQGI : .NET Framework 4.7.2 (CLR 4.0.30319.42000), 64bit RyuJIT-v4.7.3260.0
  Job-LGLQDK : .NET Core 2.2.1 (CoreCLR 4.6.27207.03, CoreFX 4.6.27207.03), 64bit RyuJIT

Platform=AnyCpu  Runtime=Clr  Server=False  




Method
Toolchain
N
Mean
Error
StdDev
Gen 0/1k Op
Gen 1/1k Op
Gen 2/1k Op
Allocated Memory/Op




0x0200Serialize
Default
100
3.199 ms
0.0417 ms
0.0390 ms
74.2188
-
-
475.79 KB


0x0200Deserialize
Default
100
3.007 ms
0.0379 ms
0.0355 ms
78.1250
-
-
501.57 KB


0x0200Serialize
.NET Core 2.2
100
2.507 ms
0.0157 ms
0.0139 ms
66.4063
-
-
424.22 KB


0x0200Deserialize
.NET Core 2.2
100
2.423 ms
0.0483 ms
0.0645 ms
70.3125
-
-
442.97 KB


0x0200Serialize
Default
10000
317.658 ms
5.1248 ms
4.7937 ms
7000.0000
-
-
47584.41 KB


0x0200Deserialize
Default
10000
302.344 ms
5.7195 ms
5.6174 ms
8000.0000
-
-
50160.67 KB


0x0200Serialize
.NET Core 2.2
10000
251.612 ms
3.2408 ms
2.7062 ms
6000.0000
-
-
42421.88 KB


0x0200Deserialize
.NET Core 2.2
10000
234.793 ms
2.5215 ms
2.2352 ms
7000.0000
-
-
44296.88 KB


0x0200Serialize
Default
100000
3,228.227 ms
26.2990 ms
23.3134 ms
77000.0000
-
-
475789.2 KB


0x0200Deserialize
Default
100000
2,999.779 ms
27.0077 ms
23.9416 ms
81000.0000
-
-
501566.45 KB


0x0200Serialize
.NET Core 2.2
100000
2,541.111 ms
20.8916 ms
18.5199 ms
69000.0000
-
-
424218.75 KB


0x0200Deserialize
.NET Core 2.2
100000
2,350.114 ms
11.5168 ms
10.2093 ms
72000.0000
-
-
442968.75 KB



JT808终端通讯协议消息对照表



序号
消息ID
完成情况
测试情况
消息体名称




1
0x0001
√
√
终端通用应答


2
0x8001
√
√
平台通用应答


3
0x0002
√
√
终端心跳


4
0x8003
√
√
补传分包请求


5
0x0100
√
√
终端注册


6
0x8100
√
√
终端注册应答


7
0x0003
√
√
终端注销


8
0x0102
√
√
终端鉴权


9
0x8103
√
√
设置终端参数


10
0x8104
√
√
查询终端参数


11
0x0104
√
√
查询终端参数应答


12
0x8105
√
√
终端控制


13
0x8106
√
√
查询指定终端参数


14
0x8107
√
消息体为空
查询终端属性


15
0x0107
√
√
查询终端属性应答


16
0x8108
√
√
下发终端升级包


17
0x0108
√
√
终端升级结果通知


18
0x0200
√
√
位置信息汇报


19
0x8201
√
√
位置信息查询


20
0x0201
√
√
位置信息查询应答


21
0x8202
√
√
临时位置跟踪控制


22
0x8203
√
√
人工确认报警消息


23
0x8300
√
√
文本信息下发


24
0x8301
√
√
事件设置


25
0x0301
√
√
事件报告


26
0x8302
√
√
提问下发


27
0x0302
√
√
提问应答


28
0x8303
√
√
信息点播菜单设置


29
0x0303
√
√
信息点播/取消


30
0x8304
√
√
信息服务


31
0x8400
√
√
电话回拨


32
0x8401
√
√
设置电话本


33
0x8500
√
√
车辆控制


34
0x0500
√
√
车辆控制应答


35
0x8600
√
√
设置圆形区域


36
0x8601
√
√
删除圆形区域


37
0x8602
√
√
设置矩形区域


38
0x8603
√
√
删除矩形区域


39
0x8604
√
√
设置多边形区域


40
0x8605
√
√
删除多边形区域


41
0x8606
√
√
设置路线


42
0x8607
√
√
删除路线


43
0x8700
x
下个版本
行驶记录仪数据采集命令


44
0x0700
x
下个版本
行驶记录仪数据上传


45
0x8701
x
下个版本
行驶记录仪参数下传命令


46
0x0701
√
√
电子运单上报


47
0x0702
√
√
驾驶员身份信息采集上报


48
0x8702
√
消息体为空
上报驾驶员身份信息请求


49
0x0704
√
√
定位数据批量上传


50
0x0705
√
√
CAN 总线数据上传


51
0x0800
√
√
多媒体事件信息上传


52
0x0801
√
√
多媒体数据上传


53
0x8800
√
√
多媒体数据上传应答


54
0x8801
√
√
摄像头立即拍摄命令


55
0x0805
√
√
摄像头立即拍摄命令应答


56
0x8802
√
√
存储多媒体数据检索


57
0x0802
√
√
存储多媒体数据检索应答


58
0x8803
√
√
存储多媒体数据上传


59
0x8804
√
√
录音开始命令


60
0x8805
√
√
单条存储多媒体数据检索上传命令


61
0x8900
√
√
数据下行透传


62
0x0900
√
√
数据上行透传


63
0x0901
√
√
数据压缩上报


64
0x8A00
√
√
平台 RSA 公钥


65
0x0A00
√
√
终端 RSA 公钥


66
0x8F00~0x8FFF
保留
保留
平台下行消息保留


67
0x0F00~0x0FFF
保留
保留
终端上行消息保留



",45
joimxjtuse/FuncTest,Java,"工作和学习中遇到的一些问题及其整理
算法
leet-code
面试题
设计模式
经典的设计模式
扩展的设计模式
基础知识等
",2
beyondye/framework,PHP,"ENPHP Framework是一个轻量级的，开包即用的PHP框架。
特别适合中小型网站的开发建设，自带数据表验证，多数据库分离支持，常用的库文件。
以简化那些80%重复功能为目标打造出此框架，如果您厌烦重量级框架，请试试ENPHP Framework。
版本库依赖

版本 PHP7+
mb_string扩展
GD2扩展

文档目录索引



 
 
 
 




入口文件配置
常量设置
数据库配置
自定义配置数据字典


全局变量数组
数据库基本操作
Model数据模型
Model数据验证


Controller控制器
View视图
Helper帮助函数
Input输入


Output输出
Session会话
Cookie管理
Lang多语言配置


Redis缓存
Security安全
Upload上传文件
Html标签生成


Grid表格生成
Image图片修饰
Smtp发送邮件
Captcha验证码生成


应用程序目录布局说明






文档内容
保留属性及函数方法

不推荐覆盖，除非你了解全局代码。

保留的属性
$this->input //输入类实例
$this->config //配置类实例
$this->output //输出类实例
$this->session //会话类实例
$this->cookie //cookie类实例
$this->lang //默认多语言类实例
$this->helper //帮助类实例
$this->security //安全类实例
$this->redis //redis类实例
$this->vars //全局变量数组
$this->db //默认数据库实例

保留的方法函数
$this->db() //自定义数据库并返回实例
$this->lang() //自定义语言类并返回实例
$this->model() //加载model并返回实例
$this->redis() //自定义redis并返回实例

入口文件配置

入口文件一般是网站的根目录index.php文件，有几个重要的常量配置。

运行环境设置
设置运行环境变量，三个值分别为测试环境，产品环境，开发环境。
// test,production,development
define('ENVIRONMENT', 'development');
应用目录设置
您开发的应用程序目录常量APP_DIR设置。
define('APP_DIR', realpath('app_dir') . DIRECTORY_SEPARATOR);
框架目录设置
框架系统文件目录常量，可以存放到其他地方，以便共用和升级。
define('SYS_DIR', realpath('system_dir') . DIRECTORY_SEPARATOR);
控制器模块设置
设置controller模块常量，模块必须是APP_DIR目录下module文件夹的子目录。
define('MODULE', 'www');
模板视图目录设置
设置模板目录常量，模板必须是APP_DIR目录下template文件夹的子目录。
define('TEMPLATE', 'www');
常量设置

常量文件位置在APP_DIR/config/下面三个子目录test,production,development中的constans.php文件分别按环境设置。

地址路由
地址路由配置,以/index.php?c=main&a=index为例子。
c代表控制器类名字，默认控制器为Main。
a代表action方法名称，默认action为index。
你可以自定义设置这些值。
define('DEFAULT_CONTROLLER', 'main');
define('DEFAULT_ACTION', 'index');
define('CONTROLLER_KEY_NAME', 'c');
define('ACTION_KEY_NAME', 'a');
字符编码
输出字符编码设置，以便$this->output->view()和$this->output->json()输出
define('CHARSET', 'utf-8');
Cookie相关设置
//有效域名
define('COOKIE_DOMAIN', 'test.com'); 

//是否https发送
define('COOKIE_SECURE', false);

//有效路径
define('COOKIE_PATH', '/');

//http只读
define('COOKIE_HTTPONLY', false);

//过期时间秒
define('COOKIE_EXPIRE', 0);
Session设置

//自定义session cookie名
define('SESSION_COOKIE_NAME', 'SE');

//session保存时间，0为关闭浏览器即失效，秒为单位
define('SESSION_EXPIRE', 0);
安全配置
//加密安全混淆值
define('ENCRYPTION_KEY', 'weryi9878sdfddtgtbsdfh');

//表单提交token session 名称
define('TOKEN_SESSION_NAME', '34efddddre');

//表单token字段名
define('TOKEN_INPUT_NAME', 'fh40dfk9dd8dkfje');

//token过期时间，秒为单位
define('TOKEN_EXPIRE', 3600);
多语言应用
//默认语言环境
define('LANG', 'zh_cn');
URL转换
URL重写转换输出模版，和路由无关，以配合$this->helper->url()使用
//url 重写
define('URL', ['mod_name'=>['controller_name/action_name'=>'/{controller_key}/{action_key}']]);

//例子 

//注意$this->helper->url()参数和数组key的顺序
define('URL', [
    'www' => [ 
    //www表示模块名称
    
        'main/index' => '/',  
        //echo $this->helper->url(['c'=>'main','a'=>'index'])
        //输出 /
        
        'main/lists/type' => '/list/{type}.html',
         //echo $this->helper->url(['c'=>'main','a'=>'lists','type'=>'2'])
         //输出 /list/2.html
        
        'main/lists/type/page' => '/list/{type}_{page}.html'
         //echo $this->helper->url(['c'=>'main','a'=>'lists','type'=>'2','page'=>'34']) 
         //输出 /list/2_34.html
                 
    ]
]);
数据库配置

数据库文件位置在APP_DIR/config/下面三个子目录test,production,development中的database.php文件分别按环境设置。
暂时只支持mysqli

default为默认数据库，可以直接$this->db访问默认数据库
$this->db('read)访问read数据库
//例子

return [
    //默认数据库
    'default' => 
    [
        'driver' => 'mysqli',
        'host' => 'set.database.to.hosts.file',
        'username' => 'root', 
        'password' => '123456',
        'database' => 'dataname',
        'port' => 3306,
        'charset' => 'utf8'
    ],
    
    //读数据库
    'read'=>
    [
        'driver' => 'mysqli',
        'host' => 'set.database.to.hosts.file',
        'username' => 'root', 
        'password' => '123456',
        'database' => 'dataname',
        'port' => 3306,
        'charset' => 'utf8'
    ]
];
自定义配置数据字典

自定义配置数据字典，主要为了应对某些应用较多的元数据存储访问
保存于APP_DIR/config目录下面PHP文件内容为数组

以APP_DIR/config/test.php为范例,配合$this->config使用
//test.php内容
return ['key2'=>'val2','key'=>['a','b','c'];

//var_dump $this->config->test
//输出 ['key2'=>'val2','key'=>['a','b','c']

//echo $this->config->test['key'][0]
//输出 a
全局变量数组
全局变量数组$var。
//全局变量数组，
//$this->vars 可以直接访问，
//默认已包含$this->vars['controller']当前控制器值
//默认已包含$this->vars['action']当前action值
$vars = [];
数据库基本操作

暂时只支持mysqli
配置好数据库以后，我们可以 $this->db 调用默认数据库。
或者可以$this->db('read')调用一个已配置为'read'的数据库。

$this->db->query($sql) 方法
原始SQL语句执行，如是select返回数据集，delete，insert，update返回布尔值。
//返回一个结果集对象句柄
$result=$this->db->query('select * from table1 where f=2;');

//返回数据的条数，int类型
$result->num_rows;

//返回结果集其中一条数据，默认第一条以对象形式返回字段
$result->row();

//以数组形式返回第3条数据
//$row['f'];
$row=$result->row(2,'array');


//以对象形式返回第4条数据
//$row->f;
$row=$result->row(3,'object')


//返回数据集，默认对象形式
$recordset=$result->result();
foreach($recordset as $rs){
    echo $rs->f;
}

//数组形式返回数据集
$recordset=$result->result('array');
foreach($recordset as $rs){
    echo $rs['f'];
}
$this->db->select($table,$condition=[]) 方法
查询数据库表返回数据集对象。

参数说明

$table 数据表名称
$condition 查询条件数组，如果为空返回全部


//查询条件
$condition= [
     'where' => ['f1'=>'2','f3>'=>'3','f4!='=>'8'], //where条件,支持运算符>,<,<>,!=,=,in,like,>=,<=
     'fields' => ['f1','f2','f3'],//返回字段
     'orderby' => ['f1'=>'desc','f2'=>'asc'], //排序
     'limit' => [0,20] //返回数据条数 ，也可以是一个int值，如：limit=>10
  ];
 
//返回数据句对象
$recordset=$this->db->select('table1',$condition);
foreach($recordset->result() as $rs){
    echo $rs->f1;
}
$this->db->insert($table,$data) 方法
插入数据到数据库表，返回布尔值。

参数说明

$table 数据表名称 
$data  插入表的数据数组


//需要插入的数据
$data=['f1'=>'1','f2'=>'2'];

$rs=$this->db->insert('table1',$data)；
if($rs){
   //插入成功，返回最后一条插入语句产生的自增ID
   $this->db->insert_id;
}
$this->db->delete($table,$where=[]) 方法
删除数据集，返回布尔值

参数说明

$table 数据表名称
$where where条件数组，为空删除全部，谨慎使用！


//删除条件
$data=['f1'=>'1','f2'=>'2'];

$rs=$this->db->delete('table1',$data)；
if($rs){
   //删除成功，返回影响数据行数
   $this->db->affected_rows;
}
$this->db->escape($str) 方法
SQL语句中的特殊字符进行转义，返回转义后字符串。
//参见 http://php.net/manual/zh/mysqli.real-escape-string.php
$this->db->escape('str');
$this->db->replace($table,$data) 方法
数据集主键如果存在就替换不然插入新数据，返回布尔值。

参数说明

$table 数据表名称
$data 需要操作的数据数组


//需要插入或替换的数据，如果主键primary=1已存在，即替换本条数据，不然插入新数据。
$data=['primary'=>1,'f1'=>'1','f2'=>'2'];

$rs=$this->db->replace('table1',$data)；
$this->db->update($table,$data,$where=[]) 方法
更新数据，返回布尔值或影响行数。

参数说明

$table 数据表名称
$data 需要更新的数据数组
$where where条件，为空更新全部


$data=['f1'=>'3','f3'=>'1'];
$where=['id'=>2];

$rs=$this->db->update('table1',$data,$where);

if($rs){
   //更新成功，返回影响数据行数
   $this->db->affected_rows;
 
}
$this->db->close() 方法
关闭数据库链接，返回布尔值。
正常情况下，框架在执行完到最后自动关闭链接，也可以提前手动关闭。
//关闭默认数据链接
$this->db->close()；

//关闭read数据链接
$this->db('read')->close()；

Model数据模型

每个model必须于数据库某个表对应。
model文件必须放置在APP_DIR/model/目录下，文件名与类名一致，区分大小写。

通过继承\system\Model，我们可以使用框架自带的功能便捷操作数据。
例如我们创建APP_DIR/model/Tablemodel.php。
//Tablemodel.php

//命名空间
namespace model;

//类必须继承一个自定义\inherit\Model类或是系统\system\Model类
class Tablemodel extends \inherit\Model
{

    //构造函数必须有
    public function __construct()
    {
    
        //运行上级构造函数
        parent::__construct();
        
        //必须设置一个数据表
        $this->table = 'test';
        
        //必须设置一个主键
        $this->primary = 'id';

        //设置一个表的结构，以便验证过滤
        $this->schema = [
            'id' => [
                'validate' => ['regex' => '/^\d+$/', 'message' => 'ID 不能为空'],
                'literal' => 'ID',
                'default' => null,
                'required' => false
            ],
            'name' => [
                'validate' => ['regex' => '/^\S+$/', 'message' => '名称不能为空'],
                'literal' => '名称',
                'default' => '',
                'required' => true
            ]];
    }

}

//我们可以这样调用model
$this->model('Tablemodel')->one(1);
$this->RDB 属性
设置读数据库，默认为default数据库
$this->RDB='read_database';
$this->WDB 属性
设置写数据库，默认为default数据库
$this->WDB='write_database';
$this->table 属性
设置model对应数据表
$this->table='table1';
$this->primary 属性
设置数据表主键字段
$this->primary='id';
$this->schema 属性
设置数据表结构，以便验证过滤，数组key必须和字段名一致

validate['regex'] 正则验证字段数据合法性


validate['message'] 提示信息


filter 过滤数据，blank|tag|entity 三个值组合使用，

blank把连续多个空白字符转换成一个,
tag过滤html标签,
entity把html标签转换成实体字符。



literal 字段的字面名字


default 字段默认值


required 是否必须填写字段

      $this->schema = [
            'id' => [
                'validate' => ['regex' => '/^\d+$/', 'message' => 'ID 不能为空'],
                'literal' => 'ID',
                'default' => null,
                'required' => false
            ],
            'name' => [
                'validate' => ['regex' => '/^\S+$/', 'message' => '名称不能为空'],
                'filter'=>'blank|tag|entity'
                'literal' => '名称',
                'default' => '',
                'required' => true
            ]
      ];
$this->all($fields) 方法
获取数据表全部数据集,大表谨慎使用。
$recordset=$this->all(['fname1','fname2']);

//注意直接返回数据集，而不是result对象
foreach($recordset as $rs){
    echo $rs->fname1;
}

$this->belongs($model, $relation_model, $relation_foreign_name, $where, $condition) 方法
多对多获取表数据,返回对象数据集

参数说明

$model 需要关联的model名称
$relation_model 关系表model名称
$relation_foreign_name 关联表主键名在关系表中的字段名




$where=['local_relation_filed_name' => 'local_primary_value']

$local_relation_filed_name 本表在关系表字段名
$local_primary_value 本表主键值





$condition 参见$this->select()参数



$this->belongs($model, $relation_model, $relation_foreign_name, $where, $condition);

$this->count($where=[]) 方法
获取数据表数据条数,适合myisam表。
//带条件的计算
$this->count(['field'=>'val']);

//获取表总条数
$this->count();
$this->delete($where=[]) 方法
删除表数据，成功返回true不然返回false。
$rs=$this->delete(['f1'=>'2']);
if($rs){
    //删除成功返回true
    echo $rs;
}
$this->hasMany($model,$where,$condition=[]) 方法
一对多获取副表数据,返回对象数据集。

参数说明

$model 需要关联的model




$where=['foreign_name' => 'local_primary_value']

$foreign 外表字段名
$local_primary_value 本表主键值





$condition 参见$this->select()参数


$this->hasMany($model, $where, $condition);
$this->hasOne($model,$primary_value) 方法
一对一获取数据,返回一行对象数据。

参数说明

$model 关联的model
$primary_value 主键唯一值


$this->hasOne($model, $primary_value);
$this->insert($data=[]) 方法
插入数据，返回布尔值。
$data=['f1'=>'1','f1'=>'2'];

$rs=$this->insert($data);

if($rs){
    //插入成功
}
$this->lastid() 方法
获取最后插入的自增主键ID。
$data=['f1'=>'1','f1'=>'2'];

$rs=$this->insert($data);

if($rs){
    //获取最后插入自增主键
    echo $this->lastid();
}
$this->one($id) 方法
通过主键数字ID或唯一字段获取一条记录。
//如果是主键数字id
$this->one(12);

//如果是唯一字段
$this->one(['uniqname'=>'abc']);
$this->query($sql) 方法
执行通用SQL语句,
如果是select返回基础数据库result对象，
执行update，insert，delete返回布尔值。
$result=$this->query('select * from table1');
$this->select($condition=[]) 方法
按条件获取表数据对象集,参数为空，返回全部数据。
$condition=[
     'where' => ['f1'=>'2'],
     'fields' => ['f1','f2'],
     'orderby' => ['f1'=>'desc','f2'=>'asc'],
     'limit' => [0,20] //或 'limit'=>20
   ];

$this->select($condition);
$this->update($data,$where=[]) 方法
更新数据记录，成功返回true，失败返回false
$data=['f1'=>2,'f2'=>3];
$where=['id'=>12];

$rs=$this->update($data,$where);

if($rs){
    //修改成功，返回影响true
    echo $rs;
}
$this->where($where,$fields=[]) 方法
按条件返回数据对象集,主要为了简化$this->select()
$where=['f1'=>'2'];
$fields=['f1','f2'];
$this->where($where,$fields);
$this->safe 属性
验证过滤入库字段数据，返回system/Safe实例。

具体参见 文档Model数据验证部分

//可以在控制器这样调用
$this->model('Tablemodel')->safe->clear($data);
Model数据验证

Model数据验证，为了用户输入数据的合法性，
提供了几个实用方法函数以配合Model的$this->schema属性使用。

$this->safe->illegalFields 属性

接受返回验证不通过非法字段名数组

$this->safe->notMemberFields 属性

接受返回不在表字段中的非法字段名数组

$this->safe->incompleteFields 属性

接受返回未完成及必须填写的字段名数组

$this->safe->clear($data) 方法
清理不存在于schema里面的字段，
不是成员的字段保存于$this->notMemberFields
返回清理后的数据
//清理之前的数据
$beforedata=[
     //假如nomember不存在于$this->schema中，将被清理掉
    'notmember'=>'val',
    //假如f1字段名存在于$this->schema
    'f1'=>'val'
];

//清理之后
$afterdata=$this->safe->clear($data);

//输出['f1'=>'val']
var_dump($afterdata);

//输出不是成员字段名['notmember']
var_dump($this->notMemberFields);
$this->safe->complete($data) 方法
验证是否缺少必要字段，
缺少的必要字段保存于$this->incompleteFields
返回布尔值
//假如$this->schema包含字段f1,f2必须填写不能为空

//注意$data没有包含字段f2
$data=['f1'=>'val'];

//验证完整性
$result=$this->safe->complete($data);

if($result){
   //通过完整性验证
}else{
    //没有通过验证

    var_dump($this->incompleteFields);
    //输出['f2']
}
$this->safe->merge($data) 方法
与schema默认数据覆盖合并，并且清理不存在于schema里面的字段，
不是成员的字段保存于$this->notMemberFields
返回合并及清理的数据
//假如$this->schema中存在f1字段，默认值等于val1

//注意f1现在的值为val2
$data=['f1'=>'val2','notmember'=>'valxx'];

//合并覆盖数据并清理非成员字段
$result=$this->safe->merge($data);

//输出['f1'=>'val2']
var_dump($result);

//输出不是成员字段名['notmember']
var_dump($this->notMemberFields);

$this->safe->validate($data) 方法
验证数据合法性，非法字段保存于$this->illegalFields
返回布尔值
//假如$this->schema中字段f1必须为int类型

//注意f1是字符串
$data=['f1'=>'val'];

//验证合法性
$result=$this->safe->validate($data);

if($result){
   //通过合法性验证
}else{
    //没有通过验证

    var_dump($this->illegalFields);
    //输出['f1']
}
Controller控制器
创建一个控制器

控制器文件必须放置在APP_DIR/module/module_name目录下，文件名必须和类名一致。


必须继承system/Controller。

创建一个控制器，以APP_DIR/module/www/Testcontroller.php为例。
//命名空间
namespace module\www;

//Testcontroller继承自定的inherit/Controller或system/Controller
class Testcontroller extends \inherit\Controller
{
    public function index()
    {
        $data['hello_world']='hello wolrd';
        
        //调用一个model
        $this->model('Testmodel')->select();
        
        //获取表单全部字段数据
        $postdata=$this->input->post();
        
        //清理不是schema成员字段
        $afterdata=$this->model('Testmodel')->safe->clear($data);
        
        //插入表单提交的数据到数据库
        $this->model('Testmodel')->insert($afterdata);

        //视图输出
        $this->output->view('main',$data);
    }
}
View视图

视图模板文件必须放置APP_DIR/template/module_name/目录下面


模板文件都是标准的原生php与htm混合代码，框架没有专门的模板功能

创建视图模板
例如我们创建一个APP_DIR/template/www/test.php，www为module模块名。
<html>
    <head><title><?php echo $title; ?></title></head>
    <body>
        <h1>
            <?php echo $heading; ?>
        </h1>
        <div><?php echo $content; ?></div>
    </body>
</html>
我们可以在控制器里调用模板。
比如下面代码：
//模板变量
$data=['title'=>'网页标题','heading'=>'小标题','content'=>'内容'];

//只需要填写文件名，支持子目录
$this->output->view('test'，$data);
Helper帮助函数

自定义帮助函数文件必须放置APP_DIR/helper/目录下面


必须以类的形式组织功能函数

自定义helper函数方法
例如我们创建APP_DIR/helper/Testhelper.php
//命名空间
namespace helper;

//我可以继承\system\System,以便使用框架内建属性和函数方法,如果不需要可以忽略
class Testhelper extends \system\System
{
    public function returntex($param){
    
      //只有继承\system\System才能调用此方法
      $this->input->get('str');
      
      return $param;
    }
}
//我们可以在控制器，视图，model里面这样调用
//$this->helper->testhelper->returntex('str');
$this->helper->url($param=[],$path=ENTRY,$anchor='') 方法

此方法框架自带


参数说明

param   查询字符串数组 
path    入口文件路径，默认值 ENTRY常量值 
anchor  锚点


配合常量URL使用，返回被匹配的URL地址字符串
//注意$this->helper->url()参数和数组key的顺序
define('URL', [
    'www' => [ 
    //www表示模块名称
    
        'main/index' => '/',  
        //echo $this->helper->url(['c'=>'main','a'=>'index'])
        //输出 /
        
        'main/lists/type' => '/list/{type}.html',
         //echo $this->helper->url(['c'=>'main','a'=>'lists','type'=>'2'])
         //输出 /list/2.html
        
        'main/lists/type/page' => '/list/{type}_{page}.html'
         //echo $this->helper->url(['c'=>'main','a'=>'lists','type'=>'2','page'=>'34']) 
         //输出 /list/2_34.html
                 
    ]
]);
不匹配URL常量返回
//假如入口文件为index.php 

$this->helper->url();
//输出 /index.php?c=main&a=index

$this->helper->url(['p1'=>'1','p2'=>2]);
//输出 /index.php?c=main&a=index&p1=1&p2=2

//注意a参数和/mod/list.php以及anchor
$this->helper->url(['a'=>'lists','p1'=>'1','p2'=>2],'/mod/list.php','anchor');
//输出 /mod/list.php?c=main&a=lists&p1=1&p2=2#anchor
$this->helper->pager($size, $total, $page, $url, $visible = 5) 方法
框架自带分页方法
//每个页面10条数据
$size=10;

//数据总数
$total=100;

//当前页码
$page=$this->input->get('page',1);

//地址模板
$url='/index/list/<%page%>.html';

//显示多少个页码链接
$visible=5;

//调用pager
$pager=$this->helper->pager($size, $total, $page, $url, $visible);

echo $pager;
//输出HTML
//
//<div class=""pager"">
//<a class=""number"" href=""/index/list/1.html"">1</a>
//<a class=""number "" href=""/index/list/2.html"">2</a>
//<a class=""number "" href=""/index/list/3.html"">3</a>
//<a class=""number "" href=""/index/list/4.html"">4</a>
//<a class=""number "" href=""/index/list/5.html"">5</a>
//<span class=""ellipsis"">...</span>
//<a  class=""number"" href=""/index/list/10"">10</a>
//<a href=""/index/list/2.html"" class=""next"">下一页</a>
//<span class=""info"">共 100 条记录</span>
//</div>
Input输入
$this->input->get() 方法
获取地址查询字符串值，不填参数返回全部数据数组
//如果var_name为null，就返回默认值default_str
$this->input->get('var_name','default_str');

//返回全部数据
$this->input->get();
$this->input->post() 方法
获取表单数据，没有参数返回全部表单字段数组
//字段不存在返回null
$this->input->post('field_name');

//返回全部
$this->input->post();
$this->input->ip() 方法
获取v4 IP地址
//如果没有获取成功返回0.0.0.0
$this->input->ip();
$this->input->isAjax() 方法
判断是否ajax请求，前端必须带HTTP_X_REQUESTED_WITH请求头部
返回布尔值
//$_SERVER['HTTP_X_REQUESTED_WITH'] == 'XMLHttpRequest'
$this->input->isAjax();
$this->input->body() 方法
获取原始请求数据,一般用于API接口
$this->input->body();
$this->input->referer() 方法
获取上一个来源地址url，以便重定向
//如果没有为空
$prev_url=$this->input->referer();

//重定向
$this->output->redirect($prev_url);
$this->input->method() 方法
获取当前请求方法
//返回 POST，GET，OPTION等
$this->input->method();
Output输出
$this->output->compress($string) 方法
删除html多余空白字符
返回处理之后的字符串
$string='<b style=""""    >  str </b><div>   ste  </div>';

$result=$this->output->compress($string);

echo $result;
//输出 <b style="""">str</b><div>ste</div>
$this->output->error($name = 'general', $data =[]) 方法

错误页面模板必须放置在APP_DIR/error/目录下面


参数说明

$name模板文件名，默认模板 genrnal
$data变量数据数组，默认数组 $data=['heading' => 'Error Message', 'message' => 'An error occurred.']


错误页面设置,自动echo内容
//通用错误页面,
$this->output->error();

//自定义错误页面，假如APP_DIR/error/404.php已存在
$this->output->error('404',['title'=>'Not Found']);
$this->output->json($status, $message, $data=[], $return=false) 方法
输出json格式数据

参数说明

$status 设置一个状态码
$message 设置一个状态消息字符串
$data 需要输出的数组数据，默认为空数组
$return 设置布尔值，是否返回内容自定义echo输出，默认自动echo内容


$this->output->json('1002','操作成功',['data'=>'val','data2'=>'val2']);
//输出 {'status':'1002','message':'操作成功','data':{'data1':'val','data2':'val2'}}

//有返回值的自定义输出
$result=$this->output->json('1002','操作成功',['data'=>'val','data2'=>'val2'],true);
echo $result;
$this->output->redirect($uri, $http_response_code=302) 方法
请求重定向

参数说明

$uri 重定向地址
$http_response_code  http头响应码，默认值为302


//转到index.php，默认响应码302
$this->output->redirect('/index.php');

//重定向到404页面
$this->output->redirect('/notfound.php',404);
$this->output->status($http_status_code) 方法
设置响应头
$this->output->status('404');
//如同 header('HTTP/1.1 404 Not Found',true)
$this->output->view($view, $data = [], $return = false, $compress = false) 方法
输出视图,自动echo输出。

参数说明

$view 视图模板文件名称
$data 视图变量数据数组
$return 是否返回内容自动输出，默认值false
$compress 是否压缩HTML，默认值false


//假如已存在APP_DIR/template/www/test.php

//模板数据
$data['var1'=>'val','var2'=>'val'];

//框架自动echo输出，
$this->output->view('test',$data);

//返回自定义echo输出，并压缩html
$result=$this->output->view('test',$data,true,true);
echo $result
Session会话
$this->session->delete($name) 方法

删除一个会话元素，可以同时删除多个,没有返回值。

//设置一个session元素
$this->session->set('name','bob');
$this->session->set('name2','foo');

//删除，
$this->session->delete('name');

//删除多个
$this->session->delete(['name','name2']);
$this->session->destroy() 方法

注销当前会话，返回布尔值。

$this->session->flash($name) 方法

取得某个会话字段的值之后删除此会话字段，字段不存在返回null。

$this->session->get($name) 方法

取得某个会话字段，字段不存在返回null。

$this->session->set($name,$value='') 方法

设置一个会话字段，永远返回true。

$this->session->regenerate() 方法

使用新生成的会话ID更新现有会话ID。

Cookie管理
$this->cookie->delete($name) 方法
删除一个或多个cookie,参数接受一个字符串或一个数组,返回布尔值。
//删除某个cookie
$this->cookie->delete('name');

//删除多个cookie
$this->cookie->delete(['name1','name2','name3']);
$this->cookie->get($name) 方法
获取一个cookie值，如果$name不存在返回null。
$this->cookie->set($name, $value, $expire = COOKIE_EXPIRE, $path = COOKIE_PATH, $domain = COOKIE_DOMAIN, $secure = COOKIE_SECURE, $httponly = COOKIE_HTTPONLY) 方法
设置一个cookie,返回布尔值。
参数说明


$name cookie名字




$value cookie值




$expire 过期时间，如果设置0，关闭浏览器失效，默认值COOKIE_EXPIRE常量




$path 有效路径，默认值COOKIE_PATH常量




$domain 有效域名，默认值COOKIE_DOMAIN常量




$secure 是否https，默认值COOKIE_SECURE常量




$httponly 是否http只读，默认值COOKIE_HTTPONLY常量


$this->cookie->many($data, $expire = COOKIE_EXPIRE, $path = COOKIE_PATH, $domain = COOKIE_DOMAIN, $secure = COOKIE_SECURE, $httponly = COOKIE_HTTPONLY) 方法
设置多个cookie,返回bool值。

参数说明

$data是一个数组，例如$data=['name'=>'val','name2'>'val2']. 
其它参数参考 $this->cookie->set()方法


Lang多语言配置

语言包必须放置APP_DIR/language目录下面。

怎么创建语言包
我们以en_us和zh_cn为例：
创建英文APP_DIR/language/en_us/test.php
<?php
return ['test'=>'test','good'=>'very good'];
创建中文APP_DIR/language/zh_cn/test.php
<?php
return ['test'=>'测试','good'=>'非常好'];
调用语言包
echo $this->lang('zh_cn')->test['good'];
//输出 非常好

echo $this->lang('en_us')->test['good'];
//输出 very good

//如果设置了LANG常量为zh_cn，我们可以这样调用
echo $this->lang->test['good'];
// 输出 非常好
Redis缓存

必须安装redis扩展才能使用

配置redis服务器
放置配置文件redis.php到APP_DIR/config/development(test 或 production)目录下。
文件代码内容：
<?php
return [
    //默认
    'default' => [
        'host' => 'set.redis.to.hosts.file', 
        'port' => 6379, 
        'password' => '', 
        'database' => 0, 
        'timeout' => 30, 
        'serialization' => true //是否自动序列化
        ],
    //队列服务器
    'queue' => [
        'host' => 'set.redis.to.hosts.file', 
        'port' => 6379, 
        'password' => '', 
        'database' => 0, 
        'timeout' => 30, 
        'serialization' => true //是否自动序列化
        ]
];
调用redis服务
所有方法函数和属性均继承原生redis模块
//如果设置了default服务器，我们可以这样调用
$this->redis->get('key');

//调用一个自定义redis服务
$this->redis('queue')->get('key');
Security安全

此类提供了一些常用的安全方法函数

$this->security->blank($str) 方法
把多个空白字符转换成一个空白字符,已被model验证引用。
$str='  dd    dd        d  ';
echo $this->security->blank($str);
// 输出 ' dd dd d '
$this->security->entity($str) 方法
把html标签转换成实体字符,已被model验证引用。

如同php内建函数 htmlspecialchars($str, ENT_QUOTES | ENT_HTML401, CHARSET);

$this->security->tag($str) 方法
清理html标签,已被model验证引用。

如同php内建函数strip_tags($str)

$this->security->token() 方法
生成一个表单token值，返回字符串。
<input name=""_tokenname_"" value=""<?php echo $this->security->token() ?>"" />
$this->security->tokenName() 方法
生成一个表单token名字，返回字符串。
<input name=""<?php echo $this->security->tokenName() ?>"" value=""<?php echo $this->security->token() ?>"" />
$this->security->checkToken() 方法
验证一个被提交上来的token是否有效,返回布尔值。
if($this->security->checkToken()){

    //token 有效

}
Upload上传文件
类方法及属性说明
//设置保存目录
$upload->dir='/www/upload';

//设置被允许的文件类型数组
$upload->extension=['jpg','gif'];

//设置接受表单数据字段
$upload->data=$_FILES['filedata'];

//自定文件名，如果不填写，将自动设置。
$upload->filename='filename';

//设置上传文件夹权限码，默认0777
$upload->mode='0777';

//必要属性设置完毕，执行上传处理，返回布尔值。
$upload->execute();

//执行完结果返回状态码
$upload->code;
// 详细请查看 $upload::ERROR_MSG

//执行完结果返回消息提示
$upload->message;
// 详细请查看 $upload::ERROR_MSG
代码例子
//实例上传类
$upload = new \system\library\Upload();

//设置被允许的文件扩展
$upload->extension = ['jpg', 'gif', 'png', 'jpge'];

//设置上传保存目录
$upload->dir = $_SERVER['DOCUMENT_ROOT'] . '/upload/' .date('y/nd/');

//接受表单数据字段
$upload->data = $_FILES['filedata'];

//自定义生成一个文件名
$upload->filename = uniqid();

//执行上传操作
if ($upload->execute()) {
    echo $upload->code; // 输出 '0'
    echo $upload->message; //输出 '上传成功'
    echo $upload->filename; //输出完成的文件名+最终扩展名
    return;
}

Html标签生成
生成html标签
简单代码例子
//参数数组
$param=[
    [
        //标签名p
        'name'='p',
        
        //标签的属性
        'properties'=>['name'=>'ptag'],
        
        //子标签元素
        'elements'=>[
            [
                'name'='i',
                'properties'=>['name'=>'itag']
            ]
    ],
    [   //标签div
        'name'='div',
        
        //标签属性
        'properties'=>['name'=>'pdiv'],
    ]
];

//创建类实例
$html= new \System\Library\Html();

//生成标签
echo $html->tags($param);
//输出 
//<p name=""ptag"">
//    <i name=""itag""></i>
//</p>
//<div name=""pdiv""></div>
Grid表格生成
待优化
$grid->fields
$grid->setField()
$grid->filters
$grid->filters()
$grid->setFilter()
$this->tools
$this->tools()
$grid->setTool()
$grid->table()
Image图片修饰
对图片进行加水印和缩放操作
简单代码例子
//创建实例
$image= new \System\Library\Image();

//设置图片宽度
$image->width=300;

//设置图片高度
$image->height=300;

//源文件路径
$image->source='G:\fw.png';

//处理之后保存路径
$image->save='G:\fw300.png';

//水印字体大小
$image->fontsize=20;

//字体文件路径
$image->font=APP_DIR.'font/1.ttf';

//图片质量
$image->quality=90;

//文本水印
$image->text='text';

//是否开启水印功能，默认false
$image->watermark=true;

//水印文件地址，图片优先
$image->markimg='G:\water.png';

//处理图片，返回布尔值
if($image->resize()){

    //处理结果状态码
    $image->code;
    
    //处理结果返回信息
    $image->message;
}

//具体状态码和状态信息请查看 $image::MAG 常量
Smtp发送邮件
SMTP邮件发送, 支持发送纯文本邮件和HTML格式的邮件
代码例子
//设置smtp服务器
$mail = new \system\library\Smtp([
    'server'=>'smtp.qq.com',
    'username' => ""name"",
    'password' => ""123456"",
    'port' => 25
  ]); 

//设置发件人
$mail->from(""XXXXX""); 

//设置发件人名字
$mail->fromname(""XXXXX""); 

//设置收件人，多个收件人，调用多次
$mail->to(""XXXXX""); 

//设置抄送，多个抄送，调用多次
$mail->cc(""XXXX""); 

//设置秘密抄送，多个秘密抄送，调用多次
$mail->bcc(""XXXXX""); 

//设置邮件主题
$mail->subject(""test""); 

//设置邮件内容
$mail->body(""<b>test</b>"");

//发送邮件，返回布尔值
if($mail->send()){
    //发送成功
}else{
    //发送失败，返回错误信息
    $mail->error();
}
Captcha验证码生成
$captcha->bgcolor 属性
设置验证码背景16进制颜色
$captcha->bgcolor='#333333';
$captcha->showBorder 属性
设置是否显示边框，布尔值
$captcha->showBorder=true;
$captcha->borderColor 属性
设置验证码边框16进制颜色
$captcha->borderColor='#cccccc';
$captcha->charLen 属性
设置验证码的位数
$captcha->charLen=4;
$captcha->fontPath 属性
设置字体放置目录,绝对路径，默认目录APP_DIR/font/
$captcha->fontPath=APP_DIR.'/font/';
$captcha->width 属性
设置图片宽度，单位像素
$captcha->height 属性
设置图片高度，单位像素
$captcha->getCode() 方法
获取生成的文字，以便保存到session
$captcha->create() 方法
生成渲染图像
$captcha->show() 方法
发送显示到浏览器
简单代码例子
//假如地址 index.php?c=captcha&a=showimg

$authcode = new \system\library\Captcha();
$authcode->create();
$this->session->set('authcode', $authcode->getCode());
$authcode->show();

//html img标签显示 <img src=""index.php?c=captcha&a=showimg"" />
应用程序目录布局说明

application目录带*号的目录必须设置

|--system 系统框架程序目录 
└--application 应用程序目录  
    |--helper   自定义工具帮助库*  
    |--library  自定义公共类库  
    |--language 语言包目录*
    |--font     字体目录*
    |--document 开发文档目录  
    |--module   控制器模块*
    |    └--www  应用模块名  
    |       └--main.php 具体controller业务逻辑文件  
    |--model model文件目录*
    |   └--test.php 具体数据model 
    |--template  视图模板文件*
    |   └--www  模块名   
    |       └--main.php 具体模板文件  
    |--config 配置文件目录*
    |   |--development 开发环境配置  
    |   |   |--database.php 数据库配置文件  
    |   |   |--constans.php 常量配置文件  
    |   |   └--redis.php redis配置文件  
    |   |--test 开发环境配置  
    |   |   |--database.php   
    |   |   |--constans.php   
    |   |   └--redis.php   
    |   └--production 产品环境配置  
    |       |--database.php   
    |       |--constans.php   
    |       └--redis.php   
    |--inherit model和controller重写继承目录*
    |   |--controller.php   
    |   └--model.php   
    └--public  应用程序入口目录  
        |--static 静态文件资源  
        └--www  此目录绑域名用  
            └--index.php 入口文件  

",19
getk2/k2,PHP,"

You've already been there... Joomla is a great content management system. In fact it's considered one of the best in the world. But the default article system in Joomla is both spartan and confusing to configure and template in newer versions! In Joomla 1.5 it was just a title and your content body. In Joomla 2.5 article images where introduced as separate fields (but without any auto-resizing) and in Joomla 3.x tags where introduced as a separate component. Have you seen the options to configure all these?
This is where K2 comes in.
K2 was built as a complete replacement of the default article system in Joomla. Install it like any Joomla extension, import your articles from the default Joomla article system and you instantly get a host of new features for your existing content: rich content forms for items (think of Joomla articles with additional fields for article images, videos, podcasts & other audio files, image galleries and attachments), hassle-free image management (uploaded item images are auto-resized to 6 configurable dimensions, either globally or per category - you can now forget about using Photoshop resizing!), comments, tagging, built-in options to extend content forms (e.g. to create product catalogs), powerful content modules fetching K2 content in any way you can imagine, frontend editing with easy to use access control settings (for content-heavy websites), powerful yet easy templating (and sub-templating) for going above the ""Joomla average"", extended user profiles, user groups, blogs, a powerful plugin API to extend item/category/user forms, ""drag and drop"" media manager and many more!
K2 is the ideal solution for managing your content, regardless of site ""size"": you can use it from a small blog to a complex corporate site or even a multi-author environment (portals, magazines etc.). To provide a practical example, using K2, you can transform your Joomla website to a news/magazine site with author blogs, product catalogs, work portfolio, knowledge base, download/document manager, directory listing, event listing and more, all this bundled under one package! And since K2 is extensible with additional fields to its base item form, you can easily create category-specific content types, e.g. article, blog post, product page, directory listing.
It's no wonder that K2 powers some of the biggest and most popular Joomla sites ever built worldwide!
These integrated features in K2 not only save website administrators precious management time (from managing a dozen extensions which would otherwise be required), but they also allow for better performance.
K2 was actually built on these 4 principles: feature-rich content in Joomla, ease of use (for any type of user), flexible templating, performance
And best of all? K2 is totally free to use!
Some facts about K2

Actively powers more than 300,000 websites worldwide (metrics are gathered since v2.7.0).
It has been downloaded more than 3 million times since March 2009.
Almost all template clubs provide K2 specific styling and display K2 as part of their demo sites.
There are hundreds of extensions supporting or integrating K2 in the Joomla Extensions Directory - see: https://extensions.joomla.org/search?q=k2
The Joomla Magazine and JoomlaGov.info (the directory for government websites built with Joomla) are powered by K2
K2 is used in some of the top Joomla websites worldwide by organizations like the Harvard University, The National Institute of Technology in Brazil, the UK's NHS, Top Gear, Groupama, Amnesty International, ActionAid, The High Court of Australia, Arturia, Cyrus Audio and many, many more.
K2 is co-designed and co-developed by JoomlaWorks (established in 2006) and Nuevvo (established in 2010), both award winning & acclaimed Joomla-centric companies.
K2 is compatible with Joomla 1.5, 2.5 and the latest 3.x releases.

Downloading K2
You can find the latest stable release on: https://getk2.org
If you are looking for the latest developer build (a snapshot from this Github repository) just use Github's ""Download ZIP"" button.
Resources
The K2 Community is the ultimate destination for everything K2! From discussions in the Community Forum to Extensions & Templates in the ""K2 Extensions Directory"", as well as a wealth of video tutorials, documentation, tips & a showcase of Joomla sites built with K2.
More at: https://getk2.org

Copyright © 2006-2018 JoomlaWorks Ltd. & Nuevvo Webware P.C.
",113
andrewrothstein/ansible-vagrant,None,"andrewrothstein.vagrant

Installs vagrant
Requirements
See meta/main.yml
Role Variables
See defaults/main.yml
Dependencies
See meta/main.yml
Example Playbook
-  hosts: servers
   roles:
     - andrewrothstein.vagrant
License
MIT
Author Information
Andrew Rothstein andrew.rothstein@gmail.com
",2
im-not-a-programmer/admin,Vue,"后台管理系统骨架
安装node_modules
npm install

开发环境
npm start

发布
npm run build

测试环境
npm run test

",2
ziangzhang10/data_bootcamp_final_project,Jupyter Notebook,"data_bootcamp_final_project
Satellite Data + Machine Learning: Final Project for the Rice Data Analytics Bootcamp.
",2
JohnnySn0w/MoD,JavaScript,"

MoD: MUD on Discord 
A wot?
For the uninitiated, a MUD is a Multi-User Dungeon. Think text-based adventure game, but as an online multiplayer game
And you stuck that on Discord?
Yeah, and man, rate limited messages per channel is a pain.
So yeah, the bot can take hand-crafted game modules and run a MUD with them. A skeleton on which to drape the flesh of world-building.
Ok. How do I run it?
See the Setup section of the wiki!

Features thus far:
Basic demo dungeon

This is the basic framework for a dungeon, and serves as a short example of what can be done at a simple level. It can easily be replaced with whatever your actual world is.

Administrative commands

db – allows for simple queries to the database, as well as functionality for on-the-fly creation of game elements
notbusy - a short-term fix for a higher level problem, fixes a player's busy status if the bot gets interrupted. Will be removed in the future when the true solution is implemented.
test – in-house manufactured unit testing

Player commands

Look – returns a description of items, rooms, npcs, enemies,
Move – change rooms in the mud
Attack – specify a target, and get whackin'
Stats – get a pm of current player stats
Talk – chat it up with an npc
Discard/Equip - discard or equip an item, respectively
Help - display available commands and syntax
Inventory - get a pm of your inventory

Prospective future features:

Automated generation of channels for game rooms from database entries
Custom player descriptions for RP
Change nickname for RP
Player housing

",2
HarryShomer/Hockey-Scraper,Python,"



Hockey-Scraper

Purpose
This package is designed to allow people to scrape both NHL and NWHL data. For the NHL, one can scrape the Play by Play
and Shift data off of the National Hockey League (NHL) API and website for all preseason, regular season, and playoff
games since the 2007-2008 season. For the NWHL, one is able to scrape the Play by Play data off of their API and website
for all preseason, regular season, and playoff games since the 2015-2016 season.

Prerequisites
You are going to need to have python installed for this. This should work for both python 2.7 and 3 (I recommend having
from at least version 3.6.0 but earlier versions should be fine).
If you don’t have python installed on your machine, I’d recommend installing it through the anaconda distribution. Anaconda comes with a bunch of libraries pre-installed so it’s easier to start off.

Installation
To install all you need to do is open up your terminal and type in:
pip install hockey_scraper


NHL Usage

Standard Scrape Functions
Scrape data on a season by season level:
import hockey_scraper

# Scrapes the 2015 & 2016 season with shifts and stores the data in a Csv file
hockey_scraper.scrape_seasons([2015, 2016], True)

# Scrapes the 2008 season without shifts and returns a dictionary containing the pbp Pandas DataFrame
scraped_data = hockey_scraper.scrape_seasons([2008], False, data_format='Pandas')

Scrape a list of games:
import hockey_scraper

# Scrapes the first game of 2014, 2015, and 2016 seasons with shifts and stores the data in a Csv file
hockey_scraper.scrape_games([2014020001, 2015020001, 2016020001], True)

# Scrapes the first game of 2007, 2008, and 2009 seasons with shifts and returns a Dictionary with the Pandas DataFrames
scraped_data = hockey_scraper.scrape_games([2007020001, 2008020001, 2009020001], True, data_format='Pandas')

Scrape all games in a given date range:
import hockey_scraper

# Scrapes all games between 2016-10-10 and 2016-10-20 without shifts and stores the data in a Csv file
hockey_scraper.scrape_date_range('2016-10-10', '2016-10-20', False)

# Scrapes all games between 2015-1-1 and 2015-1-15 without shifts and returns a Dictionary with the pbp Pandas DataFrame
scraped_data = hockey_scraper.scrape_date_range('2015-1-1', '2015-1-15', False, data_format='Pandas')

The dictionary returned by setting the default argument ""data_format"" equal to ""Pandas"" is structured like:
{
  # Both of these are always included
  'pbp': pbp_df,
  'errors': scraping_errors,

  # This is only included when the argument 'if_scrape_shifts' is set equal to True
  'shifts': shifts_df
}

Scraped files can also be saved in a separate directory if wanted. This allows one to re-scrape games quicker as we
don't need to retrieve them. This is done by specifying the keyword argument 'docs_dir' equal to True to automatically
create, store, and look in the home directory. Or you can provide your own directory where you want everything to be
stored (it must exist beforehand).
import hockey_scraper

# Create or try to refer to a directory in the home repository
# Will create a directory called 'hockey_scraper_data' in the home directory (if it doesn't exist)
hockey_scraper.scrape_seasons([2015, 2016], True, docs_dir=True)

# Path to the given directory
USER_PATH = ""/....""

# Scrapes the 2015 & 2016 season with shifts and stores the data in a Csv file
# Also includes a path for an existing directory for the scraped files to be placed in or retrieved from.
hockey_scraper.scrape_seasons([2015, 2016], True, docs_dir=USER_PATH)

# Once could chose to re-scrape previously saved files by making the keyword argument rescrape=True
hockey_scraper.scrape_seasons([2015, 2016], True, docs_dir=USER_PATH, rescrape=True)


Live Scraping
Here is a simple example of a way to setup live scraping. I strongly suggest checking out
this section of the docs if you plan on using this.
import hockey_scraper as hs


def to_csv(game):
    """"""
    Store each game DataFrame in a file

    :param game: LiveGame object

    :return: None
    """"""

    # If the game:
    # 1. Started - We recorded at least one event
    # 2. Not in Intermission
    # 3. Not Over
    if game.is_ongoing():
        # Get both DataFrames
        pbp_df = game.get_pbp()
        shifts_df = game.get_shifts()

        # Print the description of the last event
        print(game.game_id, ""->"", pbp_df.iloc[-1]['Description'])

        # Store in CSV files
        pbp_df.to_csv(f""../hockey_scraper_data/{game.game_id}_pbp.csv"", sep=',')
        shifts_df.to_csv(f""../hockey_scraper_data/{game.game_id}_shifts.csv"", sep=',')

if __name__ == ""__main__"":
    # B4 we start set the directory to store the files
    # You don't have to do this but I recommend it
    hs.live_scrape.set_docs_dir(""../hockey_scraper_data"")

    # Scrape the info for all the games on 2018-11-15
    games = hs.ScrapeLiveGames(""2018-11-15"", if_scrape_shifts=True, pause=20)

    # While all the games aren't finished
    while not games.finished():
        # Update for all the games currently being played
        games.update_live_games(sleep_next=True)

        # Go through every LiveGame object and apply some function
        # You can of course do whatever you want here.
        for game in games.live_games:
            to_csv(game)


NWHL Usage
Scrape data on a season by season level:
import hockey_scraper

# Scrapes the 2015 & 2016 season and stores the data in a Csv file
hockey_scraper.nwhl.scrape_seasons([2015, 2016])

# Scrapes the 2008 season and returns a Pandas DataFrame containing the pbp
scraped_data = hockey_scraper.nwhl.scrape_seasons([2017], data_format='Pandas')

Scrape a list of games:
import hockey_scraper

# Scrape some games and store the results in a Csv file
# Also saves the scraped pages
hockey_scraper.nwhl.scrape_games([14694271, 14814946, 14689491], docs_dir=""...Path you specified"")

Scrape all games in a given date range:
import hockey_scraper

# Scrapes all games between 2016-10-10 and 2017-01-01 and returns a Pandas DataFrame containing the pbp
hockey_scraper.nwhl.scrape_date_range('2016-10-10', '2017-01-01', data_format='pandas')

The full documentation can be found here.

Contact
Please contact me for any issues or suggestions. For any bugs or anything related to the code please open an issue.
Otherwise you can email me at Harryshomer@gmail.com.
",58
voken100g/AutoSSR,None,"Free ShadowsocksR免费 ShadowsocksR 服务
Latest update at: Sun May 12 02:18:48 UTC 2019

Introduction (English)
中文说明

20 stable servers:



ID
ASN
Server Country
Online




#.C51C
AS57494
Russia
18 hrs


#.3769
AS46844
United States
29 hrs


#.F90F
AS57494
Russia
166 hrs


#.3FE9
AS36352
United States
70 hrs


#.5B8A
AS36352
United States
12 hrs


#.ED26
AS203380
United Kingdom
14 hrs


#.973A
AS63949
Japan
18 hrs


#.BE3E
AS63949
United States
29 hrs


#.207F
AS63949
United States
29 hrs


#.B147
AS63949
Singapore
13 hrs


#.442C
AS40676
United States
21 hrs


#.B3BB
AS8075
United States
12 hrs


#.D254
AS40676
United States
12 hrs


#.F3D9
AS36352
United States
46 hrs


#.EA20

Japan
32 hrs


#.3008
AS56630
Italy
12 hrs


#.1A68
AS57494
Russia
12 hrs


#.9ED9
AS36352
United States
113 hrs


#.07A9
AS36352
United States
32 hrs


#.CF05
AS44220
Romania
60 hrs



39 online servers:



ID
ASN
Server Country
Online




#.C51C
AS57494
Russia
18 hrs


#.3769
AS46844
United States
29 hrs


#.A8BA
AS54600
China
1 hrs


#.F90F
AS57494
Russia
166 hrs


#.3FE9
AS36352
United States
70 hrs


#.5B8A
AS36352
United States
12 hrs


#.ED26
AS203380
United Kingdom
14 hrs


#.2DE5
AS57494
Russia
2 hrs


#.5607
AS63949
Japan
10 hrs


#.973A
AS63949
Japan
18 hrs


#.BE3E
AS63949
United States
29 hrs


#.207F
AS63949
United States
29 hrs


#.B147
AS63949
Singapore
13 hrs


#.418E
AS16509
Singapore
3 hrs


#.442C
AS40676
United States
21 hrs


#.B3BB
AS8075
United States
12 hrs


#.DB95
AS40676
United States
9 hrs


#.ECD6
AS40676
United States
10 hrs


#.D254
AS40676
United States
12 hrs


#.4C14
AS16509
Japan
1 hrs


#.8988
AS16509
Singapore
10 hrs


#.D833
AS16509
Japan
1 hrs


#.6F49
AS63949
Japan
9 hrs


#.F80F
AS51659
Russia
1 hrs


#.4AEC
AS31798
Canada
8 hrs


#.F3D9
AS36352
United States
46 hrs


#.B6E3
AS31798
Canada
10 hrs


#.DF1F
AS16509
South Korea
10 hrs


#.A07C
AS16509
South Korea
1 hrs


#.EA20

Japan
32 hrs


#.3008
AS56630
Italy
12 hrs


#.EE8F
AS40676
India
10 hrs


#.1A68
AS57494
Russia
12 hrs


#.186F
AS56630
Russia
8 hrs


#.24E2
AS63949
Japan
7 hrs


#.9ED9
AS36352
United States
113 hrs


#.07A9
AS36352
United States
32 hrs


#.03E7
AS56322
Hungary
4 hrs


#.CF05
AS44220
Romania
60 hrs



46 recent servers:



ID
ASN
Server Country
Online




#.C51C
AS57494
Russia
18 hrs


#.3769
AS46844
United States
29 hrs


#.A8BA
AS54600
China
1 hrs


#.F90F
AS57494
Russia
166 hrs


#.3FE9
AS36352
United States
70 hrs


#.5B8A
AS36352
United States
12 hrs


#.ED26
AS203380
United Kingdom
14 hrs


#.1C65
AS57494
Russia
1 hrs offline


#.2DE5
AS57494
Russia
2 hrs


#.E1D5
AS57494
Russia
1 hrs offline


#.5607
AS63949
Japan
10 hrs


#.D1A6
AS35916
United States
1 hrs offline


#.973A
AS63949
Japan
18 hrs


#.BE3E
AS63949
United States
29 hrs


#.207F
AS63949
United States
29 hrs


#.B147
AS63949
Singapore
13 hrs


#.418E
AS16509
Singapore
3 hrs


#.442C
AS40676
United States
21 hrs


#.B3BB
AS8075
United States
12 hrs


#.DB95
AS40676
United States
9 hrs


#.ECD6
AS40676
United States
10 hrs


#.D254
AS40676
United States
12 hrs


#.17C3
AS137571
China
1 hrs offline


#.4C14
AS16509
Japan
1 hrs


#.8988
AS16509
Singapore
10 hrs


#.D833
AS16509
Japan
1 hrs


#.6F49
AS63949
Japan
9 hrs


#.F80F
AS51659
Russia
1 hrs


#.E21E
AS16509
Singapore
1 hrs offline


#.4AEC
AS31798
Canada
8 hrs


#.F3D9
AS36352
United States
46 hrs


#.B6E3
AS31798
Canada
10 hrs


#.DF1F
AS16509
South Korea
10 hrs


#.A07C
AS16509
South Korea
1 hrs


#.EA20

Japan
32 hrs


#.3008
AS56630
Italy
12 hrs


#.EE8F
AS40676
India
10 hrs


#.EA15
AS40676
United States
1 hrs offline


#.1A68
AS57494
Russia
12 hrs


#.186F
AS56630
Russia
8 hrs


#.CC09
AS4760
Hong Kong
6 hrs offline


#.24E2
AS63949
Japan
7 hrs


#.9ED9
AS36352
United States
113 hrs


#.07A9
AS36352
United States
32 hrs


#.03E7
AS56322
Hungary
4 hrs


#.CF05
AS44220
Romania
60 hrs



",27
guyellis/plant,JavaScript,"plant
Running live at Plaaant
To get a feel a well chronicled orchard of fruit trees visit
Guy Ellis' Orchard
and use the ""Filter"" input near the top of the page to find fruit trees you might be interested
in learning about.
Plant is a website app to help you manage growing trees and plant.





Project Objectives

Versions
Questions


For Developers

Architecture Notes
Developer Setup



Objectives

Allow users to track the plants that they're growing.

Provide stats on growth rates.
Pull in weather information.
Compare against other growers.


Allow users to research plants to grow.

Users can search within a radius of their location for plants others are
growing.
Users can determine likelihood of success.


Assist users in management of their plants.

Send alerts to users when action needs to be taken based on time-of-year
or on weather. For example, should prune/fertilize on X Date, or ""there's
a freeze warning, you need to protect your avocado tree.""


Usability

Users should be able to post entries about a tree from their yard on a smart
phone.
Users should be able to take photos and add them to a plant entry from
their smart phone while making a post.
Users should be able to operate the app while disconnected with syncing
done later.



Versions
0.0.1 MVP

User can create an account using OAuth from their Google account.
User can add/delete/update each plant in their yard.
User can add entries to each plant. Each entry will have a date and
details.

0.0.2 and beyond

Add features/fields to user's account.

Add Facebook as OAuth option.
Allow user to add their GPS location to their account.


Add features to a plant post.

Allow photos to be added.
Add markdown for details to allow for formatting.


Add structured fields for each plant.

Dates for planting, germination.
Multi-bud - treat each scion as its own tree but group on this.
Perennial/annual (other?)


Add structured fields for data entry posts.

E.g. height, width etc. to calculate growth rates.
Applications of fertilizer, mulch etc for reporting on growth effectiveness.
Start and end dates for harvest. (Allow for reporting on year round
production and prediction.)
Volume harvested.


Perform calculations on entries to show changes. e.g. growth in gallons per
week.
Use user's GPS location to pull in weather data from date of first plant
entry to current date.
Keep user's weather data up-to-date.
Use user's GPS location to allow them to compare their plants to the same
plants in nearby locations.
Allow users to add parentage/genealogy to their plants.

i.e. if another user gave them a plant or they planted another plant from
one that they're already documenting on the system then they can link a parent
plant.


Support multi-bud trees.

Allow for structured data entry for root stock(s) and multiple scions.
Allow dates for adding scions to trees.


Allow for termination date of plants.

Distinguish between perennial and annuals. i.e. plants that are designed
to die annually and those that aren't. Helps in reporting life and reason
for death or disposal of perennial.


Search

User can find a male pollinator for their female plant within a certain
radius of their location.



Questions
If you have questions or want to communicate with the maintainers of the
project then please create an issue.
For Developers
Architecture
Components
plant
Components in the /app/components/plant folder.
Component naming uses CRUD (Create, Read, Update, Delete) names to identify what the component does.

Plant

Container for showing and managing a single plant
All other components in this section are used by the Plant component directly or indirectly. i.e. Plant is the top parent


PlantRead

Shows the details of the Plant
PlantEdit hidden


PlantEdit

Create or Update a Plant.
PlantRead hidden


Notes

Manages the Hide/Show ""add note"" functionality
Container to show a list of Notes


NoteEdit

Create or Update a Note.


NoteRead

Shows the details of a Note



plants
Components in the /app/components/plants folder.
Components for managing a collection and the listing of plants.
Developer Setup
Facebook and Google OAuth
You can start the site without setting up Facebook credentials.
As long as the NODE_ENV is not set to production you will be able to login as a dev user.
You do, however, need to set the Facebook and Google environment variables to a non-empty value.
In your ~/.bashrc or equivalent file (or a script you source before you start the server) set the following:
export PLANT_FB_ID=<facebook-app-id>
export PLANT_FB_SECRET=<facebook-app-secret>
export PLANT_GOOGLE_ID=<google-app-id>
export PLANT_GOOGLE_SECRET=<google-app-secret>

If want to use Facebook OAuth then you'll need to setup credentials.
(As with any site, the layout and options change over time so these instructions are an approximation.)

Go to developers.facebook.com and on the menus click on My Apps and then select Add a New App.
Select a WWW Website.
Add a name (Plant is good) and click Create New Facebook App ID.
There's a button to the top right to Skip Quickstart - hit that.
You should end up on the Dashboard. From here you want the App ID and App Secret.
Set the PLANT_FB_ID and PLANT_FB_SECRET environment variables to these values.

MongoDB
If you have Docker installed then it will pull down and spin-up MongoDB for you when you start the server.
Otherwise, you need to install Docker (recommended) or MongoDB.
Running the site

Clone the repo locally.
npm i
Terminal Window #1: npm run server

Starts the server on port 3001 using the /devops/run-server.sh script. Edit this script to fine tune how the server starts.


Terminal Window #2: npm start

Starts the Webpack Dev Server on port 9090


Navigate: http://localhost:9090

Running the tests
npm t

Debugging with VSCode

Start MongoDB
Terminal #1: Start the App/API Server with npm run server
Terminal #2: Start the Dev Server with npm start
VSCode: Select Chrome from Debug launch options and hit F5

",6
xndcn/smzdm.com,None,"smzdm.com
",10
narcitymedia/lilium-cms,JavaScript,"
Lilium CMS V4
Lilium is a lightning-fast, web based content management system built with Node.JS. Its power resides in its intelligent caching engine which reduces CPU usage, RAM, and database access.
If server-side rendering is preferred over client-side rendering, Lilium offers a simplified way of generating HTML documents, storing them on disk or RAM, and serving them faster than most HTML preprocessors. It is possible to use either LML2 which ressembles PHP, or LML3 which is an easy to use routine using Javascript template strings.
The platform has its own framework and unique API. Lilium makes it easy to create a mobile app for a website, and is network based. It can hold multiple domains for a single instance, can use multiple different databases. It is compatible with content delivery network services.
Lilium does not use Express, Mongoose, or other heavy libraries. Instead, it implements its own web server using native NodeJS libraries.
Open source details
Narcity Media is using Lilium CMS in production. However, it is currently using the V3. That means this version is not ready for production just yet. We still invite you to try the CMS and have fun with it, but we recommend to wait until the V4 is stable before deploying a website.
Installation guide
All NodeJS packages are to be installed, and are documented in the package file. You can simply run npm run setupdev in the Lilium root folder.
Automated installation
Lilium was built on Linux, and is meant to be run on Linux. Even though it will technically work on Mac, Lilium is not guaranteed to be stable on other operation systems.
The CMS requires NodeJS v8+, and MongoDB v3+. You can follow online tutorials on how to install both of these before running the npm scripts, but it's a faily simple process.
Installing NodeJS
You can follow this tutorial from the official NodeJS website.
From the nodesource instructions
# Using Ubuntu
curl -sL https://deb.nodesource.com/setup_11.x | sudo -E bash -
sudo apt-get install -y nodejs

# Using Debian, as root
curl -sL https://deb.nodesource.com/setup_11.x | bash -
apt-get install -y nodejs

Installing MongoDB
You can follow this tutorial from the official MongoDB website.
Only run one of the following depending on your Ubuntu distro
# For Ubuntu 18
echo ""deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.0 multiverse"" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.0.list

# For Ubuntu 16
echo ""deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/4.0 multiverse"" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.0.list

# For Ubuntu 14
echo ""deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/4.0 multiverse"" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.0.list

Then
sudo apt-get update
sudo apt-get install -y mongodb-org
sudo service mongod start

And MongoDB should be running. To make sure the installation was successful, you can open a shell using mongo.
Installing Lilium for development
You can clone Lilium in your favourite installation directory. At Narcity Media, we like to use /usr/share/lilium or ~/dev/lilium. Make sure the directory is owned by you, not by root. Also make sure to install it in a directory that is also owned by you since other sibling directories might be needed.
Simply cd to the directory you want to use, and git clone https://github.com/narcitymedia/lilium-cms.
Then, cd lilium-cms and npm install && npm run setupdev.
Once the installation process exits, you can start the CMS using npm start or node index.prod.js. Your browser should load the CMS if you browse to localhost:8080/lilium.
The development username and password are : lilium and lilium. Make sure to change them if you plan on deploying on a production machine.
Web panel for development
The CMS frontend is located under /apps/lilium. It is a Preact app, and is transpiled using Webpack and Babel.
Required dependencies
MAC:
brew install pkg-config cairo libpng jpeg giflib imagemagick redis
UBUNTU:
sudo apt-get install libcairo2-dev libjpeg8-dev libpango1.0-dev libgif-dev build-essential g++ libkrb5-dev imagemagick redis-server
The dependencies will be installed automatically during the npm run setupdev process. You can however install then manually if you prefer.
Localhost connections to MongoDB
If mongo still refuses to connect from NodeJS eventhough it works using the terminal cli, you might have to enable or disable authentication from your mongo config file (typically located in /etc/mongod.conf, under security: authentication).
This information can be found easily online using search queries such as ""Enable MongoDB authentication"". We ran into this issue on multiple occasion, and it ended up being a different solution every time.
MongoDB with brew
On Mac, sometimes the MongoDB service will refuse connections from Lilium for obscure reasons. Our temporary solution it to start a mongod instance in a seperate terminal and add the desired parameters including the database path and authentation db. You also get an additional output stream from mongod.
There likely is a better solution to make this work with brew service, but like mentionned earlier, we don't actively support MacOS nor do we recommend to run Lilium in production on a different OS than Linux.
I want to code, too
That must mean you're amazing. See the Lilium CMS Wiki and get started!
Script mode
It is possible to run a Javascript file in script mode. It will prevent Lilium from loading the listeners, CAIJ and other modules related to networking. The websites and databases will still be loaded on a single thread, and the script passed as an argument will be executed.
node ./includes/runscript.js [script.js]
Random quote API
In older version, we used a random quote provider to add a cute message to The Daily Lilium, which is Lilium's newspaper.
http://api.forismatic.com/api/1.0/?method=getQuote&format=json&lang=en
Working with nginx
Nginx works well with Lilium. The following configuration is the most simplified version you can use. The config works with a Lilium instance located at /usr/share/lilium/lilium-cms, and runs at port 8080.
Since Lilium does not handle HTTPS requests, using nginx in front of Lilium makes it possible to have a fully operational SSL website running on Lilium, without the SSL overhead during the local proxy upstream.
Using Certbot or LetsEncrypt, you can quicky generate an HTTPS cert.
Basic nginx configuration file
upstream lilium_proxy  {
        server 127.0.0.1:8080;
        keepalive 64;
}

map $http_sec_websocket_key $upgr {
    """"      """";           # If the Sec-Websocket-Key header is empty, send no upgrade header
    default ""websocket"";  # If the header is present, set Upgrade to ""websocket""
}

map $http_sec_websocket_key $conn {
    """"      $http_connection;  # If no Sec-Websocket-Key header exists, set $conn to the incoming Connection header
    default ""upgrade"";         # Otherwise, set $conn to upgrade
}

proxy_cache_path /usr/share/lilium/html/nginxcache levels=1:2 keys_zone=my_cache:10m max_size=10g inactive=60m use_temp_path=off;

server {
        listen 80;

        server_name localhost;
        # port_in_redirect off;

        large_client_header_buffers 8 32k;
        index index.html;

        location / {
                proxy_cache my_cache;
                proxy_cache_revalidate on;
                proxy_cache_min_uses 3;
                proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
                proxy_cache_lock on;

                alias /usr/share/lilium/html/default/;
                try_files $uri $uri.html @lilium;
        }

        location /(lilium|login)/ {
                try_files @lilium =404;
        }

        location @lilium {
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header Host $http_host;
                proxy_set_header Host $host;
                proxy_set_header X-NginX-Proxy true;

                # prevents 502 bad gateway error
                proxy_buffers 8 32k;
                proxy_buffer_size 64k;

                proxy_pass http://lilium_proxy;
                proxy_redirect off;

                # enables WS support
                proxy_http_version 1.1;
                proxy_set_header Upgrade $upgr;
                proxy_set_header Connection $conn;
        }
}

Production version
4.1.0
Founder

Erik Desjardins

Contributors

Erik Desjardins
Samuel Rondeau-Millaire
Daniel McAuley
Gabriel Cardinal
Narcity Media inc.


License
Mozilla Public License Version 2.0
About the license
TL;DR : You can use the CMS to make money as long as it remains open source. The typical use case involves no additional work.
Both individuals and businesses are allowed to use Lilium CMS.
You are allowed to host a copy of Lilium CMS, modify it, redistribute it, create something different with it.
One important thing to note : you must disclose the source code, copyright, and must document any modification done. A copy of the license must be available. However, this does not mean you need to credit Narcity Media on your website or anything of the sort. Simply make the source code available and highlight the modifications if any.
That being said, you can still use Lilium CMS for almost any purposes.
Like most open source licenses, Narcity Media is not liable if anything happens with your server, data, etc. Lilium CMS does not come with a warranty.
The previous information is not legal advice, but should give you a good idea.
Mozilla Public License Version 2.0 is a simple, permissive license with conditions easy to respect. There have a great FAQ here.
Copyright
© Narcity Media, 2019
",5
guyellis/plant-image-lambda,JavaScript,"plant-image-lambda

Image Lambda for Plant Project
Data Flow
Notes
During the upgrade from Node 4 to Node 8 more dependencies were added. This caused the code
size (of the zip package) to increase from 2.4 MB to 5.4 MB. (July 1, 2018).
",2
tolo7010/hak.lnk,None,"hak.lnk
Project Name: hak.lnk
Description: Resource Links For Hackers
Author: tolo7010

General Web Penetration Guides / Courses / Tutorials
Mobile Penetration Guides / Courses / Tutorials
Tools
CTFs / Challenges / Labs
Vulnerability Database
Infosec News
Community Forums / Discussion Boards
Personal Blogs
Useful Git Projects

Please contact me if you see broken links or there are other interesting links which should be added.
",18
PCGen/pcgen,Java,"How to compile PCGen?
Check out our WIKI: http://wiki.pcgen.org/Building_PCGen
",247
Zongsoft/Zongsoft.CoreLibrary,C#,"Zongsoft.CoreLibrary
概述
Zongsoft.CoreLibrary 类库提供了.NET开发的常用功能集以及一些相对于.NET BCL中进行了功能强化的实现。采用C#语言开发，支持跨平台可运行在 Windows、Linux、Mac OSX 等平台中。

欢迎大家在 GitHub 上提交反馈给我们，为我们点赞(Star)。
如果你愿意帮助我们完善、翻译文档，或写范例代码都请致信给我：zongsoft@gmail.com、9555843@qq.com
项目结构


Common

该命名空间内包括一些常用的工具类。其中包括相对 .NET BCL 中进行了功能强化的 Convert 类，对枚举类型操作的 EnumUtility 类，ISequence序列器、IAccumulator累加器接口以及一些扩展类。



Collections

该命名空间内包括有关集合的类。其中包括相对.NET BCL中同名集合类进行了功能强化的 NamedCollectionBase、Collection<T>、Queue 类，以及表示树型层次结构的 HierarchicalNode、HierarchicalNodeCollection、Category、CategoryCollection 这些类，以及一个支持线程安全的提供对象池管理的 ObjectPool 类和支持指定容积的内存缓存ObjectCache。



Communication

该命名空间内包括进行通讯和自定义通讯协议包解析的基类、接口，设计通讯处理程序时应尽量使用这里定义的接口或基类。具体实现请参考 Zongsoft.Net 项目。


Composition

该命名空间内包括“执行管道(ExecutionPipelines)”模式的全部接口和实现，执行管道是一套强大的扩展接口方案，通讯层的 Communication.Net.TcpServer 和 Communication.Net.FtpServer 类均采用该机制来保证服务器端的扩展性。





ComponentModel

该命名空间内包括一些相对 .NET BCL 中进行了功能强化的 TypeConverter，譬如：EnumConverter、CustomBooleanConverter、GuidConverter 类；表示应用程序上下文的 ApplicationContextBase 类，该基类提供了一个应用程序可能会使用到的常用服务；其中的 AliasAttribute 类可用来定义枚举项或其他元素的别称。



Data

该命名空间内包括进行数据访问相关类和接口，我们提供了一个支持多库同时访问、横向分表的分布式关系型数据库ORM引擎，有关这个引擎的详细信息请访问 Zongsoft.Data 项目。



Diagnostics

该命名空间内包括日志处理、诊断跟踪相关的类和接口。



Expressions

该命名空间内包括一个表达式解析以及词法解析等功能实现。



IO

该命名空间内包括一个虚拟文件目录系统的功能集，使用该虚拟文件系统可隔离不同操作系统中IO处理的差异，并支持其他外部文件系统的扩展。具体实现可参考 Zongsoft.Externals.Aliyun 这个项目中的分布式文件存储部分。



Messaging

该命名空间内包含一个消息队列处理的抽象接口，具体实现可参考 Zongsoft.Externals.Aliyun 这个项目中的消息队列部分。



Options

该命名空间内包含了一套选项配置处理的类和接口，这套选项配置以树型结构来组织应用内的所有选项配置数据，访问这些配置数据以通用的逻辑路径的方式来进行。



Configuration

该命名空间内包括一套全新的配置文件的开发接口，该套接口完全兼容 .NET BCL 中的 System.Configuration 的编程模式。


为什么我们要重新编写一套类似的配置开发接口？因为 .NET BCL 自带的配置的机制太过臃肿复杂、并且扩展性也不是太友好，我们希望应用模块的配置应该和该应用模块一样是可被插件化的，它们必须可随意插拔并且保证模块之间的隔离性，当不同模块被组装在一起的时候，这些分离的选项配置数据将自动组织成一个完整的逻辑树。



Profiles

该命名空间内包括一套对 Windows 中 INI 配置文件的开发接口，并且支持对 Section 以层次结构的访问方式。





Reflection

该命名空间内包括一个对成员动态访问的类。



Resources

该命名空间内包括一个对资源处理的 ResourceUtility 工具类。



Runtime


Caching

该命名空间内包含 Buffer 和 Cache 这两种缓存机制的功能集。



BufferManager 提供了在频繁分配不确定大小的内存片段的场景下的一个很好的解决方案，譬如在 TCP 通讯中，接收端并发的收到各个发送端发送过来的数据片段，可以采用 BufferManager 来将这些临时数据片段保存起来待到整个数据包接收完成后再反馈给上层应用完整的数据包。




ICache 表示操作缓存的接口，MemoryCache 是它的一个内存缓存的实现，远程缓存案例可参考 Zongsoft.Externals.Redis 项目。




Serialization

该命名空间内包括了一套序列化和反序列化的相关类和接口，其中包括基于字典(Dictionary)的文本这两种常用序列化实现。由于 .NET BCL 中并没有提供关于序列化器的统一接口，所以使用 ISerializer 这个接口可以隔离特定技术的实现。通过 Serializer 类的 Json 属性可获得一个文本序列化器；通过 DictionarySerializer 类的 Default 属性可获得一个字典序列化器。





Security

该命名空间内包括一个 PasswordUtility 密码操作的工具类，以及与安全、授权相关的基类和接口。


Membership

该命名空间内包括一套完整的基于角色安全的授权管理接口，它还包含了一个最佳实践的方案。具体实现请参考 Zongsoft.Security 项目。





Services

该命名空间内包括一套服务访问和管理的 IServiceProvider、IServiceProviderFactory 接口和实现 ServiceProvider、ServiceProviderFactory；以及一套有关命令装配模式的接口和实现；还有一个后台服务的工作者 IWorker 接口和 WorkerBase 基类。



Terminals

该命名空间内包括一套终端程序的接口和实现，使用该实现可以快速的完成一个强大的基于控制台的应用。


Commands

该命名空间内包括关于终端程序的一些常用命令的实现类，譬如 ExitCommand、ClearCommand、HelpCommand 等类。





Text

该命名空间内包括一套基于正则表达式的文本表达式的解析、处理的类。



Transactions

该命名空间内包括有关事务的类和接口，有关应用事务支持的实现可参考 Zongsoft.Data 数据引擎 中的事务支持。



引用说明
本项目中的所有代码均未参考过任何特定实现，特此声明！
授权协议

Zongsoft.CoreLibrary 是基于 LGPL v2.1授权协议。
您可以将本项目应用于商业活动中，但是必须确保对本项目的完整（含版权声明）引用，不要分割或部分引用该项目的源码，我们保留追究违反授权协议的权利。


为什么要开源？
但求滴水之源敢汇大海之心。
",47
yin1999/code_sharing,C++,"此代码库用于保存我做题或通过其它途径接触到或者解决问题产生代码。
",2
igortorres17/SGSA,Java,"SGSA
Sistema de Gerenciamento de Serviços Automotivos (SGSA) é um software desenvolvido pelos estudantes Hércules Moreira, Igor Torres, Ricardo Otaviano e Matheus Cruvinel do curso de Análise e Desenvolvimento de Sistemas do Instituto Federal do Triângulo Mineiro. O programa foi proposto pelo professor da disciplina de Programação Visual, Eduardo Siqueira.
Recursos
Gerenciamento de Clientes, Serviços, Veículos e Peças

Cadastro
Visualização
Alteração
Exclusão

Ordem de Serviço (OS)

Emissão de Ordem de Serviço
Inclusão de Serviços e Peças na OS
Inclusão de Mecânico Responsável
Impressão de OS

Autenticação

Acesso através de nome de usuário e senha
Níveis de acesso (Permissão)

",2
Lombiq/Combinator,C#,"Combinator Orchard module Readme
Project Description
An Orchard CMS module that combines and minifies external stylesheets and javascript files to cut down on load times.
Features

Combines and minifies css files
Combines and minifies javascript files
If local and remote resources are mixed (like a local js files with one from a CDN) preserves their original order
Preserves conditional resources and minifies (if multiple with the same condition are after each other, also combines) them
Can combine remote (CDN) resources
Can embed images into stylesheets as data urls
Experimental image sprite generation support
Resource sets can be defined for better client-side caching: you can create sets of resources that are combined separately (e.g. all jQuery scripts can be in their individual file)
Ability to share processed resources between tenants in a multi-tenant application so a set of resources is only processed once, not for every tenant (resource sharing)
Busts browser cache when resources are updated (with a query string parameter containing a time stamp)
Ability to set custom resource domain
Exposing resource processing events
LESS and SASS preprocessors, contribution of Onestop Internet, Inc.
Command line command for emptying cache (""combinator empty"")
Info comment in bundled resources about which resources were combined
Tuned to be fast
With custom IStorageProvider can work in cloud hosting too (if there is no write access to the Media folder anyway)
Import/export settings
Administration page:

Adjust combination exclusion filter
Enable/disable combination of CDN resources
Set up resource domain
Enable/disable minification and adjust exclusion filter
Enable/disable image embedding and adjust exclusion filter
Enable/disable image sprite generation
Define resource sets
Enable/disable for admin site
Empty cache



The module is also available for DotNest sites.
You can download an install the module from the Orchard Gallery.
For known issues and future plans please see the Issue Tracker.
Please make sure to read the Documentation!
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/combinator (Mercurial repository)
https://github.com/Lombiq/Combinator (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",6
nemosminer/NemosMiner,PowerShell,"NemosMiner
Updated 12 May 2019






Copyright (c) 2018-2019 Nemo and MrPlus
This is free software, and you are welcome to redistribute it
under certain conditions.
https://github.com/nemosminer/NemosMiner/blob/master/LICENSE
by Nemo/Minerx117
with Help From MrPlusGH and grantemsley
 Click to Join Discord

Have questions? Need help? We're on Discord: https://discord.gg/2BCqPxe
NemosMiner Monitors mining pools in real-time in order to find the most profitable Algo
 GUI and easy configuration
 Auto Benchmarks Each algo to get optimal speeds 
 Fully automated 
 Auto Downloads Miners
 Auto Updates
 Monitoring


Easy configuration, easy start:
  Run NemosMiner.bat
  1. Config tab
  2. Set your Wallet address and Username
  3. Select your pool 
  4. Save Config
  5. Start
  
  note: 2. you only need to change Username if you are using Miningpoolhub
  
 Algo selection / removal

  +algo for algo selection
  -algo for algo removal

  If ""+"" Used, all selected algo have to be listed
  If ""Minus"" Used, all algo selected but exluded ones.

  Do not combine + and - for the same algo

 Examples:
 Algo list = -x16r
 Will mine anything but x16r

 Algo list = -x16r,-bcd
 Will mine anything but x16r and bcd

 Algo list = +x16r
 Will mine only x16r

 Algo list = +x16r,+bcd
 Will mine only x16r and BCD

 Algo list blank
 Will mine anything

Pools variants
  24hr - uses last 24hour Actual API too request profit
     -Low switching rate
  plus - uses advanced calculations to reduce switching
     -Medium switching rate
  normal - uses current estimate API too request profit
     -High switching rate

Developer/Contributors Donation:
list and wallets is publicly available at: https://nemosminer.com/data/devlist.json
  There is a 5 minute per day donation (0.3%), that can be changed in the config (Minimum is 3)0.2%
  We want to stay completely transparent on the way donations are managed in the product. Donations occurs once every 24 hours for the selected amount of time (default 5 minutes). The first donation sequence occurs 1 hour after miners are started. If Interval is set higher than the donation time, the interval will prime. Example for default parameters. Miners started at 10, First donation cycle runs at 10:55 untill 11, Next donation cycle occurs 24 hours after.All donation time and addresses are recording in the logs folder.

NemosMiner Monitoring Server : https://nemosminer.com
 Keep tabs on all your mining rigs from one place
 You can now optionally monitor all your workers remotely, both in the GUI and via https://nemosminer.com  
 Monitoring setup instructions https://nemosminer.com/setup.php 

GUI
  Since version 3.0 NemosMiner has a GUI making it easy to configure and run.
  Relies on config files. No need to edit bat files. Simply run NemosMiner 
  Set the config on the config tab, save, close, run

Pause mining
  Ability to pause miners while keeping other jobs running (pause button)
  This will stop mining activity
  BrainPlus will still run in the background avoiding the learning phase on resume
  EarningTracker will still run in the background avoiding the learning phase on resume

prerun
  Ability to run a batch prior switching to a specific algo.
  For example, can be used to set per algo OC via nvidiaInspector
  Simply create a file named <AlgoName>.bat in prerun folder
  If <AlgoName>.bat does not exist, will try to launch prerun/default.bat
  Use overclock with caution

Per pools config (Advanced)
    - **This is for advanced users. Do not use if you do not know what you are doing.**
    - You can now set specific options per pool. For example, you can mine NiceHash on the internal wallet and other pools on a valid wallet. This configuration is provided as an example in Config\PoolsConfig-NHInternal.json
      - Available options
        - Wallet = your wallet address
        - UserName = your MPH user name
        - WorkerName = your worker name
        - PricePenaltyFactor = See explanation below
    - Algorithm = List of included or excluded Aglo on pool (see example files)
      - Usage
        - The file Config\PoolsConfig.json contains per pool configuration details. If a pool is listed in this file,
    the specific settings will be taken into account. If not, the setting for the entry name default will be used.
    **Do not delete the default entry.**
        - Edit Config\PoolsConfig.json
        - Add an entry for the pool you want to customize
          - The name must be the NemosMiner name for the pool. ie. for ahashpool, if you use Plus. The name is ahashpoolplus.
          - (**careful with json formating ;)**)
          - Best way is to duplicate the default entry
    - Note that the GUI only updates the default entry. Any other changes need to be done manualy

PricePenaltyFactor
    - When using advanced per pool configuration, it is possible to add a penalty factor for a specific pool. This simply adds as a multiplicator on estimations presented by the pool.
    - Example scenario
      - NiceHash has a 4% fee - Set PricePenaltyFactor to 0.96 (1-0.04)
      - You feel like a pool is exaggerating his estimations by 10% - Set PricePenaltyFactor to 0.9

zergpoolplus/nlpoolplus/ahashpoolplus/zpoolplus/blazepoolplus/phiphipoolplus/blockmastersplus/hashrefineryplus
  Uses calculations based on 24hractual and currentestimate ahashpool prices to get more realistic estimate.
  Includes some trust index based on past 1hr currentestimate variation from 24hr.
  AND is NOT sensible to spikes.
  This shows less switching than following Current Estimate and more switching that following the 24hr Actual.
  Better profitability.

Earnings Tracking
  Displays BTC/H and BTC/D as well a estimation of when the pool payment threshold will be reached.
  Supported pools:
        ahashpool
        zpool
        nicehash
        miningpoolhub (partial)
  If mining more that one pools, shows stats for any supported pool
  Press key e in the console window to show/hide earnings

Support running multiple instances
  **Experimental**
  More than one instance of NemosMiner can run on the same rig
  Each instance must be placed in it's own directory
  Miner has to be started prior the launch of the next instance

Optional miners (Advanced)
  Some miners are not enabled by default in NemosMiner for a variety of reasons:
   
          These are closed source and therefore not enabled in NemosMiner by default.
          Use at your own risk.

  For advanced users, check the Optional Miners checkbox on the Config tab to enable these miners.

CustomMiners (Advanced)
  Users can place any miner.ps1 from miners/optionalminers or custom user created miner.ps1 files, in CustomMiners folder 
  leaving miners and optionalminers disabled in config will enable CustomMiners folder 

Algo switching log
  Simple algo switching log in csv switching.log file found in Logs folder.
  You can easily track switching rate.

Console Display Options
  Use -UIStyle Light or -UIStyle Full in config.json
        Full = Usual display (Default)
        Light = Show only currently mining info
  UIStyle automaticaly swtiches to Full during benchmarking.

In session console display toggle
  Press key s in the window to switch between light and full display
  Press key e in the window to show/hide earnings 
  Will toggle display at next refresh

New version notification
  NemosMiner will notify new version availability


If you have Windows 7, 8, or 8.1, please update PowerShell:
update PowerShell
some miners may need 'Visual C++ 2015' if you don't already have it: (install both x86 & x64)
Visual C++ Redistributable for Visual Studio 2015/2014
some miners may need 'Visual C++ 2013' if you don't already have it: (install both x86 & x64)
Visual C++ Redistributable for Visual Studio 2013/2012
running multiple cards its recommended to increase Virtual Memory 64gb is optimal
recommended/optimal Windows Nvidia driver 430.39
Windows10
Windows7, 8, 8.1
recommended/optimal Linux Nvidia driver 430.09
Linux/Hiveos
Made for & Tested with 6x1070 6x1070ti 6x1080 6x1080ti 6x1660ti 6x2060 6x2070 6x2080 6x2080ti(users have reported up to 12cards working have not tested myself)
Some miners do not support more that 9 cards

Licensed under the GNU General Public License v3.0
Permissions of this strong copyleft license are conditioned on making available complete source code of licensed works and modifications, which include larger works using a licensed work, under the same license. Copyright and license notices must be preserved. Contributors provide an express grant of patent rights. https://github.com/nemosminer/NemosMiner/blob/master/LICENSE
",218
firstcontributions/first-contributions,None,"



First Contributions
It's hard. It's always hard the first time you do something. Especially when you are collaborating, making mistakes isn't a comfortable thing. We wanted to simplify the way new open-source contributors learn & contribute for the first time.
Reading articles & watching tutorials can help, but what's better than actually doing the stuff in a practice environment? This project aims at providing guidance & simplifying the way beginners make their first contribution. If you are looking to make your first contribution, follow the steps below.
If you're not comfortable with command line, here are tutorials using GUI tools.
Read this in other languages.
🇮🇳
🇲🇲
🇮🇩
🇫🇷
🇪🇸

🇳🇱
🇱🇹
🇷🇺
🇧🇬
🇸🇰
🇯🇵
🇻🇳
🇵🇱
🇮🇷
🇮🇷
🇰🇷 🇰🇵
🇩🇪
🇩🇰
🇨🇳
🇹🇼
🇬🇷
🇪🇬
🇸🇦
🇺🇦
🇧🇷
🇵🇹
🇮🇹
🇹🇭
🏴
🇳🇵
🇵🇰
🇧🇩
🇲🇩 🇷🇴
🇹🇷
🇸🇪
🇲🇾
🇸🇮
🇮🇱
🇨🇿

🇲🇽
🇵🇭
🇿🇦
🇿🇦
🇰🇪
🇳🇬

If you don't have git on your machine, install it.
Fork this repository
Fork this repository by clicking on the fork button on the top of this page.
This will create a copy of this repository in your account.
Clone the repository

Now clone the forked repository to your machine. Go to your GitHub account, open the forked repository, click on the clone button and then click the copy to clipboard icon.
Open a terminal and run the following git command:
git clone ""url you just copied""

where ""url you just copied"" (without the quote marks) is the url to this repository (your fork of this project). See the previous steps to obtain the url.

For example:
git clone https://github.com/this-is-you/first-contributions.git

where this-is-you is your GitHub username. Here you're copying the contents of the first-contributions repository in GitHub to your computer.
Create a branch
Change to the repository directory on your computer (if you are not already there):
cd first-contributions

Now create a branch using the git checkout command:
git checkout -b <add-your-new-branch-name>

For example:
git checkout -b add-alonzo-church

(The name of the branch does not need to have the word add in it, but it's a reasonable thing to include because the purpose of this branch is to add your name to a list.)
Make necessary changes and commit those changes
Now open Contributors.md file in a text editor, add your name to it. Don't add it at the beginning or end of the file. Put it anywhere in between. Now, save the file.

If you go to the project directory and execute the command git status, you'll see there are changes.
Add those changes to the branch you just created using the git add command:
git add Contributors.md

Now commit those changes using the git commit command:
git commit -m ""Add <your-name> to Contributors list""

replacing <your-name> with your name.
Push changes to GitHub
Push your changes using the command git push:
git push origin <add-your-branch-name>

replacing <add-your-branch-name> with the name of the branch you created earlier.
Submit your changes for review
If you go to your repository on GitHub, you'll see a  Compare & pull request button. Click on that button.

Now submit the pull request.

Soon I'll be merging all your changes into the master branch of this project. You will get a notification email once the changes have been merged.
Where to go from here?
Congrats!  You just completed the standard fork -> clone -> edit -> PR workflow that you'll encounter often as a contributor!
Celebrate your contribution and share it with your friends and followers by going to web app.
You could join our slack team in case you need any help or have any questions. Join slack team.
Now let's get you started with contributing to other projects. We've compiled a list of projects with easy issues you can get started on. Check out the list of projects in web app.
Additional material
Tutorials Using Other Tools











GitHub Desktop
Visual Studio 2017
GitKraken
Visual Studio Code



Self-Promotion
If you liked this project, star it on GitHub.
If you're feeling especially charitable, follow Roshan on
Twitter and
GitHub.
 
",6630
luyikk/NetX,C#,"NetX
谭释了RPC和ACTOR的完美结合
",2
bg1bgst333/Sample,Java,"Sample
Sample Program
",4
knqyf263/fanal,Go,"fanal
Static Analysis Library for Containers





Feature

Detect OS
Extract OS packages
Extract libraries used by an application

Bundler, Composer, npm, Pipenv, Cargo



Example
See cmd/fanal/
package main

import (
	""context""
	""flag""
	""fmt""
	""log""
	""os""

	""golang.org/x/xerrors""

	""github.com/knqyf263/fanal/cache""

	""github.com/knqyf263/fanal/analyzer""
	_ ""github.com/knqyf263/fanal/analyzer/library/bundler""
	_ ""github.com/knqyf263/fanal/analyzer/library/composer""
	_ ""github.com/knqyf263/fanal/analyzer/library/npm""
	_ ""github.com/knqyf263/fanal/analyzer/library/pipenv""
	_ ""github.com/knqyf263/fanal/analyzer/library/cargo""
	_ ""github.com/knqyf263/fanal/analyzer/os/alpine""
	_ ""github.com/knqyf263/fanal/analyzer/os/amazonlinux""
	_ ""github.com/knqyf263/fanal/analyzer/os/debianbase""
	_ ""github.com/knqyf263/fanal/analyzer/os/opensuse""
	_ ""github.com/knqyf263/fanal/analyzer/os/redhatbase""
	_ ""github.com/knqyf263/fanal/analyzer/pkg/apk""
	_ ""github.com/knqyf263/fanal/analyzer/pkg/dpkg""
	_ ""github.com/knqyf263/fanal/analyzer/pkg/rpm""
	""github.com/knqyf263/fanal/extractor""
	""golang.org/x/crypto/ssh/terminal""
)

func main() {
	if err := run(); err != nil {
		log.Fatal(err)
	}
}

func run() (err error) {
	ctx := context.Background()
	tarPath := flag.String(""f"", ""-"", ""layer.tar path"")
	clearCache := flag.Bool(""clear"", false, ""clear cache"")
	flag.Parse()

	if *clearCache {
		if err = cache.Clear(); err != nil {
			return xerrors.Errorf(""error in cache clear: %w"", err)
		}
	}

	args := flag.Args()

	var files extractor.FileMap
	if len(args) > 0 {
		files, err = analyzer.Analyze(ctx, args[0])
		if err != nil {
			return err
		}
	} else {
		rc, err := openStream(*tarPath)
		if err != nil {
			return err
		}

		files, err = analyzer.AnalyzeFromFile(ctx, rc)
		if err != nil {
			return err
		}
	}

	os, err := analyzer.GetOS(files)
	if err != nil {
		return err
	}
	fmt.Printf(""%+v\n"", os)

	pkgs, err := analyzer.GetPackages(files)
	if err != nil {
		return err
	}
	fmt.Printf(""Packages: %d\n"", len(pkgs))

	libs, err := analyzer.GetLibraries(files)
	if err != nil {
		return err
	}
	for filepath, libList := range libs {
		fmt.Printf(""%s: %d\n"", filepath, len(libList))
	}
	return nil
}

func openStream(path string) (*os.File, error) {
	if path == ""-"" {
		if terminal.IsTerminal(0) {
			flag.Usage()
			os.Exit(64)
		} else {
			return os.Stdin, nil
		}
	}
	return os.Open(path)
}


Notes
When using latest tag, that image will be cached. After latest tag is updated, you need to clear cache.
",3
PCSX2/pcsx2,C++,"PCSX2
  
PCSX2 is a free and open-source PlayStation 2 (PS2) emulator. Its purpose is to emulate the PS2's hardware, using a combination of MIPS CPU Interpreters, Recompilers and a Virtual Machine which manages hardware states and PS2 system memory. This allows you to play PS2 games on your PC, with many additional features and benefits.
Project Details
The PCSX2 project has been running for more than ten years. Past versions could only run a few public domain game demos, but newer versions can run many games at full speed, including popular titles such as Final Fantasy X and Devil May Cry 3. Visit the PCSX2 homepage to check the latest compatibility status of games (with more than 2000 titles tested), or ask for help in the official forums.
The latest officially released stable version is version 1.4.0.
Installers and binaries for both Windows and Linux are available from our website.
Development builds are also available from our website.
System Requirements
Minimum

OS: Windows Vista SP2 or newer or GNU/Linux (32-bit or 64-bit)
CPU: Any that supports SSE2 (Pentium 4 and up, Athlon64 and up) @ 1600 STR or better
GPU: DirectX 10 GPU or better
RAM: 2GB or more

Recommended

OS: Windows 7/8/8.1/10 (64-bit) or GNU/Linux (64-bit)
CPU: Intel Haswell (or AMD equivalent) @ 2000 STR or better
GPU: DirectX 11 GPU or greater
RAM: 4GB or more

Notes


You need the Visual C++ 2015 x86 Redistributables for this version to work.
Note: Visual C++ 2017 is directly compatible with Visual C++ 2015. While the project is built with Visual C++ 2015, either version will work.


PCSX2 1.4.0 is the last stable version to support Windows XP and Direct3D9. Windows XP is no longer getting updates (including security-related updates), and graphics drivers for Windows XP are older and no longer maintained.


Make sure to update your operating system, drivers, and DirectX (if applicable) to ensure you have the best experience possible. Having a newer GPU is also recommended so you have the latest supported drivers.


Because of copyright issues, and the complexity of trying to work around it, you need a BIOS dump extracted from a legitimately-owned PS2 console to use the emulator. For more information about the BIOS and how to get it from your console, visit this page.


PCSX2 mainly takes advantage of 2 CPU cores. As of this commit PCSX2 can now take advantage of more than 2 cores using the MTVU speedhack. This can be a significant speedup on CPUs with 3+ cores, but it may be a slowdown on GS-limited games (or on CPUs with fewer than 2 cores).


Requirements benchmarks are based on a statistic from the Passmark CPU bench marking software. When we say ""STR"", we are referring to Passmark's ""Single Thread Rating"" statistic. You can look up your CPU on https://cpubenchmark.net to see how it compares to PCSX2's requirements.


Screenshots








",3220
autonomous-robot-competition2019/Team-Green,C++,"Team-Green
",2
terrence2/openfa,Rust,"OpenFA
An attempt at a black-box, open-source re-implementation of the Janes Fighters Anthology's engine.



Progress

The FA engine uses many different files. Some of these are standard formats; some of them are straightforward text;
but most are extremely weird relics of a bygone computing age.
Standard formats



Extension
Asset




11K
Sound


5K
Sound


8K
Sound


XMI
eXtended MIdi



Textual



Extension
Asset
Parsed




AI
AI Program



ECM
ECM Type



GAS
Fuel Tank Type



INF
Info Page Text



JT
proJectile Type
x


M
Mission
x


MM
Mission Map
x


MT
Mission Text



NT
Npc Type
x


OT
Object Type
x


PT
Plane Type
x


SEE
Sensor Type



SEQ
Scene Timelines



TXT
Campaign Blurbs




Portable Executable Wrapper



Extension
Asset
Parsed




BI
AI Binary



CAM
Campaign



DLG
Dialog Menus



FNT
Font
x


HGR




HUD




LAY
Terrain Palette
x


MC




MNU
Menus



MUS




PTS




SH
Shape
x



Custom Binary



Extension
Asset
Parsed




BIN




CB8




FBC




PAL
Palette
x


PIC
Picture
x


T2
Terrain
x


VDO
Video




Specific Format Notes

PAL: PALETTE.PAL is the only file of this type. It contains palette data consisting of 256 3-byte entries.
Each byte contains a 6-bit (VGA) color, so must be shifted by 2 for use in modern systems. Large parts of this
palette contain the ""transparent"" color #FF00FF. These sections are used by the terrain and (presumably) the HUD/menu
to give custom look and feel to each area and plane.
SH: Shape files contain a virtual machine using word codes inside the PE wrapper, with embedded fragments of x86.
Execution jumps between virtual and machine instructions in order to achieve most dynamic plane effects.
T2: Just heights and metadata about the terrain. The textures to be rendered onto that heightmap are stored
in the MM/M files in tmap and tdict sections. Both the textures and the base colors in the T2 itself are outside
the range of the base PALETTE.PAL and depend on a fragment of the LAY file being copied into the right part of
the palette. Time-of-day effects as well as distance fogging are acheived by swapping out the palette with values
from the LAY.
VDO: These files start with RATPAC, which is probably short for Rate-Packed. This is probably a standard
format of some sort. Unfortunately, a basic google search for files with that header turned up absolutely
nothing. We need a guru who knows about ancient video encoding standards.

Development Environment Setup

git clone https://github.com/terrence2/openfa.git
cd openfa
mkdir -p test_data/{un,}packed/{USNF,USMF,ATF,ATFNATO,ATFGOLD,USNF97,FA}/installdir
Copy *.LIB from the CD and Installation directory into test_data/packed/<GAME>/
Copy any loose T2 files from the Installation directory (ATFNATO and earlier only) into test_data/packed/<GAME>/installdir/
Install the Rust language via rustup.rs
(Optional) cd into apps/unlib and run cargo run -- -o ../../test_data/unpacked/<GAME>/<LIB> ../../test_data/packed/<GAME>/<LIB> on
each of the libs that you would like to have available as raw files. This are generally faster and easier to work with when
developing than the raw LIB files
Run sh_explorer by changing directory into apps/sh_explorer/ and running cargo run -- -t <GAME>:<FILE.SH> (for example cargo run -- -t FA:F18.SH)
Run mm_explorer by changing directory into apps/mm_explorer/ and running cargo run -- -t <GAME>:<FILE.MM> (for example cargo run -- -t FA:UKR.MM)

",3
zlgopen/awtk-lite-service,C,"awtk-lite-service
为AWTK开发的轻量级服务框架，以及常见服务的实现。
一、概述
在开发AWTK应用程序时，有时需要调用耗时很长的函数，如果在GUI线程直接调用，就会阻塞GUI线程，让界面无法刷新。这时我们需要创建一个线程，把这个函数放在线程中执行，让同步调用变成异步调用，等它执行完成再通知GUI线程更新界面。
多线程编程是件麻烦的事情，很容易出现错误，而且出现问题之后很难查找原因。lite-service的目的就是为了简化这个开发过程，它提供了一个轻量级的服务框架，可以方便的实现基于线程的服务，而调用者不需了解线程相关的东西，就可以把同步调用转换成异步调用。
除了服务框架外，我们也将提供一些基础的服务，如：异步化、HTTP网络请求、媒体播放器和其它一些常见服务。

请注意: 这里假设调用者是GUI线程，执行结果和事件会发给GUI线程，事件处理函数是在GUI线程执行的，可直接操作GUI控件。

1.编译：
本项目依赖AWTK，请将AWTK取到同级目录，先编译AWTK，然后再编译本项目：
scons

2.运行Demo：

异步请求的demo

./bin/demo_async


HTTP请求的demo

./bin/demo_http


测试HTTP之前，需要用nodejs启动本地HTTP服务：
node server/index.js


二、服务框架
1. 服务的类型。
常见的服务有以下几种情况：


在后台线程执行一个函数，执行完成后，将结果返回给调用者。比如打开WIFI，打开完后通知调用者，调用者更新界面。


在后台线程执行一个函数，在执行的过程中，有进度信息通知调用者，有时调用者也取消执行。如HTTP请求就是这样，下载的进度信息需要通知调用者，让用户知道当前进度，用户也可以随时取消下载。


在后台线程执行一个函数，在执行的过程中，服务还可以接收调用者的请求。如媒体播放器，在播放的过程中可以接受暂停、前进和后退等操作。


服务可以完成就退出，也可以常驻内存，随时响应请求。
2. 调用者请求服务


通过初始化参数告诉服务要做的事情。


通过共享状态告诉服务要做的事情。


通过请求队列告诉服务要做的事情。


3. 服务通知调用者
服务通过idle_queue函数，把事件/结果发送到GUI线程，在idle函数中，执行调用者提供的回调函数。所以在回调函数中可以直接操作GUI控件。
4. 基础组件
根据以上这些情况，lite service提供了一些基础组件，以简化开发过程。


**请求队列。**调用者通过它发送请求，服务通过它分发请求。


**服务基类。**提供了服务需要的基础功能，并定义了具体服务需要实现的接口。


**服务线程。**服务线程的管理，目前没使用线程池，以后实际需要进行完善。


三、async服务
async服务把一个同步函数转成一个异步函数。使用方法如下：
下面这个函数耗时3秒，在GUI线程执行就会让界面无法刷新，所以需要异步化处理。
static ret_t do_sth_take_time(void* ctx) {
  sleep_ms(3000);

  return RET_OK;
}

为了让它异步执行，我们定义一个接受结果的函数(可选，如不要执行结果，使用NULL即可)：
static ret_t on_do_sth_take_time_done(void* ctx, ret_t result) {

  return RET_OK;
}

然后调用async_call转换为异步调用：
async_call(do_sth_take_time, on_do_sth_take_time_done, widget);


具体用法请参考：demos/demo_async.c

四、HTTP服务
目前实现了GET/POST/DELETE/PUT四种方法，可以满足常见的REST API调用。但不适合大文件传输和同时大量并发请求。
接受事件的回调函数的原型：
typedef ret_t (*http_request_on_event_t)(void* ctx, http_request_t* request, http_response_t* resp);

发起请求的函数：
ret_t http_request(http_request_t* request);

如：
  request = http_request_create_get(url, on_http_event, widget);
  http_request(request);


以下几点值得注意：

resp->done 为TRUE表示请求成功完成。
resp->fail 为TRUE表示请求失败。
resp->done 或者 resp->fail 为TRUE表示请求完成了，此时才可以释放request对象。
resp->body 是返回的内容。
resp->body_size 是返回的内容的长度。
resp->status_code 是HTTP响应码。
resp->header 是响应头。
设置request->abort标志来取消请求，取消请求是异步。


具体用法请参考：demos/demo_http.c

五、媒体播放
基于ffmpeg实现媒体播放功能(TODO)
六、API文档
API文档
",3
guyellis/plant,JavaScript,"plant
Running live at Plaaant
To get a feel a well chronicled orchard of fruit trees visit
Guy Ellis' Orchard
and use the ""Filter"" input near the top of the page to find fruit trees you might be interested
in learning about.
Plant is a website app to help you manage growing trees and plant.





Project Objectives

Versions
Questions


For Developers

Architecture Notes
Developer Setup



Objectives

Allow users to track the plants that they're growing.

Provide stats on growth rates.
Pull in weather information.
Compare against other growers.


Allow users to research plants to grow.

Users can search within a radius of their location for plants others are
growing.
Users can determine likelihood of success.


Assist users in management of their plants.

Send alerts to users when action needs to be taken based on time-of-year
or on weather. For example, should prune/fertilize on X Date, or ""there's
a freeze warning, you need to protect your avocado tree.""


Usability

Users should be able to post entries about a tree from their yard on a smart
phone.
Users should be able to take photos and add them to a plant entry from
their smart phone while making a post.
Users should be able to operate the app while disconnected with syncing
done later.



Versions
0.0.1 MVP

User can create an account using OAuth from their Google account.
User can add/delete/update each plant in their yard.
User can add entries to each plant. Each entry will have a date and
details.

0.0.2 and beyond

Add features/fields to user's account.

Add Facebook as OAuth option.
Allow user to add their GPS location to their account.


Add features to a plant post.

Allow photos to be added.
Add markdown for details to allow for formatting.


Add structured fields for each plant.

Dates for planting, germination.
Multi-bud - treat each scion as its own tree but group on this.
Perennial/annual (other?)


Add structured fields for data entry posts.

E.g. height, width etc. to calculate growth rates.
Applications of fertilizer, mulch etc for reporting on growth effectiveness.
Start and end dates for harvest. (Allow for reporting on year round
production and prediction.)
Volume harvested.


Perform calculations on entries to show changes. e.g. growth in gallons per
week.
Use user's GPS location to pull in weather data from date of first plant
entry to current date.
Keep user's weather data up-to-date.
Use user's GPS location to allow them to compare their plants to the same
plants in nearby locations.
Allow users to add parentage/genealogy to their plants.

i.e. if another user gave them a plant or they planted another plant from
one that they're already documenting on the system then they can link a parent
plant.


Support multi-bud trees.

Allow for structured data entry for root stock(s) and multiple scions.
Allow dates for adding scions to trees.


Allow for termination date of plants.

Distinguish between perennial and annuals. i.e. plants that are designed
to die annually and those that aren't. Helps in reporting life and reason
for death or disposal of perennial.


Search

User can find a male pollinator for their female plant within a certain
radius of their location.



Questions
If you have questions or want to communicate with the maintainers of the
project then please create an issue.
For Developers
Architecture
Components
plant
Components in the /app/components/plant folder.
Component naming uses CRUD (Create, Read, Update, Delete) names to identify what the component does.

Plant

Container for showing and managing a single plant
All other components in this section are used by the Plant component directly or indirectly. i.e. Plant is the top parent


PlantRead

Shows the details of the Plant
PlantEdit hidden


PlantEdit

Create or Update a Plant.
PlantRead hidden


Notes

Manages the Hide/Show ""add note"" functionality
Container to show a list of Notes


NoteEdit

Create or Update a Note.


NoteRead

Shows the details of a Note



plants
Components in the /app/components/plants folder.
Components for managing a collection and the listing of plants.
Developer Setup
Facebook and Google OAuth
You can start the site without setting up Facebook credentials.
As long as the NODE_ENV is not set to production you will be able to login as a dev user.
You do, however, need to set the Facebook and Google environment variables to a non-empty value.
In your ~/.bashrc or equivalent file (or a script you source before you start the server) set the following:
export PLANT_FB_ID=<facebook-app-id>
export PLANT_FB_SECRET=<facebook-app-secret>
export PLANT_GOOGLE_ID=<google-app-id>
export PLANT_GOOGLE_SECRET=<google-app-secret>

If want to use Facebook OAuth then you'll need to setup credentials.
(As with any site, the layout and options change over time so these instructions are an approximation.)

Go to developers.facebook.com and on the menus click on My Apps and then select Add a New App.
Select a WWW Website.
Add a name (Plant is good) and click Create New Facebook App ID.
There's a button to the top right to Skip Quickstart - hit that.
You should end up on the Dashboard. From here you want the App ID and App Secret.
Set the PLANT_FB_ID and PLANT_FB_SECRET environment variables to these values.

MongoDB
If you have Docker installed then it will pull down and spin-up MongoDB for you when you start the server.
Otherwise, you need to install Docker (recommended) or MongoDB.
Running the site

Clone the repo locally.
npm i
Terminal Window #1: npm run server

Starts the server on port 3001 using the /devops/run-server.sh script. Edit this script to fine tune how the server starts.


Terminal Window #2: npm start

Starts the Webpack Dev Server on port 9090


Navigate: http://localhost:9090

Running the tests
npm t

Debugging with VSCode

Start MongoDB
Terminal #1: Start the App/API Server with npm run server
Terminal #2: Start the Dev Server with npm start
VSCode: Select Chrome from Debug launch options and hit F5

",6
xndcn/smzdm.com,None,"smzdm.com
",10
Mindjet/LiteWeather,Kotlin,"LiteWeather
The app is available here.
Screenshot

",5
narcitymedia/lilium-cms,JavaScript,"
Lilium CMS V4
Lilium is a lightning-fast, web based content management system built with Node.JS. Its power resides in its intelligent caching engine which reduces CPU usage, RAM, and database access.
If server-side rendering is preferred over client-side rendering, Lilium offers a simplified way of generating HTML documents, storing them on disk or RAM, and serving them faster than most HTML preprocessors. It is possible to use either LML2 which ressembles PHP, or LML3 which is an easy to use routine using Javascript template strings.
The platform has its own framework and unique API. Lilium makes it easy to create a mobile app for a website, and is network based. It can hold multiple domains for a single instance, can use multiple different databases. It is compatible with content delivery network services.
Lilium does not use Express, Mongoose, or other heavy libraries. Instead, it implements its own web server using native NodeJS libraries.
Open source details
Narcity Media is using Lilium CMS in production. However, it is currently using the V3. That means this version is not ready for production just yet. We still invite you to try the CMS and have fun with it, but we recommend to wait until the V4 is stable before deploying a website.
Installation guide
All NodeJS packages are to be installed, and are documented in the package file. You can simply run npm run setupdev in the Lilium root folder.
Automated installation
Lilium was built on Linux, and is meant to be run on Linux. Even though it will technically work on Mac, Lilium is not guaranteed to be stable on other operation systems.
The CMS requires NodeJS v8+, and MongoDB v3+. You can follow online tutorials on how to install both of these before running the npm scripts, but it's a faily simple process.
Installing NodeJS
You can follow this tutorial from the official NodeJS website.
From the nodesource instructions
# Using Ubuntu
curl -sL https://deb.nodesource.com/setup_11.x | sudo -E bash -
sudo apt-get install -y nodejs

# Using Debian, as root
curl -sL https://deb.nodesource.com/setup_11.x | bash -
apt-get install -y nodejs

Installing MongoDB
You can follow this tutorial from the official MongoDB website.
Only run one of the following depending on your Ubuntu distro
# For Ubuntu 18
echo ""deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.0 multiverse"" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.0.list

# For Ubuntu 16
echo ""deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/4.0 multiverse"" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.0.list

# For Ubuntu 14
echo ""deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/4.0 multiverse"" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.0.list

Then
sudo apt-get update
sudo apt-get install -y mongodb-org
sudo service mongod start

And MongoDB should be running. To make sure the installation was successful, you can open a shell using mongo.
Installing Lilium for development
You can clone Lilium in your favourite installation directory. At Narcity Media, we like to use /usr/share/lilium or ~/dev/lilium. Make sure the directory is owned by you, not by root. Also make sure to install it in a directory that is also owned by you since other sibling directories might be needed.
Simply cd to the directory you want to use, and git clone https://github.com/narcitymedia/lilium-cms.
Then, cd lilium-cms and npm install && npm run setupdev.
Once the installation process exits, you can start the CMS using npm start or node index.prod.js. Your browser should load the CMS if you browse to localhost:8080/lilium.
The development username and password are : lilium and lilium. Make sure to change them if you plan on deploying on a production machine.
Web panel for development
The CMS frontend is located under /apps/lilium. It is a Preact app, and is transpiled using Webpack and Babel.
Required dependencies
MAC:
brew install pkg-config cairo libpng jpeg giflib imagemagick redis
UBUNTU:
sudo apt-get install libcairo2-dev libjpeg8-dev libpango1.0-dev libgif-dev build-essential g++ libkrb5-dev imagemagick redis-server
The dependencies will be installed automatically during the npm run setupdev process. You can however install then manually if you prefer.
Localhost connections to MongoDB
If mongo still refuses to connect from NodeJS eventhough it works using the terminal cli, you might have to enable or disable authentication from your mongo config file (typically located in /etc/mongod.conf, under security: authentication).
This information can be found easily online using search queries such as ""Enable MongoDB authentication"". We ran into this issue on multiple occasion, and it ended up being a different solution every time.
MongoDB with brew
On Mac, sometimes the MongoDB service will refuse connections from Lilium for obscure reasons. Our temporary solution it to start a mongod instance in a seperate terminal and add the desired parameters including the database path and authentation db. You also get an additional output stream from mongod.
There likely is a better solution to make this work with brew service, but like mentionned earlier, we don't actively support MacOS nor do we recommend to run Lilium in production on a different OS than Linux.
I want to code, too
That must mean you're amazing. See the Lilium CMS Wiki and get started!
Script mode
It is possible to run a Javascript file in script mode. It will prevent Lilium from loading the listeners, CAIJ and other modules related to networking. The websites and databases will still be loaded on a single thread, and the script passed as an argument will be executed.
node ./includes/runscript.js [script.js]
Random quote API
In older version, we used a random quote provider to add a cute message to The Daily Lilium, which is Lilium's newspaper.
http://api.forismatic.com/api/1.0/?method=getQuote&format=json&lang=en
Working with nginx
Nginx works well with Lilium. The following configuration is the most simplified version you can use. The config works with a Lilium instance located at /usr/share/lilium/lilium-cms, and runs at port 8080.
Since Lilium does not handle HTTPS requests, using nginx in front of Lilium makes it possible to have a fully operational SSL website running on Lilium, without the SSL overhead during the local proxy upstream.
Using Certbot or LetsEncrypt, you can quicky generate an HTTPS cert.
Basic nginx configuration file
upstream lilium_proxy  {
        server 127.0.0.1:8080;
        keepalive 64;
}

map $http_sec_websocket_key $upgr {
    """"      """";           # If the Sec-Websocket-Key header is empty, send no upgrade header
    default ""websocket"";  # If the header is present, set Upgrade to ""websocket""
}

map $http_sec_websocket_key $conn {
    """"      $http_connection;  # If no Sec-Websocket-Key header exists, set $conn to the incoming Connection header
    default ""upgrade"";         # Otherwise, set $conn to upgrade
}

proxy_cache_path /usr/share/lilium/html/nginxcache levels=1:2 keys_zone=my_cache:10m max_size=10g inactive=60m use_temp_path=off;

server {
        listen 80;

        server_name localhost;
        # port_in_redirect off;

        large_client_header_buffers 8 32k;
        index index.html;

        location / {
                proxy_cache my_cache;
                proxy_cache_revalidate on;
                proxy_cache_min_uses 3;
                proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
                proxy_cache_lock on;

                alias /usr/share/lilium/html/default/;
                try_files $uri $uri.html @lilium;
        }

        location /(lilium|login)/ {
                try_files @lilium =404;
        }

        location @lilium {
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header Host $http_host;
                proxy_set_header Host $host;
                proxy_set_header X-NginX-Proxy true;

                # prevents 502 bad gateway error
                proxy_buffers 8 32k;
                proxy_buffer_size 64k;

                proxy_pass http://lilium_proxy;
                proxy_redirect off;

                # enables WS support
                proxy_http_version 1.1;
                proxy_set_header Upgrade $upgr;
                proxy_set_header Connection $conn;
        }
}

Production version
4.1.0
Founder

Erik Desjardins

Contributors

Erik Desjardins
Samuel Rondeau-Millaire
Daniel McAuley
Gabriel Cardinal
Narcity Media inc.


License
Mozilla Public License Version 2.0
About the license
TL;DR : You can use the CMS to make money as long as it remains open source. The typical use case involves no additional work.
Both individuals and businesses are allowed to use Lilium CMS.
You are allowed to host a copy of Lilium CMS, modify it, redistribute it, create something different with it.
One important thing to note : you must disclose the source code, copyright, and must document any modification done. A copy of the license must be available. However, this does not mean you need to credit Narcity Media on your website or anything of the sort. Simply make the source code available and highlight the modifications if any.
That being said, you can still use Lilium CMS for almost any purposes.
Like most open source licenses, Narcity Media is not liable if anything happens with your server, data, etc. Lilium CMS does not come with a warranty.
The previous information is not legal advice, but should give you a good idea.
Mozilla Public License Version 2.0 is a simple, permissive license with conditions easy to respect. There have a great FAQ here.
Copyright
© Narcity Media, 2019
",5
guyellis/plant-image-lambda,JavaScript,"plant-image-lambda

Image Lambda for Plant Project
Data Flow
Notes
During the upgrade from Node 4 to Node 8 more dependencies were added. This caused the code
size (of the zip package) to increase from 2.4 MB to 5.4 MB. (July 1, 2018).
",2
tolo7010/hak.lnk,None,"hak.lnk
Project Name: hak.lnk
Description: Resource Links For Hackers
Author: tolo7010

General Web Penetration Guides / Courses / Tutorials
Mobile Penetration Guides / Courses / Tutorials
Tools
CTFs / Challenges / Labs
Vulnerability Database
Infosec News
Community Forums / Discussion Boards
Personal Blogs
Useful Git Projects

Please contact me if you see broken links or there are other interesting links which should be added.
",18
PCGen/pcgen,Java,"How to compile PCGen?
Check out our WIKI: http://wiki.pcgen.org/Building_PCGen
",247
Zongsoft/Zongsoft.CoreLibrary,C#,"Zongsoft.CoreLibrary
概述
Zongsoft.CoreLibrary 类库提供了.NET开发的常用功能集以及一些相对于.NET BCL中进行了功能强化的实现。采用C#语言开发，支持跨平台可运行在 Windows、Linux、Mac OSX 等平台中。

欢迎大家在 GitHub 上提交反馈给我们，为我们点赞(Star)。
如果你愿意帮助我们完善、翻译文档，或写范例代码都请致信给我：zongsoft@gmail.com、9555843@qq.com
项目结构


Common

该命名空间内包括一些常用的工具类。其中包括相对 .NET BCL 中进行了功能强化的 Convert 类，对枚举类型操作的 EnumUtility 类，ISequence序列器、IAccumulator累加器接口以及一些扩展类。



Collections

该命名空间内包括有关集合的类。其中包括相对.NET BCL中同名集合类进行了功能强化的 NamedCollectionBase、Collection<T>、Queue 类，以及表示树型层次结构的 HierarchicalNode、HierarchicalNodeCollection、Category、CategoryCollection 这些类，以及一个支持线程安全的提供对象池管理的 ObjectPool 类和支持指定容积的内存缓存ObjectCache。



Communication

该命名空间内包括进行通讯和自定义通讯协议包解析的基类、接口，设计通讯处理程序时应尽量使用这里定义的接口或基类。具体实现请参考 Zongsoft.Net 项目。


Composition

该命名空间内包括“执行管道(ExecutionPipelines)”模式的全部接口和实现，执行管道是一套强大的扩展接口方案，通讯层的 Communication.Net.TcpServer 和 Communication.Net.FtpServer 类均采用该机制来保证服务器端的扩展性。





ComponentModel

该命名空间内包括一些相对 .NET BCL 中进行了功能强化的 TypeConverter，譬如：EnumConverter、CustomBooleanConverter、GuidConverter 类；表示应用程序上下文的 ApplicationContextBase 类，该基类提供了一个应用程序可能会使用到的常用服务；其中的 AliasAttribute 类可用来定义枚举项或其他元素的别称。



Data

该命名空间内包括进行数据访问相关类和接口，我们提供了一个支持多库同时访问、横向分表的分布式关系型数据库ORM引擎，有关这个引擎的详细信息请访问 Zongsoft.Data 项目。



Diagnostics

该命名空间内包括日志处理、诊断跟踪相关的类和接口。



Expressions

该命名空间内包括一个表达式解析以及词法解析等功能实现。



IO

该命名空间内包括一个虚拟文件目录系统的功能集，使用该虚拟文件系统可隔离不同操作系统中IO处理的差异，并支持其他外部文件系统的扩展。具体实现可参考 Zongsoft.Externals.Aliyun 这个项目中的分布式文件存储部分。



Messaging

该命名空间内包含一个消息队列处理的抽象接口，具体实现可参考 Zongsoft.Externals.Aliyun 这个项目中的消息队列部分。



Options

该命名空间内包含了一套选项配置处理的类和接口，这套选项配置以树型结构来组织应用内的所有选项配置数据，访问这些配置数据以通用的逻辑路径的方式来进行。



Configuration

该命名空间内包括一套全新的配置文件的开发接口，该套接口完全兼容 .NET BCL 中的 System.Configuration 的编程模式。


为什么我们要重新编写一套类似的配置开发接口？因为 .NET BCL 自带的配置的机制太过臃肿复杂、并且扩展性也不是太友好，我们希望应用模块的配置应该和该应用模块一样是可被插件化的，它们必须可随意插拔并且保证模块之间的隔离性，当不同模块被组装在一起的时候，这些分离的选项配置数据将自动组织成一个完整的逻辑树。



Profiles

该命名空间内包括一套对 Windows 中 INI 配置文件的开发接口，并且支持对 Section 以层次结构的访问方式。





Reflection

该命名空间内包括一个对成员动态访问的类。



Resources

该命名空间内包括一个对资源处理的 ResourceUtility 工具类。



Runtime


Caching

该命名空间内包含 Buffer 和 Cache 这两种缓存机制的功能集。



BufferManager 提供了在频繁分配不确定大小的内存片段的场景下的一个很好的解决方案，譬如在 TCP 通讯中，接收端并发的收到各个发送端发送过来的数据片段，可以采用 BufferManager 来将这些临时数据片段保存起来待到整个数据包接收完成后再反馈给上层应用完整的数据包。




ICache 表示操作缓存的接口，MemoryCache 是它的一个内存缓存的实现，远程缓存案例可参考 Zongsoft.Externals.Redis 项目。




Serialization

该命名空间内包括了一套序列化和反序列化的相关类和接口，其中包括基于字典(Dictionary)的文本这两种常用序列化实现。由于 .NET BCL 中并没有提供关于序列化器的统一接口，所以使用 ISerializer 这个接口可以隔离特定技术的实现。通过 Serializer 类的 Json 属性可获得一个文本序列化器；通过 DictionarySerializer 类的 Default 属性可获得一个字典序列化器。





Security

该命名空间内包括一个 PasswordUtility 密码操作的工具类，以及与安全、授权相关的基类和接口。


Membership

该命名空间内包括一套完整的基于角色安全的授权管理接口，它还包含了一个最佳实践的方案。具体实现请参考 Zongsoft.Security 项目。





Services

该命名空间内包括一套服务访问和管理的 IServiceProvider、IServiceProviderFactory 接口和实现 ServiceProvider、ServiceProviderFactory；以及一套有关命令装配模式的接口和实现；还有一个后台服务的工作者 IWorker 接口和 WorkerBase 基类。



Terminals

该命名空间内包括一套终端程序的接口和实现，使用该实现可以快速的完成一个强大的基于控制台的应用。


Commands

该命名空间内包括关于终端程序的一些常用命令的实现类，譬如 ExitCommand、ClearCommand、HelpCommand 等类。





Text

该命名空间内包括一套基于正则表达式的文本表达式的解析、处理的类。



Transactions

该命名空间内包括有关事务的类和接口，有关应用事务支持的实现可参考 Zongsoft.Data 数据引擎 中的事务支持。



引用说明
本项目中的所有代码均未参考过任何特定实现，特此声明！
授权协议

Zongsoft.CoreLibrary 是基于 LGPL v2.1授权协议。
您可以将本项目应用于商业活动中，但是必须确保对本项目的完整（含版权声明）引用，不要分割或部分引用该项目的源码，我们保留追究违反授权协议的权利。


为什么要开源？
但求滴水之源敢汇大海之心。
",47
yin1999/code_sharing,C++,"此代码库用于保存我做题或通过其它途径接触到或者解决问题产生代码。
",2
igortorres17/SGSA,Java,"SGSA
Sistema de Gerenciamento de Serviços Automotivos (SGSA) é um software desenvolvido pelos estudantes Hércules Moreira, Igor Torres, Ricardo Otaviano e Matheus Cruvinel do curso de Análise e Desenvolvimento de Sistemas do Instituto Federal do Triângulo Mineiro. O programa foi proposto pelo professor da disciplina de Programação Visual, Eduardo Siqueira.
Recursos
Gerenciamento de Clientes, Serviços, Veículos e Peças

Cadastro
Visualização
Alteração
Exclusão

Ordem de Serviço (OS)

Emissão de Ordem de Serviço
Inclusão de Serviços e Peças na OS
Inclusão de Mecânico Responsável
Impressão de OS

Autenticação

Acesso através de nome de usuário e senha
Níveis de acesso (Permissão)

",2
Zongsoft/Zongsoft.CoreLibrary,C#,"Zongsoft.CoreLibrary
概述
Zongsoft.CoreLibrary 类库提供了.NET开发的常用功能集以及一些相对于.NET BCL中进行了功能强化的实现。采用C#语言开发，支持跨平台可运行在 Windows、Linux、Mac OSX 等平台中。

欢迎大家在 GitHub 上提交反馈给我们，为我们点赞(Star)。
如果你愿意帮助我们完善、翻译文档，或写范例代码都请致信给我：zongsoft@gmail.com、9555843@qq.com
项目结构


Common

该命名空间内包括一些常用的工具类。其中包括相对 .NET BCL 中进行了功能强化的 Convert 类，对枚举类型操作的 EnumUtility 类，ISequence序列器、IAccumulator累加器接口以及一些扩展类。



Collections

该命名空间内包括有关集合的类。其中包括相对.NET BCL中同名集合类进行了功能强化的 NamedCollectionBase、Collection<T>、Queue 类，以及表示树型层次结构的 HierarchicalNode、HierarchicalNodeCollection、Category、CategoryCollection 这些类，以及一个支持线程安全的提供对象池管理的 ObjectPool 类和支持指定容积的内存缓存ObjectCache。



Communication

该命名空间内包括进行通讯和自定义通讯协议包解析的基类、接口，设计通讯处理程序时应尽量使用这里定义的接口或基类。具体实现请参考 Zongsoft.Net 项目。


Composition

该命名空间内包括“执行管道(ExecutionPipelines)”模式的全部接口和实现，执行管道是一套强大的扩展接口方案，通讯层的 Communication.Net.TcpServer 和 Communication.Net.FtpServer 类均采用该机制来保证服务器端的扩展性。





ComponentModel

该命名空间内包括一些相对 .NET BCL 中进行了功能强化的 TypeConverter，譬如：EnumConverter、CustomBooleanConverter、GuidConverter 类；表示应用程序上下文的 ApplicationContextBase 类，该基类提供了一个应用程序可能会使用到的常用服务；其中的 AliasAttribute 类可用来定义枚举项或其他元素的别称。



Data

该命名空间内包括进行数据访问相关类和接口，我们提供了一个支持多库同时访问、横向分表的分布式关系型数据库ORM引擎，有关这个引擎的详细信息请访问 Zongsoft.Data 项目。



Diagnostics

该命名空间内包括日志处理、诊断跟踪相关的类和接口。



Expressions

该命名空间内包括一个表达式解析以及词法解析等功能实现。



IO

该命名空间内包括一个虚拟文件目录系统的功能集，使用该虚拟文件系统可隔离不同操作系统中IO处理的差异，并支持其他外部文件系统的扩展。具体实现可参考 Zongsoft.Externals.Aliyun 这个项目中的分布式文件存储部分。



Messaging

该命名空间内包含一个消息队列处理的抽象接口，具体实现可参考 Zongsoft.Externals.Aliyun 这个项目中的消息队列部分。



Options

该命名空间内包含了一套选项配置处理的类和接口，这套选项配置以树型结构来组织应用内的所有选项配置数据，访问这些配置数据以通用的逻辑路径的方式来进行。



Configuration

该命名空间内包括一套全新的配置文件的开发接口，该套接口完全兼容 .NET BCL 中的 System.Configuration 的编程模式。


为什么我们要重新编写一套类似的配置开发接口？因为 .NET BCL 自带的配置的机制太过臃肿复杂、并且扩展性也不是太友好，我们希望应用模块的配置应该和该应用模块一样是可被插件化的，它们必须可随意插拔并且保证模块之间的隔离性，当不同模块被组装在一起的时候，这些分离的选项配置数据将自动组织成一个完整的逻辑树。



Profiles

该命名空间内包括一套对 Windows 中 INI 配置文件的开发接口，并且支持对 Section 以层次结构的访问方式。





Reflection

该命名空间内包括一个对成员动态访问的类。



Resources

该命名空间内包括一个对资源处理的 ResourceUtility 工具类。



Runtime


Caching

该命名空间内包含 Buffer 和 Cache 这两种缓存机制的功能集。



BufferManager 提供了在频繁分配不确定大小的内存片段的场景下的一个很好的解决方案，譬如在 TCP 通讯中，接收端并发的收到各个发送端发送过来的数据片段，可以采用 BufferManager 来将这些临时数据片段保存起来待到整个数据包接收完成后再反馈给上层应用完整的数据包。




ICache 表示操作缓存的接口，MemoryCache 是它的一个内存缓存的实现，远程缓存案例可参考 Zongsoft.Externals.Redis 项目。




Serialization

该命名空间内包括了一套序列化和反序列化的相关类和接口，其中包括基于字典(Dictionary)的文本这两种常用序列化实现。由于 .NET BCL 中并没有提供关于序列化器的统一接口，所以使用 ISerializer 这个接口可以隔离特定技术的实现。通过 Serializer 类的 Json 属性可获得一个文本序列化器；通过 DictionarySerializer 类的 Default 属性可获得一个字典序列化器。





Security

该命名空间内包括一个 PasswordUtility 密码操作的工具类，以及与安全、授权相关的基类和接口。


Membership

该命名空间内包括一套完整的基于角色安全的授权管理接口，它还包含了一个最佳实践的方案。具体实现请参考 Zongsoft.Security 项目。





Services

该命名空间内包括一套服务访问和管理的 IServiceProvider、IServiceProviderFactory 接口和实现 ServiceProvider、ServiceProviderFactory；以及一套有关命令装配模式的接口和实现；还有一个后台服务的工作者 IWorker 接口和 WorkerBase 基类。



Terminals

该命名空间内包括一套终端程序的接口和实现，使用该实现可以快速的完成一个强大的基于控制台的应用。


Commands

该命名空间内包括关于终端程序的一些常用命令的实现类，譬如 ExitCommand、ClearCommand、HelpCommand 等类。





Text

该命名空间内包括一套基于正则表达式的文本表达式的解析、处理的类。



Transactions

该命名空间内包括有关事务的类和接口，有关应用事务支持的实现可参考 Zongsoft.Data 数据引擎 中的事务支持。



引用说明
本项目中的所有代码均未参考过任何特定实现，特此声明！
授权协议

Zongsoft.CoreLibrary 是基于 LGPL v2.1授权协议。
您可以将本项目应用于商业活动中，但是必须确保对本项目的完整（含版权声明）引用，不要分割或部分引用该项目的源码，我们保留追究违反授权协议的权利。


为什么要开源？
但求滴水之源敢汇大海之心。
",47
yin1999/code_sharing,C++,"此代码库用于保存我做题或通过其它途径接触到或者解决问题产生代码。
",2
igortorres17/SGSA,Java,"SGSA
Sistema de Gerenciamento de Serviços Automotivos (SGSA) é um software desenvolvido pelos estudantes Hércules Moreira, Igor Torres, Ricardo Otaviano e Matheus Cruvinel do curso de Análise e Desenvolvimento de Sistemas do Instituto Federal do Triângulo Mineiro. O programa foi proposto pelo professor da disciplina de Programação Visual, Eduardo Siqueira.
Recursos
Gerenciamento de Clientes, Serviços, Veículos e Peças

Cadastro
Visualização
Alteração
Exclusão

Ordem de Serviço (OS)

Emissão de Ordem de Serviço
Inclusão de Serviços e Peças na OS
Inclusão de Mecânico Responsável
Impressão de OS

Autenticação

Acesso através de nome de usuário e senha
Níveis de acesso (Permissão)

",2
Lombiq/Combinator,C#,"Combinator Orchard module Readme
Project Description
An Orchard CMS module that combines and minifies external stylesheets and javascript files to cut down on load times.
Features

Combines and minifies css files
Combines and minifies javascript files
If local and remote resources are mixed (like a local js files with one from a CDN) preserves their original order
Preserves conditional resources and minifies (if multiple with the same condition are after each other, also combines) them
Can combine remote (CDN) resources
Can embed images into stylesheets as data urls
Experimental image sprite generation support
Resource sets can be defined for better client-side caching: you can create sets of resources that are combined separately (e.g. all jQuery scripts can be in their individual file)
Ability to share processed resources between tenants in a multi-tenant application so a set of resources is only processed once, not for every tenant (resource sharing)
Busts browser cache when resources are updated (with a query string parameter containing a time stamp)
Ability to set custom resource domain
Exposing resource processing events
LESS and SASS preprocessors, contribution of Onestop Internet, Inc.
Command line command for emptying cache (""combinator empty"")
Info comment in bundled resources about which resources were combined
Tuned to be fast
With custom IStorageProvider can work in cloud hosting too (if there is no write access to the Media folder anyway)
Import/export settings
Administration page:

Adjust combination exclusion filter
Enable/disable combination of CDN resources
Set up resource domain
Enable/disable minification and adjust exclusion filter
Enable/disable image embedding and adjust exclusion filter
Enable/disable image sprite generation
Define resource sets
Enable/disable for admin site
Empty cache



The module is also available for DotNest sites.
You can download an install the module from the Orchard Gallery.
For known issues and future plans please see the Issue Tracker.
Please make sure to read the Documentation!
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/combinator (Mercurial repository)
https://github.com/Lombiq/Combinator (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",6
horyus/ethvtx,JavaScript,"


Ethereum-Ready & Framework-Agnostic Redux Store Configuration

Introduction
ethvtx is an Ethereum-Ready & Framework-Agnostic Redux configuration. This package contains all the tools to build an efficient Redux store for your Dapp. Our goal was to create a tool that will allow Dapp developers to efficiently fetch and manipulate informations about the Ethereum Blockchain. By minimizing the amount of requests and by caching and reusing as much data as possible, we decrease the impact that our apps have on the Ethereum nodes.
A complete set of dispatcher and getters are exposed to the developer and can be used directly inside any of the mapStateToProps or mapDispatchToProps functions to properly recover informations or emit actions.
The store handles transactions, accounts, contracts and blocks. Each single section has its own set of dispatchers and getters, and all are well documented in the official documentation,
Installation
npm install --save ethvtx redux redux-saga
More informations here
Documentation
An extensive usage documentation can be found here
Questions ?  !
Examples
Embark Showcase Project
There is an example project showcasing how to use ethvtx in an embark project.
It can be found here
React TS Showcase Application
The repository contains a complete React Typescript Showcase.
To setup the showcase, run:
git clone https://github.com/horyus/ethvtx
cd ethvtx
npm install
npm run build
cd examples
npm run setup
Then, from the examples directory, run:
npm run start
You can then visit the app from http://localhost:3000.
Be sure to have Metamask installed, and quadruple-check that you aren't on the Main Ethereum Network before testing transactions :) .
Status



Service
Status




Travis CI



Coveralls




",108
carlosedu13/APS_Landing_Page,CSS,"GreenTech the harmony of nature and innovation
We are a company based on the harmony between the environment and technology.

The Landing Page
Our landing page aims to guide people to a safe electronic disposal mode that still brings a point system so our taxpayers can reap rewards for caring for the environment.
Example materials

Old Mobile
Batteries
Cables
Broken TV
Broken DVD
Old iPod
Broken Notebook or/and PC
General Hardware


",3
phanrahan/mantle,Verilog,"Mantle


Mantle is part of the Magma ecosystem
of python programming tools for FPGAs.
Magma
is a programming model for building hardware.
The main abstraction in Magma is a Circuit.
Circuits are created and then wired together.
Magma circuits can be saved as structural verilog files.
Mantle
is a library of useful circuits.
Examples of mantle circuits are logic operators,
arithmetic operators,
comparison operators,
multiplexers,
decoders and encoders,
registers,
counters,
shift regiseters
and memory.
Loam
is used to model FPGAs, peripherals, parts (ICs) and boards.
Loam makes it easy to build applications
on a variety of different FPGA demonstration boards.
Currently, Mantle supports generic verilog
and the Lattice ice40
(and its open source icestorm toolchain).
A Xilinx (spartan3, spartan6, zynq) backends will be released soon.
An Altera backend is in the works.
Documentation
Documentation is hosted at http://magma-mantle.readthedocs.io/
You can also browse the markdown files contained in docs/ directly.

Combinational logic

Logical Operators
Arithmetic Operators
Comparison Operators
Multiplexers
Decoders, Encoders, and Arbiters


Sequential logic

Flip-flops and Register
Counters
Shift Registers


Memory

There also exist libraries for low-level FPGA-specific primitives.

ICE40 Primitives

Configuring Mantle
By default Mantle is configured to use the CoreIR implementation, equivalent to:
import magma as m
m.set_mantle_target(""coreir"")

Other options include: verilog and lattice.
Mantle can also be configured to synthesize low-level primitives
for a particular FPGA.
For example, to use mantle with the Lattice ice40,
set the MANTLE_TARGET  environment variable.
m.set_mantle_target(""ice40"")
Setup

Follow these instructions to install magma

$ git clone https://github.com/phanrahan/mantle
$ cd mantle
$ pip install pytest
$ pip install -e .
$ ./scripts/run_tests.sh  # this should pass

",18
Baltazar-Ortega/findme_base,TypeScript,"findme_base
Instrucciones
Entrar al proyecto y ejecutar
npm install
Reparticion de tareas
La reparticion de tareas irá cambiando segun se vayan terminando cosas
Comienzo de la aplicacion
Onboarding -> Autenticacion
Perdidos -> Dashboard (Cris)
Se puede ir

Pagina detalle de cada perro
Funcionalidad de cada card

(Rodolfo)

Descubrir como compartir en redes sociales una card de perro
Manual de uso de la aplicacion (10 paginas)
Presentacion de diapositivas

(Baltazar)

Formulario Encontrado
Formulario crear anuncio

Encontrados -> Dashboard (Charlie)

Cards encontrados
Página detalle encontrado

Logo y nombre de la aplicacion (Reyna)

Modal alguien encontró a tu perro

Workflow

UI - Todo lo visual. Usar imagenes sacadas de internet
Los datos se guardarán en arrays en los servicios, temporalmente
Implementacion de Firebase como base de datos
Autenticacion con Firebase
Adicion del feature de los mapas
onboarding
Refactorizacion del codigo para la version final
Publicacion

",2
cuthbertLab/music21,Python,"music21
music21 -- A Toolkit for Computational Musicology
Copyright © 2006-2019, Michael Scott Cuthbert and cuthbertLab
For more information, visit:
http://web.mit.edu/music21 or http://music21.readthedocs.org/en/latest/index.html
And to install, see:
http://web.mit.edu/music21/doc/usersGuide/usersGuide_01_installing.html
Music21 runs on Python 3.5+.  Version 4 was the last version to support Python 2
Released under either the BSD (3-clause) or GNU LGPL license according to your choice. See LICENSE.
Externally provided software (including the MIT licensed Lilypond/MusicXML test Suite) and
music encoding in the corpus may have different licenses. A no-corpus version of music21
is available also on GitHub


Mailing list
See: https://groups.google.com/forum/#!forum/music21list
Community Code of Conduct
Music21 encourages contributions, discussions, and usage from all people interested in
music and computers. This encouragement extends to all people regardless of (among other aspects)
gender, race, sexual orientation, disability, religion, appearance, veteran status,
gender identity, socioeconomic status, or nationality.
Members of the community will strive to be friendly, patient, and welcoming, especially of
viewpoints and experiences different from our own. We reject harassment and contributions
(in mail, comments, or code) that belittle individuals or groups of people.
We ask all members of the community to be mindful particularly about assumptions of the
gender of users (choice of pronouns in comments and code). We recognize that members
sometimes make mistakes and will, in general, accept sincere regrets for such cases.
Blatant or repeated violations of the code will result in the removal of the
contributor’s participation in the community.
The maintainers of music21 and associated sites will commit themselves to enforcing
this code of conduct. Users who notice violations, including instances of abuse,
harassment, or otherwise unacceptable behavior are requested to contact cuthbert@mit.edu.
Maintainers will respect confidentiality with regards to reports.
",839
donaldRwilliams/BGGM,R,"
BGGM
This package is described in Williams and Mulder (2019) and Williams (2018). The methods are separated into two Bayesian approaches for inference: hypothesis testing and estimation. The former is described in Williams and Mulder (2018a), and allows for testing for the presence of edges with the Bayes factor. One-sided hypothesis testing is also possible. These methods can also provide evidence for the null hypothesis. There are extensions for confirmatory hypothesis testing in GGMs, that can include inequality or equality constraints on the partial correlations. Further, it is possible to assess differences as well as similarities (i.e., the null hy-pothesis) between GGMs with the posterior predictive distribution and Bayesianmodel selection. The latter allows for testing hypothesized changes in graphicalstructures between, for example, control and treatment groups.
The estimation based methods are described in Williams (2018). The methods offer advantages compared to classical methods, in that a measure of uncertainty is provided for all parameters. For example, each node has a distribution for the variance explained (i.e., Bayesian R2). Measures of out-of-sample performance are also available, which also have a measure of uncertainty. The model is selected with credible interval exclusion of zero.
Williams, D. R. (2018, September 20). Bayesian Inference for Gaussian Graphical Models: Structure Learning, Explanation, and Prediction. (pre-print)
Williams, D. R., & Mulder, J. (2019, January 14). Bayesian Hypothesis Testing for Gaussian Graphical Models:Conditional Independence and Order Constraints. (pre-print)
Williams, D. R., Rast, P., Pericchi, L. R., & Mulder, J. (2019). Comparing Gaussian Graphical Models with the Posterior Predictive Distribution and Bayesian Model Selection. (pre-print)
Installation
You can install BGGM from git hub with:
# install.packages(""devtools"")
devtools::install_github(""donaldRwilliams/BGGM"")

Estimation
=============

1.1 Structure Learning
By structure learning we are referring to selecting the graph (i.e., the edge set E), which consists of those edges determinedto be non-zero. For demonstrative purposes, we consider a relatively small number of variables (p = 5). This The package BGGM offers a convenient analytic solution for estimating GGMs. It is implemented with:
# load pacakges
# install.packages(""devtools"")
# devtools::install_github(""donaldRwilliams/BGGM"")
library(BGGM)
library(ggplot2)
library(ggraph)

# p = 5
Y <- BGGM::bfi[,1:5]

# analytic solution
fit_analytic <- estimate(Y, analytic = T)
summary(fit_analytic)
#> BGGM: Bayesian Gaussian Graphical Models 
#> --- 
#> Type: Estimation (Analytic Solution) 
#> Posterior Samples: 
#> Observations (n): 2709 
#> Variables (p): 5 
#> Edges: 10 
#> --- 
#> Call: 
#> estimate.default(x = Y, analytic = T)
#> --- 
#> Date: Sat May 11 19:49:11 2019
Note summary(.) provides information about the fitted model, including that the analytic solution was used, the number of observations (n) and variables (p), and the number of edges.
The edge set is then selected with:
E <- select(fit_analytic, ci_width = 0.95)
summary(E)
#> BGGM: Bayesian Gaussian Graphical Models 
#> --- 
#> Type: Selected Graph (Analytic Solution) 
#> Credible Interval: 95 % 
#> Connectivity: 80 % 
#> --- 
#> Call:
#> select.estimate(x = fit_analytic, ci_width = 0.95)
#> --- 
#> Selected:
#>  
#> Partial correlations 
#>  
#>       1     2     3    4    5
#> 1  0.00 -0.24 -0.11 0.00 0.00
#> 2 -0.24  0.00  0.29 0.16 0.16
#> 3 -0.11  0.29  0.00 0.18 0.36
#> 4  0.00  0.16  0.18 0.00 0.12
#> 5  0.00  0.16  0.36 0.12 0.00
#> --- 
#>  
#> Adjacency 
#>  
#>   1 2 3 4 5
#> 1 0 1 1 0 0
#> 2 1 0 1 1 1
#> 3 1 1 0 1 1
#> 4 0 1 1 0 1
#> 5 0 1 1 1 0
#> ---
The analytic solution works directly with the precision matrix, and thus, there is not an option to summarize the posterior distributions. This is because the non-standardized elements are in the opposite direction (±) of the partial correlations, which in our experience, can lead to confusion. To summarize the posteriors change analytic = T to analytic = F:
fit_sampling <- estimate(Y, analytic = F)
E <- select(fit_sampling, ci_width = 0.95)
summary(E, summarize = T, digits = 2)
#> BGGM: Bayesian Gaussian Graphical Models 
#> --- 
#> Type: Selected Graph (Sampling) 
#> Credible Interval: 95 % 
#> Connectivity: 80 % 
#> --- 
#> Call:
#> select.estimate(x = fit_sampling, ci_width = 0.95)
#> --- 
#> Estimates: 
#>  
#>  egde post_mean post_sd   2.5%  97.5%
#>  1--2   -0.2400   0.018 -0.275 -0.204
#>  1--3   -0.1073   0.020 -0.145 -0.069
#>  2--3    0.2870   0.018  0.251  0.322
#>  1--4   -0.0074   0.019 -0.046  0.031
#>  2--4    0.1650   0.019  0.129  0.202
#>  3--4    0.1778   0.019  0.142  0.214
#>  1--5   -0.0089   0.019 -0.047  0.029
#>  2--5    0.1557   0.019  0.118  0.192
#>  3--5    0.3589   0.017  0.326  0.392
#>  4--5    0.1209   0.019  0.083  0.159
#> ---
Note that edge corresponds to that particular entry in the partial correlation matrix--i.e., 1--2 is the relation between the first and second variables, respectively.
BGGM provide several options for plotting, with each implemented as a S3 generic. For example, the partial correlations can be plotted with:
# p = 10
Y <- BGGM::bfi[,1:10]

# sampling required
fit_sampling <- estimate(Y, analytic = F)

# plot
plot_1A <- plot(fit_sampling, 
                ci_width = 0.95, 
                width = 0.1,  
                size = 2) +
            coord_cartesian() +
            theme(axis.text.x = element_text(angle = 90))
  
plot_1A

This example nicely demonstrates how the plot objects can be further customarized with ggplot2. There are two options for visualizing the selected graph. The heatmap plot is generated with:
# select the graph
E <- select(fit_sampling, ci_width = 0.95)

plot_1B <- plot(E, 
                type = ""heatmap"", 
                lower_tri = TRUE) +
           ggtitle(""Heatmap Plot"") + 
           theme(plot.title = element_text(size = 15))
plot_1B

Here lower_tri = TRUE controls which partial correlations are plotted. In this case, only the lower triangular elements are included in the plot. This can be changed with lower_tri = FALSE.
On the other hand, a “network” plot can be obtained with:
plot_1C <- plot(E, type = ""network"", 
                layout ='circle',
                node_outer = 8,
                node_inner = 7,
                node_text_size = 4) +
           ggtitle(""Network Plot"") +
           theme(plot.title = element_text(size = 15))
plot_1C

A key feature of BGGM is extending inference beyond identifying non-zero partialcorrelations. The region of practical equivalence can be used for this purpose, as it allowsfor determining which relations are practically zero. In this case, we follow Cohen’s guidelines, wherein 0.1 is considered asmall effect.This is implemented with:
# p = 10
Y <- BGGM::bfi[,1:10]

# sample from posterior
fit_sample <- estimate(Y, samples = 5000, analytic = F)

# select the graph
E <- select(fit_sample, rope = 0.1, prob = 0.95)
#> ci_width is ignored

# summary for first 10 rows
head(E, nrow = 10, summarize = T, digits = 2)
#> BGGM: Bayesian Gaussian Graphical Models 
#> --- 
#> Type: Selected Graph (Sampling) 
#> Probability: 0.95 
#> Region of Practical Equivalence:[-0.1, 0.1]
#> Connectivity: 31.1 % 
#> --- 
#> Call:
#> select.estimate(x = fit_sample, rope = 0.1, prob = 0.95)
#> --- 
#> pr_out: post prob outside of rope 
#> pr_in: post prob inside of rope 
#> --- 
#> Estimates: 
#>  
#>  egde post_mean post_sd pr_out  pr_in
#>  1--2    -0.244   0.019   1.00 0.0000
#>  1--3    -0.106   0.019   0.63 0.3652
#>  2--3     0.286   0.018   1.00 0.0000
#>  1--4    -0.015   0.019   0.00 1.0000
#>  2--4     0.161   0.019   1.00 0.0008
#>  3--4     0.161   0.019   1.00 0.0012
#>  1--5    -0.016   0.019   0.00 1.0000
#>  2--5     0.145   0.019   0.99 0.0120
#>  3--5     0.354   0.017   1.00 0.0000
#>  4--5     0.114   0.019   0.76 0.2376
#> ---
The argument prob = 0.95 requires that 95 % of the posterior density be in or out of the rope to be considered practically equivalent or different from zero. With this decision rule, as seen with head(.), edges 1--4 and 1--5 are practically equivalent to zero. This inference is made possible with BGGM.
In this case, plot(.) returns two objects: (1) the selected edges; (2) those for which there is support for the null values. These plots are displayed. This is implemented with:
plts <- plot(E, type = ""network"",
             layout ='circle',
             node_outer = 10, 
             node_inner = 9, 
             node_text_size = 6) 

plot_1D <- plts$plot_nonzero + 
             ggtitle(""Practically Non-zero"") +
             theme(plot.title = element_text(size = 15))
             
plot_1E <- plts$plot_zero + 
              ggtitle(""Practically Zero"") +
              theme(plot.title = element_text(size = 15))
             
cowplot::plot_grid(plot_1D, plot_1E)

We emphasize that GGMs are often thought to capture conditionally independent relations--i.e., evidence for the null hypothesis of no effect, conditional on the other variables in the model. However, the dominant approach assesses conditional dependence (ρi**j ≠ 0), and then sets the off-diagonal elements to zero otherwise. BGGM can explicitly answers the question of conditional independence.
3.2 Edge differences
Differences between partial correlations are often tested in GGMs; for example, with a classical (i.e., frequentist) approach that is implemented in bootnet. One contribution ofBGGMis providing Bayesian analogs for commonly used methods, as well as extensions to those methods. In this case, we can use posterior probabilities to determine which edges are practically equivalent. This is implemented with:
# edge differences
edge_difference <- edge_compare(fit_sample, contrast = ""all"", ci_width = 0.95, rope = 0.1)
#> ci_width is ignored for decision rule, but used in for plotting

# summary for first 5 rows
head(edge_difference, nrow = 5)
#> BGGM: Bayesian Gaussian Graphical Models 
#> --- 
#> Type: Edge comparison(s) 
#> Credible Interval: 95 % 
#> Region of Practical Equivalence:[-0.1, 0.1]
#> --- 
#> Call:
#> edge_compare.estimate(x = fit_sample, contrast = ""all"", ci_width = 0.95, 
#>     rope = 0.1)
#> --- 
#> Estimates: 
#>  
#>   contrast post_mean post_sd pr_out pr_in
#>  1--2-1--3    -0.137   0.031  0.886 0.114
#>  1--2-2--3    -0.529   0.024  1.000 0.000
#>  1--2-1--4    -0.229   0.029  1.000 0.000
#>  1--2-2--4    -0.404   0.026  1.000 0.000
#>  1--2-3--4    -0.404   0.027  1.000 0.000
#> ---
This output includes the posterior mean and standard deviation for each difference. Further, pr_out is the proportion of samples included between (±) 0.1. This can be interpreted as the posterior probability of practical equivalence, which has been defined with the argument rope = 0.1. Further, this powerful function can be used to assess specific contrasts. This can be accomplished, for example, with 5--1 - 6--10. Note that care must be taken when specifying the contrasts, as an error will arise if they are not in the proper format.
The object edge_difference can the be plotted with:
plot_diff <- plot(edge_difference, prob = .99)
plot_2A <- plot_diff$plt_nonzero + ggtitle(""Practically Different"") 


Hypothesis Testing
=====================


Comparing GGMs
=================


",2
systemd/systemd,C,"systemd - System and Service Manager







Details
General information about systemd can be found in the systemd Wiki.
Information about build requirements is provided in the README file.
Consult our NEWS file for information about what's new in the most recent systemd versions.
Please see the Hacking guide for information on how to hack on systemd and test your modifications.
Please see our Contribution Guidelines for more information about filing GitHub Issues and posting GitHub Pull Requests.
When preparing patches for systemd, please follow our Coding Style Guidelines.
If you are looking for support, please contact our mailing list or join our IRC channel.
Stable branches with backported patches are available in the stable repo.
",4796
earthlab/streamstats,Python,"StreamStats








Python package for interfacing with the USGS StreamStats API.

Free software: MIT license
Documentation: https://streamstats-python.readthedocs.io.


Features

Get the GeoJSON of the watershed containing a spatial point in the U.S.


Credits
This package was created with Cookiecutter and the audreyr/cookiecutter-pypackage project template.
",3
ucb-stat133/stat133-spring-2019,HTML,"Stat 133: Concepts in Computing with Data

Policies
Staff
Piazza
FAQ


Calendar

Instructor: Gaston Sanchez
Lecture: MWF 3:00-4:00pm VLSB 2050
Tentative calendar (weekly topics), subject to change depending on
the pace of the course.
Notes (📁) involves material discussed in class.
Reading (📖) involves material that expands lecture topics, as well as coding examples that you should practice on your own.
Misc (📰) is supporting material that is worth taking a look at.


0. Course Introduction

📇 Dates: Jan 22-25
📎 Topics: Welcome to Stat 133. We begin with the usual review of the course policies/logistics, expectations, topics in a nutshell, etc. Then, we move on with an unconventional introduction to computing with data using my favorite analogy ""Data Analysis is a lot like Cooking"".
📁 Notes:

Welcome to Stat 133
Data Analysis is a lot like cooking
Data Analysis Cycle: Example


📖 Reading:

Course policies
Piazza etiquette
FAQs


🔬 Lab: No lab
📰 Misc:

What is Data Science?


🔈 To Do:

Install R
Install RStudio Desktop (open source version, free)




1. The Big Picture and R Survival Skills

📇 Dates: Jan 28-Feb 01
📎 Topics: First things first. At the conceptual level we'll discuss how data analysis projects usually start with a Research Question. Also, we'll describe how Data can actually be seen from a triangular perspective (i.e. my ""3 Views of Data""). At the practical level, you'll begin learning basic survival skills for R, followed by an overall review of the RStudio workspace. Then we move on to discuss basic data types and their implementation in R around vectors and other data structures.
📁 Notes:

The Starting Point: Research Questions
The Three Views of Data
Be the Boss of your Data (talk and chalk)
Data Types and Vectors


📖 Reading:

First contact with R (tutorial)
Intro to Rmd files (tutorial)


🔬 Lab:

Getting started with R and RStudio (due Feb-01, open till Feb-17)


📰 Misc:

Introduction to R Markdown (by RStudio)


💡 Cheat sheet:

RStudio cheat sheet
R markdown cheat sheet


🎯 WARM-UP 1:

Markdown practice (due Feb-03, open till Feb-17)




2. More Data Structures: Arrays, Lists, and Dataframes

📇 Dates: Feb 04-08
📎 Topics: In this week you'll keep learning more about R data structures like arrays and lists. More specifically, we'll focus on fundamental concepts like atomicity, vectorization, recycling, and subsetting. And given that we are studying vectors and its cousins, we'll briefly review the traditional base graphics approach that is based on R vectors.
📁 Notes:

Arrays and Factors and Lists
Data Frames part 1 and part 2
Data Tables (introduction) and Spreadsheets


📖 Reading:

Intro to vectors (tutorial)
Intro to Data Technologies (preface, chapter 1, and chapter 5) (by Paul Murrell)


🔬 Lab:

Getting started with vectors and factors (due Feb-08, open till Feb-17)


📰 Misc:

chapter 20: Vectors (R for Data Science by Grolemund and Wickham)


💡 Cheat sheet:

Base R


🎯 WARM-UP 2:

Basic Data Objects (due Feb-10, open till Feb-17)




3. Transforming and Visualizing Tabular Data

📇 Dates: Feb 11-15
📎 Topics: Because data tables are so ubiquitous, you will have the chance to practice some data manipulation operations on data frames. Also, we'll discuss some considerations when importing tables (in R). Likewise, we begin a comprehensive discussion on concepts for data visualization.
📁 Notes:

Importing tables part 1 and part 2
Datavis: Classic Examples and Introduction
Datavis: Encoding Data in Graphs
Datavis: The Visual System


📖 Reading:

Organizing data in spreadsheets (by Karl Broman)
""dplyr"" tutorial slides (by Hadley Wickham)


🔬 Lab:

Data Frame Basics (due Feb-17)


📰 Misc:

tibbles vignette
Introduction to dplyr (by Hadley Wickham)


💡 Cheat sheet:

Data transformation cheat sheet
Data visualization with ggplot2


🎯 WARM-UP 3:

Basic Data Manipulation (due Feb-17)




4. More Visualization

📇 Dates: Feb 18-22 (Holiday Feb-18)
📎 Topics: We continue reviewing more concepts of data visualization. At the practical level, it's important that you learn how to manipulate them via R data frames in a more modern and syntactic way. How? By following the data plying framework provided by the package ""dplyr"".
📁 Notes:

Datavis: Using Color
Datavis: Effective Charts


📖 Reading:

""ggplot2"" lecture (by Karthik Ram)


🔬 Lab:

Data Wrangling and Graphics (due Feb-22)


📰 Misc:

Tidy Data (by Hadley Wickham)


💡 Cheat sheet:

Data transformation cheat sheet


🎯 WARM-UP 4:

More Data Wrangling (due Feb-27)




5. Housekeeping: Filesystem and Bash Commands

📇 Dates: Feb 25-Mar 01
📎 Topics: Data Analysis Projects (DAPs) are made of files and directories. Therefore, we need to review some fundamental concepts such as the file-system, the command line interface, and some basic shell commands.
📁 Notes:

Filesystem Basics
File Paths
Shell Basics
Command Line
Working with files


📖 Reading:

Linux Tutorial lessons 1-5 (by Ryan Chadwick)
The Unix Shell lessons 1-3 (by Software Carpentry)


🔬 Lab:

Command Line Basics (due Mar-01)


📰 Misc:

Linux Command Line tutorial (by Guru99)


💡 Cheat sheet:

command line cheat sheet


🎯 WORK-OUT 1:

GSW Shot Charts (due Mar-13)




6. Housekeeping: Version Control with Git and GitHub

📇 Dates: Mar 04-08
📎 Topics: We continue talking about filestructure topics, and we introduce basic notions of version control systems (VCS) using Git, and the companion hosting platform GitHub.
On the Data side, we begin our discussion about Tables: the most common form in which data is stored, handled, and manipulated. Consequently, we need to talk about the typical storage formats of tabular data, and the relationship between tables and R data frames.
📁 Notes:

Git Basics
Git Workflow


📖 Reading:

Read sections 4 to 9 in Part I Installation (Happy Git and GitHub for the useR by Jenny Bryan et al.)


🔬 Lab:

Git Basics (due Mar-08)


📰 Misc:

Data Import (R for Data Science by Grolemund and Wickham)


💡 Cheat sheet:

Data import cheat sheet
git cheat sheet


🎓 MIDTERM 1: Friday Mar-08


7. Transition to Programming Basics for Data Analysis (part 1)

📇 Dates: Mar 11-15
📎 Topics: You don’t need to be an expert programmer to be a data scientist, but learning more about programming allows you to automate common tasks, and solve new problems with greater ease. We'll discuss how to write basic functions, the notion of R expressions, and an introduction to conditionals.
📁 Notes:

Creating functions (tutorial)
Introduction to functions (tutorial)
Introduction to R expressions and conditionals (tutorial)


🔬 Lab:

Getting started with functions and conditionals (due Mar-15)


📰 Misc:

chapter 19: Functions (R for Data Science by Grolemund and Wickham)


🎯 WARM-UP 5:

Functions (due Mar-20)




8. Programming Basics for Data Analysis (part 2)

📇 Dates: Mar 18-22
📎 Topics: In addition to writing functions to reduce duplication in your code, you also need to learn about iteration, which helps you when you need to do the same operation several times. Namely, we review control flow structures such as for loops, while loops, repeat loops, and the apply family functions.
📁 Notes:

Introduction to loops (tutorial)
More about functions (tutorial)
Functions (Advanced R by H. Wickham)
Environments (Advanced R by H. Wickham)


🔬 Lab:

Getting started with loops (due Mar-22)


📰 Misc:

chapter 21: Iteration (R for Data Science by Grolemund and Wickham)


🎯 WARM-UP 6:

Loops and simulations (due Apr-03)




Spring Recess

📇 Dates: Mar 25-29
📎 Topics: Recharge your batteries


9. Testing Functions and Introduction to Shiny Apps

📇 Dates: Apr 01-05
📎 Topics: We begin with an introduction to the package ""testthat"" which provides a nice framework for testing functions. Jointly, we will discuss Shiny apps which provide an interesting companion to R, making it quick and simple to deliver interactive analysis and graphics on any web browser. In lab, you'll learn how to perform basic manipulation of strings.
📁 Notes:

Intro to testing functions (tutorial)
shiny tutorial (by Grolemund)


📖 Reading:

testthat: Get started with testing (by Wickham)
Character strings in R (r4strings by Sanchez)
Basic string manipulations (r4strings by Sanchez)


🔬 Lab:

Getting started with strings (due Apr-05)


📰 Misc:

chapter 14: Strings (R for Data Science by Grolemund and Wickham)


💡 Cheat sheet:

Stringr cheat sheet


🎯 WORK-OUT 2:

Shiny App (due Apr-17)




10. More Shiny Apps and Introduction to Regular Expressions

📇 Dates: Apr 08-12
📎 Topics: Random numbers have many applications in science and computer programming, especially when there are significant uncertainties in a phenomenon of interest. In this part of the course we'll look at some basic problems involving working with random numbers and creating simulations. Additionally, we continue the discussion about character strings with a first contact to Regular Expressions.
📁 Notes:

Introduction to random numbers
Coin toss shiny app
Regexpal tester tool.


📖 Reading:

Part 1 - How to build a Shiny app (video)


🔬 Lab:

Random numbers and simulations (due Apr-12)


📰 Misc:

Part 2 - How to customize reactions (video)
Part 3 - How to customize appearance (video)


💡 Cheat sheet:

shiny cheat sheet


🎯 WORK-OUT 2:

Keep working on your workout02 assignment.




11. More Regular Expressions

📇 Dates: Apr 15-19
📎 Topics: At its heart, computing involves working with numbers. However, a considerable amount of information and data is in the form of text. To unleash the power of strings manipulation, we need to take things to the next level and learn about Regular Expressions. Namely, Regular expressions are a tool that allows us to describe a certain amount of text called ""patterns"". We'll describe the basic concepts of regex and the common operations to match text patterns.
📁 Notes:

Long Jump World Record example
Log file example


📖 Reading:

Handling Strings in R (by Sanchez)


🔬 Lab:

Regular Expressions (due Apr-19)


💡 Cheat sheet:

Regular Expressions cheat sheet


🎯 WORK-OUT 3:

R Package (due May-03)




12. R packaging (part 1)

📇 Dates: Apr 22-26
📎 Topics: Packages are the fundamental units of reproducible R code. They include reusable functions, the documentation that describes how to use them, and sample data. In this part we'll start describing how to turn your code into an R package.
📁 Notes:

Programming S3 Classes
Methods (by Sanchez)


📖 Reading:

Package Structure (R packages by Wickham)
See package components: http://r-pkgs.had.co.nz/ (R packages by Wickham)


🔬 Lab:

HTML and Web scraping (due Apr-26)


💡 Cheat sheet:

Package Development cheat sheet


🎯 WORK-OUT 3:

R Package




13. R Packaging (part 2)

📇 Dates: Apr 29-May 03
📎 Topics: Creating an R package can seem overwhelming at first. So we'll keep working on the creation of a relatively basic package. This will give you the opportunity to apply most of the concepts seen in the course.
📁 Notes:

Pack YouR Code (by Sanchez)


📖 Reading:

See package components: http://r-pkgs.had.co.nz (R packages by Wickham)


🔬 Lab:

Take advantage of lab discussion to work on the workout03 assignment


💡 Cheat sheet:

Package Development cheat sheet


🎯 WORK-OUT 3:

Keep working on your workout03 assignment. (due May-03)




14. RRR Week and Final Exam

📇 Dates: May 06-10
📎 Topics: Prepare for final examination
📁 Notes:

No lecture. Instructor will hold OH (in 309 Evans)


🎓 FINAL: May-15th, 7-10 pm, in Wheeler 150

More details about the final will be posted on bCourses




",19
wz2cool/mybatis-dynamic-query,Java,"MyBatis Dynamic Query





The MyBatis Dynamic Query framework makes it easier to generate ""where"" and ""order"" expression dynamically in mapper xml.
mybatis-dynamic-query comes to solve four problem:

no need write lots of code in xml.
filtering or sorting maintained by java code.
hot update ""where"" and ""order"" expression.
save filter or sort descriptor and re-use them.

Docs
中文文档1.x
|
中文文档2.x
Database support

H2
MySql
SqlServer
Postresql
Oracle (TODO)

Maven
<dependency>
    <groupId>com.github.wz2cool</groupId>
    <artifactId>mybatis-dynamic-query</artifactId>
    <version>2.0.11</version>
</dependency>
Dynamic Query example

create two tables by sql.

DELETE FROM category;
INSERT INTO category (category_id, category_name, description) VALUES
  (1, 'Beverages', 'test'),
  (2, 'Condiments', 'test'),
  (3, 'Oil', 'test');

DELETE FROM product;
INSERT INTO product (product_id, category_id, product_name, price) VALUES
  (1, 1, 'Northwind Traders Chai', 18.0000),
  (2, 2, 'Northwind Traders Syrup', 7.5000),
  (3, 2, 'Northwind Traders Cajun Seasoning', 16.5000),
  (4, 3, 'Northwind Traders Olive Oil', 16.5000);

create a model map to this table.

public class ProductView {
    @Column(name = ""product_id"", table = ""product"")
    private Long productID;
    @Column(name = ""product_name"", table = ""product"")
    private String productName;
    @Column(name = ""price"", table = ""product"")
    private BigDecimal price;

    @Column(name = ""category_id"", table = ""category"")
    private Long categoryID;
    @Column(name = ""category_name"", table = ""category"")
    private String categoryName;
    @Column(name = ""description"", table = ""category"")
    private String description;

    // get, set method.
}

create a dynamic select in mapper interface / xml.

List<ProductView> getProductViewsByDynamic(Map<String, Object> params);
<select id=""getProductViewsByDynamic"" parameterType=""java.util.Map""
        resultType=""com.github.wz2cool.dynamic.mybatis.db.model.entity.view.ProductView"">
    SELECT
    <choose>
        <when test=""columnsExpression != null and columnsExpression !=''"">
            ${columnsExpression}
        </when>
        <otherwise>
            *
        </otherwise>
    </choose>
    FROM product LEFT JOIN category ON product.category_id = category.category_id
    <if test=""whereExpression != null and whereExpression != ''"">WHERE ${whereExpression}</if>
    <if test=""orderByExpression != null and orderByExpression != ''"">ORDER BY ${orderByExpression}</if>
</select>

generate expression and param map (NOTE: expression string also put into map).

@Test
public void testMultiTablesFilter() throws Exception {
    FilterDescriptor priceFilter1 =
            new FilterDescriptor(ProductView.class, ProductView::getPrice,
                    FilterOperator.GREATER_THAN_OR_EQUAL, 6);
    FilterDescriptor priceFilter2 =
            new FilterDescriptor(ProductView.class, ProductView::getPrice,
                    FilterOperator.LESS_THAN, 10);
    FilterDescriptor categoryNameFilter =
            new FilterDescriptor(ProductView.class, ProductView::getCategoryName,
                    FilterOperator.START_WITH, ""Co"");

    SortDescriptor idDescSort =
            new SortDescriptor(ProductView.class, ProductView::getProductID, SortDirection.DESC);

    Map<String, Object> params =
            // NOTE: we recommend you to set ""columnsExpressionPlaceholder""
            // in case of duplicated column name in two tables.
            // 这里你也可以不给列的站位，但是推荐使用，防止两个表有重复的名字
            MybatisQueryProvider
                    .createInstance(ProductView.class, ""columnsExpression"")
                    .addFilters(""whereExpression"",
                            priceFilter1, priceFilter2, categoryNameFilter)
                    .addSorts(""orderByExpression"", idDescSort)
                    .toQueryParam();

    List<ProductView> result = northwindDao.getProductViewsByDynamic(params);
    assertEquals(true, result.size() > 0);
}
output result
==>  Preparing: SELECT product.product_id AS product_id, product.price AS price, category.description AS description, category.category_name AS category_name, product.product_name AS product_name, category.category_id AS category_id 
FROM product LEFT JOIN category ON product.category_id = category.category_id WHERE (product.price >= ? AND product.price < ? AND category.category_name LIKE ?) 
==> Parameters: 6(Integer), 10(Integer), Co%(String)
<==    Columns: PRODUCT_ID, PRICE, DESCRIPTION, CATEGORY_NAME, PRODUCT_NAME, CATEGORY_ID
<==        Row: 2, 7.5000, test, Condiments, Northwind Traders Syrup, 2
<==      Total: 1
Dynamic Query Mapper
DynamicQueryMapper is based on tk.mybatis.mapper.
spring boot configuration

add dependency

<!-- base -->
<dependency>
    <groupId>com.github.wz2cool</groupId>
    <artifactId>mybatis-dynamic-query</artifactId>
    <version>2.0.2</version>
</dependency>
<!-- register mapper -->
<dependency>
    <groupId>tk.mybatis</groupId>
    <artifactId>mapper-spring-boot-starter</artifactId>
    <version>1.1.3</version>
</dependency>
<!-- mybatis -->
<dependency>
    <groupId>org.mybatis</groupId>
    <artifactId>mybatis</artifactId>
    <version>3.4.4</version>
</dependency>
<dependency>
    <groupId>org.mybatis.spring.boot</groupId>
    <artifactId>mybatis-spring-boot-starter</artifactId>
    <version>1.3.0</version>
</dependency>
<!-- spring boot web already has jackson-->
<!--  <dependency>
    <groupId>com.fasterxml.jackson.core</groupId>
    <artifactId>jackson-databind</artifactId>
    <version>2.9.0</version>
</dependency>-->
<!-- spring boot -->
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-web</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-aop</artifactId>
</dependency>
<dependency>
    <groupId>org.mybatis.spring.boot</groupId>
    <artifactId>mybatis-spring-boot-starter</artifactId>
    <version>1.2.0</version>
</dependency>

register DynamicQueryMapper in application.properties file.

mapper.mappers[0]=com.github.wz2cool.dynamic.mybatis.mapper.DynamicQueryMapper

scan mappers.

@SpringBootApplication
@MapperScan(basePackages = ""com.github.wz2cool.mdqtest.mapper"")
@EnableSwagger2
public class Application {
    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }
}
create mapper
public interface ProductDao extends DynamicQueryMapper<Product> {
}

",20
MarkMoHR/Awesome-Image-Colorization,None,"Awesome-Image-Colorization
A collection of Deep Learning based Image Colorization papers and demos, including Automatic and User Guided (i.e. with User Interaction) colorization, as well as video colorization.

Feel free to create a PR or an issue.


Outline

Automatic Image Colorization
User Guided Image Colorization

Based on color strokes
Based on reference color image
Based on color palette
Based on language(text)


Video Colorization

Automatically
Based on reference




1. Automatic Image Colorization



Paper
Source
Code/Project Link




Learning Large-Scale Automatic Image Colorization
ICCV 2015
[project] [code]


Deep Colorization
ICCV 2015



Learning Representations for Automatic Colorization
ECCV 2016
[project] [code]


Colorful Image Colorization
ECCV 2016
[project] [code]


Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification
SIGGRAPH 2016
[project] [code]


Unsupervised Diverse Colorization via Generative Adversarial Networks
ECML-PKDD 2017
[code]


Learning Diverse Image Colorization
CVPR 2017
[code]


Structural Consistency and Controllability for Diverse Colorization
ECCV 2018



Pixelated Semantic Colorization
1901.10889




2. User Guided Image Colorization
2.1 Based on color strokes



Image Type
Paper
Source
Code/Project Link




Manga
Manga colorization
SIGGRAPH 2006



Line art / Sketch
Auto-painter: Cartoon Image Generation from Sketch by Using Conditional Generative Adversarial Networks
1705.01908
[code]


Natural Gray-Scale
Real-Time User-Guided Image Colorization with Learned Deep Priors
SIGGRAPH 2017
[project] [code1] [code2]


Sketch
Scribbler: Controlling Deep Image Synthesis with Sketch and Color
CVPR 2017



Line art
User-Guided Deep Anime Line Art Colorization with Conditional Adversarial Networks
ACM MM 2018
[code]


Line art
Two-stage Sketch Colorization
SIGGRAPH Asia 2018
[code]


Natural Gray-Scale
Interactive Deep Colorization Using Simultaneous Global and Local Inputs (also palette based)
ICASSP 2019



Line art / Sketch
Outline Colorization through Tandem Adversarial Networks
Online Demo
[Demo] [code]


Line art
Paints Chainer
Online Demo
[Demo] [code]


Line art
Style2paints
Online Demo
[Demo] [code]


Manga
MangaCraft
Online Demo
[Demo]



2.2 Based on reference color image



Image Type
Paper
Source
Code/Project Link




Manga
Comicolorization: Semi-Automatic Manga Colorization (also palette based)
SIGGRAPH Asia 2017
[code]


Sketch
TextureGAN: Controlling Deep Image Synthesis with Texture Patches
CVPR 2018
[code]


Natural Gray-Scale
Deep Exemplar-based Colorization
SIGGRAPH 2018
[code]


Natural Gray-Scale
Example-Based Colourization Via Dense Encoding Pyramids (also palette based)
Pacific Graphics 2018
[code]


Natural Gray-Scale
A Superpixel-based Variational Model for Image Colorization
TVCG 2019



Natural Gray-Scale
Automatic Example-based Image Colourisation using Location-Aware Cross-Scale Matching
TIP 2019




2.3 Based on color palette



Image Type
Paper
Source
Code/Project Link




Natural Image
Palette-based Photo Recoloring
SIGGRAPH 2015
[project]


Manga
Comicolorization: Semi-Automatic Manga Colorization (also reference based)
SIGGRAPH Asia 2017
[code]


Natural Gray-Scale
Coloring with Words: Guiding Image Colorization Through Text-based Palette Generation (also text based)
ECCV 2018
[code]


Natural Gray-Scale
Example-Based Colourization Via Dense Encoding Pyramids (also reference based)
Pacific Graphics 2018
[code]


Natural Gray-Scale
Interactive Deep Colorization Using Simultaneous Global and Local Inputs (also strokes based)
ICASSP 2019




2.4 Based on language or text



Image Type
Paper
Source
Code/Project Link




Natural Gray-Scale / Sketch
Language-Based Image Editing with Recurrent Attentive Models
CVPR 2018
[code]


Natural Gray-Scale
Coloring with Words: Guiding Image Colorization Through Text-based Palette Generation (also palette based)
ECCV 2018
[code]


Scene Sketch
LUCSS: Language-based User-customized Colorization of Scene Sketches
1808.10544
[code]



3. Video Colorization
3.1 Automatically



Paper
Source
Code/Project Link




Fully Automatic Video Colorization with Self-Regularization and Diversity
CVPR 2019




3.2 Based on reference



Paper
Source
Code/Project Link




Switchable Temporal Propagation Network
ECCV 2018



Tracking Emerges by Colorizing Videos
ECCV 2018
[code]


Deep Exemplar-based Video Colorization
CVPR 2019




",11
pypa/warehouse,Python,"Warehouse
Warehouse is the software that powers PyPI.
See our development roadmap, documentation, and
architectural overview.

Getting Started
You can run Warehouse locally in a development environment using
docker and docker-compose. See Getting started
documentation for instructions on how to set it up.
The canonical deployment of Warehouse is in production at pypi.org.

Discussion
If you run into bugs, you can file them in our issue tracker.
You can also join the chat channels #pypa (general packaging
discussion and user support) and #pypa-dev (discussion about
development of packaging tools) on Freenode, or the pypa-dev
mailing list, to ask questions or get involved.

Testing
Read the running tests and linters section of our documentation to
learn how to test your code.  For cross-browser testing, we use an
open source account from BrowserStack. If your pull request makes
any change to the user interface, it will need to be tested to confirm
it works in our supported browsers.


Code of Conduct
Everyone interacting in the Warehouse project's codebases, issue trackers, chat
rooms, and mailing lists is expected to follow the PyPA Code of Conduct.
",2154
Lombiq/Lombiq-Fields,C#,"Lombiq Fields Orchard module readme
An Orchard CMS module that adds some useful content fields:

Media Library Upload Field: a modified version of Media Library Picker Field that enables users to upload files attached to content items.
Money Field: a field for storing money-related values such as amount and currency using the .NET Money type.

The fields have their separate features that you can enable; after this the fields will be available to be added to content types. Note that the fields also have content type-level settings.
The module is also available for DotNest sites.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/lombiq-fields (Mercurial repository)
https://github.com/Lombiq/Lombiq-Fields (Git repository)

This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",2
damng/hackernews-rss-with-inlined-content,Python,"hackernews-rss-inlined-content
Loads the hackerness rss and inlines the contents of the pages. Chrome with Selenium loads the page, dom-distiller makes the contents like they're in firefox's reader mode, and the resulting html is served as the entry description. The 300 or so entries become about 5mb. PDFs and things that yield no usible text preview will remain as they were on the old feed.
I invoke it as:
  xvfb-run python main.py

I used xvfb-run instead of headless mode because extensions are not supported in headless. One directory up, in ""../dat"", is a chrome user profile data directory that is copied for each instance of the browser run and deleted when finished. You can initialize it and add whatever extensions you want. The resulting rss file is then commited/pushed on here and served via gitpages.
This is hack level code.
The feed is available at https://damng.github.io/hackernews-rss-with-inlined-content/output.rss
If you find this useful, don't give me anything. Instead, go to http://templeos.org, donate to the TempleOS project and become a Templar. Dontate to the Brain and Behavior Research Foundation (https://www.bbrfoundation.org/).
",10
subspacecloud/subspace,HTML,"Subspace - A simple WireGuard VPN server GUI

Screenshots
Screenshot 1
Screenshot 2
Screenshot 3
Screenshot 4
Features

WireGuard VPN Protocol

The most modern and fastest VPN protocol.


Single Sign-On (SSO) with SAML

Support for SAML providers like G Suite and Okta.


Add Devices

Connect from Mac OS X, Windows, Linux, Android, or iOS.


Remove Devices

Removes client key and disconnects client.


Auto-generated Configs

Each client gets a unique downloadable config file.



Run Subspace on Portal Cloud
Portal Cloud is a hosting service that enables anyone to run open source cloud applications.
Sign up for Portal Cloud and get $15 free credit with code Portal15.
Run Subspace on a VPS
Running Subspace on a VPS is designed to be as simple as possible.

Public Docker image.
Single static Go binary with assets bundled.
Automatic TLS using Let's Encrypt.
Redirects http to https.
Works with a reverse proxy or standalone.

1. Get a server
Recommended Specs

Type: VPS or dedicated
Distribution: Ubuntu 16.04 (Xenial)
Memory: 512MB or greater

2. Add a DNS record
Create a DNS A record in your domain pointing to your server's IP address.
Example: subspace.example.com  A  172.16.1.1
3. Enable Let's Encrypt
Subspace runs a TLS (""SSL"") https server on port 443/tcp. It also runs a standard web server on port 80/tcp to redirect clients to the secure server. Port 80/tcp is required for Let's Encrypt verification.
Requirements

Your server must have a publicly resolvable DNS record.
Your server must be reachable over the internet on ports 80/tcp and 443/tcp and 51820/udp (WireGuard).

Usage
Example usage:
$ subspace --http-host subspace.example.com
Usage
  -backlink string
        backlink (optional)
  -datadir string
        data dir (default ""/data"")
  -debug
        debug mode
  -help
        display help and exit
  -http-addr string
        HTTP listen address (default "":80"")
  -http-host string
        HTTP host
  -http-insecure
        enable sessions cookies for http (no https) not recommended
  -letsencrypt
        enable TLS using Let's Encrypt on port 443 (default true)
  -version
        display version and exit
Run as a Docker container
Install WireGuard on the host
The container expects WireGuard to be installed on the host. The official image is subspacecloud/subspace.
add-apt-repository -y ppa:wireguard/wireguard
apt-get update
apt-get install -y wireguard

# Remove dnsmasq because it will run inside the container.
apt-get remove -y dnsmasq

# Set DNS server.
echo nameserver 1.1.1.1 >/etc/resolv.conf

# Load modules.
modprobe wireguard
modprobe iptable_nat
modprobe ip6table_nat

# Enable IP forwarding
sysctl -w net.ipv4.ip_forward=1
sysctl -w net.ipv6.conf.all.forwarding=1

Follow the official Docker install instructions: Get Docker CE for Ubuntu
Make sure to change the --env SUBSPACE_HTTP_HOST to your publicly accessible domain name.
# Your data directory should be bind-mounted as `/data` inside the container using the `--volume` flag.
$ mkdir /data

docker create \
    --name subspace \
    --restart always \
    --network host \
    --cap-add NET_ADMIN \
    --volume /usr/bin/wg:/usr/bin/wg \
    --volume /data:/data \
    --env SUBSPACE_HTTP_HOST=subspace.example.com \
    subspacecloud/subspace:latest

$ sudo docker start subspace

$ sudo docker logs subspace

<log output>

Updating the container image
Pull the latest image, remove the container, and re-create the container as explained above.
# Pull the latest image
$ sudo docker pull subspacecloud/subspace

# Stop the container
$ sudo docker stop subspace

# Remove the container (data is stored on the mounted volume)
$ sudo docker rm subspace

# Re-create and start the container
$ sudo docker create ... (see above)
Help / Reporting Bugs
Email support@portal.cloud
",285
subspacecloud/subspace,HTML,"Subspace - A simple WireGuard VPN server GUI

Screenshots
Screenshot 1
Screenshot 2
Screenshot 3
Screenshot 4
Features

WireGuard VPN Protocol

The most modern and fastest VPN protocol.


Single Sign-On (SSO) with SAML

Support for SAML providers like G Suite and Okta.


Add Devices

Connect from Mac OS X, Windows, Linux, Android, or iOS.


Remove Devices

Removes client key and disconnects client.


Auto-generated Configs

Each client gets a unique downloadable config file.



Run Subspace on Portal Cloud
Portal Cloud is a hosting service that enables anyone to run open source cloud applications.
Sign up for Portal Cloud and get $15 free credit with code Portal15.
Run Subspace on a VPS
Running Subspace on a VPS is designed to be as simple as possible.

Public Docker image.
Single static Go binary with assets bundled.
Automatic TLS using Let's Encrypt.
Redirects http to https.
Works with a reverse proxy or standalone.

1. Get a server
Recommended Specs

Type: VPS or dedicated
Distribution: Ubuntu 16.04 (Xenial)
Memory: 512MB or greater

2. Add a DNS record
Create a DNS A record in your domain pointing to your server's IP address.
Example: subspace.example.com  A  172.16.1.1
3. Enable Let's Encrypt
Subspace runs a TLS (""SSL"") https server on port 443/tcp. It also runs a standard web server on port 80/tcp to redirect clients to the secure server. Port 80/tcp is required for Let's Encrypt verification.
Requirements

Your server must have a publicly resolvable DNS record.
Your server must be reachable over the internet on ports 80/tcp and 443/tcp and 51820/udp (WireGuard).

Usage
Example usage:
$ subspace --http-host subspace.example.com
Usage
  -backlink string
        backlink (optional)
  -datadir string
        data dir (default ""/data"")
  -debug
        debug mode
  -help
        display help and exit
  -http-addr string
        HTTP listen address (default "":80"")
  -http-host string
        HTTP host
  -http-insecure
        enable sessions cookies for http (no https) not recommended
  -letsencrypt
        enable TLS using Let's Encrypt on port 443 (default true)
  -version
        display version and exit
Run as a Docker container
Install WireGuard on the host
The container expects WireGuard to be installed on the host. The official image is subspacecloud/subspace.
add-apt-repository -y ppa:wireguard/wireguard
apt-get update
apt-get install -y wireguard

# Remove dnsmasq because it will run inside the container.
apt-get remove -y dnsmasq

# Set DNS server.
echo nameserver 1.1.1.1 >/etc/resolv.conf

# Load modules.
modprobe wireguard
modprobe iptable_nat
modprobe ip6table_nat

# Enable IP forwarding
sysctl -w net.ipv4.ip_forward=1
sysctl -w net.ipv6.conf.all.forwarding=1

Follow the official Docker install instructions: Get Docker CE for Ubuntu
Make sure to change the --env SUBSPACE_HTTP_HOST to your publicly accessible domain name.
# Your data directory should be bind-mounted as `/data` inside the container using the `--volume` flag.
$ mkdir /data

docker create \
    --name subspace \
    --restart always \
    --network host \
    --cap-add NET_ADMIN \
    --volume /usr/bin/wg:/usr/bin/wg \
    --volume /data:/data \
    --env SUBSPACE_HTTP_HOST=subspace.example.com \
    subspacecloud/subspace:latest

$ sudo docker start subspace

$ sudo docker logs subspace

<log output>

Updating the container image
Pull the latest image, remove the container, and re-create the container as explained above.
# Pull the latest image
$ sudo docker pull subspacecloud/subspace

# Stop the container
$ sudo docker stop subspace

# Remove the container (data is stored on the mounted volume)
$ sudo docker rm subspace

# Re-create and start the container
$ sudo docker create ... (see above)
Help / Reporting Bugs
Email support@portal.cloud
",285
FengGuanxi/HDU-Experience,C++,"项目简介：
本项目意为向所有杭电学子提供各种信息，学习资料以及生活经验等。
薪火相传，只为更好的杭电。
总体结构：

学习

编译原理
C++
C语言
操作系统
Java
大学物理
大职
电子书
电路与模拟电子技术基础
电路原理
电路分析
概率论
高等数学
工程制图
考研
离散数学
计算机图形学
计算机系统结构
计算机组成原理
计算机网络
马原
数字图象处理
数字电路
数学建模
数据库原理甲
数据结构
算法导论
网络编程
软件工程
通信原理
图像处理与分析
visual basic
线性代数
信号与系统
现代经济管理基础
英语


杂项

欢迎提供资料或者建议(有偿)，邮箱（1007384211@qq.com）联系我
自愿鼓励(所有捐赠均将被用于购买学习资料)~




",52
132yse/fre,JavaScript,"
Fre
👻 Fast 1kB React like library with the same hooks API





Feature

🎉 really functionalComponent, hooks API, render props
🎊 Fiber Reconciler and hash keyed diff algorithm
🔭 minimal but wonderful , just 1 KB , no dependences

Introduction
Fre (pronounced /fri:/, like free) is a tiny and perfect js library, It means Free! ~
Use
yarn add fre
import { h, render, useState } from 'fre'

function Counter() {
  const [count, setCount] = useState(0)
  return (
    <div>
      <h1>{count}</h1>
      <button onClick={() => setCount(count + 1)}>+</button>
    </div>
  )
}

render(<Counter />, document.getElementById('root'))

Hooks API
react hooks API is a miracle, and fre will make it become a leading role
useState
useState is a base API, It will receive initial state and return a Array
You can use it many times, new state is available when component is rerender
function Counter() {
  const [up, setUp] = useState(0)
  const [down, setDown] = useState(0)
  return (
    <div>
      <h1>{up}</h1>
      <button onClick={() => setUp(up + 1)}>+</button>
      <h1>{down}</h1>
      <button onClick={() => setDown(down -1)}>-</button>
    </div>
  )
}

render(<Counter />, document.getElementById('root'))
useReducer
useReducer and useState are almost the same，but useReducer needs a global reducer
function reducer(state, action) {
  switch (action.type) {
    case 'up':
      return { count: state.count + 1 }
    case 'down':
      return { count: state.count - 1 }
  }
}

function Counter() {
  const [state, dispatch] = useReducer(reducer, { count: 1 })
  return (
    <div>
      {state.count}
      <button onClick={() => dispatch({ type: 'up' })}>+</button>
      <button onClick={() => dispatch({ type: 'down' })}>+</button>
    </div>
  )
}

render(<Counter />, document.getElementById('root'))
useEffect
useEffect takes two parameters, the first is a effect callback and the second is an array, usually props
When the array changes, the effect callback will run after commitWork
function Counter({ flag }) {
  const [count, setCount] = useState(0)
  useEffect(() => {
    document.title = 'count is ' + count
  }, [flag])
  return (
    <div>
      <h1>{count}</h1>
      <button onClick={() => setCount(count + 1)}>+</button>
    </div>
  )
}

render(<Counter />, document.getElementById('root'))
useMemo
useMemo has the same parameters as useEffect, but useMemo will be ran immediately.
function Counter() {
  const [count, setCount] = useState(0)
  return (
    <div>
      <h1>{count}</h1>
      <button onClick={() => setCount(count + 1)}>+</button>
      {(useMemo(<Sex />), [])}
    </div>
  )
}

render(<Counter />, document.getElementById('root'))
useContext
Context is the state of external create, internal use
When it changes, all components that own useContext will rerender
const ctx = createContext(0)

function App() {
  const [count, setCount] = useContext(ctx)
  return (
    <div>
      <h1>{count}</h1>
      <button onClick={() => setCount(count + 1)}>+</button>
      <Other />
    </div>
  )
}

function Other() {
  const count = useContext(ctx)[0]
  return <h1>{count}</h1>
}
FunctionalComponent
functionalComponent is a new components scheme
function App() {
  const [count, setCount] = useState(0)
  return (
    <div>
      <h1>{count}</h1>
      <button onClick={() => setCount(count + 1)}>+</button>
      <Sex count={count}/>
    </div>
  )
}

function Sex(props){
  const [sex, setSex] = useState('boy')
  return (
    <div>
      <h2>{props.count}</h2>
      <h1>{sex}</h1>
      <button onClick={() => {sex==='boy'?setSex('girl'):setSex('boy')}}>x</button>
    </div>
  )
}

render(<App />, document.getElementById('root'))
props
Props are used for component communication
function App() {
  const [sex, setSex] = useState('boy')
  return (
    <div>
      <Sex sex={sex} />
      <button
        onClick={() => (sex === 'boy' ? setSex('girl') : setSex('boy'))}
      />
    </div>
  )
}
function Sex(props) {
  return <div>{props.sex}</div>
}
Props contains children to render all the child elements of itself
const HelloBox = () => (
  <Box>
    <h1>Hello world !</h1>
  </Box>
)

const Box = props => <div>{props.children}</div>
Hooks do not support HOC and extends, but render props/children are supported by default
const HelloBox = () => (
  <Box>
    {value => {
      return <h1>{value}</h1>
    }}
  </Box>
)

const Box = props => <div>{props.children('hello world!')}</div>
Fiber
Fiber is a priority scheduling scheme.
It uses the traversal form of linked list to achieve time slicing
hash.keyed diff
Fre implements a compact diff algorithm
It uses hash to mark locations for easy comparison
JSX
The default export h function needs to be configured
import { h } from 'fre'
{
  ""plugins"": [
    [""transform-react-jsx"", { ""pragma"":""h"" }]
  ]
}
If it is a browser environment, recommend to use htm
License
MIT ©132yse inspired by anu
",415
haskell-works/cabal-cache,Haskell,"cabal-cache

Tool for caching built cabal new-build packages.
The tool is useful in development when you want to share your build haskell package dependencies of
of a particular project with another developer and also in CI where caching is useful for reducing
build times.
cabal-cache supports syncing to an archive directory or to an S3 bucket.
Installation
Several installation methods are available.
From source
cabal new-install cabal-cache
Ubuntu binaries
Dowload Ubuntu binaries from https://github.com/haskell-works/cabal-cache/releases
Using Homebrew on Mac OS X
brew tap haskell-works/homebrew-haskell-works git@github.com:haskell-works/homebrew-haskell-works.git
brew update
brew install cabal-cache
Example usage
Syncing built packages with S3 requires you have an S3 bucket with AWS
credentials stored in the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environent variables.
You should also know the AWS region the bucket was created in.
Sync to archive
Change into your project directory.
Build the project with cabal v2-build.  This will ensure your dependencies are built and
will produce a plan.json file that is required for the cabal-cache tool to know which built
packages to sync up.
Run the following command to sync to S3.
cabal-cache sync-to-archive --threads 16 --archive-uri s3://my-cabal-cache-bucket/archive --region Sydney
Run the following command to sync to archive directory.
cabal-cache sync-to-archive --threads 16 --archive-uri archive --region Sydney
Sync from S3
Change into your project directory.
Build the project with cabal v2-configure.  This will product a plan.json file that is required
for the cabal-cache tool to know which built packages to sync down.
Run the following command to sync from S3.
cabal-cache sync-from-archive --threads 16 --archive-uri s3://my-cabal-cache-bucket/archive --region Sydney
Run the following command to sync from archive directory.
cabal-cache sync-from-archive --threads 16 --archive-uri archive --region Sydney
The archive
Archive tarball format
Built packages are stored in tarballs which contain the following files:
x ${compiler_id}/${package_id}/_CC_METADATA/store-path
x ${compiler_id}/lib/libHS${package_id}-*.dylib
x ${compiler_id}/${package_id}
x ${compiler_id}/package.db/${package_id}.conf
Aside from the files in the _CC_METADATA directory, everything else is copied verbatim from cabal
store from the corresponding location.  This includes the conf file which may contain absolute paths
that would cause the built package to be non-relocatable.
As a work-around, the tarball also inclues the _CC_METADATA/store-path
file which stores the cabal store path from which the cached package was derived.
Upon unpacking, cabal-cache will rewrite the conf file to contain the new store path using the
information store in the _CC_METADATA/store-path file.  _CC_METADATA directory and its contents
will be additionally unpacked making it easy to recognise packages that have been restored using
cabal-cache.
Archive directory structure
The archive contains files in the following locations:
/Users/jky/moo-archive/${archive_version}/${compiler_id}/${package_id}.tar.gz
/Users/jky/moo-archive/${archive_version}/${store_hash}/${compiler_id}/${package_id}.tar.gz
Both tarballs are identical.  If they both exist then the first may be a symlink to the second
when store on the filesystem.
The direct subdirectories of the archive is the ${archive_verson}, for example v1.  This is the
version of the archive format.  This corresponds to the major version of the cabal-cache package.
The next directory may be the ${store_hash} or the ${compiler_id}.  If it is the ${store_hash}
then the ${compiler_id} will be a subdirectory of that.
The ${store_hash} is the hash of the store path from which the cached package originally came.
cabal-cache will preferentially restore using this version if it is available and the ${store_hash}
matches the cabal store path that is being restore to.
If the package matching the ${store_hash} cannot be found, cabal-cache will fallback to the version
without the ${store_hash}.
A version without a ${store-hash} may not exist.  See Caveats for more information.
Caveats
Packages that use absolute paths to the cabal store
Packages sometimes do things that cause their built artefacts to contain absolute paths to the cabal
store.  This unfortunately makes such built packages non-relocatable.
It is recommended that you use a fixed cabal store path rather than the default $HOME/.cabal/store
to avoid any potential issues.
See https://github.com/haskell/cabal/issues/4097 for more information.
Following are examples of how this might happen:
Paths_$pkgname
Paths_$pkgname modules have embedded within them the absolute path to the package in the cabal store
which means that packages that use some features of this module are not relocatable depending on what
they do.
Packages may query this module to get access to the package's cabal store share directory which
contains data files that the package can read at runtime.  Using cabal-cache for such packages
could mean that the package will be unable to find such data files.
To protect against this, cabal-cache will by default not sync packages down from the archive
if the package's cabal store share directory contain unusual files or directories unless the
${store_hash} matches.  Currently it only considers the doc subdirectory to be usual.  More
exceptions may be added later.
",22
SplendidStrontium/splendidstrontium.github.io,CSS,"splendidstrontium.github.io
@TODO

sitemap.html

",2
mfrasco/Metrics,R,"Metrics



How to Install this Package
This package is distributed from CRAN. From the R prompt, run install.packages(""Metrics"").
Metrics Repo
This repository contains code for the Metrics package in R. Metrics was created by Ben Hamner and came from this github repo. Hamner's repo contains packages for common machine learning metrics in several programming languages, not just R. On 2017-04-21, CRAN orphaned the R package. To revive the status of the R package, I cloned the original and created this repo. I have added new metrics, improved documentation, and fixed bugs. This repository will be the home of active development on the Metrics R package moving forward.
Community Feedback
If you notice anything wrong with the Metrics package or have any ideas on how to improve it, please create an issue in this github repository that describes your issue. I also welcome improvements to this package via a pull request. This is a simple R package, which makes it perfect for first time open source contributors. Here is a guide that walks you through how to make an open source contribution.
What Metrics are Included in this Package?
All functions in the Metrics package take at least two arguments: actual and predicted. In the table below, I abbreviate actual as x and predicted as y for the sake of mathematical brevity.



Metric Type
Metric Name
Function Name
Formula




regression
Squared Error
se



regression
Mean Squared Error
mse



regression
Root Mean Squared Error
rmse



regression
Absolute Error
ae



regression
Mean Absolute Error
mae



regression
Absolute Percent Error
ape



regression
Mean Absolute Percent Error
mape



regression
Symmetric Mean Absolute Percent Error
smape



regression
Squared Log Error
sle



regression
Mean Squared Log Error
msle



regression
Root Mean Squared Log Error
rmsle



regression
Relative Squared Error
rse



regression
Root Relative Squared Error
rrse



regression
Relative Absolute Error
rae



time series
Mean Absolute Scaled Error
mase



classification
Classification Error
ce



classification
Accuracy
accuracy



classification
F1 Score
f1



binary classification
Area Under ROC Curve
auc
. help(auc) for details.


binary classification
Log Loss
ll



binary classification
Mean Log Loss
logloss



binary classification
Precision
precision



binary classification
Recall
recall



binary classification
F-beta Score
fbeta_score




",69
Ultimate-Hosts-Blacklist/www.shallalist.de,Python,"About www.shallalist.de


About Ultimate-Hosts-Blacklist
Ultimate-Hosts-Blacklist serve a place to test and keep a track on each input sources that are present into Ultimate Hosts Blacklist.
As Ultimate Hosts Blacklist grew up it became impossible to test the whole repository, as it takes weeks to finish. That's why we use the GitHub organization system in order to create different repository for each list that are present into Ultimate Hosts Blacklist.

About PyFunceble
PyFunceble like Funceble is A tool to check domains or IP availability by returning 3 possible status: ACTIVE, INACTIVE or INVALID.
It also has been described by one of its most active user as:

[An] excellent script for checking ACTIVE, INACTIVE and EXPIRED domain names.

If you need further informations about PyFunceble or Funceble please report to our Wiki page and/or if you don't find any answer feel free to create an issue into one of the Dead Hosts's or Py-Funceble's repositories.
About the status returned by PyFunceble
For an up to date version of this part please report to the Status section of our Wiki.
ACTIVE
This status is returned when one of the following cases is met:


We can extract the expiration date from Lookup().whois().

Please note that we don't check if the date is in the past.



Lookup().nslookup() don't return server can't find domain-name.me: NXDOMAIN.


HTTOCode().get() return one the following code [100, 101, 200, 201, 202, 203, 204, 205, 206].


INACTIVE
This status is returned when all the following cases are met:

We can't extract the expiration date from Lookup().whois().
Lookup().nslookup() return server can't find domain-name.me: NXDOMAIN.

INVALID
This status is returned when the following case is met:


Domain extension has an invalid format or is unregistered in IANA Root Zone Database.

Understand by this that the extension is not present into the iana-domains-db.json file.



",2
baguette/crux-ports,Shell,"crux-ports
Some ports for the CRUX operating system. This README was modified from CRUX MATE's.
Quickstart
This git repository is also a ports collection. You can enable it like so:
On your CRUX Linux target installation download the file /etc/ports/baguette.httpup:
curl https://raw.githubusercontent.com/baguette/crux-ports/master/baguette.httpup -o /etc/ports/baguette.httpup

Enable the contrib ports collection rsync in /etc/ports if it's not already enabled:
mv contrib.rsync.inactive contrib.rsync

Edit the file /etc/prt-get.conf and enable the contrib collection (it is needed for some dependencies). Then in /etc/prt-get.conf put
prtdir /usr/ports/baguette 

Then issue
ports -u

at the command line to get the ports collection (and any updates to other port collections)
Installing the awesome window manager
Issue the following command:
prt-get depinst awesome

When this completes (it will take some time - assuming there are no problems or conflicts), enable the dbus daemon at startup in your /etc/rc.conf file (example below)
SERVICES=(dbus net crond) 

If using startx configure your .xinitrc with
exec awesome

Then issue 'startx' at the command line. Assuming kernel compile options are correct you should find yourself with a working awesome desktop (basic).
Adwaita
The Adwaita GTK+ theme is included in the gnome-themes-standard package.  The Adwaita icon theme is included in the adwaita-icon-theme package. To use them with GTK+2, add the following lines to /usr/etc/gtk-2.0/gtkrc
gtk-icon-theme-name = ""Adwaita""
gtk-theme-name = ""Adwaita""

Haskell Platform
The haskell-platform-bin port provides the Haskell Platform. It installs into the non-standard location /usr/local/haskell. This is normally a big no-no for CRUX, but I could not figure out how to get it to install in any other location and still work properly.
Once the package is installed, you need to add /usr/local/haskell/ghc-7.10.3-x86_64/bin to your PATH to use it.
Issues
If anything doesn't work as described in this README, please report your issues.
",3
baguette/crux-ports,Shell,"crux-ports
Some ports for the CRUX operating system. This README was modified from CRUX MATE's.
Quickstart
This git repository is also a ports collection. You can enable it like so:
On your CRUX Linux target installation download the file /etc/ports/baguette.httpup:
curl https://raw.githubusercontent.com/baguette/crux-ports/master/baguette.httpup -o /etc/ports/baguette.httpup

Enable the contrib ports collection rsync in /etc/ports if it's not already enabled:
mv contrib.rsync.inactive contrib.rsync

Edit the file /etc/prt-get.conf and enable the contrib collection (it is needed for some dependencies). Then in /etc/prt-get.conf put
prtdir /usr/ports/baguette 

Then issue
ports -u

at the command line to get the ports collection (and any updates to other port collections)
Installing the awesome window manager
Issue the following command:
prt-get depinst awesome

When this completes (it will take some time - assuming there are no problems or conflicts), enable the dbus daemon at startup in your /etc/rc.conf file (example below)
SERVICES=(dbus net crond) 

If using startx configure your .xinitrc with
exec awesome

Then issue 'startx' at the command line. Assuming kernel compile options are correct you should find yourself with a working awesome desktop (basic).
Adwaita
The Adwaita GTK+ theme is included in the gnome-themes-standard package.  The Adwaita icon theme is included in the adwaita-icon-theme package. To use them with GTK+2, add the following lines to /usr/etc/gtk-2.0/gtkrc
gtk-icon-theme-name = ""Adwaita""
gtk-theme-name = ""Adwaita""

Haskell Platform
The haskell-platform-bin port provides the Haskell Platform. It installs into the non-standard location /usr/local/haskell. This is normally a big no-no for CRUX, but I could not figure out how to get it to install in any other location and still work properly.
Once the package is installed, you need to add /usr/local/haskell/ghc-7.10.3-x86_64/bin to your PATH to use it.
Issues
If anything doesn't work as described in this README, please report your issues.
",3
goragod/GCMS,PHP,"GCMS 13.3.0
GCMS เป็น Ajax CMS สมบูรณ์แบบ ที่พัฒนาโดยคนไทยทั้งระบบ เพื่อให้การใช้งาน CMS แบบ Ajax เป็นเรื่องที่ง่ายขึ้น และแก้ปัญหาของ Ajax ที่เป็นที่กังวล ทั้งในเรื่องการรองรับหลายบราวเซอร์, การปิดการใช้งาน Javascript, การ Refresh หน้า, การ Bookmark, การใช้งานปุ่ม History ของบราวเซอร์ และที่สำคัญคือ SEO
GCMS เหมาะสมสำหรับการทำเว็บไซต์ทั่วไป ทั้งเว็บไซต์ส่วนบุคคล หน่วยงานราชการ หรือเว็บไซต์อื่นๆ ตั้งแต่ขนาดเล็ก ไปจนถึงเว็บไซต์ขนาดใหญ่ ด้วยการออกแบบที่เน้นอิสระในการปรับปรุงหน้าเว็บ ไม่มุ่งเน้นไปที่เว็บไซต์แบบใดแบบหนึ่งโดยเฉพาะ และมีคุณสมบัติเบื้องต้นต่อการออกแบบเว็บไซต์ทั่วไปได้ง่าย นอกจากนี้ GCMS ยังเหมาะสมเป็นอย่างยิ่ง สำหรับผู้ที่ชอบออกแบบเว็บด้วยตัวเอง ด้วยการใช้ GCMS เป็นพื้นฐานของเว็บเนื่องจาก GCMS มีระบบพื้นฐานครบถ้วน และออกแบบโมดูลตามความต้องการเพื่อใช้งานร่วมกับ GCMS ซึ่งทำให้ประหยัดเวลาในการออกแบบเว็บลงเป็นอย่างมาก
หากคุณมีข้อสงสัย ข้อเสนอแนะ หรือต้องการความช่วยเหลือเกี่ยวกับ GCMS คุณสามารถหาความช่วยเหลือนั้นได้ที่ https://goragod.com และคุณสามารถแสดงความคิดเห็นของคุณต่อ GCMS รวมถึงปัญหาในการใช้งาน ตลอดจนข้อผิดพลาดต่างๆ ของ GCMS ได้ ความคิดเห็นของคุณจะช่วยผมในการพัฒนา GCMS ให้ดียิ่งขึ้นในโอกาศต่อไป

เว็บไซต์หลัก GCMS http://gcms.in.th
ตัวอย่างเว็บไซต์ทั่วไป http://demo.gcms.in.th
ตัวอย่างเว็บโรงเรียนหรือ อบต. http://school.gcms.in.th

ข้อตกลงการนำ GCMS ไปใช้งาน

GCMS เป็น Ajax CMS ชนิด Open Source ที่แจกจ่ายให้กับทุกคนสามารถนำไปใช้งานได้ ฟรี โดยมีเงื่อนไขว่า ต้องติดข้อความหรือเครื่องหมายของผู้พัฒนาไว้เสมอ (https://goragod.com และ http://www.webshopready.com)
ห้ามนำ GCMS หรือ ส่วนหนึ่งส่วนใดของ GCMS ไป จำหน่าย จ่าย แจก หรือ ใช้งานกับบุคคลที่สาม เว้นแต่จะได้รับความยินยอมจากผู้พัฒนา
คุณสามารถพัฒนาต่อยอด เพิ่มเติม แก้ไข หรือ ดัดแปลง GCMS ได้เพื่อการใช้งานส่วนบุคคล โดยต้องติดข้อความหรือโลโกของผู้พัฒนาไว้เสมอไม่ว่าจะเปลี่ยนแปลงไปมากน้อยแค่ไหนก็ตาม
คุณสามารถพัฒนา โมดูลเสริม หรือ วิดเจ็ท หรือ ส่วนประกอบอื่นใด (นอกจากที่มีแจกจ่ายโดยผู้พัฒนาอยู่แล้ว) เพื่อใช้หรือเพื่อจำหน่าย จ่าย แจกได้ โดยให้สิทธิ์เป็นของผู้พัฒนาเอง (ขายหรือแจกเฉพาะโมดูล)
หากคุณต้องการนำ GCMS ไปใช้กับบุคลที่สาม หรือ เพื่อขาย หรือต้องการนำเครื่องหมายหรือข้อความของผู้พัฒนาออก คุณสามารถติดต่อกับผู้พัฒนาได้โดยตรง ตามที่อยู่ใน https://goragod.com

การติดตั้งและอัปเกรด

ดาวน์โหลดโค้ดทั้งหมดจาก Github
การอัปเกรด ต้องดาวน์โหลด Theme ไปติดตั้งใหม่ด้วย
แตกไฟล์และอัปโหลดไฟล์ทั้งหมดไปยัง Server
ติดตั้งหรืออัปเกรด GCMS โดยเรียก http://domain.tld/install/ และทำตามขั้นตอนต่างๆที่ตัวอัปเกรดแจ้ง
ทดสอบเรียกเว็บไซต์ โดยเข้าระบบแอดมิน
ลบไดเร็คทอรี่ install/ ออก

ในกรณีที่เป็นการอัปเกรด หลังจากการอัปเกรดเสร็จสิ้น สมาชิกทุกคนจะต้องขอรหัสผ่านใหม่ หากแอดมินไม่สามารถเข้าระบบได้ ให้ดำเนินการดังนี้
เปิดตาราง xxx_user (xxx คือ prefix ของฐานข้อมูล) มองหารายการ id 1 (หรือรายการอีเมล์ของตัวเอง) แล้วกรอกค่าต่างๆด้านล่างลงในตาราง

คอลัมน์ email ให้กรอก admin@localhost
คอลัมน์ password ให้กรอก 378b16462af3df83bc996c94706d5edd1c750a7a
คอลัมน์ salt ให้กรอก 5ba77fe459b43

จะสามารถเข้าระบบโดยใช้บัญชี อีเมล์ admin@localhost และรหัสผ่าน admin ได้ (หลังจากเข้าระบบได้แล้ว ให้เปลี่ยนอีเมล์และรหัสผ่านกลับเป็นเหมือนเดิม โดยดำเนินการบนระบบให้เรียบร้อยด้วย)
ตัวอัปเกรดไม่รองรับการอัปเกรดจาก GCMS เวอร์ชั่นต่ำกว่า 9.1.0 นะครับ โดยจะต้องทำการอัปเกรดให้เป็น 9.1.0 ก่อนถึงจะอัปเกรดให้เป็นเวอร์ชั่นล่าสุดได้
การอัปเกรดจาก GCMS 11.0.0 ขึ้นไป ให้ลบไดเร็คทอรี่ Gcms/ Kotchasan/ PDF/ Widgets/ admin/ ckeditor/ js/ modules/ ออก แล้วอัปโหลดไฟล์ทั้งหมดจากที่ดาวน์โหลดไป ไปแทนที่ เสร็จแล้วถึงเรียกตัวติดตั้ง
การอัปเกรดจาก GCMS 11.2.0 ขึ้นไป ให้อัปโหลดไฟล์ทั้งหมดจากที่ดาวน์โหลดไปแทนที่ไฟล์เดิมได้เลยเสร็จแล้วเรียกตัวติดตั้ง

",4
dluciano/pokedex,JavaScript,"
The project description can be founded here: Challenge
The pokedex is a challange I founded in github and it's a brilliant way to learn Angular and other concepts. Main knowledge applied in this project are: convert an image to a functional web page, use of frontend libraries and technologies: SASS, HTML5, Angular 6. Also CI/CD, and DRY was used because I started the project from a template of Angular with a huge amount of already done solutions (integration to jenkis, e2e with protractor, angular 6, webpack, and so on).
Tech stack:

Angular 6
NPM
Travis CI
Docker
Typescript
TSLint
Karma
Jasmine
Protractor
Webpack
Git/Github
Polyfills
Serverless/SPA/Pokeapi
Pokeapi-js-wrapper

Screens: The pokeapi proxy service is down :( I will upload it later when I implement the JS wrapper of the pokedex api or if the proxy server go online again. Nonetheless, it looks like the screens in the challange.


Docker Hub Image

Thanks to the team of — PatrickJS the template is a valuable asset.
Thanks for the awesome idea of the challenge challenge
Thanks for the template
Thanks for the library of pokedex for javascript (not implemented yet but I will)

",2
betteridiot/terminal_support,Vim script,"Terminal_support
Repo containing files used for customizing the terminal and vim

Instructions
Note: if you are mac user, just change .bashrc to .bash_profile
Also, to allow for powerline-shell support, perfom the following:
pip install powerline-shell

Clone the repo

git clone https://github.com/betteridiot/terminal_support.git

Move into directory

cd terminal_support
Note: If you are using a conda-build of Python:
Change line 113 to your /etc/profile.d/conda.sh and uncomment both lines 113-114

Now move all the files to your $HOME directory using the provided script

./build_terminal.sh
Note: this script will move any pre-existing files that overlap with this
repo into a folder in your home directory called backup. If any of the
provided files cause any issues, just recover previous files from this directory.
Additionally: If you would like to utilize powerline-shell support (and you installed it
with pip, perform the following:
cp powerline-bashrc/.bashrc ~/

What this does

Gives you a more colorful prompt
Adds git-prompt support that lists the current branch the current repo is on
Puts directories before files when listing them (ll)
Makes sure your history is large enough
Makes your tab = 4 spaces
gives you access to jupyter lab using the alias jlab

As long as your current environment has JupyterLab installed, it will run it, silence all the output, and put it in the background (not locking your prompt)



",2
PermiJW/signSGD-with-Majority-Vote,Python,"signSGD-with-Majority-Vote
This repository contains the code used for paper:

signSGD with Majority Vote is Commnunication Efficient and Byzantine Fault Tolerant

This code was originally forked from the End to end ImageNet training.
Pre-installation
Downloading ImageNet

You can download ImageNet from Kaggle.
You can download from our S3 bucket (s3://signum-majority-vote/dataset/ILSVRC.tar) (reproduction purpose only).

C++ extension installation

Put the folder 'main/bit2byte-extension' to the directory of the PyTorch source code
Execute this command on the directory of 'bit2byte-extension'
python setup.py install
You can find more information about C++ extension in PyTorch documentation

Experiments
Note: You have to execute following commands in each instance.
ImageNet Benchmark
Execute following commands on the directory of 'main'
Training Signum

ulimit -n 1000000
sudo /home/ubuntu/anaconda3/envs/fastai/bin/python3 -m torch.distributed.launch \ --nproc_per_node=1 --nnodes=[number of instances] --node_rank=[rank of instance] --master_addr=""0.0.0.0"" \ --master_port=1235 benchmark_main.py ~/ILSVRC/Data/CLS-LOC -a resnet50 -b 128 --lr 0.0001 \ --epochs 80 --save-dir ./ --world-size [number of instances] --print-freq 200 --compress --dist_backend gloo --weight-decay 1e-4 --momentum 0.9 --warm-up \ --dist-url [parameter sever's url]

Training Vanilla SGD

ulimit -n 1000000
sudo /home/ubuntu/anaconda3/envs/fastai/bin/python3 -m torch.distributed.launch \ --nproc_per_node=1 --nnodes=[number of instances] --node_rank=[rank of instance] --master_addr=""0.0.0.0"" \ --master_port=1235 benchmark_main.py ~/ILSVRC/Data/CLS-LOC -a resnet50 -b 128 --lr 0.1 \ --epochs 80 --save-dir ./ --world-size [number of instances] --print-freq 200 --all_reduce --dist_backend nccl --weight-decay 0.1 --momentum 0.9 --warm-up \ --dist-url [parameter sever's url]

QRNN Benchmark
Execute following commands on the directory of 'benchmark/QRNN'
Training Signum

/home/ubuntu/anaconda3/envs/qrnn/bin/python3 -u -m torch.distributed.launch \ --nproc_per_node=1 --nnodes=[number of instances] --node_rank=[rank of instance] --master_addr=""0.0.0.0"" \ --master_port=1235 main_signum.py --epochs 12 --nlayers 4 --emsize 400 --nhid 2500 --alpha 0 --beta 0 \ --dropoute 0 --dropouth 0.1 --dropouti 0.1 --dropout 0.1 --wdrop 0 --wdecay 0 --bptt 140 --batch_size 240 \ --optimizer signum --lr 1e-3 --momentum 0.5 --data data/wikitext-103 --save WT103.12hr.QRNN.pt --when 12 --model QRNN \ --world-size [number of instances] --dist-url [parameter sever's url] \ --save-dir ./ --distributed --multi_gpu --momentun_warm_up 

Training Adam

/home/ubuntu/anaconda3/envs/qrnn/bin/python3 -u -m torch.distributed.launch \ --nproc_per_node=1 --nnodes=[number of instances] --node_rank=[rank of instance] --master_addr=""0.0.0.0"" \ --master_port=1235 main_signum.py --epochs 12 --nlayers 4 --emsize 400 --nhid 2500 --alpha 0 --beta 0 \ --dropoute 0 --dropouth 0.1 --dropouti 0.1 --dropout 0.1 --wdrop 0 --wdecay 0 --bptt 140 --batch_size 240 \ --optimizer adam --lr 1e-3 --momentum 0.5 --data data/wikitext-103 --save WT103.12hr.QRNN.pt --when 12 --model QRNN \ --world-size [number of instances] --dist-url [parameter sever's url] \ --save-dir ./ --distributed --multi_gpu --momentun_warm_up 

QSGD Benchmark
Execute following commands on the directory of 'benchmark/QSGD'
Training Signum

ulimit -n 1000000
sudo /home/ubuntu/anaconda3/envs/fastai/bin/python3 -m torch.distributed.launch \ --nproc_per_node=1 --nnodes=[number of instances] --node_rank=[rank of instance] --master_addr=""0.0.0.0"" \ --master_port=1235 benchmark_main.py ~/ILSVRC/Data/CLS-LOC -a resnet50 -b 128 --lr 1e-4 --seed 1 \ --epochs 90 --save-dir ./ --world-size [number of instances] --print-freq 50 \ --extra_epochs 0 --compress --signum --communication_method Signum \ --dist-url [parameter sever's url] 

Training QSGD

ulimit -n 1000000
sudo /home/ubuntu/anaconda3/envs/fastai/bin/python3 -m torch.distributed.launch \ --nproc_per_node=1 --nnodes=[number of instances] --node_rank=[rank of instance] --master_addr=""0.0.0.0"" \ --master_port=1235 benchmark_main.py ~/ILSVRC/Data/CLS-LOC -a resnet50 -b 128 --lr 0.1 --seed 1 \ --epochs 90 --save-dir ./ --world-size [number of instances] --print-freq 50 \ --extra_epochs 0 --compress --signum --communication_method QSGD --qsgd_level [the level of QSGD] [--enable_max, if enable max_norm] --all_reduce \ --dist-url [parameter sever's url] 

Krum Benchmark
Execute following commands on the directory of 'benchmark/Krum'
Training Signum

ulimit -n 1000000
sudo /home/ubuntu/anaconda3/envs/fastai/bin/python3 -m torch.distributed.launch \ --nproc_per_node=1 --nnodes=[number of instances] --node_rank=[rank of instance] --master_addr=""0.0.0.0"" \ --master_port=1235 benchmark_main.py ~/ILSVRC/Data/CLS-LOC -a resnet50 -b 128 --lr 1e-3 \ --epochs 90 --save-dir ./ --world-size 7 --print-freq 50 \ --extra_epochs 0 --compress --signum --communication_method Signum \ --enable_adversary --adversary_num [the number of adversaries] [--enable_minus_adversary, enable minus adversary or it will be random one] \ --dist-url [parameter sever's url] 

Training Krum

ulimit -n 1000000
sudo /home/ubuntu/anaconda3/envs/fastai/bin/python3 -m torch.distributed.launch \ --nproc_per_node=1 --nnodes=[number of instances] --node_rank=[rank of instance] --master_addr=""0.0.0.0"" \ --master_port=1235 benchmark_main.py ~/ILSVRC/Data/CLS-LOC -a resnet50 -b 128 --lr 1e-1 \ --epochs 90 --save-dir ./ --world-size 7 --print-freq 50 \ --extra_epochs 0 --compress --signum --communication_method Signum \ --enable_krum --krum_f [the number of F] --enable_adversary --adversary_num [the number of adversaries] \ --dist-url [parameter sever's url] 

",46
LCTT/TranslateProject,Shell,"






简介
LCTT 是“Linux 中国”（https://linux.cn/）的翻译组，负责从国外优秀媒体翻译 Linux 相关的技术、资讯、杂文等内容。
LCTT 已经拥有几百名活跃成员，并欢迎更多的 Linux 志愿者加入我们的团队。


LCTT 官网： https://linux.cn/lctt/
LCTT 状态： https://lctt.github.io/

加入我们
请首先加入翻译组的 QQ 群，群号是：198889102，加群时请说明是“志愿者”。
加入的成员，请：

修改你的 QQ 群名片为“译者-您的_GitHub_ID”。
阅读 WIKI 了解如何开始。
遇到不解之处，请在群内发问。

如何开始
请阅读 WIKI。如需要协助，请在群内发问。
历史

2013/09/10 倡议并得到了大家的积极响应，成立翻译组。
2013/09/11 采用 GitHub 进行翻译协作，并开始进行选题翻译。
2013/09/16 公开发布了翻译组成立消息后，又有新的成员申请加入了。并从此建立见习成员制度。
2013/09/24 鉴于大家使用 GitHub 的水平不一，容易导致主仓库的一些错误，因此换成了常规的 fork+PR 的模式来进行翻译流程。
2013/10/11 根据对 LCTT 的贡献，划分了 Core Translators 组，最先的加入成员是 vito-L 和 tinyeyeser。
2013/10/12 取消对 LINUX.CN 注册用户的关联，在 QQ 群内、文章内都采用 GitHub 的注册 ID。
2013/10/18 正式启动 man 翻译计划。
2013/11/10 举行第一次北京线下聚会。
2014/01/02 增加了 Core Translators 成员: geekpi。
2014/05/04 更换了新的 QQ 群：198889102
2014/05/16 增加了 Core Translators 成员: will.qian、vizv。
2014/06/18 由于 GOLinux 令人惊叹的翻译速度和不错的翻译质量，升级为 Core Translators 成员。
2014/09/09 LCTT 一周年，做一年总结。并将曾任 CORE 的成员分组为 Senior，以表彰他们的贡献。
2014/10/08 提升 bazz2 为 Core Translators 成员。
2014/11/04 提升 zpl1025 为 Core Translators 成员。
2014/12/25 提升 runningwater 为 Core Translators 成员。
2015/04/19 发起 LFS-BOOK-7.7-systemd 项目。
2015/06/09 提升 ictlyh 和 dongfengweixiao 为 Core Translators 成员。
2015/11/10 提升 strugglingyouth、FSSlc、Vic020、alim0x 为 Core Translators 成员。
2016/02/18 由于选题 DeadFire 重病，任命 oska874 接手选题工作。
2016/02/29 选题 DeadFire 病逝。
2016/05/09 提升 PurlingNayuki 为校对。
2016/09/10 LCTT 三周年。
2016/12/24 拟定 LCTT Core 规则，并增加新的 Core 成员： ucasFL、martin2011qi，及调整一些组。
2017/03/13 制作了 LCTT 主页、成员列表和成员主页，LCTT 主页将移动至 https://linux.cn/lctt 。
2017/03/16 提升 GHLandy、bestony、rusking 为新的 Core 成员。创建 Comic 小组。
2017/04/11 启用头衔制，为各位重要成员颁发头衔。
2017/11/21 鉴于 qhwdw 快速而上佳的翻译质量，提升 qhwdw 为新的 Core 成员。
2017/11/19 wxy 在上海交大举办的 2017 中国开源年会上做了演讲：《如何以翻译贡献参与开源社区》。
2018/01/11 提升 lujun9972 成为核心成员，并加入选题组。
2018/02/20 遭遇 DMCA 仓库被封。
2018/05/15 提升 MjSeven 为核心成员。
2018/08/01 发布 Linux 中国通证：LCCN。
2018/08/17 提升 pityonline 为核心成员，担任校对，并接受他的建议采用 PR 审核模式。
2018/09/10 LCTT 五周年。
2018/10/25 重构了 CI，感谢 vizv、lujun9972、bestony。
2018/11/13 成立了项目管理委员会（PMC），初始成员为：@wxy （主席）、@oska874、@lujun9972、@bestony、@pityonline、@geekpi、@qhwdw。

项目管理委员及核心成员
LCTT 现由项目管理委员会（PMC）进行管理，成员如下：

🎩 主席 @wxy
🎩 选题 @oska874
🎩 选题 @lujun9972
🎩 技术 @bestony
🎩 校对 @pityonline
🎩 译者 @geekpi
🎩 译者 @qhwdw

目前 LCTT 核心成员有：

❤️ 核心成员 @vizv
❤️ 核心成员 @zpl1025
❤️ 核心成员 @runningwater
❤️ 核心成员 @FSSlc
❤️ 核心成员 @Vic020
❤️ 核心成员 @alim0x
❤️ 核心成员 @martin2011qi
❤️ 核心成员 @Locez
❤️ 核心成员 @ucasFL
❤️ 核心成员 @MjSeven

曾经做出了巨大贡献的核心成员，被列入荣誉榜：

🏆 前任选题 @DeadFire
🏆 前任校对 @reinoir222
🏆 前任校对 @PurlingNayuki
🏆 前任校对 @carolinewuyan
🏆 前任校对 @jasminepeng
🏆 功勋成员 @tinyeyeser
🏆 功勋成员 @vito-L
🏆 功勋成员 @willqian
🏆 功勋成员 @GOLinux
🏆 功勋成员 @bazz2
🏆 功勋成员 @ictlyh
🏆 功勋成员 @dongfengweixiao
🏆 功勋成员 @strugglingyouth
🏆 功勋成员 @GHLandy
🏆 功勋成员 @rusking

全部成员列表请参见： https://linux.cn/lctt-list/ 。
谢谢大家的支持！
",1353
jdlingyu/ad-wars,None,"大圣净化 使用帮助
应用下载地址(捐赠版本需激活码)

https://fir.im/adwars

集合八戒/悟空/唐僧功能，一款多种姿势去除广告的应用...
八戒 酷安下载(免费)

借助无障碍服务，通过文字/ViewId/屏幕坐标来模拟点击，可以用来跳过启动页广告。

悟空 酷安下载(免费)

基于 Xposed 框架，在不改变原生安装包的前提下，用多种拦截手段消灭启动广告。

唐僧 酷安下载(免费)

「八戒」「悟空」的规则数据共享。

寻找组织

Telegram(打开404的话 你懂的)
QQ群：559898993

目录

悟空净化
八戒助手
八戒坐标
大圣FAQ
悬浮窗解码
保持应用后台设置
悟空加速
替换规则

",24
jdayllon/contratacionestado,None,"contratacionestado
El objetivo del proyecto es generar un dataset sobre contratación del sector público para facilitar la reutilización y el análisis de la información que genera este tipo de contratación
Sobre la información recogida


La información en el repositorio procede de las páginas web de los organismos:

Junta de Andalucía (http://www.juntadeandalucia.es)
Generalitat de Catalunya. (https://contractaciopublica.gencat.cat/)



Las transformaciones aplicadas buscan facilitar el proceso de la información o enriquecer la información ofrecida por las plataformas anteriores.


La estructura de los datos se está adaptando a un modelo inspirado en CODICE 2.1 / UBL
** Más información en: https://contrataciondelestado.es/wps/portal/codice


Propiedad Intelectual
Junta de Andalucía
Tal y como se cita en el aviso legal del Portal de la Junta de Andalucía:
La Junta de Andalucía promueve el libre uso y reutilización de los textos disponibles en el presente Portal sobre los que ostenta derechos de propiedad intelectual. Dichos textos están disponibles a través de una licencia-tipo Creative Commons Reconocimiento 3.0.
De modo general, la Junta de Andalucía te autoriza a:

Copiar, redistribuir y comunicar públicamente los textos del Portal.
Hacer un uso comercial de los contenidos.
Generar obras derivadas.

He impone las siguientes obligaciones:

Reconocer explícitamente la fuente de información Identificada en el presente texto.
Incluir la misma obligación de reconocimiento en los términos de licencia de cualquier producto derivado que haga uso de esta información. * Por lo que usuarios de esta información deben tener presente esta obligación *
No desnaturalizar el sentido de la información reproducida.
Evitar cualquier rasgo de presentación que sugiera que la Junta de Andalucía apoya o promueve el uso que se hace de la información difundida. En ningún caso está permitida la reproducción de logotipos, escudos, símbolos y marcas identificativas de la Junta de Andalucía sin autorización expresa de la institución.

Generalitat de Catalunya
TODO
",2
pytorch/pytorch,C++,"

PyTorch is a Python package that provides two high-level features:

Tensor computation (like NumPy) with strong GPU acceleration
Deep neural networks built on a tape-based autograd system

You can reuse your favorite Python packages such as NumPy, SciPy and Cython to extend PyTorch when needed.

More about PyTorch
Installation

Binaries
From Source
Docker Image
Building the Documentation
Previous Versions


Getting Started
Communication
Releases and Contributing
The Team




System
2.7
3.5
3.6




Linux CPU


—


Linux GPU


—


Windows CPU / GPU
—

—


Linux (ppc64le) CPU

—



Linux (ppc64le) GPU

—




See also the ci.pytorch.org HUD.
More About PyTorch
At a granular level, PyTorch is a library that consists of the following components:



Component
Description




torch
a Tensor library like NumPy, with strong GPU support


torch.autograd
a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch


torch.jit
a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code


torch.nn
a neural networks library deeply integrated with autograd designed for maximum flexibility


torch.multiprocessing
Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training


torch.utils
DataLoader and other utility functions for convenience



Usually one uses PyTorch either as:

a replacement for NumPy to use the power of GPUs.
a deep learning research platform that provides maximum flexibility and speed.

Elaborating further:
A GPU-Ready Tensor Library
If you use NumPy, then you have used Tensors (a.k.a ndarray).

PyTorch provides Tensors that can live either on the CPU or the GPU, and accelerates the
computation by a huge amount.
We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs
such as slicing, indexing, math operations, linear algebra, reductions.
And they are fast!
Dynamic Neural Networks: Tape-Based Autograd
PyTorch has a unique way of building neural networks: using and replaying a tape recorder.
Most frameworks such as TensorFlow, Theano, Caffe and CNTK have a static view of the world.
One has to build a neural network, and reuse the same structure again and again.
Changing the way the network behaves means that one has to start from scratch.
With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to
change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes
from several research papers on this topic, as well as current and past work such as
torch-autograd,
autograd,
Chainer, etc.
While this technique is not unique to PyTorch, it's one of the fastest implementations of it to date.
You get the best of speed and flexibility for your crazy research.

Python First
PyTorch is not a Python binding into a monolithic C++ framework.
It is built to be deeply integrated into Python.
You can use it naturally like you would use NumPy / SciPy / scikit-learn etc.
You can write your new neural network layers in Python itself, using your favorite libraries
and use packages such as Cython and Numba.
Our goal is to not reinvent the wheel where appropriate.
Imperative Experiences
PyTorch is designed to be intuitive, linear in thought and easy to use.
When you execute a line of code, it gets executed. There isn't an asynchronous view of the world.
When you drop into a debugger, or receive error messages and stack traces, understanding them is straightforward.
The stack trace points to exactly where your code was defined.
We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.
Fast and Lean
PyTorch has minimal framework overhead. We integrate acceleration libraries
such as Intel MKL and NVIDIA (cuDNN, NCCL) to maximize speed.
At the core, its CPU and GPU Tensor and neural network backends
(TH, THC, THNN, THCUNN) are mature and have been tested for years.
Hence, PyTorch is quite fast – whether you run small or large neural networks.
The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.
We've written custom memory allocators for the GPU to make sure that
your deep learning models are maximally memory efficient.
This enables you to train bigger deep learning models than before.
Extensions Without Pain
Writing new neural network modules, or interfacing with PyTorch's Tensor API was designed to be straightforward
and with minimal abstractions.
You can write new neural network layers in Python using the torch API
or your favorite NumPy-based libraries such as SciPy.
If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.
There is no wrapper code that needs to be written. You can see a tutorial here and an example here.
Installation
Binaries
Commands to install from binaries via Conda or pip wheels are on our website:
https://pytorch.org
NVIDIA Jetson platforms
Python wheels for NVIDIA's Jetson Nano, Jetson TX2, and Jetson AGX Xavier are available via the following URLs:

Stable binaries:

Python 2.7: https://nvidia.box.com/v/torch-stable-cp27-jetson-jp42
Python 3.6: https://nvidia.box.com/v/torch-stable-cp36-jetson-jp42


Rolling weekly binaries:

Python 2.7: https://nvidia.box.com/v/torch-weekly-cp27-jetson-jp42
Python 3.6: https://nvidia.box.com/v/torch-weekly-cp36-jetson-jp42



They requires JetPack 4.2 and above and are maintained by @dusty-nv
From Source
If you are installing from source, we highly recommend installing an Anaconda environment.
You will get a high-quality BLAS library (MKL) and you get a controlled compiler version regardless of your Linux distro.
Once you have Anaconda installed, here are the instructions.
If you want to compile with CUDA support, install

NVIDIA CUDA 7.5 or above
NVIDIA cuDNN v6.x or above

If you want to disable CUDA support, export environment variable NO_CUDA=1.
Other potentially useful environment variables may be found in setup.py.
If you are building for NVIDIA's Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to are available here
Install Dependencies
Common
conda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing

On Linux
# Add LAPACK support for the GPU if needed
conda install -c pytorch magma-cuda90 # or [magma-cuda80 | magma-cuda92 | magma-cuda100 ] depending on your cuda version
Get the PyTorch Source
git clone --recursive https://github.com/pytorch/pytorch
cd pytorch
# if you are updating an existing checkout
git submodule sync 
git submodule update --init --recursive
Install PyTorch
On Linux
export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
python setup.py install
On macOS
export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install
On Windows
At least Visual Studio 2017 Update 3 (version 15.3.3 with the toolset 14.11) and NVTX are needed.
If the version of Visual Studio 2017 is higher than 15.4.5, installing of ""VC++ 2017 version 15.4 v14.11 toolset"" is strongly recommended.
 If the version of Visual Studio 2017 is lesser than 15.3.3, please update Visual Studio 2017 to the latest version along with installing ""VC++ 2017 version 15.4 v14.11 toolset"".
 There is no guarantee of the correct building with VC++ 2017 toolsets, others than version 15.4 v14.11.
 ""VC++ 2017 version 15.4 v14.11 toolset"" might be installed onto already installed Visual Studio 2017 by running its installation once again and checking the corresponding checkbox under ""Individual components""/""Compilers, build tools, and runtimes"".
For building against CUDA 8.0 Visual Studio 2015 Update 3 (version 14.0), and the patch are needed to be installed too.
The details of the patch can be found here.
NVTX is a part of CUDA distributive, where it is called ""Nsight Compute"". For installing it onto already installed CUDA run CUDA installation once again and check the corresponding checkbox.
Be sure that CUDA with Nsight Compute is installed after Visual Studio 2017.
cmd
REM [Optional] The following two lines are needed for Python 2.7, but the support for it is very experimental.
set MSSdk=1
set FORCE_PY27_BUILD=1

REM [Optional] As for CUDA 8, VS2015 Update 3 is required; use the following line.
set ""CUDAHOSTCXX=%VS140COMNTOOLS%..\..\VC\bin\amd64\cl.exe""

set CMAKE_GENERATOR=Visual Studio 15 2017 Win64
set DISTUTILS_USE_SDK=1

REM Run ""Visual Studio 2017 Developer Command Prompt""
for /f ""usebackq tokens=*"" %i in (`""%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe"" -version [15^,16^) -products * -latest -property installationPath`) do call ""%i\VC\Auxiliary\Build\vcvarsall.bat"" x64 -vcvars_ver=14.11

python setup.py install

Docker Image
Dockerfile is supplied to build images with cuda support and cudnn v7. You can pass -e PYTHON_VERSION=x.y flag to specify which Python version is to be used by Miniconda, or leave it unset to use the default. Build from pytorch repo directory as docker needs to copy git repo into docker filesystem while building the image.
docker build -t pytorch -f docker/pytorch/Dockerfile .

You can also pull a pre-built docker image from Docker Hub and run with nvidia-docker,
but this is not currently maintained and will pull PyTorch 0.2.
nvidia-docker run --rm -ti --ipc=host pytorch/pytorch:latest

Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.
for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you
should increase shared memory size either with --ipc=host or --shm-size command line options to nvidia-docker run.
Building the Documentation
To build documentation in various formats, you will need Sphinx and the
readthedocs theme.
cd docs/
pip install -r requirements.txt

You can then build the documentation by running make <format> from the
docs/ folder. Run make to get a list of all available output formats.
Previous Versions
Installation instructions and binaries for previous PyTorch versions may be found
on our website.
Getting Started
Three pointers to get you started:

Tutorials: get you started with understanding and using PyTorch
Examples: easy to understand pytorch code across all domains
The API Reference

Communication

forums: discuss implementations, research, etc. https://discuss.pytorch.org
GitHub issues: bug reports, feature requests, install issues, RFCs, thoughts, etc.
Slack: The PyTorch Slack hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration etc. If you are a beginner looking for help, the primary medium is PyTorch Forums. If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1
newsletter: no-noise, one-way email newsletter with important announcements about pytorch. You can sign-up here: https://eepurl.com/cbG0rv

Releases and Contributing
PyTorch has a 90 day release cycle (major releases). Please let us know if you encounter a bug by filing an issue.
We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.
If you plan to contribute new features, utility functions or extensions to the core, please first open an issue and discuss the feature with us.
Sending a PR without discussion might end up resulting in a rejected PR, because we might be taking the core in a different direction than you might be aware of.
The Team
PyTorch is a community driven project with several skillful engineers and researchers contributing to it.
PyTorch is currently maintained by Adam Paszke, Sam Gross, Soumith Chintala and Gregory Chanan with major contributions coming from hundreds of talented individuals in various forms and means.
A non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Kopf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito.
Note: this project is unrelated to hughperkins/pytorch with the same name. Hugh is a valuable contributor in the Torch community and has helped with many things Torch and PyTorch.
License
PyTorch is BSD-style licensed, as found in the LICENSE file.
",27940
meseta/git-together-0,Game Maker Language,"Make sure you read the Instructions note in the project
",7
startupengine/startupengine,HTML,"
Startup Engine
A beautiful & open-source platform for launching startups.



Key Features

 Publish & sell software/content subscriptions.
 Completely plug-and-play. Coding is optional.
 Supports any workflow, architecture, or framework.
 JSON API allows you integrate with external sites/apps.
 Completely open-source.
 1-Click Install.

Documentation
Documentation is available at https://www.startupengine.io/docs.
Deployment
Click the button below to deploy a new instance of Startup Engine to Heroku instantly.

Please reference Heroku's official guide for getting started with Laravel apps on Heroku.
Once you've installed the Heroku CLI, run the following commands on your instance:
First, generate an APP_KEY by locally running:
php artisan key:generate.
Then copy the newly generated key and run:
heroku config:set APP_KEY=APPKEYGOESHERE
The default user email is admin@example.com and the default password is password.
Change these after logging in.
Support
Found a bug? Submit an issue.
Security
If you discover a security vulnerability within Startup Engine, please send an e-mail to startupengine.io@domainsbyproxy.com
All security vulnerabilities will be promptly addressed.
License
Startup Engine is open-sourced software licensed under the MIT license.
",86
Lombiq/Smart-Notifications,C#,"Smart Notifications readme
Orchard CMS module that adds the ability to have closable (can be closed with an X in the corner like windows), fading (fades out in a few seconds) or persistent notifications (will be shown until the user closes them). The standard Notifier service is used so all existing notifications can be changed from site settings (and thus you can e.g. make all appearing notifications closable).
If you want to add closable, fading or persistent notifications from code explicitly just use the extension methods on INotifier added by the module.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/smart-notifications (Mercurial repository)
https://github.com/Lombiq/Smart-Notifications (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",2
xaos-project/XaoS,C,"XaoS
Original Authors: Jan Hubicka and Thomas Marsh.
See CREDITS file for a complete list of contributors.
Introduction
XaoS is a realtime interactive fractal zoomer. This means that it lets you
zoom smoothly into any place in the fractal you choose without the long
calculation required by most other fractal generators. It has many other
features too, like different fractal types, autopilot, special coloring
modes, random palette generation, color cycling, etc.
Website
Visit the XaoS website
for the latest news and downloads.  Source code is available on
GitHub.
Documentation
Documentation is maintained on the GitHub Wiki.
User Support
XaoS is a community-supported free software project.
The xaos-users Google Group
provides a place for XaoS users to help each other get the most out of XaoS.
XaoS developers also monitor this discussion and answer questions from time to time.
You can browse the archives freely but to prevent spam, you must join the group
in order to post. Google Groups provides options for participation from a
traditional mailing list to a completely web-based forum, so you don’t have to
get emails if you don’t want to.
Reporting Bugs
Issues are tracked on GitHub.
If you think you have found a bug in XaoS or have an idea for a new feature,
please let us know about it.
XaoS is developed on a volunteer basis and the developers work on it in their spare time.
Therefore, we can’t guarantee that issues will be addressed in a certain timeframe. If
you are able to fix a bug yourself, pull requests are very welcome.
Supported Platforms
XaoS is based on Qt, and supports Linux, Mac, and Windows.
License
Copyright (C) 1996-2019 XaoS Contributors
This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.
This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.
You should have received a copy of the GNU General Public License
along with this program.  If not, see https://www.gnu.org/licenses/.
",99
openboxes/openboxes,Groovy,"




OpenBoxes
About
OpenBoxes is an Open Source Inventory and Supply Chain Management System. The initial implementation of OpenBoxes will occur at Partners In Health-supported facilities in Haiti.
License
Copyright (c) 2012 Partners In Health.  All rights reserved.
The use and distribution terms for this software are covered by the
Eclipse Public License 1.0 (http://opensource.org/licenses/eclipse-1.0.php)
which can be found in the file epl-v10.html at the root of this distribution.
By using this software in any fashion, you are agreeing to be bound by
the terms of this license.
You must not remove this notice, or any other, from this software.
Setup development environment
Install Dependencies
Required

Java 7 (must install Java 7)
MySQL 5.7
SDK Man
Grails 1.3.9
NPM

Optional

IntelliJ IDEA 14.1
Chrome

Basic setup instructions for developers
These instructions are for developers only.  If you are a user/implementer, please check out our
Installation documentation.
1. Install Dependencies
Install required dependencies above
2. Install Grails
Check that you have SDK Man installed properly (otherwise follow instructions on the skdman install page).
$ sdk version
SDKMAN 3.1.0

Install Grails 1.3.9
$ sdk install grails 1.3.9

3. Clone repository
If you are a core contributor:
git clone git@github.com:openboxes/openboxes.git      

If you are a not core contributor, fork openboxes git repository
and replace git url with the one of your forked repository
git clone git@github.com:<gitusername>/openboxes.git      

4. Create database
Create openboxes database
mysql -u root -p -e 'create database openboxes default charset utf8;'

Create openboxes user
mysql -u root -p -e 'grant all on openboxes.* to ""openboxes""@""localhost"" identified by ""openboxes"";'

5. Create Openboxes configuration file
Edit $HOME/.grails/openboxes-config.properties
# Database connection settings
# You can use dataSource.url when you are using a non-dev/non-test database (test-app may not run properly).
# If you want to run $ grails test-app you should comment out the dataSource.url below and create a new 
# openboxes_test database.  Eventually, we will move to an in-memory H2 database for testing, but we're 
# currently stuck with MySQL because I'm using some MySQL-specific stuff in the Liquibase changesets.  My bad.

dataSource.url=jdbc:mysql://localhost:3306/openboxes
dataSource.username=openboxes
dataSource.password=openboxes

# OpenBoxes mail settings (disabled by default)
grails.mail.enabled=false

NOTE: If you are running in development mode with a copy of an existing production database, you will need to
instruct the application to not setup test fixtures automatically by uncommenting the above property:
openboxes.fixtures.enabled=false

6. Install NPM dependencies
npm install

7. Build React frontend
You can build React frontend with this command, but it will be automatically build when starting the application.
npm run bundle

8. React frontend Hot-Reload
When using this command React fronted will be rebuild automatically after any change, you just need to refresh the
browser to see the effect.
npm run watch

9. Upgrade the project to the currently installed grails version
Either of the following actions (upgrade, compile, run-app) should generate the all important Spring configuration
(/WEB-INF/applicationContext.xml) and start the dependency resolution process.
grails upgrade

OR
grails compile

The grails compile step is not necessary since grails run-app will invoke the compilation step, but it doesn't
hurt anything.
If you see any errors, run the command again.
IMPORTANT That last line is important.  Because of some quirkiness with the way older versions of Grails resolve
dependencies and generates config files, you may need to run either of these commands multiple times in order to
resolve all dependencies and generate the config files.
Once the dependency resolution phase has completed, all dependencies will be stored in a local ivy cache (usually
under $USER_HOME/.grails/ivy-cache).  You do not have to worry about this, just know that the dependencies are now
on your machine and Grails will attempt to find them there before it tries to resolve them in a remote repository.
10. Start application in development mode
The application can be run in development mode.  This starts the application running in an instance of Tomcat within
the Grails console.
You may need to run 'grails run-app' several times in order to download all dependencies.
grails run-app

11. Open application in Google Chrome
http://localhost:8080/openboxes

12. Log into OpenBoxes
You can use the default accounts (manager:password OR admin:password). Once you are logged in as an admin, you can
create own account. Or you can use the signup form to create a new account.
13. React tests
To run new frontend (React) tests type:
npm test

14. React documentation
Start a style guide dev server:
npm run styleguide

View your style guide in the browser:
http://localhost:6060

Troubleshooting
How to Debug

Run Grails in debug mode
grails-debug run-app


In Intellij navigate to Run > Edit Configurations
Create a new Remote Debug Configuration

Name: openboxes-debug
Transport: Socket
Debugger mode: Attach
Host: localhost
Port: 5005


Command line arguments should look something like this:
-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005



Problem
Caused by: java.io.FileNotFoundException: Could not open ServletContext resource [/WEB-INF/applicationContext.xml]

Solution
Execute the grails upgrade command in order to generate the files nece
$ grails upgrade

See the following stackoverflow article:
http://stackoverflow.com/questions/24243027/grails-spring-security-sample-application-not-working
",187
oestrich/grapevine,Elixir,"Grapevine

Grapevine is a MUD chat network.

MUD Coders Slack
Docs
Trello
Patreon
Discord

WebSocket Protocol
View the websocket details on Grapevine.
Server
Requirements
This is only required to run Grapevine itself, the server. These are not required to connect as a game. See the above websocket docs for connecting as a client.

PostgreSQL 10
Elixir 1.8.0
Erlang 21.2.6
node.js 10.13.0

Setup
mix deps.get
mix compile
cd assets && npm install && node node_modules/brunch/bin/brunch build && cd ..
mix ecto.reset
mix phx.server
This will start a web server on port 4100. You can now load http://localhost:4100/ to view the application.
Running Tests
MIX_ENV=test mix ecto.create
MIX_ENV=test mix ecto.migrate
mix test
Docker
docker-compose build grapevine
docker-compose up -d postgres
docker-compose run --rm grapevine migrate
docker-compose up grapevine
Telnet Web Client
For deployment the telnet application needs to be on its own erlang node. You can connect with something similar to:
config :grapevine,
  topologies: [
    local: [
      strategy: Cluster.Strategy.Epmd,
      config: [
        hosts: [
          :grapevine@localhost,
          :telnet@localhost,
        ]
      ]
    ]
  ]
Setting up a new Play CNAME

Game sets the CNAME to grapevine.haus
Game must have a homepage url
Game must have the web client enabled
Update game's record for their CNAME
Update nginx config for new domain
Run certbot for the new domain
Refresh CNAMEs in ETS Grapevine.CNAMEs.reload()

",49
indieweb/indieweb-chat-archive,PHP,"IndieWeb Chat Archive
This repo contains the full archive of IndieWeb chat log data files visible at https://chat.indieweb.org
Chat logs are added to this repo every 15 minutes.
File Format
Each channel's files can be read using QuartzDB. The files follow a simple format:
2017-12-01 23:15:06.218000 {""type"":""message"",""timestamp"":1512170106.218,""network"":""irc"",""server"":""freenode"",""channel"":{""uid"":""#indieweb"",""name"":""#indieweb""},""author"":{""uid"":""Loqi"",""nickname"":""Loqi"",""username"":""Loqi"",""name"":""Loqi"",""photo"":null,""url"":null,""tz"":""US\/Pacific""},""content"":""[@indiewebcamp] This week in the #indieweb https://indieweb.org/this-week/2017-12-01.html https://pbs.twimg.com/media/DP_z5rCVwAAGdTk.jpg (http://twtr.io/1Yx4r5CHSBC)"",""modes"":[]}


Each line begins with the timestamp.
There will always be 26 characters followed by a space.
The timestamp is UTC and has 6 digits of precision for the seconds.
The rest of the line is a JSON-encoded string representing the IRC message and who sent it.

Spam removal
For a guide on how we deal with spam in these logs, see IRC#Spam on the wiki.
",3
theabraxas/Battalion,Shell,"Battalion
Battalion is a tool designed to automate a huge portion of a standard pentest. By supplying only a domain name and website site Battalion goes through the various passive and active reconaissance tasks, enumerates publicly accessible sites and services, identifies potential misconfigurations or vulnerable technologies, discoves and identifies breached accounts, build reports, and much more.
Ultimately Battalion will automate beyond reconaissance and go so far as to trigger phishing campaigns, automatically exploit some discovered vulnerabilities, and come with post-exploitation options.
Try out Battalion and send us any feedback! https://github.com/eidolonpg and I are excited to build out this tool and make it as comprehensive and efficient as possible!
Installation
Battalion depends on a number of tools - please see the primary documentation in the Battalion Installation Guide for more information. This distribution also includes scripts for some system types in the install directory. The installation documentation provides more information on these scripts.
Using Battalion
Example: Scanning a Domain and Users
$ ./battalion.sh --name ""Test Scan"" --out /home/user/scans/company \
    --company ""My Company"" --domain ""company.com"" --nmap
This scan for My Company will produce results in the directory /home/user/scans/company. The domain scan would be based on the specified domain company.com, whereas the user scan is based upon the company name My Company. This scan also enables a light Nmap scan on the detected domains.
Required Parameters for All Scans

--name <scan name>: The scan name
--out  <directory>: The output directory (absolute path)

Required Parameters for Domain Scans

--domain <domain name>: The domain name to scan

Required Parameters for User Scans

--company <company name>: The company name per LinkedIn, used for user scraping

Optional Parameters

--email-domain <domain name>: Allows a different email domain to be configured. Use this if the primary domain is x.com but users receive mail at y.com addresses.
--subdomain-list <file>: Specify a file that provides potential subdomains, this is used by the dnsrecon tool. That tool provides some high-quality default lists.
--nmap: Enable light touch nmap scanning of subdomains
--nmap-aggressive: (Long running!) This is a VERY intense scan on each subdomain, approximately 10 minutes per subdomain.
--shodan <api key>: Specify a Shodan API key and enables a Shodan scan
--hunter <api key>: Sets a Hunter.io API Key and enables Hunter in the user scan. This will vastly speed up the user scan!
--timeout-http <seconds>: Specify a timeout in seconds for HTTP detection
--timeout-eyewitness <seconds>: Specify a timeout in seconds for EyeWitness individual scans

Disabling Major Scan Types

--disable-user: Disable the user scan
--disable-domain: Disable the domain scan

Scan Output
Battalion produces a number of directories which help categorize raw output. All of these directories will be created at the location specified by the --out parameter by the Battalion script.
Expected Scan Time (User)
The current default scan time for the user scan is rather large -- over 20 minutes. We recomment acquiring a Hunter API key to expidite this process.
Expected Scan Time (Domain)
Scans depend very much on the 'size' of the target, where the size is deteremined by the number of users and the number of detected domain records. Small targets usually take at least a few minutes to complete.
Disclaimer
This utility has been created purely for the purposes of research and for improving defense, and is not intended to be used to attack systems except where explicitly authorized. Project maintainers are not responsible or liable for misuse of the software. Use responsibly.
",43
NerdHubMC/Refined-Machinery,Java,"Refined Machinery Mod
",2
bacco007/HomeAssistant-Config,JavaScript,"TBSmartHome - Home Assistant Configuration

Here is the configuration and Documentation for my Home Assistant Setup - very much a work-in-progress
I'm from  so some of the stuff here will have limited use outside of the land downunder.

Table of Contents

Screenshots
Setup
Notes


Screenshots
A few screenshots - I'll add more as the other pages are improved


More Screenshots



Setup
Some Notes about my setup
Server

Lenovo ThinkCentre M73 Tiny (Intel Pentium G3240T, 4Gb RAM, 500Gb HDD)
Ubuntu Server 18.10


Hass.io

AppDaemon
Configurator
Glances
IDE
InfluxDB
MariaDB - Database Server
Node-RED
SSH & Web Terminal

Other Stuff I Run

Docker

Docker-Compose
Portainer
Dockermon for HA


My ""Download Stack""

Jackett
Lidarr
Radarr
SABnzbd
Sonarr
Transmission


Tautulli (Plex Reporting)
Traefik

Network
My Network isn't currently running as I would like it - mainly because I'm unable to deploy my Ubquiti gear in my current location - hopefully that will change soon and I'll update this section with a bit more detail

Notes
Lovelace UI
I've started to use the inbuilt Lovelace UI editor, so my lovelace_ui.yaml file is a backup (and JSON->YAML conversion) of the HA Storage file lovelace - there is a Python file in /custom_components/ called tb_lovelacebackup.py that does the conversion
",2
xaos-project/XaoS,C,"XaoS
Original Authors: Jan Hubicka and Thomas Marsh.
See CREDITS file for a complete list of contributors.
Introduction
XaoS is a realtime interactive fractal zoomer. This means that it lets you
zoom smoothly into any place in the fractal you choose without the long
calculation required by most other fractal generators. It has many other
features too, like different fractal types, autopilot, special coloring
modes, random palette generation, color cycling, etc.
Website
Visit the XaoS website
for the latest news and downloads.  Source code is available on
GitHub.
Documentation
Documentation is maintained on the GitHub Wiki.
User Support
XaoS is a community-supported free software project.
The xaos-users Google Group
provides a place for XaoS users to help each other get the most out of XaoS.
XaoS developers also monitor this discussion and answer questions from time to time.
You can browse the archives freely but to prevent spam, you must join the group
in order to post. Google Groups provides options for participation from a
traditional mailing list to a completely web-based forum, so you don’t have to
get emails if you don’t want to.
Reporting Bugs
Issues are tracked on GitHub.
If you think you have found a bug in XaoS or have an idea for a new feature,
please let us know about it.
XaoS is developed on a volunteer basis and the developers work on it in their spare time.
Therefore, we can’t guarantee that issues will be addressed in a certain timeframe. If
you are able to fix a bug yourself, pull requests are very welcome.
Supported Platforms
XaoS is based on Qt, and supports Linux, Mac, and Windows.
License
Copyright (C) 1996-2019 XaoS Contributors
This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.
This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.
You should have received a copy of the GNU General Public License
along with this program.  If not, see https://www.gnu.org/licenses/.
",99
openboxes/openboxes,Groovy,"




OpenBoxes
About
OpenBoxes is an Open Source Inventory and Supply Chain Management System. The initial implementation of OpenBoxes will occur at Partners In Health-supported facilities in Haiti.
License
Copyright (c) 2012 Partners In Health.  All rights reserved.
The use and distribution terms for this software are covered by the
Eclipse Public License 1.0 (http://opensource.org/licenses/eclipse-1.0.php)
which can be found in the file epl-v10.html at the root of this distribution.
By using this software in any fashion, you are agreeing to be bound by
the terms of this license.
You must not remove this notice, or any other, from this software.
Setup development environment
Install Dependencies
Required

Java 7 (must install Java 7)
MySQL 5.7
SDK Man
Grails 1.3.9
NPM

Optional

IntelliJ IDEA 14.1
Chrome

Basic setup instructions for developers
These instructions are for developers only.  If you are a user/implementer, please check out our
Installation documentation.
1. Install Dependencies
Install required dependencies above
2. Install Grails
Check that you have SDK Man installed properly (otherwise follow instructions on the skdman install page).
$ sdk version
SDKMAN 3.1.0

Install Grails 1.3.9
$ sdk install grails 1.3.9

3. Clone repository
If you are a core contributor:
git clone git@github.com:openboxes/openboxes.git      

If you are a not core contributor, fork openboxes git repository
and replace git url with the one of your forked repository
git clone git@github.com:<gitusername>/openboxes.git      

4. Create database
Create openboxes database
mysql -u root -p -e 'create database openboxes default charset utf8;'

Create openboxes user
mysql -u root -p -e 'grant all on openboxes.* to ""openboxes""@""localhost"" identified by ""openboxes"";'

5. Create Openboxes configuration file
Edit $HOME/.grails/openboxes-config.properties
# Database connection settings
# You can use dataSource.url when you are using a non-dev/non-test database (test-app may not run properly).
# If you want to run $ grails test-app you should comment out the dataSource.url below and create a new 
# openboxes_test database.  Eventually, we will move to an in-memory H2 database for testing, but we're 
# currently stuck with MySQL because I'm using some MySQL-specific stuff in the Liquibase changesets.  My bad.

dataSource.url=jdbc:mysql://localhost:3306/openboxes
dataSource.username=openboxes
dataSource.password=openboxes

# OpenBoxes mail settings (disabled by default)
grails.mail.enabled=false

NOTE: If you are running in development mode with a copy of an existing production database, you will need to
instruct the application to not setup test fixtures automatically by uncommenting the above property:
openboxes.fixtures.enabled=false

6. Install NPM dependencies
npm install

7. Build React frontend
You can build React frontend with this command, but it will be automatically build when starting the application.
npm run bundle

8. React frontend Hot-Reload
When using this command React fronted will be rebuild automatically after any change, you just need to refresh the
browser to see the effect.
npm run watch

9. Upgrade the project to the currently installed grails version
Either of the following actions (upgrade, compile, run-app) should generate the all important Spring configuration
(/WEB-INF/applicationContext.xml) and start the dependency resolution process.
grails upgrade

OR
grails compile

The grails compile step is not necessary since grails run-app will invoke the compilation step, but it doesn't
hurt anything.
If you see any errors, run the command again.
IMPORTANT That last line is important.  Because of some quirkiness with the way older versions of Grails resolve
dependencies and generates config files, you may need to run either of these commands multiple times in order to
resolve all dependencies and generate the config files.
Once the dependency resolution phase has completed, all dependencies will be stored in a local ivy cache (usually
under $USER_HOME/.grails/ivy-cache).  You do not have to worry about this, just know that the dependencies are now
on your machine and Grails will attempt to find them there before it tries to resolve them in a remote repository.
10. Start application in development mode
The application can be run in development mode.  This starts the application running in an instance of Tomcat within
the Grails console.
You may need to run 'grails run-app' several times in order to download all dependencies.
grails run-app

11. Open application in Google Chrome
http://localhost:8080/openboxes

12. Log into OpenBoxes
You can use the default accounts (manager:password OR admin:password). Once you are logged in as an admin, you can
create own account. Or you can use the signup form to create a new account.
13. React tests
To run new frontend (React) tests type:
npm test

14. React documentation
Start a style guide dev server:
npm run styleguide

View your style guide in the browser:
http://localhost:6060

Troubleshooting
How to Debug

Run Grails in debug mode
grails-debug run-app


In Intellij navigate to Run > Edit Configurations
Create a new Remote Debug Configuration

Name: openboxes-debug
Transport: Socket
Debugger mode: Attach
Host: localhost
Port: 5005


Command line arguments should look something like this:
-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005



Problem
Caused by: java.io.FileNotFoundException: Could not open ServletContext resource [/WEB-INF/applicationContext.xml]

Solution
Execute the grails upgrade command in order to generate the files nece
$ grails upgrade

See the following stackoverflow article:
http://stackoverflow.com/questions/24243027/grails-spring-security-sample-application-not-working
",187
oestrich/grapevine,Elixir,"Grapevine

Grapevine is a MUD chat network.

MUD Coders Slack
Docs
Trello
Patreon
Discord

WebSocket Protocol
View the websocket details on Grapevine.
Server
Requirements
This is only required to run Grapevine itself, the server. These are not required to connect as a game. See the above websocket docs for connecting as a client.

PostgreSQL 10
Elixir 1.8.0
Erlang 21.2.6
node.js 10.13.0

Setup
mix deps.get
mix compile
cd assets && npm install && node node_modules/brunch/bin/brunch build && cd ..
mix ecto.reset
mix phx.server
This will start a web server on port 4100. You can now load http://localhost:4100/ to view the application.
Running Tests
MIX_ENV=test mix ecto.create
MIX_ENV=test mix ecto.migrate
mix test
Docker
docker-compose build grapevine
docker-compose up -d postgres
docker-compose run --rm grapevine migrate
docker-compose up grapevine
Telnet Web Client
For deployment the telnet application needs to be on its own erlang node. You can connect with something similar to:
config :grapevine,
  topologies: [
    local: [
      strategy: Cluster.Strategy.Epmd,
      config: [
        hosts: [
          :grapevine@localhost,
          :telnet@localhost,
        ]
      ]
    ]
  ]
Setting up a new Play CNAME

Game sets the CNAME to grapevine.haus
Game must have a homepage url
Game must have the web client enabled
Update game's record for their CNAME
Update nginx config for new domain
Run certbot for the new domain
Refresh CNAMEs in ETS Grapevine.CNAMEs.reload()

",49
indieweb/indieweb-chat-archive,PHP,"IndieWeb Chat Archive
This repo contains the full archive of IndieWeb chat log data files visible at https://chat.indieweb.org
Chat logs are added to this repo every 15 minutes.
File Format
Each channel's files can be read using QuartzDB. The files follow a simple format:
2017-12-01 23:15:06.218000 {""type"":""message"",""timestamp"":1512170106.218,""network"":""irc"",""server"":""freenode"",""channel"":{""uid"":""#indieweb"",""name"":""#indieweb""},""author"":{""uid"":""Loqi"",""nickname"":""Loqi"",""username"":""Loqi"",""name"":""Loqi"",""photo"":null,""url"":null,""tz"":""US\/Pacific""},""content"":""[@indiewebcamp] This week in the #indieweb https://indieweb.org/this-week/2017-12-01.html https://pbs.twimg.com/media/DP_z5rCVwAAGdTk.jpg (http://twtr.io/1Yx4r5CHSBC)"",""modes"":[]}


Each line begins with the timestamp.
There will always be 26 characters followed by a space.
The timestamp is UTC and has 6 digits of precision for the seconds.
The rest of the line is a JSON-encoded string representing the IRC message and who sent it.

Spam removal
For a guide on how we deal with spam in these logs, see IRC#Spam on the wiki.
",3
theabraxas/Battalion,Shell,"Battalion
Battalion is a tool designed to automate a huge portion of a standard pentest. By supplying only a domain name and website site Battalion goes through the various passive and active reconaissance tasks, enumerates publicly accessible sites and services, identifies potential misconfigurations or vulnerable technologies, discoves and identifies breached accounts, build reports, and much more.
Ultimately Battalion will automate beyond reconaissance and go so far as to trigger phishing campaigns, automatically exploit some discovered vulnerabilities, and come with post-exploitation options.
Try out Battalion and send us any feedback! https://github.com/eidolonpg and I are excited to build out this tool and make it as comprehensive and efficient as possible!
Installation
Battalion depends on a number of tools - please see the primary documentation in the Battalion Installation Guide for more information. This distribution also includes scripts for some system types in the install directory. The installation documentation provides more information on these scripts.
Using Battalion
Example: Scanning a Domain and Users
$ ./battalion.sh --name ""Test Scan"" --out /home/user/scans/company \
    --company ""My Company"" --domain ""company.com"" --nmap
This scan for My Company will produce results in the directory /home/user/scans/company. The domain scan would be based on the specified domain company.com, whereas the user scan is based upon the company name My Company. This scan also enables a light Nmap scan on the detected domains.
Required Parameters for All Scans

--name <scan name>: The scan name
--out  <directory>: The output directory (absolute path)

Required Parameters for Domain Scans

--domain <domain name>: The domain name to scan

Required Parameters for User Scans

--company <company name>: The company name per LinkedIn, used for user scraping

Optional Parameters

--email-domain <domain name>: Allows a different email domain to be configured. Use this if the primary domain is x.com but users receive mail at y.com addresses.
--subdomain-list <file>: Specify a file that provides potential subdomains, this is used by the dnsrecon tool. That tool provides some high-quality default lists.
--nmap: Enable light touch nmap scanning of subdomains
--nmap-aggressive: (Long running!) This is a VERY intense scan on each subdomain, approximately 10 minutes per subdomain.
--shodan <api key>: Specify a Shodan API key and enables a Shodan scan
--hunter <api key>: Sets a Hunter.io API Key and enables Hunter in the user scan. This will vastly speed up the user scan!
--timeout-http <seconds>: Specify a timeout in seconds for HTTP detection
--timeout-eyewitness <seconds>: Specify a timeout in seconds for EyeWitness individual scans

Disabling Major Scan Types

--disable-user: Disable the user scan
--disable-domain: Disable the domain scan

Scan Output
Battalion produces a number of directories which help categorize raw output. All of these directories will be created at the location specified by the --out parameter by the Battalion script.
Expected Scan Time (User)
The current default scan time for the user scan is rather large -- over 20 minutes. We recomment acquiring a Hunter API key to expidite this process.
Expected Scan Time (Domain)
Scans depend very much on the 'size' of the target, where the size is deteremined by the number of users and the number of detected domain records. Small targets usually take at least a few minutes to complete.
Disclaimer
This utility has been created purely for the purposes of research and for improving defense, and is not intended to be used to attack systems except where explicitly authorized. Project maintainers are not responsible or liable for misuse of the software. Use responsibly.
",43
NerdHubMC/Refined-Machinery,Java,"Refined Machinery Mod
",2
bacco007/HomeAssistant-Config,JavaScript,"TBSmartHome - Home Assistant Configuration

Here is the configuration and Documentation for my Home Assistant Setup - very much a work-in-progress
I'm from  so some of the stuff here will have limited use outside of the land downunder.

Table of Contents

Screenshots
Setup
Notes


Screenshots
A few screenshots - I'll add more as the other pages are improved


More Screenshots



Setup
Some Notes about my setup
Server

Lenovo ThinkCentre M73 Tiny (Intel Pentium G3240T, 4Gb RAM, 500Gb HDD)
Ubuntu Server 18.10


Hass.io

AppDaemon
Configurator
Glances
IDE
InfluxDB
MariaDB - Database Server
Node-RED
SSH & Web Terminal

Other Stuff I Run

Docker

Docker-Compose
Portainer
Dockermon for HA


My ""Download Stack""

Jackett
Lidarr
Radarr
SABnzbd
Sonarr
Transmission


Tautulli (Plex Reporting)
Traefik

Network
My Network isn't currently running as I would like it - mainly because I'm unable to deploy my Ubquiti gear in my current location - hopefully that will change soon and I'll update this section with a bit more detail

Notes
Lovelace UI
I've started to use the inbuilt Lovelace UI editor, so my lovelace_ui.yaml file is a backup (and JSON->YAML conversion) of the HA Storage file lovelace - there is a Python file in /custom_components/ called tb_lovelacebackup.py that does the conversion
",2
Library-of-Kaeon/Library-of-Kaeon,Java,"Library of Kaeon
Philosophy
The Library of Kaeon (pronounced ""KAI-on"") is a suite of documents and utilities.
The core documents within the library describe a philosophy referred to as Angaianism,
and the rest of the documents and utilities serve as resources for various projects.
Principles
All of the documents within the Library of Kaeon are stored in a system of folders.
The subfolders of each folder are numbered.
Angaianism
The whole of the Angaianism can be summed up through the following dialogue:
Make all that ought to be into reality.

Sentient entities shall never be destroyed, shall always be in a state of maximum joy, and shall
never be manipulated, and it is good to create sentient entities.

What will you do?

The word ""Kaeon"",
which means ""the way up"",
refers to the first line in the above dialogue,
""Make all that ought to be into reality.""
Kaeon is the core principle of Angaianism.
The word ""Angaia"" means ""the whole world"".
Sections
Canon
The Canon section holds documents specifying information that defines the core principles that the rest of the library expands upon.
These documents are written in ONE,
which itself is established in one of said documents.
Chronicle
The Chronicle section holds multiple folders containing documents on various subjects.
These documents are written in ONE+,
which itself is established in one of said documents.
Collection
The Collection section holds several miscellaneous utilities.
",3
LambdaSchool/React-UI-Components,HTML,"React-UI-Components

This repository is designed to be your first exposure into the world of ReactJS. There are 2 projects to complete in this repository. Project 1 is all about implementing a Social Card in ReactJS. On Project 2 you'll be implementing a Calculator. We've given you the file structure and have gone ahead and added all the files you'll need to be set up for success for each project.

Initializing the application.

This project was put together using create-react-app (CRA). You will not need to install CRA in order to make this project work
Each project has it's own package.json file in it, we'll chat more about this later. So it's already set up for you to install some dependencies that are needed for you to be able to work within the React Ecosystem.
To start the Social Card project, you'll need to cd into Project-1-Social-Card and then into social-card and run yarn install to retrieve all the dependencies.
Inside of ../../social-card you'll then need to run yarn start to open up a React Development Server that can take your .js files as components and bundle them up to work in your new environment.
You'll repeat this last two steps for the calculator project, but you'll need to make sure that you're inside of ../../calculator directory to make this work.

Instructions

For the first project you'll work on the project found in Project-I-Social-Card.
For the second project you'll work on the project found in Project-II-Calculator.
Use the design files to build out your User Interfaces.
All components can be built out using the provided html files found in their respective directories.
Each file has been set up to work within a react.js environment. Pay attention to the notes found within each project.
Don't forget about className vs class on your JSX elements!!

Project I - Social Card

We're going to break down this assignment in terms of how you should be starting to think in react. Remember that everything is a component.
All the files you'll need for this project are found in Project-I-Social-Card/social-card/src/components. You can find all the component files you should need in their respective directory. i.e. inside of the HeaderComponents directory you should see a few .js files and a .css file.
Feel free to add any files for any extra components you may feel the need to build.
Any of the styles you write in your respective .css file should be available for your the components where the .css file is being imported.
We have drawn boxes around possible areas that could be components.

The outer box will represent the App.js file.
The red box around the header could represent the header directory with a few nested components inside, the thumbnail image and the header content.
You could go so far as to break down the header content into a header and body component.
The next box around the social card represents the React Banner image and some copy found underneath.
And then of course the footer (Stretch Problem 1) which contains your icons, could be a container for all of your icon components.




Project 1 MVP requirements


Create a <HeaderContainer /> container component that will hold your header components.

Create an <ImageThumbnail /> component using this image url https://tk-assets.lambdaschool.com/1c1b7262-cf23-4a9f-90b6-da0d3c74a5c6_lambdacrest.png as it's img src.
Create a <HeaderTitle /> component that displays the Lambda School header text, @LambdaSchool handle and timestamp.
Create a <HeaderContent /> component that displays the copy provided in the headers content.



Create a <CardContainer/ > container component that will hold your card components.

Create a <CardBanner /> component that will display this image as it's background: https://tk-assets.lambdaschool.com/fcd75197-7d12-46ec-bc9e-4130f34822fa_reactbackground.png
Create a <CardContent /> component that displays the card copy provided.
The entire <CardContainer /> should take a user to https://www.reactjs.org when clicked.



Project 1 Stretch Problems

Create a <Footer/> component that pulls in the icons and displays them properly.

Build out the functionality so that a user can click on the icons and have them react to events.


Ensure that your product is as pixel perfect as possible using any tools that you were introduced to in previous weeks.
Look up the moment.js library and figure out how to format your time-stamp in your header to be todays date.

Project II - React Calculator

For this project you're not going to be given any tips on how to break down the image file into components. Now that you've had some practice with the social card, this should be something you can start doing on your own. Just think about the image, and what potentially looks like a component.
For the MVP the calculator just needs to be displayed properly. Functionality will be a part of the stretch requirement.
All the files you'll need for this project are found in Project-II-Calculator/calculator/src/components. You can find all the component files you should need in their respective directory. i.e. inside of the ButtonComponents directory you should see a couple .js files and a .css file.
Feel free to add any files for any extra components you may feel the need to build.
Here is what your calculator should look like:



BEFORE YOU DO ANYTHING ELSE READ This
This is the time to stop and break down this image in terms of components. Each container on the screen should give you an idea of what your components should be. You're now about to begin your journey into learning how to think in React.

Project 2 MVP requirements

Create a <NumberButton /> component that can accept props and display any number/symbol passed down as text.

Example your component should be able to render a dynamic prop called text:
Your button button should also be able to accept dynamic props buttonStyle for styling



<button className={props.buttonStyle}>{props.text}</button>

Create an <ActionButton /> component that will be used for the zero character and the clear button.
Create a <CalculatorDisplay /> component that will be used as the calculator display

Project 2 Stretch Problems


Re-factor your App.js file to be a classical component that can hold state on it's constructor. (There is some documentation in training kit on how this works).

On your state object be sure to include a property for the total that can be passed down to your <CalculatorDisplay /> component.



Create some handler functions that can take in some information from an, onClick and use that information to update the total on the App state.

this.setState will be your best friend here :)

GOOD LUCK!


",9
Randrian45/pylustrator,Python,"Pylustrator
 
 

Visualisations of data are at the core of every publication of scientific research results. They have to be as clear as
possible to facilitate the communication of research. As data can have different formats and shapes, the visualisations
often have to be adapted to reflect the data as well as possible. We developed Pylustrator, an interface to directly
edit python generated matplotlib graphs to finalize them for publication. Therefore, subplots can be resized and dragged
around by the mouse, text and annotations can be added. The changes can be saved to the initial plot file as python code.
Keywords: python, matplotlib, draggable subplots, texts, annotations, code generation
Please refer to our Documentation for more information and instructions on installing it.
",4
tanepiper/ngx-tinynodes,TypeScript,"Ngx-Tinynodes
This repository is a collection of Angular components and demos with full documentation.
See the Changelog for the development diary of this site, or visit the documentation of libraries.
You can also find a fully searchable API documentation for all @tinynode components.
Links

Repository
Documentation
Demo Application Site
NPM Collection

Libraries
@tinynodes/ngx-editorjs
This project provides a set of features for using EditorJS within Angular 7+ - including a directive, component and service

Readme
Changelog
Demo
Project Folder

Development Information
This repository is run by Tane Piper and was generated using Nx.
",3
gusdnd852/Social-Robot-Bao,Python,"Social-Robot-Bao
Artificial Intelligence Cure Robot for Children with Autism


Design

",6
mirceapasoi/erc725-735,JavaScript,"ERC 725 + 735
This is an attempt at an implementation of ERC 725 v1 and ERC 735, following the specs as closely as possible. It uses the Truffle framework and Ganache CLI for testing.
Overview
The smart contract implements the following features:

deploy contract using initial set of keys (Identity.sol)
add/remove keys to identity (KeyManager.sol)
get key data, in multiple ways (KeyGetters.sol)
""proxy contract"" execution on the blockchain (MultiSig.sol)
multi-signature mechanism for MANAGEMENT_KEY and EXECUTION_KEY (MultiSig.sol)
add/remove claims to identity (ClaimManager.sol)
get claim data, in multiple ways (ClaimManager.sol)
refresh claims in identity (ClaimManager.sol)
ability to pause/unpause the contract, potentially with multi-sig (Pausable.sol)
ability to destroy the contract and return funds, potentially with multi-sig (Destructible.sol)

Architecture
The implementation tries to make extensive use of Solidity patterns for modular code i.e. libraries, abstract contracts and multiple inheritence. Here's how the class diagram looks:
                +--------------+         +------------+
                |              |         |            |
                |    ERC 165   |         | KeyStore** |
                |              |         |            |
                +---+--------+-+         +----+-------+
                    |        |                |
               +----v-----+ +v---------+ +----v-----+
               |          | |          | |          |
 +-------------+ ERC 735* | | ERC 725* | | KeyBase* |
 |             |          | |          | |          |
 |             +----------+ ++-+----+--+ +--+-------+------+--------------+
 |                           | |    |       |              |              |
 |                           | |    |       |              |              |
 |   +-----------------------+ |    | +-----+-----+  +-----v-----+ +------v-------+
 |   |                         |    | |           |  |           | |              |
 |   |                 +-------|----|-+  Pausable |  | KeyGetter | | Destructible |
 |   |    +--------------------|----|-+           |  |           | |              |
 |   |    |            |       |    | +--+--------+  +-+---------+ +--+-----------+
 |   |    |            |  +----+    |    |             |              |
 |   |    |            |  |         |    |             |              |
 |   |    |            |  |         |    |             |              |
 |   |    |            |  |         |    |             |              |
+v---v----v---+ +------v--v---+  +--v----v--+          |              |
|             | |             |  |          |          |              |
|ClaimManager | | KeyManager  |  | MultiSig |          |              |
|             | |             |  |          |          |              |
+---+---------+ ++------------+  +--+-------+          |              |
    |            |                  |                  |              |
    |            |                  |                  |              |
    |            |        +---------v------------------v---+          |
    |            |        |                                <----------+
    |            +-------->            Identity            |
    |                     |        (ERC 725 + 735)         |
    +--------------------->                                |
                          +--------------------------------+

* = Abstract contract
** = Library

Tests
Truffle tests exists for each contract, in separate files in the test/ folder. Each tests tries to count how much gas it's using for setup and during the test. Also, at the end I'm printing out
total gas used for all tests.
$ ganache-cli --allowUnlimitedContractSize -l 10000000
...
$ truffle test
...
  ✓ should be paused/unpaused by management keys (86435 gas)
	  Test only: 59,944 gas

  54 passing (1m)

Currently missing unit tests for events being emitted.
Open issues

uri is not included in the signature and could theoretically be changed without changing a claim signature. Is this intentional or not?
Claim IDs are generated using keccak256(address issuer + uint256 _topic), which doesn't work great for self-claims i.e. issuer is address(this) and we might want multiple self-claims with the same topic
Added an ExecutionFailed event in ERC725 which isn't part of the standard
For execution requests, I'm using the multi-sig threshold at the time of request, not the one at the time of execution - is that a good idea? (e.g. you request an execution, threshold is X, wait for approvals, threshold is increased to Y, initial execution is approval with X approvals)
Using ERC 165 pseudo-introspection to check if an address implements ERC 725 or 735. Is this the best pattern for that?
Added a PROFILE_TOPIC claim topic which isn't part of the standard. The intended use is to store a plain-text profile URL in data (social media, blogs, etc.). As a convention, uri should be equal to data.
Added a LABEL_TOPIC claim topic which isn't part of the standard. The intended use is to store a plain-text label in data (real name, business name, nick name, brand name, alias, etc.).
The ""proxy contract"" only supports .call, doesn't support .delegateacall or creating a new contract on behalf of the identity.

",42
ABadCandy/BaiDuBigData19-URFC,Python,"BaiDuBigData19-URFC
my two networks solution with 0.67 accuracy for 9 classification.
主要为了用Pytorch复现 https://github.com/czczup/UrbanRegionFunctionClassification 这位大神的tensorflow实现的双分支网络baseline，
同时visit数据的转换和链接中visit2array.py效果一致，即转为7×26×24(天×周×小时)的特征矩阵。
不同点：

数据预处理：除了简单的平移旋转降噪外还事先去除了全黑和全白图片、去雾、直方图均衡化等，剩余训练集图片数为39730张。
图片网络：采用原始3×100×100的尺寸输入，利用imagenet的预训练模型se_resnext101_32x4d进行微调。
visit网络：输入尺寸为7×26×24，不过与tf版本不同之处为前者将24作为通道数，该版本将7作为通道数，这样长宽基本一致，可以利用cifar10或cifar100
的预训练ResNet系列模型进行微调，这里采用的是无预训练的dpn26网络。
特征融合：图片网络最后一层的特征向量维度为256，visit网络最后特征维度为64，concat后为320，最后接9个节点的全连接层进行分类。

使用说明：

预处理数据下载链接：https://pan.baidu.com/s/1Pil1LCesVy4m6Fsb02Ggow  提取码：q5vp

在当前目录下新建data文件夹，将下载好的数据解压至该目录下，最后可以看到data文件夹下有npy,train.test三个子文件夹，
其中npy里存有转换好的train_visit和test_visit，train和test两个子文件夹里分别存放了筛选和预处理后的39730张训练图片，以及原始的1w张测试图片。


执行 pip install -r requirements.txt 安装必要的运行库。


执行 python multimain.py 即可开始训练和测试，其中一些超参数如epoch,batch_size等可在config.py中修改。


等第3步执行完后会在sumbit文件夹下生成csv格式的预测结果，为了与提交系统要求保持一致需要再运行 python submission.py，最终在submit文件夹下得到submit.txt即可提交。


提交结果如图：

",5
Unity-Technologies/EditorXR,C#,"EditorXR
Author XR in XR - Initial public release was on December 15, 2016 via blogpost
Experimental Status
It’s important to note that EditorXR is an experimental feature. As such, there is no formal support (e.g. FogBugz, support@unity3d.com, Premium Support, etc.) offered, so please do not use these channels. Instead, take your questions, suggestions, comments to our dedicated forum.
To help ensure you have a good experience, and to help us answer your questions (hey, we’re a small team!), we encourage you to try it out first with a small VR-ready scene. Please use life-sized objects, nothing too big or small. Dive in and have fun just playing around, instead of trying to use your existing project.
As with any experimental/preview/alpha/beta build, it is always a good idea to make a backup of your project before using the build.
Experimental means this:

We're still adding features!
The current menus, tools, workspaces, actions, etc. are not the end-all-be-all. Each of these have individual designs that will change as we experiment with what works best for UX. EditorXR was designed in such a way that we plan on you being able to replace all of these defaults, too, if you so desire.
Namespaces, classes, software architecture, prefabs, etc. can change at any point. If you are writing your own tools, then you might need to update them as these things change.
There won’t always be an upgrade path from one release to the next, so you might need to fix things manually, which leads to the next point...
Stuff can and will break (!)
There’s no guarantee that this project will move out of experimental status within any specific timeframe.
As such, there is no guarantee that this will remain an actively supported project.

Getting Started
If you've made it here, but aren't accustomed to using GitHub, cloning repositories, etc. and are simply looking to give EditorXR a spin, then take a look at the Getting Started Guide. Once you're up and running we recommend you join the discussion on the EditorXR forum.
For Software Developers
If you're a developer, we recommend that you take a look at the Getting Started Guide and the companion document Extending EditorXR. You'll need to clone the repository into an existing project using the instructions below.
Git Dependencies

git-lfs
git-submodule

Project Asset Dependencies

Textmesh Pro
Legacy Input Helpers (2019.1+)

Users of 2018.3 do not need Legacy Input Helpers



Cloning

Create a new Unity project or use an existing one
From the command line change directory to your project's Assets directory.
Run git lfs clone --recursive -b development https://github.com/Unity-Technologies/EditorXR (Use HTTPS!)

Updating
Because this project uses git-submodule, you'll need to execute git submodule update after pulling whenever a submodule is updated. You could execute this command always just to be safe or if you notice that a submodule is showing as modified after pulling changes.
Optionally, you could add a git hook for post-checkout or use a GUI (e.g. SourceTree) that does this automatically for you.
Project Settings
If you plan on making changes to EditorXR and/or contributing back, then you'll need to set the Asset Serialization property under Edit->Project Settings->Editor to Force Text.
Assembly Definitions
In order to support a variety of platform configurations, and to optionally strip its code out of player builds, EditorXR uses assembly definitions. Some of EditorXR's dependencies do not include assembly definitions in their current forms, so after importing EditorXR (in Unity 2018.3 and below), you must add them.
For easy set-up, EditorXR includes a .unitypackage (Patches/Dependencies_asmdef.unitypackage) containing an assembly definition for the PolyToolkit and UnityEngine.SpatialTracking, which are referenced by EditorXR. Simply import it via Assets > Import Package > Custom Package...
This is not required for Unity versions 2019.1 and above, though you will need to add an assembly definition in order to reference PolyToolkit.
All contributions are subject to the Unity Contribution Agreement (UCA)
By making a pull request, you are confirming agreement to the terms and conditions of the UCA, including that your Contributions are your original creation and that you have complete right and authority to make your Contributions.
",634
LibreOffice/core,C++,"LibreOffice
 
LibreOffice is an integrated office suite based on copyleft licenses
and compatible with most document formats and standards. Libreoffice
is backed by The Document Foundation, which represents a large
independent community of enterprises, developers and other volunteers
moved by the common goal of bringing to the market the best software
for personal productivity. LibreOffice is open source, and free to
download, use and distribute.
A quick overview of the LibreOffice code structure.
Overview
You can develop for LibreOffice in one of two ways, one
recommended and one much less so. First the somewhat less recommended
way: it is possible to use the SDK to develop an extension,
for which you can read the API docs here
and here.
This re-uses the (extremely generic) UNO APIs that are also used by
macro scripting in StarBasic.
The best way to add a generally useful feature to LibreOffice
is to work on the code base however. Overall this way makes it easier
to compile and build your code, it avoids any arbitrary limitations of
our scripting APIs, and in general is far more simple and intuitive -
if you are a reasonably able C++ programmer.
The build chain and runtime baselines
These are the current minimal operating system and compiler versions to
run and compile LibreOffice, also used by the TDF builds:

Windows:

Runtime: Windows 7
Build: Cygwin + Visual Studio 2017


macOS:

Runtime: 10.10
Build: 10.13.2 + Xcode 9.3


Linux:

Runtime: RHEL 6 or CentOS 6
Build: either GCC 7.0.0; or Clang 5.0.2 with libstdc++ 7.3.0


iOS (only for LibreOfficeKit):

Runtime: 11.4 (only support for newer i devices == 64 bit)
Build: Xcode 9.3 and iPhone SDK 11.4



If you want to use Clang with the LibreOffice compiler plugins, the minimal
version of Clang is 5.0.2. Since Xcode doesn't provide the compiler plugin
headers, you have to compile your own Clang to use them on macOS.
You can find the TDF configure switches in the distro-configs/ directory.
To setup your initial build environment on Windows and macOS, we provide
the LibreOffice Development Environment
(LODE) scripts.
For more information see the build instructions for your platform in the
TDF wiki.
The important bits of code
Each module should have a README file inside it which has some
degree of documentation for that module; patches are most welcome to
improve those. We have those turned into a web page here:
https://docs.libreoffice.org/
However, there are two hundred modules, many of them of only
peripheral interest for a specialist audience. So - where is the
good stuff, the code that is most useful. Here is a quick overview of
the most important ones:



Module
Description




sal/
this provides a simple System Abstraction Layer


tools/
this provides basic internal types: 'Rectangle', 'Color' etc.


vcl/
this is the widget toolkit library and one rendering abstraction


framework
UNO framework, responsible for building toolbars, menus, status bars, and the chrome around the document using widgets from VCL, and XML descriptions from /uiconfig/ files


sfx2/
legacy core framework used by Writer/Calc/Draw: document model / load/save / signals for actions etc.


svx/
drawing model related helper code, including much of Draw/Impress



Then applications



Module
Description




desktop/
this is where the 'main' for the application lives, init / bootstrap. the name dates back to an ancient StarOffice that also drew a desktop


sw/
Writer


sc/
Calc


sd/
Draw / Impress



There are several other libraries that are helpful from a graphical perspective:



Module
Description




basegfx/
algorithms and data-types for graphics as used in the canvas


canvas/
new (UNO) canvas rendering model with various backends


cppcanvas/
C++ helper classes for using the UNO canvas


drawinglayer/
View code to render drawable objects and break them down into primitives we can render more easily.



Rules for #include directives (C/C++)
Use the ""..."" form if and only if the included file is found next to the
including file. Otherwise, use the <...> form. (For further details, see the
mail Re: C[++]: Normalizing include syntax ("""" vs
<>).)
The UNO API include files should consistently use double quotes, for the
benefit of external users of this API.
loplugin:includeform (compilerplugins/clang/includeform.cxx) enforces these rules.
Finding out more
Beyond this, you can read the README files, send us patches, ask
on the mailing list libreoffice@lists.freedesktop.org (no subscription
required) or poke people on IRC #libreoffice-dev on irc.freenode.net -
we're a friendly and generally helpful mob. We know the code can be
hard to get into at first, and so there are no silly questions.
",630
Ogg-Technologies/warframe-database,None,"warframe-database
Contains data about warframe
",2
CreatorFan/Unipus-Helper,JavaScript,"Liberator of the oppressed by Unipus

中文版


Introduction
It is a Chrome Extension whose purpose is to liberate broad Chinese college students who suffer oppression by Unipus. Liberate them from boring blank filling and Unipus's devils talons.
Functions

Fill the blank with correct answer automatically.
Generate mistakes to mislead the evil Unipus's system automatically.
Submit all automatically.(developing)
Hang up online 2 hours everyday.(developing)

How to use it

It is easy to use. Just load the unzip file under developer mode on your Chrome.
If you don't use Chrome, you can use it in the same way with a Chromium core browser.

About us
We are a group of college students who are tired of filling the blank on Unipus in vain. We want to liberate students who suffer oppression by Unipus with use our technological capability.
Join us
We need you!
No matter you can code and be a developer or just help us to format key files. History will witness your contribution and all the user will never forget your distinctive contribution.
To contribute

Document of ""To Contribute""
""如何贡献""中文版

",3
aliyunfe/weekly,None,"阿里云前端技术周刊
当下，前端技术蓬勃发展，从 jQuery 到三大框架，从 RN、Weex 到小程序，以及通过 Node.js 为我们拓展更大的疆域，各类技术层出不穷。这就需要我们怀揣着好奇心去探索学习，开阔技术视野，加深技术广度和深度。互联网上的资源虽多但不精，因此希望将优质的学习资源进行整合，《阿里云前端技术周刊》全新启航！
本周刊由阿里云智能商业中台体验技术团队整理编写，希望能给社区带来更多有价值的内容，迎接云上的大前端时代！
知乎：阿里云中台前端/全栈团队专栏
Github：阿里云前端技术周刊
翻译小组：阿里云翻译小组
如何投稿
如何订阅本周刊
与更多同行交流
目录
《阿里云前端技术周刊》第一期
《阿里云前端技术周刊》第二期
《阿里云前端技术周刊》第三期
《阿里云前端技术周刊》第四期
关于我们
我们是阿里云智能商业中台体验技术团队 详情
如有兴趣加入我们，简历/沟通请至： ranmo.cy@alibaba-inc.com/yeshu.lrt@alibaba-inc.com
",606
t-edson/PicPas,Pascal,"Donate to the project

PicPas 0.8.8
Multi-platform Pascal cross-compiler for Microchip 8 bits PIC microcontrollers.

PicPas is a Pascal compiler and IDE, written in Lazarus, which generates executable code for Baseline and Mid-range PIC microcontrollers.
No additional libraries or software required to compile. PicPas generates the *.hex file directly.
PicPas works with a simplified version of the Pascal language, that has been adapted to work with limited resources small devices.
Currently, it only supports basic types.
It includes a complete IDE/debugger/simulator to facilitate the development.
The PicpPas compiler includes advanced optimization options so the code obtained is generally more compact than the obtained with other compilers.
Installation
PicPas doesn't need installation, and have not dependencies, except the commons of the operative system, where it's runnig.
To run, it's only needed to download the folder from GitHub. There is compiled binaries for Windows-64 version (PicPas-win64.exe), Ubuntu version (PicPas-linux) and a Mac version (PicPas-Mac.dmg).
If it's required other platform, it need to be compiled from the source code.
When starting, PicPas could generate warning messsages, if not needed folders exist.
Hello World
As an example the following code, is to blink a LED on port B:
{Sample program to blink a Led on PORTB.7}
program BlinkLed;
uses PIC16F84A;
{$FREQUENCY 8MHZ}
var
  pin: bit absolute PORTB.7;
begin                          
  TRISB := 0;   //all outputs
  while true do 
    delay_ms(1000);
    pin := not pin;
  end;
end.

The processor target is defined including the correspondent unit in the USES section.
The CPU clock is defined using the directive {$FREQUENCY } and must be after the USES section.
Devices supported
Almost all the Mid-range and Baseline PIC devices are supported:
BASELINE DEVICES:
PIC10F200 PIC10F200 PIC10F202 PIC10F204 PIC10F206 PIC10F220 PIC10F222
PIC12F508 PIC12F509 PIC12F510 PIC12F519
PIC16F505 PIC16F506 PIC16F526 PIC16F527 PIC16F54 PIC16F57 PIC16F59
MID-RANGE DEVICES:
PIC10F320 PIC10F322
PIC12F609 PIC12F615 PIC12F617 PIC12F629 PIC12F635 PIC12F675 PIC12F683
PIC12F752
PIC16F73 PIC16F74 PIC16F76 PIC16F77 PIC16F83 PIC16F84 PIC16F87 PIC16F88
PIC16F610 PIC16F616 PIC16F627 PIC16F627A PIC16F628 PIC16F628A PIC16F630
PIC16F631 PIC16F636 PIC16F639 PIC16F648A PIC16F676 PIC16F677 PIC16F684
PIC16F685 PIC16F687 PIC16F688 PIC16F689 PIC16F690
PIC16F707 PIC16F716 PIC16F720 PIC16F721 PIC16F722 PIC16F722A PIC16F723
PIC16F723A PIC16F724 PIC16F726 PIC16F727 PIC16F737 PIC16F747 PIC16F753
PIC16F767 PIC16F777 PIC16F785
PIC16F818 PIC16F819 PIC16F870 PIC16F871 PIC16F872 PIC16F873 PIC16F874
PIC16F874A PIC16F876 PIC16F877 PIC16F882 PIC16F883 PIC16F884 PIC16F886
PIC16F887
PIC16F913 PIC16F914 PIC16F916 PIC16F917 PIC16F946
Support are implemented using units. So if we need compile to the PIC16f628A, we write:
program anything;
uses PIC16F628A; 
begin
  ...
end. 

There is not yet support for Enhanced Mid-range, or the PIC18 High-preformance families of PIC.
IDE
PicPas includes an IDE integrated to the compiler, to help on developing the programs.
Some features of the IDE are:
•	Cross-platform.
•	Multiple editors windows.
•	Syntax highlighting, code folding, word, and line highlighting for Pascal and ASM.
•	Code completion, and templates for the common structures IF, REPEAT, WHILE, …
•	Shows the assembler code and the resources used.
•	Support for themes (skins).
•	Code tools for completion and navigation.
•	Check syntax in REAL TIME!!!.
•	Several setting options.
•	Translated to english, french, spanish and german.



Debugger/Simulator
PicPas includes a graphical debugger/simulator for instructions of the Mid-Range core:

To control the execution, the following keys can be used:
F5 -> Set a breakpoint in the current position of  the assembler code.
F6 -> Reste the device.
F7 -> Step by step into subroutine.
F8 -> Step by step over subroutine.
F9 -> Run the program in real time.
Optimization Comparison
PisPas has been compared in two code optimization competition against the best profesional compilers for PIC microcontrollers, obtaining the first place in both.
Firts competition
Compiling a simple light sequence:
https://github.com/AguHDz/PicPas-Librerias_y_Programas/tree/master/Comparacion%20PicPas-Otros%20Compiladores
Result:

Second competition
Compiling a digital clock using I2C and the DS1307:
https://www.facebook.com/groups/electronicaymicrocontroladores/permalink/1812269192135162/
Result

Language Reference
Program structure
program <Name>;  //optional
uses
  //Units declarations

const
  //Constants declarations

var
  //Variables declarations

//<Procedures declaration>

begin
  //Main program body
end.

Unit structure
unit <name>;
interface
uses
  //units declaration
const
  //Constant declaration
var
  //Variable declaration

//Procedures declaration

implementation

uses
  //units declaration
const
  //Constant declaration
var
  //Variable declaration

//Procedures implementation

end.

Operators
Operator            Precedence
=================== ==========
 NOT, sign “-“         6
 *, DIV, MOD, AND      5
 +, -, OR, XOR         4
 =, <>, <, <=, >, >=   3
 := +=                 2

Types
Type           Size
============== ==========
 bit           1 bit
 boolean       1 bit
 byte          1 byte
 char          1 byte
 word          2 bytes
 dword         4 bytes

Numerical types are all unsigned.
Variables
Variables are defined with the VAR keyword:
var
  var1 : byte;
  var2 : bit;
  large_name_variable: boolean;

Variables can be defined too, at an absolute memory address:
var
  PORTB: BYTE absolute $06;
  pin0: bit; absolute $06.0;
  pin1: boolean; absolute PORTB.bit1;

Bit access can be performed too, using fields:
  var_byte.bit0 := 1;
  var_byte.bit7 := 0;

Specific byte of a word, can be access using fields:
  word_var.Low := $ff;
  word_var.High := $ff;

Control structures
PicPas doens't follow the common Pascal syntax. Instead, a new Modula-2, style syntax is implemented.
The common control structures have the following forms:
IF <condition> THEN 
  <block of code>
END;

IF <condition> THEN 
  <block of code>
ELSE
  <block of code>
END;

IF <condition> THEN 
  <block of code>
ELSIF <condition> THEN 
  <block of code>
ELSE
  <block of code>
END;

WHILE <condition> DO
  <block of code>
END;

REPEAT
  <block of code>
UNTIL <condition>;

FOR  <variable> := <start-value> TO <end-value> DO 
  <block of code>
END;

System Functions
System functions are always available in code. They don't need to be defined or included in a unit.
FUNCTION       DESCRIPTION
============== =================================================
delay_ms()	   Generate a time delay in miliseconds, from 0 to 65536.
Inc()          Increase a variable.
Dec()          Decrease a varaible.
SetBank()      Set the current RAM bank.
Exit()         Exit from a procedure or end the program.
Ord()          Convert a char to a byte.
Chr()          Convert a byte to a char.
Bit()          Convert an expression to a bit expression.
Byte()         Convert an expression to a byte expression.
Word()         Convert an expression to a word expression.
DWord()        Convert an expression to a dword expression.
SetAsInput()   Set a 8-bits port or a pin as an input.
SetAsOutput()  Set a 8-bits port or a pin as an output.

Procedure and Functions
PicPas use the Modula-2 syntax for procedures and functions:
Proedures are declared in the common Pascal syntax:
  procedure proc2(par1: byte);
  begin
    if par1 = 0 then 
      exit;
    else
      par1 := 5;
    end;  
  end;

Functions are declared the same, but indicating the type to return:
procedure TheNext(par1: byte): byte;
begin
  exit(par1 + 1);
end;

The return value is indicated with the exit() instruction.
When using in procedures parameters, a REGISTER parameter can be included:
procedure QuickParameterProc(register regvar: byte);
begin
  //Be carefull if put some code here
  PORTB := regvar;
end;

REGISTER parameters are fast, because they use the W register, so only one REGISTER parameter can be used.
As REGISTER parameter is stored in W register, any operation using the W register, could lose its value, so the first operation in a procedure, using a REGISTER parameter must be read this parameter.
Interrupts
To manage interrupts, PicPas let us to define a special kind of Procedure:
  procedure My_ISR; interrupt;
  begin

    //ISR code

  end;

The name of the procedure is not important, but the declaration must be followed but the reserved word INTERRUPT.
Only one INTERRUPT procedure is allowed in a program.
When PicPas compile an INTERRUPT procedure, some special criterias are considered:

Are always compiled starting in the address 0x0004.
A RETFIE instruction is added to the end of the routine.
No additional bank switching instructions are generated at the beginning of the procedure. It is the responsibility of the programmer to properly handle the banks within the routine.

INTERRUPT procedures don't save the value of registers or the control flags. This should be done manually.
ASM blocks
PicPas have a complete support for inserting ASM code inside the Pascal source.
ASM blocks must be included between the delimiters ASM and END:
procedure DoSomething;
begin
  x := 10;
  asm
    ;Add 2 to the address $20 
    MOVLW 2
    ADDWF $20, F
  end
end;

ASM blocks are not instructions, that's why they are not finished with "";"". It lets the ASM block, to be included in almost any place of the source code, like a comment.
WARNING: Changing the RAM bank, inside an ASM block, can generate errors in compilation or in the code compiled. PicPas know always the current RAM bank, when compiling, but is not aware of the changes can be made inside ASM blocks.
Absolute and relative Labels can be used too:
asm 
  GOTO $+1   ;jump one position forward
end

asm 
  ;infinite loop
label:
  NOP
  GOTO label
end

Program variables can be accessed, using his common name:
var 
 byte1: byte; 
 car1: char; 
 bit1: bit;
 bol1: boolean; 
 word1: word;
 dword1: dword;
begin
  //Low level clear
  asm 
    CLRF byte1
    CLRF car1
    BCF bit1
    BCF bol1
    CLRF word1.Low
    BCF word1.high.bit1
	CLRF dword1.low
	CLRF dword1.high
	CLRF dword1.extra
	CLRF dword1.ultra
  end
end.

Constant can be accessed too, using the same way.
It's possible to use the directive ORG inside a ASM block, too:
  asm 
    org $-2
  end
  vbit := 1;

The address in ORG, can be absolute or relative.
WARNING: Changing the PC pointer with ORG, can generate errors in the compilation or in the code compiled.
Pointers
Pointers are supported in PicPas, only for addresses going from $00 to $FF (1 byte size), thus they can cover only the RAM memory in banks 0 and 1.
Pointers must be declared usin first, a type declaration in the common Pascal style:
type
  ptrByte: ^Byte;
  ptrByte: ^Word;
var
  pbyte: ptrByte;
  pword: ptrWord;

Pointers can be assigned like variables or using addresses form others varaibles:
type
  ptrByte: ^Byte;
var
  pbyte: ptrByte;
  m    : byte;
begin
  pbyte := @m;    //Assign address
  pbyte^ := $ff;  //Write value
  //Now “m” is  $ff
end.

The operator ""@"" return the address of a variable.
Pointers support some basic operations:
Assign   :	p1 := p2;
Compare  : 	if p1 = p2 then ...
Increment:	Inc(p);
Decrement:	Dec(p);
Add      :	p1 + p2 + 1
Subtrac  :	p1 - 5
Directives
Directives are special instructions inserted in the source code that are interpreted and executed by the compiler when compiling the source code (in compilation time).
Directive Programming Language
Directives have their own programmig language. It's a simple and interpreted language (with instructions, variables, operators and conditional structures) what is different from Pascal.
Some features of this programming language are:

It's case insensitive, like Pascal is.
Instructions are contained in one single line and are delimited by {$ … }
It's not a typed language. Variables can change their type and value in execution and different type variables can be assigned.
Variables don't need to be defined before using.
There are only two types for variables: strings and numbers.

Variables
Variables are assigned with the instruction $SET:
{$SET x = 1}
{$SET y = 1 + x}
{$SET x = 'I'm now a string'}

$SET, is not a declaration, but an assignment. First time a variable is assigned, it's created.
Content of a variable, can be shown using instructions like $MSGBOX oo $INFO:
{$MSGBOX 'x is:' + x}
System Variables
There are some system variables, accessible from the directives language. They are:
{$MSGBOX PIC_MODEL} -> Shows the PIC model defined.
{$MSGBOX PIC_FREQUEN} -> Shows the Clock frequency.
{$MSGBOX PIC_MAXFREQ} -> Shows the Max Clock frequency for the device.
{$MSGBOX PIC_NUMBANKS} -> Shows the RAM banks number for the device.
{$MSGBOX SYN_MODE} -> Shows the syntax Mode of the compiler.
{$MSGBOX CURRBANK} -> Shows the current RAM bank.
(*) To see the complete list, check the User Manual.
List of Directives
The next directives are supported by PicPas:
$PROCESSOR
Specify the target device model of the microcontroller. Example:
{$PROCESSOR PIC16F628A}

The devices supported using $PROCESSOR directive are:
Baseline: PIC10F200 PIC10F202 PIC10F204 PIC10F206
Mid-Range: PIC16C63 PIC16CR63 PIC16C65 PIC16C65A PIC16CR65 PIC16F72 PIC16F83 PIC16CR83 PIC16F84 PIC16CR84 PIC16F84A PIC16F870 PIC16F871 PIC16F872 PIC16F873 PIC16F873A PIC16F874 PIC16F874A PIC16F876 PIC16F876A PIC16F877 PIC16F877A PIC16F887 PIC16F627A PIC16F628A PIC16F648A
This directive is a short form to define a device, however it's preferred to define devices using directives, like $SET_STATE_RAM, $SET_MAPPED_RAM, $CLEAR_STATE_RAM.
$FREQUENCY
Specify the clock frequency, in MHz or KHz. Example:
{$FREQUENCY 10Mhz}

Frequency information is used for:

The compiler, when needed to generate delays.
The simulator, for Real Time simulation.

If delays are used in the program, only some frequencies are supported. They are:
1MHz, 2Mhz, 4Mhz, 8MHz, 10MHz, 12MHz, 16MHz or 20MHz.
If frequency is not specified, the default value is 4MHz.
$MODE
Specify the syntax mode, used by the compiler. The allowed values are:
{$MODE PICPAS} -> Default mode. Use the new syntax for the control structures.
{$MODE PASCAL} -> Clasic Pascal mode. Use the common Pascal syntax for the control structures.
$MSGBOX
Shows a text message in the screen:
{$MSGBOX 'Hello World'} -> Shows the message 'Hello World' in the screen.
{$MSGBOX PIC_MODEL} -> Shows the system variable PIC_MODEL, that is the PIC model defined.
{$MSGBOX PIC_FREQUEN} -> Shows the Clock frequency.
{$MSGBOX 'clock=' + PIC_FREQUEN}  -> Shows the message: ""clock=8000000"" (if the Frequency was set to 8MHz).
$MSGERR
Shows a text message in the screen, with an error icon.
$MSGWAR
Shows a text message in the screen, with a warning icon.
$CONFIG
Sets the configuration bits of the device.
{$CONFIG $3FFD}

{$define _CP_ON       =     0x000F}
{$define _CP_OFF      =     0x3FFF}
{$define _WDT_OFF     =     0x3FFB}
{$define _LP_OSC      =     0x3FFC}
{$define _XT_OSC      =     $3FFD}

{$CONFIG _CP_OFF, _XT_OSC, _WDT_OFF }

{$CONFIG _CP_OFF _XT_OSC _WDT_OFF }

$INCLUDE
Includes the contents of a external file, into de source code:
{$INCLUDE aaa.pas}
{$INCLUDE d:\temp\aaa.txt}
x := {$INCLUDE expression.txt};

$OUTPUTHEX
Defines the name of the output binary file *.hex.
{$OUTPUTHEX myoutput.hex}  // Relative path
{$OUTPUTHEX d:\temp\myoutput.hex}  //Absolute path

When relative path is used, the file will be created in the same folder the Pascal program is.
If it's not defined the name of the *.hex file, it will be used the name of the program/unit compiled. So if the program is called ""myprogram"" (and the file is ""myprogram.pas""), then the *.hex file will be ""myprogram.hex"".
Directive {$OUTPUTHEX}, can be placed in any part of the source code and can be used several times. If so, the output file will be the defined by the last directive.
$DEFINE
Define symbols or macros
To define a symbol we can do:
{$define MY_SYMBOL}

Once defined, it can be tested using $IFDEF directive.
To define a macro we can do:
{$DEFINE macro=Hello}

Then we can expand a macro, in the code, using the way:
{$macro}
Following, there a sample code:
{$DEFINE pin_out=PORTB.0}
uses PIC16F84A;
begin
  SetAsOutput({$pin_out});
  {$pin_out} := 1;
end.

$SET
Set a value for a variable. If variables doesn't exist, it will be created.
{$SET pin_out='PORTB.0'}
uses PIC16F84A;
begin
  SetAsOutput({$pin_out});
  {$pin_out} := 1;
end.

Variables can be numbers or string.
Variables supports expresions:
{$SET a_var = 1 + 2 * another_var + 2 ^ sin(0.5)}

Unlike macros, variables values are solved when assigned. Macros values, are solved when macro is referenced.
$IFDEF, $ELSE, $ENDIF
This directives let us to define conditional compilation blocks:
Directive $IFDEF check the existence of some macro or variable and according to that, compile or not some blocks of code.
It has two forms:
{$IFDEF <identifier>} 
... 
{$ENDIF}

{$IFDEF <identifier>} 
... 
{$ELSE}
... 
{$ENDIF}

The next code is an example of use:
{$DEFINE MyPinOut=PORTB.0}
uses PIC16F84A;
begin
{$IFDEF MyPinOut}
{$ELSE}
  {$DEFINE MyPinOut=PORTB.1}
{$ENDIF}
  SetAsOutput({$MyPinOut});
  {$MyPinOut} := 1;
end.

$IFNDEF
This directive is the opposite version of $IFDEF.
{$DEFINE MyPinOut=PORTB.0}
uses PIC16F84A;
begin
{$IFNDEF MyPinOut}
  {$DEFINE MyPinOut=PORTB.1}
{$ENDIF}
  SetAsOutput({$MyPinOut});
  {$MyPinOut} := 1;
end.

$IF
This directives let us to define conditional compilation blocks, using expressions:
Directive $IF evaluates an expression, and according to the result, compile or omit some blocks of code.
The common syntax is:
{$IF <expression>} 
... 
{$ENDIF}

A long way can be used too:
{$IF <expression>} 
... 
{$ELSE}
... 
{$ENDIF}

The following code shows an example of use:
{$IF value>255}
var x: word;
{$ELSE}
var x: byte;
{$ENDIF}

As there is not a boolean type, a boolean expression returns the number 1 when the expression is TRUE and 0 when the expression is FALSE.
On the other side, instruction {$IF} will consider as TRUE, any number different from 0, or any string not empty.
$IFNOT
It's the opposite version of $IF.
{$IFNOT value>255}
var x: byte;
{$ELSE}
var x: word;
{$ENDIF}

$SET_STATE_RAM
Set the state of the RAM memory for the current device.
The state of a byte of RAM can have 3 values:

SFR: Special Function Register, like STATUS or TRISB.
GPR: General Purpose Register. Used as free memory for the user.
NIM: Not implemented cell.

$SET_STATE_RAM, let us to define the state of the RAM using a range of addresses.
The syntax of $SET_STATE_RAM is:
{$SET_STATE_RAM <list of commands>}

COmmands are separaed by commas. One command have teh syntax:
-:
One valid example, for this directive, would be:
{$SET_STATE_RAM '000-00B:SFR'};

That indicates the bytes in RAM from $000 to $00B are SFR.
Addresses are expressed always in hexadecimal.
Other example are:
//Multiple commands in one directive
{$SET_STATE_RAM '000-00B:SFR, 00C-04F:GPR'}  
//Set state for all banks
{$SET_STATE_RAM '000-00C:SFR:ALL'}  
//Set state for all banks and map them to bank 0
{$SET_STATE_RAM '000-00C:SFR:ALLMAPPED'}  

$SET_MAPPED_RAM
Define mapped regions of the RAM memory, for the current device.
RAM memory can be implemented as independent or mapped RAM. Mapped RAM usually points to other RAM bank. One register can be mapped in several banks. That's the case of registers like STATUS or INTCON, mapped in all the banks of the RAM.
$SET_MAPPED_RAM, can map ranges of RAM in register GPR and SFR. It has not sense to map unimplemented RAM.
The syntax for $SET_MAPPED_RAM is:
{$SET_MAPPED_RAM <list of commands>}

Commands are separated by commas. One command have the form:
Start address>-<End address>:<Target bank>

Target bank can be:
bnk0, bnk1, bnk2 or bnk3 for the Mid-Range PIC core devices (14 bits instruction).
bnk0, bnk1, bnk2, bnk3, bnk4, bnk5, bnk6 or bnk7 for the Baseline PIC core devices (12 bits).
A valid example, for a Mid-Range PIC would be:
{$SET_MAPPED_RAM ' 080-080:bnk0'};
This instruction defines the RAM address $080 as a register mapped at the bank 0, corresponding to the address 0x00.
Addresses are expresed always as a 3 digit hexadecimal number.
$CLEAR_STATE_RAM
USed to define the initial state of RAM memory.
$CLEAR_STATE_RAM, set the state of all the RAM as unimplemented, clearing all previous setting.
It's used before of starting to define the RAM for a device, using the directives $SET_STATE_RAM and $SET_MAPPED_RAM.
$RESET_PINS
Clear all the configuration for the pines defined in the microcontroller.
{$RESET_PINS}

This directive is generally used before of defining the microcontollers pins with the directive {$SET_PIN_NAME}
$SET_PIN_NAME
Define the name for a specified pin of the microcontroller.
The syntax is:
{$SET_PIN_NAME <pin number>:<name>}

One example would be:
{$SET_PIN_NAME '2:VDD'}

This definition would make the label ""VDD"" will appear in the pin 2 of the graphic representation of the PIC, when using the debugger.,
$MAP_RAM_TO_PIN
Assign some bits of the RAM, to physical pins of a microcontroller. This is used to map the registers GPIO, PORTA, PORTB, …, to pins of the device.
This assignment is needed to a have a better visual effect in the simulation of the PIC, when using the debugger. This way we will see the pin highlighted when it has a high level (bit set to 1).
The syntax of $MAP_RAM_TO_PIN is:
{$MAP_RAM_TO_PIN <address>:<list of associations>}

Associations are separated by commas. One association have the form:
<number of bit>-<number of pin>

One valid example would be:
{$MAP_RAM_TO_PIN '005:0-17,1-18,2-1,3-2,4-3'};

This instruction indicates the bits  0, 1, 2, 3 and 4, of the address $05, are mapped to the pins 17, 18, 1, 2 y 3 respectively.
Values for number of bit and pins are in decimal.
$SET_UNIMP_BITS
Defines bits not implemented in some specific positions of the RAM.
This setting is used to model the RAM in a accurate way (to the bit level) in order to have a better and realistic simulation of the device.
The syntax of $SET_UNIMP_BITS is:
{$SET_UNIMP_BITS <list of commands>}

The commands are separated by commas. One command have the form:
<address>:<mask>

The address and the mask are expressed in hexadecimal using 3 and 2 digits respectively.
One valid example would be:
{$SET_UNIMP_BITS '005:1F'};

And indicates the bits 5, 6 and 7, of the position $005 (PORTA) are not implemented in the hardware and will be read always as 0.
$SET_UNIMP_BITS1
Defines bits not implemented in some specific positions of the RAM.
This instruction works in the same way of $SET_UNIMP_BITS, but the unimplemented bits will be read always as 1, instead of 0.
One valid example would be:
{$SET_UNIMP_BITS1 '004:E0'};
And indicates the bits 5, 6 and 7, of the position $004 are not implemented in the hardware and will be read always as 1.
(*) For more information about directives, check the User Manual.
Defining custom devices
PicPas have complete support to define the hardware of microcontrollers, using directives.
Practically all devices from Baseline and Mid-Range families can be defined in this way.
Following, there is an example of defining a microcontoller similar to the  PIC16F84:
//Define hardware
{$SET PIC_MODEL='MY_PIC'}
{$SET PIC_MAXFREQ = 1000000}
{$SET PIC_NPINS = 18}
{$SET PIC_NUMBANKS = 2}
{$SET PIC_NUMPAGES = 1}
{$SET PIC_MAXFLASH = 1024}
//Clear memory state
{$SET_STATE_RAM '000-1FF:NIM'}
//Define RAM state
{$SET_STATE_RAM '000-00B:SFR, 00C-04F:GPR'}
{$SET_STATE_RAM '080-08B:SFR, 08C-0CF:GPR'}
//Define mapped RAM
{$SET_MAPPED_RAM '080-080:bnk0, 082-084:bnk0, 08A-08B:bnk0'}
{$SET_MAPPED_RAM '08C-0CF:bnk0'}
//Define unimplemented bits in RAM
{$SET_UNIMP_BITS '003:3F,083:3F,005:1F,085:1F,00A:1F,08A:1F'}

To see more examples of definig devices, check the folders /devices10 and /devices16.
PicPas Limitations
•	Only basic types are implemented: bit, byte, char, boolean, word an dword(limited support).
•	Cannot declare arrays or records.
•	No recursion implemented, Because of the limited hardware resources, available in PIC devices.
•	No float point implemented.
Some of these limitations must be solved in next versions.
Development
PicPas is a free software (GPL license) and it's opened for the collaboration of anyone who is interested.
There is still, much work for development or documentation, so any help will be appreciated.
Source Code
The source code of the compiler is in the folder /Source.
To compile PicPas, it's needed to have the following libraries:

SynFacilUtils
MisUtils
MiConfig
PicUtils
t-Xpres
UtilsGrilla
ogEditGraf

All of them, must be availables on the GitHub. Check the versions used.
These libraries don't include package. They are only files in folders that need to be included when compiling PicPas.
PicPas has been compiled, using the version 1.8.0 of Lazarus. Tested in Windows, Ubuntu and Mac.
To have more information about the compiler, check the Technical Documentation (Only in spanish by now).
Libraries
PicPas is a new project and it's still in development and there are not dedicated libraries for the compiler.
The best repository for libraries and useful code is in: https://github.com/AguHDz/PicPas-Librerias_y_Programas
",39
nkonev/blog,Java,"



Features

Zero-downtime update deployment
Fast page loading due client-side rendering
Fulltext search by posts
Updating posts through web STOMP on main page
Draft posts that visible only for author and administrator
User locking
User deletion (with migrating posts to special deleted user)
Pages prerendering for crawlers with rendertron
Dynamically setting header, subheader and background image without server restart
Auto cleaning ""orphanned"" images from PostgreSQL, and ""orphaned"" posts from Elasticsearch
Cluster out from the box - simple scale it with docker service scale BLOGSTACK_blog=4
Login through Facebook, Vkontakte OAuth2 providers
Binding several OAuth2 account to same blog account
Simply installation with docker swarm
Applications like Vkontakte/Facebook apps. Example store application on Go
Self-sufficient frontend asset. No CDN used.

Requirements
Run

Docker 18.06.0+

Development

JDK 12
docker-compose 1.16.1 +
Google Chrome (as default browser for webdriver-test). Just dnf install chromium in latest Fedora.

FAQ
Q: Can I run it without docker ?
A: Yes, you can achieve it by manually install PostgreSQL, RabbitMQ, Redis, Elasticsearch and configure it's connections in config or through commandline. See Spring Boot documentation https://docs.spring.io/spring-boot/docs/2.1.1.RELEASE/reference/html/boot-features-external-config.html.
Q: How to build frontend if I am backend developer ?
A:
./mvnw -P frontend generate-resources
Q: How to build full jar (with static) ?
A:
./mvnw -P frontend clean package
It will download java dependencies and nodejs with frontend dependencies.
Q: Why does blog wait for PostgreSQL, Elasticsearch, Redis, RabbltMQ port availability on boot?
A: Primarily for deploy tests runned inside Travis. When there isn' t these waits, I had spirously tests fails due inpredictable time of Elasticsearch boot.
Embedded API documentation
Embedded documentation are available at http://127.0.0.1:8080/docs/index.html
Request version info
This will available after full package, e. g. after resource filtering of git.template.json and renaming result in target/classes/static dir to git.json
curl -i http://127.0.0.1:8080/git.json

Running on Windows without docker
First you should install Redis, PostgreSQL, Rabbit MQ, Elasticsearch
and manually setup them (create database, schema, user for PostgreSQL, install web stomp plugin and create user for RabbitMQ).
Redis Windows x86 which works on my PC (Windows 7 x86)
http://bitsandpieces.it/redis-x86-32bit-builds-for-windows
2.8.2104 http://fratuz610.s3.amazonaws.com/upload/public/redis-builds/x86/redis-windows-x86-2.8.2104.zip - requires enabled swapfile.
run
redis-server.exe --maxheap 8Mb

Next you should use localhost IP addresses and disable asciidoctor:
mvnw -P local -Dasciidoctor.skip=true clean test

Demo Run / Installation
cd docker
./swarm-init.sh
I strongly recommend copy and rename docker-compose.template.yml to docker-compose.stack.yml.
Next I'll use renamed file.
Copy files on your server:
scp -r /path/to/blog/docker/* user@blog.test:/path/to/blog/
chmod 600 traefik/acme.json
Manual changes
Let' s assume cd docker.
a) ./swarm-init.sh
b) In docker-compose.template.yml or docker-compose.stack.yml:.
Change tag in service blog image: nkonev/blog:current-test -> image: nkonev/blog:latest
Also you can remove demo profile
c) Change next properties:
      - SPRING_MAIL_HOST=smtp.yandex.ru
      - CUSTOM_EMAIL_FROM=username@yandex.ru
      - SPRING_MAIL_USERNAME=username
      - SPRING_MAIL_PASSWORD=password
      - CUSTOM_BASE-URL=http://blog.test
 

And remove explicit ports definition where it's don't need - postgres, redis, rabbit, because of docker publishes ports by add it to iptables chain.
If you very want, you can skip setting these properties, but you'll have non-working email, wrong links in emails and so on.
d) Generating monitoring grafana & prometheus password
sudo yum install -y httpd-tools
# generate login and hash with replaced $ with $$ sign for able to copy-paste to docker-compose.stack.yml
htpasswd -nb admin admin | sed -e 's/\$/\$\$/g'
e) Set journald logging with appropriate tag for all services
    logging:
      driver: ""journald""
      options:
        tag: blog
f) Uncomment & change SSL setting in ./traefik/traefik.toml
g) Configure notifications in ./alertmanager/alert.yml
i) For able to http(s) request your domain registrar name with curl from container
ensure that
cat /proc/sys/net/ipv4/ip_forward
returns non-zero
next
Option a)
firewall-cmd --permanent --zone=public --add-port=80/tcp
firewall-cmd --permanent --zone=public --add-port=443/tcp
firewall-cmd --reload
Check
firewall-cmd --list-all-zones
iptables -t nat --line-numbers --numeric --list
Option b) insert iptables rule
iptables -I INPUT -i docker_gwbridge -p tcp -m multiport --dports 80,443 -j ACCEPT
If all ok, you should do it persistent by
chmod +x /etc/rc.local
vim /etc/rc.local
iptables -I INPUT -i docker_gwbridge -p tcp -m multiport --dports 80,443 -j ACCEPT
echo ""Successful inserted docker_gwbridge rule""
Starting with docker swarm
Next you can
docker stack deploy --compose-file docker-compose.stack.yml BLOGSTACK
docker service scale BLOGSTACK_blog=4
docker service ls
See postgres volume
docker volume inspect BLOGSTACK_postgresql_blog_dev_data_dir
See logs of jars
via journalctl (see applied tags in docker-compose.stack.yml):
journalctl -f CONTAINER_TAG=blog
journalctl -f CONTAINER_TAG=blog -o verbose
journalctl -f CONTAINER_TAG=blog CONTAINER_TAG=postgresql CONTAINER_TAG=redis CONTAINER_TAG=rabbitmq
or via docker
docker service logs -f BLOGSTACK_blog
Remove
docker stack rm BLOGSTACK
Remove exited containers
docker rm $(docker ps -aq -f name=BLOGSTACK_blog -f status=exited)
Test on local machine
curl
curl -H ""Host: blog.test"" http://127.0.0.1:8088
curl -H ""Host: grafana.blog.test"" -u ""admin:admin"" http://127.0.0.1:8088
curl -H ""Host: prometheus.blog.test"" -u ""admin:admin"" http://127.0.0.1:8088
curl -H ""Host: alertmanager.blog.test"" -u ""admin:admin"" http://127.0.0.1:8088
Browser
We add domains to /etc/hosts for browser sends correct Host header
sudo tee --append /etc/hosts <<'EOF'
127.0.0.1 blog.test
127.0.0.1 grafana.blog.test
127.0.0.1 prometheus.blog.test
127.0.0.1 alertmanager.blog.test
EOF
Maintenance
docker ps -aq | xargs docker rm
docker volume ls -q | xargs docker volume rm
docker images -q -a | xargs  docker rmi
Open PostgreSQL
docker exec -it $(docker ps --filter label=com.docker.swarm.service.name=BLOGSTACK_postgresql -q) psql -U blog
docker exec -it $(docker ps --filter label=com.docker.swarm.service.name=TESTBLOGSTACK_postgresql -q) psql -U blog
Open blog
docker exec -it $(docker ps --filter label=com.docker.swarm.service.name=BLOGSTACK_blog -q | head -n 1) bash
SEO
First configure custom.rendertron.serviceUrl - setup correct url of Rendertron installation. See also dockerized build.
How to add SEO metrics scripts
Just prepend file: location which contains index.html, and copy modified index.html to there folder.
spring.resources.static-locations: file:/var/www/, file:backend/src/main/resources/static/, classpath:/static/
So firstly Spring Mvc will looking in /var/www, next in $PWD/backend/src/main/resources/static/...
If your search(Yandex Metrics for example) checks for existence script - request will passed through rendertron, which wipes <script> tags.
In order to solve it, use custom.seo.script=file:/var/www/seo.html - Rendertron filter will inject content of
this file before closing </head>.
Grafana
Fix disk usage in https://grafana.com/dashboards/1860
Set query
100 - ((node_filesystem_avail_bytes{mountpoint=""/rootfs""} * 100) / node_filesystem_size_bytes{mountpoint=""/rootfs""})
Set Instant
TODO

re-implement buttons css
sitemap for SEO
edit metainfo for SEO by user
change post owner by admin
change comment owner by admin
LDAP
Google OAuth2 login
search by comments

",5
Pathoschild/StardewMods,C#,"This repository contains my SMAPI mods for Stardew Valley. See the individual mods for
documentation and release notes.
Mods
Active mods:


Automate (source)
Place a chest next to a machine (like a furnace or crystalarium), and the machine will
automatically pull raw items from the chest and push processed items into it. Connect multiple
machines with a chest to create factories.


Chests Anywhere (source)
Access your chests from anywhere and organise them your way. Transfer items without having to
run around, from the comfort of your bed to the deepest mine level.


Content Patcher (source)
Load content packs that change the game's images and data without replacing XNB files. Unlike
XNB mods, these content packs get automatic update checks and compatibility checks, are easy to
install and uninstall, and are less likely to break due to game updates.


Crops Anytime Anywhere (source)
Lets you grow crops in any season and location, including on grass/dirt tiles you normally
couldn't till.


Data Layers (source)
Overlays the world with visual data like accessibility, bee/Junimo/scarecrow/sprinkler coverage,
etc. It automatically includes data from other mods if applicable.


Debug Mode (source)
Press a button to view debug information and unlock the game's built-in debug commands
(including teleportation and time manipulation).


Fast Animations (source)
Speed up many animations in the game (currently eating, drinking, milking, shearing, and
breaking geodes). Optionally configure the speed for each animation.


Lookup Anything (source)
See live info about whatever's under your cursor when you press F1. Learn a villager's favourite
gifts, when a crop will be ready to harvest, how long a fence will last, why your farm animals
are unhappy, and more.


Noclip Mode (source)
Toggle noclip mode at the press of a button,
letting you walk through anything (even map boundaries).


Rotate Toolbar (source)
Rotate the top inventory row for the toolbar by pressing Tab (configurable).


Skip Intro (source)
Skip straight to the title screen or load screen (configurable) when you start the game. It also
skips the screen transitions, so starting the game is much faster.


Small Beach Farm (source)
Replaces the riverlands farm with a fertile pocket beach, suitable for slower or challenge runs.


Tractor Mod (source)
Lets you buy a tractor to more efficiently till/fertilize/seed/water/harvest crops, clear rocks, etc.


Inactive mods:


No Debug Mode
(deleted) Disables SMAPI's F2 debug mode, which can cause unintended effects like skipping an
entire season or teleporting into walls. No longer needed after SMAPI 1.0.


The Long Night (source)
Disables collapsing. You just stay awake forever and the night never ends (until you go to bed).
Broke permanently in Stardew Valley 1.3.20.


Translating the mods
The mods can be translated into any language supported by the game, and SMAPI will automatically
use the right translations.
(❑ = untranslated, ↻ = partly translated, ✓ = fully translated)



 
Chests Anywhere
Data Layers
Debug Mode
Lookup Anything
Noclip Mode
Tractor Mod




Chinese
✓
✓
✓
↻ partial
❑
✓


French
↻ partial
↻ partial
✓
↻ partial
❑
✓


German
✓
✓
✓
↻ partial
❑
✓


Hungarian
❑
❑
❑
❑
❑
❑


Italian
❑
❑
❑
❑
❑
❑


Japanese
↻ partial
↻ partial
✓
↻ partial
❑
✓


Korean
✓
✓
✓
↻ partial
❑
✓


Portuguese
↻ partial
↻ partial
✓
↻ partial
❑
✓


Russian
✓
✓
✓
↻ partial
❑
✓


Spanish
✓
↻ partial
✓
↻ partial
❑
✓


Turkish
❑
❑
❑
❑
❑
❑



Contributions are welcome! See Modding:Translations
on the wiki for help contributing translations.
Compiling the mods
Installing stable releases from Nexus Mods is recommended for most users. If you really want to
compile the mod yourself, read on.
These mods use the crossplatform build config
so they can be built on Linux, Mac, and Windows without changes. See the build config documentation
for troubleshooting.
Compiling a mod for testing
To compile a mod and add it to your game's Mods directory:

Rebuild the project in Visual Studio or MonoDevelop.
This will compile the code and package it into the mod directory.
Launch the project with debugging.
This will start the game through SMAPI and attach the Visual Studio debugger.

Compiling a mod for release
To package a mod for release:

Switch to Release build configuration.
Recompile the mod per the previous section.
Upload the generated bin/Release/<mod name>-<version>.zip file from the project folder.

",140
Lombiq/Helpful-Libraries,C#,"Helpful Libraries Orchard module Readme
Project Description
Libraries that can be handy when developing for Orchard.
Includes:

Contents Libraries with dynamic pages
Authentication Libraries
Dependency Injection Libraries
Key Value Store
Parallel Extensions Extras
Serialization Libraries
Service Validation Libraries
Tasks Libraries and Jobs
Utilities

You can download an install the module from the Orchard Gallery.
Documentation
This module needs at least Orchard 1.8!
Libraries
The module consists of the following independent libraries (all in their own features):

Authentication Libraries
Contents Libraries
Dependency Injection Libraries
Key Value Store
Parallel Extensions Extras is an exception, as it isn't a feature
Serialization Libraries
Service Validation Libraries
Tasks Libraries
Utilities

You can use these libraries as described on the above pages. Don't forget to add a reference to the Piedone.HelpfulLibraries project from your project. Also correctly list the feature of the used libraries as a dependency in your module's Module.txt.
Public APIs are always documented so please always read method comments.
The module is also available for DotNest sites.
See the Version history.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/helpful-libraries (Mercurial repository)
https://github.com/Lombiq/Helpful-Libraries (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",6
jhu-cs-uima-sp19/uniporter_app,Java,"Sprint 2 ------>
User accounts:
username: beary@jhu.edu
password: beary
username:jessy@jhu.edu
Password: password
Username: john@jhu.edu
Password: password
Username: madhu@jhu.edu
Password: madhu
For sprint 2, we added the ability for a user to delete their pending ride. We added a confirmation message before they delete for error prevention on the user end. If a user cancels their pending ride 24 hours before or earlier than their preferred leaving time then the code will reoptimize all the groups so that the other users can still save as much money as possible on their ubers. If the user cancels the ride within the 24 hours before their leaving time then the ride shares are not reoptimized and the group loses one person from their group. We also added a confirmation page that reviews all the information that a user put into the add new ride sequence so that the user can check that all their information is correct before submitting the ride request. We also updated the pending rides page into a sectioned recycler view so that it groups the rides based on whether they are past or upcoming rides. The pending rides are also sorted and presented so that the most recent and upcoming rides are at the top. We also added the feature to enter the date in the scheduled rides page so that the user can easily filter through their rides without having to scroll. We also decided to make it so that a user can only see the share rides they are a part of and not all the existing share ride groups in the database. We also added input restrictions so that users cannot input new rides in past dates and only to future dates. This ensures that the share rides are finalized 24 hours before the time they want to leave. We also added a feature to message the other people within your ride share group so the users can communicate their exact locations and whether they are running a little late. This was built using Firebase. We still need to improve the stylistic design of the chatroom. We also included a notifications feature that we created using Firebase. At the moment we are still sending notifications manually and we still have to integrate notifications within messages. We also decided to add some color to the nav drawer since it was completely white before. We made the top portion have a have a blue and green gradient background but kept the rest of it white to keep the simplicity. We also reduced the amount of things in the nav drawer to log out, scheduled rides, pending rides, and add new ride so that it was simple and the user can’t get lost and stay focused on the main features of the app. We also changed the animations so that it was less abrupt. The card view animation was changed into a smooth fade. The backend algorithm checks to make sure that all the user’s luggage can fit into the uber or uberXL if needed. The main priority was to check that the user did not miss their flight so we made sure the user left early enough. We made sure the user never had to walk further than they wanted to. The user information, rides, and preferences are all stored in the Postgres user database. The optimization algorithm of the ride share groups is done on the Uniporter compute server separate from other processing.

SPRINT 1 ---->
README: As a part of Sprint 1 in terms of back end, we first created a Postgres Database to store the following: User accounts, User past preference inputs for their rides, User past share rides. We integrated Postgres with Django and Django Rest Framework to create API-endpoints. We also set up serializers, view-sets, and migration models for the following, and use the Django ORM to map these nested data points to Postgres RDBMS.:Users, Preferences, Tags, Rides and set up authentication and correct association between them. Our servers were linked to the app backend with API’s (RetrofitClientRides, RetrofitClientSharerides, RetrofitClientUser)
In terms of front end, we created the main landing page, the register page, the log-in page, and pages to add a new ride. We have made it so that once you log-in once, you are always logged into the app. Once you are on the main landing page, you can open click on the hamburger to open the nav drawer. From there we have included three options: “New Ride”, “Pending Rides”, and “Log out”. You can add a new ride by clicking on “New Ride”. This will take you to the first page in a sequence of pages (you can see what part of the step you are on based on the circles at the bottom). There is the flight information page where you can put in your airline (from a spinner with all airlines at BWI) and the flight number and we will make an API call to get the date information and time of the flight. If we can't find it or you would rather prefer, you can enter the flight time manually (this will be done in the second sprint since we were not able to get the right format). The second page ask where you live on campus. This spinner drops down to show a list of all the common residences on campus. We took out the option for them to manually enter an address since it would involve a lot of working converting the address to GPS coordinates we use to calculate the optimal meeting location. The third page takes you to where you can customize how much you are willing to walk to meet with your group. The fourth page is for how much luggage you are bringing. Since we cannot account for every type of large luggage we made a spinner that will enable the user to give us an approximate size of the special item based on a scale we created and explained in a short paragraph. The last page is how early the user is willing to leave (in hours) before their flight. We added an x button in the corner of every page to exit the add a ride process. In the second sprint we want to add a pop-up that tells the user that they will lose the data they have entered up to this point. We have a final confirmation page after which their request is submitted and all the information is sent to the database backend.
Some sample users and passwords for testing purposes:
Username: madhu@jhu.edu, password: madhu
Username: lilian@jhu.edu, password: heehee
Username: tyang28@jhu.edu, password: R35YVR9S!
",2
gnodipac886/MatebookXPro-hackintosh,Shell,"Matebook X Pro Hackintosh
This is the guide to install macOS onto the Huawei Matebook X Pro.
Feel free to help a broke student out at the bottom of the page. :)
English | 中文| Español
DISCLAIMER
The project is still in its beta/testing state.
Proceed at your own risk, I shall not take responsibility for any damages caused.
My Matebook X Pro's Hardware Configuration:

CPU: i7-8550U @ 1.8GHz
16GB RAM
Nvidia GTX MX 150 / Intel UHD 620
3K display @ 3000x2000
512 Gb Toshiba SSD
USB Wifi: Edimax N150
Builtin Bluetooth: Intel Wireless Bluetooth 8265

What works:

Intel UHD 620 Graphics Acceleration
Brightness
Sleep
Realtek alc256 Audio via AppleALC
Keyboard with Volume Controls and Brightness controls (via VoodooPS2)
Camera support up to 10.14.3
Trackpad and Native Gestures via Custom VoodooI2C
Touchscreen with multi-touch capabilities (think of it as a large trackpad)
Battery Percentage
Bluetooth (Reboot from Widows required - should persist after single reboot)
Power Management - I'm getting around 8-9 hours.
Wifi via USB dongle
Liton SSDs are now supported.
HDMI 2.0 support, up to theoretically 4K @60Hz. (Only 4K @30 tested due to equipment limitations)

What doesn't Work:

dGPU (Nvidia Optimus not supported on MacOS)
eGPU (not tested)
Fingerprint Sensor
Intel Wifi (soldered onto the motherbaord)

Let's Get Started
What you need:

Huawei Matebook X Pro (either i7 or i5 model)
macOS or OS X downloaded from the Mac App Store
8GB USB stick
External USB Wifi Dongle
USB C dock (for connecting to external mouse for initial setup)

BIOS Settings

f2 is for booting into BIOS
f12 is for boot override
Any version of the BIOS is good, but I'm on version 1.26
Restore Defaults
Disable Secure boot
Matebook's BIOS is rewrite protected, EFI tool is useless against this BIOS.

Pre-Install:
Prior to installing macOS, it is a good idea to backup any important files on Windows.


You can also leave Windows intact, but it can get tricky. Read here for more information:


This guide for creating USB and installing using Clover UEFI works well for this laptop:


For the installation purposes, please use the HD620 plist that rehabman provides in his guide for your installation USB.


Set config.plist/Graphics/ig-platform-id=0x12345678 for installation.
I ended up wiping windows and installing it afterwards, if you do so, fingerprint sensor will stop working, please follow the guide from this link:
Install macOS according to post 2 of this guide.
Post Installation
You should now be at your desktop.
Download

Clover Configurator Pro
USB Wi-fi Drivers
Newest Clover Bootloader, and install it to your boot disk

Mount EFI partition if not mounted already
Clone the repository via terminal or download it and swap the CLOVER folder downloaded for the one in your EFI directory.
IMPORTANT BrcmFirmwareRepo.kext is in /CLOVER/kexts/Other from this repository - make sure to move it to /Library/Extensions. These kexts will allow bluetooth to persist after a single reboot from Windows.
Note if you have the i5 version, or any other configurations of the laptop sold exclusively in China, you should:

For i5 models: you have to make a custom CPUFriendProvider for Power Management by following this guide:

DSDT fixes
Add the VoodooI2C patches (One for the SKL+ one for Windows 10 Patch)
Add the following code to your DSDT.aml to fix brightness keys.
into method label _Q0A replace_content
begin
// Brightness Down\n
    Notify(\_SB.PCI0.LPCB.PS2K, 0x0405)\n
end;
into method label _Q0B replace_content
begin
// Brightness Up\n
    Notify(\_SB.PCI0.LPCB.PS2K, 0x0406)\n
end;

Reboot
Updates
5/1/2019: Most Important Update Yet

Native brightness is now working
macOS is able to automatically adjust the brightness accroding to the ambient light sensor
Native Sleep is now working, not more glitchy screen after computer comes out from sleep, fixed by injecting custom EDID values
Native graphics: we are now using KBL graphics, we had to change the maximum link rate to HBR in order for the screen to work
Better audio: speakers are now louder, you can always just use voodooHDA but you will lose headphone detection
WhatEverGreen updated to version 1.2.8
VoodooI2C kext updated
Note* you still need to patch your DSDT for trackpad to work, and brightness keys to work.

4/11/2019: New LiteOn Patch

If you have problems updating to 10.14.4 (seeing a prohibited sign), its likely that the problem is caused by your liteon drive, please replace the following patch in your config.plist before you update.

      <dict>
        <key>Comment</key>
        <string>IONVMeFamily: Ignore FLBAS bit:4 being set - for Plextor/LiteOn/Hynix</string>
        <key>Disabled</key>
        <false/>
        <key>Name</key>
        <string>IONVMeFamily</string>
        <key>Find</key>
        <data>SBr2wRAPhQ==</data>
        <key>Replace</key>
        <data>SBr2wQAPhQ==</data>
      </dict>

4/2/2019: Config for Updating/Installing

Added New config-install/update.plist in CLOVER folder for installing purposes. You may choose this config in the boot screen of Clover: options - configs - config-install/update.plist

4/1/2019: 10.14.4 & New Power Management Kexts

New CPUFriend and CPUFriendProvider kexts for better battery life. (~9 hrs)
Run the following code if you would like to make a custom version of the power management kexts to your liking, then install the kexts located at your desktop to Clover. Source

    sh -c ""$(curl -fsSL https://raw.githubusercontent.com/daliansky/XiaoMi-Pro/master/one-key-cpufriend/one-key-cpufriend.sh)""



Undervolt the CPU/GPU/Cache via a shell: Place the new ""voltageshift"" file into your downloads folder and run the ""voltageset.command"" script to undervolt, and the ""voltageinfo.command"" to check your results. Furthermore, you can also set custom values to what you would like to undervolt to based on your hardware (i5 vs i7) by editing the script. Source
WhatEverGreen updated to version 1.2.7
Lilu updated to version 1.3.5
New config.plist in CLOVER comes with 10.14.4 graphics patch in kexttopatch (credit gnodipac886)
New 10.14.4 graphics patch

Comment: CFL patch for MateBook X Pro (10.14.4 credit gnodipac886)
Name: AppleIntelCFLGraphicsFramebuffer
Find: <48ff0557 f607008b 96c02500 008a8e95>
Replace: <b8040000 008986bc 25000031 c05dc395>

    <dict>
        <key>Comment</key>
        <string>CFL patch for MateBook X Pro (10.14.4 credit gnodipac886)</string>
        <key>Find</key>
        <data>SP8FV/YHAIuWwCUAAIqOlQ==</data>
        <key>Name</key>
        <string>AppleIntelCFLGraphicsFramebuffer</string>
        <key>Replace</key>
        <data>uAQAAACJhrwlAAAxwF3DlQ==</data>
        <key>Disabled</key>
        <false/>
    </dict>

2/1/2019 : 10.14.3

New Virtural SMC replacing FakeSMC
Added support for 4K video output with HDMI audio support
Added tools.zip for editing system files such as config.plist or DSDT
Support for firevault2 (In theory, never tested)
Added vanilla 10.14.3 framebuffer graphics kext, if you have replaced the kext before with a custom version, please swap it out in /System/Library/Extension and then use kextutility in tools.zip to rebuild permissions then reboot with 10.14.3 config.plist.
Added ""Configs"" for past config.plists and plists for KBL or SKL graphics (NEED HELP)
Other tweaks to CLOVER folder to support VituralSMC kext.
Updated NoTouchID.kext to newest versions for Mojave support which elimates any lags when promted for user password
Remember to apply brightness key patches to you DSDT.aml so you can play with them for no reason
New config.plist in CLOVER comes with 10.14.3 graphics patch in kexttopatch (credit gnodipac886)
Reports of Thunderbolt eGPU was able to work when booted with eGPU plugged in, no hotplug support yet
Support for Liteon SSDs confirmed with new config.plist in CLOVER and in Configs folder

1/23/2019 : 10.14.3 Update Graphics

New 10.14.3 graphics patch

CFL patch for MateBook X Pro (10.14.3 credit gnodipac886)
Name: AppleIntelCFLGraphicsFramebuffer
Find: <48ff0589 4d07008b 96c02500 008a8e95>
Replace: <b8040000 008986bc 25000031 c05dc395>

			<dict>
				<key>Comment</key>
				<string>CFL patch for MateBook X Pro (10.14.3 credit gnodipac886)</string>
				<key>Find</key>
				<data>SP8FiU0HAIuWwCUAAIqOlQ==</data>
				<key>Name</key>
				<string>AppleIntelCFLGraphicsFramebuffer</string>
				<key>Replace</key>
				<data>uAQAAACJhrwlAAAxwF3DlQ==</data>
				<key>Disabled</key>
				<false/>
			</dict>

1/21/2019

New Whatevergreen replaced old custom version
Lilu updated
New Applealc to support native audio codec
Custom version of I2C trackpad kexts for better support
Added KBL and SKL config.plists for people who are interested to help out. Main issue: Blackscreen/ internal screen not recognized
config.plist minor fixes

Credits:

Darren_Pan on reddit
midi and Maemo on discord
Chinese Matebook X Pro Hackintosh community
Spanish Matebook X Pro Hackintosh community
All the developers who developed the kexts used in this guide.

Help a broke student out:

PayPal
Venmo

QR Codes:



PayPal
Venmo.
WeChat
支付宝











Good Luck!
",74
ozikot/AtCoder,C++,"AtCoder
Strage box of AtCoder code
各ファイル内のREADME.mdに解法とコードを載せ始めました(2019/1/14~)
備忘録
next_permutation()はsort済みのvector or 配列を使用
カテゴリー別
https://ozikot.github.io/cp-categorize/
",3
belowthetree/MiniProgram,JavaScript,"MiniProgram
移动开发（微信小程序项目）
",3
bsiddiqui/hapi-router,JavaScript,"hapi-router
    
Route loader for hapi.
Hapi v17
hapi-router requires Hapi v18.

Hapi 18, hapi-router@5
Hapi 17, hapi-router@4
Hapi <= 16, hapi-router@3

Install
// If you're using Hapi v18
$ npm i -S hapi-router@5

// If you're using Hapi v17
$ npm i -S hapi-router@4

// If you're using < Hapi v17
$ npm i -S hapi-router@3.5.0
Usage
try {
  await server.register({
    plugin: require('hapi-router'),
    options: {
      routes: 'src/**/*Route.js' // uses glob to include files
    }
  })
} catch (err) {
  // Handle err
  throw err
}
Options
routes
Required 
Type: string / array
The glob pattern you would like to include
ignore
Type: string / array
The pattern or an array of patterns to exclude
cwd
Type: string
The current working directory in which to search (defaults to process.cwd())
Specifying Routes
Any files that match your routes glob will be loaded
Example route file:
module.exports = [
  {
    path: '/test1',
    method: 'GET',
    handler: function (request, reply) {
      reply('hello');
    }
  },
  {
    path: '/test2',
    method: 'GET',
    handler: function (request, reply) {
      reply('hello');
    }
  }
]
Glob Primer
Example globs:
'routes/*.js'    // match all js files in the routes directory
'routes/**/*.js' // recursively match all js files in the routes directory
'**/*Route.js'   // match all js files that end with 'Route'
From isaacs:
""Globs"" are the patterns you type when you do stuff like ls *.js on
the command line, or put build/* in a .gitignore file.
The following characters have special magic meaning when used in a
path portion:

* Matches 0 or more characters in a single path portion
? Matches 1 character
[...] Matches a range of characters, similar to a RegExp range.
If the first character of the range is ! or ^ then it matches
any character not in the range.
!(pattern|pattern|pattern) Matches anything that does not match
any of the patterns provided.
?(pattern|pattern|pattern) Matches zero or one occurrence of the
patterns provided.
+(pattern|pattern|pattern) Matches one or more occurrences of the
patterns provided.
*(a|b|c) Matches zero or more occurrences of the patterns provided
@(pattern|pat*|pat?erN) Matches exactly one of the patterns
provided
** If a ""globstar"" is alone in a path portion, then it matches
zero or more directories and subdirectories searching for matches.
It does not crawl symlinked directories.

",70
Lombiq/Git-Hg-Mirror-Common,JavaScript,"Git-hg Mirror Common readme
Orchard CMS module serving as the frontend of the two-way Git-Mercurial repository syncing service Git-hg Mirror.  The service component is Git-Hg Mirror Daemon.
This is a C# project that you'll need Visual Studio to work with. Commits in the master/default branch represent deployments, i.e. the latest commit in that branch shows the version currently running in production.
The project's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror itself:

https://bitbucket.org/Lombiq/git-hg-mirror-daemon-common (Mercurial repository)
https://github.com/Lombiq/Git-hg-Mirror-Common (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub. Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
Developer overview
To work with the module locally you'll need to put it in an Orchard solution among the modules in a folder named exactly ""GitHgMirror.Common"". You'll need the same Orchard version as it's stated in the Module.txt file.
After enabling the module you'll see the same UI to create mirroring configurations than on githgmirror.com (note that it won't exactly look the same since the theme component of the website is not open source, since it's of little use to anybody else).
",2
HL7/PDDI-CDS,HTML,"PDDI-CDS
Documents and code related to the CDS and Pharmacy WG sponsored implementation guide for potential drug-drug interaction clinical decision support.
CI Build
Commits to this repository will automatically trigger a build which is pushed to the following location:
http://build.fhir.org/ig/HL7/PDDI-CDS/index.html
Project Wiki Page
http://wiki.hl7.org/index.php?title=PDDI_CDS
Who do I talk to?

Project Facilitators

Richard Boyce boycerd@upmc.edu
Guilherme Del Fiol guilherme.delfiol@utah.edu
Local Build
java -jar ""org.hl7.fhir.igpublisher.jar"" -ig ig.json

ig publisher GUI
Please read more about the implementation guide publishing process here: http://wiki.hl7.org/index.php?title=IG_Publisher_Documentation
Running the publisher""
    java -jar org.hl7.fhir.igpublisher.jar

Then, execute ig.json from the GUI. This will render webpages with html files in output folder (e.g., index.html).
",2
clockworkpi/CPI,None,"CPI
version control info.
",3
SourMesen/Mesen-S,C++,"Mesen-S
Mesen-S is a cross-platform SNES emulator for Windows & Linux built in C++ and C#.
If you want to support this project, please consider making a donation:

Development Builds
Development builds of the latest commit are available from Appveyor. For release builds, see the Releases tab on GitHub.
Warning: These are development builds and may be unstable. Using them may also increase the chances of your settings being corrupted, or having issues when upgrading to the next official release. Additionally, these builds are currently not optimized via PGO and will typically run 20-30% slower than the official release builds.
Windows: 
Linux: 
Roadmap
Mesen-S is very early in its development and some features are still missing.
The following should be added over time (in no particular order):

Movies
Netplay
Cheats
Additions/improvements in the debugging tools
Lua scripting
Support for the enhancement chips used in some games
Libretro core (once the emulation core is stable/accurate enough)

Compiling
See COMPILING.md
License
Mesen is available under the GPL V3 license.  Full text here: http://www.gnu.org/licenses/gpl-3.0.en.html
Copyright (C) 2019 M. Bibaud
This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.
This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.
You should have received a copy of the GNU General Public License
along with this program.  If not, see http://www.gnu.org/licenses/.
",81
xieyuheng/cicada,TypeScript,"Cicada
Aims

Libraries and tools for topological and geometric modeling

Community

We enforce C4 as collaboration protocol -- The C4 RFC
Style Guide -- observe the style of existing code and respect it
Code of Conduct
Source code -- github, gitlab
cicada-rs -- an old version of the same project written in rust
IRC -- #cicada-language
CI -- gitlab-ci

Docs

A Recursive Combinatorial Description of cell-complex

A paper about the definition of cell-complex in this project



Modules


npm install cicada-lang


API Docs


Try examples


Contents:

int
num
euclid
combinatorial-game
cell-complex
homology
cicada-core
cicadascript



int int
examples/int-module.js:
let assert = require (""assert"")

let ut = require (""cicada-lang/lib/util"")
let int = require (""cicada-lang/lib/int"")

{
  /**
   * generic `row_canonical_form`
   *   i.e. `hermite_normal_form` for integers
   */

  let A = int.matrix ([
    [2, 3, 6, 2],
    [5, 6, 1, 6],
    [8, 3, 1, 1],
  ])

  let B = int.matrix ([
    [1, 0, -11, 2],
    [0, 3, 28, -2],
    [0, 0, 61, -13],
  ])

  assert (
    A.row_canonical_form () .eq (B)
  )
}

{
  /**
   * generic `diag_canonical_form`
   *   i.e. `smith_normal_form` for integers
   */

  let A = int.matrix ([
    [2, 4, 4],
    [-6, 6, 12],
    [10, -4, -16],
  ])

  let S = int.matrix ([
    [2, 0, 0],
    [0, 6, 0],
    [0, 0, -12],
  ])

  assert (
    A.diag_canonical_form () .eq (S)
  )
}

{
  /**
   * solve linear diophantine equations
   */

  let A = int.matrix ([
    [1, 2, 3, 4, 5, 6, 7],
    [1, 0, 1, 0, 1, 0, 1],
    [2, 4, 5, 6, 1, 1, 1],
    [1, 4, 2, 5, 2, 0, 0],
    [0, 0, 1, 1, 2, 2, 3],
  ])

  let b = int.vector ([
    28,
    4,
    20,
    14,
    9,
  ])

  let solution = A.solve (b)

  if (solution !== null) {
    solution.print ()

    assert (
      A.act (solution) .eq (b)
    )
  }
}
num num

with config-able epsilon for numerical stability

examples/num-linear-algebra.js:
let assert = require (""assert"")

let ut = require (""cicada-lang/lib/util"")
let num = require (""cicada-lang/lib/num"")

{
  /**
   * `reduced_row_echelon_form` is like `row_canonical_form`
   *   it reduces pivots to one
   *   while respecting `epsilon` for numerical stability
   */

  let A = num.matrix ([
    [1, 3, 1, 9],
    [1, 1, -1, 1],
    [3, 11, 5, 35],
  ])

  let B = num.matrix ([
    [1, 0, -2, -3],
    [0, 1, 1, 4],
    [0, 0, 0, 0],
  ])

  A.reduced_row_echelon_form () .print ()
  A.row_canonical_form () .print ()

  assert (
    A.reduced_row_echelon_form () .eq (B)
  )
}
eu euclid

module theory over euclidean ring

for generic matrix algorithms



cg combinatorial-game

a game engine for n-player perfect information games
example games:

tic-tac-toe
hackenbush -- demo



cx cell-complex

cell-complex based low dimensional algebraic topology library
docs/a-recursive-combinatorial-description-of-cell-complex.md

gh graph

[TODO]
graph theory -- one dimensional cell-complex

hl homology

cellular homology of cell-complex

examples/four-ways-to-glue-a-square.js:

let cx = require (""cicada-lang/lib/cell-complex"")
let hl = require (""cicada-lang/lib/homology"")
let ut = require (""cicada-lang/lib/util"")

class sphere_t extends cx.cell_complex_t {
  constructor () {
    let builder = new cx.cell_complex_builder_t ()
    let [south, middle, north] = builder.attach_points (3)
    let south_long = builder.attach_edge (south, middle)
    let north_long = builder.attach_edge (middle, north)
    let surf = builder.attach_face ([
      south_long,
      north_long,
      north_long.rev (),
      south_long.rev (),
    ])
    super (builder)
  }
}

class torus_t extends cx.cell_complex_t {
  constructor () {
    let builder = new cx.cell_complex_builder_t ()
    let origin = builder.attach_point ()
    let toro = builder.attach_edge (origin, origin)
    let polo = builder.attach_edge (origin, origin)
    let surf = builder.attach_face ([
      toro,
      polo,
      toro.rev (),
      polo.rev (),
    ])
    super (builder)
  }
}

class klein_bottle_t extends cx.cell_complex_t {
  constructor () {
    let builder = new cx.cell_complex_builder_t ()
    let origin = builder.attach_point ()
    let toro = builder.attach_edge (origin, origin)
    let cross = builder.attach_edge (origin, origin)
    let surf = builder.attach_face ([
      toro,
      cross,
      toro.rev (),
      cross,
    ])
    super (builder)
  }
}

class projective_plane_t extends cx.cell_complex_t {
  constructor () {
    let builder = new cx.cell_complex_builder_t ()
    let [start, end] = builder.attach_points (2)
    let left_rim = builder.attach_edge (start, end)
    let right_rim = builder.attach_edge (end, start)
    let surf = builder.attach_face ([
      left_rim, right_rim,
      left_rim, right_rim,
    ])
    super (builder)
  }
}

calculate homology groups:

let report = {
  ""sphere"": hl.report (new sphere_t ()),
  ""torus"": hl.report (new torus_t ()),
  ""klein_bottle"": hl.report (new klein_bottle_t ()),
  ""projective_plane"": hl.report (new projective_plane_t ()),
}

ut.log (report)

let expected_report = {
  sphere:
   { '0': { betti_number: 1, torsion_coefficients: [] },
     '1': { betti_number: 0, torsion_coefficients: [] },
     '2': { betti_number: 1, torsion_coefficients: [] },
     euler_characteristic: 2 },
  torus:
   { '0': { betti_number: 1, torsion_coefficients: [] },
     '1': { betti_number: 2, torsion_coefficients: [] },
     '2': { betti_number: 1, torsion_coefficients: [] },
     euler_characteristic: 0 },
  klein_bottle:
   { '0': { betti_number: 1, torsion_coefficients: [] },
     '1': { betti_number: 1, torsion_coefficients: [ 2 ] },
     '2': { betti_number: 0, torsion_coefficients: [] },
     euler_characteristic: 0 },
  projective_plane:
   { '0': { betti_number: 1, torsion_coefficients: [] },
     '1': { betti_number: 0, torsion_coefficients: [ 2 ] },
     '2': { betti_number: 0, torsion_coefficients: [] },
     euler_characteristic: 1 }
}

Pictures by Guy Inchbald, a.k.a. Steelpillow

cc cicada-core

[TODO]
a dependently-typed programming language
game semantics
logic programming interface

cs cicadascript

[TODO]
js syntax frontend of cicada-core

License

GPLv3

",10
clockworkpi/CPI,None,"CPI
version control info.
",3
SourMesen/Mesen-S,C++,"Mesen-S
Mesen-S is a cross-platform SNES emulator for Windows & Linux built in C++ and C#.
If you want to support this project, please consider making a donation:

Development Builds
Development builds of the latest commit are available from Appveyor. For release builds, see the Releases tab on GitHub.
Warning: These are development builds and may be unstable. Using them may also increase the chances of your settings being corrupted, or having issues when upgrading to the next official release. Additionally, these builds are currently not optimized via PGO and will typically run 20-30% slower than the official release builds.
Windows: 
Linux: 
Roadmap
Mesen-S is very early in its development and some features are still missing.
The following should be added over time (in no particular order):

Movies
Netplay
Cheats
Additions/improvements in the debugging tools
Lua scripting
Support for the enhancement chips used in some games
Libretro core (once the emulation core is stable/accurate enough)

Compiling
See COMPILING.md
License
Mesen is available under the GPL V3 license.  Full text here: http://www.gnu.org/licenses/gpl-3.0.en.html
Copyright (C) 2019 M. Bibaud
This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.
This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.
You should have received a copy of the GNU General Public License
along with this program.  If not, see http://www.gnu.org/licenses/.
",81
xieyuheng/cicada,TypeScript,"Cicada
Aims

Libraries and tools for topological and geometric modeling

Community

We enforce C4 as collaboration protocol -- The C4 RFC
Style Guide -- observe the style of existing code and respect it
Code of Conduct
Source code -- github, gitlab
cicada-rs -- an old version of the same project written in rust
IRC -- #cicada-language
CI -- gitlab-ci

Docs

A Recursive Combinatorial Description of cell-complex

A paper about the definition of cell-complex in this project



Modules


npm install cicada-lang


API Docs


Try examples


Contents:

int
num
euclid
combinatorial-game
cell-complex
homology
cicada-core
cicadascript



int int
examples/int-module.js:
let assert = require (""assert"")

let ut = require (""cicada-lang/lib/util"")
let int = require (""cicada-lang/lib/int"")

{
  /**
   * generic `row_canonical_form`
   *   i.e. `hermite_normal_form` for integers
   */

  let A = int.matrix ([
    [2, 3, 6, 2],
    [5, 6, 1, 6],
    [8, 3, 1, 1],
  ])

  let B = int.matrix ([
    [1, 0, -11, 2],
    [0, 3, 28, -2],
    [0, 0, 61, -13],
  ])

  assert (
    A.row_canonical_form () .eq (B)
  )
}

{
  /**
   * generic `diag_canonical_form`
   *   i.e. `smith_normal_form` for integers
   */

  let A = int.matrix ([
    [2, 4, 4],
    [-6, 6, 12],
    [10, -4, -16],
  ])

  let S = int.matrix ([
    [2, 0, 0],
    [0, 6, 0],
    [0, 0, -12],
  ])

  assert (
    A.diag_canonical_form () .eq (S)
  )
}

{
  /**
   * solve linear diophantine equations
   */

  let A = int.matrix ([
    [1, 2, 3, 4, 5, 6, 7],
    [1, 0, 1, 0, 1, 0, 1],
    [2, 4, 5, 6, 1, 1, 1],
    [1, 4, 2, 5, 2, 0, 0],
    [0, 0, 1, 1, 2, 2, 3],
  ])

  let b = int.vector ([
    28,
    4,
    20,
    14,
    9,
  ])

  let solution = A.solve (b)

  if (solution !== null) {
    solution.print ()

    assert (
      A.act (solution) .eq (b)
    )
  }
}
num num

with config-able epsilon for numerical stability

examples/num-linear-algebra.js:
let assert = require (""assert"")

let ut = require (""cicada-lang/lib/util"")
let num = require (""cicada-lang/lib/num"")

{
  /**
   * `reduced_row_echelon_form` is like `row_canonical_form`
   *   it reduces pivots to one
   *   while respecting `epsilon` for numerical stability
   */

  let A = num.matrix ([
    [1, 3, 1, 9],
    [1, 1, -1, 1],
    [3, 11, 5, 35],
  ])

  let B = num.matrix ([
    [1, 0, -2, -3],
    [0, 1, 1, 4],
    [0, 0, 0, 0],
  ])

  A.reduced_row_echelon_form () .print ()
  A.row_canonical_form () .print ()

  assert (
    A.reduced_row_echelon_form () .eq (B)
  )
}
eu euclid

module theory over euclidean ring

for generic matrix algorithms



cg combinatorial-game

a game engine for n-player perfect information games
example games:

tic-tac-toe
hackenbush -- demo



cx cell-complex

cell-complex based low dimensional algebraic topology library
docs/a-recursive-combinatorial-description-of-cell-complex.md

gh graph

[TODO]
graph theory -- one dimensional cell-complex

hl homology

cellular homology of cell-complex

examples/four-ways-to-glue-a-square.js:

let cx = require (""cicada-lang/lib/cell-complex"")
let hl = require (""cicada-lang/lib/homology"")
let ut = require (""cicada-lang/lib/util"")

class sphere_t extends cx.cell_complex_t {
  constructor () {
    let builder = new cx.cell_complex_builder_t ()
    let [south, middle, north] = builder.attach_points (3)
    let south_long = builder.attach_edge (south, middle)
    let north_long = builder.attach_edge (middle, north)
    let surf = builder.attach_face ([
      south_long,
      north_long,
      north_long.rev (),
      south_long.rev (),
    ])
    super (builder)
  }
}

class torus_t extends cx.cell_complex_t {
  constructor () {
    let builder = new cx.cell_complex_builder_t ()
    let origin = builder.attach_point ()
    let toro = builder.attach_edge (origin, origin)
    let polo = builder.attach_edge (origin, origin)
    let surf = builder.attach_face ([
      toro,
      polo,
      toro.rev (),
      polo.rev (),
    ])
    super (builder)
  }
}

class klein_bottle_t extends cx.cell_complex_t {
  constructor () {
    let builder = new cx.cell_complex_builder_t ()
    let origin = builder.attach_point ()
    let toro = builder.attach_edge (origin, origin)
    let cross = builder.attach_edge (origin, origin)
    let surf = builder.attach_face ([
      toro,
      cross,
      toro.rev (),
      cross,
    ])
    super (builder)
  }
}

class projective_plane_t extends cx.cell_complex_t {
  constructor () {
    let builder = new cx.cell_complex_builder_t ()
    let [start, end] = builder.attach_points (2)
    let left_rim = builder.attach_edge (start, end)
    let right_rim = builder.attach_edge (end, start)
    let surf = builder.attach_face ([
      left_rim, right_rim,
      left_rim, right_rim,
    ])
    super (builder)
  }
}

calculate homology groups:

let report = {
  ""sphere"": hl.report (new sphere_t ()),
  ""torus"": hl.report (new torus_t ()),
  ""klein_bottle"": hl.report (new klein_bottle_t ()),
  ""projective_plane"": hl.report (new projective_plane_t ()),
}

ut.log (report)

let expected_report = {
  sphere:
   { '0': { betti_number: 1, torsion_coefficients: [] },
     '1': { betti_number: 0, torsion_coefficients: [] },
     '2': { betti_number: 1, torsion_coefficients: [] },
     euler_characteristic: 2 },
  torus:
   { '0': { betti_number: 1, torsion_coefficients: [] },
     '1': { betti_number: 2, torsion_coefficients: [] },
     '2': { betti_number: 1, torsion_coefficients: [] },
     euler_characteristic: 0 },
  klein_bottle:
   { '0': { betti_number: 1, torsion_coefficients: [] },
     '1': { betti_number: 1, torsion_coefficients: [ 2 ] },
     '2': { betti_number: 0, torsion_coefficients: [] },
     euler_characteristic: 0 },
  projective_plane:
   { '0': { betti_number: 1, torsion_coefficients: [] },
     '1': { betti_number: 0, torsion_coefficients: [ 2 ] },
     '2': { betti_number: 0, torsion_coefficients: [] },
     euler_characteristic: 1 }
}

Pictures by Guy Inchbald, a.k.a. Steelpillow

cc cicada-core

[TODO]
a dependently-typed programming language
game semantics
logic programming interface

cs cicadascript

[TODO]
js syntax frontend of cicada-core

License

GPLv3

",10
runningcheese/RunningCheese-Firefox,JavaScript,"RunningCheese Firefox V10
RunningCheese Firefox 是一款旨在提高Firefox易用性的浏览器，界面优美功能强大，操作简单容易上手是它的特色，为你在工作学习上提供极大的便利。V系列Firefox 将坚定地朝这个方向前进，力求简洁易用，让更多的人加入 Firefox 阵营。

开发前后经历4年的 RunningCheese Firefox V10 正式版发布了！全新的 Firefox Quantum 架构让 Firefox 焕发了第二春，有史以来最棒的 Firefox，高速流畅，一别卡顿。在速度比肩 Chrome 浏览器的同时，还保留了 Firefox 的强大功能，正式版V10已经可以完全替代传统架构的V9，如果你追求的是简洁高效，那么这款 Firefox 浏览器一定适合你！
更新内容：

基于全新 Firefox Quantum 架构 （Firefox 64），速度是真的快~
Firefox Quantum 是史上最棒的 Firefox，V10也是 V系列 Firefox 中最好用的。
完善了主题界面，优化各种细节，修改已知的问题，完全可以做为主力浏览器使用。
配置一键自动更新，省去了不停折腾的烦恼，出现问题需要修复也可以一键修复。

下载地址：
高速下载：https://firefox.runningcheese.com 
百度网盘：https://pan.baidu.com/s/1nvGrYbR 
腾讯网盘：https://share.weiyun.com/5pjDbnL 
谷歌网盘：https://drive.google.com/drive/folders/19DUhiuNxoPciVSIZwkQjPz_tbuHT1dPO
问题反馈：https://www.runningcheese.com/v10
使用手册：https://www.runningcheese.com/firefox-usage
开发手册：https://www.runningcheese.com/firefox-development
如果觉得好用，可以按上方的 ★Star 帮助更多的朋友发现这个项目。
",313
socketry/protocol-websocket,Ruby,"Protocol::WebSocket
Provides a low-level implementation of the WebSocket protocol according to RFC6455. It only implements the latest stable version (13).

Installation
Add this line to your application's Gemfile:
gem 'protocol-websocket'
And then execute:
$ bundle

Or install it yourself as:
$ gem install protocol-websocket

Usage
Here is a basic WebSocket client:
stream = # connect to remote system
framer = Protocol::WebSocket::Framer.new(stream)

frame = framer.read_frame
Contributing

Fork it
Create your feature branch (git checkout -b my-new-feature)
Commit your changes (git commit -am 'Add some feature')
Push to the branch (git push origin my-new-feature)
Create new Pull Request

License
Released under the MIT license.
Copyright, 2019, by Samuel G. D. Williams.
Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
",2
ppriyank/Bert-Coref-Resolution-Lee-,Python,"Bert-Coref-Resolution-Lee-
A replicate of Official github of End-to-end Neural Coreference Resolution
(https://arxiv.org/pdf/1707.07045.pdf)
Use  this for setting  up the requrements and preparing  glove vectors/Elmo(https://github.com/kentonl/e2e-coref)
For setting up the prince cluster to  support running the scripts : follow : https://github.com/ppriyank/Prince-Set-UP
bert_end_2_end.py & train-bert_end2end.py
Replaced bert model to generate embedings at run time to replace glove vectors and  elmo vectors in original paper
Since Bert works in sequences, and original code is written using sentences as chunks, the sequence is converted into run time  splits of sesntences.  For detailed explanation go  to line  #323.
For easier explanation of tensorflow code go to  https://stackoverflow.com/questions/34970582/using-a-variable-for-num-splits-for-tf-split/56015552#56015552 (my own answer)
",2
lydzhng/lydzhng.github.io,HTML,"lydzhng.github.io
my website
",3
cms-sw/cms-sw.github.io,JavaScript,"CMSSW work pages
They include:

Actual documentation.
Various scripts to import log files sparse in /afs to the git repository.

Importing log files to the repository.
A reasonable amount of processed log files, usually in json format, can be
stored in this git repository and not cause scalability issues, since git
is extremely good at compressing similar files.
This allows us to serve integration builds results via Github
Pages
In order to populate the data directory:
git clone cms-sw.github.com
cd cms-sw.github.com
./process-logs --logdir <path-to-your-toplevel-log-directory>
make -j 20
git commit data -m'Results updated'
git push origin master

Contributing to repository.
This repository contains two branches - master and code. All user submitted changes should go to code branch which will then be merged into master branch. Auto-generated data such as JSON files submitted by Cms Bot should go directly in to master. This should solve PR issues like this.
",6
ppriyank/Bert-Coref-Resolution-Lee-,Python,"Bert-Coref-Resolution-Lee-
A replicate of Official github of End-to-end Neural Coreference Resolution
(https://arxiv.org/pdf/1707.07045.pdf)
Use  this for setting  up the requrements and preparing  glove vectors/Elmo(https://github.com/kentonl/e2e-coref)
For setting up the prince cluster to  support running the scripts : follow : https://github.com/ppriyank/Prince-Set-UP
bert_end_2_end.py & train-bert_end2end.py
Replaced bert model to generate embedings at run time to replace glove vectors and  elmo vectors in original paper
Since Bert works in sequences, and original code is written using sentences as chunks, the sequence is converted into run time  splits of sesntences.  For detailed explanation go  to line  #323.
For easier explanation of tensorflow code go to  https://stackoverflow.com/questions/34970582/using-a-variable-for-num-splits-for-tf-split/56015552#56015552 (my own answer)
",2
lydzhng/lydzhng.github.io,HTML,"lydzhng.github.io
my website
",3
cms-sw/cms-sw.github.io,JavaScript,"CMSSW work pages
They include:

Actual documentation.
Various scripts to import log files sparse in /afs to the git repository.

Importing log files to the repository.
A reasonable amount of processed log files, usually in json format, can be
stored in this git repository and not cause scalability issues, since git
is extremely good at compressing similar files.
This allows us to serve integration builds results via Github
Pages
In order to populate the data directory:
git clone cms-sw.github.com
cd cms-sw.github.com
./process-logs --logdir <path-to-your-toplevel-log-directory>
make -j 20
git commit data -m'Results updated'
git push origin master

Contributing to repository.
This repository contains two branches - master and code. All user submitted changes should go to code branch which will then be merged into master branch. Auto-generated data such as JSON files submitted by Cms Bot should go directly in to master. This should solve PR issues like this.
",6
keywish/keywish-panther-tank,C++,"Please Contact Us
Technical support email: abbott@emakefun.com 
Sales email: ken@keywish-robot.com
The latest information download address: https://github.com/keywish/keywish-panther-tank
The branch switch method

Panther-tank

Product Introduce
""Tank"" is ATMEGA328P-PU as the main control chip, and TB6612FGN is used as a multi-functional
crawler car for motor drive chip. Compared with the traditional car, ""Tank"" is also equipped with wireless
control (Bluetooth, infrared remote control). It can automatically avoid obstacles. Of course, Maker can also
add or subtract other functions through its own Idea, such as adding automatic tracking, PS2 gamepad,
adding wifi control, robotic arm, etc.
""Tank"" is equipped with all kinds of materials, technical manuals, routines, etc., and teaches you from
entry to proficiency. Every electronic enthusiast can easily get started and realize the functions they want.
Feature
*	High power all metal geared motor
*	Integral stamping molding kit,easier Installation,tighter
*	2400mAH,7.4v ,rechargeable li-battery,longer battery life,and more dynamic
*	2 RGB turn lights
*	Buzzer Turn around reminder
*	Infrared remote control
*	Android App control
Required Best Buy Links
Buy on Amazon 
Buy on Aliexpress
Video Links
Component Introduce
Assembly
Function
Download method

",2
Lombiq/Tidy-Orchard-Development-Toolkit,C#,"Tidy Orchard Development Toolkit Readme
The Tidy Orchard Development Toolkit allows you to develop Orchard-based applications in a way that you own code (e.g. extensions, configuration) is completely separated from the core Orchard source.
This makes Orchard development not only tidier but it also allows you to:

Manage your extensions better: e.g. now you can keep all your modules under a single repository (with subrepositories for other modules) instead of having all your modules in separate repositories.
Updating or upgrading the Orchard source is a matter of pulling in the latest changes from the Orchard repository.
You can even keep a single (or just a few) folders on your computer with the Orchard source that you link to from each of your solutions, thus minimizing storage space usage and build time.

Keep in mind that this toolkit is purely experimental! It can in no way support a production scenario. Also the aim was to get Orchard working in its basics: it fully runs. Other areas like deplyoment wasn't explored yet.
Creating a Tidy Orchard solution
There is a sample solution with all of the below tasks already done: see the Tidy Orchard Development Quick Start. This solution has all the details just referenced here.

Create a folder in the root for your web project (e.g. “Orchard.Web” but the name is not mandatory) and copy the contents of Orchard.Web there (the Web csproj can also have an arbitrary name). Modify the Web.config as in the sample.
Add the Toolkit to the Lombiq.TidyOrchardDevelopmentToolkit under your web project's folder.
Add the full Orchard source to the Web project's folder under a folder called ""Orchard"". This should be the full Orchard source (e.g. with the lib and src folders in the root). Please note that you have to remove the Web.config from Orchard.Web.
Copy the Orchard solution file to the root (and optionally rename it).
Change all project references of the solution to point to the new web project's content (assuming your web project's folder is called Orchard.Web):

Replace ""Orchard\ with ""Orchard.Web\Orchard\src\Orchard\ (including the quotes).
Replace Orchard.Tests\ with Orchard.Web\Orchard\src\Orchard.Tests.
Replace Orchard.Web.Tests\ with Orchard.Web\Orchard\src\Orchard.Web.Tests.
Replace ""Orchard.Web\ with ""Orchard.Web\Orchard\src\Orchard.Web\ (including the quotes).
Replace Orchard.Tests.Modules\ with Orchard.Web\Orchard\src\Orchard.Tests.Modules.
Replace Orchard.Core.Tests\ with Orchard.Web\Orchard\src\Orchard.Core.Tests.
Replace Orchard.WarmupStarter\ with Orchard.Web\Orchard\src\Orchard.WarmupStarter.
Replace ""Tools\ with ""Orchard.Web\Orchard\src\Tools\ (including the quotes).
Replace Orchard.Specs\ with Orchard.Web\Orchard\src\Orchard.Specs.
Replace Orchard.Profile\ with Orchard.Web\Orchard\src\Orchard.Profile.
Replace Orchard.Web\Orchard\src\Orchard.Web\Orchard.Web.csproj back to Orchard.Web\Orchard.Web.csproj.


Copy over the contents of the original Orchard.Web folder to your own web folder except the Core, Modules, Media and Themes folders.
Adjust Orchard.Web.csproj:

Replace ....\lib\ with Orchard\lib.
Replace ProjectReference Include=""..\ with ProjectReference Include=""Orchard\src.
Replace ProjectReference Include=""Core\ with ProjectReference Include=""Orchard\src\Orchard.Web\Core.


Add the Toolkit's project to the solution and reference it from the web project.
Register the Toolkit's Autofac module in the HostComponents.config file.
Register the TidyDevelopmentHttpModule in the Web.config and change the handlers declaration to use the appropriate accessPolicy.
Add your own themes and modules under the Web project's folder under ""Modules"" and ""Themes"" folders, respectively.
Modify module project files according to the Orchard App Host documentation so they support the new solution structure.

Instead of copying you can always create symlinks with mklink instead.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/tidy-orchard-development-toolkit (Mercurial repository)
https://github.com/Lombiq/Tidy-Orchard-Development-Toolkit (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",3
digital-flowers/react-animated-css,JavaScript,"react-animated-css
React component to show or hide elements with animations using Animated.css


demo
https://digital-flowers.github.io/react-animated-css.html
install
npm i react-animated-css --save
Note You have to include Animated.css in your html page, this component is just a wrapper for it.
<head>
  <link rel=""stylesheet"" href=""https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.5.2/animate.min.css"">
</head>
how to use
very easy to use, just wrap your content with the animated component
import {Animated} from ""react-animated-css"";

<Animated animationIn=""bounceInLeft"" animationOut=""fadeOut"" isVisible={true}>
    <div>
        hello world ;)
    </div>
</Animated>

then you can just toggle the  isVisible property to see the animation.
Properties

animationIn animation in name, default ""fadeIn""
animationOut animation out name, default ""fadeOut""
animationInDelay animation in delay, default 0
animationOutDelay animation out delay, default 0
animationInDuration animation in delay, default 1000
animationOutDuration animation out delay, default 1000
style react style property for the inner component
isVisible if the component is visible or not, default true
innerRef react ref property for the inner component
className react className property for the inner component
animateOnMount apply animationIn on mount or not, default true

List of animation
All the following animation from animated.css are supported.



﻿Animation Name




bounce


flash


pulse


rubberBand


shake


headShake


swing


tada


wobble


jello


bounceIn


bounceInDown


bounceInLeft


bounceInRight


bounceInUp


bounceOut


bounceOutDown


bounceOutLeft


bounceOutRight


bounceOutUp


fadeIn


fadeInDown


fadeInDownBig


fadeInLeft


fadeInLeftBig


fadeInRight


fadeInRightBig


fadeInUp


fadeInUpBig


fadeOut


fadeOutDown


fadeOutDownBig


fadeOutLeft


fadeOutLeftBig


fadeOutRight


fadeOutRightBig


fadeOutUp


fadeOutUpBig


flipInX


flipInY


flipOutX


flipOutY


lightSpeedIn


lightSpeedOut


rotateIn


rotateInDownLeft


rotateInDownRight


rotateInUpLeft


rotateInUpRight


rotateOut


rotateOutDownLeft


rotateOutDownRight


rotateOutUpLeft


rotateOutUpRight


hinge


jackInTheBox


rollIn


rollOut


zoomIn


zoomInDown


zoomInLeft


zoomInRight


zoomInUp


zoomOut


zoomOutDown


zoomOutLeft


zoomOutRight


zoomOutUp


slideInDown


slideInLeft


slideInRight


slideInUp


slideOutDown


slideOutLeft


slideOutRight


slideOutUp



note:
From React 17.x.x componentWillReceiveProps will be deprecated and a different strategy is introduced.
",64
rudty/nodekell,TypeScript,"nodekell
Functional library for nodejs






almost all functions support currying
supports parallel functions
supports async generator
supports lazy evaluation
typescript support (ver:3.4, target:es2018)

Installation
npm install nodekell
Import Module
const F = require(""nodekell"");
Quick Example
const v = await F.run(
    F.range(Infinity),//[0,1,2...]
    F.filter(e => e % 2 == 0), //[0,2,4...] 
    F.map(e => e + 1), //[1,3,5...]
    F.take(5), // [1,3,5,7,9]
    F.reduce((acc, e) => acc + e)); // 1+3+5+7+9
console.log(v);//25
const v = await F.run(
    F.repeat(2), //[2,2,2,..]
    F.map(e => e + 1), //[3,3,3...]
    F.take(5), // [3,3,3,3,3]
    F.distinct, // [3]
    F.collect); // generator to array
console.log(v);//[3]
Functions / Examples
currying

run
pipe
compose
curry

functional

filter
map
take
takeWhile
fmap          [change]
flatMap    [change]
flat
dflat
reverse
forEach
zip
zipWith
drop
dropWhile
emptyThen
errorThen
distinct
distinctBy
splitBy
innerJoin
leftInnerJoin
rightInnerJoin
outerJoin
leftOuterJoin
rightOuterJoin
then
tap
concat
union
scanl
scanl1
buffer

functional / parallel

parallel_set_fetch_count
pfilter
pmap
pfmap
pcalls

generator

range
seq
rangeOf [deprecated]
repeat
rangeInterval
iterate

aggregate

foldl
foldl1
reduce
foldr
foldr1
collect
collectMap
collectSet
maxBy
minBy
max
min
some
every
count
sum
average
groupBy
orderBy
sortBy
order
sort

util / else

sleep
head
tail
interval
timeout
withTimeout
notNil [deprecated]
isNil
cond
otherwise


run
combination left to right functions
first arguments received second functions argument
from second received combine functions
returns promise
const v = await F.run(
            F.range(10),//[0~9]
            F.filter(e => e % 2 == 0), //[0,2,4,6,8] 
            F.map(e => e + 1), //[1,3,5,7,9]
            F.reduce((acc, e) => acc + e)) // 1+3+5+7+9
console.log(v + 1); // 25 + 1
this expands to
const v = await F.reduce((acc, e) => acc + e, // 1+3+5+7+9
                    F.map(e => e + 1, //[1,3,5,7,9]
                        F.filter(e => e % 2 == 0, //[0,2,4,6,8]
                            F.range(10)))); //[0~9]
pipe
combination left to right functions
only first function can use multiple arguments.
return value is promise.
see also compose
const rs = F.pipe(
    e => e.sort(), //[1,2,3,4,5]
    F.reverse, //[5,4,3,2,1]
    F.collect); //generator to array
const a = [1,5,4,3,2];
const result = await rs(a);//call
console.log(result); //[5,4,3,2,1]
const double1 = F.pipe(
    F.map(e => e + e), //[2,4,6,8]
    F.collect);
const a = [1,2,3,4];
const r1 = await double1(a);
console.log(r1); // [2,4,6,8]
const double2 = F.pipe(
    t => t.map(e => e + e)); // Array.map

const a = [1,2,3,4];
const r2 = await double2(a); // return promise
console.log(r2); // [2,4,6,8]
compose
combination right to left functions
only last function can use multiple arguments.
return value is promise.
see also pipe
const rs = F.compose(
    F.collect, //generator to array
    F.reverse, //[5,4,3,2,1]
    e => e.sort() //[1,2,3,4,5]
);
const a = [1,5,4,3,2];
const result = await rs(a);//call
console.log(result); //[5,4,3,2,1]
const double1 = F.compose(
    F.collect,
    F.map(e => e + e) //[2,4,6,8]
);
const a = [1,2,3,4];
const r1 = await double1(a);
console.log(r1); // [2,4,6,8]
const double2 = F.compose(
    t => t.map(e => e + e)); // Array.map

const a = [1,2,3,4];
const r2 = await double2(a); // return promise
console.log(r2); // [2,4,6,8]
curry
if all arguments are not given for the function,
it returns the function that stored the argument
const myAdd = F.curry((a,b,c) => a + b + c);
const myAdd1 = myAdd(1);
const myAdd2 = myAdd1(2);
const myAdd3 = myAdd2(3);//<- real call
console.log(myAdd3); // print 6
const myAdd = F.curry((a,b,c) => a + b + c);
const r = myAdd(1,2,3); // <- real call
console.log(r); // print 6
filter
const a = [1,2,3,4,5];
const filtered = F.filter(e=> e % 2 == 0, a)
for await (const e of filtered) {
    console.log(e);
}
//print
//2
//4
const r = await F.run(
       [1,2,3,4,5], 
       F.filter(e => e % 2 == 0));

for await (const e of r) {
    console.log(e);
}
//print 
//2
//4
map
const a = [1,2,3,4,5];
for await (const e of F.map(e=> e * 2, a)) {
    console.log(e);
}
//print 2 4 6 8 10
const v = await F.run([1,2,3,4,5],
            F.map(e => e + 1),
            F.collect);
console.log(v);
//print 2 3 4 5 6        
take
const a = [1,2,3,4,5];
const t = F.take(3, a);
console.log(await F.collect(t)); // print 1 2 3
const v = await F.run(
    F.range(Infinity),
    F.take(2),
    F.collect
);
console.log(v); // print 0 1
takeWhile
const a = [1,2,3,1,2,3];
const t = F.takeWhile(e => e < 3, a);
console.log(await F.collect(t)); // print 1, 2
fmap
change
support for an iterable with non-iterable elements is deprecated.
example) F.fmap(e => e, [[1],[2],3,4,5])
current: [1,2,3,4,5]
after: throw Error
use F.flat or F.map instead.
const a = [[1],[2],[3],[4],[5]];
const f = F.fmap(e => e, a);
console.log(await F.collect(f)); // print [1,2,3,4,5]
const a = [ 
        [Promise.resolve(1)],
        Promise.resolve([2]),
        [3],
        [4],
        [5]];
const f = F.fmap(e => e, a);
console.log(await F.collect(f)); // print [1,2,3,4,5]
flatMap
same as fmap
flat
const a = [ 
        [Promise.resolve(1)],
        Promise.resolve([2]),
        [3],
        4,
        5];
const f = F.flat(a);
console.log(await F.collect(f)); // print [1,2,3,4,5]
dflat
Similar to flat, but works recursively
const r = F.dflat([[[1],[2]]],[[3]],[4]);
const c = await F.collect(r);
console.log(c);//print [1,2,3,4]
const r = F.dflat(""HELLO"");
const c = await F.collect(r);
console.log(c);//print [""H"",""E"",""L"",""L"",""O""]
reverse
const a = [1,2,3,4,5];
const t = F.reverse(a);
console.log(await F.collect(t)); // print 5,4,3,2,1
forEach
const beginTime = Date.now();
await F.run(
    F.range(100), 
    F.forEach(async e => {
        await F.sleep(100)
    }));
const endTime = Date.now();
console.log(endTime - beginTime); 
// print 121
// works concurrency
zip
const a = [1,2,3,4,5];
const b = [6,7,8,9,10];
const z = F.zip(a, b);
const arr = await F.collect(z);
for (const e of arr) {
    console.log(e);
    //print
    //[1,6]
    //[2,7]
    //[4,9]
    //[5,0]
}
zipWith
const a = [{id:1}, {id:2}];
const b = [{name:""a""}, {name:""b""}];

const myZip = (f, s) => {
    return [f.id, s.name];
};

const z = F.zipWith(myZip,a, b);
const arr = await F.collect(z);
for (const e of arr) {
    console.log(e);
}
//print
//[1,""a""]
//[2,""b""]
drop
const a = [1,2,3,4,5];
const r = F.drop(3, a)
const result = await F.collect(r);
console.log(result); // print [4, 5]
const a = [1,2,3,4,5];
const r = F.drop(Infinity, a)
const result = await F.collect(r);
console.log(result); // print []
dropWhile
const a = [1,2,3,4,1];
const r = F.dropWhile(e=> e < 3, a)
const result = await F.collect(r);
console.log(result); // print [3,4,1]
const a = [Promise.resolve(1),2,3,4,1];
const r = F.dropWhile(e=> e < 3, a)
const result = await F.collect(r);
console.log(result); // print [3,4,1]
emptyThen
const v = await F.run(F.range(Infinity),
            F.take(0), // take 0 
            F.emptyThen([1,2,3,4,5]), // new array
            F.map(e => e + 1), // 2,3,4,5,6
            F.collect);
console.log(v); // 2,3,4,5,6
const v = await F.run(F.range(Infinity),
    F.take(0),// take 0
    F.emptyThen(()=> { return [1,2,3] }), // new array from function
    F.map(e => e + 1), // 2,3,4
    F.collect) 
console.log(v);// 2,3,4
const v = await F.run(F.range(Infinity),
    F.take(3), // [0,1,2]
    F.emptyThen(([9,9,9]),//not work
    F.map(e => e + 1), //[1,2,3]
    F.collect);
console.log(v); //2,3,4
errorThen
catch error
const v = await F.run([1,2,3,4,5],
    F.filter(e =>{
        if (e > 2) {
            throw new Error(""hello"")
        }
        return e;
    }), // [1, 2 error! 
    F.errorThen([9,8]),//catch and return 9,8
    F.collect); // 
console.log(v);
//print 1,2,9,8
const v = await F.run([1,2,3,4,5],
    F.filter(e =>{
        if (e > 2) {
            throw new Error(""hello error"");
        }
        return e;
    }), // [1, 2 error! 
    F.errorThen((reason) => {
        console.log(reason); //hello error
        return [9,8];
    }),//catch and return 9,8
    F.collect); // 
console.log(v);
//print 
//hello error 
//callstack... 
//1,2,9,8
distinct
const a = [1,2,1,2,2,3];
const r = F.distinct(a);
const result = await F.collect(r);
console.log(result); // print 1,2,3
distinctBy
const a = [{num:1}, {num:1}, {num:2}];
const r = F.distinctBy(e=>e.num, a);
const result = await F.collect(r);
for (const m of result) {
    console.log(m);
}
//print
//{num:1}
//{num:2}
splitBy
to iterable from any
const helloWorld = ""hello world"";
const r = await F.splitBy(e=>e.split("" ""), helloWorld);
for await(const e of r) {
    console.log(e);
}
//print 
//hello
//world
innerJoin
same as leftInnerJoin
leftInnerJoin
support Map, Object({})
const a = [{id:1, name:""foo""}, {id: 2, name:""bar""}, {id: 3, name:""hoo""}];
const b = [{id:1, value:3}, {id: 2, value: 4}];
const j = await F.innerJoin((v1,v2) => v1.id === v2.id , a, b);
const r = await F.collect(j);
console.log(r);
// print
// [{id:1, name:""foo"", value:3},
// {id:2, name:""bar"", value:4}]
rightInnerJoin
support Map, object({})
the result is the same as innerJoin, but the output order is right iterator
const a = [{id:1, value:3}]; 
const b = [{id:1, name:""foo""}, {id: 2, name:""bar""}, {id: 3, name:""hoo""}];
const j = await F.rightInnerJoin((v1,v2) => v1.id === v2.id , a, b);
const r = await F.collect(j);
console.log(r);
//  print
// [{id:1, name:""foo"", value:3}]
outerJoin
same as leftOuterJoin
leftOuterJoin
support Map, object({})
const a = [{id:1, name:""foo""}, {id: 2, name:""bar""}, {id: 3, name:""hoo""}];
const b = [{id:1, value:3}, {id: 2, value: 4}];
const j = await F.outerJoin((v1,v2) => v1.id === v2.id, a, b);
const r = await F.collect(j)
console.log(r);
// print
// [{id:1, name:""foo"", value:3},
// {id:2, name:""bar"", value:4},
// {id:3, name:""hoo""}]
rightOuterJoin
support Map, object({})
const a = [{id:1, value:3}]; 
const b = [{id:1, name:""foo""}, {id: 1, name:""bar""}, {id: 1, name:""hoo""}];
const j = await F.rightOuterJoin((v1,v2) => v1.id === v2.id , a, b);
const r = await F.collect(j);
console.log(r);
// print
// [{id:1, name:""foo"", value:3},
// {id:1, name:""bar"", value:3},
// {id:1, name:""hoo"", value:3}]
then
like promise then
see also tap
const v = await F.run([1,2,3,4,5],
    F.then(async function*(iter) {
        for await(const e of iter) {
            console.log(e);
            yield e;
        }
    }),
    F.map(e => e + 1),
    F.collect);
console.log(v);
//print
//1
//2
//3
//4
//5
//[2,3,4,5,6]
const v = await F.run([1,2,3,4,5],
    F.then(iter => {
        return iter; //do nothing
    }),
    F.collect);
console.log(v);
// print
// [1,2,3,4,5]
tap
call first argument with second argument
then returns the second argument
return promise wrap
see also then
const v = await F.run([1,2,3,4,5],
    F.tap(console.log), //print and return Promise([1,2,3,4,5])
    F.map(e => e + 1),
    F.collect);
concat
merge 2 ranges
const c = F.concat([1,2,3],[4,5,6]);
for await(const e of c) {
    console.log(e);
}
//print [1,2,3,4,5,6]
const v = await F.run(
    F.concat([1,2,3],[4,5,6]),
    F.collect);
console.log(v);
//print [1,2,3,4,5,6]
union
same as concat
scanl
const s = F.scanl((a,b) => a + b, 0, [1,2,3]);
const r = await F.collect(s);
console.log(r);
//print [0,1,3,6]
const r = F.scanl((a, b) => a/b, 64, [4,2,1]);
const r = await F.collect(s);
console.log(r);
//print [64,16,8,8]
scanl1
const s = F.scanl((a,b) => a + b, [1,2,3]);
const r = await F.collect(s);
console.log(r);
//print [1,3,6]
const r = F.scanl1((a, b) => a/b, [64,4,2,1]);
const r = await F.collect(s);
console.log(r);
//print [64,16,8,8]
buffer
creates a list by dividing the iterator at specified interval
const b = F.buffer(1, [1,2,3,4,5]);
const c = await F.collect(b);
console.log(c); //print [[1],[2],[3],[4],[5]]
const b = F.buffer(2, [1,2,3,4,5]);
const c = await F.collect(b);
console.log(c); //print [[1,2],[3,4],[5]]
parallel_set_fetch_count
Set the fetch count of the parallel functions.
after setting, the parallel function is called by count at the same time.
default fetch count is 100
F.parallel_set_fetch_count(3);

await F.run(
    F.range(Infinity),
    F.pmap(async e =>{
        console.log(e);
        return e + 1;
    }),
    F.take(1),
    F.collect);
//print
//0
//1
//2
F.parallel_set_fetch_count(200);

await F.run(
    F.range(Infinity),
    F.pmap(async e =>{
        console.log(e);
        return e + 1;
    }), // fetch and execute first [0..199]
    F.take(1), // take 0 and execute 200. in pmap:[3..102]
    F.collect);
//print
//0
//1
//2
//3
//4
//5
//...
//...
//198
//199
//200
pfilter
Same as filter, but calls a fetch count of functions concurrently.
useful for async function or return promise.
//F.parallel_set_fetch_count(100);  default is 100
const v = await F.run(
    F.range(Infinity),
    F.pfilter(async e =>{
        console.log(e);

        //somthing async work...

        return e % 2 === 0;
    }),// fetch and execute first [0..99]
    F.take(2),// take 2 and execute 100, 101, 102 in pmap:[3..102]
    F.collect);
console.log(v);
//print
//1
//2
//3
//...
//...
//99
//[0,2]
pmap
Same as map, but calls a fetch count of functions concurrently.
useful for async function or return promise.
//F.parallel_set_fetch_count(100); default is 100
const v = await F.run(
    F.range(Infinity),
    F.pmap(async e =>{
        console.log(e);

        //somthing async work...

        return e + 1;
    }), // fetch and execute first [0..99]
    F.take(2), // fetch 0, 1, excute 100, 101 in pmap:[2..101]
    F.collect);
console.log(v);
//print
//0
//1
//2
//...
//...
//99
//100
//101
//[1,2]
pfmap
Same as fmap, but calls a fetch count of functions concurrently.
useful for async function or return promise.
// F.parallel_set_fetch_count(100); default is 100
const v = await F.run(
    F.range(Infinity),  //0,1,2,...
    F.map(e=> [e]),     //[0],[1],[2]...
    F.pfmap(async e =>{
        console.log(e); //print [0] ...

        //somthing async work...

        e.push(42);     // [0,42],[1,42],[2,42]... 
        return e ;
    }),
    F.take(5),          //[0,42,1,42,2]
    F.collect);         //iterator to array
console.log(v);
//print
//[0]
//[1]
//[2]
//...
//...
//[99]
//[0,42,1,42,2]
pcalls

async function generator

const gfn2 = async function* () {
    yield _ => Promise.resolve(1);
    yield _ => 2;
    yield async _ => await Promise.resolve(3);
    yield async _ => await 4;
};
const c = F.pcalls(gfn2());
const r = await F.collect(c);
console.log(r);
//print [1,2,3,4]

call vaarg async functions

/*same as */Promise.all([fn1(),fn2(),fn3(),fn4()])
const fn1 = () => {
    return 1;
};
const fn2 = () => {
    return Promise.resolve(2);
};
const fn3 = async () => {
    return 3;
};
const fn4 = async () => {
    return Promise.resolve(4);
};
const c = F.pcalls(fn1, fn2, fn3, fn4);
const r = await F.collect(c);
console.log(r); //print [1,2,3,4]
range
for (const e of F.range(10)) {
    console.log(e);
}
//print 0 ~ 9

for (const e of F.range(10, 0, -1)) {
    console.log(e);
}
//print 10 ~ 1
seq
make iterable(array, set, map, iteratorObject) to asyncIterator
const a = [1,2,3,4,5];
for await(const e of F.seq(a)) {
    console.log(e);
}
//print 1,2,3,4,5
const a = new Map([[1,2],[3,4]]);
for await(const e of F.seq(a)) {
    console.log(e);
    //print 
    //[1,2]
    //[3,4]
}
rangeOf
deprecated
deprecated. use flat or dflat instead.
repeat
const r = F.repeat(3);
for await(const e of r) {
    console.log(e);
}
//print 
//3
//3
//3
//....
const v = await F.run(
    F.repeat(1), //[1,1,1....]
    F.map(e => e + 1), //[2,2,2....]
    F.take(5), //[2,2,2,2,2]
    F.collect); //generator => array
console.log(v);
//print [2,2,2,2,2]
const r = F.repeat(()=>{return 3;});
for await(const e of r) {
    console.log(e);
}
//print 
//3
//3
//3
//....
rangeInterval
first argument is set to the repeat interval
for await (const e of F.rangeInterval(100, 5)) {
    console.log(e);
}
//print
// [sleep 100]
// 0
// [sleep 100]
// 1
// [sleep 100]
// 2
// [sleep 100]
// 3
// [sleep 100]
// 4
// [sleep 100]
for await (const e of F.rangeInterval(100, 5, 0, -1)) {
    console.log(e);
}
//print
// [sleep 100]
// 5
// [sleep 100]
// 4
// [sleep 100]
// 3
// [sleep 100]
// 2
// [sleep 100]
// 1
// [sleep 100]
iterate
apply a function to an argument to produce a sequence
const r = await F.run(
            F.iterate(F.inc, 1),
            F.take(5),
            F.collect);
console.log(r);
//print 
//[1,2,3,4,5]
const fibo = (a) => [a[1], a[0] + a[1]];
const r = await F.run(
    F.iterate(fibo, [0, 1]),//[0, 1], [1, 1], [1, 2], [2, 3] ...
    F.map(F.head),//[0,1,1,2 ... 
    F.take(10),//[0,1,1,2,3,5,8,13,21,34]
    F.collect);//generator to array
console.log(r);
//print
//[0,1,1,2,3,5,8,13,21,34]
foldl
const a = [1,2,3,4,5];
const sum = await F.foldl((acc, e) => acc + e, 0, a); 
console.log(sum); // print 15
const a = [""w"",""o"",""r"",""l"",""d""];
const sum = await F.foldl((acc, e) => acc + e, ""hello"", a); 
console.log(sum); // print ""helloworld""
foldl1
take 1 items and call foldl
const a = [1,2,3,4,5];
const sum = await F.foldl1((acc, e) => acc + e, a); 
console.log(sum); // print 15;
reduce
same as foldl1
const a = [1,2,3,4,5];
const sum = await F.reduce((acc, e) => acc + e, a); 
console.log(sum); // print 15;
foldr
const arr = [1,2,3,4,5];
const r = await F.foldr((a, b) => a + b, 0, arr);
console.log(r); // print 15
const arr = [64,2,1];
const r = await F.foldr((a, b) => a / b, 1, arr);
console.log(r); // print 32
const arr = [""1"",""2"",""3"",""4""];
const r = await F.foldr((a, b) => a + b, ""5"", arr);
console.log(r); // print 12345
foldr1
const arr = [1,2,3,4,5];
const r = await F.foldr1((a, b) => a + b, 0, arr);
console.log(r); // print 15
const arr = [64,2,1];
const r = await F.foldr1((a, b) => a / b, arr);
console.log(r); // print 32
const arr = [""1"",""2"",""3"",""4"",""5""];
const r = await F.foldr1((a, b) => a + b, arr);
console.log(r); // print 12345
collect
iterator or asyncIterator to Array
const mapped = F.map(e => e + 1, a); 
console.log(mapped); // print asyncGenerator
const collected = await F.collect(mapped);
console.log(collected); //print [2,3,4,5,6]
const v = await F.run(
    F.range(Infinity),//[0,1,2....]
    F.filter(e => (e % 3) === 0), //[0,3,6...] 
    F.map(e => e + 1), //[1,4,7...]
    F.take(5), // generator([1,4,7,10,13])
    F.collect);  // generator => array
console.log(v); //[1,4,7,10,13]
collectMap
const a = [[1,2],[3,4]];
const m = await F.collectMap(a); // new Map([[1,2],[3,4]])
for(const [k,v] of m) {
    console.log(k, v);
}
//print 
//1 2
//3 4
collectSet
const a = [1,2,3,1,2,3];
const m = await F.collectSet(a); //new Set([1,2,3])
for(const e of m) {
    console.log(e);
}
//print 
//1
//2
//3
const a = ""hello world"";
const m = await F.collectSet(a); //new Set(""helo wrd"")
for(const e of m) {
    console.log(e);
}
//print 
//helo wrd
maxBy
const a = [10,9,8,7];
const r = await F.maxBy(e => e, a);
console.log(r); // print 10;
const a = [1,10,9,8,7,11];
const r = await F.maxBy(e => Math.floor(e/10), a) //compare [0,1,0,0,0,1]
console.log(r); // print 10
minBy
const a = [0,10,9,8,7];
const r = await F.minBy(e => e, a);
console.log(r); // print 0
const a = [7,10,9,8,1,11];
const r = await F.minBy(e => Math.floor(e/10), a) //compare [0,1,0,0,0,1]
console.log(r); // 7
max
const a = [Promise.resolve(10),9,8,7];
const r = await F.max(a);
console.log(r);
//print 10
min
const a = [10,9,8,Promise.resolve(7)];
const r = await F.min(a);
console.log(r);
//print 7
some
const a = [1,2,3,4,5];
const r = await F.some(e=> e % 2 == 0, a); //found '2' return
console.log(r); // true
const r = await F.run(
    F.range(Infinity), //[0...Infinity]
    F.some(e=> Promise.resolve(e > 100)) // found '101' return
);
console.log(r); // true
every
const a = [1,2,3,4,5];
const r = await F.every(e=> e  >= 0, a); // all elem >= 0 return true
console.log(r); // true
const a = [1,2,3,4,5];
const r = await F.every(e=> Promise.resolve(e < 3), a); 
//1 ok, 2 ok, 3 no return false
console.log(r); // false
count
const a = [1,2,3,4,5];
const n = await F.count(a);
console.log(n); // print 5
sum
const a = [1,2,3,4,5];
const n = await F.sum(a);
console.log(n); // print 15
const a = ""abcde"";
const n = await F.sum(a);
console.log(n); // print abcde
average
const a = [1,2,3,4,5];
const s = await F.average(a);
console.log(s); // print 3
const a = [1.0,2.0,3.0,4.0,5.5];
const s = await F.average(a)
console.log(s); //print 3.1
groupBy
returns a Map that is aggregated through a function.
key is the return value of the function, and value is the source.
const a = [
    {type: ""tea"",
        price: 1},
    {type: ""tea"",
        price: 2},
    {type: ""phone"",
        price: 3},
    {type: ""phone"",
        price: 4},
];
//returns new Map(... )
const r = await F.groupBy(e => e.type, a);
console.log(r.get(""tea""));
//print [ { type: 'tea', price: 1 }, { type: 'tea', price: 2 } ]
console.log(r.get(""phone""));
//print [ { type: 'phone', price: 3 }, { type: 'phone', price: 4 } ]
orderBy
same as sortBy
sortBy
const a = [{ year: 1990 }, { year: 2005 }, { year: 1958 }];

const ASC = 'ASC'; // or 'asc'
const DESC = 'desc'; // or 'DESC'

const sortedByASC0 = F.sortBy(e => e.year, F.asc, a);
const sortedByASC1 = F.sortBy(e => e.year, ASC, a);
const sortedByDESC0 = F.sortBy(e => e.year, F.desc, a);
const sortedByDESC1 = F.sortBy(e => e.year, DESC, a);

await F.collect(sortedByASC0);
// [{ year: 1958 }, { year: 1990 }, { year: 2005 }]
await F.collect(sortedByDESC1);
// [{ year: 2005 }, { year: 1990 }, { year: 1958 }]
const a = [3, 6, 2, 3, 7, 10, 23, 21, 22, 16, 13, 14, 17, 20];

const sortedByASC = F.sortBy(e => e, F.asc, a);
const sortedByDESC = F.sortBy(e => e, F.desc, a);

await F.collect(sortedByASC);
// [2, 3, 3, 6, 7, 10, 13, 14, 16, 17, 20, 21, 22, 23]
await F.collect(sortedbyDESC);
// [23, 22, 21, 20, 17, 16, 14, 13, 10, 7, 6, 3, 3, 2]
const a = 'Haskell Brooks Curry';

const sortedByASC = F.sortBy(e => e, F.asc, a);
const sortedByDESC = F.sortBy(e => e, F.desc, a);

await F.collect(sortedByASC).then(e => [e.join('')]);
// ['  BCHaekklloorrrssuy']
await F.collect(sortedByDESC).then(e => [e.join('')]);
// ['yussrrroollkkeaHCB  ']
order
same as sort
sort
const a = [3, 6, 2, 3, 7, 10, 23, 21, 22, 16, 13, 14, 17, 20];

const sortedByASC = F.sort(F.asc, a);
const sortedByDESC = F.sort(F.desc, a);

await F.collect(sortedByASC);
// [2, 3, 3, 6, 7, 10, 13, 14, 16, 17, 20, 21, 22, 23]
await F.collect(sortedbyDESC);
// [23, 22, 21, 20, 17, 16, 14, 13, 10, 7, 6, 3, 3, 2]
const a = 'Haskell Brooks Curry';

const sortedByASC = F.sort(F.asc, a);
const sortedByDESC = F.sort(F.desc, a);

await F.collect(sortedByASC).then(e => [e.join('')]);
// ['  BCHaekklloorrrssuy']
await F.collect(sortedByDESC).then(e => [e.join('')]);
// ['yussrrroollkkeaHCB  ']
sleep
like other language
const beginDate = Date.now();
await F.sleep(1000);
const endDate = Date.now();
console.log(endDate - beginDate); // print 1009
head
get first element
warning: if use head for generator, result is not the same
const a = [1,2,3,4,5];
console.log(await F.head(a)); //print 1
console.log(await F.head(a)); //print 1
const a = F.seq([10,9,8,7]); // make generator
console.log(await F.head(a)); //print 9
console.log(await F.head(a)); //print 8
console.log(await F.head(a)); //print 7
const a = [];
try{
    await F.head(a);
}catch(e) {
    console.log(e);
} 
//print empty iter 
tail
get from the second
warning: if use tail for generator, result is not the same
const a = [1,2,3,4,5];
const t = F.tail(a);
console.log(await F.collect(t)); // print 2 3 4 5
const a = F.seq([10,9,8,7]); //make generator
for await (const e of F.tail(a)){
    console.log(e);
}
for await (const e of a) {
    //a is empty...
    console.log(""a is empty"");
}
//print 
//9
//8
//7
interval
works like built-in function setInterval

timer works same time start of the function
available async function.
only one function is executed at a time.

F.interval(1000, async () => {
    await F.run(
        F.range(5),
        F.then(async _ =>{
            await F.sleep(100);
            console.log(""WORK!"");
        }));
});
///print 
//WORK!
// 1 sec 
//WORK!
// 1 sec
//... 
F.interval(10, async () => {
    await F.run(
        F.range(5),
        F.then(async _ =>{
            await F.sleep(1000);
            console.log(""WORK!"");
        }));
});
///print 
//WORK!
// 1 sec 
//WORK!
// 1 sec
//... 
timeout
@changed iterator timeout use withTimeout instead.
const foo = async () => {
    await F.sleep(1);/*something work*/
    return 1;
};
const v = await F.timeout(40, foo());
console.log(v);
//print 1;
try{
    await F.timeout(40, async ()=>{
        await F.sleep(1000);
    });
} catch(e) {
   console.log(e); 
}
//print
//timeout error
//callstack...
withTimeout
it is effective to use timeout at the bottom of the run
const res = [];
try{
    const iter = await F.run(
        F.range(Infinity),
        F.map(e => e + 1),
        F.map(async e => {
            await F.sleep(5);
            return e;
        }),
        F.take(10)
        F.withTimeout(40));
    
    for await (const e of iter) {
        res.push(e);
    }
} catch(ex) {
    console.log(ex);
}
console.log(res);
//print 
//timeout error
//callstack...
//[1,2,3,4,5,6]
try{
    const a = [1,2,3,4,5];
    const t = F.withTimeout(50, 
        F.map(async e => {
            await F.sleep(5);
            return e;
        }, a));
    const v = await F.collect(t);
    console.log(v);
}catch(ex) {
    console.log(ex);
}
//print 
//timeout error
//callstack....
notNil
deprecated
use isNil instead.
return false null, undefined, NaN true otherwise
console.log(F.notNil(NaN)); // false
console.log(F.notNil(undefined)); //false
console.log(F.notNil(null)); //false
console.log(F.notNil(""null"")); // true
console.log(F.notNil(""NaN"")); //true
console.log(F.notNil(0)); // true
console.log(F.notNil(false)); // true
isNil
return false null, undefined, NaN true otherwise
console.log(F.isNil(NaN)); // true
console.log(F.isNil(undefined)); //true
console.log(F.isNil(null)); //true
console.log(F.isNil(""null"")); // false 
console.log(F.isNil(""NaN"")); //false
console.log(F.isNil(0)); // false
console.log(F.isNil(false)); // false
console.log(F.isNil([])); // false
console.log(F.isNil({})); // false
cond
Requires an even number of arguments
if the first argument is true, it returns the second argument
const r = await F.cond( 
    false, ""ff"",
    true, ""tt"",
    F.otherwise, ""oo""
);
console.log(r); // ""tt""
const r = await F.cond( 
    Promise.resolve(false), ""ff"",
    Promise.resolve(true), ""tt"",
    F.otherwise, ""oo""
);
console.log(r); // ""tt""
otherwise
if (F.otherwise) {
    console.log(""WORK!"");
}
if (F.otherwise()) {
    console.log(""WORK!"");
}
//print 
//WORK!
//WORK!
License

",3
clodonil/Python-Fundamentals,Python,"Python Fundamentals (Python 3)
Objetivo
Capacitar estudantes com nenhum ou prévio conhecimento na linguagem Python a desenvolver aplicações ricas com acesso a banco de dados e interface WEB
Carga Horária
O curso tem carga horária de 40 horas
Metodologia
Conteúdo

Módulo 1
Módulo 2
Módulo 3
Módulo 4
Módulo 5

Temos um grupo no whatsapp para tirar as suas dúvidas:

click aqui para entrar no grupo

By:

Autor   = ['Clodonil Honorio Trigo','clodonil@nisled.org']
linkdin = 'https://www.linkedin.com/in/clodonil-trigo-4155722a'
Blog    = 'http://www.devops-sys.com.br'
",4
sqlmapproject/sqlmap,Python,"sqlmap
     
sqlmap is an open source penetration testing tool that automates the process of detecting and exploiting SQL injection flaws and taking over of database servers. It comes with a powerful detection engine, many niche features for the ultimate penetration tester and a broad range of switches lasting from database fingerprinting, over data fetching from the database, to accessing the underlying file system and executing commands on the operating system via out-of-band connections.
The sqlmap project is sponsored by Netsparker Web Application Security Scanner.
Screenshots

You can visit the collection of screenshots demonstrating some of features on the wiki.
Installation
You can download the latest tarball by clicking here or latest zipball by clicking  here.
Preferably, you can download sqlmap by cloning the Git repository:
git clone --depth 1 https://github.com/sqlmapproject/sqlmap.git sqlmap-dev

sqlmap works out of the box with Python version 2.6, 2.7 and 3.x on any platform.
Usage
To get a list of basic options and switches use:
python sqlmap.py -h

To get a list of all options and switches use:
python sqlmap.py -hh

You can find a sample run here.
To get an overview of sqlmap capabilities, list of supported features and description of all options and switches, along with examples, you are advised to consult the user's manual.
Links

Homepage: http://sqlmap.org
Download: .tar.gz or .zip
Commits RSS feed: https://github.com/sqlmapproject/sqlmap/commits/master.atom
Issue tracker: https://github.com/sqlmapproject/sqlmap/issues
User's manual: https://github.com/sqlmapproject/sqlmap/wiki
Frequently Asked Questions (FAQ): https://github.com/sqlmapproject/sqlmap/wiki/FAQ
Twitter: @sqlmap
Demos: http://www.youtube.com/user/inquisb/videos
Screenshots: https://github.com/sqlmapproject/sqlmap/wiki/Screenshots

Translations

Bulgarian
Chinese
Croatian
French
Greek
Indonesian
Italian
Japanese
Polish
Portuguese
Russian
Spanish
Turkish
Ukrainian

",14120
algolia/algolia.github.io,CSS,"Algolia Community website
This is the source code of https://community.algolia.com/. The deployment to this live website is automated
when changes are pushed to the source branch.
Local setup
Requirements
To run this project, you will need:

Node.js >= 9.80, via nvm - install instructions
Yarn >= 1.5.1 - install instructions (""Alternatives"" tab): curl -o- -L https://yarnpkg.com/install.sh

Pro tip: Remove any brew installed/globall system installed Node.js and Yarn, just use nvm and Alternatives installation, they works perfectly.
Then:
nvm install
nvm use
Dev
To develop on this project, do:
yarn
yarn dev
Deploy
The deploy steps are directly handled by Gulp.js in the gulpfile.
yarn build
yarn deploy
Updating the underlying index
The current appId storing the data is latency (accessible for writes by Algolia employees).
If you want to update the data, first test it on a new index: modify algolia-projects.json and run:
appId= adminApiKey= yarn update-index
Once you are sure this is the right config, then run the same command using latency credentials.
You may encounter issues while running yarn deploy, you can fix it by following this GH thread
",10
dktr0/estuary,Haskell,"Estuary is a platform for collaboration and learning through live coding. It enables you to create sound, music, and visuals in a web browser. Key features include:

built-in tutorials and reference materials
a growing collection of different interfaces and live coding languages
support for networked ensembles (whether in the same room or distributed around the world)
text localization to an expanding set of natural languages
visual customization via themes (described by CSS)

The development of Estuary is the result of ongoing collaborative work that has been supported by two grants from Canada's Social Sciences and Humanities Research Council (SSHRC) - initially for the project ""Projectional interfaces for musical live coding"", and more recently as part of the project ""Platforms and practices for networked, language-neutral live coding"". Estuary builds upon, and depends on, the work of many others, including but not limited to all those who contribute to Reflex , TidalCycles, and other languages and projects. Estuary is free and open source software, released under the terms of the GNU Public License (version 3).
You don't need to install or build anything to start using Estuary - you can just point your web browser to an existing deployment of it, and start making things! Please note that Chrome or Chromium are strongly recommended as the preferred browsers for use with Estuary. At the time of writing, a stable, recent version of Estuary is online 24/7 at a test server belonging to the research group at McMaster University that is working
on Estuary - you can try it out anytime at the following URL (and if you have
questions take them either to the #estuary channel on talk.lurk.org
or the ""estuary"" Google group): http://intramuros.mcmaster.ca:8002
If you do want to build Estuary from source yourself, please refer to the instructions in BUILDING.md. Note that building Estuary depends on a rather complex tool-chain and can be time-consuming. Don't hesitate to reach out on the lurk or Google group channels mentioned above for assistance.
",29
enChenging/android_posthouse,None,"🏃android驿站🏃

为方便使用android开源库，所以本人耗费大量时间进行整理。如果收录的项目有错误或者你知道有比较好的开源项目，可以通过Issues反馈给我，我将尽快的更新到android驿站中，希望android驿站可以成为android开发者在开发过程中的实用工具。项目Star数一般1个月会更新一次，此开源库整理会不断更新，你的点赞👍将是我不断更新的最大动力。

☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️

目录

1----------------------------------------------RecycleView(各种类型列表) 🍀 (43)
2----------------------------------------------Refresh(下拉刷新) 🍀 (12)
3----------------------------------------------TabLayout 🍀 (8)
4----------------------------------------------ProgressBar(进度条) 🍀 (53)
5----------------------------------------------ViewPager 🍀 (18)
6----------------------------------------------Button(按钮) 🍀 (31)
7----------------------------------------------Menu(菜单、浮动菜单) 🍀 (27)
8----------------------------------------------Dialog(对话框)🍀 (12)
9----------------------------------------------ImageView(图片)🍀 (57)
10---------------------------------------------Splash(启动页、引导页)🍀 (16)
11---------------------------------------------TextView🍀 (23)
12---------------------------------------------EditText🍀 (5)
13---------------------------------------------Banner(轮播图)🍀 (10)
14---------------------------------------------BottomNavigation(底部导航)🍀 (12)
15---------------------------------------------Toolbar🍀 (9)
16---------------------------------------------Chart(图表)🍀 (10)
17---------------------------------------------WebView🍀 (5)
18---------------------------------------------Layout(布局)🍀 (17)
19---------------------------------------------Picker(选择器)🍀 (15)
20---------------------------------------------Calendar(日历时间)🍀 (12)
21---------------------------------------------Card🍀 (11)
22---------------------------------------------Blur(模糊效果)🍀 (12)
23---------------------------------------------Theme、StatusBar(主题样式、状态栏)🍀 (13)
24---------------------------------------------Notify(通知)🍀 (5)
25---------------------------------------------Badge(徽章)🍀 (7)
26---------------------------------------------Login(表单)🍀 (15)
27---------------------------------------------Timeline(时间轴)🍀 (4)
28---------------------------------------------Spinner🍀 (3)
29---------------------------------------------SearchView(搜索视图)🍀 (14)
30---------------------------------------------TagView(标签)🍀 (9)
31---------------------------------------------MaterialDesign(MD风格)🍀 (11)
32---------------------------------------------Toast🍀 (7)
33---------------------------------------------SeekBar🍀 (3)
34---------------------------------------------ScrollView🍀 (6)
35---------------------------------------------SwipeBack(滑动返回)🍀 (8)
36---------------------------------------------RatingBar(评分控件)🍀 (7)

",2
ppengtang/pcl.pytorch,Python,"PCL: Proposal Cluster Learning for Weakly Supervised Object Detection
By Peng Tang, Xinggang Wang, Song Bai, Wei Shen, Xiang Bai, Wenyu Liu, and Alan Yuille.
This is a PyTorch implementation of our PCL. The original Caffe implementation of PCL is available here.
We embed the trick proposed in our ECCV paper for better performance.
The final performance of this implementation is mAP 49.2% and CorLoc 65.0% on PASCAL VOC 2007 using a single VGG16 model. The results are comparable with the recent state of the arts.
Introduction
Proposal Cluster Learning (PCL) is a framework for weakly supervised object detection with deep ConvNets.

It achieves state-of-the-art performance on weakly supervised object detection (Pascal VOC 2007 and 2012, ImageNet DET, COCO).
Our code is written based on PyTorch, Detectron.pytorch, and faster-rcnn.pytorch.

The original paper has been accepted by CVPR 2017. This is an extened version.
For more details, please refer to here and here.
Comparison with other methods
(a) Conventional MIL method;
(b) Our original OICR method with newly proposed proposal cluster generation method;
(c) Our PCL method.


Architecture



Visualizations
Some PCL visualization results.



Some visualization comparisons among WSDDN, WSDDN+context, and PCL.



License
PCL is released under the MIT License (refer to the LICENSE file for details).
Citing PCL
If you find PCL useful in your research, please consider citing:
@article{tang2018pcl,
    author = {Tang, Peng and Wang, Xinggang and Bai, Song and Shen, Wei and Bai, Xiang and Liu, Wenyu and Yuille, Alan},
    title = {{PCL}: Proposal Cluster Learning for Weakly Supervised Object Detection},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    volume = {},
    number = {},
    pages = {1--1},
    year = {2018}
}

@inproceedings{tang2017multiple,
    author = {Tang, Peng and Wang, Xinggang and Bai, Xiang and Liu, Wenyu},
    title = {Multiple Instance Detection Network with Online Instance Classifier Refinement},
    booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
    pages = {3059--3067},
    year = {2017}
}

Contents

Requirements: software
Requirements: hardware
Basic installation
Installation for training and testing
Extra Downloads (Models trained on PASCAL VOC)
Usage
TODO

Requirements: software
Tested under python3.

python packages

pytorch==0.4.1
torchvision>=0.2.0
cython
matplotlib
numpy
scipy
opencv
pyyaml==3.12
packaging
pycocotools  — also available from pip.
tensorboardX  — for logging the losses in Tensorboard
sklearn


An NVIDAI GPU and CUDA 8.0 or higher. Some operations only have gpu implementation.
NOTICE: different versions of Pytorch package have different memory usages.

Requirements: hardware

NVIDIA GTX 1080Ti (~11G of memory)

Installation

Clone the PCL repository

git clone https://github.com/ppengtang/pcl.pytorch.git & cd pcl.pytorch

Compile the CUDA code:

cd $PCL_ROOT/lib
sh make.sh
Installation for training and testing

Download the training, validation, test data and VOCdevkit

wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar
wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar
wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCdevkit_18-May-2011.tar

Extract all of these tars into one directory named VOCdevkit

tar xvf VOCtrainval_06-Nov-2007.tar
tar xvf VOCtest_06-Nov-2007.tar
tar xvf VOCdevkit_18-May-2011.tar


Download the COCO format pascal annotations from here and put them into the VOC2007/annotations directory


It should have this basic structure


$VOC2007/                           
$VOC2007/annotations
$VOC2007/JPEGImages
$VOC2007/VOCdevkit        
# ... and several other directories ...

Create symlinks for the PASCAL VOC dataset

cd $PCL_ROOT/data
ln -s $VOC2007 VOC2007
Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects.


[Optional] follow similar steps to get PASCAL VOC 2012.


You should put the generated proposal data under the folder $PCL_ROOT/data/selective_search_data, with the name ""voc_2007_trainval.pkl"", ""voc_2007_test.pkl"". You can downlad the Selective Search proposals here.


The pre-trained models are available at: Dropbox, VT Server. You should put it under the folder $PCL_ROOT/data/pretrained_model.


Download models trained on PASCAL VOC
Models trained on PASCAL VOC can be downloaded here.
Usage
Train a PCL network. For example, train a VGG16 network on VOC 2007 trainval
CUDA_VISIBLE_DEVICES=0 python tools/train_net_step.py --dataset voc2007 \
  --cfg configs/baselines/vgg16_voc2007.yaml --bs 1 --nw 4 --iter_size 4
Note: The current implementation has a bug on multi-gpu training and thus does not support multi-gpu training.
Test a PCL network. For example, test the VGG 16 network on VOC 2007:
On trainval
python tools/test_net.py --cfg configs/baselines/vgg16_voc2007.yaml \
  --load_ckpt Outputs/vgg16_voc2007/$MODEL_PATH \
  --dataset voc2007trainval
On test
python tools/test_net.py --cfg configs/baselines/vgg16_voc2007.yaml \
  --load_ckpt Outputs/vgg16_voc2007/$model_path \
  --dataset voc2007test
Test output is written underneath $PCL_ROOT/Outputs.
Note: Add --multi-gpu-testing if multiple gpus are available.
Evaluation
For mAP, run the python code tools/reval.py
./tools/reeval.py $output_dir/detections.pkl \
  --dataset voc2007test --cfg configs/baselines/vgg16_voc2007.yaml
For CorLoc, run the python code tools/reval_discovery.py
./tools/reeval.py $output_dir/discovery.pkl \
  --dataset voc2007trainval --cfg configs/baselines/vgg16_voc2007.yaml
What we are going to do

 Add PASCAL VOC 2012 configurations.
 Upload trained models.
 Support multi-gpu training.

",13
Jackarunda/gmod,Lua,"The Garry's Mod Additions Pack!
Huzzah!
The Plan
We're taking all of Jackarunda's past addons and resurrecting them into a single big community project to re-upload to the workshop.
Once most of the content is copied, converted and working, Jackarunda will split the content of this repo up and upload mutliple Workshop items so that everyone can enjoy this content once again and use it on their servers bug-free. Maintenance and additions will continue from there and Jackarunda will handle the workshop aspect of things. If you help as a playtester, you will be mentioned in the credits for the packs if you wish. If you help as a coder or file-folder organizer/converter in any way, you will be added as a co-publisher to the workshop items if you wish. This is a community project.
Contributing
We don't really need hardcore functional programming to make this work. If we do, jackarunda will do it. What we really need right now is:

move all of these files from X folder to Y folder


combine the content of these files into one file and put it in Z folder


take the content of this file and put it in a section of another file and delete this file


see what resources are needed from these files and copy the resources into the new folder


rename this file and then rename all usages of it in all the other files


rename this sound file and then replace any usages of it in these files


look through this folder for any large texture files, and if you find them, downsize them in GIMP or something and replace


look throgh this folder for large sound files and if you find them, then convert them to MP3 in Audacity or something and replace

If you think you can help, let Jackarunda know in the gmod Discord.
Setup
You must have a github account, first. You can make one for free.
Ask Jackarunda in the Discord to be added as a collaborator (tell him your github name), and he will find your github account and add you. Then you go back to github into your profile and accept the collaboration invite. Being a collaborator is important because it will allow you to push/pull without needing to fork (which is more complicated). Note that being a collaborator is a trusted position, because you will have the ability to mess up the repo (though if you do, Jackarunda will revoke your permissions and revert the changes).
If you're just edting the wiki file or the bugs file, you can just click the little edit buttons here on the github website to edit the files in-browser. Simple.
If, however, you're going to be moving/organizing content, then you need to have Git and know how to use it.
If you don't know anything about Git, then skip down and read the Git Noob section before continuing.
Clone this repo into your local gmod/addons directory. It will work just fine in gmod right from there, because this repo is orgnized exactly like a Gmod Legacy Addon. You can make changes directly right there, test them in gmod automatically (lua auto-reload ftw), and then push them up into git. Simple.
All the old addons can be found for download here:
FunGuns: https://www.dropbox.com/s/kxxbex74acct06r/FunGuns.7z?dl=0
Homicide: https://www.dropbox.com/s/qoegratt6amsxdl/Homicide.7z?dl=0
Defense Solutions: https://www.dropbox.com/s/ac8xg6tibl1gxfr/JIDS.7z?dl=0
OpSquads: https://www.dropbox.com/s/2k54kb7lq8ikw5o/OpSquads.7z?dl=0
Explosives: https://www.dropbox.com/s/8inhop8y3panltc/JIEX.7z?dl=0
SENTs: https://www.dropbox.com/s/7yc1gz3yw8oe88r/SENTs.7z?dl=0
BFS 2114: https://www.dropbox.com/s/qrpwohdcwypmbvr/JIBFS2114.7z?dl=0
Old BFS 2114 Wiki: http://jibfs.wikia.com/wiki/JIBFS_Wiki
Git Noob?
Git is a source control system, which is a system that allows multiple people to work together on a software project and not step on eachother's toes. Mostly. Github.com is a website, one of many, that hosts Git Repositories, which are like living containers for projects (contain all the files and assets and records etc).
Note that Git is a tech industry standard across the whole entire world and there are hundreds of thousands of millions of billions of blogs, tutorials, guides, documents, questions, answers, etc. etc. etc all over the internet that can help you with Git.
To work on git projects and do more than just edit text files in-browser, you need to have Git and Git Bash installed on your machine.
https://git-scm.com/downloads download and install all this from here
Note that Github recently made a GUI program for doing git operations, but IMO it's kinda pointless since the moment anything goes wrong you have to use the git bash command-line anyways, so might as well not bother. But you can use it if you wish.
Once git and git bash are installed on your machine, start a git bash window (probably from the start menu). Then you need to move the window's operating location into your gmod addons folder, so enter a command that looks something like this:
cd ""C:/Users/DickBagMcGee/Program Files (x86)/Steam/steamapps/common/garrysmod/garrysmod/addons""
But obviously the path is unique for you. Note that when git installs they usually add shell extensions so you can right click or shift+right click and open a new git bash window anywhere in your Explorer, which is easier than CDing every time. Now clone this repo into a new folder into your addons folder by using the command:
git clone https://github.com/Jackarunda/gmod.git gmod-additions-pack
This will create the addon in your gmod. It'll take a while to download. Once this is done, you can literally play in gmod with the addon from right there. You can then make any changes you wish in that local folder, renaming things, adding things, editing files, etc. You only need to clone the repo once, ever, unless yours gets really badly messed up and you need to delete and re-clone. But that should never happen.
Before you make any changes, always make sure to do the command: git pull origin master, because this will pull down the latest version of the repo into your local folder. You always want to be up-to-date.
When you've made changes you want to commit, do the following, in order:


make a new branch with
git checkout -b my_new_unique_branch_name
usually you make a branch name that contains your name and something relating to the work you did


stage all the changes you've made with
git add .


commit the changes to your new local branch you just made, with a comment, by entering
git commit -m ""fixing some bugs and adding more hookers""


push your branch up to the repo with
git push origin my_new_unique_branch_name


go to your browser, to the github repo page, and click the pull request button for the branch you just made


tell jackarunda about it in discord. We'll look at it, maybe fix a few things, and merge it into the master branch. All done.


then you should go back locally and git checkout master and then git pull origin master to get the latest content right from master. Then you can make more changes and start from step 1.


For more complicated operations regarding git, you can consult the wealth of information on the internet and/or ask jackarunda.
ToDo:
Include content from all the previous addons.
OpSquads has been included already. Next on the list is JI Defense Solutions.


download JIDS source from dropbox


copy whatever entities/weapons you like from JIDS into this repo, along with all dependent other entities and effects


convert all entities/weapons to single-file format if not already


take the sounds from JIDS and copy them into this repo's sound folder (sound/snds_jack_gmod), then replace all instances of their use in the entity/weapon files


copy all needed materials and textures from JIDS into respective folders in this repo, keeping the organization scheme the same and renaming if necessary (and replacing usage calls in code if necessary)


do the same for models and particle effects


test in gmod


Jackarunda will handle most of the fixing/optimizing of the code.
Custom Additions
If you're a glua coder and want to add new features or items to the pack, let Jackarunda know and make a PR. Custom contributions to the pack are welcome if they are of a quality and style similar to or surpassing that of existing content.
Bugs
If you find a bug, put it in the bugs.txt file here or tell Jackarunda about it in the gmod channel.
",3
regro/libcfgraph,None,"libcfgraph
Cron Status: 
Graph Data for Conda Forge Library
This repository houses metadata for all of conda-forge's artifacts and is updated hourly.
It is intendeded to be used in conjunction with libcflib which can convert the mountains
of json into something a little nicer to work with
",5
lightyen/react-app-typescript,TypeScript,"react app typescript · 
這是一個我學習 typescript & react 用的開發環境範本，使用自訂義的 webpack 設定，結合 redux, react-router, Sass 等相關技術鏈，開箱即用且跨平台。（不定期更新）
安裝以下開發環境








安裝完後檢查環境是否正確運作
code -v
node -v
yarn -v
安裝相關 Visual Studio Code 擴充元件

Debug for Firefox
Debug for Chrome
TSLint
Awesome Typescript Problem Matcher
Prettier
Format Files
EditorConfig for VS Code

其他我常用的

GitLens
Material Icon Theme
Markdown All in One
TODO Highlight
Fira Code (字型)

Build 建置
# clone this repo
git clone https://github.com/lightyen/react-app-typescript.git

# 進入專案資料夾
cd react-app-typescript

# 檢查或下載 dependencies
yarn

# 開始！
yarn dev

# or 建置 production
yarn build
Debug
在 vscode 中按下 F5 後 launch browser 進行調試，或者直接在瀏覽器使用開發者工具(F12)

Firefox 需要去 about:debugging 勾選 Enable debugging of add-ons 才可以使用

詳細資訊描述在：.vscode/launch.json
懶人包


程式碼風格
約定程式碼風格：

string 以雙引號 "" 表示
statement 除非特例，否則結尾不使用分號 ;
縮排 4 個空格

editorconfig, prettier 程式碼風格
按 F1 > Start Format Files: Workspace 可以格式化所有的程式碼風格

不喜歡我風格的朋友可以自行修改 .editorconfig, .prettierrc, tslint.json

其他知識參考


https://reactjs.org/


https://www.typescriptlang.org/


React Beginners Tutorial


Getting Started With TypeScript


React With TypeScript


React Hook


Awesome


broswerlist

https://browserl.ist/

",4
notadd/notadd,TypeScript,"
Overview
中文文档
Notadd is an open source, Nest.js framework-based microservice development architecture that allows you to build a microservices system using the right modules and addons for different business needs. Notadd officially provides an abstract public service layer. Within the service layer, each module provides the Grpc interface for the Notadd main program to call. For example, a CMS system, you can use the officially provided nt-module-cms and nt-module-user modules as the underlying service layer. Then use the Notadd main program to write your API layer code according to the protobuf message protocol defined by the service layer.
Features

[Microservice] Supports stand-alone deployment and microservice
[High Performance] Asynchronous high-performance applications, tens of thousands of concurrent
[Easy to maintain] Developed with Typescript, intelligent code hints and compile-time code checking mechanisms
[Pluggable] modular development system, according to business needs, select the appropriate module, build the API layer

Technology stack

Typescript
Nest.js
GraphQL
TypeORM
Grpc
Redis

System Architecture

Modular design
Enterprise Official Website: CMS module + neditor plug-in, message board plugin
Information release: CMS module, user module + CMS multi-user plugin, Neditor plugin
WeChat Mall: User module, Mall module, WeChat module + WeChat big turntable, payment plug-in, offline verification plug-in
Dining plan: User module, Mall module, WeChat module + ordering plugin, scan code payment plug-in, passenger flow monitoring plug-in ... + infrared sensor development, WiFi probe expansion
Hotel Program: User module, Hotel module, WeChat module + booking plugin, payment plug-in, smart WiFi plugin + WiFi probe expansion, door card system expansion
CRM system: User module, CRM module ...
More to imagine ...
Quick Start

Clone Rpc sample service to the local nt-rpc-demo
Clone the user service to the local nt-module-user
Start the microservice according to the instructions of nt-rpc-demo and nt-module-user
Clone this project to your local
Installation depends on yarn install
Start yarn start
Open a browser and go to localhost:5000/graphql
Test GraphQL API


Note: The Notadd main program provides demo code at this stage, and does not rule out the removal of all graphql api code later.

Module list

nt-module-user user module
nt-module-cms  CMS module

Addon list

nt-addon-pay payment addon
nt-addon-wechatapi wechatapi addon

Contribution
Welcome to Pull requests. For major changes, please file a Issue and discuss with us what you want to change.
Contributors
Thanks to all those who have contributed to notadd!

Communication
Tencent QQ Group：322247106
Forum: Under construction
Blog: Under construction
Excellent Repositories

Swoft Modern High performance AOP and Coroutine PHP Framework, base on Swoole 2
ThinkSNS Plus Use of Laravel framework to achieve the user ecosystem.
Neditor A modern editor based on the Ueditor.

Sponsor
We would like to thank the following sponsors for funding the development of our Notadd. If you are interested in becoming a sponsor, please visit Notadd's Gitee Page:

China Xian · Benchu Network
China Hanzhou · upyun
China Xian · Mada Network

(Please ask your company to support this open source project by becoming a sponsor)

Backer
Thank you to all our backers! Become a backer

License
The Notadd is open-sourced software licensed under the Apache 2.0 license.

TODO

 Internationalization (i18n) support
 Public services such as cms、pay、config、storage、logger, etc.
 Service governance, fuse, downgrade, load, registration and discovery
 Support PWA technology, implement off-screen reminders, web-off form saving, webpage offline message push

",1872
eshaibu/react-todo,JavaScript,"React Todo
Instructions

Clone the project
If cloning for the first time, create a .env file(if not created already), copy the contents .env.example into the new .env file
Update your .env file with the appropriate values

Set the environment value for
REACT_APP_API_BASE_URL: The base url of the API, the default value is http://localhost:8000/api/v1

Install dependencies => npm install
To start the application run npm start
To run test run npm test

",2
keywish/keywish-hummer-bot-v2.0,C++,"Please Contact Us
Technical support email: abbott@emakefun.com
Sales email: ken@keywish-robot.com
The latest information download address: https://github.com/keywish/keywish-hummer-bot-v2.0

Hummer-bot

Introduction
Hummer-Bot is a multi-function car, it is based on the Arduino UNO and the L298N motor drive module. You can freely install various sensor modules such as servo, ultrasonic, infrared obstacle avoidance, infrared tracking, etc. It can trace and avoid obstacles automatically, and it supports multiple remote control methods: infrared remote control, Bluetooth APP, PS2 handle (optional). And we will provide CD with the best tutorial including the programs and codes which bring you to the robot car world. It is the best choice for electronics enthusiasts, makers, and Arduino enthusiasts.
Feature

3-channel infrared tracking modules 
2 sets of infrared obstacle avoidance modules 
ultrasonic obstacle avoidance with servo 
2-channel DC motor drive 
2 rechargeable lithium batteries:3000mZh/3.7v, longer battery life 
Real-time battery power detection 
Infrared remote control 
Bluetooth app control 
PS2 handle control (optional) 

Required Best Buy Links
Buy on Amazon 
Buy on AliExpress
Video Links
Component introduction 
Function 
Assembly 
Download Method

",4
G-little/priest,JavaScript,"priest
dubbo mybatis springboot base soa rest api framework with customer code generator
前言
做过几个软件项目开发的人，可能或多或少都会发现，除去项目本身业务细节的区别，项目与项目之间还是有大量的功能重复，
如用户中心、接口调用权限认证、支付中心、账户钱包、聊天消息、管理后台的权限和菜单管理，几乎是每个项目会用到的基础模
块，本项目旨在以priest 项目 (priest 项目是一个以 incubator-dubbo - Spring Boot - Mybatis3 为基础的SOA开源开发框架)
为基础，将常用的通用非业务模块独立组件化，让新的业务项目研发如积木组装一样简单，让开发人员的全部放到业务细节的快速
实现上，让项目创业的人员配置简单化，成本最小化。
项目介绍
priest项目基于springboot+dubbo+mybatis的分布式敏捷开发框架，将 JSR303 hibernate-validate 验证体系完美融合dubbo服务框架。并以此为基础开发了maven-code-generator 插件，让研发人员从重复的增删改查工作中彻底解脱。
本项目为后续开源的所有项目的基石项目，后续计划陆续的开源项目将会有 用户中心 支付中心 敬请期待！
更新日志
2019-04-17


用户模块微信OAuth2登录支持


支付模块用户账户支持


2019-04-09


增加了用户模块

短信注册
用户信息补充
获取用户信息
restApi 接口调用认证
token 过期刷新
用户主动登出



增加后台管理模块

权限管理
菜单管理
管理员管理
代码生成demo



代码生成插件

后台管理jsp代码生成支持
代码生成 list attribute 标签支持



项目计划



模块
功能描述
完成度




priest-pay
支付宝支付
 🔘


priest-pay
微信支付
 🔘


priest-pay
用户账户
 🔘


dubbo-extend
改为dubbo 原生JSR303 参数验证
 🔘


priest-user
token 生成
 ✔️


priest-user
restApi token 验证
 ✔️


priest-generator
管理后台页面自动生成
 ✔️



组织结构
├── dubbo
│   ├── assembly     --dubbo 打包相关配置
│   └── bin          --dubbo 启动脚本
├── dubbo-extend     --dubbo 扩展支持dubbo接口参数校验
├── plugin-test      --代码生成插件测试项目
├── priest-admin     --管理后台项目
│   ├── priest-admin-api     --管理后台api项目
│   ├── priest-admin-common  --管理后台公用模块
│   ├── priest-admin-dao     --管理后台数据库访问层
│   ├── priest-admin-http    --管理后台WEB界面
│   └── priest-admin-service --管理后台dubbo服务
├── priest-common            --priest 项目公用模块
├── priest-common-web        --priest 项目公用WEB模块
├── priest-demo              --priest 服务化样例项目
│   ├── priest-demo-api      --priest 服务化样例api项目
│   ├── priest-demo-dao      --priest 服务化样例dao项目
│   ├── priest-demo-http     --priest 服务化样例http项目
│   └── priest-demo-service  --priest 服务化样例dubbo服务项目
├── priest-generator         --priest 代码生成插件项目
├── priest-user              --用户项目
│   ├── priest-user-api      --用户api项目
│   ├── priest-user-dao		 --用户数据库访问层
│   ├── priest-user-http     --用户WEB接口
│   ├── priest-user-service  --用户dubbo服务
│   └── priest-user-token    --用户token生成模块
└── wiki_images              --wiki 引用图片
  
Getting Started

项目依赖

mysql 数据库
zookeeper 注册服务
maven
jdk 1.8+

项目编译


进入项目根目录


打开项目 priest-demo/priest-demo-dao/src/main/resources/demo.sql 建立创建测试数据库及表


修改 priest-demo/priest-demo-dao/pom.xml develop profile 关于jdbc的配置见下图



修改 根目录pom.xml develop profile 关于zookeeper配置见下图



进入项目根目录


mvn clean install -Pdevelop


项目运行

dubbo service 启动

运行  priest-demo/priest-demo-service/src/test/java/com/little/g/demo/TestDubbo.java main


http 启动

进入priest-demo/priest-demo-http 项目目录
执行 mvn spring-boot:run  ，观察控制台日志输出，出现如下日志，便是启动成功了。




接口访问测试


curl http://127.0.0.1:8888/user/test
开发流程
数据库建表
本例以order表为例子建表语句如下:
	
CREATE TABLE `order` (
  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `uid` int(11) DEFAULT NULL COMMENT '用户ID',
  `money` bigint(15) DEFAULT NULL COMMENT '金额',
  `create_time` bigint(15) DEFAULT NULL COMMENT '创建时间',
  `status` tinyint(4) DEFAULT NULL COMMENT '状态',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
	
dao 生成

打开priest-demo/priest-demo-dao/src/test/resources/generatorConfig.xml追加如下配置

<table tableName=""order"" delimitIdentifiers=""true"" >
            <generatedKey column=""id""  sqlStatement=""JDBC"" />
</table>     



调用priest-demo-dao的 mybatis-generator插件的  mybatis-generator:generate 任务,执行结果如下:



关于mybatis-generator的详细配置和文档可参见 MyBatis Generator


api 生成

拷贝 com.little.g.demo.model.Order 至 priest-demo-api 项目的 com.little.g.demo.api.dto 重命名为 OrderDTO
打开priest-demo/priest-demo-api/src/main/conf/GenerateConfig.xml 追加如下配置

<generateFile packagePath=""/com/little/g/demo/api"" templateName=""Service.tpl"" fileName=""OrderService.java"">
        <property name=""entityName"" value=""Order"" />
</generateFile>




调用priest-demo-api的 generator插件的 generator:generate 任务,执行结果如下:



关于generator 插件的详细配置和文档可参见


service 生成

打开priest-demo/priest-demo-service/src/main/conf/GenerateConfig.xml 追加如下配置

<generateFile packagePath=""/com/little/g/demo/service"" templateName=""ServiceImpl.tpl"" fileName=""OrderServiceImpl.java"">
        <property name=""entityName"" value=""Order"" />
</generateFile>




调用priest-demo-service的 generator插件的 generator:generate 任务,执行结果如下:



priest-demo-service dubbo-config.xml 追加dubbo service 接口暴露
<dubbo:service interface=""com.little.g.demo.api.OrderService"" ref=""orderService""/>


关于generator 插件的详细配置和文档可参见 priest generator


http 生成

打开priest-demo/priest-demo-http/src/main/conf/GenerateConfig.xml 追加如下配置

<generateFile packagePath=""/com/little/g/demo/web"" templateName=""Controller.tpl"" fileName=""OrderController.java"">
        <property name=""entityName"" value=""Order"" />
</generateFile>




调用priest-demo-http的 generator插件的 generator:generate 任务,执行结果如下:



priest-demo-http dubbo-consume.xml 追加dubbo service 引用
<dubbo:reference id=""orderService"" interface=""com.little.g.demo.api.OrderService"" />


关于generator 插件的详细配置和文档可参见 priest generator


管理后台代码生成
管理后台的dao、api和service 的生成与restApi的生成逻辑本质没有太大区别，只是在接口原有逻辑的基础上增加了分页逻辑，对于既有接口逻辑又有后台管理的模块，可以采用管理后台的 api、service模板
进行生成，下面只单独列出 admin 管理页面的代码生成配置：

打开priest-admin/priest-admin-http/src/main/conf/GenerateConfig.xml 追加如下配置

注意 :默认的web文件生成路径位于 WEB-INF/jsp+${webPath} 若要修改web文件生成路径，直接修改插件的webSource配置

<!-- Controller 生成配置 -->
<generateFile packagePath=""/com/little/g/admin/web/controllers/test"" templateName=""Controller.tpl"" fileName=""BookController.java"">
        <property name=""packageName"" value=""com.little.g.admin.web.controllers.test"" />
        <property name=""basePackage"" value=""com.little.g.admin"" />
        <property name=""entityName"" value=""Book"" />
        <property name=""module"" value=""BOOK"" />
        <property name=""uri"" value=""/book"" />
    </generateFile>
    
<!-- list.jsp 生成配置 -->
<generateFile webPath=""/book"" templateName=""list.tpl"" fileName=""book-list.jsp"">
        <property name=""uri"" value=""/book"" />
        <property name=""module"" value=""图书"" />
        <list name=""attributes"">
            <attribute name=""name"" required=""true"" comment=""书名"" />
            <attribute name=""price"" required=""false"" comment=""价格"" />
            <attribute name=""author"" required=""false"" comment=""作者"" />
            <attribute name=""publisher"" required=""false"" comment=""出版商"" />
        </list>
    </generateFile>

<!-- edit.jsp 生成配置 -->
<generateFile webPath=""/book"" templateName=""edit.tpl"" fileName=""book-edit.jsp"">
        <property name=""uri"" value=""/book"" />
        <property name=""paramName"" value=""book"" />
        <property name=""module"" value=""图书"" />
        <list name=""attributes"">
            <attribute name=""name"" required=""true"" comment=""书名"" />
            <attribute name=""price"" required=""false"" comment=""价格"" />
            <attribute name=""author"" required=""false"" comment=""作者"" />
            <attribute name=""publisher"" required=""false"" comment=""出版商"" />
        </list>
    </generateFile>



调用priest-admin-http的 generator插件的 generator:generate 任务,执行结果如下:



priest-admin-http dubbo-consume.xml 追加dubbo service 引用
<dubbo:reference id=""orderService"" interface=""com.little.g.demo.api.BookService"" />


访问admin项目增加菜单




admin 生成界面效果截图

列表页:

修改页:


关于generator 插件的详细配置和文档可参见 priest generator

批量生成脚本
您也可以在配置完成后，直接执行批量生成脚本，完成上述所有步骤
脚本位于 priest-demo 根目录
	sh	code_generate.sh
	
最后项目重新编译运行，新开发的接口就可以测试了
",31
pandas-dev/pandas,Python,"



pandas: powerful Python data analysis toolkit

  

Latest Release














Package Status







License







Build Status















Coverage






Downloads







Gitter







What is it?
pandas is a Python package providing fast, flexible, and expressive data
structures designed to make working with ""relational"" or ""labeled"" data both
easy and intuitive. It aims to be the fundamental high-level building block for
doing practical, real world data analysis in Python. Additionally, it has
the broader goal of becoming the most powerful and flexible open source data
analysis / manipulation tool available in any language. It is already well on
its way towards this goal.
Main Features
Here are just a few of the things that pandas does well:

Easy handling of missing data (represented as
NaN) in floating point as well as non-floating point data
Size mutability: columns can be inserted and
deleted from DataFrame and higher dimensional
objects
Automatic and explicit data alignment: objects can
be explicitly aligned to a set of labels, or the user can simply
ignore the labels and let Series, DataFrame, etc. automatically
align the data for you in computations
Powerful, flexible group by functionality to perform
split-apply-combine operations on data sets, for both aggregating
and transforming data
Make it easy to convert ragged,
differently-indexed data in other Python and NumPy data structures
into DataFrame objects
Intelligent label-based slicing, fancy
indexing, and subsetting of
large data sets
Intuitive merging and joining data
sets
Flexible reshaping and pivoting of
data sets
Hierarchical labeling of axes (possible to have multiple
labels per tick)
Robust IO tools for loading data from flat files
(CSV and delimited), Excel files, databases,
and saving/loading data from the ultrafast HDF5 format
Time series-specific functionality: date range
generation and frequency conversion, moving window statistics,
moving window linear regressions, date shifting and lagging, etc.

Where to get it
The source code is currently hosted on GitHub at:
https://github.com/pandas-dev/pandas
Binary installers for the latest released version are available at the Python
package index and on conda.
# conda
conda install pandas
# or PyPI
pip install pandas
Dependencies

NumPy: 1.13.3 or higher
python-dateutil: 2.5.0 or higher
pytz: 2015.4 or higher

See the full installation instructions
for recommended and optional dependencies.
Installation from sources
To install pandas from source you need Cython in addition to the normal
dependencies above. Cython can be installed from pypi:
pip install cython
In the pandas directory (same one where you found this file after
cloning the git repo), execute:
python setup.py install
or for installing in development mode:
python setup.py develop
Alternatively, you can use pip if you want all the dependencies pulled
in automatically (the -e option is for installing it in development
mode):
pip install -e .
See the full instructions for installing from source.
License
BSD 3
Documentation
The official documentation is hosted on PyData.org: https://pandas.pydata.org/pandas-docs/stable
Background
Work on pandas started at AQR (a quantitative hedge fund) in 2008 and
has been under active development since then.
Getting Help
For usage questions, the best place to go to is StackOverflow.
Further, general questions and discussions can also take place on the pydata mailing list.
Discussion and Development
Most development discussion is taking place on github in this repo. Further, the pandas-dev mailing list can also be used for specialized discussions or design issues, and a Gitter channel is available for quick development related questions.
Contributing to pandas 
All contributions, bug reports, bug fixes, documentation improvements, enhancements and ideas are welcome.
A detailed overview on how to contribute can be found in the contributing guide. There is also an overview on GitHub.
If you are simply looking to start working with the pandas codebase, navigate to the GitHub ""issues"" tab and start looking through interesting issues. There are a number of issues listed under Docs and good first issue where you could start out.
You can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to subscribe to pandas on CodeTriage.
Or maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking ‘this can be improved’...you can do something about it!
Feel free to ask questions on the mailing list or on Gitter.
",19467
KenanSulayman/heartbeat,None,"GCM R 20/0c/400 L 20/0c/400
DL8V8IJ03h7Gakgzy9MWgMsFIR8bBuoGMZkr2wAwcqwr40UYBHZ0ABxhdzE=W+oOj4PAtrfX7QB2xw2rB2spuZiF6OasThsTcfcpsv+7655zwM+lxR0yohDNzgjpzmEgjcJ8njLL4f0p...
iod1vs2qQInUGYgNBspJuTwswUm6LC7JGA+BjmTJxgoJlmog5iL+Izmq15s=lWq2Q1IZmzx4A25qIkdic87q4vw5hiVRRcLJx/XN7H5LzTKPk2Ho7IrggTQcN5nQ/8u/n6n1ykQk+Sub...
",13
algolia/places,JavaScript,"
    
Algolia Places provides a fast, distributed and easy way to use an address search autocomplete JavaScript library on your website.
See the website for more information.
Read the blog post introducing Algolia Places.
Fill the Google form to report any irrelevant results.
Demo
Watch more examples on the website.

Getting started
To use Algolia Places, all you need is an <input> and some JavaScript code that will load
and use the places.js library.
CDN <script>
Our JavaScript library is available on the jsDelivr CDN and also on  cdnjs.
<script src=""https://cdn.jsdelivr.net/npm/places.js@1.16.4""></script>
 is the latest version.
Here's a small example using it:
<input type=""search"" id=""address-input"" placeholder=""Where are we going?"" />

<script>
  var placesAutocomplete = places({
    appId: <YOUR_PLACES_APP_ID>,
    apiKey: <YOUR_PLACES_API_KEY>,
    container: document.querySelector('#address-input')
  });
</script>
Using npm
Algolia Places is also available on npm.
Install the module:
npm install places.js --save
Put an <input> in your html page:
<input type=""search"" id=""address-input"" placeholder=""Where are we going?"" />
Initialize the places.js library:
var places = require('places.js');
var placesAutocomplete = places({
  appId: <YOUR_PLACES_APP_ID>,
  apiKey: <YOUR_PLACES_API_KEY>,
  container: document.querySelector('#address-input')
});
Full documentation is available on the Algolia Places website.
Contributing
Wanna contribute? Awesome, please read the contributing guide.
",4722
duck8823/duci,Go,"duci
      
duci [zushi] (Docker Under Continuous Integration) is a simple ci server.
DSL is Unnecessary For CI
Let's define the task in the task runner.
Let's define the necessary infrastructure for the task in the Dockerfile.
duci just only execute the task in docker container.
Features

Execute the task in Docker container
Execute the task triggered by GitHub pull request comment or push
Execute tasks asynchronously
Create GitHub commit status
Store and Show logs

How to use
Target Repository
The target repository must have Dockerfile in repository root or .duci/Dockerfile.
If there is .duci/Dockerfile, duci read it preferentially.
In Dockerfile, I suggest to use ENTRYPOINT.
e.g.
ENTRYPOINT [""mvn""]
CMD [""compile""]
ENTRYPOINT [""fastlane""]
CMD [""build""]
When push to github, duci execute mvn compile / fastlane build.
And when comment ci test on github pull request, execute mvn test / fastlane test.
Using host environment variables
If exists ARG instruction in Dockerfile, override value from host environment variable.
ARG FOO=default
ARG BAR
and you can use as envrionment variable in command.
ARG FOO=default
ENV FOO=$FOO
Runtime configuration
volumes
You can use volumes options for external dependency, cache and etc.
Set configurations in .duci/config.yml
volumes:
  - '/path/to/host/dir:/path/to/container/dir'
environment variable
You can set environment variables in docker container.
Add the following to .duci/config.yml
environments:
  - ENVIRONMENT_VAIRABLE=value
Server Settings
Installation
# binary will be $(go env GOPATH)/bin/duci
curl -sfL https://raw.githubusercontent.com/duck8823/duci/master/install.sh | sh -s -- -b $(go env GOPATH)/bin
duci version
You can also install a specific version. (replace vX.Y.Z with the specific version from the releases page):
# binary will be $(go env GOPATH)/bin/duci
curl -sfL https://raw.githubusercontent.com/duck8823/duci/master/install.sh | sh -s -- -b $(go env GOPATH)/bin vX.Y.Z
duci version
Setting SSH (optional)
If target repository is private, You can use SSH key to clone repository from github.com.
Please set the public key of the pair at https://github.com/settings/keys.
Add Webhooks to Your GitHub repository
duci start to listen webhook with port 8080 (default) and endpoint /.
In GitHub target repository settings (https://github.com/<owner>/<repository>/settings/hooks),
Add endpoint of duci to Payload URL and application/json to Content type respectively.
Run Server
$ duci server
Server Configuration file
You can specify configuration file with -c option.
The configuration file must be yaml format.
Possible values ​​are as follows.
server:
  workdir: '/path/to/tmp/duci'
  port: 8080
  database_path: '$HOME/.duci/db'
github:
  # (optional) You can use SSH key to clone. ex. '${HOME}/.ssh/id_rsa'
  ssh_key_path: ''
  # For create commit status. You can also use environment variable
  api_token: ${GITHUB_API_TOKEN}
job:
  timeout: 600
  concurrency: 4 # default is number of cpu
You can check the configuration values.
$ duci config
Using Docker
You can use Docker to run server.
$ docker run -p 8080:8080 \
             -e GITHUB_API_TOKEN=<your toekn> \
             -v /var/run/docker.sock:/var/run/docker.sock \
             duck8823/duci

When you want to clone with SSH in container,
$ docker run -p 8080:8080 \
             -e GITHUB_API_TOKEN=<your toekn> \
             -e SSH_KEY_PATH=/root/.ssh/id_rsa \
             -v ~/.ssh:/root/.ssh:ro \ 
             -v /var/run/docker.sock:/var/run/docker.sock \
             duck8823/duci

Run with docker-compose
With docker-compose, you can also start ui and reverse proxy together.
$ git clone https://github.com/duck8823/duci.git
$ cd duci
$ docker-compose up -d
If you start up on another host, set your host name (default: localhost) to environment variable DUCI_HOST.
Read job log
GitHub send payload as webhook including X-GitHub-Delivery header.
You can read job log with the X-GitHub-Delivery value formatted UUID.
$ curl -XGET http://localhost:8080/logs/{X-GitHub-Delivery}
The endpoint returns NDJSON (Newline Delimited JSON) formatted log.
{""time"":""2018-09-21T22:19:42.572879+09:00"",""message"":""Step 1/10 : FROM golang:1.11-alpine""}
{""time"":""2018-09-21T22:19:42.573093+09:00"",""message"":""\n""}
{""time"":""2018-09-21T22:19:42.573494+09:00"",""message"":"" ---\u003e 233ed4ed14bf\n""}
{""time"":""2018-09-21T22:19:42.573616+09:00"",""message"":""Step 2/10 : MAINTAINER shunsuke maeda \u003cduck8823@gmail.com\u003e""}
{""time"":""2018-09-21T22:19:42.573734+09:00"",""message"":""\n""}
...

Health Check
This server has an health check API endpoint (/health) that returns the health of the service. The endpoint returns 200 status code if all green.
$ curl -XGET -I http://localhost:8080/health
HTTP/1.1 200 OK
Date: Wed, 31 Oct 2018 20:33:42 GMT
Content-Length: 0

The check items are as follows

Whether the Docker daemon is running or not

You can also check with health sub-command.
$ duci health
INFO[14/Jan/2019 07:17:38.864] ok.

License
MIT License
Copyright (c) 2018 Shunsuke Maeda
See LICENSE file
",34
mitchellkrogza/Phishing.Database,Shell,"
Phishing Domain Database
A Testing Repository for Phishing Domains, Web Sites and Threats.
Below are results of Domains that have been tested to be Active, Inactive or Invalid.
These Lists update every few minutes.

Version: V0.1.10158
ACTIVE Phishing Domains (Tested): 136982 (94 %)
INACTIVE Phishing Domains (Tested): 8153 (6 %)
INVALID Phishing Domains (Tested): 387 (0 %)

Total Phishing URL's Captured: 147709 ❗️ Large File

ACTIVE Phishing Domains (Current Tests): 136982 (94 %)
INACTIVE Phishing Domains (Current Tests): 8153 (6 %)
INVALID Phishing Domains (Current Tests): 387 (0 %)

Purpose of this repo?
This is just one of a number of extensive projects dealing with testing the status of harmful domain names and web sites. We test sources of Phishing attacks to keep track of how many of the domain names used in Phishing attacks are still active and functioning. We sort all domains from all sources into one list, removing any duplicates so that we have a clean list of domains to work with.

How do you test?
We make use of the awesome PyFunceble Testing Suite written by Nissar Chababy. Over 2 years in development this testing tool really provides us with a reliable source of active and inactive domains and through regular testing even domains which are inactive and may become active again are automatically moved back to the active list.

Contributing
If you have a source list of phishing domains why not contribute them to this project for testing? Simply send a PR adding your input source details and we will add the source.


Contributors

Mitchell Krog
Nissar Chababy


MIT License
Copyright (c) 2018 Mitchell Krog
https://github.com/mitchellkrogza
Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
",33
dustinpfister/blog_posts,JavaScript,"blog_posts
These are the markdown files that are used to build by website here at github pages at https://dustinpfister.github.io. The build process involves the use of the node.js powered static site generator called hexo.
1 - Setup
After setting up a new instance of hexo by calling hexo init, clone this repo down, then delete the _posts folder in the hexo folder and create a new symbloic link for _posts in place of it pointing to the _posts folder in this repo. Some of the posts do use custom hexo tags that can be found in my hexo_sitesource repo.
1.1 - Making a symbolic link in Windows 10
In windows I use the mklink command to make a symbolic link to the _posts folder in this repo.
mklink /d C:\path\to\hexo-project-folder\source\_posts C:\path\to\blog_posts\_posts

cmd.exe will need to be started with admin privileges, and the /d option will need to be used as this is a like to a directory.
1. 2 - Making a symbolic link in Linux
In linux systems I would use the ln command to make a soft link
$ ln -s /home/github/hexo-project/source/_posts /home/github/blog_posts/_posts

Other repos of interest that involve blog_posts

dustinpfister.github.io - the current deployment of my github pages site that is built with the markdown found here.
hexo_sitesource - This repo contains the instance of hexo that I am using, including the theme.

",2
Lombiq/HipChat-to-Microsoft-Teams-Migration-Utility,C#,"Lombiq HipChat to Microsoft Teams Migration Utility Readme
Utility to migrate Atlassian HipChat content to Microsoft Teams. You can use this instead of waiting for official support. We're testing this utility at Lombiq Technologies (a web development company working with Microsoft technologies) with a 4GB+ HipChat export package containing more than 200k messages (in rooms; a lot more with private chats). The tool is intended for very technical users and developers.
Currently the app can import rooms, messages and attachments from a HipChat export file into configured Teams channels or existing channels (in configured teams). Messages will appear under the user's name doing the import, but messages will include the name of the original user too. To overcome the rate limit of the Teams API multiple HipChat messages can be imported into a single Teams message. See the issue tracker on GitHub for missing features and bugs. HipChat /quote and /code commands are converted into their Teams equivalents.
Note that this being a utility with just temporary use simplicity of implementation was favored against long-term maintainability. Note that the guide assumes you're using Windows but everything should work equally well under any OS supported by .NET Core. However, released executables are available only for Windows 64b currently.
Running the app
If you're a .NET developer then grab the latest source and run the app from the source (you'll need at least VS 2017 and 2.2 of the .NET Core SDK). Otherwise download the latest release from GitHub and run the exe file in the zip. Be sure to check the full usage guide below.
Usage
Keep in mind that you need to be both a HipChat and a Teams admin in your company for this to work.

As a HipChat admin export your HipChat data from under you HipChat URL (e.g https://lombiq.hipchat.com), Group admin, Data export. Select to export every kind of data and the whole history. Use a password without any special characters or spaces! Save the file under a path without any special characters.
Download the OpenSSL binaries if your system doesn't have them already. Recommended is the 1.0.2a (not any other version!) x64 zip from here (direct link to file). Unzip it to a folder whose path doesn't contain any special characters or spaces, run openssl.exe and decrypt the export file with the following command: aes-256-cbc -d -in C:\path\to\export\file.tar.gz.aes -out C:\export.tar.gz -pass pass:password.
Use your favorite ZIP utility (7-Zip recommended) to extract the gz and tar so finally you'll end up with an unencrypted, unzipped export folder (this will contain folders like rooms and users and some further files like rooms.json and users.json). If you get a ""The parameter is incorrect"" error in 7-Zip then first unzip the gz archive to a folder, then unpack the tar file as a second step. While this decrypt-unzip could be automated it's a yak shaving of epic proportions (but feel free to contribute it if you wish!) but you'll have to do it once any way.
Go to the Graph Explorer and log in. Note that the user account you're logging in there will be visible as the author of the messages you import, so it's recommended to use a special user account for this (like ""HipChat Import""). Confirm the required permissions. Then acquire the necessary permissions as following:

Click on ""show more samples"", turn ""Microsoft Teams"" and ""Microsoft Teams (beta)"" on.
Try to run e.g. the Microsoft Teams / create channel operation. You'll get an error that you don't have the necessary permissions. Click on ""modify your permission"".

Select the following permissions: Group.ReadWrite.All, User.Read.All. You'll need to log in again.


Once the permissions are OK then run an API request (it can be any of the samples, even just /me. Copy the bearer token (just the token, without the ""Bearer"" text) used by the request into the AppSettings.json configuration file under the AuthorizationToken config. You can e.g. use Chrome DevTools (open with F12 in Chrome) to see this token in the Request headers:

Specify the rest of the configuration as well:

ExportFolderPath: The file system path to the folder where you unzipped the HipChat export package.
NumberOfHipChatMessagesToImportIntoTeamsMessage: You may be able to guess :). If it's greater than 1 then multiple HipChat messages will be imported into a single Teams message. You can use this to overcome Graph API throttling limitations. It seems that a safe general maximum is about 25 (suitable for rooms with many HTML bodied notifications too), with more HipChat messages the request will be too large (depends on how long messages usually are, that can vary a lot); go with lower if you want to be sure. If the value is too high you'll get ""Importing x HipChat messages into a Teams message resulted in a message too large."" errors. A good strategy is try the value 50 (or even 100), then lower it if you get a lot of errors to find out what suits your chat history best. The Teams API rate limit is at about 1800 requests a day, so you'll only be able to import 1800 messages if you don't use this option before throttling kicks in. Importing into multiple teams (see the HipChatRoomsToTeams option) may increase this overall limit.
ShortenLongMessagesToCharacterCount: HipChat messages can be longer than allowed by Teams, so importing some longer HipChat messages can fail. If this configuration is 0 then in such a case importing will fail and you'll need to manually shorten the message in the HipChat export package; otherwise the message will automatically be shortened to the given character count.
UploadAttachments: If set to true HipChat file attachments will be uploaded to the respective Teams channels, linked from (or in case of images, embedded into) their corresponding messages. Set to false if you don't want attachments to be uploaded.
HipChatRoomsToTeams: Map HipChat room names to team names in Teams, so their corresponding channels will be created there. This way you can configure under which team to create channels. Since channels can't be moved across teams you need this if you don't want all the channels under a single team. Configure multiple room name-team name pairs like this: ""HipChatRoomsToTeams"": { ""$Default"": ""Team 1"", ""$Archived default"":  ""Archive"", ""Room 1"": ""Team 2"" }. If no team is configured for a room then the one under $Default will be used; similarly the team under $Archived default will be used for archived rooms (if this config is missing then the team under $Default will be used for archived rooms too). If a given team is not found then it'll be created (for security reasons as a Private team, you can change this later).
HipChatRoomsToChannels: Map HipChat room names to channel names in Teams. This way you can import the content of HipChat rooms into existing Teams channels, like utilizing the default General channel. Uses the same syntax as HipChatRoomsToTeams. If there's no mapping for a given room then a channel will be created for it. Note that this works together with HipChatRoomsToTeams: so e.g. if you want to import multiple rooms into multiple teams' General channels, then first configure the teams for the rooms with HipChatRoomsToTeams, then configure the room (""General"" in this example) with HipChatRoomsToChannels.


Run the app and wait for the import to complete. In the console you'll see status and possibly error messages. Since not all errors can be resolved automatically, and the bearer token can expire too. So don't let the app run too long (about half an hour) without checking its status.

Notable features missing and bugs
See the issue tracker on GitHub.
Some implementation notes

Here's some information on the HipChat export's schema.
Some inspiration is taken from https://github.com/microsoftgraph/csharp-teams-sample-graph.

Contribution and Feedback
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/hipchat-to-microsoft-teams-migration-utility/ (Mercurial repository)
https://github.com/Lombiq/HipChat-to-Microsoft-Teams-Migration-Utility (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub. Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",7
HillTopTRPG/quoridorn-vue-cli-3,Vue,"◆このリポジトリ
Quoridorn Vue CLI 3 版
◆元の資材
Quoridorn Vue CLI 2 版
https://github.com/HillTopTRPG/quoridorn
quoridorn-vue-cli-3
Project setup
npm install

Compiles and hot-reloads for development
npm run serve

Compiles and minifies for production
npm run build

Run your tests
npm run test

Lints and fixes files
npm run lint

Customize configuration
See Configuration Reference.
",4
Lombiq/HipChat-to-Microsoft-Teams-Migration-Utility,C#,"Lombiq HipChat to Microsoft Teams Migration Utility Readme
Utility to migrate Atlassian HipChat content to Microsoft Teams. You can use this instead of waiting for official support. We're testing this utility at Lombiq Technologies (a web development company working with Microsoft technologies) with a 4GB+ HipChat export package containing more than 200k messages (in rooms; a lot more with private chats). The tool is intended for very technical users and developers.
Currently the app can import rooms, messages and attachments from a HipChat export file into configured Teams channels or existing channels (in configured teams). Messages will appear under the user's name doing the import, but messages will include the name of the original user too. To overcome the rate limit of the Teams API multiple HipChat messages can be imported into a single Teams message. See the issue tracker on GitHub for missing features and bugs. HipChat /quote and /code commands are converted into their Teams equivalents.
Note that this being a utility with just temporary use simplicity of implementation was favored against long-term maintainability. Note that the guide assumes you're using Windows but everything should work equally well under any OS supported by .NET Core. However, released executables are available only for Windows 64b currently.
Running the app
If you're a .NET developer then grab the latest source and run the app from the source (you'll need at least VS 2017 and 2.2 of the .NET Core SDK). Otherwise download the latest release from GitHub and run the exe file in the zip. Be sure to check the full usage guide below.
Usage
Keep in mind that you need to be both a HipChat and a Teams admin in your company for this to work.

As a HipChat admin export your HipChat data from under you HipChat URL (e.g https://lombiq.hipchat.com), Group admin, Data export. Select to export every kind of data and the whole history. Use a password without any special characters or spaces! Save the file under a path without any special characters.
Download the OpenSSL binaries if your system doesn't have them already. Recommended is the 1.0.2a (not any other version!) x64 zip from here (direct link to file). Unzip it to a folder whose path doesn't contain any special characters or spaces, run openssl.exe and decrypt the export file with the following command: aes-256-cbc -d -in C:\path\to\export\file.tar.gz.aes -out C:\export.tar.gz -pass pass:password.
Use your favorite ZIP utility (7-Zip recommended) to extract the gz and tar so finally you'll end up with an unencrypted, unzipped export folder (this will contain folders like rooms and users and some further files like rooms.json and users.json). If you get a ""The parameter is incorrect"" error in 7-Zip then first unzip the gz archive to a folder, then unpack the tar file as a second step. While this decrypt-unzip could be automated it's a yak shaving of epic proportions (but feel free to contribute it if you wish!) but you'll have to do it once any way.
Go to the Graph Explorer and log in. Note that the user account you're logging in there will be visible as the author of the messages you import, so it's recommended to use a special user account for this (like ""HipChat Import""). Confirm the required permissions. Then acquire the necessary permissions as following:

Click on ""show more samples"", turn ""Microsoft Teams"" and ""Microsoft Teams (beta)"" on.
Try to run e.g. the Microsoft Teams / create channel operation. You'll get an error that you don't have the necessary permissions. Click on ""modify your permission"".

Select the following permissions: Group.ReadWrite.All, User.Read.All. You'll need to log in again.


Once the permissions are OK then run an API request (it can be any of the samples, even just /me. Copy the bearer token (just the token, without the ""Bearer"" text) used by the request into the AppSettings.json configuration file under the AuthorizationToken config. You can e.g. use Chrome DevTools (open with F12 in Chrome) to see this token in the Request headers:

Specify the rest of the configuration as well:

ExportFolderPath: The file system path to the folder where you unzipped the HipChat export package.
NumberOfHipChatMessagesToImportIntoTeamsMessage: You may be able to guess :). If it's greater than 1 then multiple HipChat messages will be imported into a single Teams message. You can use this to overcome Graph API throttling limitations. It seems that a safe general maximum is about 25 (suitable for rooms with many HTML bodied notifications too), with more HipChat messages the request will be too large (depends on how long messages usually are, that can vary a lot); go with lower if you want to be sure. If the value is too high you'll get ""Importing x HipChat messages into a Teams message resulted in a message too large."" errors. A good strategy is try the value 50 (or even 100), then lower it if you get a lot of errors to find out what suits your chat history best. The Teams API rate limit is at about 1800 requests a day, so you'll only be able to import 1800 messages if you don't use this option before throttling kicks in. Importing into multiple teams (see the HipChatRoomsToTeams option) may increase this overall limit.
ShortenLongMessagesToCharacterCount: HipChat messages can be longer than allowed by Teams, so importing some longer HipChat messages can fail. If this configuration is 0 then in such a case importing will fail and you'll need to manually shorten the message in the HipChat export package; otherwise the message will automatically be shortened to the given character count.
UploadAttachments: If set to true HipChat file attachments will be uploaded to the respective Teams channels, linked from (or in case of images, embedded into) their corresponding messages. Set to false if you don't want attachments to be uploaded.
HipChatRoomsToTeams: Map HipChat room names to team names in Teams, so their corresponding channels will be created there. This way you can configure under which team to create channels. Since channels can't be moved across teams you need this if you don't want all the channels under a single team. Configure multiple room name-team name pairs like this: ""HipChatRoomsToTeams"": { ""$Default"": ""Team 1"", ""$Archived default"":  ""Archive"", ""Room 1"": ""Team 2"" }. If no team is configured for a room then the one under $Default will be used; similarly the team under $Archived default will be used for archived rooms (if this config is missing then the team under $Default will be used for archived rooms too). If a given team is not found then it'll be created (for security reasons as a Private team, you can change this later).
HipChatRoomsToChannels: Map HipChat room names to channel names in Teams. This way you can import the content of HipChat rooms into existing Teams channels, like utilizing the default General channel. Uses the same syntax as HipChatRoomsToTeams. If there's no mapping for a given room then a channel will be created for it. Note that this works together with HipChatRoomsToTeams: so e.g. if you want to import multiple rooms into multiple teams' General channels, then first configure the teams for the rooms with HipChatRoomsToTeams, then configure the room (""General"" in this example) with HipChatRoomsToChannels.


Run the app and wait for the import to complete. In the console you'll see status and possibly error messages. Since not all errors can be resolved automatically, and the bearer token can expire too. So don't let the app run too long (about half an hour) without checking its status.

Notable features missing and bugs
See the issue tracker on GitHub.
Some implementation notes

Here's some information on the HipChat export's schema.
Some inspiration is taken from https://github.com/microsoftgraph/csharp-teams-sample-graph.

Contribution and Feedback
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/hipchat-to-microsoft-teams-migration-utility/ (Mercurial repository)
https://github.com/Lombiq/HipChat-to-Microsoft-Teams-Migration-Utility (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub. Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",7
HillTopTRPG/quoridorn-vue-cli-3,Vue,"◆このリポジトリ
Quoridorn Vue CLI 3 版
◆元の資材
Quoridorn Vue CLI 2 版
https://github.com/HillTopTRPG/quoridorn
quoridorn-vue-cli-3
Project setup
npm install

Compiles and hot-reloads for development
npm run serve

Compiles and minifies for production
npm run build

Run your tests
npm run test

Lints and fixes files
npm run lint

Customize configuration
See Configuration Reference.
",4
toehmler/bt-seg,Python,"Automatic Brain Tumor Segmentation
Table Of Contents:

High Grade Gliomas
MRI Scans
Why do we need Automatic Segmentation?
Dataset
Convolutional Neural Networks
Our Model

High Grade Glioma Brain Tumors
Glioma tumors are the most common type of brain tumor, comprising approximately 33% of all brain tumors. These tumors originate in
different types of glial cells, which surround and provide support for neurons in the brain. Gliomas are classified from Grade I to
Grade IV by their various rates of growth. While Grade I gliomas are usually removable surgically with a promising survival rate,
high grade gliomas (glioblastomas) are one of the most deadly cancers, with only a 5% survival rate after 5 years. The current standard of
treatment for these tumors generally consists of some combination of surgery, radiation and chemotherapy. The early detection and diagnosis
of these tumors is crucial to long-term survival rate. This is generally done through brain imaging, including MRI, CT and PET scans.
Of these, the most commonly used imaging technique is the MRI scan, due to its ability to non-invasively provide accurate characterizations
of different tissue types.
MRI Scans
Magnetic resonance imaging (MRI) scans work by applying a strong magnetic field to align protons in the brain, before using radiofrequency
pulses to disturb the alignment. When the radiofrequency field is turned off, MRI's can measure the energy emission as protons return to
alignment with the magnetic field. MRI scans are particularly effective at imaging soft tissue, and organs like the brain and the heart.
MRI's visual the brain through taking a series of two-dimensional 'slices' (at 1mm increments) in one of three planes:
coronal, sagittal and axial. In these slices, each pixel represents a 1mm^3 voxel. For the purposes of our model,
we used slices in the axial plane because it is easier to visualize/IN BRATS DAtASET and the resolution is the HIGHEST??
Why do we need Automatic Segmentation?
Given the huge number of slices generated by these MRI scans (we have 620 for each patient), it is incredibly laborious for a radiologist to go
label the voxels (240x240) in each slice (155 slices). This is time radiologists can use to focus on other tasks. Thus, an effective automatic segmentation method
could provide a much more efficient alternative, saving the radiologist and the patient valuable time. Indeed, one state-of-the-art algorithm
published in 2017 can provide a segmentation between 25 seconds and 3 minutes (Havaei et al.), which is manually inconceivable.
Further, manually applying these labels require high level of expertise and are prone to human error. The use of a highly-trained convolutional
neural network might be able to pick up small contrasts and edges that are hard to detect with the human eye.
PICTURE OF DIFFERENT SLICES?
Dataset
All MRI brain scans were provided by the BRATS 2015 challenge database (https://www.smir.ch/BRATS/Start2015).
This dataset consists of 246 high-grade glioma cases and 54 low-grade glioma cases. Each scan consists of 155 slices in four different
modalities: T1, T1 with contrast, T2 and FLAIR (each of these uses a different pulse sequence to create different pixel contrasts in
a MR brain image). Thus, there are 620 MR images for each patient, and 186,000 images overall. Further, each patient has a fifth image
providing the 'ground truth' labels for each pixel. In this dataset, the labels are as follows: '0' is 'non-tumor;' '1' is 'necrosis';
'2' is 'edema'; '3' is 'non-enhancing tumor'; '4' is 'enhancing tumor.' There is a label for each pixel in each 240x240 voxel slice, generating
8,928,000 labels for each patient, and 2,678,400,000 labels in the dataset overall (300 patients). These ground truth segmentation labels are
manually provided by radiologists.
Convolutional Neural Networks
Convolutional Neural Networks (CNN) are deep learning algorithms that are commonly used for image processing, object detection and classification
tasks. Neural networks can 'learn' through the fine tuning of large numbers of weight and biases in the network to adapt to a specific task.
These networks are modelled after the structure of the human brain, as they consist of a series of complex layers of connections between
artificial 'neurons,' or perceptrons. The first CNN, entitled 'AlexNet,' was created in 2012 by Geoffrey Hinton and his colleagues at the
University of Toronto. Since then, they have been used extensively, but the application of CNN's to medical images is a very new development.
CNN's are a well-suited tool for our task of the automatic segmentation of tumors.
References
",2
commercialhaskell/all-cabal-metadata,Shell,"all-cabal-metadata
Current metadata for all cabal files on Hackage. Generated using the
all-cabal-tool
package.
",7
tengge1/ShadowEditor,JavaScript,"Shadow Editor
Language: 中文 / 繁體中文 / English / 日本語 / Le français / русский

名称：Shadow Editor
版本：v0.2.1(开发中)
简介：基于three.js的场景编辑器。

v0.2.1即将更新

更新示例程序，主要是编辑器二维菜单演示。码云 GitHub

v0.2.0更新

发布日期：2019年5月3日
更新日志：


默认不再加载ammo.js。只有场景中存在刚体或柔软体时，才在播放时自动加载ammo.js，提升编辑器启动速度和非物理场景运行速度。
新增二维菜单：按钮、标签、面板、水平线、条形图、时间、竖直线、日期、时间圆盘、键值标签、表单、仪表、柱状图、折线图、侧边栏、柱状图2、散点图、饼状图、弦图、力导向图、树状图、集群图、包图、分区图。可拖动、保存、载入，可在播放器中查看。（开发中，仅供测试）

项目截图

温馨小窝。(仅供参考)



三维地球。Gitee GitHub


点击此处查看更多截图。


源码
GitHub
码云
文档
GitHub
码云


演示
GitHub
码云
数据库及资源
百度网盘20190116
提取码：n8je


主要功能

基于three.js/WebGL的3D场景在线编辑器，服务端使用MongoDB保存动画、音频、类别、角色、贴图、材质、网格模型、粒子、预设体、场景数据。
内置几何体：平面、正方体、圆、圆柱体、球体、二十面体、轮胎、纽结、茶壶、酒杯、精灵、文本；线段、CatmullRom曲线、二次贝塞尔曲线、三次贝塞尔曲线、椭圆曲线。
内置光源：环境光、平行光、点光源、聚光灯、半球光、矩形光。
支持多种不同3D格式模型和动画导入。支持3ds、3mf、amf、assimp(anim)、awd、babylon、binary、bvh(anim)、collada、ctm、draco、fbx(anim)、gcode、gltf(anim)、js(anim)、json(anim)、kmz、lmesh(anim)、md2、mmd(anim)、nrrd、obj、pcd、pdb、ply、prwm、sea3d(anim)、stl、vrm、vrml、vtk、x 31种3D文件格式，带anim的表示支持动画。多种3D文件同时支持json和二进制格式。mmd文件同时支持pmd和pmx格式，支持vmd格式的模型和相机动画。它也是唯一支持lmesh(lolking网站lol模型)的编辑器。
内置材质：线条材质、虚线材质、基本材质、深度材质、法向量材质、兰伯特材质、冯氏材质、点云材质、标准材质、物理材质、精灵材质、着色器材质、原始着色器材质。
支持纹理：颜色纹理、透明纹理、凹凸纹理、法线纹理、位移纹理、镜面纹理、环境纹理、光照纹理、遮挡纹理、自发光纹理。
支持贴图：图片、立方体贴图、视频贴图。
内置组件：背景音乐、粒子发射器、天空、火焰、水、烟、布组件。
可视化修改场景、相机等物体属性，提供40多种不同修改面板。
在线编辑js脚本、着色器程序，带智能提示。
自带播放器，实时演示场景动态效果，支持全屏和新窗口播放，可以直接嵌入项目iframe中。
支持补间动画、骨骼动画、粒子动画、mmd动画、lmesh动画（lolking网站lol模型）。
支持场景、模型、贴图、材质、音频、动画、粒子、预设体、角色资源管理，支持自定义分类，根据汉字和拼音快速搜索。其中，粒子、预设体、角色资源管理暂未实现相应功能。
支持第一视角控制器、飞行控制器、轨道控制器、指针锁定控制器、轨迹球控制器5种控制器。
支持点阵化特效、颜色偏移特效、残影特效、背景虚化、快速近似抗锯齿(FXAA)、毛刺特效、半色调特效、全屏抗锯齿(SSAA)、像素特效、可扩展环境光遮挡(SAO)、多重采样抗锯齿(SMAA)、屏幕空间环境光遮蔽(SSAO)、时间抗锯齿(TAA)。
提供历史记录和日志功能，支持撤销、重做。
支持导出gltf、obj、ply、stl模型。
支持bullet物理引擎。正方体、圆形、圆柱体、二十面体、酒杯、平面、球体、茶壶、轮胎、纽结和加载的模型都支持刚体组件。支持可视化设置碰撞体形状（正方体、球体）、质量和惯性。
具有平移、旋转、缩放、在物体表面绘制点、线、贴花的工具，实时统计场景种物体、顶点、三角形数量。
支持场景一键导出功能。
中英文双语支持。
支持色调旋转(hue-rotate)、饱和度、亮度、高斯模糊(blur)、对比度、灰度、颜色反转(invert)、复古(sepia)滤镜。
支持版本控制。

使用指南
该项目仅支持Windows系统，电脑上需要安装.Net Framework 4.5。
推荐使用最新版谷歌浏览器，不保证兼容其他浏览器。

安装NodeJs，在最外层目录，执行以下命令。

npm install
npm run build

下载MongoDB，安装并启动MongoDB服务。MongoDB服务的默认端口为27017。

mongod --dbpath=D:\mongodb\db --logpath=D:\mongodb\log\mongoDB.log --install --serviceName MongoDB
net start MongoDB

编辑文件ShadowEditor.Web/Web.config，将27017修改为你电脑上MongoDB服务的端口。

<add key=""mongo_connection"" value=""mongodb://127.0.0.1:27017"" />


使用Visual Studio 2017打开项目，生成ShadowEditor.Web项目。


将ShadowEditor.Web部署在iis上即可在浏览器中访问。


为了保存各种类型文件能正常下载，需要在iis上添加以下两个MIME类型。





文件扩展名
MIME类型
说明




.*
application/octet-stream
各种格式后缀文件


.
application/octet-stream
无后缀文件




编译文档，请安装gitbook。

npm install -g gitbook-cli
然后切换到docs-dev目录，安装gitbook插件。
gitbook install
然后切换到上级目录，执行以下命令生成文档。
npm run build-docs
常见问题

上传模型时为什么都是上传失败？

需要把模型贴图等资源压缩成一个zip包，而且入口文件不能嵌套文件夹。服务端会解压上传的zip包放到~/Upload/Model文件下，并在MongoDB _Mesh表里添加一条数据。

如何将多个模型组合在一起？

基本几何体都支持多层嵌套。可以添加一个组（在几何体菜单中），然后在场景树状图上，将多个模型拖动到组上。
更新日志
v0.2.0

发布日期：2019年5月3日
更新日志：


默认不再加载ammo.js。只有场景中存在刚体或柔软体时，才在播放时自动加载ammo.js，提升编辑器启动速度和非物理场景运行速度。
新增二维菜单：按钮、标签、面板、水平线、条形图、时间、竖直线、日期、时间圆盘、键值标签、表单、仪表、柱状图、折线图、侧边栏、柱状图2、散点图、饼状图、弦图、力导向图、树状图、集群图、包图、分区图。可拖动、保存、载入，可在播放器中查看。（开发中，仅供测试）

v0.1.9

发布日期：2019年4月20日
更新日志：


修复属性面板修改名称时，文字几何体文字不改变bug。
修复地图卡顿问题。
新增地图组件，动态切换谷歌地图、必应地图、天地图。
使用立体纹理为地球实现星空背景。
地图添加太阳特效。
地图保存载入。
在播放器中播放GIS场景。
新增三维GIS演示。Gitee GitHub
新增补间动画演示。Gitee GitHub
限制地轴与y轴的夹角在一定范围内。

v0.1.8

发布日期：2019年4月7日
更新日志：


设置面板放到选项菜单中。
新增色调旋转(hue-rotate)、饱和度、亮度、高斯模糊(blur)、对比度、灰度、颜色反转(invert)、复古(sepia)滤镜。
滤镜设置保存在场景配置中，并在编辑器和播放器解析。
创建GIS场景。(演示)
修复当模型最外层是Scene时，属性面板显示物体环境组件和各种后期处理组件的bug。
场景层次树，节点前面添加一个矩形，表示该节点类型。

v0.1.7

发布日期：2019年3月23日
更新日志：


修复物体改变后，场景树状图无法及时响应，无法记住树节点展开状态，树节点顺序错乱的bug。
物体名称超长自动显示省略号。鼠标移到节点上，显示完整名称。
新增全屏播放功能、新窗口播放功能。播放器和编辑器彻底解除耦合，播放器可独立运行。
可将编辑好的场景嵌入iframe运行。地址：/view.html?sceneID=sceneID。可使用新窗口播放功能获取该地址。
物体选中效果优化：等宽描边。
修复整理模型工具，复制模型时未复制文件夹中所有文件的bug。
所有设置改为存储在localStorage中，不再保存在场景配置中。
重构平行光帮助器、半球光帮助器、点光源帮助器、矩形光帮助器、聚光灯帮助器，采用事件驱动方式，不再对编辑器严重依赖。
整理文件夹结构。重写历史面板模块，彻底删除耦合度高的Outliner控件。
英文翻译优化。
新增场景一键导出功能，自动分析场景所需模型和资源，所有所需资源放在/temp/yyyyMMddHHmmss文件夹。
导出不带资源的编辑器功能。
重新发布了演示项目：Gitee GitHub
修复渲染器设置中，阴影、γ输入、γ输出、γ因子设置无效bug。
平面上点、线、喷涂工具一次只能绘制一个。
修复补间动画无法播放问题。

v0.1.6

发布日期：2019年3月10日
更新日志：


使用xtype.js，采用非侵入式开发方式重构UI框架。
主框架使用绝对定位重新布局。
底部面板支持最大化和还原。
底部面板显示资源统计信息。
搜集整理大量贴图和模型，并进行分类。
整理贴图工具。
整理模型工具。
整理缩略图工具。
添加点光源，默认不再添加圆球和光晕。
添加半球光，默认不再添加天空球。
添加矩形光，默认不再添加矩形白色屏幕。
由于原来的选中效果是使用后期处理实现的，产生了严重性能损耗和锯齿。所以用法线挤出和模板测试的方法重新实现选中效果。
重写后的选中效果不再产生锯齿，不默认开启快速抗锯齿(FXAA)功能，提高了性能。旧场景请在场景属性中取消勾选快速近似抗锯齿(FXAA)，并重新保存。
默认启用时不再加载任何后期处理(postprocessing)相关着色器和特效类库，提高加载速度。
修复创建脚本注释未汉化bug。
折叠底部面板功能。
重写场景树状图控件，支持折叠、拖动、选中。
点击场景选中模型时，场景树状图优先选中整个模型，而不是模型的一部分，而且会自动展开并滚动到所选模型。

v0.1.5

发布日期：2019年2月23日
更新日志：


线段、CatmullRom曲线、二次贝塞尔曲线、三次贝塞尔曲线、椭圆曲线可视化编辑、保存和载入。
修复时间轴上的动画无法拖动的bug。
修复无法在场景树状视图将物体拖动到组上的bug。
基本几何体都支持多层嵌套，可以正常保存载入。
修复视角控件尺寸计算bug。
修复视角控件可能被其他物体遮挡的bug。

v0.1.4

发布日期：2019年2月11日
更新日志：


新增一个指示方向的控件。
新增线段、CatmullRom曲线、二次贝塞尔曲线、三次贝塞尔曲线、椭圆曲线。(暂不支持保存)

v0.1.3

发布日期：2019年1月28日
更新日志：


多语言支持：支持中文和英文，支持语言动态切换。
新增曲线几何体。

v0.1.2

发布日期：2019年1月11日
更新日志：


场景新增版本控制。场景表仅保存最新场景，历史数据保存在 场景名称_history表中。
保存材质自动生成材质球缩略图。
保存载入服务端模型修改后的材质。
正方体、圆形、圆柱体、二十面体、酒杯、平面、球体、茶壶、轮胎、纽结、加载模型都支持刚体组件。支持可视化设置碰撞体形状（正方体、球体）、质量和惯性。
新增正方体和球体物理形状帮助器。

v0.1.1

发布日期：2018年12月30日
更新日志：


修复mmd动画和音频不同步问题。支持多个mmd模型与模型动画、相机动画同步。
新增点阵化特效、颜色偏移特效、残影特效、背景虚化、快速近似抗锯齿(FXAA)、毛刺特效、半色调特效、全屏抗锯齿(SSAA)、像素特效、可扩展环境光遮挡(SAO)、多重采样抗锯齿(SMAA)、屏幕空间环境光遮蔽(SSAO)、时间抗锯齿(TAA)。
新增粒子、预设体、角色面板。（暂未实现具体功能）

v0.1.0

发布日期：2018年12月15日
更新日志：


重新梳理模型导入功能。目前支持3ds、3mf、amf、assimp(anim)、awd、babylon、bvh(anim)、collada、ctm、draco、fbx(anim)、gcode、gltf(anim)、js(anim)、json(anim)、kmz、lmesh(anim)、md2、mmd(anim)、nrrd、obj、pcd、pdb、ply、prwm、sea3d(anim)、stl、vrm、vrml、vtk、x 31种3D文件格式，带anim的表示支持动画。多种3D文件同时支持json和二进制格式。mmd文件同时支持pmd和pmx格式，支持vmd格式的模型和相机动画。它也是唯一支持lmesh(lolking网站lol模型)的编辑器。
播放器新增第一视角控制器、飞行控制器、轨道控制器、指针锁定控制器、轨迹球控制器5种控制器，在相机面板设置。
场景面板，编辑场景分类，根据类别、名称、全拼、拼音首字母实时过滤。
模型面板，编辑模型分类，根据类别、名称、全拼、拼音首字母实时过滤。
贴图面板，编辑贴图分类，根据类别、名称、全拼、拼音首字母实时过滤。
材质面板，编辑材质分类，根据类别、名称、全拼、拼音首字母实时过滤。
音频面板，编辑音频分类，根据类别、名称、全拼、拼音首字母实时过滤。
材质组件，新增保存材质和从材质面板选择材质功能。
纹理、透明纹理、凹凸纹理、法线纹理、置换纹理、粗糙纹理、金属纹理、环境纹理、光照纹理、遮挡纹理、发光纹理从贴图面板选择贴图功能。
删除上个版本场景窗口、模型窗口、贴图窗口、音频窗口。

v0.0.9

发布日期：2018年11月25日
更新日志：


新增布料带动画。
gltf模型导入带动画。
skinned morph(*.js)模型导入带动画。(新版three.js示例中已经移除该模型。)
平面画点工具。
平面画线工具。
平面贴花工具。
选中物体效果优化。

v0.0.8

发布日期：2018年10月27日
更新日志：


编辑器文档更新。
立体贴图上传服务端，并可设置为场景背景。
所有场景一键发布静态网站，便于部署到GitHub Pages服务上。
柏林地形组件、序列化和反序列化，并可在播放器中展示。
上传mp4视频贴图，并可以设置到材质上，在三维场景中播放视频。
增加水组件。

v0.0.7

发布日期：2018年10月14日
更新日志：


场景、模型、纹理、音频、mmd资源编辑功能，可上传预览图。
材质纹理属性编辑功能。
播放器重新架构。
粒子发射器、天空、火焰、烟保存、载入、播放优化。
刚体组件不再默认添加，改为从组件菜单中手动添加。

v0.0.6

发布日期：2018年9月30日
更新日志：


提供补间动画支持。可以在时间轴上可视化修改补间动画，并在播放器中播放。
新增上传mmd模型（pmd和pmx格式）和mmd动画，可以在播放器中播放。
新增上传lmesh模型，可在播放器中播放。
基本几何体、光源、地形封装，便于进一步开发。

v0.0.5

发布日期：2018年9月16日
更新日志：


布局修改：右侧改为两栏，左边栏提供场景层次图和js脚本管理功能，右边栏是属性、设置和历史面板。
在编辑场景下方新增动画编辑（未完成），并把日志查看移动到这里。
属性面板组件化改造，新增基本信息、相机、几何体、光源、材质、粒子发射器、物理配置、场景、影子、
位移、音频监听器、背景音乐等多个组件。
背景音乐支持保存载入，提供音频管理。
修复编辑着色器程序功能，实时查看着色器效果。
新增茶壶参数编辑组件。
各种几何体都可以开启反射。

v0.0.4

发布日期：2018年9月2日
更新日志：



脚本编辑优化，脚本不再跟物体绑定，可以跟场景一起保存载入，提供javascript、vertexShader、fragmentShader、programInfo示例脚本。自定义脚本支持init、start、update、stop、onClick、onDblClick、onKeyDown、onKeyUp、onMouseDown、onMouseMove、
onMouseUp、onMouseWheel、onResize 13种事件。


背景支持纯色、背景图片、立体贴图三种不同类型，可以保存载入。


新增网格、相机、点光源、平行光、聚光灯、半球光、矩形光、帮助器、骨骼9种帮助器的显示隐藏设置。


新增日志面板。


平板新增镜面特效。


v0.0.3

发布日期：2018年8月15日
更新日志：


使用asp.net开发web服务端，使用MongoDB保存模型和场景数据。
15种格式3D模型的上传，并可以保存到场景。
场景的创建、保存、载入。
组、12种内置几何体、5种光源可以保存场景并载入。
85种three.js对象的序列化和反序列化。

v0.0.2

发布时间：2018年6月9日
更新日志：


使用rollup重构three.js自带编辑器的代码。

v0.0.1

发布时间：2017年6月21日
更新日志：


主要完成three.js自带编辑器的翻译。

相关链接

Three.js官网：https://threejs.org/
LOL模型查看器：https://github.com/tengge1/lol-model-viewer
模型下载1：https://sketchfab.com/3d-models?features=downloadable
模型下载2：https://www.3dpunk.com/work/index.html?category=downloadable

",216
JustusAvramenko/delenda_est,JavaScript,"Check out 0ad.mod.io for the latest Delenda Est release for the official releases of 0 A.D.
This repository is a work in progress and is not guaranteed to work with the latest ""release"" of 0 A.D. Rather, it is more compatible with the ""SVN"" or ""Development"" version of the game, which is bleeding edge.
",18
jpd002/Play-,C++,"Play!
Play! is an attempt to create a PlayStation 2 emulator for Windows, macOS, UNIX, Android & iOS platforms.
Compatibility information is available on the official Compatibility Tracker. If a specific game
doesn't work with the emulator, please create a new issue there.
For more information, please visit purei.org.
Project Dependencies
External Libraries

boost

Repositories

Play! Dependencies
Play! Framework
Play! CodeGen
Nuanceur

Building
General Setup
You can get almost everything needed to build the emulator by using the Play! Build project. You can also checkout every repository individually if you wish to do so, but make sure your working copies share the same parent folder.
In the end, your setup should look like this:
C:\Projects

CodeGen
Dependencies
Framework
Nuanceur
Play

Common Building Instructions
First you'd need to clone Play-Build which provides you with the needed subprojects required to build Play!.
Then setup the submodules and the dependency submodule(s) too.
git clone https://github.com/jpd002/Play-Build.git
cd Play-Build
git submodule update -q --init --recursive
git submodule foreach ""git checkout -q master""
cd Dependencies
git submodule update --init
cd ..

Building for Windows
The Easiest way to build the project on windows is to open Qt Creator and directed it to the cmake file in /project/dir/Play-/CMakeLists.txt.
You can also build the project using Visual Studio or cmdline, for that you must follow these isnstruction:
To build for Windows you will need to have CMake installed on your system.
cd Play
mkdir build
cd build

# Not specifying -G would automatically generate 32-bit projects.
cmake .. -G""Visual Studio 15 2017 Win64"" -DCMAKE_PREFIX_PATH=""C:\Qt\5.10.1\msvc2017_64"" -DUSE_QT=YES

You can now build the project by opening the generated Visual Studio Solution or continue through cmdline:
cmake --build . --config Release

Note: --config can take any of Release/Debug/RelWithDebInfo
Building for macOS & iOS
If you don't have CMake installed on your system, you can install it using brew with the following command: brew install cmake.
There are two ways to generate a build for macOS, either by using makefiles or by using Xcode.
cd Play
mkdir build
cd build

# Not specifying -G would automatically pick Makefiles
cmake .. -G""Xcode"" -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=~/Qt/5.1.0/clang_64/
cmake --build . --config Release
# OR
cmake .. -G""Unix Makefiles"" -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=~/Qt/5.1.0/clang_64/
cmake --build .

To generate a build for iOS, you will need to add the following parameters to the CMake invocation:
-DCMAKE_TOOLCHAIN_FILE=../../../Dependencies/cmake-ios/ios.cmake -DTARGET_IOS=ON
iOS build doesnt use Qt so please omit -DCMAKE_PREFIX_PATH=...
Note: while iOS build can be generated with Makefiles, they will not be FAT binaries.
Example:
cmake .. -G""Xcode"" -DCMAKE_TOOLCHAIN_FILE=../../../Dependencies/cmake-ios/ios.cmake -DTARGET_IOS=ON
Building for UNIX
if you dont have cmake or openal lib installed, you'll also require Qt (preferably version 5.6) you can install it using your OS packaging tool, e.g ubuntu apt install cmake libalut-dev
on UNIX systems there is 3 ways to setup a build, using qt creator, makefile or Ninja


QT Creator

Open Project -> Play/CMakeLists.txt



Makefile/Ninja


cd Play
mkdir build
cd build

# Not specifying -G would automatically pick Makefiles
cmake .. -G""Unix Makefiles"" -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=/opt/qt56/
cmake --build .
# OR
cmake .. -G""Ninja"" -DCMAKE_PREFIX_PATH=/opt/qt56/
cmake --build . --config Release

Note CMAKE_PREFIX_PATH refers to the qt directory containing bin/libs folder, the above example uses a backport repo to install qt5.6 on trusty, if you install qt from qt offical website, your CMAKE_PREFIX_PATH might look like this ~/Qt5.6.0/5.6/gcc_64/
Building for Android
Building for Android has been tested on macOS and UNIX environments.
Android can be built using Android Studio or through Gradle.

Android Studio:

Files->Open Projects->Directory To Play/build_android
Install NDK using sdk manager
edit/create Play/build_android/local.properties

OSX: add a newline ndk.dir=/Users/USER_NAME/Library/Android/sdk/ndk-bundle replacing USER_NAME with your macOS username
UNIX: add a newline ndk.dir=~/Android/Sdk/ndk-bundle
Windows: add a newline C:\Users\USER_NAME\AppData\Local\Android\sdk\ndk-bundle
Please Leave an empty new line at the end of the file





Note, these examples are only valid if you installed NDK through Android Studio's SDK manager.
Otherwise you must specify the correct location to Android NDK.
Once this is done, you can start the build.

Gradle: Prerequisite Android SDK & NDK (Both can be installed through Android Studio)

edit/create Play/build_android/local.properties

OSX:

add a newline sdk.dir=/Users/USER_NAME/Library/Android/sdk replacing USER_NAME with your macOS username
add a newline ndk.dir=/Users/USER_NAME/Library/Android/sdk/ndk-bundle replacing USER_NAME with your macOS username


UNIX:

add a newline sdk.dir=~/Android/Sdk
add a newline ndk.dir=~/Android/Sdk/ndk-bundle


Windows:

add a newline sdk.dir=C:\Users\USER_NAME\AppData\Local\Android\sdk
add a newline ndk.dir=C:\Users\USER_NAME\AppData\Local\Android\sdk\ndk-bundle


Please Leave an empty new line at the end of the file





Note, these examples are only valid if you installed NDK through Android Studio's SDK manager.
Otherwise you must specify the correct location to Android NDK.
Once this is done, you can start the build.
cd Play/build_android
sh gradlew assembleDebug

about Release/Signed builds.
Building through Android Studio, you have the option to “Generate Signed APK”.
Building through Gradle, you must create a text file Play/build_android/keystore.properties and add the following properties to it, storeFile,storePassword,keyAlias,keyPassword.
E.g of keystore.properties
storeFile=/location/to/my/key.jks
storePassword=mysuperhardpassword
keyAlias=myalias
keyPassword=myevenharderpassword

Please Leave an empty new line at the end of the file
cd Play/build_android
sh gradlew assembleRelease
# or on Windows
gradlew.bat assembleRelease

",690
openstack/openstack,Python,"OpenStack Tracking Repo
zuul gates all of the contained projects in an effective single
timeline. This means that OpenStack, across all of the projects, does
already have a sequence of combinations that have been explicitly
tested, but it's non-trivial to go from a single commit of a particular
project to the commits that were tested with it.
Gerrit's submodule tracking feature will update a super project every
time a subproject is updated, so the specific sequence created by zuul
will be captured by the super project commits.
This repo is intended to be used in a read-only manner. Any commit in this
repo will get a collection of commits in the other repos that have
explicitly been tested with each other, if that sort of thing is important
to you.
",3052
TravelMapping/UserData,None,"UserData
Repository for User Files
.list files for TravelMapping users are stored here, and are retrieved from here for site updates.
TM users are welcome to update their own lists by submitting pull requests through to this repository or to submit list file updates by email to travmap@teresco.org to be placed here by the site admins.
See http://travelmapping.net and http://forum.travelmapping.net/ for more information about Travel Mapping.
",8
openstack/nova,Python,"Team and repository tags



OpenStack Nova
OpenStack Nova provides a cloud computing fabric controller, supporting a wide
variety of compute technologies, including: libvirt (KVM, Xen, LXC and more),
Hyper-V, VMware, XenServer, OpenStack Ironic and PowerVM.
Use the following resources to learn more.

API
To learn how to use Nova's API, consult the documentation available online at:

Compute API Guide
Compute API Reference

For more information on OpenStack APIs, SDKs and CLIs in general, refer to:

OpenStack for App Developers
Development resources for OpenStack clouds


Operators
To learn how to deploy and configure OpenStack Nova, consult the documentation
available online at:

OpenStack Nova

In the unfortunate event that bugs are discovered, they should be reported to
the appropriate bug tracker. If you obtained the software from a 3rd party
operating system vendor, it is often wise to use their own bug tracker for
reporting problems. In all other cases use the master OpenStack bug tracker,
available at:

Bug Tracker


Developers
For information on how to contribute to Nova, please see the contents of the
CONTRIBUTING.rst.
Any new code must follow the development guidelines detailed in the HACKING.rst
file, and pass all unit tests.
Further developer focused documentation is available at:

Official Nova Documentation
Official Client Documentation


Other Information
During each Summit and Project Team Gathering, we agree on what the whole
community wants to focus on for the upcoming release. The plans for nova can
be found at:

Nova Specs

",2469
Ultimate-Hosts-Blacklist/adblock.mahakala.is,Python,"About adblock.mahakala.is


About Ultimate-Hosts-Blacklist
Ultimate-Hosts-Blacklist serve a place to test and keep a track on each input sources that are present into Ultimate Hosts Blacklist.
As Ultimate Hosts Blacklist grew up it became impossible to test the whole repository, as it takes weeks to finish. That's why we use the GitHub organization system in order to create different repository for each list that are present into Ultimate Hosts Blacklist.

About PyFunceble
PyFunceble like Funceble is A tool to check domains or IP availability by returning 3 possible status: ACTIVE, INACTIVE or INVALID.
It also has been described by one of its most active user as:

[An] excellent script for checking ACTIVE, INACTIVE and EXPIRED domain names.

If you need further informations about PyFunceble or Funceble please report to our Wiki page and/or if you don't find any answer feel free to create an issue into one of the Dead Hosts's or Py-Funceble's repositories.
About the status returned by PyFunceble
For an up to date version of this part please report to the Status section of our Wiki.
ACTIVE
This status is returned when one of the following cases is met:


We can extract the expiration date from Lookup().whois().

Please note that we don't check if the date is in the past.



Lookup().nslookup() don't return server can't find domain-name.me: NXDOMAIN.


HTTOCode().get() return one the following code [100, 101, 200, 201, 202, 203, 204, 205, 206].


INACTIVE
This status is returned when all the following cases are met:

We can't extract the expiration date from Lookup().whois().
Lookup().nslookup() return server can't find domain-name.me: NXDOMAIN.

INVALID
This status is returned when the following case is met:


Domain extension has an invalid format or is unregistered in IANA Root Zone Database.

Understand by this that the extension is not present into the iana-domains-db.json file.



",2
alexandradilja/MonsterHousing,HTML,"MonsterHousing
",2
villares/sketch-a-day,JavaScript,"
sketch-a-day
one visual idea a day
Hi! I'm Alexandre Villares, welcome!
I try to make one small program (sketch) a day. I usually put the code here: github.com/villares/sketch-a-day
Feel free to contact me regarding licenses to use my work, teaching opportunities, consulting or other projects.
You may also support my artistic work, open source teaching resources and research with donations :)
Get updates from my sort-of-weekly newsletter: [sketch-mail]

2018

2019


sketch_190511b [Py.Processing]


sketch_190510a [Py.Processing]


sketch_190509a [Py.Processing]
Ugly but works :)


sketch_190508a [code for Py.Processing]
Back unfolding solids... 2D faces missing.


Using pyp5jsto run on your browser
sketch_190507a [code for pyp5js]


Old sketch tweaked to port tomorrow.


Using pyp5js to run on your browser
sketch_190505a
[code for pyp5js]
[code for Py.Processing]


Using pyp5jsto run on your browser
sketch_190504a code:pyp5js


sketch_190503a [Py.Processing]


sketch_190502a [Py.Processing]


sketch_190501a [Py.Processing]


sketch_190430b [Py.Processing]


sketch_190429b [Py.Processing]


sketch_190428b [Py.Processing]
Mixing in my var_bar() circle/circle tangent shape


sketch_190427b [Py.Processing]
Now based on 4 quaternary digits (256 variations). Each digit can be 0, 1, 2, or 3. And zero means no shape drawn for that layer/position. Still influenced by @arjanvandermeij :)


sketch_190426b [Py.Processing]


sketch_190425b  [Py.Processing]


sketch_190424b [Py.Processing]
Inspired by ""trit"" grids of balanced ternary digits from @arjanvandermeij
(4 ternary digits -> 81 variations)


sketch_190424a [Py.Processing]


sketch_190423a [Py.Processing]
Now I drag any point, inclunding of holes, and remove points.
TODO: Add points; Drag polys.


sketch_190422a [Py.Processing]
Object Oriented remake of the poly editor in progress, in order to have multiple polygons with multiple holes each.


sketch_190421 [Py.Processing]
This clean up and tweak of studies of Design By Numbers alphabet designed by Peter Cho from last year will count as today's sketch :)
Check out the other pieces, pixel and scaleable fonts I created here: http://github.com/villares/DesignByNumbers-alphabet


sketch_190420a [Py.Processing]
Some refactoring and coordinate annotations that I'll use in my classes :)


sketch_190419a [Py.Processing]
Press space-bar to order outer_pts clockwise, and inner_pts anticlockwise (counterclockwise, in the US)


sketch_190418a [Py.Processing]


sketch_190417a [Py.Processing]
Back to the editor... dragging points.


sketch_190416a [Processing Java Mode]
Very simple 3D example


sketch_190415a [Py.Processing]


sketch_190414a [Py.Processing]
Let's unfold some simple extrudes


sketch_190413b [Py.Processing]


sketch_190412a [Py.Processing]


sketch_190411a [Py.Processing]
Now we are talking!
I've got the divided top right, brought back the tabs,
and also found a bug on 190408a (and fixed it)...


sketch_190410a [Py.Processing]
major re-org, still broken...


sketch_190409b [Py.Processing]
Subdivided top! (not quite there yet...)


sketch_190408a [Py.Processing]
With glue tabs!


sketch_190407a [Py.Processing]
Now I can change the base proportions.


sketch_190406a [Py.Processing]
First unfold version ready!


sketch_190405a [Py.Processing]
Almost done!


sketch_190404a [Py.Processing]
Study for a ""Terrain box"" a paper surface ""unit"".


sketch_190403a [Py.Processing]


sketch_190402a [Py.Processing]


sketch_190401b [Py.Processing]


sketch_190331a [Py.Processing]


sketch_190330a [Py.Processing]


sketch_190329a [Py.Processing]


sketch_190328a [Py.Processing]


sketch_190327a [Py.Processing]


sketch_190326a [Py.Processing]


sketch_190325a [Py.Processing]


sketch_190324a [Py.Processing]
A retake of sketch_190207a + work from sketch_190321 :)
Will stall sometimes...
as there is an unsafe while loop
selecting pointing nodes... (also present on 190323)


sketch_190323a [Py.Processing]


sketch_190322a [Py.Processing]


sketch_1903221b [Py.Processing]


sketch_190320a [Py.Processing]


sketch_190319a [Py.Processing]


sketch_190318a [Py.Processing]


sketch_190317a [Py.Processing]


sketch_190316a [Py.Processing]


sketch_190315a [Py.Processing]


sketch_190314a [Py.Processing]


sketch_190313a [Py.Processing]


sketch_190312a [Py.Processing]


sketch_190311a [Py.Processing]


sketch_190310a [Py.Processing]
Refactor and a not very good filling test


sketch_190309a [Py.Processing]
Deque collection for a dynamic history on Z


sketch_190308a [Py.Processing]


sketch_190307a [[Py.Processing](https://villare
s.github.io/como-instalar-o-processing-modo-python/index-EN)]
An graph much like the ones before this, but made invisible, is behind the (virtual) corners of this rounded poly.


sketch_190306a Py.Processing]


sketch_190305a [Py.Processing]
Mais grapholia ;)
Removi parte do código para controle com potenciômetros pois infelizmente a comunicação serial está quebrada neste momento no Processing Modo Python :((


sketch_190304a [Py.Processing]
Grapholia ;)
Retomando um sketch de grafos com 4 parâmetros ajustaveis (via teclado ou potenciômetros ligados em um Arduino)


sketch_190303a [Py.Processing]


sketch_190302a [Py.Processing]


sketch_190301a [Py.Processing]


sketch_190228a [Py.Processing]


sketch_190227a [Py.Processing]


sketch_1tus90226a [Py.Processing]


sketch_190225a [Py.Processing]


sketch_190224a [Py.Processing]


sketch_190223a [Py.Processing]


sketch_190222a [Py.Processing]


sketch_190221a [Py.Processing]


sketch_190220a [Py.Processing]


sketch_190219a [Py.Processing]


sketch_190218a [Py.Processing]


sketch_190217a [Py.Processing]


sketch_190216a [Py.Processing]


sketch_190215a [Py.Processing]


sketch_190214a [Py.Processing]


sketch_190213a [Py.Processing]


sketch_190212a [Py.Processing]


sketch_190211b [Py.Processing]


sketch_190211a [Py.Processing]


sketch_190210c [Py.Processing]
""a"" and ""b"" are Java and Python ports of a C# round corner.


sketch_190209a [Py.Processing]


sketch_190208a [Py.Processing]


sketch_190207a [Py.Processing]


sketch_190206b [Py.Processing]


sketch_190205a [Py.Processing]


sketch_190204a [Py.Processing]


sketch_190203a [Py.Processing]


sketch_190202a [Py.Processing]
Retake of sketch #57 180226 with the variable ""bar"" from yesterday.


sketch_190201a [Py.Processing]


sketch_190131a [Py.Processing]


sketch_190130a [Py.Processing]


sketch_190129a [Py.Processing]


sketch_190128b [Py.Processing]


sketch_190127a [Py.Processing]


sketch_190126a [Py.Processing]


sketch_190125a [Py.Processing]


sketch_190124a [Py.Processing]


sketch_190123a [Py.Processing]


sketch_190122a [Py.Processing]


sketch_190121a [Py.Processing]


sketch_190120a [Py.Processing]


sketch_190119a [Py.Processing]


sketch_190118a [Py.Processing]


sketch_190117b [Py.Processing]


sketch_190116a [Py.Processing]


sketch_190115a [Py.Processing]


sketch_190114a [Py.Processing]


sketch_190113a [Py.Processing]


sketch_190112a [Py.Processing]


sketch_190111a [Py.Processing]


sketch_190110a [Py.Processing]
sketch_190110b [Py.Processing]


sketch_190109a [Py.Processing]


sketch_190108a [Py.Processing]


sketch_190107a [Py.Processing]


sketch_190106a [Py.Processing]


sketch_190105a [Py.Processing]


sketch_190104a [Py.Processing]
Module tweaks


sketch_190103a [Py.Processing]
Made this today, thinking about my new newsletter: [sketch-mail]


[sketch_190102a]https://github.com/villares/sketch-a-day/tree/master/2019/sketch_190102a) [Py.Processing]


sketch_190101a [Py.Processing]
",24
zqlovejyc/SQLBuilder.Core,C#,"SQLBuilder.Core
项目介绍
.NET Standard 2.0版本SQLBuilder
码云地址
https://gitee.com/zqlovejyc/SQLBuilder.Core
",2
powerje/NyanDroid,Java,"Live wallpaper celebrating Ice Cream Nyanwiches. Available on Google Play.
Nyan Droid created by Daniel Sandler.
MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMNNNNMMMNNNNNNNNNNNNNNNNNNMMMNNNNMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMy++hMMd++++++++++++++++++dMMh++yMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMdyyhhhs//////////////////shhhyydMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMy////////////////////////yMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMyoo+//smmmmmd//////dmmmmms//+ooyMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMs/////ohhdMMN//////hhhNMMs/////sMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMs/////-  :MMN//////`  dMMs/////sMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMyss+//////-:/sss//////:-:oss+/////+ssyMMMMMMMMMMMMM
MMMMMMMMMMMMMo////////////////////////////////////oMMMMMMMMMMMMM
MMMMMMMMMMMMMo////////////////////////////////////oMMMMMMMMMMMMM
MMMMMMMMMMMMMhyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyhMMMMMMMMMMMMM
MMMMNNNNNNMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMNNNNNNNMMMMMMNNNNNNMMMM
MMMNooooooNMMmmmmmmmmmmmmmmmmmmmmmmmmmmmmo/////ommmMMNooooooNMMM
MMMN//////NMMmddddddmmddddddddddmmmdddddd/...../ddmMMN//////NMMM
MMMN//////NMMmdddddNMMNdddddddddMMMdddddd:     :ddmMMN//////NMMM
MMMN//////NMMmddddddmmmdddddddddmmmdddddd:     :ddmMMN//////NMMM
MMMN//////NMMmddddddddddddddddddddddddddd:     :ddmMMN//////NMMM
MMMN//////NMMmdddddddddddmMMMdddddddddddd:     :ddmMMN//////NMMM
MMMN//////NMMmddddddddddddmmmdddddddddddd:     :ddmMMN//////NMMM
MMMN//////NMMmddddddddddddddddddddddddddd:     :ddmMMN//////NMMM
MMMN//////NMMmdddddmMMNdddddddddMMMdddddd:     :ddmMMN//////NMMM
MMMN//////NMMmdddddmNNmdddddddddNNNdddddd:     :ddmMMN//////NMMM
MMMN//////NMMmddddddddddddddddddddddddddd:     :ddmMMN//////NMMM
MMMMddddddMMMmddddddddddddNNNdddddddddddd:     :ddmMMMddddddMMMM
MMMMMMMMMMMMMmdddddddddddmNNNdddddddddddd:     :ddmMMMMMMMMMMMMM
MMMMMMMMMMMMMmddddddddddddddddddddddddddd:     :ddmMMMMMMMMMMMMM
MMMMMMMMMMMMMmdddddmNNmdddddddddNNNdddddd:     :ddmMMMMMMMMMMMMM
MMMMMMMMMMMMMmdddddmMMNdddddddddMMMdddddd:     :ddmMMMMMMMMMMMMM
MMMMMMMMMMMMMmddddddddddddddddddddddddddd+-----+ddmMMMMMMMMMMMMM
MMMMMMMMMMMMMmmmmmmmmmmmmmmmmmmmmmmmmmmmmyoooooymmmMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMdoooooyMMMMMMMMMMMMyooooodMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMh/////oMMMMMMMMMMMMo/////hMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMh/////oMMMMMMMMMMMMo/////hMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMh/////oMMMMMMMMMMMMo/////hMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMh+++++sMMMMMMMMMMMMs+++++hMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMNNNNNNNMMMMMMMMMMMMNNNNNNNMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM

Licensing
Nyan Droid source is released under the Apache 2.0 license.
dyan.mp3 (droid droid dr-dr-droid audio) created by Jeff Sharkey and released under the CC license.
",9
Lombiq/Lombiq-Orchard-Visual-Studio-Extension,C#,"Lombiq Orchard Visual Studio Extension readme

Visual Studio extension with many features and templates frequently used by  Lombiq developers. Contains Orchard-related as well as generic goodies.
Check out the extension's Readme for more info (it's there and not in the root of the repository so it's also accessible from inside VS).
The project's logo was created by Ulises TJ.
",6
larseggert/iana-assignments,XSLT,"IANA Assignments Mirror
Mirror of the IANA registries. Periodically updated via
rsync -avzH rsync.iana.org::assignments .

from https://www.iana.org/protocols.
For obvious reasons, pull requests are not (currently) accepted.
",3
C4G/BLIS,PHP,"BLIS
C4G Basic Laboratory Information System
How to run BLIS
Clone the repository onto your machine. Download the BLIS runtime files from: [http://blis.cc.gatech.edu/files/BLISRuntime.zip]
Unzip all files from BLISRuntime.zip into the the BLIS/  directory in your repository clone.
Run BLIS.exe to start BLIS.
",13
kazedayo/GaldenApp-v2,Swift,"1080-Green
HKGalden app for iOS written in Swift
This is the branch of my HKGalden app for the upcoming new HKGalden forum by abc(owner of na.cx)
App Store Available Now!
https://itunes.apple.com/us/app/1080-green/id1437419169?l=zh&ls=1&mt=8
Compile to try yourself
You can compile this project using XCode and try it on your iOS device. Before compiling please run 'pod install' command under project diretory. If you don't have cocapods installed in your Mac, please install it from here: https://cocoapods.org
Buy me a coffee
https://paypal.me/kazeteitoku
",2
GovReady/govready-q,Python,"GovReady-Q Compliance Server
The GovReady-Q Compliance Server is an open source GRC platform for highly automated, user-friendly, self-service compliance assessments and documentation. It's perfect for DevSecOps.
GovReady-Q solves the painful compliance bottleneck of needing months to authorize applications that deploy and redeploy in minutes.




ATTENTION!




GovReady-Q software is ""Beta"" software best suited for early adopters needing faster compliance for DevSecOps.



Documentation
Visit our Documentation at govready-q.readthedocs.io.
Read What You Most Need to Know About GovReady-Q.
Using Hosted GovReady-Q
There's nothing to install. Q.GovReady.com is the hosted, multi-tenant version of GovReady-Q.

Visit Q.GovReady.com
Fill out the form ""About your organization"" and ""About you"" to create your account
Don't worry about the Service Levels -- everything's available to everyone during the Beta phase
We'll contact you to help you get started

The hosted version is an excellent solution if have one project/system you are trying to get through NIST SP 800-53 or NIST SP 800-171 compliance, or you are have just trying to pull together a few specific compliance documents like your Privacy Policy or Rules of Behaivor. The hosted service operated by GovReady® PBC, the company behind GovReady-Q Compliance Server.
If you have questions about if hosted version, email info@govready.com.
Downloading GovReady-Q



Downloading
Where




Current release on Docker
https://hub.docker.com/r/govready/govready-q/


Nightly Build on Docker
https://hub.docker.com/r/govready/govready-q-nightly/


Clone the GitHub repo
https://github.com/govready/govready-q



Installing GovReady-Q



Deployment Guide




Installing on Workstations for Development


Deploying with Docker


Deploying on RHEL 7 / CentOS 7


Deploying on Ubuntu



Support
Join our mailing list and stay informed of developments.
Noteworthy
GovReady-Q is open source and incorporates the emerging OpenControl data standard for reusable compliance content.
License / Credits
This repository is licensed under the GNU GPL v3.

Emoji icons by http://emojione.com/developers/.
Generic server icon by Stock Image Folio from Noun Project.

",27
gdmarmerola/r-tesouro-direto,R,"r-tesouro-direto
Projeções de Investimentos no Tesouro Direto com R
No período recente de alta da taxa de juros e inflação crescente (2014-2015), o Tesouro Direto tem se mostrado uma boa opção de investimento. Visualizar a evolução de diversas carteiras hipotéticas e compará-las ao longo do tempo pode ajudar significativamente o investidor a fazer uma alocação de recursos alinhada com seus objetivos. Este pacote usa alguns recursos de web scraping e o pacote gráfico ggplot2 do R para fazer algumas estimativas e projeções de investimentos no tesouro direto.

Modalidades implementadas:

Tesouro Prefixado (LTN)
Tesouro IPCA (NTN-B Principal)
Tesouro IPCA com Juros Semestrais (NTN-B)


Não-implementadas (to-do):

Tesouro Prefixado com Juros Semestrais (NTN-F)
Tesouro Selic (LFT)


Pacotes necessários:

quantmod: séries temporais, modelagem financeira
ggplot2: gráficos
rvest: web scraping
reshape2: formatação de dataframes
lubridate: trabalhar com datas



Funcionamento
O objetivo do pacote é criar séries temporais a partir de informações de investimentos. São utilizadas duas funções básicas:
proj_tesdir: criar série temporal de investimento
proj_tesdir(valor_ini, juros_anuais, data_ini, data_fim)


valor_ini: montante inicial do investimento
juros_anuais: número que represente a média prevista dos juros no período, ou lista que associe um valor de juros por ano (mais na seção Funcionalidades)
data_ini, data_fim: datas de início e fim do investimento

calcular_cupons: calcular pagamento de cupons
calcular_cupons(serie_td, tx_cupom, data_ini, data_fim)


serie_td: série temporal do principal
juros_anuais: juros associados ao pagamento de cupons
data_ini, data_fim: datas de início e fim do investimento

Para entender melhor como essas funções são utilizadas segue um exemplo de utilização.
Exemplo de utilização
Um script de exemplo é fornecido no repositório. Para começar a usar as funções, devemos compilar o script base:
# colocar aqui o working directory
# windows: usar barras duplas -> '\\'
setwd('your-path/r-tesouro-direto')

# compilar o script base
source('your-path/td-base.r')

Utilizando as funções mencionadas anteriormente, podemos estimar projeções para as modalidades disponíveis de investimento.
Tesouro Prefixado
Uma maneira organizada de gerenciar as informações é criar um objeto list() e utilizar suas entradas como argumentos da função proj_tesdir:
### Tesouro Prefixado (LTN) ###

# valor inicial: 3000, taxa prefixada 13% a.a.
# duracão do investimento: 1 ano (01-01-2016 até 01-01-2017)
prefix_2017 = list(montante_ini = 3000, 
                   tx_anual = 13,
                   data_ini = ""2016-01-01"",
                   data_venc = ""2017-01-01"")

# cria uma série temporal (xts) com a projecão do investimento
serie_prefix_2017 = proj_tesdir(prefix_2017[['montante_ini']],
                                prefix_2017[['tx_anual']],
                                prefix_2017[['data_ini']],
                                prefix_2017[['data_venc']])


Tesouro IPCA
Aqui é utilizada a função extrair_ipca() para buscar o valor (12 meses) do IPCA no site do Valor Econômico. É importante ressaltar que dessa forma existe a aproximação que o IPCA não mudará ao longo do investimento. Porém, é possível definir uma lista com um valor de IPCA por ano, para um ajuste mais fino (como mostrado na seção Funcionalidades).
### Tesouro IPCA (NTN-B Principal) ###

# vencimento em 2019, montante inicial de 10000, taxa de 6%
ipca_2019 = list(montante_ini = 10000, 
                 tx_anual = 6.00 + extrair_ipca(), # 6% rendimento real
                 data_ini = ""2016-01-01"",
                 data_venc = ""2019-05-15"")

# série temporal
serie_ipca_2019 = proj_tesdir(ipca_2019[['montante_ini']],
                              ipca_2019[['tx_anual']],
                              ipca_2019[['data_ini']],
                              ipca_2019[['data_venc']])

Tesouro IPCA com Juros Semestrais
Nesta modalidade há o pagamento de juros semestrais. Portanto, no objeto list() é colocada uma entrada tx_cupom para representar os juros semestrais. Ao final, são criadas duas séries, uma representando o montante principal e outra o pagamento de cupons.
### Tesouro IPCA com cupons semestrais (NTN-B) ###

# supondo VNA de 3000 e compra de 2 unidades: 6000
vna_exemplo = 3000

ipca_2020 = list(montante_ini = 2*vna_exemplo,
                 tx_anual = extrair_ipca(),
                 tx_cupom = 6,
                 data_ini = ""2016-02-01"",
                 data_venc = ""2020-08-15""
                 )

# série temporal
serie_ipca_2020 = proj_tesdir(ipca_2020[['montante_ini']],
                              ipca_2020[['tx_anual']],
                              ipca_2020[['data_ini']],
                              ipca_2020[['data_venc']])

# calcula os cupons com base na série principal
ipca_2020_cups = calcular_cupons(serie_ipca_2020,
                                 ipca_2020[['tx_cupom']],
                                 ipca_2020[['data_ini']],
                                 ipca_2020[['data_venc']])

Fluxo de caixa
Após criar as séries temporais, podem ser criadas mais duas séries de apoio:

imobilizado: representa o total investido
resgates: mostra o fluxo de pagamentos

Dessa forma, é possível visualizar uma estimativa do fluxo de caixa do investidor ao longo do tempo, informação valiosa no ato de alocar recursos. No pacote isso é feito da seguinte maneira:

## desempenho geral

# juntar projecões em um único dataframe
merged_proj = merge.xts(serie_prefix_2017, serie_ipca_2019, 
                        serie_ipca_2020, ipca_2020_cups) #[""/2015-09-04""]

# série que mostra o fluxo de pagamentos
fluxo_resgate = valor_de_resgate(merged_proj,
                                 ""2017-01-01"",
                                 ""2019-05-15"",
                                 ""2020-08-15"")
      
# série que mostra o total investido 
imobilizado = rowSums(merged_proj[,1:3], na.rm=TRUE)

# fluxo de pagamentos: removendo NAs
resgates = rowSums(merge.xts(merged_proj[,4], fluxo_resgate), na.rm=TRUE)


Gerar gráficos
Antes de gerar os gráficos, é feita uma etapa de pré-processamento de forma que as séries estejam em um formato fácil de ser utilizado com o ggplot.
# unindo os dataframes que serão plotados
to_plot = cbind(data.frame(merged_proj), 
                data.frame(imobilizado),
                data.frame(resgates),
                data.frame(dates = index(merged_proj)))

# formatando o dataframe na forma adequada para o ggplo2
plot_df = melt(to_plot, 'dates')

Finalmente, podemos plotar as séries.
# criando o gráfico
plt = ggplot(plot_df,aes(x=dates,y=value,group=variable,color=variable)) 
plt + geom_line(aes(group=variable),size=1) + scale_x_date() + ggtitle(""Portfolio corrente"") 

Em azul escuro, é mostrado o total investido e sua evolução com o tempo. Em lilás, o ""fluxo de caixa"" do investidor. Cada investimento é representado por uma linha (se não houver pagamento de cupons) ou duas (se houver pagamento de cupons).

Funcionalidades
Nesta seção são mostradas algumas funcionalidades interessantes do pacote.
Funções de apoio
Funções simples e úteis utilizadas para criar as séries temporais.

### Funcões úteis ###

# 1) Extrair dias úteis entre duas datas:
# (usa o arquivo ./data/dates.csv como base)
# formato deve ser 'YYYY-MM-DD'
print( get_workdays('2015-01-01', '2015-01-10') ) 

# 2) Número de dias entre duas datas (corridos)
print( get_n_dias('2015-01-10', '2015-01-01') )

# 3) Retornar fatia do IR correspondente ao intervalo de dias
print( get_fatia_ir(100) ) # 1a faixa
print( get_fatia_ir(200) ) # 2a faixa
print( get_fatia_ir(400) ) # 3a faixa
print( get_fatia_ir(800) ) # 4a faixa

# 4) Extrair IPCA do site Valor Econômico (12 meses) 
print( extrair_ipca() )

# 5) Extrair Selic do site Valor Econômico (12 meses)
print( extrair_selic() )


Cálculo automático do IR
O Imposto de Renda é calculado e descontado automaticamente nas projeções, segundo as regras do tesouro direto. Nos momentos em que há mudança de faixa na alíquota de imposto é possível observar pequenos saltos na rentabilidade:
plot(serie_ipca_2019)


Definição de lista de juros anuais
Para sanar o problema de utilizar uma única taxa de juros para todo o período de um investimento, é possível criar uma lista em que cada entrada corresponde à uma taxa prevista para um ano. Vamos utilizar 3 exemplos:

IPCA constante ao longo do investimento
IPCA crescente ao longo do investimento
IPCA decrescente ao longo do investimento

Nota: a variação das taxas são um pouco exageradas de forma que as diferenças entre os exemplos sejam visíveis.
IPCA constante
A projeção neste caso é feita da mesma forma mostrada no script exemplo:
taxa_constante = 10

ipca_constante = list(montante_ini = 10000, 
                      tx_anual = taxa_constante,
                      data_ini =  ""2015-01-01"",
                      data_venc = ""2019-05-15"")

# série temporal
serie_ipca_constante = proj_tesdir(ipca_constante[['montante_ini']],
                                   ipca_constante[['tx_anual']],
                                   ipca_constante[['data_ini']],
                                   ipca_constante[['data_venc']])

plot(serie_ipca_constante)


IPCA crescente
Neste caso, vamos supor que o IPCA aumente no ritmo de 2% ao ano. Para representar isto, utilizamos um objeto do tipo list(), taxa_crescente.
# ipca aumentando 2% a cada ano
taxa_crescente = list('2015' = 10,
                      '2016' = 12,
                      '2017' = 14,
                      '2018' = 16,
                      '2019' = 18) 

ipca_crescente = list(montante_ini = 10000, 
                      tx_anual = taxa_crescente,
                      data_ini =  ""2015-01-01"",
                      data_venc = ""2019-05-15"")

# série temporal
serie_ipca_crescente = proj_tesdir(ipca_crescente[['montante_ini']],
                                   ipca_crescente[['tx_anual']],
                                   ipca_crescente[['data_ini']],
                                   ipca_crescente[['data_venc']])

plot(serie_ipca_crescente)


IPCA decrescente
Finalmente, aqui vemos o caso inverso do anterior, em que o IPCA cai 2% ao ano até o fim do investimento.
# fazer a projecão de um IPCA 2019 utilizando taxas previstas para cada ano

# ipca diminuindo 2% a cada ano 
taxa_decrescente = list('2015' = 10,
                        '2016' = 8,
                        '2017' = 6,
                        '2018' = 4,
                        '2019' = 2) 

ipca_decrescente = list(montante_ini = 10000, 
                        tx_anual = taxa_decrescente,
                        data_ini =  ""2015-01-01"",
                        data_venc = ""2019-05-15"")

# série temporal
serie_ipca_decrescente = proj_tesdir(ipca_decrescente[['montante_ini']],
                                     ipca_decrescente[['tx_anual']],
                                     ipca_decrescente[['data_ini']],
                                     ipca_decrescente[['data_venc']])

plot(serie_ipca_decrescente)


",7
KaidemonLP/Open-Fortress-Source,C++,"Open Fortress: Source Code
",25
fuhd/studynotes,None,"studynotes
学习笔记
",5
ahf/dotfiles,Perl,"Alex's Dotfiles
This repository contains dotfiles for various programs that I use regularly.
The files are split into ""logical"" modules that are maintained with the GNU
Stow ""symlink farm manager"".
",2
zapret-info/z-i,None,"z-i
Register of Internet Addresses filtered in Russian Federation
",926
limintao/vue-calendars,CSS,"vue-calendars


A simple calendar selection component based on vue.js!You can customize which day is not optional, or you can define subscripts for each day (or days). Single or multiple(/interval) choice!

Getting Started
install
By npm
npm install vue-calendars --save

or download code and include it
<script src='dist/vue-calendars.js'></script>

Usage
Register component globally!
// Your entry main.js

import Vue from 'vue'
import App from './App.vue'
import vCalendar from ""vue-calendars""

Vue.use(vCalendar)
new Vue({
  el: '#app',
  render: h => h(App)
})

or register locally in your .vue file
Example
<template>
  <div class=""hello"">
    <div class=""chooseView"" @click=""openCalendar"">
        <span class=""item"">选择日期：</span>
        <span class=""result"">开始时间：{{ selectDate[0] }}</span>
    </div>
    
    <v-calendar 
        :option=""option"" 
        :click-action=""setSelectDate""
        :multi-selection=""isMultiple""
        :interval-selection=""isInterval""
        :subscript=""subscript""
        :items-subscript=""itemsSubscript""
        ></v-calendar>
    
  </div>
</template>

<script>
export default {
  name: 'HelloWorld',
  data () {
    return {
        option: {
            open: false,  //是否打开日历📅；
            aroud: 12, //显示多少月的数据
            title: '选择出行日期'
        },
        selectDate: [],    //当前选择的日期
        isMultiple: false, //是否多选，false单选、true多选
        isInterval: true,   // 是否是区间选择
        subscript: ""可约"",  //所有的日期下标标题
        itemsSubscript:[    // 自定义哪天不可选和自定义标题
            {
                date: '2018-05-31',
                title: '不可休',
            },
            {
                date: '2018/06/01',
                title: '不可休',
            },
            {
                date: '2018,06,22',
                title: '不可休',
            }
        ]
    }
  },
  
  methods:{
    openCalendar() {
        this.option.open = true;
    },
    setSelectDate(d) {   //设置点击的日期,返回的是一个数组
        this.selectDate= d;
    }
  }
}
</script>
A sample screenshot is here,

Options



Option
Description




option
传入一组object aroud(当前日期),如当前日期为2018/04/25 around为3 则显示2018/04 2018/05 2018/06 3个月；open(是否显示日历)，true(显示) or false（隐藏）; title(要显示的标题), string


click-action
选择日期之后执行的方法，可接方法名，返回的是一个包含日期的数组


multi-selection
是否多选,true(多选) or false(单选)


interval-selection
是否区间选择，与多选冲突，如果同时设true则按多选操作


subscript
所有日期的下标


items-subscrip
选择哪些日期不可选，或哪些日期的自定义下标




有什么问题欢迎随时提Issues！😊

",3
jacobnisnevich/overrustle-vods,JavaScript,"OverRustle VODs
OverRustle VODs is a simple web app that plays Destiny Twitch VODs with the destiny.gg/OverRustle chat along the side in real-time
Running the Project
OverRustle VODs requires Ruby to run properly. I've tested with Ruby 2.2.1p85 using rvm on Ubuntu 14.04. To build the project simply run
bundle install

to install all gem dependencies and then start the project with
ruby app.rb

",2
Lombiq/Orchard-Application-Host,C#,"Orchard Application Host Readme
Project Description
A light-weight framework that allows you to write arbitrary code (console or web applications, anything) empowered with Orchard.
Overview
The Orchard Application Host is a portable environment that lets you run your application inside a standalone Orchard shell. I.e. you can write any app with an Orchard developer experience, without using an Orchard web app. This enables you to use Orchard's features and services from any application (not just web applications), including:

Automatic dependency injection
Helpers and utilities
Data access services, including content management
Orchard-style events
Shapes
Caching
Localization
Logging
Background tasks

With Orchard Application Host you can create console applications, Windows services, desktop applications, cloud workers or any other type of app that uses Orchard's capabilities. No more low-level project start: you get an application framework that you can begin developing awesome software with, utilizing your Orchard knowledge and Orchard's power.
You can see a demo of the Orchard Application Host on the recording of the Orchard Community Meeting.
Among others Orchard Application Host powers the reverse proxy of the Hosting Suite and Hastlayer too.
The project's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/orchard-application-host (Mercurial repository)
https://github.com/Lombiq/Orchard-Application-Host (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
Using Orchard App Host as source in a solution

See examples in Lombiq.OrchardAppHost.Sample and for a full usage scenario with a non-Orchard solution in the Orchard Application Host Quick Start.
Disable SessionConfigurationCache otherwise you'll get ""The invoked member is not supported in a dynamic assembly."" exceptions that are harmless but prevent the session cache from being used anyway.
You'll get a ""The invoked member is not supported in a dynamic assembly."" exception during the first startup from AbstractDataServicesProvider but this is harmless.
Also from AbstractDataServicesProvider you'll get a ""Could not load file or assembly 'NHibernate.XmlSerializers ...' or one of its dependencies. The system cannot find the file specified."" exception that is also harmless.
If you want to use anything, even indirectly, from Orchard.Core, you have to add a project reference to it. E.g. even if you don't access anything from Orchard.Core but you use a service that gets ISiteService injected what in turn has an implementation in Orchard.Core then you indirectly depend on Orchard Core; thus, you have to add a project reference to it.
When using SQL CE you should add a reference to its assembly System.Data.SqlServerCe and set it as Copy Local = true.
Imported extensions don't need to declare a Module.txt but still can have features: by default they get a feature with the same name as the assembly's (short) name and also all OrchardFeature attribute usages will be processed and their values registered as features.
Note that starting Orchard App Host will currently take over ASP.NET MVC and Web API controller instantiation, see this Orchard issue.

Solution structure
The solution must follow this folder structure:

NuGet.config (see explanation below)
Lombiq.OrchardAppHost

Lombiq.OrchardAppHost.csproj


Orchard (a full Orchard source, i.e. the lib, src folder under it)
Arbitrarily named subfolder for 3rd party modules, e.g. Modules. Put your own modules here.

Module1

Module1.csproj





The Orchard Application Host Quick Start solution shows these conventions.
Configuring NuGet
A custom NuGet.config file is needed in the root of your solution so NuGet packages used by Orchard can be properly loaded. This should configure repositoryPath as following:
<?xml version=""1.0"" encoding=""utf-8""?>
<configuration>
  <config>
    <add key=""repositoryPath"" value=""Orchard\src\packages"" />
  </config>
</configuration>

Also see the example in the Orchard Application Host Quick Start.
Making assembly references compatible with different solution structures
3rd party modules may reference dlls from the Orchard lib folder or use the same NuGet packages as Orchard. By default these references will break since modules in an Orchard solution are under src/Orchard.Web/Modules, not above the Orchard folder (and thus paths differ). To make a module compatible with both standard Orchard solutions and Orchard App Host solutions add the following elements to the modules's csproj:
<!-- Orchard App Host (https://github.com/Lombiq/Orchard-Application-Host) compatibility start. Enabling the usage of a lib folder at a different location. -->
<ItemGroup>
  <LibReferenceSearchPathFiles Include=""..\..\Orchard\lib\**\*.dll"">
    <InProject>false</InProject>
  </LibReferenceSearchPathFiles>
  <NuGetReferenceSearchPathFiles Include=""..\..\Orchard\src\packages\**\*.dll"">
    <InProject>false</InProject>
  </NuGetReferenceSearchPathFiles>
</ItemGroup>
<Target Name=""BeforeResolveReferences"">
  <RemoveDuplicates Inputs=""@(LibReferenceSearchPathFiles->'%(RootDir)%(Directory)')"">
    <Output TaskParameter=""Filtered"" ItemName=""LibReferenceSearchPath"" />
  </RemoveDuplicates>
  <CreateProperty Value=""@(LibReferenceSearchPath);$(AssemblySearchPaths)"">
    <Output TaskParameter=""Value"" PropertyName=""AssemblySearchPaths"" />
  </CreateProperty>
  <RemoveDuplicates Inputs=""@(NuGetReferenceSearchPathFiles->'%(RootDir)%(Directory)')"">
    <Output TaskParameter=""Filtered"" ItemName=""NuGetReferenceSearchPath"" />
  </RemoveDuplicates>
  <CreateProperty Value=""@(NuGetReferenceSearchPath);$(AssemblySearchPaths)"">
    <Output TaskParameter=""Value"" PropertyName=""AssemblySearchPaths"" />
  </CreateProperty>
</Target>
<PropertyGroup Condition=""Exists('..\..\Orchard\lib')"">
  <ModulesRoot>..\..\Orchard\src\Orchard.Web\Modules\Orchard.Alias\</ModulesRoot>
</PropertyGroup>
<!-- Orchard App Host (https://github.com/Lombiq/Orchard-Application-Host) compatibility end. -->

Also make sure to prefix every project reference that points to one of Orchard's built-in projects with $(ModulesRoot) (assembly references don't need to be changed):
<ProjectReference Include=""$(ModulesRoot)..\..\..\Orchard\Orchard.Framework.csproj"">
  <Project>{2D1D92BB-4555-4CBE-8D0E-63563D6CE4C6}</Project>
  <Name>Orchard.Framework</Name>
</ProjectReference>
<ProjectReference Include=""$(ModulesRoot)..\..\..\Orchard\Orchard.Framework.csproj"">
  <Project>{2D1D92BB-4555-4CBE-8D0E-63563D6CE4C6}</Project>
  <Name>Orchard.Framework</Name>
  <Private>false</Private>
</ProjectReference>

",3
wx-chevalier/Web-Series,None,"
中文版本 | English Version
现代 Web 开发基础与工程实践
Copyright © 2018 王下邀月熊
Web 开发，入门易，深度难，分为初窥门径、登堂入室、融会贯通等阶段。本仓库存放 ITCS 技术体系与知识图谱-Web 前端相关领域的 Web 开发基础与工程实践的相关博客、示例代码与开源项目、整理成的系列书籍等内容；目前为了更好地体系化阅读，笔者将所有的内容规整到了不同的系列文章 / 书籍中；代码等实践模板请参考 fe-boilerplate。

Nav | 导航
如果您想快速检索，那么建议前往 xCompass/alfred-sg 进行交互式地检索、查找需要的文章/链接/书籍/课程。
如果您对于 JavaScript 基础语法尚不完全了解，那么建议您首先浏览现代 JavaScript 语法基础与工程实践或者 JavaScript-CheatSheet 以了解基础的 JavaScript 语法及实践应用。
如果您想快速地了解 Web 开发实践，或者是想查阅某些清单，那么建议您前往 Awesome-CheatSheets/Web；或者从 Specials 开始阅读，它会包含 Web 开发简史与变迁，数据流驱动的界面，模块化与组件化，工具化与工程化，前后端分离与全栈架构，微前端与大前端，运行机制与性能优化，等内容。
接下来，您可以选择以下章节中感兴趣的模块进行深度阅读：


基础篇: 对于 HTML、CSS、DOM 等 Web 开发中涉及的基础知识与理念的总结介绍。


工程实践篇: 构建工具，测试，安全，WebAssembly。


架构优化篇: 组件化，状态管理，性能优化，PWA。


React 篇：近年来前端领域百花齐放，各种技术方案争妍斗艳，各领风骚。本书立足于其中的佼佼者 React，深入浅出的介绍 React、Webpack 、 ES6、Redux 、 MobX 等常见前端开发工具与开发库的用法，帮助初学者能够迅速成为一名合格前端工程师。而本书也不仅局限于工具使用的层面，探寻各种技术方案背后蕴含的设计思想与架构模式，从前端工程化的角度讨论前端开发者在进阶过程中需要掌握的工程实践、模块化与组件化、质量保障、性能优化等知识要点。最终帮助开发者在前端开发中能够因地制宜的指定合理方案，以尽可能快的速度实现可信赖的产品。


Vue 篇：本部分目前正逐步启动，笔者的初衷是希望能够保证本书章节与 React 与前端工程化实践尽可能一致，从而更方便地去介绍不同技术栈下相通的设计理念；目前本书的目录只是拷贝自 React 与前端工程化实践，未来笔者会逐步完善。


Preface | 前言
这是一个最好的时代，也是最坏的时代，我们亲身经历着激动人心的变革，也往往会陷入选择的迷茫。随着浏览器版本的革新与硬件性能的提升，Web 前端开发进入了高歌猛进，日新月异的时代，无数的前端开发框架、技术体系争妍斗艳，让开发者们陷入困惑，乃至于无所适从。特别是随着现代 Web 前端框架(Angular、React、Vue.js)的出现，JavaScript、CSS、HTML 等语言特性的提升，工程化、跨平台、大前端等理论概念的提出，Web 前端开发的技术栈、社区也是不断丰富完善。
任何一个编程生态都会经历三个阶段，首先是原始时期，由于需要在语言与基础的 API 上进行扩充，这个阶段会催生大量的辅助工具。第二个阶段，随着做的东西的复杂化，需要更多的组织，会引入大量的设计模式啊，架构模式的概念，这个阶段会催生大量的框架。第三个阶段，随着需求的进一步复杂与团队的扩充，就进入了工程化的阶段，各类分层 MVC，MVP，MVVM 之类，可视化开发，自动化测试，团队协同系统；这个阶段会出现大量的小而美的库。
Web 前端开发可以追溯于 1991 年蒂姆·伯纳斯-李公开提及 HTML 描述，而后 1999 年 W3C 发布 HTML4 标准，这个阶段主要是 B/S 架构，没有所谓的前端开发概念，网页只不过是后端工程师的顺手之作，服务端渲染是主要的数据传递方式。接下来的几年间随着互联网的发展与 REST 等架构标准的提出，前后端分离与富客户端的概念日渐为人认同，我们需要在语言与基础的 API 上进行扩充，这个阶段出现了以 jQuery 为代表的一系列前端辅助工具。
2009 年以来，智能手机开发普及，移动端大浪潮势不可挡，SPA 单页应用的设计理念也大行其道，相关联的前端模块化、组件化、响应式开发、混合式开发等等技术需求甚为迫切。这个阶段催生了 Angular 1、Ionic 等一系列优秀的框架以及 AMD、CMD、UMD 与 RequireJS、SeaJS 等模块标准与加载工具，前端工程师也成为了专门的开发领域，拥有独立于后端的技术体系与架构模式。而近两年间随着 Web 应用复杂度的提升、团队人员的扩充、用户对于页面交互友好与性能优化的需求，我们需要更加优秀灵活的开发框架来协助我们更好的完成前端开发。这个阶段涌现出了很多关注点相对集中、设计理念更为优秀的框架，譬如 React、Vue.js、Angular 2 等组件框架允许我们以声明式编程来替代以 DOM 操作为核心的命令式编程，加快了组件的开发速度，并且增强了组件的可复用性与可组合性。而遵循函数式编程的 Redux 与借鉴了响应式编程理念的 MobX 都是非常不错的状态管理辅助框架，辅助开发者将业务逻辑与视图渲染剥离，更为合理地划分项目结构，更好地贯彻单一职责原则与提升代码的可维护性。在项目构建工具上，以 Grunt、Gulp 为代表的任务运行管理与以 Webpack、Rollup、JSPM 为代表的项目打包工具各领风骚，帮助开发者更好的搭建前端构建流程，自动化地进行预处理、异步加载、Polyfill、压缩等操作。
版权
 
笔者所有文章遵循 知识共享 署名 - 非商业性使用 - 禁止演绎 4.0 国际许可协议，欢迎转载，尊重版权。如果觉得本系列对你有所帮助，欢迎给我家布丁买点狗粮(支付宝扫码)~

",4356
InfiniteAdventures/ia-trilogy,HTML,"Infinite Adventures: zweiteiliger Doppelband
»Alle aussteigen, wir klauen jetzt einen A380.«
Eine frei lizenzierte Romanserie über ein witzig-verrücktes Verbrecher-Quartett, schräge Raumpiraten, böse Diktatoren und die Benutzung eines Helikopters als Raumschiff. Open Source, geschrieben in LaTeX (LuaTeX).  🍕 💡 💣 💻
German: Freely licensed novel series about absurd crime and science fiction, featuring crazy space pirates, evil dictators and a modified helicopter used as a spaceship. Open source, written in LaTeX (LuaTeX).
",3
GallVp/visualEEG,MATLAB,"visualEEG

visualEEG is a MATLAB/GUIDE based toolbox which can be used for very basic analysis of EEG/EMG data. The goal of this project is to develop a single window interactive tool.


Fig 1. The main GUI window of visualEEG which allows an interactive processing of data.

Source Code and Tutorials related to Publications

README for ""Automated Labeling of Movement-Related Cortical Potentials using Segmented Regression"", IEEE Transactions on Neural Systems and Rehabilitation Engineering, doi: 10.1109/TNSRE.2019.2913880

Compatibility
Currently visualEEG is being developed on macOS Mojave, MATLAB 2017b. However, in the past, it has been tested to work on OSX El Capitan MATLAB R2015b; Windows 7, MATLAB 2014a; and Linux (Ubuntu LTS 14.04), MATLAB 2012b.
Installation
Latest Version with Source Code (Matlab Required)

Clone the git repository using git. Or, download a compressed copy here.

$ git clone https://github.com/GallVp/visualEEG


From MATLAB file explorer, enter the visualEEG folder by double clicking it. Type visualEEG in the MATLAB command window and hit enter to run.

Binary Versions (Matlab Not Required)
Version 2.1: Windows installer, Mac installer
Wiki
A detailed documentation with tutorials is available here.
Third Party Libraries
visualEEG uses following third party libraries. The licenses for these libraries can be found next to source files in their respective libs/thirdpartlib folders.

barwitherr Copyright (c) 2014, Martina Callaghan. Source is available here.
export_fig Copyright (c) 2014, Oliver J. Woodford, Yair M. Altman. Source is available here.
pooledmeanstd Copyright (c) 2012, R P. Source is available here.

",2
Lombiq/Arithmetics,C#,"Unum - Proof of concept readme
This project was developed as part of Hastlayer, the .NET HLS tool that converts .NET programs into equivalent logic hardware implementations.
Its goal is to implement a Unum proof of concept: the number type and an example using it, all transformable with Hastlayer.
The project's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/arithmetics (Mercurial repository)
https://github.com/Lombiq/Arithmetics (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub. Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
About Unum
Unum is a new number format invented by Dr. John L. Gustafson that can be used to store any number with exact precision (or known error). It can be used to achieve better range and accuracy than IEEE floating point formats while eliminating the algebraic errors that the IEEE floats are prone to.
For more about its advantages see: http://ubiquity.acm.org/article.cfm?id=2913029.
",6
koji/icTrainer,Python,"



icTrainer is a python module which allows users to train image classifier easily
Basically, this module is for python3
Install
$ pip install ictrainer

Also you can install manually.
clone repo
$ git https://github.com/koji/icTrainer.git
$ cd icTrainer/ictrainer
$ python setup.py install

How to Use
In this gude, we will create a dog/cat image classifier.
1.Collect Images
https://icrawler.readthedocs.io/en/latest/
$ ictrainer --mode collect --keyword dog -n 250
$ ictrainer --mode collect --keyword cat -n 250

You'll have dogs & cats images under dataset folder.
2. Resize images
In this step, we will change all images size for training. The current input size must be 320 x 180(required).
This step may be mess up images you collected, so you need to check all images manually. In the furture, there will be a function that save your time.
$ ictrainer --mode resize --target dog
$ ictrainer --mode resize --target cat

For people want to use resize mode for other thing, you can use reize images with the following command.
The folder structure should be the same the above.
$ ictrainer --mode resize --target cat --image_width 480 --image_height 320

3.Create folders for classes
This step, we'll need to create folders and distribute images to train & validation folder.
3-1. create folders
Create a couple of folders under dataset.
This step will be automated in the future.
 dataset
    ├── train
    │   ├── cat
    │   └── dog
    └── val
        ├── cat
        └── dog

3-2. distribute images
Move images we got via image collect mode. In this case, probably we have 250 images for each other.
We will put 225 images for train and 25 images for validation so that train/dog has 225 images and validation/dog has 25 images. The cats should be the same.
4.Train Images
There are some options we need to put. The most important one is --classes which will be labels. In this case, we have dog & cat, so we need to put them as classes.
--batch: batch size default 16
--epoch: epoch default 30
--mname: output model name
--lr: learning rate default 1e-3
momentum: mementum default 0.9
We will use default settings.
$ ictrainer --mode train --classes ""cat"" ""dog"" --mname ""dogAndcat_""

5 Face detection
From 0.2.0 ictrainer allows you to use face detection. The command is following. This function is using OpenCV Cascade filter to detect front faces. When you successfully run this command, ictrainer creates output folder and there are faces and something which means still you need check all image by yourself.
--mode face
--target target folder
$ ictrainer --mode face --target dataset/celeb

video
image collecting mode
https://www.youtube.com/watch?v=k5r_xrW_cxE
pre-train model
smart device
https://github.com/koji/icTrainer/blob/master/model/smartdevice_epoch30.h5
classes = ['echo', 'echoplus', 'echoshow', 'googlehome', 'googlehomemini', 'nest']   

",4
nexus-uw/make-account-green,Shell,"make-account-green
Purpose
This is a lazy repo that will generate public activity for this account to game the contributions graph.
Why
Because any system can be gamed
Notes
(idea forked from http://zeke.sikelianos.com/npm-and-github-automation-with-heroku/)
",3
SourMesen/Mesen,C++,"Mesen
Mesen is a cross-platform NES/Famicom emulator for Windows & Linux built in C++ and C#.
If you want to support this project, please consider making a donation:

Website (https://www.mesen.ca)
Documentation (https://www.mesen.ca/docs)
Development Builds
Development builds of the latest commit are available from Appveyor. For stable release builds, see the Releases section below.
Warning: These are development builds and may be unstable. Using them may also increase the chances of your settings being corrupted, or having issues when upgrading to the next official release. Additionally, these builds are currently not optimized via PGO and will typically run 20-30% slower than the official release builds.
Windows: 
Linux: 
Releases
Windows
The latest version is available on the website.  Older releases are available from the releases tab on GitHub.
Ubuntu
The official releases (same downloads as the Windows builds above) also contain the Linux version of Mesen, built under Ubuntu 16 - you should be able to use that in most cases if you are using Ubuntu.
The Linux version is a standard .NET executable file and requires Mono to run - you may need to configure your environment to allow it to automatically run .exe files through Mono, or manually run Mesen by using mono (e.g: ""mono Mesen.exe"").
The following packages need to be installed to run Mesen:

mono-complete
libsdl2-2.0
gnome-themes-standard

Note: Mono 5.18 or higher is recommended, some older versions of Mono (e.g 4.2.2) have some stability and performance issues which can cause crashes and slow down the UI.
The default Mono version in Ubuntu 18.04 is 4.6.2 (which also causes some layout issues in Mesen).  To install the latest version of Mono, follow the instructions here: https://www.mono-project.com/download/stable/#download-lin
Arch Linux
Packages are available here: https://aur.archlinux.org/packages/mesen
Roadmap
Things that may or may not be added in the future, in no particular order:

Support for more UNIF boards and more NES/Famicom input devices
Shaders
TAS editor

Compiling
See COMPILING.md
License
Mesen is available under the GPL V3 license.  Full text here: http://www.gnu.org/licenses/gpl-3.0.en.html
Copyright (C) 2014-2019 M. Bibaud
This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.
This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.
You should have received a copy of the GNU General Public License
along with this program.  If not, see http://www.gnu.org/licenses/.
",411
sanderhelleso/project_bio,JavaScript,"project_bio 🤳🏼
Project Bio is a platform currently in development, serving as a ""middleman"" for social media influencers to increase sales of product and grow a bigger audience.






Status
In development
Copyright
Sander Hellesø
",6
webx-top/echo,Go,"Echo
 
Echo is a fast and unfancy web framework for Go (Golang). Up to 10x faster than the rest.
This package need >= go 1.9
Features

Optimized HTTP router which smartly prioritize routes.
Build robust and scalable RESTful APIs.
Run with standard HTTP server or FastHTTP server.
Group APIs.
Extensible middleware framework.
Define middleware at root, group or route level.
Handy functions to send variety of HTTP responses.
Centralized HTTP error handling.
Template rendering with any template engine.
Define your format for the logger.
Highly customizable.

Quick Start
Installation
$ go get github.com/webx-top/echo
Hello, World!
Create server.go
package main

import (
	""net/http""
	""github.com/webx-top/echo""
	""github.com/webx-top/echo/engine/standard""
)

func main() {
	e := echo.New()
	e.Get(""/"", func(c echo.Context) error {
		return c.String(""Hello, World!"", http.StatusOK)
	})
	e.Run(standard.New("":1323""))
}
Start server
$ go run server.go
Browse to http://localhost:1323 and you should see
Hello, World! on the page.
Routing
e.Post(""/users"", saveUser)
e.Get(""/users/:id"", getUser)
e.Put(""/users/:id"", updateUser)
e.Delete(""/users/:id"", deleteUser)
Path Parameters
func getUser(c echo.Context) error {
	// User ID from path `users/:id`
	id := c.Param(""id"")
}
Query Parameters
/show?team=x-men&member=wolverine
func show(c echo.Context) error {
	// Get team and member from the query string
	team := c.Query(""team"")
	member := c.Query(""member"")
}
Form application/x-www-form-urlencoded
POST /save



name
value




name
Joe Smith


email
joe@labstack.com



func save(c echo.Context) error {
	// Get name and email
	name := c.Form(""name"")
	email := c.Form(""email"")
}
Form multipart/form-data
POST /save



name
value




name
Joe Smith


email
joe@labstack.com


avatar
avatar



func save(c echo.Context) error {
	// Get name and email
	name := c.Form(""name"")
	email := c.Form(""email"")

	//------------
	// Get avatar
	//------------
	_, err := c.SaveUploadedFile(""avatar"",""./"")
	return err
}
Handling Request

Bind JSON or XML payload into Go struct based on Content-Type request header.
Render response as JSON or XML with status code.

type User struct {
	Name  string `json:""name"" xml:""name""`
	Email string `json:""email"" xml:""email""`
}

e.Post(""/users"", func(c echo.Context) error {
	u := new(User)
	if err := c.MustBind(u); err != nil {
		return err
	}
	return c.JSON(u, http.StatusCreated)
	// or
	// return c.XML(u, http.StatusCreated)
})
Static Content
Server any file from static directory for path /static/*.
e.Use(mw.Static(&mw.StaticOptions{
	Root:""static"", //存放静态文件的物理路径
	Path:""/static/"", //网址访问静态文件的路径
	Browse:true, //是否在首页显示文件列表
}))
Middleware
// Root level middleware
e.Use(middleware.Log())
e.Use(middleware.Recover())

// Group level middleware
g := e.Group(""/admin"")
g.Use(middleware.BasicAuth(func(username, password string) bool {
	if username == ""joe"" && password == ""secret"" {
		return true
	}
	return false
}))

// Route level middleware
track := func(next echo.HandlerFunc) echo.HandlerFunc {
	return func(c echo.Context) error {
		println(""request to /users"")
		return next.Handle(c)
	}
}
e.Get(""/users"", func(c echo.Context) error {
	return c.String(""/users"", http.StatusOK)
}, track)
Cookie
e.Get(""/setcookie"", func(c echo.Context) error {
	c.SetCookie(""uid"",""1"")
	return c.String(""/setcookie: uid=""+c.GetCookie(""uid""), http.StatusOK)
})
Session
...
import (
	...
	""github.com/webx-top/echo/middleware/session""
	//boltStore ""github.com/webx-top/echo/middleware/session/engine/bolt""
	cookieStore ""github.com/webx-top/echo/middleware/session/engine/cookie""
)
...
sessionOptions := &echo.SessionOptions{
	Engine: `cookie`,
	Name:   `SESSIONID`,
	CookieOptions: &echo.CookieOptions{
		Path:     `/`,
		Domain:   ``,
		MaxAge:   0,
		Secure:   false,
		HttpOnly: true,
	},
}

cookieStore.RegWithOptions(&cookieStore.CookieOptions{
	KeyPairs: [][]byte{
		[]byte(`123456789012345678901234567890ab`),
	},
})

e.Use(session.Middleware(sessionOptions))

e.Get(""/session"", func(c echo.Context) error {
	c.Session().Set(""uid"",1).Save()
	return c.String(fmt.Sprintf(""/session: uid=%v"",c.Session().Get(""uid"")))
})
Websocket
...
import (
	...
	""github.com/admpub/websocket""
	""github.com/webx-top/echo""
	ws ""github.com/webx-top/echo/handler/websocket""
)
...

e.AddHandlerWrapper(ws.HanderWrapper)

e.Get(""/websocket"", func(c *websocket.Conn, ctx echo.Context) error {
	//push(writer)
	go func() {
		var counter int
		for {
			if counter >= 10 { //测试只推10条
				return
			}
			time.Sleep(5 * time.Second)
			message := time.Now().String()
			ctx.Logger().Info(`Push message: `, message)
			if err := c.WriteMessage(websocket.TextMessage, []byte(message)); err != nil {
				ctx.Logger().Error(`Push error: `, err.Error())
				return
			}
			counter++
		}
	}()

	//echo
	ws.DefaultExecuter(c, ctx)
	return nil
})
More...
Sockjs
...
import (
	...
	""github.com/webx-top/echo""
	""github.com/admpub/sockjs-go/sockjs""
	ws ""github.com/webx-top/echo/handler/sockjs""
)
...

options := ws.Options{
	Handle: func(c sockjs.Session) error {
		//push(writer)
		go func() {
			var counter int
			for {
				if counter >= 10 { //测试只推10条
					return
				}
				time.Sleep(5 * time.Second)
				message := time.Now().String()
				log.Info(`Push message: `, message)
				if err := c.Send(message); err != nil {
					log.Error(`Push error: `, err.Error())
					return
				}
				counter++
			}
		}()

		//echo
		ws.DefaultExecuter(c)
		return nil
	},
	Options: &sockjs.DefaultOptions,
	Prefix:  ""/websocket"",
}
options.Wrapper(e)
More...
Other Example
package main

import (
	""net/http""

	""github.com/webx-top/echo""
	// ""github.com/webx-top/echo/engine/fasthttp""
	""github.com/webx-top/echo/engine/standard""
	mw ""github.com/webx-top/echo/middleware""
)

func main() {
	e := echo.New()
	e.Use(mw.Log())

	e.Get(""/"", func(c echo.Context) error {
		return c.String(""Hello, World!"")
	})
	e.Get(""/echo/:name"", func(c echo.Context) error {
		return c.String(""Echo "" + c.Param(""name""))
	})
	
	e.Get(""/std"", func(w http.ResponseWriter, r *http.Request) {
		w.Write([]byte(`standard net/http handleFunc`))
		w.WriteHeader(200)
	})

	// FastHTTP
	// e.Run(fasthttp.New("":4444""))

	// Standard
	e.Run(standard.New("":4444""))
}
See other examples...
Middleware list



Middleware
Import path
Description




BasicAuth
github.com/webx-top/echo/middleware
HTTP basic authentication


BodyLimit
github.com/webx-top/echo/middleware
Limit request body


Gzip
github.com/webx-top/echo/middleware
Send gzip HTTP response


Secure
github.com/webx-top/echo/middleware
Protection against attacks


CORS
github.com/webx-top/echo/middleware
Cross-Origin Resource Sharing


CSRF
github.com/webx-top/echo/middleware
Cross-Site Request Forgery


Log
github.com/webx-top/echo/middleware
Log HTTP requests


MethodOverride
github.com/webx-top/echo/middleware
Override request method


Recover
github.com/webx-top/echo/middleware
Recover from panics


HTTPSRedirect
github.com/webx-top/echo/middleware
Redirect HTTP requests to HTTPS


HTTPSWWWRedirect
github.com/webx-top/echo/middleware
Redirect HTTP requests to WWW HTTPS


WWWRedirect
github.com/webx-top/echo/middleware
Redirect non WWW requests to WWW


NonWWWRedirect
github.com/webx-top/echo/middleware
Redirect WWW requests to non WWW


AddTrailingSlash
github.com/webx-top/echo/middleware
Add trailing slash to the request URI


RemoveTrailingSlash
github.com/webx-top/echo/middleware
Remove trailing slash from the request URI


Static
github.com/webx-top/echo/middleware
Serve static files


MaxAllowed
github.com/webx-top/echo/middleware
MaxAllowed limits simultaneous requests; can help with high traffic load


RateLimit
github.com/webx-top/echo/middleware/ratelimit
Rate limiting HTTP requests


Language
github.com/webx-top/echo/middleware/language
Multi-language support


Session
github.com/webx-top/echo/middleware/session
Sessions Manager


JWT
github.com/webx-top/echo/middleware/jwt
JWT authentication


Markdown
github.com/webx-top/echo/middleware/markdown
Markdown rendering


Render
github.com/webx-top/echo/middleware/render
HTML template rendering


ReverseProxy
github.com/webx-top/reverseproxy
Reverse proxy



Handler Wrapper list



Wrapper
Import path
Description




Websocket
github.com/webx-top/echo/handler/websocket
Example


Sockjs
github.com/webx-top/echo/handler/sockjs
Example


Oauth2
github.com/webx-top/echo/handler/oauth2
Example


Pprof
github.com/webx-top/echo/handler/pprof
-


MVC
github.com/webx-top/echo/handler/mvc
Example



Credits

Vishal Rana - Author
Hank Shen - Author
Nitin Rana - Consultant
Contributors

License
Apache 2
",17
vic64/nlpcraft,Scala,"

 
 

Overview
NLPCraft is an open source library for adding a natural language interface to any applications. Think Amazon
Alexa that is developer friendly, works with any private data source, has no hardware or software lock-in while
giving you more NLP powers:

Download and Maven/Grape/Gradle/SBT instructions
Documentation, Javadoc, and REST APIs
Example data models
Licensed under Apache 2.0 License with Commons Clause.

For any questions, feedback or suggestions:

Send us a note at support@nlpcraft.org
Post a question at Stack Overflow using nlpcraft tag
If you have a bug or an idea open new issue here on GitHub.

Copyright
Copyright (C) 2013-2019 DataLingvo Inc. All Rights Reserved.
",15
user1121114685/koolproxyR_rule_list,None,"koolproxyR_rule_list
本项目只是收集kpr贡献规则。
此项目任何人都可以自由提交修改，此列表将帮助更多人使用Kpr去广告。
",5
Lombiq/Orchard-Login-as-Anybody,C#,"Login as Anybody readme
Project Description
Orchard module for administrators to be able to log in as any user.
Documentation
After enabling the module you'll see a new tab under Users. You can log in as any registered user there. This is useful if you want to see how your Orchard app behaves for certain users.
This feature is only available to site owners, thus it's not way to get around security.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/orchard-login-as-anybody (Mercurial repository)
https://github.com/Lombiq/Orchard-Login-as-Anybody (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",2
dream-frame/Dream-Frame,CSS,"Dream-Frame

Bring a sexy translucent look to Discord with customizable options! BetterDiscord theme.
Downloads

Download for Powercord

Don't have a powerful computer? Try Dream Frame Lite!
Helpful Links

How to install Powercord: https://github.com/powercord-org/powercord/wiki/Installation

Donation
Donate: https://Paypal.me/CorbsEditor
Exploring Dream Frame

Why Dream Frame?
Dream Frame offers many options like changing colors, icons, sizes, and so on. There are more options are come, more advanced options to come as well if you're into that. This theme is the best translucent look to Discord, nothing can beat it!
All of the Options Offered by Dream Frame
The creator tries to add as many options as possible and without failing thanks to the fallback system from v2.2
The following is offered as of now:

Background
Background blur
Background opacity
Background scaling
Background Rotation
Emoji menu height
Emoji size
Home icon
Avatar in chat size
Status color changing
Accent text, accent color
Font, font size
Border radius options
Custom background for your own profile
Server indicator color
Titlebar icons(Windows)
Titlebar button colors(macOS)

More is to come!
",4
muesli/telephant,QML,"Telephant!
A lightweight but modern Mastodon client, written in Go & QML.

Features

 Live feed via Mastodon's Streaming API
 Multi pane support
 Linux/macOS/Windows (Android & iOS should be working, but aren't tested yet)
 Media previews
 Shortened URL resolving
 System notifications
 Multiple accounts (work-in-progress)
 Support for more networks

Installation
Packages & Installers

Windows 64bit
Linux Static 64bit

From Source
Make sure you have a working Go environment (Go 1.8 or higher is required).
See the install instructions.
You will also need Qt5 and its development headers installed.
Dependencies
Before you can build Telephant you need to install the Go/Qt bindings.
Qt5 dependencies (Ubuntu example)
apt-get --no-install-recommends install build-essential libglib2.0-dev libglu1-mesa-dev libpulse-dev
apt-get --no-install-recommends install libqt*5-dev qt*5-dev qt*5-doc-html qml-module-qtquick*

Qt Bindings
export QT_PKG_CONFIG=true
go get -u -v github.com/therecipe/qt/cmd/...
$(go env GOPATH)/bin/qtsetup -test=false

Building Telephant
mkdir -p $(go env GOPATH)/src/github.com/muesli
cd $(go env GOPATH)/src/github.com/muesli
git clone https://github.com/muesli/telephant.git

cd telephant
go get -u -v
$(go env GOPATH)/bin/qtdeploy build desktop .

Within a Docker container
Follow the build instructions above, but instead of the last command, run:
$(go env GOPATH)/bin/qtdeploy -docker build linux

Run it
./deploy/linux/telephant


Development



",85
wx-chevalier/Distributed-Infrastructure-Series,None,"
深入浅出分布式基础架构
深入浅出分布式基础架构是笔者归档自己，在学习与实践软件分布式架构过程中的，笔记与代码的仓库；主要包含分布式计算、分布式系统、数据存储、虚拟化、网络、操作系统等几个部分。本部分详细的基础架构请参考笔者在 2016: 我的技术体系结构图一文中的描述；本仓库目前包含的主要内容分为开源项目与各个技术领域的文章。

建议前往 xCompass/alfred-sg 交互式地检索、查找需要的文章/链接/书籍/课程，或者直接浏览本仓库的目录以了解更多内容。
Nav | 导航


Linux 篇


分布式计算篇


虚拟化与容器调度篇


分布式存储篇


MySQL 篇


Redis 篇


Preface | 前言
版权


笔者所有文章遵循 知识共享 署名-非商业性使用-禁止演绎 4.0 国际许可协议，欢迎转载，尊重版权。如果觉得本系列对你有所帮助，欢迎给我家布丁买点狗粮(支付宝扫码)~

更多相关信息请查看关于页面。
",411
bounswe/bounswe2019group9,Python,"Who are we ?
We are Group #9 in CMPE 352 course offered in Bogazici University in Spring , 2019. You can visit our  Wiki Page for more details.

İrem Uğuz (Communicator)
İbrahim Kaplan
Arda Budak
Burhan Akkuş
Egemen Göl
Emirhan Yasin Çetin
Gamze Gülbahar
Halit Özsoy
Ahmet Gedemenli
Ali Ramazan Mert

",4
bounswe/bounswe2019group9,Python,"Who are we ?
We are Group #9 in CMPE 352 course offered in Bogazici University in Spring , 2019. You can visit our  Wiki Page for more details.

İrem Uğuz (Communicator)
İbrahim Kaplan
Arda Budak
Burhan Akkuş
Egemen Göl
Emirhan Yasin Çetin
Gamze Gülbahar
Halit Özsoy
Ahmet Gedemenli
Ali Ramazan Mert

",4
cbuijs/shallalist,None,"shallalist
ShallaList unpacked.
http://www.shallalist.de/
",5
mrhso/Cangjie_Note,HTML,"警告：嚴禁擅自將此項目內含資料盜到別處！
Cangjie_Note
简化字版说明
此倉頡筆記會揭示倉頡不為人知的一些細節，同時也屬於「倉頡 Project」中的冰山一角。
主要內容講解
倉頡內碼相關
講述倉頡內碼的轉換原理。
主觀人士倉頡探案集（我有問題問紅蓮）
收錄老楊詢問沈奶奶的一系列信件。
這一欄目比較主觀，請選擇性觀看。
倉頡規則相關
講述倉頡的取碼等規則。
官方解說
官方對於倉頡的一些解說。
官方資料
由官方手中得到的一些資料。
倉頡番外
民間倉頡的一些小細節。
其餘「倉頡 Project」項目

Cangjie3-Plus
Cangjie5
Cangjie_DIY

Binary 資料
有些資料屬於 Binary，不便於在 Git 倉庫管理。
但是我們會盡力將其文本化，因此一般沒有必要去獲取。
列表

文傳時代

《文傳內碼對照表》正文本化。
《五六倉頡對照內碼表》正文本化。
《六代取碼規則》已文本化。



遠程倉庫總表

GitHub
GitLab
碼雲
Bitbucket
Visual Studio Team Services

友情連接

「倉頡之友·馬來西亞」論壇
「天蒼人頡」論壇
「倉頡輸入法」QQ 群組 30476878
「倉頡輸入法」Freenode IRC 頻道 #CJDFH
「倉頡輸入法」Telegram 群組 @changjei

項目官方討論群組

QQ 群組 609486016
Freenode IRC 頻道 #ezinput
Telegram 群組 @ezinput

",2
jihuun/web_crawlers,HTML,"web_crawlers
Variety of scripts for web crawling
",4
lanternpro/intro,None,"intro
",4
lannonbr/vscode-issue-tracker,JavaScript,"VS Code Issue Tracker

The VS Code Issue Tracker is a visualization of the issue count on the Microsoft/vscode repository over time.
The initial inspiration for this was to track the progress of microsoft/vscode#58336.
You can visit the site live here: vscode-issue-tracker.netlify.com
Webstack Description
The basis of the issue tracker is a serverless backend combined with a static frontend.
The backend consists of an AWS Lambda function which sends a query to the Github V4 GraphQL API once an hour to see the current number of issues on the repository. That is stored in a Cloud Firestore database.
Then, I have a scheduled job to pull the last 3 days as well as last month of entries for the recent and monthly graphs. It saves these entries to a local JS file and commits it to the repo once an hour through Git.
Finally, I wrote a simple frontend using C3.js to display the two graphs on a page. This is then committed to GitHub here and deployed to Netlify across their Application Delivery Network.
Future plans include making this more generalized so anyone can spin up a very similar project with ease.
",91
kentcdodds/bookshelf,JavaScript,"Build a ReactJS App - Bookshelf

👋 hi there! My name is Kent C. Dodds and this is the
source material for
Build a ReactJS App!




 
Pre-Workshop Instructions/Requirements
In order for us to maximize our efforts during the workshop, please do the
following:

 Setup the project (follow the setup instructions below) (~5 minutes)
 Install and setup Zoom on the computer you will be
using (~5 minutes)
 Watch The Beginner's Guide to React
(available free on Egghead.io), or have the equivalent experience (77
minutes)
 Watch my talk
Why React Hooks
(35 minutes)
 Go through my
Learn React Hooks Workshop, or
have the equivalent basic experience of using hooks. You should be
experienced with useState, useEffect, and useRef.
 Go through my
Advanced React Hooks,
or have the equivalent basic experience of using advanced hooks. You
should be experienced with useContext, useReducer, useMemo, and
useCallback.

The more prepared you are for the workshop, the better it will go for you.
System Requirements

git v2 or greater
NodeJS v8 or greater
yarn v1 or greater (or npm v6 or greater)

All of these must be available in your PATH. To verify things are set up
properly, you can run this:
git --version
node --version
yarn --version # or npm --version
If you have trouble with any of these, learn more about the PATH environment
variable and how to fix it here for windows or
mac/linux.
Setup
You should be able to work through the entire workshop in the browser. This is
actually the recommended approach as it requires absolutely no setup whatsoever.
Go to this codesandbox
and you should be good to go.

If you'd rather be able to work through the workshop on your own computer, then
follow the following instructions.
After you've made sure to have the correct things (and versions) installed, you
should be able to just run a few commands to get set up:
# If you were given instructions for a specific branch to use, then use this command
# git clone --single-branch --branch <branchname> https://github.com/kentcdodds/bookshelf.git

# otherwise, this is fine:
git clone https://github.com/kentcdodds/bookshelf.git

# then do this:
cd bookshelf
npm run setup --silent
This may take a few minutes. It will ask you for your email. This is
optional and just automatically adds your email to the links in the project to
make filling out some forms easier If you get any errors, please read through
them and see if you can find out what the problem is. You may also want to look
at Troubleshooting. If you can't work it out on your own
then please file an issue and provide all the output from the
commands you ran (even if it's a lot).
Running the app
To get the app up and running (and really see if it worked), run:
npm start
This should start up your browser. If you're familiar, this is a standard
react-scripts application.
You can also open
the deployment of the app on Netlify.
Running the tests
npm test
This will start Jest in watch mode. Read the
output and play around with it.
Helpful Emoji 🐨 💰 💯 🦉 📜 💣 🚨
Each exercise has comments in it to help you get through the exercise. These fun
emoji characters are here to help you.

Kody the Koala Bear 🐨 will tell you when there's something specific you
should do
Marty the Money Bag 💰 will give you specific tips (and sometimes code)
along the way
Hannah the Hundred 💯 will give you extra challenges you can do if you
finish the exercises early.
Olivia the Owl 🦉 will give you useful tidbits/best practice notes and a
link for elaboration and feedback.
Dominic the Document 📜 will give you links to useful documentation
Berry the Bomb 💣 will be hanging around anywhere you need to blow stuff
up (delete code)
Alfred the Alert 🚨 may occasionally show up in the test failures with
potential explanations for why the tests are failing.

Troubleshooting

""npm run setup"" command not working
Here's what the setup script does. If it fails, try doing each of these things
individually yourself:
# verify your environment will work with the project
node ./scripts/verify

# install dependencies
npm install

# verify the project is ready to run
npm run lint
npm run test:coverage
npm run build

If any of those scripts fail, please try to work out what went wrong by the
error message you get. If you still can't work it out, feel free to open an
issue with all the output from that script. I will try to help if I
can.



  ""Error: ENOSPC: System limit for number of file watchers reached"" when running
  tests

Try increasing your system's file watchers limit:
echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf && sudo sysctl -p


Read more about what’s happening at
https://github.com/guard/listen/wiki/Increasing-the-amount-of-inotify-watchers#the-technical-details


Contributors
Thanks goes to these wonderful people
(emoji key):
Kent C. Dodds💻 📖 🚇 ⚠️Vojta Holik🎨 💻
This project follows the
all-contributors
specification. Contributions of any kind welcome!
License
This material is available for private, non-commercial use under the
GPL version 3. If you
would like to use this material to conduct your own workshop, please contact me
at kent@doddsfamily.us
",16
hafen/htmlwidgetsgallery,CSS,"htmlwidgets gallery
This repository serves the htmwidgets gallery.
Adding a widget
If you are a widget author, you can register your widget by doing the following:

Fork this repository.
Create a png thumbnail of an interesting plot from your widget that will look good on a retina screen at 350x300 pixels and put this file in the images directory of this repository.
Add an entry for your widget in the _config.yml file of this repository with the meta data for your widget (copy another entry and modify).  Please see below for guidance on the meta data.
Push your changes and create a pull request.  To ensure the quality of widgets added to the registry and consistency in how they are displayed, you should expect some amount of discussion during your pull request.

Meta data requirements:

name: the actual name of the R package (required)
thumbnail: location of the thumbnail (required, standard is images/ghuser-ghrepo.png)
url: url to the desired landing page you'd like people to first see for the widget (the widget's home page, a vignette, or as a final resort, if not specified, the widget's github page)
jslibs: a comma separated list of javascript library names that the widget depends on, with markdown links to the home pages of the libraries
ghuser: the github user/org where the github repository for the widget resides (required)
ghrepo: the github repository name where the widget resides (required)
tags: comma separated list (with no spaces) of tags that describe the widget - see other widget's tags for ideas
cran: true if the package is on CRAN, else false
examples: url or list of urls of examples (blog posts, gists, vignettes)
ghauthor: the github handle for the primary author of the widget
short: a short (preferably one sentence) description of the package that will be displayed in limited space under the widget thumbnail in the gallery - ideally should be more than ""An htmlwidget interface to library x"" as that is obvious from jslib, etc. - instead, should describe what you can do with the widget using library x
description: a longer form description

",54
StaceyWhitmore/color-organizer,JavaScript,"Color organizer
To improve my workflow as a UI/UX developer I thought to make this start of a color organizer React app to help me to organize swatches of my favorite colors and color schemes by their hex and rgb values (Similar to what you might see in a stack of Pantone color swatches used by print a maker--only in digital form).
",3
briancrink/dotfiles,HTML,"




Fork
  , review, 🚀 to ~/. 

Setup


Set up a New Machine


Set up Web Workflows


Set up Social Feeds


Migrate Music Collections


Jailbreak Apple Devices


Tools

Autodot
Dotshare.it
Desktoppr
Fileicon
Homebrew Bundle
Homebrew Quicklook
Homebrew Casks
Homebrew Fonts

Guides

Archlinux Wiki: Dotfiles
GitHub's Unofficial Guide to Dotfiles
How To Use and Modify Dotfiles: Corey Schafer
Getting Started with Dries Vints Dotfiles
Dotfiles are Meant to be Forked
Getting Started with Lars Kappert's Dotfiles
Getting Started with Mohammed Ajmal Siddiqui's Dotfiles: Pt. 1
Pt. 2
Hassan Ali's Guide to Managing Dotfiles
Victor Augusteo's Method to Syncing Dotfiles

Reference

Shell Startup Scripts
The Purpose of Bashrc
Bashrc Loading Order

👨‍💻Community👩‍💻

@ptb's dotfiles
Ben Alman's dotfiles
Cătălin Mariș dotfiles
Dikiaap's dotfiles
Dries Vints dotfiles:
Brewfile
Gianni Chiappetta's dotfiles
Guillermo Caracuel Ruiz
Jan Moesen's dotfiles
Kevin Suttle's dotfiles
Lars Kappert's dotfiles
Lars Kapperts's awesome dotfiles
Lauri ‘Lri’ Ranta's dotfiles
Mathias Bynens dotfiles
Matijs Brinkhuis's dotfiles
Mohammed Ajmal Siddiqui's dotfiles
Nicolas Gallagher's dotfiles
Tom Ryder's dotfiles

",2
conda-forge/openblas-feedstock,Shell,"About openblas
Home: http://www.openblas.net/
Package license: BSD 3-Clause
Feedstock license: BSD 3-Clause
Summary: An optimized BLAS library based on GotoBLAS2 1.13 BSD version.
Current build status

Travis






Appveyor







Azure








VariantStatus

linux_aarch64_target_platformlinux-aarch64






linux_ppc64le_target_platformlinux-ppc64le






linux_target_platformlinux-64






osx_target_platformosx-64






win_c_compilervs2015target_platformwin-64vc14












Current release info



Name
Downloads
Version
Platforms











Installing openblas
Installing openblas from the conda-forge channel can be achieved by adding conda-forge to your channels with:
conda config --add channels conda-forge

Once the conda-forge channel has been enabled, openblas can be installed with:
conda install openblas

It is possible to list all of the versions of openblas available on your platform with:
conda search openblas --channel conda-forge

About conda-forge

conda-forge is a community-led conda channel of installable packages.
In order to provide high-quality builds, the process has been automated into the
conda-forge GitHub organization. The conda-forge organization contains one repository
for each of the installable packages. Such a repository is known as a feedstock.
A feedstock is made up of a conda recipe (the instructions on what and how to build
the package) and the necessary configurations for automatic building using freely
available continuous integration services. Thanks to the awesome service provided by
CircleCI, AppVeyor
and TravisCI it is possible to build and upload installable
packages to the conda-forge
Anaconda-Cloud channel for Linux, Windows and OSX respectively.
To manage the continuous integration and simplify feedstock maintenance
conda-smithy has been developed.
Using the conda-forge.yml within this repository, it is possible to re-render all of
this feedstock's supporting files (e.g. the CI configuration files) with conda smithy rerender.
For more information please check the conda-forge documentation.
Terminology
feedstock - the conda recipe (raw material), supporting scripts and CI configuration.
conda-smithy - the tool which helps orchestrate the feedstock.
Its primary use is in the construction of the CI .yml files
and simplify the management of many feedstocks.
conda-forge - the place where the feedstock and smithy live and work to
produce the finished article (built conda distributions)
Updating openblas-feedstock
If you would like to improve the openblas recipe or build a new
package version, please fork this repository and submit a PR. Upon submission,
your changes will be run on the appropriate platforms to give the reviewer an
opportunity to confirm that the changes result in a successful build. Once
merged, the recipe will be re-built and uploaded automatically to the
conda-forge channel, whereupon the built conda packages will be available for
everybody to install and use from the conda-forge channel.
Note that all branches in the conda-forge/openblas-feedstock are
immediately built and any created packages are uploaded, so PRs should be based
on branches in forks and branches in the main repository should only be used to
build distinct package versions.
In order to produce a uniquely identifiable distribution:

If the version of a package is not being increased, please add or increase
the build/number.
If the version of a package is being increased, please remember to return
the build/number
back to 0.

Feedstock Maintainers

@gillins
@groutr
@isuruf
@jakirkham
@jschueller

",5
grmat/base16-firefox,HTML,"base16-firefox
base16 themes for Mozilla Firefox. Using WebExtensions Themes.

Example screenshot with Tomorrow Night by chriskempson
Building


Populate working directory with sources.yaml


Use a builder, update colour schemes and add base16-firefox to the templates build


Use a builder to generate the theme files inside the firefox folder, e.g.
pybase16 build -o templates/firefox/ext/



Build the WebExtension, either manually or with web-ext (web-ext build -s ext)


Usage

Load extension temporarily via about:debugging or via about:addons (if signed)
or
Install from AMO
Navigate to about:addons, open the preferences for base16 and select your theme.

Known issues
While the WebExtension stores the theme in its local storage, and it should automatically reload the theme on Firefox restart, it will not be represented in the preferences page. The selector will initially always show the first entry.
",6
rooseveltframework/roosevelt,JavaScript,"Roosevelt MVC web framework
  
Roosevelt is a web application development framework based on Express that aims to be the easiest web framework on the Node.js stack to learn and use.
Some notable features:

Minimal boilerplate to get started. Teddy Roosevelt—the most badass President of all-time—curtailed the abuse of monopolists, so there's no way he would ever put up with all the indecipherable boilerplate common to other web frameworks.
Default directory structure is simple, but easily configured.
Concise default MVC architecture.
Uses Teddy HTML templates by default which are much easier to read and maintain than popular alternatives. Can be configured to use any templating system that supports Express.
LESS and UglifyJS preconfigured out of the box to intelligently minify your external facing CSS and JS files. Other preprocessors are supported via wrapper modules.
Built-in, easy to use interface to browserify bundling for frontend JS modularization using the Node.js module exports and require syntax.
Automatic HTML validation in development mode of your post-server rendered HTML using a local instance of the Nu HTML Checker. 


Note: this is documentation for Roosevelt 0.14.x. If you need API documentation for a previous version of Roosevelt, look here.
Table of contents

Create and run a Roosevelt app

Using the Roosevelt app generator
Create a Roosevelt app manually
Available npm scripts
Available command line arguments
Combining npm scripts and command line arguments
Recognized environment variables


Default directory structure

Default .gitignore


Configure your app with parameters

App behavior parameters
MVC parameters
Statics parameters
Public folder parameters
Events
Event list


Making controller files
Making model files
Making view files
Express variables exposed by Roosevelt
Express middleware and other configurations automatically loaded by Roosevelt
Authoring your own CSS and JS preprocessors
Documentation for previous versions of Roosevelt

Create and run a Roosevelt app
First you will need to install Node.js. Both the current and LTS version of Node.js are supported. It is recommended that you install using a Node.js version manager like nvm rather than the official installer, as a version manager will allow you to switch between multiple versions of Node.js easily.
Some important caveats to note:

nvm is not available on Windows. Windows users should try out nvm-windows or nvs.
It is also recommended that Windows users use a terminal that supports emojis, such as cmder, at least until Microsoft rolls out this planned update to cmd.exe.
Linux/macOS users who install Node.js without a version manager like nvm may need to resolve some commonly encountered permissions headaches associated with npm. As such, use of nvm is strongly recommended.

The Java JDK is also required for development work. The JDK is required for the local HTML validator feature.
Once you have a sane development environment, you can proceed with the standard install procedure below.
Using the Roosevelt app generator
The Roosevelt app generator is a command line script based on Yeoman that can create a sample Roosevelt app for you.
To use it, first globally install Yeoman and the Yeoman-based Roosevelt app generator:
npm i -g yo generator-roosevelt

Create a Roosevelt app using the Roosevelt app generator:
yo roosevelt

Then follow the prompts.
Afterward, cd to your app's directory and install dependencies:
npm i

Run in development mode:
npm run d

Or run in production mode:
npm run p

Create a Roosevelt app manually
It is also possible to create a Roosevelt app without using the app generator. This will result in a more minimalist default configuration (e.g. no CSS or JS preprocessors enabled by default).
To do that:

First create a new folder and cd into it.
Then npm i roosevelt. This will create a node_modules folder with Roosevelt and its bare minimum dependencies.
Create a file named app.js.
Put this code in app.js:
require('roosevelt')({
  'generateFolderStructure': true
}).startServer()

Then node app.js. If the generateFolderStructure param is set to true like the above code example, an entire Roosevelt app with bare minimum viability will be created and the server will be started. See below for more information about parameter configuration.

Available npm scripts
Roosevelt apps created with the app generator come with the following notable npm scripts prepopulated in package.json:


npm run production: Runs the app in production mode.

Default shorthands:

npm run prod
npm run p
npm start


Script is short for: node app.js --production-mode



npm run development: Runs the app in development mode.

Default shorthands:

npm run dev
npm run d


Script is short for: node app.js --development-mode



npm run kill-validator: Finds the HTML validator process and kills it if it is running.

Default shorthand:

npm run kv


Script is short for: node ./node_modules/roosevelt/lib/scripts/killValidator.js



npm run clean: Removes all build artifacts (symlinks and directories) auto-generated by Roosevelt. Will prompt to confirm before deleting any files.

Default shorthand:

npm run c


Script is short for: node ./node_modules/roosevelt/lib/scripts/appCleanup.js



npm run config-audit: Scans current rooseveltConfig and scripts in package.json and warns about any params or npm scripts that don't match the current Roosevelt API:

Default shorthand:

npm run a


Script is short for: node ./node_modules/roosevelt/lib/scripts/configAuditor.js
Note: this will run automatically whenever you run npm i as well.



Available command line arguments

node app.js --production-mode : Runs the app in production mode.

Default shorthands:

--prod
-p




node app.js --development-mode : Runs the app in development mode.

Default shorthands:

--dev
-d




node app.js --cores <m> : Configures how many CPUs your app will run on.

<m> can be either a number representing the desired cores, or you can supply max to use all available CPUs.

Default is 1.


Default shorthand:

-c




node app.js --enable-validator : Forces the HTML validator to be enabled.

Default shorthands:

--html-validator
-h




node app.js --disable-validator : Forces the HTML validator to be disabled.

Default shorthands:

--raw
-r




node app.js --background-validator : Forces the HTML validator to run as a detached background process.

Default shorthand:

-b




node app.js --attach-validator : Forces the HTML validator to run as an attached process.

Default shorthand:

-a




node app.js --enable-validator-autokiller : Forces the HTML validator autokiller to be enabled.

Default shorthands:

--html-validator-autokiller
-k




node app.js --disable-validator-autokiller : Forces the HTML validator autokiller to be disabled.

Default shorthands:

--no-autokiller
-n




node app.js --host-public : Forces Roosevelt to always host the public folder even when alwaysHostPublic is set to false. Useful for testing production mode.

Default shorthands:

--statics
-s





Combining npm scripts and command line arguments
The npm scripts can be combined with the command line flags.
For example, running npm run d -- -r will run your app in development mode and force the HTML validator to be disabled.
Recognized environment variables
The following is a list of environment variables that Roosevelt listens for.

NODE_ENV:

Set to production to force the app into production mode.
Set to development to force the app into development mode.


NODE_PORT: Default HTTP port to run your app on.
HTTP_PORT: Default HTTP port to run your app on. Takes precedence over NODE_PORT.
HTTPS_PORT: Default HTTPS port to run your app on.
ROOSEVELT_VALIDATOR:

Set to detached to force the HTML validator to run as a detached background process.
Set to attached to force the HTML validator to run as an attached process.


ROOSEVELT_AUTOKILLER:

Set to on to spawn a process to kill the HTML validator if it is running in the background and idle for more than a certain amount of time. The timeout can be configured in app behavior params.
Set to offto disable the HTML validator autokiller.



Environment variable precedence:


Environment variables supersede your app's params.


Environment variables can be overridden with command line arguments.


Default directory structure

app.js: Entry point to your application. Feel free to rename this, but make sure to update package.json's reference to it.
mvc: Folder for models, views, and controllers. All configurable via params (see below).

controllers: Folder for controller files.
models: Folder for model files.
views: Folder for view files.


node_modules: A standard folder where all modules your app depends on (such as Roosevelt) are installed to. This folder is created by the npm i command.
package.json: A standard file in Node.js apps for configuring your app.
public: All contents within this folder will be exposed as static files.
statics: Folder for source CSS, image, JS, and other static files. By default some of the contents of this folder are symlinked to from public, which you can configure (see below).

css: Folder for source CSS files.
images: Folder for source image files.
js: Folder for source JS files.


.gitignore: A standard file which contains a list of files and folders to ignore if your project is in a git repo.

Default .gitignore
The default .gitignore file contains many common important things to ignore, however you may need to tweak it to your liking before committing a fresh Roosevelt app to your git repo.
Some notable things ignored by default and why:

public: It's recommended that you don't create files in this folder manually, but instead use the staticsSymlinksToPublic param detailed below to expose folders in your statics directory via auto-generated symlinks.
.build: By default Roosevelt will compile LESS and JS files down to minified versions in statics/.build when the server starts. As such, it's not recommended to place files in the build directory manually.
node_modules: This folder will be auto-generated when you run the npm i step to set up your app. Since some modules you might include later in your app can be platform-specific and are compiled for your OS during the install step, it's generally not recommended to commit the node_modules folder to git.

Configure your app with parameters
Roosevelt is designed to have a minimal amount of boilerplate so you can spend less time focused on configuration and more time writing your app. All parameters are optional. As such, by default, all that's in app.js is this:
require('roosevelt')().startServer();
Roosevelt will determine your app's name by examining ""name"" in package.json. If none is provided, it will use Roosevelt Express instead.
Also, while it is recommended that you pass params to Roosevelt via package.json under ""rooseveltConfig"", you can also pass params programmatically via Roosevelt's constructor like so:
require('roosevelt')({
  paramName: 'paramValue',
  param2:    'value2',
  etc:       'etc'
}).startServer();
This is particularly useful for setting params that can't be defined in package.json such as event handlers (see below).
App behavior parameters


port: The HTTP port your app will run on.

Default: [Number] 43711.



enableCLIFlags: Enables parsing of command line flags.

Default: [Boolean] true.



generateFolderStructure: When enabled Roosevelt will generate user specified directories (e.g. MVC parameters and statics parameters).

Default: [Boolean] true.

Exception to default: When package.json is not present or rooseveltConfig is not present in package.json, this param will be automatically set to false by default. This is a defensive measure to minimize the risk of files and folders being created in scenarios when they are not wanted.


This param is useful in scenarios when you want to create a Roosevelt app entirely from nothing (without using generator-roosevelt). See create a Roosevelt app manually for an example.



appDir: Root directory of your application.

Default: [String] The directory where your project package.json is located.



localhostOnly: Listen only to requests coming from localhost in production mode. This is useful in environments where it is expected that HTTP requests to your app will be proxied through a more traditional web server like Apache or nginx. This setting is ignored in development mode.

Default: [Boolean] true.



logging: Params to pass to roosevelt-logger. See roosevelt-logger params documentation for configuration options.


Default: [Object]
{
  ""methods"": {
    ""http"": true,
    ""info"": true,
    ""warn"": true,
    ""error"": true,
    ""verbose"": false
  }
}


You can also declare a custom log types and classify them as logs, warnings, or errors:


Default logging param with custom log type called debug added to it: [Object]
{
  ""http"": true,
  ""info"": true,
  ""warn"": true,
  ""verbose"": false,
  ""debug"": {
    ""enable"": true,
    ""type"": ""error""
  }
}


enable param: Enables or disables the custom log.

Default: [Boolean] true.



type param: Specifies what kind of log your custom log is:

Allowed values: [String] info, warn, or error.







minify: Enables HTML minification as well as the minification step in supporting CSS and JS compilers.

Default: [Boolean] true.
Note: Automatically disabled during development mode.



htmlValidator: Params to send to html-validator:


enable: [Boolean] Enables or disables the built-in HTML validator.

Note: The validator is only available in development mode.



exceptions: Sending a custom request header can disable the validator on a per request basis. The name of this request header and model value can be customized with this param.


Default: [Object]
""exceptions"": {
  ""requestHeader"": ""Partial"",
  ""modelValue"": ""_disableValidator""
}




port: [Number] Port to spawn the validator process on.


separateProcess: [Object] How to run the validator:

enable: [Boolean] Run the validator as a detached background process.
autoKiller: [Boolean] Spawns a process to kill the validator if it is running in the background and idle for more than a certain amount of time.
autoKillerTimeout: [Number] Time (in milliseconds) that the validator auto-killer process waits before it kills the validator running in the background.



showWarnings: [Boolean] When set to true, shows HTML validation warnings in addition to errors.


Default: [Object]
{
  ""enable"": true,
  ""separateProcess"": {
    ""enable"": true,
    ""autoKiller"": true,
    ""autoKillerTimeout"": 360000
  },
  ""port"": 48888,
  ""exceptions"": {
    ""requestHeader"": ""Partial"",
    ""modelValue"": ""_disableValidator""
  },
  ""showWarnings"": true
}




multipart: Settings to pass along to formidable using formidable's API for multipart form processing. Access files uploaded in your controllers by examining the req.files object. Roosevelt will remove any files uploaded to the uploadDir when the request ends automatically. To keep any, be sure to move them before the request ends. To disable multipart forms entirely, set this param to false.

Default: [Boolean]
{
  ""multiples"": true
}




toobusy: Params to pass to the node-toobusy module.


maxLagPerRequest: [Number] Maximum amount of time (in milliseconds) a given request is allowed to take before being interrupted with a 503 error.


lagCheckInterval: [Number] Interval (in milliseconds) for checking event loop lag in milliseconds.


Default: [Object]
{
  ""maxLagPerRequest"": 70,
  ""lagCheckInterval"": 500,
}




bodyParser: Parameters to supply to body-parser.json.


Default: [Object]
{
  ""urlEncoded"": {
    ""extended"": true
  },
  ""json"": {}
}




checkDependencies: Whether or not to warn if dependencies are out of date.

Default: [Boolean] true.



cores: By default, Roosevelt will run on 1 CPU, but you can change the number of cores that the app will run on with this param.

Default: [Number] 1.
To use all available cores, set this value to max.



shutdownTimeout: Maximum amount of time in milliseconds given to Roosevelt to gracefully shut itself down when sent the kill signal.

Default: [Number] 30000 (30 seconds).



HTTPS parameters

https: [Object] Run a HTTPS server using Roosevelt.

Object members:

enable: Enable a HTTPS server.

Default: [Boolean] false.


force: Disallow unencrypted HTTP and route all traffic through HTTPS.

Default: [Boolean] false.


port: The port your app will run a HTTPS server on.

Default: [Number] 43733.


authInfoPath: [Object] Specify either the paths where the server certificate files can be found or set the appropriate parameters to be a PKCS#12-formatted string or certificate or key strings.

Default: undefined
Object members:

p12: [Object] Parameter used when the server certificate/key is in PKCS#12 format.

Object members:

p12Path:  [String] Either the path to a PKCS#12-formatted file (.p12/.pfx) or a PKCS#12-formatted string or buffer (i.e. the result of fs.readFileSync(/path/to/file/example.p12))

Default: undefined


passphrase: [String] The password used to encrypt the PKCS#12-formatted file or string.

Default: undefined.






authCertAndKey: [Object] Parameter used when the server certificate and key are in separate PEM-encoded files.

Object members:

cert: [String] Either the path to a PEM-encoded certificate file (.crt, .cer, etc.) or a PEM-encoded certificate string.

Default: undefined


key: [String] Either the path to a PEM-encoded key file (.crt, .cer, etc.) or a PEM-encoded key string for the certificate given in cert.

Default: undefined










caCert: [String] Either the path to a PEM-encoded Certificate Authority root certificate or certificate chain or a PEM-encoded Certificate Authority root certificate or certificate chain string. This certificate (chain) will be used to verify client certificates presented to the server. It is only needed if requestCert and rejectUnauthorized are both set to true and the client certificates are not signed by a Certificate Authority in the default publicly trusted list of CAs curated by Mozilla.

Default: undefined.


requestCert: [Boolean] Set whether to request a certificate from the client attempting to connect to the server to verify the client's identity.

Default: undefined.


rejectUnauthorized: [Boolean] Set whether to reject connections from clients that do no present a valid certificate to the server. (Ignored if requestCert is set to false.)

Default:  undefined.


Default: [Object] {}.





MVC parameters


modelsPath: Relative path on filesystem to where your model files are located.

Default: [String] ""mvc/models"".



viewsPath: Relative path on filesystem to where your view files are located.

Default: [String] ""mvc/views"".



viewEngine: What templating engine to use, formatted as ""fileExtension: nodeModule"".


generator-roosevelt default: [String] ""html: teddy"".


Also by default when using the generator, the module teddy is marked as a dependency in package.json.


Bare Roosevelt default (when an app is created without the generator): [String] none. Can also be set to null to use no templating engine.


To use multiple templating systems, supply an array of engines to use in the same string format. Each engine you use must also be marked as a dependency in your app's package.json. Whichever engine you supply first with this parameter will be considered the default.


Example configuration using multiple templating systems: [Object]
{
  ""viewEngine"": [
    ""html: teddy"",
    ""mustache: mustache"",
    ""handlebars: handlebars"",
    ""ejs: ejs""
  ]
}




controllersPath: Relative path on filesystem to where your controller files are located.

Default: [String] ""mvc/controllers"".



errorPages: Relative path on filesystem to where your various error page controller files are located. If you do not supply them, Roosevelt will use its default ones instead:

notFound: Your 404 Not Found error page.

Default: [String] ""404.js"".


internalServerError: Your Internal Server Error error page.

Default: [String] ""5xx.js"".


serviceUnavailable: Your 503 Service Unavailable error page.

Default: [String] ""503.js"".





routers: [Object] List of Express Routers to create for groups of controllers or static files. If none are defined, Roosevelt will default to creating one single global router with the route prefix / that all controllers and all static files will be routed through.

Object members:


controllers: [Array] List of Express Routers which can be used to (among other things) prefix a whole series of controller routes.

prefix: [String] The URL path prefix for the router to use.
files: [Array] List of files or directories in controllersPath that will be mounted to this route.
Default: [Boolean] false.
Note: controllers can also be a [String] that represents a schema file within the controllers directory. That file should define and export the list of Express Routers the same way as you would within the rooseveltConfig.
Example usage:

[{
  ""prefix"": ""/something"",
  ""files"": [""controller.js"", ""directory""]
}]


public: [Array] List of Express Routers which can be used to (among other things) prefix a whole series of static files into a series of public routes.

prefix: [String] The URL path prefix for the public directory to use.
dirs: [Array] List of directories in publicFolder that will be mounted to this route.
Default: [Boolean] false.
Example usage:

[{
  ""prefix"": ""/something"",
  ""dirs"": [""css"", ""images"", ""js""]
}]






Statics parameters


staticsRoot: Relative path on filesystem to where your source static assets are located. By default this folder will not be made public, but is instead meant to store unprocessed or uncompressed source assets that will later be preprocessed and exposed in public.

Default: [String] ""statics"".



htmlMinifier: How you want Roosevelt to minify your HTML:


enable: [Boolean] Enable HTML minification.

Note: Minification is automatically disabled in development mode.



exceptionRoutes: [Array] List of controller routes that will skip minification entirely. Set to false to minify all URLs.


options: [Object] Params to supply to html-minifier's API.


Default: [Object]
{
  ""enable"": true,
  ""exceptionRoutes"": false,
  ""options"": {
    ""removeComments"": true,
    ""collapseWhitespace"": true,
    ""collapseBooleanAttributes"": true,
    ""removeAttributeQuotes"": true,
    ""removeEmptyAttributes"": true
  }
}




css: [Object] How you want Roosevelt to configure your CSS preprocessor:


sourcePath: Subdirectory within staticsRoot where your CSS files are located. By default this folder will not be made public, but is instead meant to store unminified CSS source files which will be minified and written to a build directory when the app is started.


compiler: [Object] Which Roosevelt CSS preprocessor middleware (if any) to use.


nodeModule: [String] Node module name of the Roosevelt CSS preprocessor middleware you wish to use.

Note: Your chosen Roosevelt CSS preprocessor module must also be marked as a dependency in your app's package.json.



params: [Object] Params to send to the Roosevelt CSS preprocessor middleware if it accepts any.


Note: The default preprocessor for a Roosevelt app created with generator-roosevelt is roosevelt-less, which is marked as a dependency in package.json on freshly generated Roosevelt apps. See roosevelt-less usage for details on what params are available.

The Roosevelt team also maintains roosevelt-sass, an alternative to roosevelt-less.



generator-roosevelt default configuration: [Object]
{
  ""nodeModule"": ""roosevelt-less"",
  ""params"": {
    ""cleanCSS"": {
      ""advanced"": true,
      ""aggressiveMerging"": true
    },
    ""sourceMap"": null
  }
}


Bare Roosevelt default (when an app is created without the generator): [String] none. Can also be set to null to use no CSS compiler.




whitelist: Array of CSS files to whitelist for compiling. Leave undefined to compile all files. Supply a : character after each file name to delimit an alternate file path and/or file name for the minified file.

Example array member: [String] less/example.less:.build/css/example.min.css (compiles less/example.less into .build/css/example.min.css).



output: Where to write compiled CSS files to. This folder will be symlinked into public by default.


symlinkToPublic: [Boolean] When enabled Roosevelt will automatically add your CSS directory to the staticsSymlinksToPublic param.

Note: If the compiler is enabled output will be symlinked. If not,  sourcePath will be symlinked.



versionFile: If enabled, Roosevelt will create a CSS file which declares a CSS variable containing your app's version number from package.json. Enable this option by supplying an object with the member variables fileName and varName.


Default: null.


Example usage (with roosevelt-less): [Object]
{
  ""fileName"": ""_version.less"",
  ""varName"": ""appVersion""
}


Assuming the default Roosevelt configuration otherwise, this will result in a file statics/css/_version.less with the following content:
/* do not edit; generated automatically by Roosevelt */ @appVersion: '0.1.0';


Some things to note:

If there is already a file there with that name, this will overwrite it, so be careful!
It's generally a good idea to add this file to .gitignore, since it is a build artifact.





Default: [Object]
{
  ""sourcePath"": ""css"",
  ""compiler"": {
    ""nodeModule"": ""roosevelt-less"",
    ""params"": {
      ""cleanCSS"": {
        ""advanced"": true,
        ""aggressiveMerging"": true
      },
      ""sourceMap"": null
    }
  },
  ""whitelist"": null,
  ""output"": "".build/css"",
  ""symlinkToPublic"": true,
  ""versionFile"": null
}




js: [Object] How you want Roosevelt to configure your JS compiled:


sourcePath: Subdirectory within staticsRoot where your JS files are located. By default this folder will not be made public, but is instead meant to store unminified JS source files which will be minified and written to a build directory when the app is started.


compiler: Which Roosevelt JS minifier middleware (if any) to use.


nodeModule: [String] Node module name of the Roosevelt JS minifier middleware you wish to use.

Note: Your chosen Roosevelt JS minifier module must also be marked as a dependency in your app's package.json.



showWarnings: [Boolean] Set to true to display compiler module warnings.


params: [Object] Params to send to the Roosevelt JS minifier middleware if it accepts any.


Note: The default minifier for a Roosevelt app created with generator-roosevelt is roosevelt-uglify, which is marked as a dependency in package.json on freshly generated Roosevelt apps. See roosevelt-uglify usage for details on what params are available.

The Roosevelt team also maintains roosevelt-closure, an alternative to roosevelt-uglify.



generator-roosevelt default configuration: [Object]
{
  ""nodeModule"": ""roosevelt-uglify"",
  ""showWarnings"": false,
  ""params"": {}
}


Bare Roosevelt default (when an app is created without the generator): [String] none. Can also be set to null to use no JS minifier.




whitelist: Array of JS files to whitelist for minification. Leave undefined to compile all files. Supply a : character after each file name to delimit an alternate file path and/or file name for the minified file.

Default: null (compiles all JS files, if a JS minifier is enabled).
Example array member: [String] library-name/example.js:lib/example.min.js (compiles library-name/example.js into lib/example.min.js).



blacklist: Array of JS files to exempt from minification. These files will be copied as-is to the build folder. Leave undefined to compile all files.

Default: null (compiles all JS files, if a JS minifier is enabled).
Example: [String] example.js.



output: Where to write compiled JS files to. This folder will be symlinked into public by default.

Default: [String] "".build/js"".



symlinkToPublic: [Boolean] When enabled Roosevelt will automatically add your JS directory to the staticsSymlinksToPublic param.

Note: If the compiler is enabled output will be symlinked. If not,  sourcePath will be symlinked.



bundler: Params related to bundling JS with browserify:


Note: Use of browserify in Roosevelt is optional. If no bundles are defined here, the browserify step will be skipped.


bundles: [Array] Declare one or more source JS files in your sourcePath to be browserify bundles via its bundle method.

env: [String] Bundle only in dev or prod mode. Omitting env will result in bundling in both modes.
params: [Object] The browserify params to send to browserify. If it is not set, these default params will be sent: {""paths"": <your jsPath>}.
Examples: [Array] of [Objects]


Browserify bundle example declaring one bundle:
[
  {
    ""outputFile"": ""bundle.js"",
    ""files"": [
      ""landingPage.js"",
      ""main.js"",
      ""etc.js""
    ],
    ""params"": {
      ""someOpt"":
      ""someValue""
    }
  }
]



Browserify bundle example declaring one bundle only used in dev mode:
[
  {
    ""outputFile"": ""bundle.js"",
    ""env"": ""dev"",
    ""files"": [
      ""landingPage.js"",
      ""main.js"",
      ""etc.js""
    ],
    ""params"": {
      ""someOpt"": ""someValue""
    }
  }
]



Browserify bundle example declaring multiple bundles:
[
  {
    ""outputFile"": ""bundle1.js"",
    ""files"": [
      ""landingPage.js"",
      ""main.js"",
      ""etc.js""
    ],
    ""params"": {
      ""someOpt"": ""someValue""
    }
  },
  {
    ""outputFile"": ""bundle2.js"",
    ""files"": [
      ""somethingElse.js"",
      ""anotherThing.js"",
      ""etc.js""
    ]
  },
  etc...
]




Default: [Array] [].



output: Subdirectory within sourcePath where you would like browserify to deposit bundled JS files it produces.

Default: [String] "".bundled"".



expose: Whether or not to copy the output directory to your build directory.

Default: [Boolean] true.





Default: [Object]
{
  ""sourcePath"": ""js"",
  ""compiler"": {
    ""nodeModule"": ""roosevelt-uglify"",
    ""showWarnings"": false,
    ""params"": {}
  },
  ""whitelist"": null,
  ""blacklist"": null,
  ""output"": "".build/js"",
  ""symlinkToPublic"": true,
  ""bundler"": {
    ""bundles"": [],
    ""output"": "".bundled"",
    ""expose"": true
  }
}




cleanTimer: Time in milliseconds to allow before considering files in CSS/JS compile directories stale and recommending running npm run clean.

Default: [Number] 604800000 (1 week)
Useful time conversions to milliseconds to configure this param with:

1 day: 86400000
1 week: 604800000
1 month: 2419200000


Set to 0, null, or anything that isn't a number to disable the check entirely.



Public folder parameters


publicFolder: All files and folders in this directory will be exposed as static files.

Default: [String] ""public"".



favicon: Location of your favicon file.

generator-roosevelt default: [String] ""images/favicon.ico"".
Bare Roosevelt default (when an app is created without the generator): [String] none. Can also be set to null to use no favicon.



staticsSymlinksToPublic: Array of folders from staticsRoot to make symlinks to in your public folder, formatted as either ""linkName: linkTarget"" (whitespace optional) or simply ""linkName"" if the link target has the same name as the desired link name.

Default: [Array] of [Strings]
[
  ""css: .build/css"",
  ""images"",
  ""js: .build/js""
]




versionedPublic: If set to true, Roosevelt will prepend your app's version number from package.json to your public folder. Versioning your public folder is useful for resetting your users' browser cache when you release a new version.

Default: [Boolean] false.



alwaysHostPublic:  By default in production mode Roosevelt will not expose the public folder. It's recommended instead that you host the public folder yourself directly through another web server, such as Apache or nginx. However, if you wish to override this behavior and have Roosevelt host your public folder even in production mode, then set this setting to true.

Default: [Boolean] false.



Events
Roosevelt provides a series of events you can attach code to by passing a function to the desired event as a parameter to Roosevelt's constructor like so:
require('roosevelt')({
  onServerStart: (app) => { /* do something */ }
});
Event list

onServerInit(app): Fired when the server begins starting, prior to any actions taken by Roosevelt.

app: The Express app created by Roosevelt.


onServerStart(app): Fired when the server starts.

app: The Express app created by Roosevelt.


onReqStart(req, res, next): Fired at the beginning of each new request.

req: The request object created by Express.
res: The response object created by Express.
next: Callback to continue with the request. Must be called to continue the request.


onReqBeforeRoute(req, res, next): Fired just before executing the controller.

req: The request object created by Express.
res: The response object created by Express.
next: Callback to continue with the request. Must be called to continue the request.


onReqAfterRoute(req, res): Fired after the request ends.

req: The request object created by Express.
res: The response object created by Express.



Making controller files
Controller files are places to write Express routes. A route is the term Express uses for URL endpoints, such as http://yoursite/blog or http://yoursite/about.
To make a new controller, make a new file in the controllers directory. For example:
module.exports = (router, app) => { // router is an Express router and app is the Express app created by Roosevelt

  // standard Express route
  router.route('/about').get((req, res) => {

    // load a data model
    let model = require('models/dataModel');

    // render a Teddy template and pass it the model
    res.render('about', model);
  });
};
Note: If custom routers are being used, the res.redirect() method will prepend the prefix to redirects that are relative to the hostname. To override this setting pass true as the last argument.
Sometimes it is also useful to separate controller logic from your routing. This can be done by creating a reusable controller module.
An example would be creating a reusable controller for ""404 Not Found"" pages:
// reusable controller ""notFound.js""
module.exports = (app, req, res) => {
  let model = { content: 'Cannot find this page' };
  res.status(404);
  res.render('404', model);
}
Reusable controller modules differ from standard controller modules in that they accept req and res arguments in addition to app. They are meant to be called from within routes rather than define new routes.
This allows them to be called at will in any other controller's route when needed:
// import the ""notFound"" controller logic previously defined
const throw404 = require('controllers/notFound');

module.exports = (router, app) => {
  router.route('/whatever').get((req, res) => {

    // test some logic that could fail
    // thus triggering the need for the 404 controller
    if (something) {

      // logic didn't fail
      // so render the page normally
      let model = require('models/dataModel');
      res.render('whatever', model);
    }
    else {

      // logic failed
      // so throw the 404 by executing your reusable controller
      throw404(app, req, res);
    }
  });
};
Making model files
Since the above example requires a model file named dataModel, you will need to make that too. To do that, place a file named dataModel.js in mvc/models.
Here's a simple example dataModel.js data model:
module.exports = {some: 'data'};
Making view files
Views by default are Teddy templates. See the Teddy documentation for information about how to author Teddy templates.
You can also use different templating engines by tweaking Roosevelt's MVC parameters.
Express variables exposed by Roosevelt
Roosevelt supplies several variables to Express that you may find handy. Access them using app.get('variableName').



Express variable
Description




express
The Express module.


viewEngine e.g. teddy by default
Any view engine(s) you define will be exposed as an Express variable. For instance, the default view engine is teddy. So by default app.get('teddy') will return the teddy module.


formidable
The formidable module. Used for handling multipart forms.


morgan
The morgan module. HTTP request logger middleware.


appName
The name of your app derived from package.json. Uses ""Roosevelt Express"" if no name is supplied.


appVersion
The version number of your app derived from package.json.


appDir
The directory the main module is in.


package
The contents of package.json.


staticsRoot
Full path on the file system to where your app's statics folder is located.


publicFolder
Full path on the file system to where your app's public folder is located.


cssPath
Full path on the file system to where your app's CSS source files are located.


jsPath
Full path on the file system to where your app's JS source files are located.


cssCompiledOutput
Full path on the file system to where your app's minified CSS files are located.


jsCompiledOutput
Full path on the file system to where your app's minified JS files are located.


jsBundledOutput
Full path on the file system to where your app's bundled JS files are located.


modelsPath
Full path on the file system to where your app's models folder is located.


viewsPath
Full path on the file system to where your app's views folder is located.


controllersPath
Full path on the file system to where your app's controllers folder is located.


params
The params you sent to Roosevelt.


flags
Command line flags sent to Roosevelt.


logger
The logging module used for simple parameterized logging.



Additionally the Roosevelt constructor returns the following object:



Roosevelt object members
Description




expressApp
[Object] The Express app created by Roosevelt.


httpServer
[Object] The http server created by Roosevelt. httpServer is also available as a direct child of app, e.g. app.httpServer.


httpsServer
[Object] The https server created by Roosevelt. httpsServer is also available as a direct child of app, e.g. app.httpsServer.


initServer
[Method] Starts the HTML validator, sets up some middleware, runs the CSS and JS preprocessors, and maps routes, but does not start the HTTP server. Call this method manually first instead of startServer if you need to setup the Express app, but still need to do additional setup before the HTTP server is started. This method is automatically called by startServer once per instance if it has not yet already been called.


startServer
[Method] Calls the listen method of http, https, or both (depending on your configuration) to start the web server with Roosevelt's config.


stopServer
[Method] Stops the server and takes an optional argument stopServer('close') which stops the server from accepting new connections before exiting.



Express middleware and other configurations automatically loaded by Roosevelt
In addition to exposing a number of variables to Express and providing the MVC interface outlined above, Roosevelt also:

Includes the compression middleware.
Includes the cookie-parser middleware.
Disables x-powered-by and etag.
Logs HTTP requests to the console using morgan, specifically morgan('combined').
Includes the method-override middleware.

Authoring your own CSS and JS preprocessors
There are two ways to replace Roosevelt's default CSS and JS preprocessors with another one.
The first way is to swap out the default roosevelt-less CSS preprocessor module or roosevelt-uglify JS preprocessor module for something else, e.g. roosevelt-sass, roosevelt-closure, or a custom module that you've created.
You can also define your own preprocessors on the fly at start time in Roosevelt's constructor like so:
let app = require('roosevelt')({
  cssCompiler: app => {
    return {
      versionCode: app => {
        // write code to return the version of your app here
      },
      parse: (app, fileName) => {
        // write code to preprocess CSS here
      }
    }
  },
  jsCompiler: app => {
    return {
      parse: (app, fileName) => {
        // write code to preprocess JS here
      }
    }
  }
})
API:

cssCompiler(app): Custom CSS preprocessor.

versionCode(app): Function to return the version of your app.

app: The Express app created by Roosevelt.


parse(app, fileName): Function to preprocess CSS.

app: The Express app created by Roosevelt.
fileName: Name of file to compile.




jsCompiler(app): Custom JavaScript preprocessor.

parse(app, fileName): Function to preprocess JavaScript.

app: The Express app created by Roosevelt.
fileName: Name of file to compile.





Lastly, in order to activate the custom preprocessor feature, alter package.json to declare the compiler nodeModule to be custom:
""css"": {
  ""compiler"": {
    ""nodeModule"": ""custom"",
  [...]
""js"": {
  ""compiler"": {
    ""nodeModule"": ""custom"",
  [...]
Documentation for previous versions of Roosevelt

0.13.x
0.12.x
0.11.x
0.10.x
0.9.x
0.8.x
0.7.x
0.6.x
Older… here be dragons.

",55
SneakyTurt1e/CN_MainlandIP_PAC,JavaScript,"CN MainlandIP PAC
This is the PAC of most IP in mainland China.
Can use in the proxy(WhiteList).
Usage


Use SwitchyOmega as an example


At first create a new profile.




Select PAC profiles and enter a name.



Enter the URL of PAC and update.


How does it work
If any IP in this PAC, your proxy will direct to it.
If not, will able to use your proxy.
",2
Wadauk/scihub_ck,Perl,"scihub_ck
一个简版的Sci-hub可用域名检查工具。
Scihub_ck is a tiny tool for checking the working domain of sci-hub. Sci-Hub is a website with over 64.5 million academic papers and articles available for direct download. In 2015 academic publisher Elsevier filed a legal complaint in New York City against Sci-Hub alleging copyright infringement, and the subsequent lawsuit led to a loss of the original sci-hub.org domain. Following the first domain loss, Sci-Hub has cycled through a number of domains, some of which have been blocked in certain countries.
For check the working domains timely, I developed the tiny tool based on Shell and Perl.
Usage
使用方法：
perl scihub_ck list_all
Description
列表文件包括262个域名后缀供检查。运行产生的文件将保存在web文件夹内。程序运行完成后，保留大小为27kb的文件即为可用域名。
The list file contained 262 domains to check. The output file will be generated into the 'web' directory. The files with 27 kb are corresponding to the working domains.
Working domains

Update every minute

Online version is available now: https://wadauk.github.io/scihub_ck/index.html
Versions
v1.0.0
local version
本地版发布
v1.0.1
add online version
增加在线版
v1.0.2
add pipeline shell script for local version
增加本地版流程脚本
v1.0.3
speed up and shorten the update cycle for online version
提速并缩短在线版更新周期
v1.0.4
move the codes from pi to cloud and speed up
代码迁移至阿里云并提速
v1.0.5
High speed and stable version. New strategy to speed up. Every fast check only need less than 20 seconds!
高速稳定版本
",72
Mailbrix/lists,None,"lists
IP whitelist & blacklist operated by Mailbrix.mx
Lists are updated hourly and individual records are expired after 1 day of inactivity.
whitelist
The whitelist is intended to be fed to greylisting agents in order to reduce latency,
do not assume the IP addresses in that list to be ""safe"" from spam:
some are safe, others are just less likely.
Applying the whitelist should reduce drastically the greylisting of legitimate hosts,
it is built from three means:

the resolution of SPF records at major mail services providers NOT providing ESP services
lists of legitimate senders gathered from various mailing-lists as well as trusted domains
domain and/or IP reputation

you can't pay your way to the whitelist, be good and you might hit the list naturally
blacklist
The blacklist is intended to be fed to your firewall or MTA in order to IP-block a sender.
The lists are currently generated through various means, it consists mainly of IP addresses that:

do not have rDNS and FCrDNS
hit mailbrix.mx operated spam traps
send Unsolicitated Commercial Email (UCE) to adresses subscribed to specific lists
brute-force addresses
have a very bad domain and/or IP reputation
were reported by a list of trusted sources

you can't pay your way out of the blacklist, be good and you will not enter it
How to use
You can fetch the latest whitelist and blacklist from this repository and feed it directly in your MTA, firewall or antispam solution.
Direct links to latest version:

whitelist
blacklist

",6
openbmc/openbmc,Python,"OpenBMC

The OpenBMC project can be described as a Linux distribution for embedded
devices that have a BMC; typically, but not limited to, things like servers,
top of rack switches or RAID appliances. The OpenBMC stack uses technologies
such as Yocto,
OpenEmbedded,
systemd, and
D-Bus to allow easy
customization for your server platform.
Setting up your OpenBMC project
1) Prerequisite

Ubuntu 14.04

sudo apt-get install -y git build-essential libsdl1.2-dev texinfo gawk chrpath diffstat


Fedora 28

sudo dnf install -y git patch diffstat texinfo chrpath SDL-devel bitbake \
    rpcgen perl-Thread-Queue perl-bignum perl-Crypt-OpenSSL-Bignum
sudo dnf groupinstall ""C Development Tools and Libraries""

2) Download the source
git clone git@github.com:openbmc/openbmc.git
cd openbmc

3) Target your hardware
Any build requires an environment variable known as TEMPLATECONF to be set
to a hardware target.
You can see all of the known targets with
find meta-* -name local.conf.sample. Choose the hardware target and
then move to the next step. Additional examples can be found in the
OpenBMC Cheatsheet



Machine
TEMPLATECONF




Palmetto
meta-ibm/meta-palmetto/conf


Zaius
meta-ingrasys/meta-zaius/conf


Witherspoon
meta-ibm/meta-witherspoon/conf


Romulus
meta-ibm/meta-romulus/conf



As an example target Palmetto
export TEMPLATECONF=meta-ibm/meta-palmetto/conf

4) Build
. openbmc-env
bitbake obmc-phosphor-image

Additional details can be found in the docs
repository.
Build Validation and Testing
Commits submitted by members of the OpenBMC GitHub community are compiled and
tested via our Jenkins server.  Commits are run
through two levels of testing.  At the repository level the makefile make check directive is run.  At the system level, the commit is built into a
firmware image and run with an arm-softmmu QEMU model against a barrage of
CI tests.
Commits submitted by non-members do not automatically proceed through CI
testing. After visual inspection of the commit, a CI run can be manually
performed by the reviewer.
Automated testing against the QEMU model along with supported systems are
performed.  The OpenBMC project uses the
Robot Framework for all automation.  Our
complete test repository can be found
here.
Submitting Patches
Support of additional hardware and software packages is always welcome.
Please follow the contributing guidelines
when making a submission.  It is expected that contributions contain test
cases.
Bug Reporting
Issues are managed on
GitHub.  It is recommended you search through the issues before opening
a new one.
Features of OpenBMC
Feature List

Host management: Power, Cooling, LEDs, Inventory, Events, Watchdog
Full IPMI 2.0 Compliance with DCMI
Code Update Support for multiple BMC/BIOS images
Web-based user interface
REST interfaces
D-Bus based interfaces
SSH based SOL
Remote KVM
Hardware Simulation
Automated Testing

Features In Progress

OpenCompute Redfish Compliance
User management
Virtual media
Verified Boot

Features Requested but need help

OpenBMC performance monitoring

Finding out more
Dive deeper into OpenBMC by opening the
docs repository.
Contact

Mail: openbmc@lists.ozlabs.org https://lists.ozlabs.org/listinfo/openbmc
IRC: #openbmc on freenode.net
Riot: #openbmc:matrix.org

",352
Mailbrix/lists,None,"lists
IP whitelist & blacklist operated by Mailbrix.mx
Lists are updated hourly and individual records are expired after 1 day of inactivity.
whitelist
The whitelist is intended to be fed to greylisting agents in order to reduce latency,
do not assume the IP addresses in that list to be ""safe"" from spam:
some are safe, others are just less likely.
Applying the whitelist should reduce drastically the greylisting of legitimate hosts,
it is built from three means:

the resolution of SPF records at major mail services providers NOT providing ESP services
lists of legitimate senders gathered from various mailing-lists as well as trusted domains
domain and/or IP reputation

you can't pay your way to the whitelist, be good and you might hit the list naturally
blacklist
The blacklist is intended to be fed to your firewall or MTA in order to IP-block a sender.
The lists are currently generated through various means, it consists mainly of IP addresses that:

do not have rDNS and FCrDNS
hit mailbrix.mx operated spam traps
send Unsolicitated Commercial Email (UCE) to adresses subscribed to specific lists
brute-force addresses
have a very bad domain and/or IP reputation
were reported by a list of trusted sources

you can't pay your way out of the blacklist, be good and you will not enter it
How to use
You can fetch the latest whitelist and blacklist from this repository and feed it directly in your MTA, firewall or antispam solution.
Direct links to latest version:

whitelist
blacklist

",6
openbmc/openbmc,Python,"OpenBMC

The OpenBMC project can be described as a Linux distribution for embedded
devices that have a BMC; typically, but not limited to, things like servers,
top of rack switches or RAID appliances. The OpenBMC stack uses technologies
such as Yocto,
OpenEmbedded,
systemd, and
D-Bus to allow easy
customization for your server platform.
Setting up your OpenBMC project
1) Prerequisite

Ubuntu 14.04

sudo apt-get install -y git build-essential libsdl1.2-dev texinfo gawk chrpath diffstat


Fedora 28

sudo dnf install -y git patch diffstat texinfo chrpath SDL-devel bitbake \
    rpcgen perl-Thread-Queue perl-bignum perl-Crypt-OpenSSL-Bignum
sudo dnf groupinstall ""C Development Tools and Libraries""

2) Download the source
git clone git@github.com:openbmc/openbmc.git
cd openbmc

3) Target your hardware
Any build requires an environment variable known as TEMPLATECONF to be set
to a hardware target.
You can see all of the known targets with
find meta-* -name local.conf.sample. Choose the hardware target and
then move to the next step. Additional examples can be found in the
OpenBMC Cheatsheet



Machine
TEMPLATECONF




Palmetto
meta-ibm/meta-palmetto/conf


Zaius
meta-ingrasys/meta-zaius/conf


Witherspoon
meta-ibm/meta-witherspoon/conf


Romulus
meta-ibm/meta-romulus/conf



As an example target Palmetto
export TEMPLATECONF=meta-ibm/meta-palmetto/conf

4) Build
. openbmc-env
bitbake obmc-phosphor-image

Additional details can be found in the docs
repository.
Build Validation and Testing
Commits submitted by members of the OpenBMC GitHub community are compiled and
tested via our Jenkins server.  Commits are run
through two levels of testing.  At the repository level the makefile make check directive is run.  At the system level, the commit is built into a
firmware image and run with an arm-softmmu QEMU model against a barrage of
CI tests.
Commits submitted by non-members do not automatically proceed through CI
testing. After visual inspection of the commit, a CI run can be manually
performed by the reviewer.
Automated testing against the QEMU model along with supported systems are
performed.  The OpenBMC project uses the
Robot Framework for all automation.  Our
complete test repository can be found
here.
Submitting Patches
Support of additional hardware and software packages is always welcome.
Please follow the contributing guidelines
when making a submission.  It is expected that contributions contain test
cases.
Bug Reporting
Issues are managed on
GitHub.  It is recommended you search through the issues before opening
a new one.
Features of OpenBMC
Feature List

Host management: Power, Cooling, LEDs, Inventory, Events, Watchdog
Full IPMI 2.0 Compliance with DCMI
Code Update Support for multiple BMC/BIOS images
Web-based user interface
REST interfaces
D-Bus based interfaces
SSH based SOL
Remote KVM
Hardware Simulation
Automated Testing

Features In Progress

OpenCompute Redfish Compliance
User management
Virtual media
Verified Boot

Features Requested but need help

OpenBMC performance monitoring

Finding out more
Dive deeper into OpenBMC by opening the
docs repository.
Contact

Mail: openbmc@lists.ozlabs.org https://lists.ozlabs.org/listinfo/openbmc
IRC: #openbmc on freenode.net
Riot: #openbmc:matrix.org

",352
bukun/GISLite,Python,"Introduction of GISLite

In English
Static site generator (SSG) for GIS data publishment as light WebGIS application.
Example: http://www.osgeo.cn/gislite/

说明
基于开源GIS技术开发，使用静态网站形式对GIS数据进行发布。
演示网站： http://www.osgeo.cn/gislite/
基于 MapServer 的服务器端GIS数据图层发布管理系统。
目的是用于解决发布较多数量的地图时的数据更新、样式修改，以及不同样式组合应用的问题。
尽量实现数据源唯一，使用 XLSX 文件定义样式。
主要实现GIS数据图层的发布，但也实现了多源数据发布为单个地图切片，以及多个图层发布为图层分组的功能。

基于MapServer、MapProxy
使用开放电子表格格式 XLSX 定义样式
可用于团队地理信息数据快速发布管理


使用技术

MapServer
MapProxy
LeafletJS
Python 3
Jinja2


运行方式
run_gislite.py

或
python3 run_gislite.py


相关网站

http://www.osgeo.cn/pygis/  《Python与开源GIS》，使用 Python 读取与处理 GIS数据 的工具。
http://www.osgeo.cn/webgis/  涉及到 MapServer， MapProxy， Leaflet 的在线 WebGIS 教学网站 。


运行环境安装
开发与测试运行于 Debian Stretch / Ubuntu 18.04 。 在管理员权限下安装运行环境：
apt install -y apache2 php libapache2-mod-fcgid cgi-mapserver mapserver-bin libapache2-mod-php
apt install -y python3-openpyxl python3-mapproxy
apt install -y build-essential  python3-gdal python3-pip
pip3 install mapproxy

另外，需要GIS数据，路径由 cfg.py 中的 ``GIS_BASE``指定。
程序需要的资源，都在 cfg.py 中定义。 TILE_SVR 是 MapProxy 服务地址。

MapProxy使用
使用了 MapProxy 生成地图切片。下面是脚本运行的方式。
首先建立子项目：
~/.local/bin/mapproxy-util create -t base-config wcs_imgmap

或
mapproxy-util create -t base-config wcs_imgmap

然后运行：
~/.local/bin/mapproxy-util serve-develop ./out_mapproxy.yaml -b 0.0.0.0:8011

或
# mapproxy-util serve-develop ./mapproxy.yaml -b 0.0.0.0:8011

",9
marcelochaves95/MathKit,Swift,"MathKit
MathKit is a Swift library for iOS and macOS that implements common 2D and 3D vector and matrix functions, useful for games or vector-based graphics.
MathKit takes advantage of Swift language features such as function and operator overloading and struct methods to provide a more elegant interface than most C, C++ or Cocoa-based graphics APIs.
MathKit also provides a  handy replacement for the GLKit vector math types and functions, which are not available yet in Swift due to their reliance on union types.
MathKit is a completely standalone library, relying only on the Foundation framework. However, it provides optional compatibility extensions for MapKit, SceneKit and Quartz (CoreGraphics/CoreAnimation) for interoperability with UIKit, AppKit, SpriteKit and SceneKit.
MathKit is designed to be efficient, but has not been heavily optimized yet, and does not yet take advantage of architecture-specific hardware acceleration using the Accelerate framework.
Implementations



 Matrix3x3




 Matrix4x4




 Quaternion




 Scalar




 Vector2




 Vector3




 Vector4




 MapKit




 Quartz




 SceneKit




 Unit Tests



License
MIT license. See the LICENSE file for details.
",2
wikimedia/mediawiki-api-demos,Python,"
MediaWiki API Demos
The MediaWiki Action API is a web service that allows access to some wiki-features like authentication, page operations, and search. It can provide meta information about the wiki and the logged-in user.
This repository contains demo apps and code snippets in Python and Javascript to assist developers for easy use of various modules of the API:

Python
Javascript

Apps

Article ideas generator:
Demo app that suggests articles from various categories that don't yet exist on English Wikipedia. The app uses Parse and Links module.
Nearby places viewer:
Demo of geo search for wiki pages near a location using the Geolocation API and MediaWiki Action API's Geosearch module.
Picture of the day viewer:
Demo app that uses prop=images module to fetch Wikipedia's Picture of the Day (POTD) from a template page and displays it on a webpage. The app also allows users to go backward or forward a date to view other POTD.
User contributions feed:
Demp app that uses list=usercontribs module to fetch the top 50 edits made by a user.
View more demo apps

Installation
$ git clone https://github.com/wikimedia/mediawiki-api-demos.git
$ cd mediawiki-api-demos

For running python code samples: 
$ cd python
$ python3 filename.py 
Note: Install any necessary python modules with pip and enter any credentials 
required in the file to run the sample code

For running javascript code samples:
$ cd javascript
$ node filename.js
Note: Install any necessary node modules with npm and enter any credentials required
in the file to run the sample code

Contributing code samples or demo apps
If you would like to contribute a demo that you have built or a sample code that you have identified as missing for an API module documentation on MediaWiki.org, first create an issue in this repository.
If the repo contributors or maintainers agree that the proposed sample code or demo app would be a useful addition to this repository, go ahead and start working on the issue. Send a pull request when you have your changes ready to be accepted or merged!
Follow the instructions below to create a pull request:
$ git pull origin master
This makes sure that your master branch is up to date

$ git checkout -b BRANCHNAME origin/master
This will create a new branch (BRANCHNAME) from the latest 'master' and check it out for you. 

$ git add filename
$ git commit -m ""your commit message""
Make and commit your changes

$ git pull --rebase origin master
Rebase your changes against master

$ git push origin BRANCHNAME
Push your changes and create a pull request 
Learn more: https://help.github.com/en/articles/creating-a-pull-request

For GET requests modules only, python and javascript sample code can be auto-generated by following the instructions below:
$ cd mediawiki-api-demos
Add module information to `modules.json`
$ python autogenerator.py
Make desired changes to the newly generated file(s)

",50
FrankD412/frankd412.github.io,HTML,"Moon Jekyll Theme 
Sorry guys but there will be no update until I buy a new laptop.
######(If you like this theme or using it, please give a ⭐️ for motivation.)
Moon is a minimal, one column jekyll theme.
Features

Minimal, you can focus on your content
Responsive
Disqus integration
Syntax highlighting
Optional post image
Social icons
Page for sharing projects
Optional background image
Simple navigation menu
MathJax support

Preview


See a live version of Moon hosted on GitHub.
Getting Started
To learn how to install and use this theme check out the Setup Guide for more information.
",2
parkr/status,JavaScript,"status
This is a repository hosting a status site for my various web properties.
This repository uses sourcegraph/checkup to write to the updates/ directory.
Web
Normal usage of this repository is just visiting https://www.parkermoore.de/status/. It shows a lovely series of graphs for my web properties. It tracks up, down, and degraded states.
Generating
This repo uses Jess Frazelle's Docker image, r.j3ss.co/checkup to run checkup.
It is passed a configuration file like this:
{
  ""storage"": {
    ""provider"": ""github"",
    ""access_token"": ""some_api_access_token_with_repo_scope"",
    ""repository_owner"": ""owner"",
    ""repository_name"": ""repo"",
    ""committer_name"": ""Commiter Name"",
    ""committer_email"": ""you@yours.com"",
    ""branch"": ""gh-pages"",
    ""dir"": ""updates""
  },
  ""checkers"": [
    {
      ""type"": ""http"",
      ""endpoint_name"": ""Example HTTP"",
      ""endpoint_url"": ""http://www.example.com""
    }
  ]
}
Then, I run checkup on a cron. It will automatically write to GitHub.
",4
splewis/csgo-multi-1v1,SourcePawn,"csgo-multi-1v1


Status: Supported.
The multi1v1 plugin sets up any number of players in 1v1-situations on specially made maps and they fight in a ladder-type system each round. The winners move up an arena, and the losers go down an arena. Players choose between specific round types (for example: ""rifle"", ""pistol"", ""awp""), and the plugin automatically spawns and gives players the appropriate weapons each round start.
Also see the AlliedModders thread and the the wiki for more information.
Features

Round types: there are 3 round types: rifle, pistol, and awp
Player selection: players can select to allow pistol and awp rounds or ban them, rifle rounds are always allowed
Player preference: players can also select a preference of round type, if player preferences match they will play that type
Weapon selection: players can select their primary (i.e. their rifle) and their pistol
Armor on pistol rounds: helmets are taken away, and kevlar is also taken away if the player selected an upgraded pistol
ELO ranking system: optionally, player statistics can be stored in a database, see below for details

For plugin developers
Check the multi1v1.inc file to see what natives and forwards are avaliable to tweak the behavior of the plugin in more sophisticated ways.
Extra plugins
Sometimes it's easier to add something in a seperate plugin than add more convars, thus some features may be in support plugins. These are all optional.

multi1v1_flashbangs: if both players in an arena say ""yes"" to getting flashbangs, a flashbang is given to each player
multi1v1_kniferounds: adds unranked knife rounds
multi1v1_online_stats_viewer: adds the !stats and related commands that open up a stats webpage in a MOTD panel

Download
Stable releases are in the GitHub Releases section.
I strongly recommend using the Updater plugin which can automatically update the plugin for bug fixes.
Any changes made through an automatic update will be backwards compatible.
You may also download the latest development build if you wish. If you report any bugs from these, make sure to include the build number (when typing sm plugins list into the server console, the build number will be displayed with the plugin version).
Installation
Requirements
Sourcemod 1.9 or later.
Instructions
Download the archive and extract the files to the game server. From the download, you should have installed the following (to the csgo directory):

addons/sourcemod/plugins/multi1v1.smx
addons/sourcemod/configs/multi1v1_weapons.cfg
addons/sourcemod/translations
cfg/sourcemod/multi1v1

Configuration
The file cfg/sourcemod/multi1v1/multi1v1.cfg will be autogenerated when the plugin is first run and you can tweak it if you wish.
You may also tweak the values in cfg/sourcemod/multi1v1/game_cvars.cfg, which is executed by the plugin each map start.
Here is a brief list of some cvars available. See the auto-generated cfg/sourcemod/multi1v1/multi1v1.cfg file for descriptions.

sm_multi1v1_autoupdate: whether the plugin attempts to use the auto-updater plugin
sm_multi1v1_pistol_behavior: what types of pistols (if any) should be given in non-pistol rounds
sm_multi1v1_roundtime: length of the round
sm_multi1v1_use_database: whether the plugin attempts to store player statistics (e.g. elo ranking) in a MySQL database
sm_multi1v1_verbose_spawns: whether the plugin will dump information on player-spawn clustering on map starts

addons/sourcemod/configs/multi1v1_weapons.cfg contains the list of weapons that are available under the rifle and pistol menus. You are free to add or remove weapons from this as long as they match the correct format. Note that the team part is only for making sure the player gets the correct weapon skin, otherwise it has no effect.
More help on setting up the stats system
There is a wiki page that explains how to setup the stats system with the provided components.
Building
The build process is managed by my smbuilder project. You can still compile multi1v1.sp without it, however.
To compile, you will need:

SMLib (required)

You should make sure you have a relatively recent version of smlib - some changes were made to accommodate sourcemod 1.7 changes.
Maps
I have a workshop collection of maps I know of. The ""am_"" prefix stands for aim_multi, reflecting the fact that the maps are similar to aim_ maps but there are multiple copies of them.
Note: standard maps (de_dust2, etc.) or aim maps (aim_map, etc.) will not work with this plugin. Maps must be custom-made with multiple arenas.
Guidelines for making a multi-1v1 map:

Create 1 arena and test it well, and when are you happy copy it
Create a bunch of arenas, I'd recommend making at least 16
The players shouldn't be able to see each other on spawn
Each group of spawns (e.g. all CT spawns in arena 1) must be within 1600.0 units of each other, this is required to cluster spawns into the arenas and not configurable
Ensure that the arenas are sufficiently far apart so players don't hear shooting in other arenas
If you want to edit your map, it's easiest to delete all but 1 arena and re-copy them. Be warned this can cause issues with the game's lighting and clients may crash the first time they load the new map if they had downloaded the old one previously
You should avoid areas where it's easy for 1 player to hide; ideally they should have to cover multiple angles if they sit in one spot
Here is an example map: am_grass2.vmf
The cvar sm_multi1v1_verbose_spawns can be set to 1 to log information about how the spawns were partitioned into arenas on map changes

Using the statistics database
Note: SQLite is not supported. Only MySQL is.
You should add a database named mult1v1 to your databases.cfg file like so:
""multi1v1""
{
    ""driver""            ""mysql""
    ""host""              ""123.123.123.123""   // localhost works too
    ""database""          ""game_servers_database""
    ""user""              ""mymulti1v1server""
    ""pass""              ""strongpassword""
    ""timeout""           ""10""
    ""port""          ""3306""  // whatever port MySQL is set up on, 3306 is default
}

To create a MySQL user and database on the database server, you can run:
CREATE DATABASE game_servers_database;
CREATE USER 'mymulti1v1server'@'123.123.123.123' IDENTIFIED BY 'strongpassword';
GRANT ALL PRIVILEGES ON game_servers_database.multi1v1_stats TO 'mymulti1v1server'@'123.123.123.123';
FLUSH PRIVILEGES;

Make sure to change the IP, the username, and the password. You should probably change the database as well, especially if you already have one set up you can use.
Schema:
mysql> describe multi1v1_stats;
+--------------+-------------+------+-----+---------+-------+
| Field        | Type        | Null | Key | Default | Extra |
+--------------+-------------+------+-----+---------+-------+
| accountID    | int(11)     | NO   | PRI | 0       |       |
| serverID     | int(11)     | NO   | PRI | 0       |       |
| auth         | varchar(64) | NO   |     |         |       |
| name         | varchar(64) | NO   |     |         |       |
| wins         | int(11)     | NO   |     | 0       |       |
| losses       | int(11)     | NO   |     | 0       |       |
| rating       | float       | NO   |     | 1500    |       |
| lastTime     | int         | NO   |     | 0       |       |
| recentRounds | int         | NO   |     | 0       |       |
+--------------+-------------+------+-----+---------+-------+

Note that the accountID field is what is returned by GetSteamAccountID, which is ""the lower 32 bits of the full 64-bit Steam ID (referred to as community id by some) and is unique per account.""
auth is the steam ID auth string, and the lastTime field is the last time the player connected to the server.
The time comes from GetTime, which returns the ""number of seconds since unix epoch"".
recentRounds is simply incremented each time the player completes a round. This can be used, for example, to check the rounds played on a daily basis and lower ratings if a player didn't play a certain number of rounds.
Clientprefs Usage/Cookies
Player choices (round type preferences, weapon choices) can be saved so they persist across maps for players (via the SourceMod clientprefs API). Installing SQLite should be sufficient for this to work.
If you have a game-hosting specific provider, they may already have SQLite installed
Custom Round Types
There are two ways to add your own round types: through writing another plugin using the forward and natives in multi1v1.inc, and
defining a round type in a config file.
Adding round types via a config file
This is the simpler approach, but you are fairly restricted in the logic you can use. The file to edit is addons/sourcemod/configs/multi1v1_customrounds.cfg.
Here is an example file that adds a scout round and a knife round:
""CustomRoundTypes""
{
    ""scout""
    {
        ""name""      ""Scout""
        ""ranked""        ""1""
        ""ratingFieldName""       ""scoutRating""
        ""optional""      ""1""
        ""enabled""       ""1""
        ""armor""     ""1""
        ""helmet""        ""1""
        ""weapons""
        {
            ""weapon_knife""      """"
            ""weapon_ssg08""      """"
        }
    }
    ""knife""
    {
        ""name""      ""Knife""
        ""ranked""        ""0""
        ""optional""      ""1""
        ""enabled""       ""1""
        ""armor""     ""1""
        ""helmet""        ""1""
        ""weapons""
        {
            ""weapon_knife""      """"
        }
    }
}

Adding round types via another plugin
Using the natives in multi1v1.inc, you can write more complex logic into a round type. To get a simple example, check multi1v1_kniferounds.sp. The key is calling Multi1v1_AddRoundType within the Multi1v1_OnRoundTypesAdded forward.
typedef RoundTypeWeaponHandler = function void (int client);
typedef RoundTypeMenuHandler = function void (int client);

// Registers a new round type by the plugin.
native int Multi1v1_AddRoundType(const char[] displayName,
                                 const char[] internalName,
                                 RoundTypeWeaponHandler weaponsHandler=Multi1v1_NullWeaponHandler,
                                 bool optional=true,
                                 bool ranked=false,
                                 const char[] ratingFieldName="""",
                                 bool enabled=true);
Note that the multi1v1 plugin will

create and update the column for the round-type stats if you set the round type as ranked and give a non-empty string as the ratingFieldName parameter ( note that these columns are only created on database-connections)
create and update the ""allow x rounds"" clientprefs cookie for you (it uses the interalName when naming the cookie)

Contribution and Suggestions
First, check the issue tracker to ask questions or make a suggestion.
If you have a suggestion you can mark it as an enhancement.
Guidelines

Create a fork on github, clone that, then create a branch to work on git checkout -b mybranchname
Follow the code-style already used as much as you can
Submit a pull request when you're happy with the new feature/enhancement/bugfix
Favor readability and correctness over all else
For a moderately advanced feature, it may be simpler to write it as a plugin that uses the multi1v1 natives from multi1v1.inc
Keep it simple, stupid

",157
xndcn/bing-wallpaper-archive,None,"bing-wallpaper-archive
There is a official archive site for bing wallpapers but without videos.
So I have to save them here.
",7
borislav-milkov/CS151-Blackjack,Java,"Blackjack

A WIP Blackjack game following an MVC (Model-View-Controller) design pattern.



Built With

Java
Java Swing
Eclipse

Versioning
Git and GitHub are used for version control.
Authors

Devin Gonzales
Borislav Milkov

",2
lawrencefinn/cloudsidecar,Go,"Cloud Sidecar
Introduction
Cloud Sidecar (CS) is a utility to allow software to be written in a cloud agnostic manner while being able to take advantage
of the features a specific cloud may offer.  It runs next to your existing application and implelments a common API that
is compatible with most cloud SDKs.  Cloud Sidecar allows you to switch providers (or use multiple providers) for common
cloud products like file storage, key vale store, NoSQL database, queues, messaging, etc...
How It Works
Cloud Sidecar exposes an API that is compatible with some AWS APIs.  It is meant to run next to your application as a
sidecar.  It is configuration driven (hot reloads)
and requires very little code change to be used.
Boto3 Python Instructions
Just pass in an endpoint_url when you create a resource or any boto object.  Example:
client = boto3.resource(
    ""dynamodb"",
    region_name=""us-east"",
    aws_access_key_id=""meow"",
    aws_secret_access_key=""cow"",
    endpoint_url='http://localhost:3452',
    use_ssl=False,
)
client = boto3.client(
    ""kinesis"",
    region_name=""us-east"",
    aws_access_key_id=""meow"",
    aws_secret_access_key=""cow"",
    endpoint_url='http://localhost:3451',
    use_ssl=False,
)

This is assuming CS is running on the same host on port 3452 and 3451
Java AWS Instructions
Similar to boto, just set an endpoint when creating your client.  Example:
AmazonS3ClientBuilder.standard()
    .withEndpointConfiguration(new EndpointConfiguration(""http://localhost:3451"", ""bleh""))
    .withPathStyleAccessEnabled(true)
    .build()

This is assuming that CS is running on the same host on port 3451
Pyhon Google Cloud API v2 Not Implemented
You need to extend the Client and change the service address.  Example:
from google.cloud.bigtable_v2.gapic import bigtable_client

class Bleh(bigtable_client.BigtableClient):
  SERVICE_ADDRESS = 'localhost:3453'

bleh = Bleh()

Java Google Cloud API v2 Not Implemented
Set the host of the service via the Java API.  This might vary based on service. Example:
StorageOptions.newBuilder().setHost(""http://localhost:1234"").setProjectId(""boo"").build().getService

InstantiatingGrpcChannelProvider prov = InstantiatingGrpcChannelProvider.newBuilder().setEndpoint(""localhost:1234"").build()
BigtableDataSettings settings = BigtableDataSettings.newBuilder().setTransportChannelProvider(prov).setInstanceName(InstanceName.of(""project"", ""instance"")).build()
BigtableDataClient.create(settings).readRow(""aaa"", ""bbb"")

Installing and compiling
Requires dep
Just run clone and run dep ensure to get dependencies. run go build main.go to compile.
Configure
Take a look at example.yaml
Run
./main example.conf
Plugins
CS lets you add on your own code or third party code.  Plugins do not require recompiling CS, just drop them into a certain path and restart.
Handlers
Handler plugins let you define your own handler code for a config section (port).  It can do whatever you want, raw requests are just passed on.
A handler plugin just needs to expose a Register function with the signature func Register(*mux.Router) awshandler.HandlerInterface.  Your plugin
must be compiled (go build -buildmode=plugin -o your_plugin.so your_plugin_source.go) and placed in plugin/handler.  Set the service_type in the config to the plugin file name without an extension, so your_plugin in this case.
See plugin/handler/example.go
Middleware
Middleware plugins lets you intercept requests before going to a handler.  Great for metrics, logging, adding some crazy logic, etc..
A middleware plugin must expose a Register function with the signature func Register(config *viper.Viper) func(http.Handler) http.Handler).
You need to configure the middleware in the top level middlware section with the type value is the plugin filename without .so.
The plugin file must live in plugin/middlware/.  You can then add add the middleware by adding a middleware section to your config.
See plugin/middleware/example.go
Notes
dep ensure -add gopkg.in/yaml.v2
antlr -Dlanguage=Go -o dynamo_parser Dynamo.g4
",2
skyhigh119/JavQ,Java,"JavQ
Java Library, Simpler your code..

Install
Just import this lib.
Api
I have not made documentation., JavQ will show you on Tester.java
",2
buddystudio/BuddyPP,C,"Buddy++
Buddy++ is an open source Arduino IDE developed based on the Java FX framework. BuddyPP is more beautiful and convenient than the official IDE. We designed many features for zero base developers. We have added a user-friendly interactive programming mechanism in BuddyPP. Developers can generate code in the setting window without the need to keep in mind all sorts of boring keywords, data types, and grammatical structures.

The lastest version: 1.2.1
Buddy++是BuddyStudio基于Java FX框架开发的开源的Arduino集成开发环境，与Arduino官方的IDE相比除了界面更美观、更简约实用，我们还针对零基础开发者设计了各种特色功能，其交互式的编程方式极大地为入门开发者降低了门槛。

最新版本：1.2.1
",16
Aeronica/mxTune,Java,"mxTune - a music mod for minecraft forge
This mod adds musical instruments that allow you to play music in MML format. This is a format already used in some popular online games so there are many tunes available or you can create your own. You can play solo or in groups of up to eight players.
Minecraft Forum WIP Post
Development Blog





CurseForge Badges by way2muchnoise
Java Profiler  provided by ej-technologies GmbH
",4
markphelps/flipt,Go,"


A feature flag solution that runs in your existing infrastructure








Documentation
https://flipt.dev/
What is Flipt
Flipt is an open source feature flag application that allows you to run experiments across services in your environment.
This means that you can deploy Flipt within your existing infrastructure and not have to worry about your information being sent to a third party or the latency required to communicate across the internet.
Flipt includes native client SDKs as well as a REST API so you can choose how to best integrate Flipt with your applications.
Flipt Features
Flipt enables you to add feature flag support to your existing applications, with a simple, single UI and API.
This can range from simple on/off feature flags to more advanced use cases where you want to be able to rollout different versions of a feature to percentages of your users.
Flipt features include:

Fast. Written in Go. Optimized for performance
Stand alone, easy to run server with no external dependencies
Ability to create advanced distribution rules to target segments of users
Native GRPC client SDKs to integrate with your applications
Simple REST API
Modern UI and debug console

Why Flipt
Flipt allows you to focus on building your applications without having to worry about implementing your own feature flag solution that works across your infrastructure.
On top of this, Flipt provides a nice, modern UI so that you can always monitor the state of your feature flags and experiments in a single place.
Running Flipt
Flipt is a single, self contained binary that you run on your own servers or cloud infrastructure. There are a multitude of benefits to running Flipt yourself, including:

🔒 Security - No data leaves your servers and you don't have to open your systems to the outside world to communicate with Flipt. It all runs within your existing infrastructure.
🚀 Speed - Since Flipt is co-located with your existing services, you do not have to communicate across the internet to another application running on the other side of the world which can add excessive latency and slow down your applications.
✅ Simplicity - Flipt is a single binary with no external dependencies. This means there is no database server to manage or connect to, no clusters to configure, and data backup is as simple as copying a single file.

Try It
❯ docker run --rm -p 8080:8080 -p 9000:9000 markphelps/flipt:latest
Flipt UI will now be reachable at http://localhost:8080/.
For more permanent methods of running Flipt, see the Installation section.
Licensing
There are currently two types of licenses in place for Flipt:

Client License
Server License

Client License
All of the code required to generate GRPC clients in other languages as well as the existing GRPC Go client are licensed under the MIT License.
This code exists in the rpc/ directory.
The client code is the code that you would integrate into your applications, which is why a more permissive license is used.
Server License
The server code is licensed under the GPL 3.0 License.
If there are any concerns about the use of this license for the server, please open an issue on GitHub so that we can discuss publicly.
Author

Website: https://markphelps.me
Twitter: @mark_a_phelps
Email: mark.aaron.phelps at gmail.com

Contributing
I would love your help! Before submitting a PR, please read over the Contributing guide.
How To Contribute
No contribution is too small, whether it be bug reports/fixes, feature requests, documentation updates, or anything else that can help drive the project forward.
Here are some good places to start:

Project Kanban Board
Help Wanted Issues
Good First Issue Issues
Documentation Issues

Cheers! 🍺
Support Development
Or if you would like to support the continued development of Flipt (and my caffeine addiction), you could always Buy Me A Coffee!

Pro Version
My plan is to soon start working on a Pro Version of Flipt for enterprise. Along with support, some of the planned features include:

User management/permissions
Multiple environments
Audit log
Streaming updates
Metrics
HA support

If you or your organization would like to help beta test a Pro version of Flipt, please get in touch with me:

Twitter: @mark_a_phelps
Email: mark.aaron.phelps at gmail.com

",732
CleverRaven/Cataclysm-DDA,C++,"


Cataclysm: Dark Days Ahead
Cataclysm: Dark Days Ahead is a roguelike set in a post-apocalyptic world. While some have described it as a ""zombie game"", there is far more to Cataclysm than that. Struggle to survive in a harsh, persistent, procedurally generated world. Scavenge the remnants of a dead civilization for food, equipment, or, if you are lucky, a vehicle with a full tank of gas to get you the hell out of Dodge. Fight to defeat or escape from a wide variety of powerful monstrosities, from zombies to giant insects to killer robots and things far stranger and deadlier, and against the others like yourself, who want what you have...
Download
Visit our website for download links to all stable and experimental releases.
The source can either be downloaded as an archive, or you can clone it from our GitHub repository.
Compile
Please read COMPILING.md - it covers general information and more specific recipes for Linux, OS X, Windows and BSD. See doc/COMPILER_SUPPORT.md for details on which compilers we support. And you can always dig for more information in doc/.
We also have the following build guides:

Building on Windows with MSYS2 at COMPILING-MSYS.md
Building on Windows with vcpkg at COMPILING-VS-VCPKG.md
Building with cmake at COMPILING-CMAKE.md  (unofficial guide)

Contribute
Cataclysm:Dark Days Ahead is the result of contributions from over 800 volunteers under the Creative Commons Attribution ShareAlike 3.0 license. The code and content of the game is free to use, modify, and redistribute for any purpose whatsoever. See http://creativecommons.org/licenses/by-sa/3.0/ for details.
Some code distributed with the project is not part of the project and is released under different software licenses, the files covered by different software licenses have their own license notices.

Please see CONTRIBUTING.md for details.
Community
Forums:
https://discourse.cataclysmdda.org
Wiki:
http://cddawiki.chezzo.com/cdda_wiki/index.php
GitHub repo:
https://github.com/CleverRaven/Cataclysm-DDA
IRC:
irc.freenode.net ; #CataclysmDDA
http://webchat.freenode.net/?channels=#CataclysmDDA
Frequently Asked Questions
Is there a tutorial?
Yes, you can find the tutorial in the Special menu at the main menu (be aware that due to many code changes the tutorial may not function). You can also access documentation in-game via the ? key.
How can I change the key bindings?
Press the ? key, followed by the 1 key to see the full list of key commands. Press the + key to add a key binding, select which action with the corresponding letter key a-w, and then the key you wish to assign to that action.
How can I start a new world?
World on the main menu will generate a fresh world for you. Select Create World.
I've found a bug / I would like to make a suggestion. What should I do?
Please submit an issue on our GitHub page. If you're not able to, send an email to kevin.granade@gmail.com.
",2978
csingley/ofxtools,Python,"Open Financial Exchange (OFX) Tools for Python








ofxtools is a Python library for working with Open Financial Exchange (OFX)
data - the standard format for downloading financial information from banks
and stockbrokers.  OFX data is widely provided by financial institutions so
that their customers can import transactions into financial management
software such as Quicken, Microsoft Money, or GnuCash.
If you want to download your transaction data outside of one of these
programs - if you wish to develop a Python application to use this data -
if you need to generate your own OFX-formatted data... ofxtools is for you!

What is it?
ofxtools requests, consumes and
produces both OFXv1 (SGML) and OFXv2 (XML) formats.
It converts serialized markup to/from native Python objects of
the appropriate data type, while preserving structure.
It also handles Quicken's QFX format, although it ignores Intuit's proprietary
extension tags.
In a nutshell, ofxtools makes it simple to get OFX data and extract it,
or export your data in OFX format.
ofxtools takes a comprehensive, standards-based approach to processing OFX.
It targets compliance with the OFX specification, specifically OFX versions
1.6 and 2.03.

ofxtools Coverage of the OFX Specification

Section 7 (financial institution profile)
Section 8 (service activation; account information)
Section 9 (email over OFX)
Section 10 (recurring bank transfers)
Section 11 (banking)
Section 12 (bill pay)
Section 13 (investments)



This should cover the great majority of real-world OFX use cases.  A particular
focus of ofxtools is full support of the OFX investment message set,
which has been somewhat neglected by the Python community.
The major item remaining on the ofxtools ""to do"" list is to implement the
tax schemas.  It's currently a low priority to implement Section 14 (bill pay)
or the extensions contained in OFX versions beyond 2.03, but you're welcome to
contribute code if you need these.
Some care has been taken with the data model to make it easily maintainable
and extensible.  The ofxtools.models subpackage contains simple, direct
translations of the relevant sections of the OFX specification.  Using existing
models as templates, it's quite straightforward to define new models and
cover more of the spec as needed (the odd corner case notwithstanding).
More than 10 years' worth of OFX data from various financial institutions
has been run through the ofxtools parser, with the results checked.  Test
coverage is high.

Where is it?
Full documentation is available at Read the Docs.
For ease of installation, ofxtools is released on PyPI.
Development of ofxtools is centralized at GitHub, where you will find
a bug tracker.

Installation Dependencies
ofxtools requires Python version 3.6+, and depends only on the standard
libary (no external dependencies).
NOTE: As of version 0.6, ofxtools no longer supports Python version 2,
which goes EOL 2020-01-01.
",71
Lombiq/Orchard-Azure-Application-Insights,C#,"Hosting - Azure Application Insights Readme
This Orchard CMS module enables easy integration of Azure Application Insights telemetry into Orchard. Just install the module, configure the instrumentation key from the admin and collected data will start appearing in Application Insights. The module is tenant-aware, so in a multi-tenant setup you can configure different instrumentation keys to collect request tracking and client-side tracking data on different tenants. This is also available on all sites of DotNest, the Orchard CMS as a Service.
Warning: this module is only compatible with the Orchard 1.9+.
Note that the module depends on the Helpful Libraries module so you should have that installed as well.
Hosting - Azure Application Insights is part of the Hosting Suite, which is a complete Orchard DevOps technology suite for building and maintaining scalable Orchard applications.
The module was created by Lombiq, one of the core developers of Orchard itself.
Configuration
You can configure the module, including setting the AI instrumentation key from the admin site, for each tenant. You can also set an application-wide instrumentation key to be used by all tenants (if the module is enabled) in the static configuration (i.e. Web.config, Azure Portal) with the key shown in the Constants class.
To collect detailed dependency data and server resource information you'll need to install the AI Status Monitor (for VMs and local development) or the Application Insights site extension for Azure Web Apps. Be aware that both tools will add DLLs to the app and create an ApplicationInsights.config file, but neither are needed. To fix this remove the config file and re-deploy (or locally: delete Orchard.Web/bin and App_Data/Dependencies and re-build) the app.
Using the AI site extension is currently not supported, the extension needs to be modified.
Extending the module with custom telemetry data
You can send custom events (i.e. totally new events like a user action happening) through a TelemetryClient object (you can see examples for this in the official AI documentation). You can create such an object for the current configuration (i.e. what is also used to send request telemetry) through
ITelemetryClientFactory.CreateTelemetryClientFromCurrentConfiguration().
You can also hook into the default behaviour of the module and e.g. extend what is send during request tracking by implementing the module's event handlers, see the
Events folder/namespace. Particularly you can implement IRequestTrackingEventHandler to add custom data to the request telemetry e.g. by adding custom properties to the Properties dictionary. Furthermore you can implement ITelemetryConfigurationEventHandler to alter the configuration used by any telemetry-sending operation like adding your own ITelemetryInitializers to the TelemetryInitializers collection.
Note on assembly binding errors when using dynamic compilation
Note that when you modify this one or a dependent project in the Orchard solution and then refresh a page without doing a manual rebuild (i.e. letting Orchard's dynamic compilation do the job) you can get the following error:

Could not load file or assembly 'Microsoft.Diagnostics.Tracing.EventSource, Version=1.1.11.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a' or one of its dependencies. The located assembly's manifest definition does not match the assembly reference.

This is because on the fly assembly redirection (see below) doesn't work for some reason in such cases. To solve the issue simply restart the website (from IIS or by restarting IIS Express) after doing a manual build.
Updating AI libraries
When assembly binding redirects are changed make sure to also edit AssemblyRedirectSetupShellEventHandler that mimicks such redirects instead of relying on the Web.config.
Note that since there is a 260 characters limit on paths on Windows, all unused library folders and files should be removed and folders shortened.
After updating you can check for breaking changes between the old and new assembly versions with BitDiffer.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/hosting-azure-application-insights (Mercurial repository)
https://github.com/Lombiq/Orchard-Azure-Application-Insights (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",11
ffwff/hana,Rust,"🌸 hana
hana, a small object oriented programming language.
NOTE: hana's interpreter is under a rewrite in Rust! Please checkout the master branch
Cloning
You'll have to clone it recursively:
git checkout master
git clone --recursive https://github.com/ffwff/hana

Building
(building was tested by using gcc-7 on an x64 with Linux, mileage may vary on other architectures)
You'll need to install libffi and libgc (BoehmGC garbage collector).
For release builds, just do:
make RELEASE=1

It is recommended that you build the interpreter with libreadline and the ENABLE_READLINE flag set for a better REPL.
To bootstrap the init bytecode, compile it using a debug/release build, then remake the interpreter:
make
make build/init.bin
make RELEASE=1 INCLUDE_BYTECODE=1

For debug:
make DEBUG=1

Running
Once built, you can write hana code into a source file, then invoke the interpreter like this:
./main program.hana

Alternatively you could try things out in the REPL:
./main

In the REPL, to type a newline, simply put \ at the end of the line then press enter.
Documentation
see DOCUMENTATION.md
Examples
see DOCUMENTATION.md#Examples for more
Hello World
print(""Hello World\n"")

Variables
name = ""Alice""
age = 20
print(name, "" is "", age, "" years old.\n"")

Fibonacci numbers
// Regular recursive
fib(n) = n <= 1 ? 1 : fib(n-1) + fib(n-2)
print(fib(30), ""\n"")

// Faster recursive (with tail-call optimization!)
fibrec(n, prev, curr) = n <= 0 ? curr : fibrec(n-1, prev+curr, prev)
fib(n) = fibrec(n+1, 1, 0)
print(fib(50), ""\n"")

License
GPLv3 License
",7
projecthorus/horusbinary,C,"Project Horus's Low-Speed Binary Telemetry System
This repository contains documentation and scripts to work with the new horus_demod MFSK/RTTY demodulator, developed by David Rowe. Currently this demodulator provides ~2.5dB better RTTY decode performance than dl-fldigi 3.21.50, and ~0.5dB better performance than fldigi 4.0.1.
It also adds support for a binary-packet 4FSK mode, designed specifically for high-altitude balloon telemetry, and which is intended to supercede RTTY for all Project Horus launches. Preliminary testing shows it has ~6dB improved demodulation performance over RTTY at the same baud rate.
Currently we are developing the modem under Linux & OSX, with the eventual aim to produce a cross-platform GUI. For now, the demodulator is available as a command-line utility, with additional binary packet processing and uploading of data to Habitat performed by the horusbinary.py python script.
These modems have recently been added to the FreeDV GUI, to allow easier usage. Refer to this guide for instructions on using FreeDV to decode Horus Binary telemetry: https://github.com/projecthorus/horusbinary/wiki/FreeDV---HorusBinary-Setup-&-Usage-Instructions
Modes Supported
The horus_demod modem (located within the codec2-dev repo) is in early development, and currently only supports:
MFSK - Horus Binary Packets
Horus Binary packets take the form:
<preamble><unique word><payload>
where
<preamble> = 0x1B1B1B1B
<unique word> = 0x2424

The payload consists of a 22-byte long binary packet, encoded with a Golay (23,12) code, and then interleaved and scrambled, for a total encoded length of 43 bytes. The binary packet format is available here, and the golay-encoding/interleaving/scrambling is performed by horus_l2_encode_packet.
At the start of a packet is a Payload ID (one byte). A lookup table for payload IDs is located here. If you are going to fly your own payload using this mode, you should get a payload ID allocated for your use. This can be done by submitting an issue or a pull request to this repository.
Packets are then transmitted using 4FSK modulation, at 100 baud.
A worked example for generating encoding these packets is available in the RS41HUP repository.
RTTY (UKHAS-Standard Sentences)
UKHAS-standard telemetry sentences, sent via RTTY can be decoded. These take the general form:
$$$$$CALLSIGN,other,fields,here*CRC16\n

Note the use of five (5) '$' symbols at the start of the sentence. This is used as a 'unique word' for packet dection, and must be present. Other quantities of '$'s will not be detected.
Only RTTY telemetry with the following parameters are supported:

Baud Rate: 100
Tone Spacing: 150 to ~1 kHz will work
Encoding: ASCII 7N2 (7-bit ASCII, no parity, 2 stop bits)
CRC: CRC16-CCITT

Hardware Requirements
Both the RTTY and MFSK modes are narrow bandwidth, and can be received using a regular single-sideband (SSB) radio receiver. This could be a 'traditional' receiver (like a Icom IC-7000, Yaesu FT-817 to name but a few), or a software-defined radio receiver. The point is we need to receive the on-air signal (we usually transmit on 70cm) with an Upper-Sideband (USB) demodulator, and then get that audio into your computer.
If you are using a traditional receiver, you'll likely either have some kind of audio interface for it, or will be able to connect an audio cable between it and your computer's sound card. Easy!
With a RTLSDR, you will need to use software like GQRX (Linux/OSX), SDR#, or SDR Console to perform the USB demodulation. You'll then need some kind of loop-back audio interface to present that audio as a virtual sound card. This can be done using:

Linux - via the snd-aloop module. Some information on this is here.
OSX - Using the SoundFlower application.
Windows - Use VBCable

You're also going to need some sort of antenna to receive the signal from the balloon payload, but I figure that's a bit out of scope for this readme!
Software Dependencies
To be able to use the horusbinary.py Python script, you will need a Python interpreter and a few libraries.
Linux / OSX
Under Linux (Ubuntu/Debian) install the required packages using:
$ sudo apt-get install git python-numpy python-pyqtgraph python-crcmod python-requests python-pip sox

Under OSX, Macports or Homebrew should be able to provide the above packages.
If the python-pyqtgraph, python-crcmod and python-requests packages are not available via your package manager, you can try installing them via pip using sudo pip install pyqtgraph crcmod requests.
Windows
Under Windows, the Anaconda Python distribution provides almost everything you need. Download and install the Python 2.7 version of Anaconda. When installing make sure the 'Add Anaconda Python to system PATH' tickbox is checked, else the below commands will not work.
Once Anaconda is installed, grab the rest of the required dependencies by opening an administrator command prompt, and running:
> pip install crcmod python-dateutil

You may wish to set the python interpreter (which should be located at C:\ProgramData\Anaconda2\python.exe) as the default program to open .py files.
Downloading this Repository
You can either clone this repository using git:
$ git clone https://github.com/projecthorus/horusbinary.git

or download a zip file of the repository from here.
Building Horus-Demod
If you wish to use the command-line demodulator (Linux/OSX only) instead of FreeDV, follow these instructions. Otherwise, skip to the next section.
Build Dependencies
We may require a few dependencies to be able to use the new modem. Under Ubuntu/Debian, you can install the required packages using:
$ sudo apt-get install subversion cmake build-essential libfftw3-dev libspeexdsp-dev libsamplerate0-dev libusb-1.0-0-dev

Compiling horus_demod
We need to compile the horus_demod binary (taken from the codec2 repository). This can be accomplished by performing (within this directory):
$ cd src
$ make
$ cp horus_demod ../
$ cd ../

Configuration File
The file user.cfg should be modified to reflect the callsign you wish to use when uploading data to Habitat.
Simply change the following section as appropriate:
[user]
# Your callsign -  used when uploading to the HabHub Tracker.
callsign = YOUR_CALL_HERE

# Your station latitude/longitude, which will show up on tracker.habhub.org.
# These values must be in Decimal Degree format.
# Leave the lat/lon at 0.0 if you do not wish your station plotted on the map.
station_lat = 0.0
station_lon = 0.0
# Radio/Antenna descriptions.
# An optional short decription of your radio/antenna setup.
radio_comment = Your Radio Description Here
antenna_comment = Your Antenna Description Here

Receiving Using FreeDV
NOTE: Horus Binary support in FreeDV is still in development.
Instructions on decoding Horus Binary telemetry using FreeDV are available here: https://github.com/projecthorus/horusbinary/wiki/FreeDV---HorusBinary-Setup-&-Usage-Instructions
Usage - Horus Demod
The horus_demod binary accepts 48khz 16-bit signed-integer samples via stdin, and can decode either RTTY or the MFSK (binary) packets. Successfuly decoded packets are output via stdout, and debug information is provided via stderr.
Suitable audio inputs could be from a sound card input, or from a SDR receiver application such as GQRX.
The horusbinary.py python script will accept decoded packets from horus_demod, and upload them to the HabHub tracker, for display on a map. Uploading to Habitat can be inhibited using the --noupload option. The --stdin option tells horusbinary.py to listen for data via stdin, instead of from UDP packets.
We can string these applications together in the command shell using 'pipes', as follows:
Demodulating from a Sound Card
$ sox -d -r 48k -c 1 -t s16 - | ./horus_demod -m binary - - | python horusbinary.py --stdin

The above command records from the default sound device.
Demodulating using rtl_fm
This assumes you want to use an rtl-sdr dongle on a headless Linux machine.
rtl_fm -M raw -s 48000 -p 0 -f 434410000 | ./horus_demod -q -m binary - - | python horusbinary.py --stdin

Tune 1600 Hz below the expected centre frequency, and make sure that your dongle has a known ppm adjustment.
Demodulating using GQRX
This assumes you have GQRX installed (sudo apt-get install gqrx) and working, have set up a USB demodulator over the signal of interest, and have enabled the UDP output option by clicking the UDP button at the bottom-right of the GQRX window.
$ nc -l -u localhost 7355 | ./horus_demod -m binary - - | python horusbinary.py --stdin

Replace binary in the above command with RTTY to demodulate RTTY telemetry.
On some platforms nc requires the listen port to be specified with the -p argument. In those cases, use:
$ nc -l -u -p 7355 localhost | ./horus_demod -m binary - - | python horusbinary.py --stdin

",2
modin-project/modin,Python,"
Scale your pandas workflows by changing one line of code









To use Modin, replace the pandas import:
# import pandas as pd
import modin.pandas as pd
Installation
Modin can be installed from PyPI:
pip install modin
Full Documentation
Visit the complete documentation on readthedocs: http://modin.readthedocs.io
Scale your pandas workflow by changing a single line of code.
Modin uses Ray to provide an effortless way
to speed up your pandas notebooks, scripts, and libraries. Unlike other distributed
DataFrame libraries, Modin provides seamless integration and compatibility with existing
pandas code. Even using the DataFrame constructor is identical.
import modin.pandas as pd
import numpy as np

frame_data = np.random.randint(0, 100, size=(2**10, 2**8))
df = pd.DataFrame(frame_data)
To use Modin, you do not need to know how many cores your system has and you do not need
to  specify how to distribute the data. In fact, you can continue using your previous
pandas notebooks while experiencing a considerable speedup from Modin, even on a single
machine. Once you’ve changed your import statement, you’re ready to use Modin just like
you would pandas.
Faster pandas, even on your laptop

The modin.pandas DataFrame is an extremely light-weight parallel DataFrame. Modin
transparently distributes the data and computation so that all you need to do is
continue using the pandas API as you were before installing Modin. Unlike other parallel
DataFrame systems, Modin is an extremely light-weight, robust DataFrame. Because it is
so light-weight, Modin provides speed-ups of up to 4x on a laptop with 4 physical cores.
In pandas, you are only able to use one core at a time when you are doing computation of
any kind. With Modin, you are able to use all of the CPU cores on your machine. Even in
read_csv, we see large gains by efficiently distributing the work across your entire
machine.
import modin.pandas as pd

df = pd.read_csv(""my_dataset.csv"")
Modin is a DataFrame designed for datasets from 1KB to 1TB+
We have focused heavily on bridging the solutions between DataFrames for small data
(e.g. pandas) and large data. Often data scientists require different tools for doing
the same thing on different sizes of data. The DataFrame solutions that exist for 1KB do
not scale to 1TB+, and the overheads of the solutions for 1TB+ are too costly for
datasets in the 1KB range. With Modin, because of its light-weight, robust, and scalable
nature, you get a fast DataFrame at small and large data. With preliminary cluster
and out of core
support, Modin is a DataFrame library with great single-node performance and high
scalability in a cluster.
modin.pandas is currently under active development. Requests and contributions are welcome!
More information and Getting Involved

Documentation
Ask questions on our mailing list modin-dev@googlegroups.com.
Submit bug reports to our GitHub Issues Page.
Contributions are welcome! Open a pull request.

",2149
magestat/magento2-split-order,PHP,"Split Order for Magento 2
This extension allows your online store to split the order into an order for each item in the cart. With different order IDs, customers can view all the order ids in their Order History and track each item separately. The admin generate separate invoices and shipments for each splitted order. Shipping charges and tax are also split based on items. This extension Magento 2 default offline payment methods: Check / Money Order and Cash on Delivery.
  
1. Installation
Install via composer (recommend)
Run the following command in Magento 2 root folder:
composer require magestat/module-split-order
Using GIT clone
Run the following command in Magento 2 root folder:
git clone git@github.com:magestat/magento2-split-order.git app/code/Magestat/SplitOrder
2. Activation
Run the following command in Magento 2 root folder:
php bin/magento module:enable Magestat_SplitOrder --clear-static-content
php bin/magento setup:upgrade
Clear the caches:
php bin/magento cache:clean
3. Configuration

Go to Stores > Configuration > Magestat > Split Order.
Select Enabled option to enable module.
Change the options selecting the attribute to split the order just like you want.

Contribution
Want to contribute to this extension? The quickest way is to open a pull request on GitHub.
Support
If you encounter any problems or bugs, please open an issue on GitHub.
Need help setting up or want to customize this extension to meet your business needs? Please email willianlkeller@outlook.com and if we like your idea we will add this feature for free or at a discounted rate.
Known issues

Doesn't work with Braintree, Paypal via Braintree, Paypal Express Checkout

© Magestat.
",33
BuckeyeSoftware/rex,C++,"Rex engine
Directory layout

src/rx contains engine source files
src/lib contains third party source files
inc/rx contains engine include files
inc/lib contains third party include files

",8
SecOps-Institute/Tor-IP-Addresses,None,"Tor-IP-Addresses
An IP Addresses list of Tor Nodes and Tor Exit Nodes
",4
52ABP/Home,None,"52ABP
我们是一个基于开源项目围绕中国特色的.NET 全栈开发社区，目前的重心是围绕.Net Core和Angular 两个生态，来搭建高可用的应用开发框架。
我们旨在开发高可用、多复用的框架、工具以及系统。
我们对分布式架构/系统，小程序、微信、APP都很有兴趣。
我们的目前是打造一套符合中国人开发习惯的框架，想包含AI, .NET Core, Linux, Docker, Jenkins, PostgreSql, Entity Framework Core, npm, yarn, vue, angular, redis, rabbitmq, mongodb, jexus, ElasticSearch, nginx, azure, kubernates, service fabric 等企业级可落地的应用框架生态服务。
关注我们

如果您觉得我的文章质量还不错，欢迎打赏，也可以订阅我的视频哦 
未得到授权不得擅自转载本文内容,52abp.com 保留版权
交流 QQ 群：952387474 点击链接加入 QQ 群《微软 MVP 带你学 ASP.NET CORE》

视频专区

【收费】腾讯课堂:https://ke.qq.com/course/392589?tuin=2522cdf3 

赞赏是某种的肯定




关注微信公众号：角落的白板报

",50
shen1986/shenBlog,TypeScript,"
小沈的个人网站
展示自己的个人博客网站
契机和概要

经历了长时间的前端学习，想把自己的学习成果展现出来，所以想做一个展示自己水平的个人博客网站。
一开始只要求能把成果做出来就行，后期可能着重点在画面的一些效果演示。
网站内容主要是分享一些个人经历，和技术文案。
预想的是前后台分离。后台主要提供接口和处理数据。前端主要是表示。

用到的技术：	//TODO

后端技术：Node.js

模板引擎： art-template
Web 开发框架： koa2


前端技术：Vue，less, html

包管理工具: webpack
蚂蚁: Ant-Design
图标字体库: icomoon
开源工具包：bootstrap

原来的栅格是12，改了源代码把栅格改成24重新编译




部署：Docker
数据持久化：mysql

实施计划

前期调查：根据github上面的网站，做一个自己的页面设计，最好有原型图
制作前台页面和后台静态页面
设计表结构。
开发阶段。
调优阶段。
发布。
维护。

具体实施		//TODO

前期调查

markdown文件的基本常用编写语法（图文并茂）。

MD学习


参照网站查找。

参照网站1
参照网站2
参照网站3
fangzh


原型图制作工具了解。

Axure工具下载
墨刀 它里面有很多都是现成的组件，直接布局拖动就好了
由于上述的学习需要一些时间，偏离主题，暂时只用用一些简单的工具来制作草图。不过多浪费时间在这上面。等有时间了再来学习。


制定基础要件，明确要做哪些范围。

前台

首页（logo）

轮播图。
最新文章
文章列表


文章

文章列表


归档

可按分类进行查询


点滴

小标签页


慢生活

生活点滴分享




后台

首页

系统信息 v1.0


文章管理

文章列表
文章添加（富文本，markdown）


收藏管理

收藏列表
收藏添加


说说管理

说说列表
说说添加


系统管理




前端的模板引擎用art-template，入门门槛较低。
代码风格和规约制定MaintainableJavaScript
基础工程创建。	（预计5月底6月初开始）


制作前台页面和后台静态页面

先做了一个自我介绍的模板html,用了art-template模板引擎。

resume
发现一个动态较牛的简历，参照着做了下，稍后会做成公用模板动态个人介绍


又做了一个服务端渲染（有利于SEO）的个人介绍,用了EJS模板引擎。(没有使用express，比较简单的实现)

resume-nodejs


除了翻页，前台页面基本做完了。

前台博客页面TypeScript版

翻页准备在后台页面做好之后再做，有数据好做点。


Vue后台前端模板页面





补充说明

标记TODO的地方以后会根据实际情况追加。
一开始用http，最后要改成Https
做2套画面，电脑和手机各一套，预计一年时间。先做PC端，手机端作为以后调优和维护的内容。
预计访问量较低，不做分布式架构。
考虑到SEO,前台页面用html加模板引擎，后台页面用VUE
PC端不做浏览器兼容。最新的IE，Google，FireFox基本能使用就行。
考虑到周期太长，采用敏捷开发的思想，先做一个个人介绍的网站以后逐步追加新的内容
这周发现hexo这个博客简化工具，现阶段先使用Hexo
https://mlab.com/
用docker的话还能使用jekyll
.art文件如何后html的变色和提示

",3
jgarber623/spaceholder.cc,Ruby,"
A space-themed image placeholder service.





Usage
Jamming on a prototype? Cranking on buildout but you don't have content images from your client yet? Drop the following in your markup and marvel at the wonders of the universe:
<img src=""https://spaceholder.cc/400x300"" alt=""This is an awesome spaceholder!"">
Replace 400x300 with whatever pixel dimensions you like: 200x50, 1200x400, 240x240. You'll be traveling the outer reaches so long as the format is a number, an x (the letter ""x""), and another number. For sanity's sake, both dimensions may be no larger than 5000.
If you'd like a square image, use a single number in the URL:
<img src=""https://spaceholder.cc/400"" alt=""This is an awesome square spaceholder!"">
Improving SpaceHolder
You want to help make SpaceHolder better? Hell yeah! I like your enthusiasm. For more on how you can help, check out CONTRIBUTING.md.
Donations
If diving into Ruby isn't your thing, but you'd still like to support SpaceHolder, consider making a donation! Any amount—large or small—is greatly appreciated. As a token of my gratitude, I'll add your name to the Acknowledgments below.


Acknowledgments
Tip o' the hat to Brad Frost for the inspiration and Alex Rehberg for the name. Many thanks to Ste Grainer for SpaceHolder's logo and visual design.
All those amazing photos are sourced from NASA on The Commons and carry no known copyright restrictions.
SpaceHolder is written and maintained by Jason Garber.
License
SpaceHolder is freely available under the MIT License. Use it, learn from it, fork it, improve it, change it, tailor it to your needs.
",26
jgarber623/spaceholder.cc,Ruby,"
A space-themed image placeholder service.





Usage
Jamming on a prototype? Cranking on buildout but you don't have content images from your client yet? Drop the following in your markup and marvel at the wonders of the universe:
<img src=""https://spaceholder.cc/400x300"" alt=""This is an awesome spaceholder!"">
Replace 400x300 with whatever pixel dimensions you like: 200x50, 1200x400, 240x240. You'll be traveling the outer reaches so long as the format is a number, an x (the letter ""x""), and another number. For sanity's sake, both dimensions may be no larger than 5000.
If you'd like a square image, use a single number in the URL:
<img src=""https://spaceholder.cc/400"" alt=""This is an awesome square spaceholder!"">
Improving SpaceHolder
You want to help make SpaceHolder better? Hell yeah! I like your enthusiasm. For more on how you can help, check out CONTRIBUTING.md.
Donations
If diving into Ruby isn't your thing, but you'd still like to support SpaceHolder, consider making a donation! Any amount—large or small—is greatly appreciated. As a token of my gratitude, I'll add your name to the Acknowledgments below.


Acknowledgments
Tip o' the hat to Brad Frost for the inspiration and Alex Rehberg for the name. Many thanks to Ste Grainer for SpaceHolder's logo and visual design.
All those amazing photos are sourced from NASA on The Commons and carry no known copyright restrictions.
SpaceHolder is written and maintained by Jason Garber.
License
SpaceHolder is freely available under the MIT License. Use it, learn from it, fork it, improve it, change it, tailor it to your needs.
",26
z1lc/core,Java,"core


Description
core is my personal repository. Most of the code is related to providing data for a
quantified self dashboard on Klipfolio. Data is
ETL'd and sent to a
PostgreSQL database hosted on
Google Cloud SQL. Hibernate is used as the ORM and schema
generator. Everything is scheduled with Quartz.
Data Sources

Anki local SQLite database
Goodreads API
Google Sheets API
HERE API
Human API
Kiva API
Last.fm API
LeetCode scraping
LIFX API
RescueTime API
RottenTomatoes scraping
Toodledo API & scraping
WakaTime API
Wikipedia: Wikimedia API, DBpedia scraping,
MediaWiki API

Dependencies

Install the gcloud sdk.

Run gcloud init, enter your credentials into browser.
When prompted, select project z1lc-qs / arctic-rite-143002.


Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to point to z1lc-qs.json. More info
here.
Install Anki, ideally a version ≥2.1.

Log into Anki and sync.
Install the AnkiConnect add-on.


To avoid having passwords and API keys stored alongside code in Git, this project uses a file called
secrets.json which provides secrets to the application at runtime. Ensure you've provided a valid mapping for each
com.robertsanek.util.SecretType within the secrets.json, and that it is located in the root directory. You can find
out where this directory is for your platform by calling
com.robertsanek.util.platform.CrossPlatformUtils::getRootPathIncludingTrailingSlash. You can refer to the
secrets.template.json file for an example of what the real secrets.json should look like.
If you plan on running the ETL command, ensure you've run the ETL_SETUP command once beforehand.

How to run
Pass a command-line argument to select one of the below (documented in Main.java).

ETL will run all ETLs and then run DQ.
DQ will run data quality checks.
HABITICA will generate an html document with a summary of Habitica dailies.
PASSIVE_KIVA will generate an html document with short-duration Kiva loans from
highly-rated field partners.
WIKI will extract basic information about popular Wikipedia articles that refer to people, outputting a csv
file and images to import into Anki.
ETL_SETUP needs to be triggered before ETLs are run.
DAEMON will run some combination of the above commands on a specified schedule. See Main.java for the
exact scheduling.

Example: java -jar target/core-1.0-SNAPSHOT.jar -command etl_setup -type manual
",3
randomwangran/myEmacs,Emacs Lisp,"Motivation
I’ve been using GNU/Emacs for two years, and I am so grateful that I
  met GNU/Emacs. It makes my life interaction with computers in a much
  more simulating way.
The current configuration is just an intermediate state of
  exploration, it is still growing up. As a relatively new user of
  GNU/Emacs, I do not know too much of it, so many times you will see
  the ill-structure and not-good-looking elisp code, but I will try to
  make it simple but not simpler.
In late 2018, I start to learn Emacs as a real-emacser. The definition
  of real-emacser to me, is to use Emacs as much as possible, to
  learn from people’s elisp code as much as possible, to write and
  share my elisp to the world as much as possible. I just want continue
  to explore this vast but magic ocean.

“What we have once enjoyed deeply we can never lose. All that we
    love deeply becomes a part of us.”

In addition to my ‘master’, you will find this README file. Basically,
  it is a documentation to this journey. It is a motivation for me and I
  hope it can, perhaps, do the same to you.
Table of Contents

Motivation
Mastering Emacs

if forget kbd?
try undo tree


Branch info
Collected questions

why people use ‘git-submodule’, what’s their pros and cons?
when using projectile to index a very large project, it takes forever.
why desktop+ is perfectly working even I am using 26.1?
how to customize the SRC block in LaTex?
how to highlight kbd in org-mode that is shown on GitHub?
what is git-annex
how to launch dired when use projectile?
~C-x C-x~ and ~C-u C-SPE~
tages table looks like a handy tool
why M-a not go back one sentence?
build c++ complier on windows
I want to use emacs as xshell entry point
annoying second cmd emacs launcher
what is ~org-habit.el~
hard time with kbd defination
What is treemacs
Writing commit message how to define a title?
how to constomize the template in magit popup windows?
open file in a general way?
my emacs getting slower
customize capture-mode so that I could add question to here more easily?
how to use git-stash to hand this situation?
how to set indentaion on the code block area?
how to cut a sentence into two but indent properlly?
what is the meaning of sharp in kill-ring?
why dired mode cannot + a folder easily
how to write this diagram in elisp?
how to disable ~/.. command?
for 1st installation on a new computer
magit: after push close the entry panel?
why M-, works in scratch buffer not other?
why Lisp-Interaction mode could automatically find TAGS file
how to use capture to resent file to here?
how to draw ascii art?
how to insert some hyper line within a org-file
what is difference between let and setq?
what is different ‘p’ and ‘P’
how to do copy file multiply times
dired creat a file call PARENT.foam
how to show parent folder name
how to do bracket expansion in dired?
how to set book mark for an org file?
how to do multiply copy to multiply subfolder
how to window explore like copy
org-mode file
how to jump like info links?
dire complicated but might rdc useful question
tty <f1> issue


Side projects

annotate editfns.c source file
read creator of smart-parents
read yin’s blog about lisp
I want to read this blog word-by-word
study centaur emacs
help documentation of spacemacs
read this .emacs


A-ha moment

insert kbd block
basic maneuver
org-mode
ivy
talking with my hero
magit
projectile
searching
jump back and forth
running tty-emacs
info file
dired
major mode
regexp replace
git lfs
macro
Fill Prefix
diredp marks subfolders
calculator
elisp regular expression
message buffer
get cursor position
find something that might exist
kbd for replace-regexp
string-insert-rectangle


appreciating

an emacs-hacker gives me a star!
a second emacs-hacker gives me a star!!
wow a Tinkoff hacker gives me a star!
blue fish
idreamshen



Mastering Emacs
By Mickey Petersen Page 90/281
if forget kbd?
Not a problem, just type C-h.
For example, you know the prefix of some commands, but don’t know
  the following stuff. Just type C-h.
C-x 8 C-h
And I finally know how to insert this guy: ←
try undo tree
undo-tree
Branch info

‘master’ is forever?
‘linux-exp’ for GNU/Linux: <2019-02-22 Fri> I gave up this
    branch because I found a new better way to manage my
    configuration for emacs
‘appreciating’ for thank you

Collected questions
Those questions will be mared as [TODO] for two months, if it is not
  answer by me, I will post those questions on-line for helping.
It seems that the time is not important. This is not a semester that
  I need to finish it. If the question is not solved. It simply means
  that I am not that interested in that question. If I need to solve
  it, no matter what (unless I am dead), I will figure it out, for
  sure.
why people use ‘git-submodule’, what’s their pros and cons?
<2018-12-07 Fri>
https://emacsair.me/2016/05/17/assimilate-emacs-packages-as-git-submodules/
when using projectile to index a very large project, it takes forever.
<2018-12-07 Fri>
There’s discussion issue. Some Windows user share their experience,
  but I have no idea with the following concepts:

‘alien’ index
‘native’ index

What are those index methods? How they work? What if I really want
  to use projectile on very large project on Windows?
why desktop+ is perfectly working even I am using 26.1?
<2018-12-07 Fri>

some times buffer fails to load the desired files

how to customize the SRC block in LaTex?
<2018-12-10 Mon>
Insteady of explorting \begin{verbatim} is that possible to
  explort {\tiny \begin{verbatim}?
I need further study following code:
(defun org-export-format-code-default (element info)
  ""Return source code from ELEMENT, formatted in a standard way.

ELEMENT is either a `src-block' or `example-block' element.  INFO
is a plist used as a communication channel.

This function takes care of line numbering and code references
inclusion.  Line numbers, when applicable, appear at the
beginning of the line, separated from the code by two white
spaces.  Code references, on the other hand, appear flushed to
the right, separated by six white spaces from the widest line of
code.""
  ;; Extract code and references.
  (let* ((code-info (org-export-unravel-code element))
         (code (car code-info))
         (code-lines (split-string code ""\n"")))
    (if (null code-lines) """"
      (let* ((refs (and (org-element-property :retain-labels element)
                        (cdr code-info)))
             ;; Handle line numbering.
             (num-start (org-export-get-loc element info))
             (num-fmt
              (and num-start
                   (format ""%%%ds  ""
                           (length (number-to-string
                                    (+ (length code-lines) num-start))))))
             ;; Prepare references display, if required.  Any reference
             ;; should start six columns after the widest line of code,
             ;; wrapped with parenthesis.
             (max-width
              (+ (apply 'max (mapcar 'length code-lines))
                 (if (not num-start) 0 (length (format num-fmt num-start))))))
        (org-export-format-code
         code
         (lambda (loc line-num ref)
           (let ((number-str (and num-fmt (format num-fmt line-num))))
             (concat
              number-str
              loc
              (and ref
                   (concat (make-string (- (+ 6 max-width)
                                           (+ (length loc) (length number-str)))
                                        ?\s)
                           (format ""(%s)"" ref))))))
         num-start refs)))))
how to highlight kbd in org-mode that is shown on GitHub?
<2018-12-13 Thu>
I made several attempts in <2018-12-13 Thu>, but none of them
  successed.
what is git-annex
<2018-12-10 Mon>
  why there is 0/155 user unhappy about it?
an comparison in 2015
how to launch dired when use projectile?
<2018-12-13 Thu>
For example, I have a *.one file find, after C-x p f, is that
  possible to use launch-file instead of directly open it?
C-x C-x and C-u C-SPE
I never use those two commands before, how to properly use them?
C-x C-x: it will mark all the region from the current cursor
  position back to the previous marking point.
But it is hard to use in the real work. previous marking point is
  hard to remember. For example, I do not know where prevous marking
  point is when I am typing this.
In what kind of situation, should I use C-x C-x?
tages table looks like a handy tool
<2018-12-18 Tue>
but! I cannot find it on Windsows machine.
why M-a not go back one sentence?
<2018-12-18 Tue>
I found it goback to the begining of a paragraph not a sentence,
  which is annoying.
build c++ complier on windows
<2018-12-23 Sun>
I found this blog looks great! Have fun if have some time.
https://caiorss.github.io/Emacs-Elisp-Programming/Emacs_On_Windows.html#sec-1-7-1
I want to use emacs as xshell entry point
<2018-12-26 Wed>
ON windows, is that possible?
annoying second cmd emacs launcher
<2018-12-26 Wed>
After I upgrade to 26.1, these’s always to icon on the bar. I want
  it goback to one.
I end up using ahk to defined WIN+e to launch runemacs.exe
At the same time, I clean everything on the bottom bar, which will
  awlays make emacs window shortcut at the begining, which will help
  me to switch the app.
In addtion, it’s a ‘minimization’. Will see how it works
what is org-habit.el
<2018-12-27 Thu>
I happen to find this file, when I was doing spacemacs project
c:/Program Files
  (x86)/emacs-26.1-x86_64/share/emacs/26.1/lisp/org/org-habit.el
hard time with kbd defination
<2018-12-28 Fri>
I want to use C-x ha to entry the habit stuff…
But it conflict with C-x h (mark-whole-buffer), which is somewhat
  a default and important emacs kbd.
How to deal with this situation?
End up using C-x w: what means what you want to do.
What is treemacs
How it is different from neotree?
Writing commit message how to define a title?
<2018-12-28 Fri>
Just leave a blank line.
how to constomize the template in magit popup windows?
<2018-12-24 Mon>
For example, when I write git commit message, I need something
  like:

reading-books: IPEL :: XXX ::: XXX

I think just add a snippet is okay, but how to achive this?
open file in a general way?
<2018-12-30 Sun>
  I found the qutebrowser’s search command promp very useful.
O KEYWORKS search-text
In my case:
I want to
SOMEKBD KEYWORKS filename
I don’t know ivy has this kind of capbility, dired+, or bookmarks?
my emacs getting slower
<2018-12-31 Mon>
If I used desktop+ to load some kits, it becomes slower especially
  when I switch between apps. Not a big issue when I am compleltely
  within the Emacs.
Seems not… When I save file it takes me ~4 seconds, which makes
  me crazy.
Remark: initilization time for Desktop+ is long, but after that
  everything good. Saving file issue is hardware issue not related to
  emacs.
customize capture-mode so that I could add question to here more easily?
<2018-12-30 Sun>
how to use git-stash to hand this situation?
<2019-01-05 Sat>
For example, I am writing this journey file
\\**** my journey
\\***** intial thoughts

I add some notes there, but have difficult inserting a in-file
  link. I starpage about how to insert ‘<<>>’, but I found another
  interesting org-mode command: C-c C-o if the marker is in source
  code block.
I found it run the command! It is so cool and the function it use
  is org-babel. I love this command, so I add something docments to
  this point.
However, when I finish the doc, I found it is really hard to
  make two comments obvious! I was editing one file, so all the
  changes will go to one commit.
I know git-stash works for this kind of dirty work, but how?
how to set indentaion on the code block area?
If the code block is automatically indent and it is in a sub-tree
  item. The indentation would be so ugly how to deal with it?
how to cut a sentence into two but indent properlly?
E.g.: this is sis is sifnidfn df.
After some magics:
this is sis i
  s sifnidfn df.
what is the meaning of sharp in kill-ring?
(nth 0 kill-ring)
#(""      I got something like this:"" 0 32 (fontified t font-lock-fontified t))
What’s the meaning of “#”?
why dired mode cannot + a folder easily
It will always generate a folder that is autocomplated. It seems
  related to emacs swiper?

~C-M-j~(ivy-immediate-done): Exits with the current input instead of the
    current candidate (like other commands).  This is useful e.g. when
    you call find-file to create a new file, but the desired name
    matches an existing file. In that case, using C-j would select that
    existing file, which isn’t what you want - use this command
    instead.

how to write this diagram in elisp?
internal of the kill ring
how to disable ~/.. command?
My patient is gone when waiting Find file: ~/..
But sometimes accident happen.
for 1st installation on a new computer
There’s always some issues with magit. It cannot be installed
  automatically. I had to do package-list-packages, which is not
  very handy.
magit: after push close the entry panel?
magit after push close panel, i.e. ‘q’
‘q’     (‘magit-mode-bury-buffer’)
‘P p’     (‘magit-push-current-to-pushremote’)

;;;###autoload (autoload 'magit-push-current-to-pushremote ""magit-push"" nil t)
(define-suffix-command magit-push-current-to-pushremote (args &optional set)
""Push the current branch to its push-remote.

When `magit-remote-set-if-missing' is non-nil and
the push-remote is not configured, then read the push-remote from
the user, set it, and then push to it.  With a prefix argument
the push-remote can be changed before pushed to it.""
:if 'magit--pushbranch-suffix-predicate
:description (lambda () (magit--pushbranch-suffix-description t))
(interactive (list (magit-push-arguments)
(magit--transfer-maybe-read-pushremote ""push"")))
(magit--transfer-pushremote set
(lambda (_ branch remote/branch)
(magit-git-push branch remote/branch args))))
Not sure how to change something in above function to run `q’.
why M-, works in scratch buffer not other?
To solve this issue, I end up study OF’s tags system.
First download `exuberant ctags’ from
  https://sourceforge.net/projects/ctags/
Then use this script to build tags:
etagsCmd=""/home/superran/bin/bin/etags -e --extra=+fq --file-scope=no --c-kinds=+p -o .tags/etags -L -""

find -H ""/home/superran/opt/OpenFOAM-dev/"" \
  \( -name ""*.[HC]"" -o -name lnInclude -prune -o -name Doxygen -prune \) | \
 $etagsCmd
Finally, use:
`M-.  xref-find-definitions` to do the awsome jump bewteen source
  code.
why Lisp-Interaction mode could automatically find TAGS file
It’s really a magit to me.
how to use capture to resent file to here?
Idea itself is important.
how to draw ascii art?
 kill-ring                  ---- kill-ring-yank-pointer
|                       |
|                       v
|     --- ---          --- ---      --- ---
 --> |   |   |------> |   |   |--> |   |   |--> nil
      --- ---          --- ---      --- ---
       |                |            |
       |                |            |
       |                |             -->""yet older text""
       |                |
       |                 --> ""a different piece of text""
       |
        --> ""some text""


how to insert some hyper line within a org-file
I found target point could be marked as:
<<target>>
To reference this target, just use square bracket to ref it.
But how to use kbd to do so?
what is difference between let and setq?
what is different ‘p’ and ‘P’
1st understanding
I read this page for understanding what is the difference between
  ~”p”~ and ~”P”~.

“p” tells Emacs to pass the prefix argument (C-u NUMBER C-x
    C-e) to the function
“P” cannot use a default number. It is coders’ responsibility to
    make sure the input argument passed into the function

2nd study
It seems my 1st understanding is not correct. I wrote this piece:
if I use “P”
(defun ran-search (STR)
  ""Searching""
  (interactive ""p"")
  (match-beginning STR))
     (ran-search 0)
I got some number
However, if I try to use “P”:
(defun ran-search (STR)
  ""Searching""
  (interactive ""P"")
  (match-beginning STR))
And I want to pass the prefix argument C-u NUMBER M-x
  ran-search ()to ran-search, it fails.
how to do copy file multiply times
Then name would be like

fileToBeCopy_copy_1
fileToBeCopy_copy_2
fileToBeCopy_copy_3
    …

dired creat a file call PARENT.foam
Everytime to creat such a dummy file is painful
	emacs get file parent folder name
    (file-name-d

     (file-name-absolute-p ""rms/foo"")

     (concat () "".foam"")

     emacs get directory name

     (defun test
	  (file-name-nondirectory (directory-file-name (buffer-file-name))))

     (buffer-file-name)

     (defun show-file-name ()
      ""Put the current file name on the clipboard""
      (interactive)
      (let ((filename (if (equal major-mode 'dired-mode)
			   default-directory
			 (buffer-file-name))))
	 (when filename
	   (with-temp-buffer
	     (insert filename)
	     (clipboard-kill-region (point-min) (point-max)))
	   (message filename))))

how to show parent folder name
I try to revise `show-file-name’ to do this without luck.
May try it latter.
  (defun show-parent-name ()
 ""Put the current file name on the clipboard""
 (interactive)
 (let ((filename (if (equal major-mode 'dired-mode)
                     default-directory
                   (buffer-file-name))))
   (when filename
     (with-temp-buffer
	(insert filename)
	(clipboard-kill-region (search-backward ""/"" (search-backward ""/"")) (search-backward ""/""))
	(message filename))))

how to do bracket expansion in dired?
I’ve attempted to do this without luck. But
  `eshell-brace-expansion’ itself is good. Need to further
  study how dired copy file.
   (defun eshell-brace-expansion (str)
  (let* ((parts (split-string str ""[{}]""))
         (prefix (car parts))
         (body   (nth 1 parts))
         (suffix (nth 2 parts)))
    (mapcar (lambda (x) (concat prefix x suffix))
            (split-string body "",""))))

(dired-do-copy (eshell-brace-expansion ""test-{a,b,c}""))

mkdir ""test-{a,b,c}""(|eshell-brace-expansion)
how to set book mark for an org file?
For example, the doc for c++ journey is so long. Each time I open
  that file, I need some extra time to find where I am.
Is that possible open the org file and then jump into exactly where
  I was?
how to do multiply copy to multiply subfolder
Marked all the files, then call “a function”, marked all the
  folders to be copied to. RET. Everythings done.
how to window explore like copy
Mark files then (A Function), every marked files are copied with a
  -prefix in the current directory?
org-mode file
For a long file like this file, is that possible to set an anchor
  point such that I could open it and marker is pointing at the most
  recent place when I open this file?
This my current solution:
I use org-store-link to mark where I am. Then I put this link at
  the beginning section of this file. A little bit easy to jump to
  working place.
how to jump like info links?
I have some links in org files. Sometimes, I want to jump between
  those links. Is that possible to press ‘TAB’ to jump in betweent
  those link. Just like the info file?
Sovled: C-c C-x C-n org-next-link
dire complicated but might rdc useful question
Is that possible to pass all the marked files (abs path?) into
  scratch buffer?
Then, by using macro, it would be much more powerful.
tty <f1> issue
I found when using emacs in tty, <f1> not working when the current
  buffer is in dired mode, but for other mode it still works.
Side projects
annotate editfns.c source file
read instrction
journey
<2019-01-16 Wed>
To view c source code, I download the emacs-26-x86_64-deps.zip.
  But it does not have a src.
study how to annotate
After see the encouragement from the creator, I try to spend
  20 minutes everyday to commit myself to this beautiful project.
(defun elsa-pluralize (word n)
""Return singular or plural of WORD based on N.""
(if (= n 1)
word
(concat word ""s"")))
(elsa-pluralize ""test"" 2)
further study type annotations

The `(elsa-pluralise :: …)` inside a comment form provides
    additional information to the Elsa analysis.  Here we say that the
    function following such a comment takes two arguments, string and int,
    and returns a string.

Emm, it’s quite hard to follow this point:
(elsa-pluralize :: String -> Int -> String)
How do we know which one is the input and which is output? For
  example, is that possible this function has one argument, String,
  generating two outputs, Int and String?
questions:

what is test predicates?
what is Cons type?
    
Cons types are specified by prefixing wrapping the `car` and
        `cdr` types with a `Cons` constructor, so `Cons Int Int` is a
        type where the `car` is an int and `cdr` is also an int, for
        example `(1 . 3)`.

I Swiper, (C-s: Cons), this is what I got:
	 ./editfns.c:664:     Consider the case where the point (`.') is between the fields `x' and `y':
	 ./editfns.c:893:	/* Constrain NEW_POS to FIELD_BOUND.  */
    
I don’t where I can find it in the C source code within
      editfns.c. Maybe this kinda data is used in other source
      code.


What is meaning of dot in this ()?
(1 . 3)
I found this explaination is great. But when I want to comment,
  I cannot do it due to insufficient reputation.
The list should be started with a single apostrophe, otherwise
  the GNU/Emacs would refuse to recognize the following as a list.
'(a b c d e)                       ; a normal list
'(a . (b . (c . (d . (e . nil))))) ; the same list in primitive form
I don’t understand how to create function types?


Function types are created by separating argument types and
      the return type with `->` token.


What does this mean?

Some type constructors have optional arguments, for example
    writing just `Cons` will assume the `car` and `cdr` are of type
    `Mixed`.

Cons are the data type in Elsa?
read this pull request
As suggested by the creator, I start to reading this pull request
  in hoping a btter understanding the data type in Elsa’s world.
After a brief go-though of this page. I still do not have a very
  clear idea of what’s going on there. For example:
     (defun add-one (x)
	 (declare (elsa (int) int))
	 (1+ x))

     (add-one 4)

     (defun simple-add-one (x)
	 (1+ x))

     (simple-add-one 4)
Why we need to declare?
(declare (elsa (int) int))
I know the purpose of this project is to check the elisp code
  without running it. But, right now, I don’t have a clear mind.
So, better to play with Elsa first.
con and cdr
why cdr and car are int?
I check the source code cdr:
	DEFUN (""cdr"", Fcdr, Scdr, 1, 1, 0,
	       doc: /* Return the cdr of LIST.  If arg is nil, return nil.
	Error if arg is not nil and not a cons cell.  See also `cdr-safe'.

	See Info node `(elisp)Cons Cells' for a discussion of related basic
	Lisp concepts such as cdr, car, cons cell and list.  */)
	  (register Lisp_Object list)
	{
	  return CDR (list);
	}
I cannot understand why:

Return the cdr of LIST

will return an interger type of data? Would it imply that it does
  some magic stuff: return CDR (list);. From a newbie’s eye,
  the return value of ~”cdr”~, will call a function CDR (list),
  which looks like recursive function?
go the minibuf.c
How does he knows there are 22 functions?
install cask
I configure the path for python, emacs, and cask today, but got
  the following error:
	       C:\Users\user\Desktop\New folder>cask init [--dev]
     Traceback (most recent call last):
	 File ""C:\Users\user\test\rep\cask\bin\cask"", line 425, in <module>
	   main()
	 File ""C:\Users\user\test\rep\cask\bin\cask"", line 387, in main
	   if ENVB.get(b'TRAVIS', b'') == b'true':
	 File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\_collectio
     ns_abc.py"", line 660, in get
	   return self[key]
	 File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\os.py"", li
     ne 666, in __getitem__
	   value = self._data[self.encodekey(key)]
	 File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\os.py"", li
     ne 735, in encodekey
	   return encode(key).upper()
	 File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\os.py"", li
     ne 730, in check_str
	   raise TypeError(""str expected, not %s"" % type(value).__name__)
     TypeError: str expected, not bytes

     C:\Users\user\Desktop\New folder>cask install
     Traceback (most recent call last):
	 File ""C:\Users\user\test\rep\cask\bin\cask"", line 425, in <module>
	   main()
	 File ""C:\Users\user\test\rep\cask\bin\cask"", line 387, in main
	   if ENVB.get(b'TRAVIS', b'') == b'true':
	 File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\_collectio
     ns_abc.py"", line 660, in get
	   return self[key]
	 File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\os.py"", li
     ne 666, in __getitem__
	   value = self._data[self.encodekey(key)]
	 File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\os.py"", li
     ne 735, in encodekey
	   return encode(key).upper()
Need Python 2.6 ? instead of 36?
Interesting, PATH read the path from beginning to the end. Cool.
issue when installing
C:\Users\user\Documents\rep\elsaTest>cask exec elsa elsa.el
cask exec: error: Failed to execute elsa elsa.el: (2, 'No such file or directory
')
Did you run cask install?
preparing to issue installation
I try to run elsa on a windows x64 machine. Not sure it was
  tested yet. If running it on Linux is a must, I would like to
  try. But, in case someone has already play with in Windows. I
  just log something that I’ve encounted.
git clone https://github.com/emacs-elsa/Elsa.git somewhere to your computer.
I’ve download elsa via git clone.
The latest commit: 78273ac.
Add (depends-on “elsa”) to Cask file of your project
I’ve created a new folder called: elsaTest, which contains two
  files:
   	-rw-rw-rw-  1 user None    643 01-24 21:06 Cask
	-rw-rw-rw-  1 user None   4.9k 01-17 09:53 elsa.el

In the Cask file:
	(source gnu)
	(source melpa)

	(depends-on ""bind-key"")
	(depends-on ""cask"")
	(depends-on ""dash"")
	(depends-on ""drag-stuff"")
	(depends-on ""exec-path-from-shell"")
	(depends-on ""expand-region"")
	(depends-on ""f"")
	(depends-on ""flycheck"")
	(depends-on ""flycheck-cask"")
	(depends-on ""htmlize"")
	(depends-on ""idle-highlight-mode"")
	(depends-on ""magit"")
	(depends-on ""multiple-cursors"")
	(depends-on ""nyan-mode"")
	(depends-on ""pallet"")
	(depends-on ""popwin"")
	(depends-on ""prodigy"")
	(depends-on ""projectile"")
	(depends-on ""s"")
	(depends-on ""smartparens"")
	(depends-on ""smex"")
	(depends-on ""use-package"")
	(depends-on ""web-mode"")
	(depends-on ""yasnippet"")
	(depends-on ""elsa"")
The elsa.el is the exactly same file I clone from:
  https://github.com/emacs-elsa/Elsa.git
Run cask link elsa <path-to-elsa-repo>

open the program: C:\Windows\system32\cmd.exe
go to the path: C:\Windows\user\Documents\rep\Elsa>_
type the command: cask link elsa .

cask exec elsa <file-to-analyse> to analyse the file. Currently only one file at a time can be analysed.

change the path to: C:\Users\user\Documents\rep\elsaTest>
show the content of this directory:
    C:\Users\user\Documents\rep\elsaTest>ls
Cask elsa.el
type the command: C:\Users\user\Documents\rep\elsaTest>cask exec elsa elsa.el

	 cask exec: error: Failed to execute elsa elsa.el: (2, 'No such file or directory
')
         Did you run cask install?


type command: C:\Users\user\Documents\rep\elsaTest>cask
    install

then, a bunch of things shown in the cmd.exe

type command: C:\Users\user\Documents\rep\Elsa>cask link
    elsa .
type command: ~C:\Users\random\Documents\rep\elsaTest>cask exec elsa elsa.el

cask exec: error: Failed to execute elsa elsa.el: (2, ‘No such file or directory
  ‘)
  Did you run cask install?~
to illustrate the installation of cask I did this:
C:\Users\user\Documents\rep\cask>cask version

C:\Users\user\Documents\rep\cask>0.8.4

add an issue
Not sure how to correctly link elsa.
This is what I’ve done:
C:\Users\user\Documents\rep\Elsa>pwd
/c/Users/user/Documents/rep/Elsa

C:\Users\user\Documents\rep\Elsa>cask link elsa elsa.git

C:\Users\user\Documents\rep\Elsa>Cannot create link elsa to non existing path:
c:/Users/user/Documents/rep/Elsa/elsa.git
cask link elsa .git

C:\Users\user\Documents\rep\Elsa>Link source c:/Users/user/Documents/rep/Els
a/.git does not have a Cask or elsa-pkg.el file
cask link elsa Cask

C:\Users\user\Documents\rep\Elsa>Cannot create link elsa to non existing path:
c:/Users/user/Documents/rep/Elsa/Cask

2nd round
I add another test.el file in that testElsa folder, and
  installed elsa via package-install (“melpa”). After that, I
  wrote the following code in test.el to test it. Hoping I could
  get the message: “Condition always evaluates to non-nil.”
(defun suspicious-if-else-branch (x)
(if t
""always runs""
""never runs""))
So I try to install flycheck-elsa:
	Status: Incompatible because it depends on uninstallable packages.
	    Archive: melpa
	    Version: 20181029.1421
	     Commit: a48d1b1c28f908dcb0bc3aece38c161059df336d
	    Summary: Flycheck for Elsa.
	   Requires: emacs-25, seq-2.0, cask-0.8.4 (not available)
	   Homepage: https://github.com/emacs-elsa/flycheck-elsa
	   Keywords: convenience 
	Other versions: 20181029.1421 (installed).
Today (<2019-02-02 Sat>) I check it:
	flycheck-elsa is an installed package.

	     Status: Installed in ‘flycheck-elsa-20181029.1421/’ (unsigned). Delete
	    Version: 20181029.1421
	     Commit: a48d1b1c28f908dcb0bc3aece38c161059df336d
	    Summary: Flycheck for Elsa.
	   Requires: emacs-25, seq-2.0, cask-0.8.4
	   Homepage: https://github.com/emacs-elsa/flycheck-elsa
	   Keywords: convenience 
	Other versions: 20181029.1421 (melpa).

	Flycheck integration for Elsa.  See README.md
It is installed. I also install flycheck today.
  Howwever, when I put the marker on the t codition always
  evaluates to non-nil not show up.
It is really hard for me to install it on windows. I try it on
  linux tonight, and I found emake is possible a good choice to
  run elsa.
try on linux
I download cask. Not sure but not sure how to build it
  correctly.  /home/superran/gitHubRep/cask/bin then, type: ./go,
  everything looks great. (I don’t know why make not working),
  so I successfully installed cask.
Although I have the following code non-nil:
	(executable-find ""cask"")
The desirable description is not there.
I download cask again from GitHub with zip file, but different
  from another machine, I got this error message:
Cloning into '/home/superran/.cask'...
fatal: unable to find remote helper for 'https'
Cask could not be installed. Try again later, or report an issue at https://github.com/cask/cask/issues

So I go back to old system.
prepare issuefile system
  The desirable description is not there, i.e, when the pointer is
  on t (Detect suspicious branching logic), the prompt should
  remind user the following message:
  
Condition always evaluates to non-nil.
steps to follow

Instead of using git clone, I use package-install:
    
cask:
            cask is a dependency package.

	    Status: Installed in ‘cask-20181107.942/’ (unsigned).
	   Version: 20181107.942
	   Summary: Cask: Project management for Emacs package development
	  Requires: s-1.8.0, dash-2.2.0, f-0.16.0, epl-0.5, shut-up-0.1.0, cl-lib-0.3, package-build-1.2, ansi-0.4.1
    Required by: flycheck-elsa-20181029.1421
	  Homepage: http://github.com/cask/cask
	  Keywords: [speed] [convenience]
    Other versions: 20181107.942 (melpa).
        

elsa:
            elsa is an installed package.

	    Status: Installed in ‘elsa-20190110.1457/’ (unsigned). [Delete]
	   Version: 20190110.1457
	   Summary: Emacs Lisp Static Analyser
	  Requires: trinary-1.0.0, emacs-25.1, f-0, dash-2.14, cl-lib-0.3
    Other versions: 20190110.1457 (melpa).

    [back]
        

flycheck-elsa:
            flycheck-elsa is an installed package.

	    Status: Installed in ‘flycheck-elsa-20181029.1421/’ (unsigned). [Delete]
	   Version: 20181029.1421
	    Commit: a48d1b1c28f908dcb0bc3aece38c161059df336d
	   Summary: Flycheck for Elsa.
	  Requires: emacs-25, seq-2.0, cask-0.8.4
	  Homepage: https://github.com/emacs-elsa/flycheck-elsa
	  Keywords: [convenience] 
    Other versions: 20181029.1421 (melpa).

    Flycheck integration for Elsa.  See README.md

    [back]
        



Add (depends-on “elsa”) to Cask file of your project
    Please see Cask file descrbed in the next section: test file info

Run cask link elsa <path-to-elsa-repo>
[superran@ip15 pureTest]$ pwd
/home/superran/pureTest
[superran@ip15 pureTest]$ cask link elsa ../gitHubRep/Elsa
[superran@ip15 pureTest]$ cask exec elsa test.el
cask exec: error: Failed to execute elsa test.el: [Errno 2] No such file or directory
Did you run cask install?
[superran@ip15 pureTest]$ ls
Cask  test.el
[superran@ip15 Elsa]$ pwd
/home/superran/gitHubRep/Elsa
[superran@ip15 Elsa]$ ls
bin               elsa.el                    elsa-extension-dash.el   elsa-pkg.el         elsa-state.el          elsa-typed-syntax.el     images
Cask              elsa-english.el            elsa-extension-eieio.el  elsa-reader.el      elsa-typed-builtin.el  elsa-typed-thingatpt.el  LICENSE
dev               elsa-error.el              elsa-extension-elsa.el   elsa-ruleset.el     elsa-typed-cl.el       elsa-type-helpers.el     README.md
elsa-analyser.el  elsa-extension-builtin.el  Elsafile.el              elsa-rules-list.el  elsa-typed-eieio.el    elsa-types.el            tests
elsa-check.el     elsa-extension-cl.el       elsa-font-lock.el        elsa-scope.el       elsa-typed-subr.el     elsa-variable.el
    

test file info
  
test.el
      	    (defun suspicious-if-else-branch ()
	      (if t
		  ""always run""
		""never run""))

	    (executable-find ""cask"")
	    (exec-path)
      

Cask
      	    (source melpa)
	    (source gnu)

	    (package ""elsa"" ""0.1.0"" ""Emacs Lisp Static Analyser"")

	    (files ""*.el"" ""bin/elsa"")

	    (depends-on ""cl-lib"" ""0.3"")
	    (depends-on ""dash"" ""2.14"")
	    (depends-on ""f"" ""0"")
	    (depends-on ""emacs"" ""25.1"")
	    (depends-on ""trinary"" ""1.0.0"")

	    (development
	     (depends-on ""elsa"")
	     (depends-on ""undercover"")
	     (depends-on ""buttercup""))

	    (depends-on ""elsa"")
      

operating system info
      	    LSB Version:	n/a
	    Distributor ID:	CentOS
	    Description:	CentOS Linux release 7.5.1804 (Core) 
	    Release:	7.5.1804
      

emacs version
          GNU Emacs 26.1 (build 1, x86_64-pc-linux-gnu) of 2018-06-29
          

minor modes
          	    Enabled minor modes: Auto-Composition Auto-Compression Auto-Encryption
	    Blink-Cursor Electric-Indent File-Name-Shadow Font-Lock Global-Eldoc
	    Global-Font-Lock Ivy Line-Number Menu-Bar Mouse-Wheel Override-Global
	    Projectile Shell-Dirtrack Tool-Bar Tooltip Transient-Mark
	    Window-Numbering
          




questions
why I cannot open C source code within HELP buffer?
For example:
cdr is a built-in function in ‘C source code’.

(cdr LIST)
Return the cdr of LIST.  If arg is nil, return nil.
    Error if arg is not nil and not a cons cell.  See also ‘cdr-safe’.
See Info node ‘(elisp)Cons Cells’ for a discussion of related basic
    Lisp concepts such as cdr, car, cons cell and list.

If RET when marker is on C source code, swiper just throw me a
  brunch of meanless completions. But it works for elisp code.
For example, if I put my point under myemacs, then f1 f, I
  will imediately jump into the HELP buffer:
And if I RET at ‘~/.emacs’  I will jump into myemacs source code
  at the exactly the position of this function.
read creator of smart-parents
read yin’s blog about lisp
I want to read this blog word-by-word
study centaur emacs
help documentation of spacemacs
https://github.com/syl20bnr/spacemacs/issues/11741
mission
my mission is to summarize the commits on this page into
  CHANGELOG.develop file.
questions
too much deletion?
When I open this commit, I found that the author has deleted
  something in CHANGELOG.org file.
Maybe, this should not to be include to this commit.
Further more, I’ve notice that the author was editing something
  related org-edn. What’s that?
what is edn
When I was checking the commit, I found author used org-edn-xx
  what are they?
I found that: edn is a data format, which can be found at this.
what is encoding and utf-8
In reading edn stuff, I found I have no idea what’s this.
why spacemacs do not use (provide 'xx.el)
<2018-12-28 Fri>
In [[https://github.com/syl20bnr/spacemacs/blob/87a2145b4a77ab836d5ff6cccbf4f3fb17d87186/core/tools/export/_worker.el][_worker.el]], there’s no (provide ‘_worker.el), which is not
  the case in most of my packages in my emacs.d files.
Why they don’t need to do this?
what is Haskell?
Pure function programming. Sounds cool.
what == in org-mode file
<2018-12-28 Fri>
For example: what's this?
suggestions

In the document, use alphabet order to do the log?

read this .emacs
A-ha moment
insert kbd block
C-c u
basic maneuver

new frame C-x 5 2
delete frame C-x 5 0
mark function C-M h

org-mode
movement
within same level
C-c C-f
C-c C-b
goback to upper level
C-c C-u
links
navigation
C-c C-o (org-open-at-point): jump between markers internally
  C-x & (org-mark-ring-goto): jump back
inserting
creating new headline
same level: put markers on header: C-RET
snippet
<2018-12-16 Sun>
When editing in the org-file,
is that possible let <s TAB expand to elisp code?
Further more, can I add some information on the head so that one
  can expand the source code by the main programming language the
  document is working on.
For example, in this file, one expect it will expand to elisp. In
  some c++ file, it will expand to c++?
what is save-excursion ?
<2018-12-16 Sun>
The author claim that the use of save-excursion is good
  hoursekeeping, but I do not understand.
How to use save-excursion?
C-x n s, you need 100% focus.
Go back:
C-x n w
s for short; and w for wide.
C-x n n, if region being mark, I can focus on the important
  stuff.
C-x n w, again back to normal case
manipulation

C-o (org-open-line): quit handy to edit doc with more space
[not sure how to define kbd for this function]
    (org-store-links): very useful for internal links, but it only
    works for header? After store the links, inserting is pretty
    easy. Nice job, my emacs.

table

C-c SPC handy deletion

(org) Built-in table editor

`C-c SPC     (`org-table-blank-field’)’: delete with much more
    ease

copy column
decent method

mark the column
c-x r r r
c-x r i r

ivy

C-c C-o save search buffer to a new buffer
    I use it to illustrate what I’ve done to the matched string.

talking with my hero
I happen to find this secret someday without checking
  magit manual. After reads your reply, I’ve checked magit manual,
  nothing about it.
I then go to git manual, this page discussed my secret.
They suggest less than 50 character. I did an experment: on this
  commit
It has a maximum number of 69 visable character.
I see you point. The namingspace issue is a big concern if commit
  messages are written in this way.
I do not have too much knowledge on how to organize commit
  message. I see people using CHANGELOG.org stuff to track the
  commit (For example). But they also have have some headaches.
Maybe, just using the namingspace would be an easy way.
I also try to find extend the number 69, for example to 100, to
  make title long for you, but I do not find this info.

Magit 20180620.1224, Git 2.12.2.windows.2, Emacs 26.1, windows-nt

==P.S.
Something unrelated to this comment.
I just want to express my thanks to you Henry Weller. Althought
  I’ve ackknowleged OpenFOAM foundation in thesis. It is quite
  different if I could express my appreciation letting you know.
OpenFOAM not only helps my thesis, it, more importantly, opens a
  door to the vast occean that I never even being to. So many great
  people, so many opensource projects (GNU, emacs, etc…).
After graduation, I still using your source code to do numerical
  analyses and I am enjoying usying your code :). Although I have
  tons of tons things that I do not understand, but I will learn them
  little by little.
Thank you,
magit
movement
alt-p/n: jump between blocks
projectile
movement
-D Opens the root of the project in dired in another frame.
searching
Windows
searching a file with name
M-x everything
  Then give the file name.
searching a string in current folder
grep -s ""delete-and-extract-region"" * .*
searching a string in all subfolder
grep -rl ""clone()"" ./*
jump back and forth
M-. runs the command xref-find-definitions
back:
M-, runs the command xref-pop-marker-stack
running tty-emacs
Using git-bash is good enough, ‘alt+RET’ gives you a much better
  tty experience even you are in a M$ Windsows machine.
info file
This is a fantastic tool for reading manual.
To use my own dir file: do this within the file ~/.bash_profile
INFOPATH=.:$HOME/gitHubRep/myEmacs/ref/myInfo/:/usr/local/emacs/info
export INFOPATH
To refresh into file just edited, go to one of the node and jump
  back it will then refresh.
where to put manual?
On a windows, I put them in C:/info/ folder.
On linuxs, everything installed already, but I do not have access to change it.
manuipulating

M-n create another windows

refresh a new info file
C-c C-m C-b within a *.info source code.
<2019-02-27 Wed> not sure why C-c c-m is bonded to C-c RET?
‘C-c C-m C-b’ ‘M-x makeinfo-buffer’ should work.
  `C-c RET C-b’  makeinfo-buffer should also work.
refresh the info buffer: d
Although c-m is stop by RET it still works.
project to write info file with my computer related notes
dired
compress or uncompress
compress:

mark the files
press ‘c’
input the name in mini buffer: *.tar.gz

uncompress:

press ‘Z’
yes

replace a string in file
Once you have a *.py, make it as a TEMP file, where TEMP is the
  key parameters (PATH).
Then copy this file with unique names.
Mark the file.
Press `M-q`, just as you do for replacement in a buffer.
show marked list files
C-M-l
‘C-c C-v’ like in Windows

w
+
revise name

major mode
There is only one major mode for a given buffer, you cannot turn
  off a major mode, but you can switch to another.
regexp replace
This will append some texts at the end of each line.
M-x replace-regexp <RET> ^.\{0,72\}$ <RET>
\,(format ""%-72sABC%05d"" \& \#) <RET>
I also found that the mini buffer is so useful. No need to retype
  all those regexp, just ust `M-n’ to do some little adjustment.
git lfs
I want to track those big file in the remote but I don’t want them
  locally. This is an experiment I did.
I added this to `.gitconfig’ in $HOME
  [lfs]
	fetchexclude = *

However, I found that it is hard to manually fetch the file I
  want. Sometimes, I need a file that is ignored by this global
  configuration.
As a result, I delete this `.gitconfig`.
macro
f3 f4
  then f4
Fill Prefix
Emacs makes me “wow” this morning again.
‘C-x .’
  Set the fill prefix (‘set-fill-prefix’).
So, I do not need to M-x reg…-replace everytime I need to comment
  region in writing Matlab code.
diredp marks subfolders
dired+: diredp-mark-files-regexp-recursive

to mark all subfolder: M-+ % m then type “reg-exp”
    I found a more robust way to do recursive mark:
passing a possitive prefix: C-u 1 M-+ % m

to unmark: `C-u 1 M-+ % m`

Do shell comand recursively:

‘M-+ &’

calculator
Super handy!
`C-x * q`
Even the kill ring is shipped.
elisp regular expression
backslash
I was about to ask the following question
(defun reg-test ()
  (re-search-forward ""c[ad]\{1,2\}r""))
;; -> reg-test: Search failed: ""c[ad]{1,2}r""

M-x re-search-forward RET c[ad]\{1,2\}r
;; mark move to end of car

In elisp, you need use double backslash to represent single
  backslash in regular expression.
info:elisp#Regexp Special

Note that ‘\’ also has special meaning in the read syntax of Lisp
    strings (*note String Type::), and must be quoted with ‘\’.  For
    example, the regular expression that matches the ‘\’ character is
    ‘\’.  To write a Lisp string that contains the characters ‘\’,
    Lisp syntax requires you to quote each ‘\’ with another ‘\’.
    Therefore, the read syntax for a regular expression matching ‘\’ is
    ‘”\\”’.

message buffer
To make all the message in the centered of the buffer, just move
  the cursor at the end of Message buffer.
get cursor position
What if I know this function early?
C-x = : what-cursor-position
find something that might exist
M-x apropos
kbd for replace-regexp
C-M-% runs the command query-replace-regexp
I was about to change this kbd and was thinking which kbd to
  bind. I use M-% a lot, so I guess C-M-% would be a great choice. At
  the time I check this kbd, I find Emacs is inline with me!
Wow.
Don’t forget to press ‘!’ after input the content.
string-insert-rectangle
To use this function:

set mark at the upper left corner of the rectangle
move the point to the last raw of the rectangle
input the content of the rectangle

blablabla
  blablabla
  blablabla
  blablabla
  blablabla
  blablabla
  blablabla
  blablabla
blablabla
  blablabla
  blablabla
  blablabla
  blablabla
  blablabla
  blablabla
  blablabla
blablabla
  blablabla
  blablabla
  blablabla
  blablabla
  blablabla
  blablabla
  blablabla
  blablabla
  blablabla
appreciating
an emacs-hacker gives me a star!
I am so excited! Nobody gives me a star on my-emacs utill his
  appreciating. He is the creator of smart-(). It’s a remarkable day.
a second emacs-hacker gives me a star!!
Wow, he used org-mode to configure stuff, pretty cool. Thank you.
wow a Tinkoff hacker gives me a star!
Oh, man, he is an havily user of (use-package), so cool.
blue fish
He/She is using doom/space, which I cannot read them easily. I
  found the comments in init.el is funny.

(ivy              ; a search engine for love and life
latex ; writing papers in Emacs has never been so fun
perl  ; write code no one else can comprehend
(markdown +pandoc)  ; writing docs for people to ignore

idreamshen
Although s/he does not share a .emacs, I found one of articles in
  his/her blog is very useful:
git clone -b 'v2.0' --single-branch --depth 1
https://github.com/git/git.git
Only clone a branch that you are interested in. I will implyment
  this work flow to my project template.
",5
megabulk/Fix-for-iTunes-file-truncation,None,"Fix-for-iTunes-file-truncation
Fixes the problem described here
with the solution described here
The script loops through all files selected (or in playlist, if nothing's selected), removes them from iTunes, re-adds them, and plays each for 1.5 seconds. Re-adds star rating, play count, last played date
To do

Ask user if they want to do this crazy thing!

",2
hexo-simple-theme/hexo_ejs,JavaScript,"DEMO
",2
shopizer-ecommerce/shopizer-shop-angular,CSS,"Shopizer
Demo : https://shopizer-angular.herokuapp.com/
Development server
Run ng serve for a dev server. Navigate to http://localhost:4200/. The app will automatically reload if you change any of the source files.
",2
CHEF-KOCH/Warez,HTML,"


The biggest Warez list on the entire internet! This list is an overview of Warez related topics, discussions and provides some background information about the scene.
Disclaimer
I or GitHub do not supporting warez - we're also not responsible for external links or their content! If you dislike the information I provide then contact the website owner/webmaster/hoster directly and fill a DMCA request.
Why was the list created?

Research reasons!
Freedom of information!
Most lists you find are outdated, not actively maintained or full of malware links.

Credits
Since this list is huge I can't name each and every single one of you, but here is a short credit list which hopefully covers all of them:

All contributors.
All demoscene people which helped to build the scene.
All developers which build the mentioned programs on this list.
All other people which helped to push this little project forward.
All people which are involved in the piracy & netsec scene or created forums to talk about such topics.

What does not belong on this list?

Illegal programs (cracks, keygens etc.) or direct links to copyright material.
Links to banned websites.
Links to offline websites.
Software which is outdated or not actively maintained, open source software is preferred.
Insecure programs (no audit- or code-review).
Why is program x not listed e.g. K-9 mail, it's FOSS! Not every mention makes sense, especially in a security context.

How can I contribute to this list?

You can submit a pull request - after you read the contributions guidelines.

How to search on this page?

A search function is planned, among other small features.
Since there is (currently) - no filter or search you can navigate trough the page from within your Browser via Ctrl + F or Cmd + F.

Project structure

.github - Documents, Todo etc
Homepage - General Overview (which gets a search/indexer)
Readme.md - This file, provides a smaller overview (includes only website links and no tutorials etc).
Scene Info.md - Beginner Info to get into the scene and warez.
Tools.md - Lists all programs related to warez.
[Extensions & Scripts.md] - Lists Browser Scripts + Extensions which might makes your pirate life a little bit easier.
Tutorials.md - Lists all warez related tools.
Banned.md - Lists all banned groups, programs and topics
Offline.md - Lists all offline or compromised websites.
Trusted.md - Lists trusted groups or people which are known to crack or upload stuff.

Contact


Liability for Contents

MEPs approve sweeping changes to copyright law
The Legalities of Linking
COPYRIGHT LIABILITY FOR LINKING AND EMBEDDING - Klaris Law (.PDF)
EU court says linking to copyrighted material isn't illegal
IP Address is Not Enough to Identify Pirate, US Court of Appeals Rules - (.PDF)
New EU Piracy Watchlist Targets Key Pirate Sites and Cloudflare - (.PDF)
Domain Registrar Can be Held Liable for Pirate Site, Court Rules
Reporting When Pirate Releases Hit The Internet is Apparently Illegal Now
Web Sheriff
List of websites blocked in the United Kingdom

Social Media Alternatives

MeWe - Google+ replacement.
Pleroma - Host your own social media.
Mastodon - Like a decentralized Twitter.
Movim - Movim is a social network, based on XMPP, with Chat and chatrooms, news & communities features.

Penetration Testing Distributions

Android Tamer - OS for Android Security Professionals. Includes all the tools required for Android security testing.
ArchStrike - Arch GNU/Linux repository for security professionals and enthusiasts.
AttifyOS - GNU/Linux distribution focused on tools useful during Internet of Things (IoT) security assessments.
BackBox - Ubuntu-based distribution for penetration tests and security assessments.
BlackArch - Arch GNU/Linux-based distribution for penetration testers and security researchers.
Buscador - GNU/Linux virtual machine that is pre-configured for online investigators.
Kali - GNU/Linux distribution designed for digital forensics and penetration testing.
Network Security Toolkit (NST) - Fedora-based bootable live operating system designed to provide easy access to best-of-breed open source network security applications.
Parrot - Distribution similar to Kali, with multiple architecture.
PentestBox - Opensource pre-configured portable penetration testing environment for Windows OS.
The Pentesters Framework - Distro organized around the Penetration Testing Execution Standard (PTES), providing a curated collection of utilities that eliminates often unused toolchains.

Public Reverse & Cracking Discussion Forums

Cracked.to - Cracked.to is a cracking forum and community.
Cracked A forum for cracking related stuff.
Crackia - Crackia Cracking Forum.Find the latest Cracking info, Premium Account Cracking Forum.
Cracking Forums - GeoIP Ban - Cracking Forum, Cracking Tutorials, Free Premium Accounts.
Cracking Pro  - Cracking Tutorials, Free Premium Accounts, Cracking Configs, Combolists & Proxylists.
CrackingGOD Forum -  Cracking. Hack. Graphics. Webmaster. Marketplace.
Crackmes.cf - (mirror of crackmes.de + reboot)
Forum ExeTools - The original old school forum to share cracking knowledge.
Reverse Club - (needs invite code)
Team-IRA [TIRA] - TIRA Team International Reversers Alliance (needs invite code)
Tuts 4 You - One of the oldest forums to discuss reversing related stuff.

NFO Viewers & KeyGen Music

Defacto2
Evangelion.keygenmusic.org
KeyGen Music
Keygen Music Archive
NFO Force

Anti-DRM Protects, Plugins & Source Code

[C++] Steamless - SteamStub DRM Remover + Homepage
HexRaysPyTools
Microsoft Research Detours Package

VPN Subscription Services (no-logs)

ExpressVPN ExpressVPN - A VPN with 256-bit encryption, over 94 countries, and no logs. Also rated as one of the fastest VPNs out there.
NordVPN - Protect your privacy online and access media content with no regional restrictions, and audit can be found here
Private Internet Access - Popular subscription-based VPN provider with a proven track record for not keeping logs.
ProtonVPN - High-speed Swiss made VPN that safeguards your privacy.

Self-hosted VPNs

n2n - A Peer-to-peer VPN.
OpenVPN - OpenVPN provides flexible VPN solutions to secure your data communications, whether it's for Internet privacy, remote access for employees, securing IoT, or for networking Cloud data centers.
PeerVPN - PeerVPN is a software that builds virtual ethernet networks between multiple computers.
Pritunl - Enterprise Distributed OpenVPN and IPsec Server.
sshuttle - Transparent proxy server that works as a poor man's VPN.
WireGuard VPN - WireGuard is an extremely simple yet fast and modern VPN that utilizes state-of-the-art cryptography. It aims to be faster, simpler, leaner, and more useful than IPSec.

Ad-Blocker

An Overview of Ad Blocking Technology - Basically explains how an Ad Blocker works.
AdGuard - Claims to be the worlds most advance Ad Blocker.
Filterlists - Provides an overview of Ad blocking filters/projects.
uBlock origin - A fast and efficient Ad Blocker.
Nano Defender - Fork of uBlock with sme gimmicks, the defender addon is an anti-adblock defuser for Nano Adblocker and uBlock Origin.
Wikipedia's Website about Ad blocking - Wikipedia's website about ad blockers.

Piracy focused discussion Channels & Blogs

/f/Piracy - Raddle forum for piracy.
/r/privacy - The intersection of technology, privacy, and freedom in a digital world.
Prism Break - Opt-out of global data surveillance programs like PRISM, XKeyscore and Tempora.
TechWorm - Techworm is a Tech, Cyber-security news platform.
TorrentFreak - TorrentFreak is a publication dedicated to bringing the latest news about copyright, privacy, and everything related to filesharing.
/v/piracy - Voat Forum for piracy.

Archives

GOG Dump
Nintendo Games Collection
Microsoft Games Collection

Email Service Providers

Mailbox - Mailbox.org fights for privacy eMails since years and is a big player when it comes to eMail.
Posteo - Email green, secure, simple and ad-free!
ProtonMail - Secure Email Based made in Switzerland.
Tutanota - A secure and open source eMail provider.

Temp eMail Service Providers

10 Minute Mail - Disposable, Private mailboxes.
Cock.li - Yeah, it's mail with cocks.
Disposable - Disposable is arobust disposable email (burner emails) - API designed to help you verify whenever email address is coming from disposable service.
Nada - Fast & free.
Temp Mail - Keep spam out of your mail and stay safe - just use a disposable temporary email address!

eMail self-hosting

docker-mailserver - A fullstack but simple mailserver (smtp, imap, antispam, antivirus, ssl...) using Docker.
FastMail - Email, calendars and contacts done right.
Rainloop - SIMPLE, MODERN & FAST WEB-BASED EMAIL CLIENT.
Roundcube - MIME support, address book, folder manipulation, message searching and spell checking.

DarkNet

DarkNet Stats - Monitors DarkNet Forums & Markets.

Anti-Spammer

Project Honey Pot - Online fraud & abuse tracker.

Audio

Mp3va.com - Free Music (MP3s).
New Album Releases - New Album Releases.

Android

Wikipedia's list of free and open-source applications

APK Forums & Platforms

ACMARKET - Download cracked & modified android apps & games free.
Androeed - (RU) - Russian APK site.
Android Zone - Another place to find premium links for APKs.
APKmb - Download paid Apps & Android Games for free.
APKMirror - Provides legit mirrors with checksums for various apps.
ApkPure - Another free APK mirror site.
Aptoide - Cracked and legit apk's.
BlackMod - Lots of cracked Android games.
iHackedit - Provides Android Apps & Games including Mods.
libre.io - (requires login) - A small forum with some exclusive apps & games.
Mobilism Forum - Large forum of mobile apps and books.
On HAX - APK mirrors for paid and free applications, the website also provides modded APK's.
RevDl - Direct download site for Android apps & games.

Decentralized Networks

Freenet - Freenet is free software which lets you anonymously share files, browse and publish ""freesites"" (web sites accessible only through Freenet) - and chat on forums, without fear of censorship.
I2P - I2P is an anonymous overlay network - a network within a network. It's intended to protect communication from dragnet surveillance and monitoring by third parties such as ISPs.
Loki - Decentralised network that allows users to transact and communicate privately over the internet.
SILO - Offers complete privacy across the network (work in progress project in cooperationship with Loki).
Tor - Tor is free software and an open network that helps you defend against traffic analysis.
Zeronet - Open, free and uncensorable websites, using Bitcoin cryptography and BitTorrent network.

Hardened Operating Systems & Resources

cuckoo - Open source automated malware analysis system.
Overview of Security-focused operating system on Wikipedia
Qubes OS - Qubes OS is a security-oriented operating system.
SIFT - Forensic workstation made by SANS.
Security related Operating Systems @ Rawsec - Complete list of security related operating systems.
Security @ Distrowatch - Website dedicated to talking about, reviewing, and keeping up to date with open source operating systems.
Tails - Tails is a live operating system that you can start on almost any computer from a USB stick or a DVD.

Domain Names

Domainr - Domainr allows you to find domain names and short URLs. Instantly check availability and register for all top-level domains.
Njalla - A privacy-aware domain registration service.
xip.io - Magic domain name that provides wildcard DNS for any IP address.

Countries where downloading copyright content is legal (for personal use only) though publishing it & sharing it - is still illegal

Poland
Spain
Spain
Switzerland

Countries where both downloading & sharing is illegal

Argentina
Bangladesh
Brazil
Canada
Chile
Columbia
Czech Republic
Denmark
Egypt
Greece
India
Iran
Israel
Malaysia
Mexico
Netherlands
Philippines
Romania
Singapore
Slovakia
Slovenia
South Korea
Uruguay

Countries where torrenting is highly illegal

Australia
China
Finland
France
Germany
Itlay
Japan
Korea
Latvia
Portugal
Russia
UAE
UK
US

Countries GeoIP block and/or shutdown the websites

Canada
China
Germany
India
Itlay
Japan
Latvia
Malaysia
Portugal
Russia
Singapore
South Korea
UAE
UK
US

Torrenting

/r/torrents - Questions and discussion about all things torrent-related.
BitTorrent - Wikipedia's article on the BitTorrent file sharing protocol.
Live Tracer - Pre-time tracer for scene releases.
magent2torrent.me - Converts magnet links to torrent files.
peerflix Google Search - Searches Heroku-deployed instances of Peerflix for streaming torrents.
RapidBay - Rapid bay is a self hosted video service/torrent client that makes playing videos from torrents easy.
The Pirate Society - A members-only forum for pirates.
Torrage - Torrage is a free service for caching torrent files online.
Torrents.csv - An open source - collaborative repository of torrents, consisting of a single, searchable torrents.csv.
Torznab - Newznab-like API offering a standardized recent/search API for both TV and movies.
Where are torrents permitted? - A world map (picture) which shows where warez torrenting is allowed.
ZBIGZ - Zbigz is a very good solution if your PC or laptop is locked by your ISP, it allows you to download torrents via Browser.

Trackers

/r/trackers - A subreddit for discussing public & private trackers.
A Simple Guide To A Better Ratio - A good tracker requires you to upload what you download. This guide explains many of the methods involved with keeping on top of this sometimes difficult task.
Bravo List - A public tracker directory.
Tracker Twitters - List Of Private Torrent Trackers & BitTorrent News Accounts To Follow On Twitter.
Trackers List - An updated list of public BitTorrent trackers.

TV-Show Calendar

at.my TV - TV Calendar, TV Episode Guide, TV Show Listings.
DuckieTV - A personalized calendar that tracks the shows you like

Private Trackers

/PTG tracker manifesto - List of private trackers
0QoLttS.jpg - Screenshot of a table from somewhere of private trackers and their sign-up requirements
AlphaRatio - (AR) - A good starter tracker with lots of freeleech content.
AnimeBytes - (AB) - community centralized around Japanese media, including anime, manga, and music.
Audionews - (AN) - Private torrent tracker for music production audio. (DJ apps, audio editor, DAW apps etc) - Open signups on the 1st-2nd every month.
Awesome HD - (AHD) - Awesome-HD is a private tracker for quality enthusiasts.
BakaBT - (BBT) - a torrent tracker which specializes in serving anime fans
BeyondHD - (BHD) - BeyondHD is a ratioless torrent tracker dedicated to HD movies and TV shows in High Definition.
Bibliotik - (BI) - Popular ebooks/audiobooks private tracker
Bitspyder - (BS) - Bitspyder is an educational torrent site devoted to e-Learning content such as e-Books, video courses, and audio books.
Blutopia - (BLU) - Blutopia is a private tracker for HD movies and HD TV shows.
CGPeers - (CGP) - CGPeers is a private torrent tracker for all things computer graphics: tutorials, graphics software, 3D, visual effects, design, and computer-assisted art.
Filelist - (FL) - Large Romanian general tracker with mostly English content. No RAR files allowed. Scene torrents are unrared, and then allowed.
GazelleGames - (GGn) - Currently the largest private tracker for games.
HD-Forever - (HD-F) - HD-Forever is a French private tracker for HD movies.
HD-Space - (HDS) - HD-Space is a private torrent tracker hosting HD movies, TV shows, and music torrents. A good tracker for beginners.
HD4Free - (HD4F) - HD4Free is a general HD tracker with a good range of content.
IPTorrents - (IPT) - Private tracker with movies, books, and more.
JPopsuki - (JPop) - JPopsuki is a torrent tracker focused on Asian music.
MyAnonaMouse - (MAM) - Private E-Learning tracker with about 360 000 torrents including audiobooks, e-learning, musicology, and radio.
MySpleen - (MS) - MySpleen is a private tracker which specialises in comedy, animation, and TV series.
Nostalgic Torrents - (NT) - Private tracker for anime, comics/manga, documentaries, movies, TV - PRE 2013, TV - PRE 2009 With Original Commercials, etc. Also known as The-Archive and HeyNow.
PassThePopcorn - (PTP) - ratio-based torrent tracker for movies (requires login).
PolishSource - (PS) - PolishSource is a big private Polish ratio-less tracker.
PolishTracker - (PT) - PolishTracker is the oldest private Polish tracker.
Private Tracker Flowchart - V4 of the private tracker flowchart. Somewhat out of date.
Private trackers - Guide on how to get into (and survive) - the world of private trackers.
PrivateHD - (PHD) - PrivateHD is a private BitTorrent tracker focused on high definition movies and TV show torrents.
RED Interview Prep - This site was written as a guide for potential users to learn about music formats, transcodes, torrenting, and burning and ripping — everything you need to know in order to pass the RED interview.
Redacted - (CH) - Largest private music tracker at 1.5 million torrents.
SceneTime
TheGeeks - (TGBZ) - Private tracker for e-learning
TorrentLeech - (TL) - Well-known popular private tracker
Tracker Spreadsheet - Comprehensive spreadsheet of private trackers (somewhat out of date).
TVChaos UK - (TVCUK) - Private tracker for British television
UHDBits - (UHD) - UHDBits is a Vietnamese private torrent tracker focused on HD movies and TV shows.
WorldOfP2P - (WOP) - Private tracker for Movies, TV and general P2P stuff.

Semi-Private Trackers

ArenaBG - A Bulgarian tracker with an English translation available.
NoNaMe Club - Russian semi-private tracker and forum.
ruTracker - RuTracker is a huge Russian torrent site with a thriving file-sharing community.
Zamunda.net - A Bulgarian tracker with English and Russian translations available.

Public Trackers

1337x - 1337x is a torrent site that offers verified torrent downloads.
BTDB - Large BitTorrent DHT search engine.
DIGBT - DIGBT is a DHT torrent search engine.
ETTV + EZTV - ETTV is a torrent site specific for movie torrents.
g4u - (Ger) - Movies, TV Shows & Games.
Games4theworld - Torrents and magnet links for games.
GloTorrents - Download Movies, TV, Games and Other Torrents Free.
Goldesel - (Ger) - Games, Movies, Audio & eBooks.
HDSector - Bollywood / Hindi / Hollywood HD Movies.
Idope (Clone) - iDope is a torrent search engine presenting direct magnet links, comments and up to date seeder/leecher statistics.
Isohunt2 - Clone of the original ISOHunt torrent index and repository.
KickAss Torrents - Community-made reincarnation launched in 2016.
LimeTorrents - LimeTorrents has been around for more than half a decade.
MagnetDL - Magnet link only search engine.
metal-tracker.com - Heavy metal music tracker.
Monitor Shodan - Keep track of the devices that you have exposed to the Internet. Setup notifications, launch scans and gain complete visibility into what you have connected.
moviemagnet - Verified torrents for movies.
OTorrents - Yet another public torrent tracker.
Pirateiro - Pirateiro is a torrent index for Brazilian and Portuguese torrents.
RARBG - Public tracker with its own release group, RARBG was founded in 2008 and specializes in high quality video releases.
Remove fake TPB torrents - Script that automatically hides fake torrents on The Pirate Bay based on conditional logic.
Rustorka - (RU) - Software, Games & More.
rutor - Russian public tracker.
Saavn - A search engine designed to find old and new music releases.
Shodan - Shodan is the world's first search engine for Internet-connected devices.
SkyTorrents - Revival of the recently-shut-down, privacy-focused, ad-free torrent indexer.
The Pirate Bay - Well-known torrent site which is somehow still running, blocked in most places. (Be warned: It mines coins in the background!)
The Proxy Bay - Can't access The Pirate Bay? Try one of these proxy sites.
Tor Lock - TorLock is a torrent site that offers verified torrent downloads.
Torlock - Torlock is a torrent index and torrent search that helps to access the latest in TV series and movies.
Torrent9 - French torrent search engine.
TorrentFunk - TorrentFunk is a torrent site providing verified torrents for all kinds of content.
TorrentGalaxy - Public tracker with a clean UI.
TorrentInvites - #1 To Buy, Trade, Sell Or Find Free Tracker Invites!
TorrentKing - Torrentking is a popular movie torrent site.
Torrentz2 - A good replacement of the defunct Torrentz.eu.
trackerslist - An updated list of public BitTorrent trackers.
WebOas - A search engine designed to find warez, music and other stuff in public dirs.
WorldWide Torrents - Another public tracker with a reasonably nice UI.
xbit.pw - A Magnet site search engine.
YggTorrent - French tracker and search engine (have a download/upload ratio limitation).
YTS - Small-size HD movies in a good quality from YIFY.
Zonatorrent - Spanish tracker.
Zooqle - Zooqle is a relatively new torrent index providing a huge database of verified torrents.

Tracker Aggregators

AIO Search - Torrent search engine.
High Resolution Music - FLAC Music collection.
rats-search - P2P Bittorrent search engine.
snowfl - Snowfl is a torrent aggregator which searches various public torrent indexes in real-time.
SolidTorrents - A clean, privacy focused torrent search engine.
Torrents.me - Torrents.me combines popular torrent sites and specialized private trackers in a torrent multi-search.
TParser - Russian torrent sites indexer, good for FLAC music other other stuff.

Tracker Proxies

Cardigann - A proxy server for adding new indexers to Sonarr, SickRage, and other media managers.
CouchPotatoServer - Automatic Movie Downloading via NZBs & Torrents.
Jackett - API Support for your favorite torrent trackers, it translates queries from apps (Sonarr, Radarr, SickRage, CouchPotato, Mylar, DuckieTV, qBittorrent, etc) - into tracker-site-specific http queries, parses the html response, then sends results back to the requesting software.
NewzLeech - Newzleech is a Usenet file search engine.
nzbhydra2 - Primarily a Usenet meta search engine but also supports Torznab.
Radarr - A fork of Sonarr to work with movies à la Couchpotato.
SickRage - Automatic Video Library Manager for TV Shows. It watches for new episodes of your favorite shows, and when they are posted it does its magic.
Sonarr - Smart PVR for newsgroup and Bittorrent users.

Tracker Invites

/r/Invites - Post wanted ads for private tracker invites here.
/r/OpenSignups - Open Signups - When Private Trackers Open Their Doors To The Public.
BTRACS - An automatic information site which periodically checks closed community BitTorrent trackers for being open for signup.
getting_into_private_trackers - Helpful resource from the /r/trackers Wiki.
Open sign-ups thread - /r/trackers thread for posting trackers that are currently open for registration.
Opentrackers.org - Private Torrent Trackers & File Sharing.

rTorrent

flood - A web UI for rTorrent with a Node.js backend and React frontend.
rTorrent ArchWiki Page - Detailed article to answer most common questions about rTorrent.
rTorrent Seedbox Guide - This guide is a single-page, comprehensive guide to take you step-by-step through installation and configuration.
rtorrent-ps - Extended rTorrent distribution with a fully customizable canvas and colors, other feature additions, and complete docs.
rTorrent - rTorrent is a text-based ncurses BitTorrent client written in C++.
rutorrent-themes - A collection of default and new, original themes for ruTorrent.
ruTorrent - Yet another web front-end for rTorrent.

WebTorrent Clients

Instant.io - Streaming file transfer over WebTorrent (torrents on the web).
magnetoo - Fancy new in-browser WebTorrent streaming service.
WebTorrent Desktop - WebTorrent Desktop is for streaming torrents.
βTorrent - Fully-featured WebTorrent - browser client written in HTML, JS and CSS

Seedboxes

/r/seedboxes - A place to discuss seedboxes and everything related to them.
2Giga.link - Free file hoster, Torrent caching & Premium link generator.
Bitport.io - Download torrents in the cloud and stream them online.
DediSeedBox - Netherlands located Seeedbox service.
FileStream.me - Free subscription offers 200Mb max file size and 200GB storage total.
Furk.net - Free trial offers 1GB per day or 5GB per week if you can get an invite/voucher or use Facebook.
Put.io automator - A suite of commands for managing torrents, transfers and files on Put.IO.
Seedr - Essentially a seedbox you can paste torrents into which returns a streamable direct link.
Seedbox.io - Provides 99,9% uptime, cheap and claims to have a good support.
SeedSync - SeedSync is a GUI-configurable, LFTP-based file transfer and management program.
Torrent Safe - Free-plan includes 1GB max file size, 2 days file lifetime.
UltraSeedbox - Cheap Seedbox, fast and reliable.
ZXCFiles - A similar service that allows you to paste magnet links or upload torrent files and get a DDL. First 20GB are free.

Seedbox Hosting Providers

/u/Andy10gbit - Reddit user with good deals on servers and seedboxes.
Bytesized Hosting - ""The best Plex server hosting in town"".
Chmuranet - Chmuranet is a small private boutique seedbox provider.
FeralHosting - Shared seedbox hosting provider.
Hetzner - Reliable and affordable server host.
Kimsufi - Affordable dedicated servers.
NZB Monkey - NZB download helper utility.
Online.net - Seedbox-friendly, affordable, dedicated server host.
OVH - Large cloud server provider.
Seedboxes.cc - Reliable and affordable web hosting, with the power of your friendly monsters!
SeedHost - ""Seedhost.eu is the oldest continuously operating seedbox hosting provider on the internet.""
SoYouStart - Another dedicated server host.
UltraSeedbox - ""Plex optimized"" servers to rent.
Whatbox - Whatbox is a BitTorrent CDN.
Xirvik - Preconfigured seedbox servers.

Tracker Frameworks

Gazelle - A web framework geared towards private torrent trackers with a focus on music.
meanTorrent - A BitTorrent Private Tracker CMS with Multilingual, and IRC announce support, CloudFlare support.
NexusPHP - BitTorrent private tracker scripts written in PHP.
OpenTracker - OpenTracker is an open and free BitTorrent tracker project.
Torrent-Tracker-Platforms - A Curated List Of Torrent Tracker Platforms/Codebases Written In Multiple Coding Languages.
UNIT3D - The Nex-Gen Private Torrent Tracker (Aimed For Movie / TV Use).

Usenet Providers

Eweka News - Netherlands-based Usenet provider.
Newsdemon - Cheap and cheerful Usenet provider with frequent discounts.
Newsgroup Ninja - $7.99 per month, SSL encryption, Unmetered usage, Unlimited speeds.
Newsgroup Ninja - Popular Usenet provider with a competitive subscription fee.
Premiumizer - A download management tool for premiumize.me cloud downloads.
Tweaknews - Dutch Usenet provider that offers a Highwinds news feed.
Usenet Crawler - Movies, eBooks, TV Series, Anime & more.
Usenet Providers and Backbones - This is a simple overview of the current companies, backbones, providers and resellers in the Usenet landscape.
Usenet.Farm - Usenet reseller with 1000+ days retention.
UsenetExpress - UsenetExpress is a powerful new tier-1 Usenet provider which offers strong security, a 10GB uplink per server and up to 150 streams for an excellent price.
WorldSrc - Movies, software, apps, games, music, and images available for fast direct download + torrents.
XS News - European Usenet feed.

Usenet Indexing Software

nZEDb-deploy - A collection of scripts to automate and simplify the deployment of a nZEDb Usenet Indexer using the new format of their GitHub repository.
nZEDb - A fork of nnplus(2011) - | NNTP / Usenet / Newsgroup indexer.
newznab-tmux - Laravel based usenet indexer.
newznab - Newznab is a usenet indexing application, that makes building a usenet community easy.

Usenet Paid Indexers

DOGnzb - Invite-only NZB site (although they do have a registration page - at the moment)
DrunkenSlug - Popular NZB indexer with a free tier and decent retention.
NZBCat - NZBCat is an invite-only nZEDb NZB indexer.
NZBFinder - Usenet indexer and newznab API with a clean UI and 8+ year backlog of NZBs.
NZBgeek - Affordable Usenet indexer operating since 2014.
omgwtfnzbs - Invite-only NZB indexer with a funny name.

Usenet Free Indexers

6box - A recently revived free Usenet indexing service with a generous API.
Binsearch - With this site you can search and browse binary Usenet newsgroups.
NZBIndex - The first free Usenet indexer you find in your Google search results.
NZBKing - The service allows you to search and browse binary files that have been posted to Usenet newsgroups.
Usenet Crawler - Usenet indexer with API access for registered users.

Portables & Repacks

FoxxApp PAF Portables - Windows Software repacks and portables.

Custom ""Google"" Search Engines

FileChef - Get direct download links for almost anything!
Jimmyr - Yet abother Music search engine.
lumpySoft.com - A Google index search with predefined tags.
mattpalm.com/search - Get direct download links for almost anything.
Musgle - Musgle in action just type a song title, or the artist name, or both in a search bar and hit 'Enter' - you will be redirected to the Google page with relevant search results. Click on one of those results, and you will have a chance to directly download the song you are searching for - very smooth!
opendirectory-finder - Get direct download links for almost anything.
The Eye CGS Engine - The-Eye - CGS Engine
wtfnzb - Open dir for Software.

FTP Indexers

Davos - Web-based FTP automation for Linux servers.
Mamont's open FTP Index - Browsable directory listing of publicly available FTP-sites.
Napalm FTP Indexer - NAPALM FTP Indexer lets you search and download files located on public FTP servers.
pftp - pftp means Port-File-Transfer-Program not to muddle up with standard FTP which is quite different, it allows you to send and receive directories recursively and move the dirs.

DDL Search Engines and Crawlers

Alluc - Search engine with over 80 million streaming-links from over 700 VOD services, video hosters and file-hosters.
IPLIVE - DDL search engine.
MegaSearch - Search engine for finding content hosted on Mega and other premium hosts like OpenLoad.
OD-Database - Database of searchable open directories curated by The-Eye.eu.
ololo - ololo is a video streaming link search engine.
Orion - Orion is a service that indexes metadata and links from a variety of public websites and networks, including torrent, usenet, and hoster indexes.
VideoSpider - VideoSpider crawls various websites and search engines to find movie and TV episode streaming links.

GoG Repack & Releases + Retro Games

Good-Old-Downloads/gg - A fork of Good Old Downloads' ""GOG Games"" hosted on Tor.
Good-Old-Downloads (GitHub source code) - - Good-Old-Downloads is shooting down, see here - why. There is a full encrypted dump avbl. here, make sure you read the integrated readme file!
Torminatorr.com - Good-Old-Downloads mirror page.

DDL Link Sites

/r/DataHoarder/ - Share download links (similar to /r/opendirectories/).
/f/MEGAlinks - Aims to replace the old /r/megalinks directory.
/r/GDriveLinks - (Multi) - Google Drive Download Links.
/r/ZippyShare - (Multi) - DL links hosted on ZippyShare (blocked in the UK).
0DayDown (CN/EN) MacOS, Music & other links (works with JDownloader).
3dl.tv - (Ger) - Music, Movies, TV Shows, apps & more.
Adit-HD Forum - Forum which provides links to HD rips.
AdiT-HD - Direct movie download database.
AppNee Freeware Group - Massive DDL site, eBooks, Programs, Games, Operating Systems, etc.
Audioz - Provides Audio stuff.
AvaxHome - Another DDL site with eBooks, TV, movies, magazines, software, comics, newspapers, games, graphics, etc.
AVXHome - Best of eBooks, Software, Mag & more.
BetaArchive - Windows ISOs, Windows tools & more.
Board4All - A forum which provides and shares all sorts of stuff.
Boerse.to - (GER) - A german warez forum.
Byte.to - (GER) - Movies (SD/HD/UHD), Docus, WWE & Series.
DDL-Warez - (GER) - German software, movies & tv board.
DDLValley - DDL links for Movies, Games, Tv Shows, Apps, Ebooks and Music.
DeeJayPirate's Pastebin - Pastebin user who uploads premium links for TV shows
DirtyWarez Forum - (EN-US) - Popular warez forum with films, TV shows, ebooks, anime, games, and more.
DL4All - Various Games, Tutorials, TV Shows, Music and mobile stuff.
Dospelis - Spanish DDL indexer.
DownArchive - DDL blog with premium links on a number of hosts. Lots of software
DownloadLY.IR - (IR) - Software download portal.
DownTurk - Software portal.
ExeLab Forums
FilmRls Movies, Series and TV Shows.
filewarez.tv - Invite-only, hosts both Mega and Google Drive links for TV shows
FTP Mirrors - Windows ISO Downloads Mirrors (ftp links not visible in GitHub's Markdown!)
GLOAD.cc - (Ger) - Provides Zippyshare and Openload as download-friendly mirrors.
hdencode - Videos/Movies in HD Quality.
IceFilms.info - Another DDL site with TV and movie links on FileUpload, GoUnlimited, Filecandy, and more.
Intercambios Virtuales - (ES) - Yet another software portal.
KickassWarez - (EN/RU) - Sometimes offline, hosts TV, movies, magazines, software, comics, newspapers, games, graphics, etc.
LavTeam - (RU) - Another russian software portal. Like dust in the desert!
Mawto - (IR) Android apps, Windows Software & Games.
MaxRelease - Games, software, magazines, movies, music & tv shows.
MkvCage -  Big tracker for TV shows & movies.
Movie Glide - Videos/Movies & TV Shows in HD Quality.
MovieFiles - Direct download search engine which generates Google Drive links
Moviesleak - Yet another movie page, the focus is in IMDB annoucements.
Mutaz - Tracks scene software releases.
MyGully - (Ger) - Replacement for the old gulli board.
NaMaMe Club - Provides software for Windows.
NFForce - The old NForce group and it's small website.
NFOHump - Gaming, Apps, Help & more.
NGB.to - (Ger) - Another Gulli/MyGully clone.
Nsane Forum - Public forum for everyone to talk about software & news around the world.
NulledTeam Underground - Software forum.
Onkyo4k - Tracks movies and tv shows.
OSZone - (RU) - Software portal.
PCPortal - (RU) - One of the biggest russian software forums.
PSARips - Popular site for movies and TV shows, includes torrent files
PuZo.org - Direct Download (DDL) Websites for sofwtare.
RADIXX11 - (EN-US) - Software forum.
RapidLinks - (RU) - Software, Movies & more.
RapidMoviez - Direct movie website.
Reduson - (EN/RU) - Another software portal for various type of releases like software, magazines, games and many more.
ReleaseBB - (RU) - Russian Software Portal.
Releaselog - Website for eBooks, Games & more.
RSLinks - Tracks scene software releases and mirrors them.
Rsload - (RU/EN) - A big software portal.
RU-Board - (RU) - Well-known warez board.
RuTracker - (RU) - If you didn't alreay knew RUTracker you never heard of Warez at all.
SceneSource - WordPress powered website dedicated to bringing you the latest info on new scene releases
Serials - Serial keys for software that may or may not work.
SilentGround - (EN)
Snahp Forum - Forum which provides links to HD rips & software (only mega & zippyshare links are allowed).
Soft9 - (RU) - Russian Software Portal.
Soft98.iR - (IR) - All Software Download in Only One Website.
SoftArchive - (RU/EN) - Software portal for various software and scene announcements.
SoftOBase - (RU) - Software forum.
SolidShare - (TR) - Software Portal.
Tekspert - (EN/US) - Software forum.
TheWarezFolder - For All Your Download needs.
TNCTR - (TR) - Turkish Network Community for software, ebooks, apps, portable, AIO & coding.
Twighlight - Warez release blog (sometimes offline!).
TwoDDL - Direct software download links.
UpTown - Software portal.
Vidics - Vidics provides Tv shows and movie releases.
Warez-BB-org - Elite warez forum. ![(invite needed)][inviteneeded]
WarezBB.org - Invite-only elite forum.
Warezforum Asia - PDF's.
Watch Series - WatchSeries provides TV Shows (as the name might already suggests).
watchepisodeseries - Watch Episode Series provides TV Shows releases from the scene.
WatchTVSeries - Watch TV shows online.
Win7DL - Software, Movies, eBooks & more.
ZeroBoard - A board which provides Windows related stuff.

DVB

DVBKing - SkyStar, SoftCam Key ProgDVB Satellite Receiver Dish Network DirecTV HD TV.

Premium Link Generators

File Hosting Wiki - This site aims to provide the most complete lists of premium link generators, torrent downloaders and more, with (possibly) - frequent updates.
Free Premium Leeches - Search up for the website you want to download from and pick from a massive list.
Small File sharing table - Small File sharing table which shows maximum file limit, language etc.
Small cloud storage table - Small cloud storage table which shows maximum file limit, language etc.
OffCloud - A simple, elegant and intuitive SaaS to retrieve any data from the cloud.
Mega-Debrid - 207 Hosters supported.
Premiumize - Combine direct and secure access to premium services.
Premium Link Generator - Over 26 hosts to choose from (needs paid account).
Real-Debrid - Real-Debrid is an unrestricted downloader that allows you to quickly download files hosted on the Internet or instantly stream them into an innovative web player.
Reevown - A free download service with which you can perform premium downloads.
UploadedPremiumLink.xyz - Generate online premium links.

Premium Link Hoster

4shared
DBREE
K!M - Might replaces the original Megaupload (one day).
Mediafire
Mega.nz
NitroFlare
OpenLoad
PutLocker
RapidGator
Sendspace
Uploaded
Zippyshare - - Blocked in the UK, use a proxy or VPN.

Open Directories

""All resources I know related to Open Directories"" - Thorough post from /u/ElectroXexual.
/r/opendirectories - Unprotected directories of pics, vids, music, software and otherwise interesting files.
36 GB of Flash Games - Posted by /u/blue_star_.
FileMasta - Search servers for video, music, books, software, games, subtitles and much more.
httpdirfs - A filesystem which allows you to mount HTTP directory listings.
opendirectories-bot - Bot used on /r/opendirectories for analysing the contents of open directories posted on the subreddit.
Panelshow.club - Directory of panel show TV episodes from /r/panelshow.
The-Eye - The Eye is a non-profit website dedicated towards content archival and long-term preservation.
The Holy Grail of Indexes - Posted by /u/shadow_hunter104.

Anime & Cartoon Streaming (720p+)

480mkv
AnimeKisa - Subs and dubs, no ads, funded by donations, self-hosted.
Animelon - Subs only, multilingual, no ads, funded by donations, only one source, videos hosted by Google, aimed for Japanese learners.
Aniwatcher - Subs and dubs, pop-up ads on video player, downloadable, multiple sources.
AnimeFreak.TV - Subs and sometimes dubs, banner and pre-roll ads, one source.
9Anime - Subs and dubs, many ads, many player alternatives, videos hosted by Google.
Animehd47 - Subs and dubs, banner ads, videos hosted by Google, multiple sources.
Animeboys - (Ger) - Subs and dubs, banner and pop-up ads, multiple sources.
Animefever - Subs only, multilingual, banner ads, self-hosted.
AnimeLand - Dubs only, banner ads, pop-ins, videos hosted by Google (proxy), only 1 source, downloadable.
Anime Rush
Anime Show
Anime Streams - Sub and dub, banner ads.
Anime8
AnimeBam - Subs only, banner ads, only one source.
AnimeDao - No ads, subs only, multiple sources, videos hosted by Google.
Animeflv
AnimeFreak
AnimeHeaven
AnimeHub - Sub and dub, banner ads.
AnimePahe - Subs only, pop-up ads, doesn't show videos with adblocker on, downloadable, only one source.
AnimePie - Subs and dubs, no ads, multiple sources(including from other anime sites).
Animer Reborn
AnimeRush - Subs only, pop-ins, one source.
AnimeSeries
Anime Simple - Subs and dubs, banner ads, multiple sources.
AnimeTV - Subs and dubs, banner ads, many player alternatives.
animeultima - Subs and dubs, banner and pop-up ads, multiple sources.
AnimeVibe - Subs and dubs, no ads, multiple sources, downloadable, funded by donations.
AnimeWorldBD - Banner ads, some videos can only be downloaded, small list.
AnimeXD
Cartoon Crazy
CartoonWire
Chia-Anime
Club Anime
DaiWEEB - Subs only (EN and JP), no ads, only one source, self-hosted, aimed for Japanese learners.
DarkAnime.stream - Subs, no ads, downloadable, some sources.
DubbedAnime - Subs and dubs, banner and pop-up ads, multiple sources.
EyeOnAnime - Subs and dubs, banner ads, uses multiple uploaders/players.
GoGoAnime - Subs and dubs, many ads, many player alternatives.
Hi10Anime
HotAnime - Subs and dubs, lots of banner ads, pop-ins, possibly self-hosted videos, downloadable (via OpenLoad).
Justdubs
Kawaiifu - Videos hosted by Google, only one source, style similar to niconico, missing multiple anime.
KickAssAnime - Subs and dubs, banner ads, some sources.
KimCartoon
Kissanime.ru - or Kissanime.ac (mirror)
KissCartoon
Mangarock
Mejor Torrent - (ES)
MioMio
MoviesEver
NineAnime
OtakuStream - Subs only, banner and pop-up ads, downloadable, multiple sources.
Online WatchCartoon
Otakustream
PutlockerSeries
Randaris Anime - Banner ads, multiple sources, eng & german subs, captcha.
RyuAnime
Serienjunkies - (GER)
SGAnime - Subs only, no ads, few sources.
Supercartoons
Toonova
TVBox
TVRaven
Twist.most
WatchCartoon
WatchCartoonsOnline
WatchAnime - Subs and dubs, banner and pop-up ads, multiple sources.
WatchSeries 2.0
WatchSeries

Specialty Sites

1Liberty - (fr-FR)
6VoierFilms - (fr-FR)
Classic Cinema Online - Classical Films
Classic Movies Channel - Youtube Classic Films
Cliver - Espanol
Club MST3k - Every episode of MST3K
Cuevana2 - Espanol
DaebakDrama - Korean
Danimados - Espanol
DPStream - (fr-FR)
DramaCool - Foreign
Dramago - (fr-FR)
Einthusan - Foreign
EmuleIsland - (fr-FR)
Film1k - - Movies with nudity
Filmz.cc - (fr-FR)
FilmZen - (fr-FR)
FrenchStream - (fr-FR)
K-Streaming - (fr-FR)
KingsofHorror - - Youtube Horror
Layarkaca - Foreign
MutantSorority - - Youtube Horror
Pelisplus - Espanol
Rulu - Youtube Red Series
StreamComplete - (fr-FR)
Time2Watch - (fr-FR)
Top Documentary Films - Documentaries
TromaMovies - Youtube Horror
VFStream - (fr-FR)
Videoneat - Documentaries/Science Movies
WatchAsian
Moviezworldz Hindi movies & TV Shows.
WTvF! - Youtube Grindhouse

Random Streaming Sites

/r/BestOfStreamingVideo
/r/MovieStreamingSites
#1 Movies Website - Watch movies online for free in HD quality without downloading or signing up.
1Movies - Watch Free HD movies online & free download movies at 1movies.pl.
Arabseed - (AR) - Online shopping from a great selection at Digital Music Store.
AZMovies - AZMovies your best source for watching movies online, with High Quality 1080p movies, you can stream anytime.
cine.to -  Sit back and relax while watching the newest Cinema or your favorite Movie for free. Just cine.to & chill.
cinebloom - Action Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy History Horror Music Musical Mystery Romance Sci-Fi Sport Thriller War Western.
Daxiv Video - Movies & TV shows online or stream right to your smart TV, game console, PC, Mac, mobile, tablet and more. Primarily Chinese content.
DP Stream - (FR) - Films/Series/Animés a votre dispositions sur différents herbergeurs. dpstream.net.
eMule Island.ru - (FR/RU) - Site de téléchargement gratuit, Telecharger des films complet, series, ebooks, spectacles, documentaires et bien plus, sur uptobox, 1fichier.
Filmstream.online - (FR) - Regarder des films gratuits illimités de sur Filmzenstream. Regarder complet des films en streaming hd gratuitement vf sans inscription en française.
FilmXY - Download Free Unlimited Movies Online From Filmxy At Great Quality!! Here You Can Download Movies in Bluray, 1080p, 720p, HD, HDTV, Web-dl, DvD-rip & more
Filmz.cc - (FR) -
Flixanity - Watch movies and TV shows online. Watch from devices like iOS, Android, PC, PS4, Xbox One and more. Registration is 100% free and easy.
FlixGo - Ralph Breaks the Internet. Avengers: Infinity War. Incredibles 2. Ant-Man and the Wasp ... Dawn of Justice. Captain America: Civil War. FlixGo.
FMOVIES - Openload, MyCloud, RapidVideo, Streamango
French Stream - (FR) - Regarder Facilement et Gratuitement Les Meilleurs Films et Séries en Streaming HD Sans aucune Publicité Gênante ...
HD MOVIES - Watch free movies online in 1080p at HDM.to - Stream & download the latest HD movies online for free without registration.
HDEUROPIX - Free Streaming HD Movies Online with captions. Full Movies Streaming Popular TV Series Watch Free HD topeuropix.net.
HDO - Watch HD Movies Online For Free the latest movies, tv-series without Registration at hdonline.to.
HDOnline - Free Movies Online!
INDOXXI - Movies releases.
KStreaming - (FR) - Film Streaming et Série Streaming Gratuit.
libertyVF - (FR) -
LookMovie - Watch Movies and TV Shows for Free in 1080p and 720p. New Movies and Episodes are added every hour.
M4UFree.TV - Free Movies Online. Watch Movies Online Free. Watch all your favorite movies and tv shows online for free on M4ufree .
MegaShare - Watch Full Movies and TV Series Online Free.
MKVHub - MkvHub is the best website to download high-quality 720p, 1080p WEB-DL HDRip BluRay Movies and TV Shows with Single Direct Download Link.
Movie123 - Look no further than Movie123 if you are looking for the best sites to watch free movies online.
Nox - (Ger) - Filme, HD-Filme, 3D-Filme, Serien und Spiele - News. Reporters Without Borders.
onemov - Online Full HD Movie Free.
openloadmovies.net - Reliable movie streaming site which uses OpenLoad.
PelisPedia - (es-do) - Movies & TV Shows.
QQMovies - Stream Movies and TV Shows online in HD quality, 1080p, 4K.
Qwemovies - Watch HD Movies Online For Free and Download the latest movies without Registration at qwemovies.to.
Rainierland - Official home of rainierland - no ads and only good movies.
Sokrostream.vip - (FR) - 2019 Films · 2018 Films · Action · Science-Fiction · Comédie · Horreur · Rapport
Solarmovie - Watch Movies Online and Watch Tv-Series online On Solarmovie without Registration.
Streamcomplete - (FR) - Streaming gratuit des films en VF, Regarder les meilleurs sélections des films complets en version française a voir online.
StreamCouch - Watch free the newest movie stream indexed as they appear online, in HD high quality.
StreamCR - Watch movies and TV series online for free. Stream episodes of Game of Thrones, Breaking Bad, Stranger Things and more!
TakiART - Watch and download latest Hollywood movies for free.
Time2Watch - (FR) - Films, séries et mangas en streaming et téléchargement gratuit pour PC, iPhone, iPad et autres Smartphones.
VF-Streaming - (FR) - Voir Les Meilleurs Films, Séries Et Manga En Streaming HD Gratuit Sans inscription Sur VF Stream Venez découvrir les derniers films complet en français.
VodLocker - Official home of vodlocker - no ads and only good movies.
VoirFilms.ws - (FR) -Voir Film Streaming, Streaming Film, telecharger, Films, regarder film streaming, dvdrip, film en streaming, voirfilms, gratuit.
WatchFree - Watch Movies Online Free. Watch your favorite movies and tv-series in hd quality on watchfree.to + putlocker.
WatchFullMovie - Watch 1000 Free full Movies on IMVBox. Free Live TV, Serials and Theatre on IMVBox
xPau.se - Movies, TV Shows & more.
Yes! Movies - Watch movies full HD online free. Watch latests episode series online. Over 9000 free streaming movies, documentaries & TV shows.
XMovies8 - Best site to watch free movies online, just search your favorite movies and Enjoy.
YMovies - Watch Free Full Length yify Movies Online on Yify TV. Torrents, Watch Films online in HD 720p and 1080p quality yts on Yify TV.

Video Game Music (OST)

FFShine Forum - A place for game and video game music.

Sports Streaming

/r/CFBStreams
/r/MLBStreams
/r/mmafights
/r/MMAStreams
/r/motorsportsstreams - Reddit community for motorsports streams.
/r/nbastreams
/r/ncaaBBallStreams
/r/nflstreams
/r/NHLStreams
/r/redsoccer
/r/rugbystreams
/r/WWEstreams
720pStream
Best Sport Streaming - Site that rates sport streaming services.
BossCast.net
cricfree.sx
Cricfree - Offers popular sports streams.
cytu.be/r/heemstream
Drakula Stream
EPCTV
firstrownow.eu
footybite - Soccer streaming site.
Giostreams
Kickboxing - (PO)
LiveTV - Wide variety of sports, results/live scores, video archive and betting.
MamaHD - 24/7 feeds, sports streams offers a clean UI.
myfeed2all.eu
MyP2P
Rojadirect
Send It - Live stream listings for sports, news, gaming, and more.
serbiaplus.club
SportsHD
Stream2Watch
Streamwoop
taima.tv/r/mma
Time4TV
TV Link
VIP Box Sports
VIPBox - (Spanish) - Many sport streams, TV which as a friendly UI.
vipleague.sx
Wiziwig

Media Centre Applications

Emby - A personal media server with apps on just about every device.
Gerbera - UPnP Media Server for 2018 (Based on MediaTomb).
Kodi - An award-winning free and open source home theater/media center software and entertainment hub for digital media.
Myflix - Myflix tries to be a somewhat simple and lightweight ""DIY Netflix"", similar to Plex, streama or Emby, for your DIY NAS, especially aimed at the Raspberry Pi/Odroid/etc ecosystem.
OpenPHT - A community driven fork of Plex Home Theater.
OSMC - OSMC (short for Open Source Media Center) - is a Linux distribution based on Debian that brings Kodi to a variety of devices.
Serviio - Serviio is a free media server. It allows you to stream your media files (music, video or images) - to renderer devices (e.g. a TV set, Blu-ray player, games console or mobile phone) - on your connected home network.
Streama - Self hosted streaming media server.
Stremio - Multi-platform video content aggregator with a comprehensive add-on system for extending functionality
Subsonic - Music and movie streaming server with a client app and web frontend.
Viewscreen - A personal video streaming server.

Stremio Addons

Juan Carlos Torrents - Allows streaming from torrents collected from KAT.cr and others.
Open Directories addon - Finds HTTP streams for movies/shows from open directories.
PirateBay addon - Fetch PirateBay entries on a single episode or series.
Popcorn Time addon - Watch from YTS and EZTV in Stremio
RAR addon - Watch content from RARBG in Stremio.
Zooqle addon - Watch movies and series indexed by Zooqle from RARBG, KAT, YTS, MegaTorrents and other torrent trackers.
/r/plexshares - A nice place to find Plex Media Server shares.
BaconFeet - ""Bringing a difference in streaming to the masses..."".
CDRomance - PSP, PSX, PS2, Gameboy, NDS, SNES, Dreamcast, and Gamecube ROMs and ISOs.
Elysium - Plex media streaming service.
MediaButler - Discord bot for use with PleX and several other apps that work with it.
Mellow - Discord Bot which can communicate with several APIs like Ombi, Sonarr, Radarr and Tautulli which are related to home streaming.
Ombi - Want a Movie or TV Show on Plex or Emby? Use Ombi!
Plex Requests - Simple automated way for users to request new content for Plex
plexrequests-meteor - Meteor version of the original Plex Requests
redump.org - Disc preservation database and internet community dedicated to collecting precise and accurate information about every video game ever released on optical media of any system.
Steamless - Steamless is a DRM remover of the SteamStub variants. The goal of Steamless is to make a single solution for unpacking all Steam DRM packed files. Steamless aims to support as many games as possible.

Plex Logging and Metrics

Plex-Data-Collector-For-InfluxDB - Collects data about your Plex server and sends it to InfluxDB
plexWatch - Notify and Log watched content on a Plex Media Server
Tautulli - Tautulli is a 3rd party application that you can run alongside your Plex Media Server to monitor activity and track various statistics.

Kodi

/r/Addons4Kodi - Discussion and links pertaining to unofficial addons for Kodi Media Center
Elementum - Elementum addon is an addon for Kodi, that manages your virtual library, syncs with your Trakt account.
Exodus Redux - The newest Exodus fork around, paired with LambdaScrapers.
Gaia - Grants the ability to instantly watch high quality files via cached torrents from Real-Debrid or Premiumize.
kodi-headless - A headless install of kodi in a docker container, most useful for a mysql setup of kodi to allow library updates to be sent without the need for a player system to be permanently on.
Official Plex Addon - Official Plex add-on for Kodi.
Placenta - A Fork of Exodus / Covenant with more options and links from Mr Blamo and Muad'Dib.
PlexKodiConnect - Plex integration in Kodi done right.
Plexus - Plexus is used in conjunction with Sparkle to play Ace Stream links.
Sparkle - Kodi addon for finding acestream links.
Tooonmania2 - lets you watch cartoons, dubbed anime and movies (from animetoon) - and subbed anime and movies (from animeplus).
tvtorrentorganizer - Bash 4 Script to Organize TV Show Downloads for Kodi
Ultimate Kodi Guide - ULTIMATE GUIDE TO INSTALL KODI + POPULAR STREAMING ADDONS by /u/giorgiomilan
Yoda - Another solid Exodus/Covenant fork, and this time it's from S-media.

Gaming Infos, Emus & More

/r/CrackWatch - New video game crack releases are posted here.
CreamAPI AutoInstaller - A python script to auto install Cream API for Steam games in order to get all DLCs for free.
Goldberg Steam Emulator - The  project is an attempt to make a generic Steam ddl that lets you play multiplayer games on a LAN without any internet connection.
SmartSteamEmu - A Steam emulator.

Game Repacks

""A simple script for easily downloading emulator.games ROMs"" - Reddit guide and userscript created by /u/estel_smith to allow you to easily download ROMs from Emulator.Games.
BlackBox
Dark Umbra - Forum for sourcing games.
DODI
ElAmigos Games - Premium links to cracked games.
Emulator.Games - Download or play ROMs on your PC, Mobile, Mac, iOS and Android devices.
FitGirl Repacks - Popular DDL and torrent site for game repacks.
Kaoskrew - Repacks from popular repacker SKIDROW, CPY, Razor1911, PLAZA etc.
Kapital Sin -
Nicoblog - Plenty of ISOs, ROMs & repacks.
qoob.name - Repacker site of popular cracker teams like CPY, PLEX etc.
R.G Mechanics - Various repacks.
Revolt Group - Official Revolt website.
Skidrow Repacks - Repacks from popular repacker SKIDROW. Lots of anime stuff too!
Tapochek - Official R.G Mechanics repacks can be found here.
Xatab Repacks - Russian game repacker - primarily torrents.

ROMs

3DSISO - A community based ROM database.
ByAlvRo's Collection - 1Fichier Mirror (132 TB various) via reddit
CoolRom - Your #1 emulation choice.
Darkumbra - Nintendo 3ds CIA files.
Digiex - A forum to share and talk about ROMs, Games & other console games.
Doperoms - A huge collection with over 170,000 ROM files.
Emulanium - Emulators, cheats & roms.
myabandonware - More than 14000 old games to download for free!
No Intro DAT-o-MATIC - List of Xbox, Nintendo etc games.
Old Games Finder - Old Games Finder is an automated old games search engine. (avoid ISO Zone links, as that site is dead).
Rom Links Megathread - 1Fichier, GDrive, Mega - Nintendo, Sony, Microsoft, Romsets, Arcade and other ROm collections.
ROM/ISO sites - Wiki page from gametechwiki.com with more links to ROM and ISO websites.
Romulation.net - Collection of over ~28,000 console game ROMs.
Romsmania - Another great ROMs collection with a decent UI to find stuff quick.
ROMNation - Lots of ROMs.
The Eye ROMs - Open directory of ROMs from The-Eye.
The ROM Depot - Around 3 TB of ROMs (requires a VPN).
Vimm's Lair - Large collection of ROMs.
Ziperto - Nintendo 3ds CIA files, especially for JRPGs.

Good Old Download alternatives

GOD Games - GOD is alive.
GOD Project - Work in Progress Project, aims to reboot the old GOD project.

Console Games (various)

/r/PkgLinks - A place to share working Playstation 4 PKGs.
/r/SwitchNSPs - Nintendo Switch games.
gazellegames - Another xBox 360 collection.
NoPayStation - A Database for PSN Content including Vita, PS3, PSX, and PSP.
PleasureDome Tracker - MAME torrents.
SPEEDLounge - (Ger) - XBox360, XBox, PlayStation and other Games.
Up2date list for Xbox 360 - An up2date list for Xbox 360 games.
xbox360iso - XBox 360 Game collection.

Game Cheats

MPGH - Multiplayer game hacking (makes money via ads).

PC Games

Bzinho Games - Scene releases mirrors.
CompucaliTV - (SP) - Games, Video & more via Mega, OpenLoad, & others).
CS.Rin.RU - NoStream Gaming Servers, Repacks & mods. (Tor Mirror: csrinru3c2ownkep.onion/forum/ + [Userscript] CS.RIN.RU Enhanced) -
DL2-DLFox - FTP Mirror for various Games.
Firestorm - Games & more.
GameBurnWorld - Provides cracks for Games.
GameCopyWorld - Provides cracks for Games.
GamesFull - (SP) - ElAmigos Games and other scene releases. Mega, GDrive, MediaFire.
Games Turret - Proves games via file hosters.
MegaGames - Same like GameCopyWorld, online since 20+ years, old but gold!
OVA Games - Cracks to latest Game Releases from PLAZA, CODEX & Co.
PCMYMJuegos - (SP) - Spanish website for SteamWorkFixs and other stuff. Mega, GDrive, MediaFire.
Skidrow & Reloaded - Fanmade Skidrow & Reloaded mirror website.
SpartaGames - (PT/BR) - Torrent links and direct server mirrors to various scene releases.
Small-Games - (RU) - A russian website which provides their own releases.
Torrents Gamestorrent - PC Game releases from CODEX, SKIDROW, PLAZA & Co.
VseTop - (RU) - Yet another russian website which buy their own games and release it to the mass.
Warmane - Hosts private WoW Servers.

Games Achievements

SSElauncher - SSELauncher Comfy Edition 2018 By LoodBot/Syahmixp (Steam Emulator).
Steam Achievement Manager - A manager for game achievements in Steam.

GameCube Games

Archive.org - Gamecube NTSC-J: Your gonna need an account with archive.org but it should work, speeds are decent.
GDrive

3DS

3DS Decrypter utility - Decrypt 3DS files.
3dscia - Nintendo 3ds CIA files.
GDrive - EN/US based ROMs.
GDrive - All regions collection.
GDrive
MEga.nz - DS Best of Collection
Mega.nz - 3DS Virtual Console
Nintendo 3DSISO - A forum which shares 3DS Roms.
ziperto.com - 3DS CIA collection.

GameBoy Advance

GDrive (mirror) - Password=snahp.it
Mega.nz - Password=snahp.it

Mame Games

PleasureDome - Various Mame games.

Nintendo Switch Games

GDrive (more frequently updated
GDrive
Switch SN Checker

Game Boy

G-Drive - Includes Game Boy ROM's (together with N64 etc) - and BIOS files.

Wii U Games

GDrive - EU only games.
GDrive - Wiiware and VC collection
GDrive - Wii Scrubbed ISOs retail Collection NTSC
GDrive - Mirror from abolve link.

CD Key Sellers

All Key Shop
CDKeyPrices
CDKeys
DLC Compare
G2a
G2Play
GO CD Keys
Ultimatum Game Keys (UGK)

Homebrew and Custom Firmware(s)

/r/3dshacks - Nintendo 3DS hacking and homebrew.
/r/ps3homebrew - News, updates, apps, and answers regarding PS3 homebrew!
/r/ps4homebrew - News, releases, and questions regarding the PS4 jailbreak, homebrew, and mods.
/r/SwitchHacks - Another Nintendo Switch hacking subreddit.
/r/SwitchHaxing - Nintendo Switch hacking & homebrew subreddit.
/r/vitahacks - A place to discuss Vita hacking and homebrew.
/r/WiiHacks - This reddit is for people interested in modifying their Wii.
/r/YuzuPiracy - Links for Yuzu, the open-source Nintendo Switch emulator.
3DS Hacks Guide - A complete guide to 3DS custom firmware, from stock to boot9strap.
The ultimate guide to Nintendo 3DS Piracy - 3DS piracy guide by /u/crazy5.

Anime

/r/animepiracy wiki - Lists for sourcing Anime streaming sites, manga sites, and more.
/r/animepiracy - This sub is about streaming and torrent websites for anime.
/r/sjain_guides - Guides and downloads for CS:GO, Windows 10 gaming optimisations, and more
9Anime Downloader - Download Full Seasons from 9anime.
Alternatives to Kiss websites - /r/KissCartoon wiki page with lots of anime sites.
AniDex - Torrent tracker and indexer, primarily for English fansub groups of anime.
AniLinkz - Large database of streaming anime episodes.
Anime Kaizoku - Up to 1080p DDL links, mostly Google Drive.
Anime Twist - An anime direct streaming site with a decent UI and video player.
anime-sharing - Forum for sharing anime series.
animEncodes - Anime sharing page.
AnimeOut - Over 1000's of Encoded Anime with DDL links.
GoGo Anime - Popular website for watching anime.
Hi10 Anime - High Quality 10-bit Anime Encodes.
HorribleSubs - Download anime via torrent files, magnet links, XDCC, and premium link hosts.
KissAnime - Subs and dubs, many ads, many player alternatives, captcha, anti adblocker, videos hosted by Google & Facebook.
Kissanime - Dubs and up2date animes.
Monimo - Netflix like web app for watching animes.
Nyaa - BitTorrent software for cats (Repo).
NyaaPantsu - Primarily Anime torrents but includes an open directory of DDL links too.
Series9 - Search engine for movies and tv shows (incl. animes).

Cartoons

animetoon - Lots of streaming via premium hosts for cartoons.
KissCartoon - Popular cartoon streaming site.
Toonova - Another site for streaming cartoons.
watchcartoononline.com - Cartoons, dubbed/subbed anime streaming site.
watchcartoononline.io - Large DDL site for cartoons as well as anime and movies.
KimCartoon Large cartoon collection, primarily Openload.
WatchCartoon Outdated site layout, still active, uses Openload.

Album Art

Album Art Downloader - Find and update their album art for their music collection.
newalbum.club - Search and download free music & album arts. No account required!

Music

94hiphop - Download Free Hip Hop Albums!
Beets - The purpose of beets is to get your music collection right once and for all. It catalogs your collection, automatically improving its metadata as it goes using the MusicBrainz database.
CDBao - Chinese invite-only page for music.
LibreSonic - Media streaming software.
MOOVAL - Easily move your playlists, tracks and likes from one streaming service to another.
Madsonic - Madsonic is a web-based media library and media streamer with jukebox functionality.
MusicBrainz - MusicBrainz is an open music encyclopedia that collects music metadata and makes it available to the public.
Redacted - Elite music scene (requires invite).
RuTracker - Ru-Tracker, music info, releases & software.
Slsknet - Soulseek is an ad-free, spyware free, just plain free file sharing network for Windows, Mac and Linux.
airsonic - Airsonic is a free, web-based media streamer, providing ubiquitous access to your music.
streethiphop] - Download free music.

Music Streaming

datmusic - Search engine with a clean UI for streaming music in your browser
GoSong - Streamable MP3s
Hikarinoakariost - Site with Japanese music
mp3.li - Another MP3 streaming site
mp3Clan - Free music streaming
MP3Juices - MP3 search engine tool which uses YouTube
MusicPleer - Another music streaming site with a decent search engine.
Muxiv Music - Stream 45 million songs on all your devices, online or offline. Primarily Chinese content.
SongsPK - Mainly for downloading Bollywood songs. Domain changes frequently.

iOS JailBreak Firmware

IPSW - Provides Jailbreak firmware.

iOS Apps

Cinema Time - Similar like Popcorn Time.
Cotomovies - Streaming Movies and TVShows app.
HDX Online - Another alternative for Cinema Time.
Total files - Basically the IDM under the iOS Download Managern.

iOS Stores

App Valley - Basically the Aptoide under the iOS Stores.
Cydia - Cydia is an alternative to Apple's App Store for ""jailbroken"" devices.

iOS Store Repos

Xarold

iTunes

forked-daapd - Linux/FreeBSD DAAP (iTunes) - and MPD media server with support for AirPlay devices (multiroom), Apple Remote (and compatibles), Chromecast, Spotify and internet radio.
How to Remove DRM From iTunes Movies and TV Shows - HowToGeek article on how to use TunesKit and Requiem
Plus Premieres - Download newest iTunes music in M4A format.
Requiem - Requiem is a program that removes Apple's DRM (called FairPlay) - from songs, videos, and books purchased on iTunes.
TunesKit - iTunes DRM removal tool.

Spotify

BlockTheSpot - Video, Audio & Banner ad-block/skip for Spotify.
Spotify Ad-Free - Modified Client(s), Information, etc.
Spotify Downloader - Download Spotify playlists with albumart and meta-tags.
BlockTheSpot - Video, Audio & Banner ad-block/skip for Spotify.
Spotify Megathread - /r/Piracy Spotify-related discussion and future developments.
Spytify - Records Spotify without ads while it plays and includes media tags to the recorded files.

SoundCloud

scdl - Soundcloud Music Downloader.
scddlr.com - SoundCloud To Mp3 Converter Online.

Software

/r/piracy/wiki/tools - Windows/Office activation tools, and images/installers for Windows, Office, and Adobe
Appked - MacOS application sharing website.
CrackingPatching.com - Cracked software
CracksNow - Cracks for Android, Windows, and macOS applications.
gallery-dl - Command-line program to download image-galleries and -collections from several image hosting sites
Nulled - Nulled is a cracking community where you can find links to cracked software
Team-OS HKRG - Windows software and various activation tools.
Vestathemes - Vestathemes is a website for WordPress - themes and plugins.

Windows

AME - Windows 10 AME aims at delivering a stable, non-intrusive yet fully functional build of Windows 10 to anyone, who requires the Windows operating system natively.
Krakatoa - Office, Windows, KMS and Key checkers.
MDL Forums - Windows topics, hotfixes & self-made tools.
PCBeta - Windows ISOs, hotfixes and discussions.
SamLab - (RU) - Windows Board, ISOs, Hotfixes & more.
Shadow-Trooperz - Provides Windows ISO's and other Windows related software links.
TechBench - Find official Windows isos for Windows 7/8/10.
UUPDump - In-official Windows Hotfix repository + Windows dumps.

Windows Resources (Hotfixes & Patches)

AskWoody - News, tips, advice, support for Windows, Office, PCs & more.
RCC - RCC, check your system's trusted root certificate store.
Simplix Blog - Windows Hotfix repository.
Windows ISO + Hotfix mirrors - AdGuard provides mirrors to hotfixes and Windows ISO's/ESD's.

Windows Activation

PIDChecker - Validate and check Microsoft Product Keys.
KMS Activator - The original repo for KMS related activation & research.

Windows 10 Downloads & Verification

Windows 10 1903 Build 18362.30 Final (May Update '19) - Download + checksum links.
Windows and Office Genuine ISO Verifier - Freeware tool to verify Windows & Office images.
Windows ISO Downloader - Allows an easy and comfortable way to download genuine Windows 7, 8.1 and 10 disk images (ISO) directly from Microsoft's servers (tool contains ads).

eBooks

/r/DHExchange - PDF/eBooks trading.
Apprentice Alf's Blog - Everything you ever wanted to know about DRM and ebooks, but were afraid to ask.
Authorama - This public domain book site has a wide variety of ebooks for free, by Lewis Carroll, Emerson, Kafka, and more.
b-ok - Free ebook library
Baen Free Library - You can download ebooks for HTML, RTF, Microsoft Reader and for Palm, Psion, and Window CE.
Bartleby - While Bartleby charges for some titles, it has a free ebook store here.
bibliomania - You will find over 2,000 classic texts from bibliomania, plus study guides, reference material and more.
BookStack - BookStack is a simple, self-hosted, easy-to-use platform for organising and storing information.
Bookyards - This online ""library to the world"" has over 17,000 ebooks, plus links to other digital libraries.
Boundless - Affordable online textbooks & study materials.
Calibre-Web - Web app for browsing, reading and downloading eBooks stored in a Calibre database
COPS - Calibre OPDS (and HTML) - PHP Server: Web-based light alternative to Calibre content server / Calibre2OPDS to serve ebooks (epub, mobi, pdf, etc.)
Custom Search Engine - A Google custom search engine specifically for ebooks.
DailyLit - Get free downloads sent to your email by RSS feed.
DeDRM_tools A github repository of all the scripts and other tools for removing DRM from ebooks.
DLEBook.me - eBooks, Magazines & software.
ebook Directory - From children's books to IT books to literature to reference, you'll find lots of free titles and book packages here.
ebookee.org - PDF/eBooks trading.
eBookLobby - You'll find lost of self-help, hobby and reference books here, plus children's fiction and more.
eReader.com - eReader.com has many classic lit selections for free.
FicSave - An Open-Source Online Fanfiction Downloader.
Free-eBooks.net - Besides browsing topics such as biography, fan fiction, games, history, or tutorials, you can submit your own ebook, too.
freebookspot.es - Various eBook's.
Get Free Ebooks - This website has free ebooks in categories from writing to environment to fiction to business, plus features and reviews.
Globusz - There are no limits on the number of free books you can download on this online publishing site.
Guide to Copy Kindle Content to PDF using Calibre -
Gutenberg - Project Gutenberg was the first to supply free ebooks, and today they have almost 30,000 free titles in stock.
How can I remove DRM from my ebooks? - DeDRM tools provides a big FAQ and scripts, tools to remove DRM on eBooks.
iBiblio - Find archives, ebooks, tutorials, language books, and more from iBiblio.
LibGen - eBook search. (Mirror) - + (another Mirror)
ManyBooks.net - You can conduct an advanced search, type in a title or author, browse categories or select books by language, from Finnish to Bulgarian to Catalan to Swedish.
Planet PDF - Planet PDF has made available classic titles like Anna Karenina and Frankenstein for free.
Read Print Library - These novels and poems are all free.
Starry.com - These novels and anthologies were last updated in 2006, but you'll still find an interesting selection of online and virtual novels.
TehParadox - eBooks, Apps, Games & more.
The idiot proof guide to downloading ebooks off IRC - Posted by /u/Servaplur
The Online Books Page - You'll be able to access over 35,000 free ebooks from this site, powered by the University of Pennsylvania.
Ubooquity - Ubooquity is a free home server for your comics and ebooks library.

eBook Search Indexer

ABook - One of the oldest book search indexers.
BinSearch - Binary Usenet search engine which can be used for eBooks and other stuff.
EBookEE - Tech, Database, Java and many many other categories.
FreeBookSpot - Similar to eBookEE.
GingaDaddy - A usenet newsgroup for eBooks. (needs login)
Google Search - Preset of indexed websites to search for comics.
oznzb - Yet another search engine.
EBook Bike - Another search indexer which claims to be the ""largest"" on the Internet.
Ebook 3000 - Free ebooks download

Magazines

ebook3000.com - Various magazines.
MagazineLib - Free PDF and interactive e-magazines.
magazinesdownload.org - Magazines hosted on free, fast, file hosting sites.
PDF Giant - Various categories of downloadable PDFs.

Academic Papers & Material

Academic Torrents - A Community-Maintained Distributed Repository for researchers, by researchers. Making 32.66TB of research data available!
BookSC - The world's largest scientific articles store. 50,000,000+ articles for free.
LibGen - search engine for articles and books on various topics, which allows free access to content that is otherwise paywalled or not digitized elsewhere
PDF-Gigant - Lots of different magazines.
Sci-Hub - The first pirate website in the world to provide mass and public access to tens of millions of research papers.

Textbooks

All IT eBooks - A big database of free, direct links for IT and programming ebooks
forcoder - Ebooks & Elearning for Programming
Guide for Finding Textbooks - Extensive tutorial by /u/Amosqu
How to ""rent"" your textbooks for free from Amazon - ""Going to college? Living off top ramen for dinner? Let me show you have to ""rent"" your textbooks for free & for life!""
it-ebooks - Large selection of free and open source IT ebooks
PDF/Ebook trackers for college textbooks - Old-but-still-useful list of ebook/textbook trackers, DDL sites, and IRC communities

Direct Download Streaming

Catchvideo.net - Catchvideo.net is a free online website, which allows you to download a video url from YouTube, Facebook, Dailymotion, Vimeo and more.
HDencode - Download Movies and TV Shows - #1 Source for High Definition Releases.
Movie Files - Download Movies For free.
Movies ""R"" Us - The newest movies in 1080p. Available with DDL through MediaFire and streaming through AnonFile.

Audiobooks

ZLibrary - Part of Z-Library project. The world's largest ebook library.
AAXtoMP3 - Convert Audible's .aax filetype to MP3, FLAC, M4A, or OPUS.
AudioBook Bay - Download unabridged audiobook for free or share your audio books, safe, fast and high quality.
AudioBooks.Cloud - DDL links to a lot of audiobooks.
Booksonic - Booksonic is a server and an app for streaming your audiobooks to any pc or android phone.
The-Eye /public/AudioBooks - Audiobooks hosted by ""The Eye"".
Tokybook - Yet another free audiobook streaming site.
BookFI - The largest ebook library.
Ebooks Shares - A lot of eBooks & audiobooks!
P2PEiite - (HTTP only!) - Yet another oldschool eBook website.

Science Books

BookSC - Part of Z-Library project. The world's largest scientific articles store. 70,000,000+ articles for free.

Comicbooks

getcomics.info - Comics, release info & more.
Comic Extra - General Comic informations.
Gazee! - A WebApp Comic Reader for your favorite digital comics. Reach and read your comic library from any web connected device with a modern web browser.
GetComics - GetComics started as an alternative place to get downloaded comic. files, particularly US based comics published by DC and Marvel.
Readcomicbooksonline - Tends to Error 520 occasionally.
readcomiconline.to - Manga and comics which are been uploaded daily.
ReadComics - Several misc comics published.
WorldWideTorrents - Provides comic releases.

Manga

/r/manga - Everything and anything manga! (manhwa is okay too!)
KissManga - Another manga website.
Madokami 0-E - Download manga titles named 0 to E.
Madokami F-K - Download manga titles named F to K.
Madokami L-Q - Download manga titles named L to Q.
Madokami novels, raws and artbooks - Download novels, manga raws and artbooks.
Madokami R-Z - Download manga titles named R to Z.
MangaDex - MangaDex is an online manga reader that caters to all languages.
MangaZone - A manga reader app.

Documentaries

/r/Documentaries - Popular documentaries subreddit.
DocuWiki-net - DocuWiki.net serves as an index of documentary films on the Edonkey Network.
MVGroup - A forum which shares documentaries via P2P.
whatwhat888 big list of documentary sites (streaming and download) - An old post by /u/whatwhat888 that may still be useful.

Fonts, Icons & Graphics

Get the Font - Searches through GitHub for fonts
GFXDomain - Forum for graphic design resources and software
GFxtra - DDL links for graphics, icons, 3D models, and more
GraphicEx - Stock/vector graphics, PhotoShop/InDesign resources, fonts, and more
How to download paid Fonts for free - Post by /u/Bebhio on how to use clever Google searches to find fonts online
Tomato.to - Supports Shutterstock, Gettyimages, Adobestock, Fotolia, Vectorstock, iStockphoto, PNGTree & PicFair.
Web4Sync - Forum with DDL links catering to web development, graphics design, 3D animation, and photography

P2P-Networks

eDonkey network - A decentralized, mostly server-based, peer-to-peer file sharing network + Server Status
FastTrack - Protocol used by the Kazaa, Grokster, iMesh, and Morpheus file sharing programs
Fildo - Android music streaming app which fetches files from third party MP3 search engines.
Gnutella - P2P network behind the popular LimeWire file sharing app
IPFS - Distributed Web - Peer-to-peer distributed file system that seeks to connect all computing devices with the same system of files
Kad - The Kad network is a peer-to-peer (P2P) - network which implements the Kademlia P2P overlay protocol.
Napster - Peer-to-peer file sharing Internet service that emphasized sharing digital audio files, typically audio songs, encoded in MP3 format.
Peer-to-peer file sharing - Detailed Wikipedia page about file sharing
TiviMate IPTV player - A popular Android app for watching IPTV on Android set-top boxes.
YouTube Vanced - Vanced is a well known modded version of YouTube with many features such as adblocking and background playback and many more.

Ripping, Transcoding, Converting

Automatic Ripping Machine - The A.R.M. (Automatic Ripping Machine) - detects the insertion of an optical disc, identifies the type of media and autonomously performs the appropriate action
DVD Decrypter - The original unofficial DVD Decrypter mirror since June 7th, 2005.
DVDFab - DVD/Blu-ray ripping tool, alternative use AnyDVD HD.
ffmpeg - A complete, cross-platform solution to record, convert and stream audio and video.
Handbrake - HandBrake is a tool for converting video from nearly any format to a selection of modern, widely supported codecs.
MakeMKV - MakeMKV is your one-click solution to convert video that you own into free and patents-unencumbered format that can be played everywhere.
sickbeard_mp4_automator - Automatically convert video files to a standardized mp4 format with proper metadata tagging to create a beautiful and uniform media library
The Encoding Guide - An in-depth guide on video encoding.

Cloud Storage

/r/PlexACD - Discussion about unlimited cloud storage for Plex libraries
Connect Your Plex Server To Your Google Drive - This tutorial will help you connect your Google Drive to your Plex server using Plexdrive.
google-drive-ocamlfuse - FUSE filesystem over Google Drive
plexdrive - Mounts your Google Drive FUSE filesystem (optimized for media playback)
rclone-gdrive - Wiki page on setting up Google Drive with rclone cache and crypt
rclone - Rsync for cloud storage.

File Renaming and Tagging

/r/datacurator - Subreddit for discussion about the curation of digital data. Be it sorting, file formats, file encoding, best practices, discussion of your setup, tips and tricks, asking for help etc.
Beets - Beets is a music library manager.
docker-filebot - A docker container for FileBot
filebot-node - a client-server application that'll allow you to run filebot commands
FileBot - The ultimate tool for organizing and renaming your Movies, TV Shows and Anime as well as fetching subtitles and artwork. It's smart and just works.
iFlicks2 - Useful for adding metadata to movies and TV shows
MediaElch - Media manager for Kodi. Metadata & artwork retrieval, as well as renaming.
MediaInfo - MediaInfo is a convenient unified display of the most relevant technical and tag data for video and audio files.
MediaMonkey - Manage a movie/music library from 100 to 100,000+ audio/video files and playlists
Metatogger - Metatogger is the new generation of tag editor allowing you to rename, tag and easily sort your audio files.
MP3TAG - Mp3tag is a powerful and easy-to-use tool to edit metadata of audio files.
Picard - Picard is an open source cross-platform music tagger written in Python.

Mobile Apps

4PDA.ru - 4PDA is the biggest Russian forum about mobile devices. You can find endless amount of APKs and Mobile software there. For download registration is required, this might help you to solve the captchas.
AiOwares.com - RePacks, mods and other software.
Android Republic - Android Republic is similar to Mobilism, provides mirrors to various apks.
Android Zone - koumkouat website for Android APK's/Games/GPS.
Android-1 - Provides apps & app mods.
AnYme - Unofficial Anime App for MyAnimeList.
AppCake - AppCake is also known as AC Market and provides free apks.
apk4free - Android apk mirrors and patches.
APKDot - APKMirror clone website.
Apkmos - The Best App Store For Download Android Apps, Android Games, Android Themes, Android Wallpapers And Much More For Your Android Smartphone.
Baltagy's Website - Apps, Mods, RePacks and portable releases.
Blokada - Blokada is a compact app that transparently blocks unwanted content like ads, tracking, malware and other annoyances.
Cygery AdSkip for YouTube - Automatically click on the ""Skip ad"" button in the YouTube™ app when it appears.
FilePursuit Pro - FilePursuit provides a very powerful file indexing and search service allowing you to find a file among millions of files located on web servers.
Haxoff - Haxoff provides cracked games & Android APK's.
HiAppHere
MyJDownloader - enables you to remote control your desktop JDownloader from your pocket while you're on the go.
nzb360 - nzb360 is a full-featured NZB manager that focuses on providing the best experience possible for controlling all of your usenet needs.
Ombi - Companion app for Ombi to request Plex content
Perfect Player - Perfect Player is set-top box style IPTV/Media player for watching videos on TVs, tablets and smartphones.
Platin Mods - As the name says, provides several mods for apks's & games.
ProSmart - ProSmart is a russian site which provides several apks, mods and games.
Release-APK - Balatan's APK page.
Tachiyomi - Tachiyomi is a free and open source manga reader for Android.
Tautulli Remote - Mobile version of Tautilli for monitoring Plex on the go
Trashbox - Trashbox is the russian Mobilism.
YMusic - YouTube Music Player & Downloader

XPosed + Magisk

Magisk - Root & Universal Systemless Interface.
Systemless Xposed For SDK 27 (Android 8.1) - Magisk Xposed version (needs Magsik).
VirtualXposed - Xposed version for non-rooted devices.

Android License Verification Patcher

LuckyPatcher - Patch applications, remove ads and install a modded Google Play Store to bypass Google's license verification.
Jasi Patcher (also known as Uret Patcher) + [ToolKit] - Patching tool for android intended to bypass restrictions in the apps & games, it includes custom patches, support patches, universal patches, offline emulation, spoof, hooks, tools and utilities.

Streaming Apps

99kubo - (Ads) - 99Kubo is a paradise for movie buffs,couch potatoes & social networkers.
AniméGlare - (Ads) - lets you stream any anime for free.
AniméVibe - Watch Anime Online Free HD both Subbed and Dubbed on AnimeVibe without Advertisements!
AOS TV - Watch More than 1000+ Live TV Channels free on your Android Phone from across the world.
ApolloTV - Open-source aggregator for various online video content.
BeeTV - (Ads) - Watch movies & tv shows for free on Android device, Amazon Fire Stick, Fire TV, Nvidia Shield, etc.
CKayTV - (Ads) - Allows you to stream free videos from across the web directly on your Android and Firestick devices.
CinemaHD - (Ads) - FireTV Stick, Nvidia Shield, support Real-Debird, external players, Trakt.tv, series Guide.
Cinema - A lot of Movies & TV/Shows to watch and download.
DreamTV (Terrarium Clone) - (now called Redline) - Needs invite - Download various Movies.
Evolve TV - (Ads) - Watch for free more than 1100+ channels from all over the world, it also works with MXPlayer together.
Exousia - Watch Live Tv & Movies, Sports.
Fildo - Yet another Music streaming app.
Filmix - Watch movies and TV shows using AndroidTV or mobile devices.
KinoTor - (RU) - Provides movies and videos from several russian directories.
Kokotime - Kokotime is an addon-based, simple, free and elegantly designed app that will let you watch all your favorite media content in a unique and elegant user friendly design.
KrakenTV - Watch dozens of different TV channels from the comfort of your Android device.
Live TV - Watch Indian TV Channels live on your mobile free.
Liveflix - The app allows to watch your favorite channels easily, with a very simple UI.
Mega Shows - Watch & Download your favorite movies and TV shows.
Mobdro - Mobdro constantly searches the web for the best free video streams and brings them to your device.
Morph TV (Morpheus Fork)
Newest Movies HD - (en-US) - Watch movies and TV shows using AndroidTV or mobile devices.
Orion TV - (SH) - Allows you to watch live TV channels and recorded selected shows (72h Catch-up TV).
PhoenixTV - Morpheus Fork
RevTV - (es-ES) - Live TV, Movies, TV Shows in Spanish
TVPato2 - (es-ES) - Spanish Live TV App.
TVZion - + Reddit
TeaTV - App for Android, Windows, and macOS for watching 1080p movies and TV shows for free.
TitaniumTV (Terrarium Clone)
UnlockMyTV (Cinema Clone AdFree)
ZippyTv HD - ZippyTv is one of the largest online TV platform with over 500 Live TV Channels.

Big Media Libraries

/r/BestOfStreamingVideo - Reddit, random streaming sites
/r/MovieStreamingSites - Reddit, random streaming sites
123Movies.ooo - Watch & stream full HD movies & TV series online for free.
2TwoMovies - Watch Free Movies Online. TwoMovies is a free online video service that offers large collection of full length movies.
5Movies - Watch FULL HD Quality 1080/720p movies and latest tv series online for free, download the latest movies without registration at all.
8Putlocker - Watch Movies HD Online for Free, you can watch all movies here. All TV Series, Asian Dramas, Anime & Cartoons.
Afdah - Afdah is a web scraper coded to crawl and index online movie sites.
BS.to - (GER) - German Video-on-Demand-Website for TV-Shows, Cartoons & Movies.
CafeHulu - e Best Place To Watch FREE Tv Series And Cartoons.
EZTV (EZTV.AG) - Well known group for movies and series.
filechef - Search Direct Downloads
FreeMoviez - Watch free movies online.
Los-Movies - You can stream High Quality movies and cinema films without any redirection.
M4UFree.TV - Unique design, HD server with backup and additional hosts
Movie4k - Huge Movie/TV Library
Primewire - Free Movies
Putlocker.onl - Watch movies and TV Series for free, watch series full episodes online free with HD quality on Putlocker.
Putlockerfreely - Watch your favorite movies online free on Putlocker. Discover thousands of latest movies online.
Putlockeri - Watch your favorite movies online free on Putlocker.
Putlockertv - Watch movies and TV Series for free, watch series full episodes online free with HD quality on Putlocker.
Sharemovies - Watch Movies in Theatre, Anime & Cartoons and TV Series in HD 1080.
Sockshare - Huge Movie/TV Library
SolarMovieHD - Watch Movies Online and Watch Tv-Series online On Solarmovie without Registration.
Solarmovies - SolarMovie claims to be the biggest Library of free movies and tv shows.
Streaming Multireddit
TORRENTDOWNLOADS - It’s a no-nonsense index that provides torrents to millions of users each month.
Viooz - (RU) - Discover thousands of movies, watch your favorite movies online for free on VZM, Viooz.

TV & Sports Streaming

123TV
720pstream
All The Best Fights
Arconaitv
Bellator
BilaSport
Couch Tuner
CrickFRee
Daily TV Fix
First Row Sports
Full MMA Videos
LiveTVCafe
M4uFree.tv
MamaHD
MMA Core
MMA Versus
More Live Sports Sites
NewEpisodes
Pluto.tv
ProjectFreeTV
r/MMAFights
Ripple
Send It
SeriesFree
SeriesTop
SportsHD
StreamAll
TVSeries4u
TVZion
uStreamix
VipBox
Watchepisodes4
Watchepisodeseries
WatchKobe

720p Streaming

123GoMovies
123Moviesuk
123NetflixPro
1movies
bmovies + (Mirror) + Mirror)
CafeMovie
ddlhub
FFilms
Fmovies + (Mirror) + (Mirror)
flixtor.to
Flixtor
Full YouTube Movies -
GOMovieshub
HackIMDb
HDeuropix
HDFlix
HDVix
ILoveToWatch
IWannaWatch
Khaanflix
MegaDDL - Various movies via Mega, 1Fitcher, Openload and other file-hoster.
MovieGlide
Movies24
MovieZion
Niter
Oakmovies
Openloadmovies.tv
pahe.in - Streams via Uptostream, Google Drive, Openload or Mega.
ProSpice
Putlockers.fm
Solarmoviefreez
Solarmoviez
Spacemov
StreamDreams
Streamlord
Streamm4u
TimeToWatch
Tubi
TVMovies
ULMovies
VidCloud
WatchFree.ac
Watchfree.at
Watchfree.movie
WatchOnline.al
WatchSeries 2.0
XMovies8
Yes! Movies
YesMovies.fun
ZMovies

1080p Streaming

1Movies
CineBloom
Crackle
crawler
Daxiv
Filmxy
HD Europix + Mirror
HD Multireddit - 1080p Openload, Google Video & Vimeo,..
IceFilms
Kodi - + (Setup guide)
LeonFlix
Megashare9
MovieNight
MovieJagg
MovieStreams
OneMov
OnMovies
QQMovies
RainierLand - Openload, Streamango
SceneSource
Stream Likers
TeaTV
TopEuroPix
UWatchfree
Video2k
Videospider
Vmovee
WatchFullMovie
xPause
Ymovies

Wrestling & MMA

Fight-BB
MMA Releaselog
WWE-TV - (Ger)

Torrent Apps (Android/iOS)

/r/moddedandroidapps - User modified Android App(s).
aTorrent - Another popular torrent client for Android.
BiglyBT - Free, open source torrent client for Android phone, tablet, Chromebook, & Android TV
Flud - Flud is a simple and beautiful BitTorrent client for Android.
LibreTorrent - LibreTorrent is a Free as in Freedom torrent client for Android 4+, based on libtorrent.
Transdrone - Transdrone allows you to manage the torrents you run on your home server or seedbox.
Trireme for Deluge - A Deluge thin client for Android. Written in Flutter.
Vuze - Lightweight & powerful BitTorrent app.

Discord Clients

Ripcord - A closed source no tracking or analytics Discord client (without electron framework part).

Discord Servers

/f/MEGAlinks (Snahp) - Official /f/MEGAlinks Discord server.
/hbg/ Homebrew General A Discord server that shares newer Nintendo Switch Games.
r/PkgLinks - Reddup game sharing Discord Server.
/r/soccerstreams - Official Discord server for the recently-killed /r/soccerstreams subreddit.
9YearOldPirates - Official Discord server for the 9YearOldPirates relaese group.
AniméGlare - Official AniméGlare Discord channel.
AniméVibe - Official AniméVibe Discord channel.
APK'S 2 Day - This is a discord server that acts as a hub for numerous streaming apps.
ApolloTV - The official ApolloTV Discord server.
DoujinStyle - Discord server with Doujin related materials. Things such as Japanese doujin music & games.
DreamTV - A Discord server for the official DreamTV Android app.
legal acquisition club - A piracy server to discuss and share movies, tv and apps.
Morph TV (Morpheus Fork) - Morph TV (Morpheus Fork) official Discord channel.
piratesonline - Official piratesonline.us Discord Server.
PlayStation Homebrew - Home of /r/ps3homebrew and /r/ps4homebrew.
The Eye - Official Discord server for the-eye.eu
The Ratio - A community of seedbox enthusiasts. Buying advice, application setup, and automation help.
WarezNX - Nintendo Switch Warez community server.

IPTV + DVR

/r/IPTV - Subreddit some may find helpful for gauging the current state of IPTV providers.
/r/iptvresellers - Promotions and advertisements from IPTV providers.
/r/IPTVReviews - Reviews of IPTV service providers.
2019 Oscar DVD Screeners - List of DVD screeners for 2019's Oscars.
Academy Awards 2019 Screeners Megathread - Post by /u/idoideas listing all available DVDSCR releases for 2019.
allsprk.tv - Channel-hoppable live streaming site with a chat room.
antennas - HDHomeRun emulator for Plex DVR to connect to Tvheadend.
IPTV Community - Technology and IPTV discussion website, useful for finding an IPTV provider/reseller.
IPTV Providers list - A recently created list of 40+ IPTV providers with notes.
MythTV - Free Open Source software digital video recorder.
STBEmulator - Popular Android app for using IPTV streams with EPG.
telly - IPTV proxy for Plex Live written in Golang.
tvheadend - Tvheadend is a TV streaming server for Linux supporting DVB-S, DVB-S2, DVB-C, DVB-T, ATSC, IPTV,SAT>IP and other formats through the unix pipe as input sources.
UlstreaMix - Live TV streaming site, predominantly sports.
xTeVe - M3U Proxy for Plex DVR.
Xtream Editor - Xtream Editor allow you to create, edit and sort m3u playlists online.

Acestreams

aceproxy - Ace Stream HTTP Proxy. (abandonware)
acestream.org - Ace Stream is a peer-to-peer streaming application that lets you stream live sports and other content
AceStreamSearch - Ace Stream Broadcasts Search
FireDrop - 100GB free cloud storage.
iktason/aceproxy - A docker image to run aceengine + aceproxy, e.g. to watch Torrent-TV.ru.

IRC

autodl-irssi - autodl-irssi is a plugin for irssi that monitors IRC announce channels for torrent trackers and downloads torrent files based on user-defined filters.
XDCC Tutorial - XDCC Downloading For Beginners: Do It Like A Pro!
XDCC - XDCC (Xabi DCC or eXtended DCC) - is a computer file sharing method which uses the Internet Relay Chat (IRC) - network as a host service.
ZNC - An advanced IRC bouncer.

IRC Networks

Beast-XDCC irc://irc.abjects.net/BEAST-XDCC - One more XDCC source.
irc.irchighway.net/ebooks irc://irc.irchighway.net/ebooks - A nice, friendly irc channel for trading ebooks.
irc.undernet.org/bookz irc://irc.undernet.org/bookz - For downloading ebooks (use @search <book name> for a list of available ebooks)
irc://irc.abandoned-irc.net/#ZOMBIE-WAREZ irc://irc.abandoned-irc.net/#ZOMBIE-WAREZ - Zombie Warez channel for various software.
Moviegods irc://irc.abjects.net/MOVIEGODS - XDCC file sharing network, join #mg-chat to continue downloading
The Source irc://irc.scenep2p.net/THE.SOURCE - Another XDCC source
irc.p2p-network.net - P2P file sharing network
Orpheus - Formerly known as Apollo
p2p-network.net channel list - List of all channels on the p2p-network.net IRC network

IRC Search Engines

ixIRC - ixIRC lets you search through 17 IRC networks, 32 channels, and over 189915 user supplied XDCC packs.
SunXDCC - Another XDCC file search engine
xdcc.eu - XDCC search engine indexing packets from a large number of networks
xWeasel - xWeasel is a free stand-alone Download Client based on IRC technology including a multifunctional XDCC Search Engine.

DC++

AirDC++ - Windows GUI and Linux Web DC++ client in active development, with ADC, IPv6 and DHT support.
DC++ - Wikipedia page describing DC++
Direct Connect (protocol) - Wikipedia page describing Direct Connect.
EiskaltDC++ - Windows/Linux/macOS DC++ client, with ADC and DHT support
FlylinkDC++ - Windows DC++ and BitTorrent client in active development, with ADC and DHT support.
Linux DC++ - Easy to configure and use DC++ client
LinuxDC++ - Utilizing the latest DC++ core, LinuxDC++ offers similar functionality to the Windows client like segmented downloading, TTH based file integrity, etc. with a GTK+ user interface.
Tankafett - List of public DC++ hubs, previously known as hublist.org and TheHubList.com.

Full Movies On

/r/1080pMoviesOnline
/r/BestOfStreamingVideo
/r/fullcartoonsonyoutube
/r/FullLengthFilms
/r/FullMovieonViooz
/r/fullmovierequest
/r/FullMoviesDailyMotion
/r/fullmoviesongoogle
/r/fullmoviesonopenload
/r/Fullmoviesonvimeo
/r/fullmoviesonyoutube
/r/fulltvshowsonvimeo
/r/fulltvshowsonyoutube
fullmoviesandtv multireddit - All of the above subreddits as a multireddit

Full TV Shows On

/r/freefolk - Streams for new episodes of Game of Thrones.
/r/ProshotMusicals - Subreddit for all those theatre obsessed people who want proshots instead of bootlegs to be seen.
castnow - Castnow is a command-line utility that can be used to play back media files on your Chromecast device.
How To Get Everything On Netflix - Posted by /u/huldre99.
iNFekt - A text viewer application that has been carefully designed around its main task: viewing and presenting NFO files.
SceneLinkList - SceneLinkList is a project initiated to display and share as many scene and warez links as possible.
TheTrove - The Trove is a non-profit website dedicated to content archival and long-term preservation of RPGs.

Content Discovery

2160p BluRay Remux List - Complete list of all available 2160p remuxes.
Flox - Flox is a self-hosted movie, series and nime watch list.
IMDb - Find movies, TV shows, celebrities, and more.
JustWatch - On JustWatch you are able to find out where to watch your favorite movies & TV series
Letterboxd - Your life in film, the social network for film lovers.
MetaCritic - website that aggregates reviews of media products: music albums, video games, films, TV shows, and formerly, books.
Movieo - Discover, organize and track over 250,000 movies.
nulled.cc - Warez Scripts and a forum for additional piracy related resources.
popular-movies - Tries to create a list of popular movies based on a series of heuristics
SIMKL - Similar like traktv but offers a Chrome app for Netflix.
Squawkr.io - Sends you a notifications when movies are available for download.
Trakt.tv - A platform that does many things, but primarily keeps track of TV shows and movies you watch.
TVmaze - TVmaze is a community of TV lovers and dedicated contributors that discuss and help maintain TV information on the web.
What is my movie? - AI-powered movie search. ""Use your own words, or search with titles, actors, directors, genres etc. We find movies for you to watch.""
WhereYouWatch - Follow upcoming movies and receive email alerts when they are out online as a download or stream – pirated or via retail.

HTCP

Anonmasky - Anonmasky is a beautiful startpage for geeks out there. Clone of weboas.is.
Heimdall - An Application dashboard and launcher
HTPC-Manager - A fully responsive interface to manage all your favorite software on your Htpc.
iDashboard-PHP - HTPC Dashboard to load website services, written in PHP (predecessor to Organizr)
Logarr - ""Logarr"" is a self-hosted, PHP-based, single-page log consolidation tool which formats and displays log files for easy analysis.
Monitorr - Self-hosted PHP-based web front platform that displays the status of any webapp or service in real time.
Muximux - A lightweight way to manage your HTPC
Organizr - HTPC/Homelab Services Organizer - Written in PHP
weboas.is - Another website for pirates.

Proxy Sites to bypass filters to unblock blocked Websites

ByPassed - ByPassed is an all-in-one solution to unblock censored websites including thepiratebay, kickass, eztv, yts, extratorrent & more.
ProxySite.cc + (Mirror) + (for Videos) - Free web proxy site to bypass filters and unblock blocked websites anonymously.
Unblocked - Proxy site for accessing your favourite blocked sites.

Proxies & alternative Protocols/Networks

cjdns - Cjdns (Caleb James DeLisle's Network Suite) is a networking protocol and reference implementation, founded on the ideology that networks should be easy to set up
Freenet - Freenet is free software which lets you anonymously share files, browse and publish ""freesites"" (web sites accessible only through Freenet) and chat on forums, without fear of censorship.
GnUNet - GNUnet is a framework for secure peer-to-peer networking that does not use any centralized or otherwise trusted services
Psiphon - Run your own server, invite your friends, build a community, provide free and unfiltered Internet access to the world.
Scuttlebutt - A decent(ralised) secure gossip platform that aims to harmonize four perspectives of life: Environment reflecting Technology reflecting Community reflecting Society.
Shadowsocks - A secure socks5 proxy, designed to protect your Internet traffic.

Stream Synchronisation

/r/Movie_Club - Where you can get together with strangers and watch a great movie every week!
&chill - Watch videos with other people, it acts like a video streaming service.
ArconaiTV - Another stream sharing platform with a nice UI.
CyTube - Channel-based shared streaming platform for synchronised viewing of YouTube and Google Drive videos
Netflix Party - Netflix Party is a Chrome extension for watching Netflix remotely with other users.
sync - Node.JS Server and JavaScript/HTML Client for synchronizing online media
SyncLounge - Third party tool that allows you to watch Plex in sync with your friends/family, wherever you are.
watch2gether - Enjoy the internet in sync with your friends. Watch videos, listen to music or go shopping on Watch2Gether.

Secure PasteBin alternatives

PrivateBin - A minimalist, open source online pastebin where the server has zero knowledge of pasted data.
ZeroBin + Tor Mirror & source code

DNS based Ad-Blocking

Pi-hole - Pi-Hole is a Linux network-level advertisement and internet tracker blocking application which acts as a DNS sinkhole.

Emby Servers

/r/EmbyShares - This subreddit is dedicated to the sharing of Emby servers.

URL Shortener

Anon.to - URL shortener to de-referer or null-referer your links.

Premium Link Generators

Deepbrid - 1,3 GB of premium links, a ""pro"" version is also avbl. without any download limits.
Free Rapidleech - Daily updated free premium hoster logins. Be Warned: Fake pages are looking similar!

Hardware Security Token

Adding two factor authentication to KeePass & KeePass2Android
Configure YubiKey with Google, LastPass and KeePass
How to use GPG with YubiKey (bonus: WSL)

Reverse Proxies

bitmitigate.com
blazingfast.io
Cloudflare
ddos-guard.net
Geniusguard.com
puroxy.org
x4b.net

Free graphics

PixelPro.io - Provides free (and paid) graphics for streamers.

SMS

Crypton - Secure SMS Cloud service.
SMS Privacy - Send & recieve SMS securely.

Shared Accounts

/f/SharedAcc - Share and use account logins to preserve your online privacy.

Detecting Fake Torrents

Hide Fake Torrent on ThePirateBay - Browser extension to Hide Fake Torrent on The PirateBay.
Solid Torrents - A torrent index search engine, which checks if the torrent has known ""fake"" seeders/leechers based on a predefined database.
The Pirate Bay Tweaks (based on this script) - Userscript to detect fake TPB uploaders/torrents.

YouTube alternatives

DTube - D.Tube aims to become an alternative to YouTube that allows you to watch or upload videos on IPFS and share or comment about it on STEEM.
Minds - Minds is a open source and decentralized platform for Internet freedom.
PeerTube - A decentralized video hosting network, based on free/libre software.

Telegram Piracy Discussion Channels

@bestwarez - Maybe one of the biggest warez telegram channel.
@DeezerMusicBot - Music bot which downloads tracks from Deezer.
@fullalbums - Full Music Albums.
@iMediaShare - Provides links to Movies, TV shows, apps & and more.
@intermedia - Channel for movies.
@itorrentsearchbot - Searchbot which finds torrent and magnet links on 1337x.to by keyword search
@movies_inc - Another Telegram channel for downloading several movies in different quality.
@Moviezworldz - Official Moviezworldz channel.
@MusicHuntersBot - Another music downloader bot for Telegram.
@piracy - A modest group with over 400 pirates.
@piratebazaar - Telegram Channel which provides warez information, lists & more.
@Qualitymovies - Lots of 720p Blu-Ray movie releases.
@RickyChristanto Chan - Movie releases channel provides usually stuff from YTS.
@SMLoadrCommuntiy - Telegram community for SMLoadr.
@UretPatcherByJasi2169 - The official Uret Android patcher group.
@vkmusic_bot - Find and download pretty much any song (uses vkmusic search engine).
@warez - Yet another warez channel on Telegram.

VPS Hosting Providers

24Shells - Get High performance dedicated servers with cheap unmetered bandwidth and managed server hosting options.
2x4.ru - A fast and reliable russian VPS hoster.
AlexHost.md - Yet another russian VPS hoster service.
CyberHour - Rusian based host provider which allows Warez.
Datasource.ch - Swiss made offshore VPS provider.
Hostkey - Dedicated Servers & more.
KnownSRV.com - They use offshore data centers in Romania, Luxembourg and the Netherlands.
NetEngi - Rent professional web hosting of high quality and cheap price with cPanel.
RapidLeechHost.com - RapidLeechHost only allows warez linking on their offshore plans, using servers based in Netherlands.
SporeStack - Truly Hidden Hosting.
Warez Hosting - Private & Anonymous VPS Hosting!
WebCare360 - Powerful Offshore cpanel shared hosting provider.
WRZHost.com - WrzHost specializes in allowing warez, and acts as a safe-haven for people wanting to start warez-related projects with anonymity. They offer shared hosting, Linux VPS’s and dedicated servers.

Homework

Lit Answers - Find homework answers.
Slader - Find textbooks with answers.

Documents Downloaders

DocDownloader.com - Scribd, Issuu, Slideshare & Academia downloader.

Web Page testing

wptagent - Cross-platform WebPageTest agent which supports allmost all current OS.

Alternative Search Engines

DavidWon.com - An alternative search engine for Reddit, Google etc which comes with pre-defined tags to find some things faster.
Mega.nz Search Engine - A search engine for Mega.nz files.
Search Encrypt - The Privacy Based Search Engine.
Startpage.com - The world's most private search engine.
Jive Search - A search engine that doesn't track you.

Online Video Converter/Ripper

MP3Juices.cc - Free MP3 Downloads.
2conv.com - Convert videos from YouTube in 1 click!
Flvto.biz - YouTube Online Converter for Video/Music.
Facebook Down - Facebook Video Downloader.
Online Video Converter - Free online video conversion tool.
Twitter Video Downloader - Twitter Video Downloader.

Subtitles

Addic7ed - Quality Subtitles for TV Shows and movies.
RenameThemSubs - Rename subtitles files according to TV show names found in a directory.
Subscene - Quality Subtitles for Movies.

",140
decred-proposals/mainnet,None,"politeia
Politeia overview
This is the repository for the Decred Politeia service.
Source Code
https://github.com/decred/politeia
Contact
https://decred.org/community
",6
dejavuzhou/md-genie,Go,"Golang Auto Markdown
Chinese Movie Board


movie_2019-05-12.md


movie_2019-05-11.md


movie_2019-05-10.md


movie_2019-05-09.md


movie_2019-05-08.md


movie_2019-05-07.md


movie_2019-05-06.md


movie_2019-05-05.md


movie_2019-05-04.md


movie_2019-05-03.md


movie_2019-05-02.md


movie_2019-05-01.md


movie_2019-04-30.md


movie_2019-04-29.md


movie_2019-04-28.md


movie_2019-04-27.md


movie_2019-04-26.md


movie_2019-04-25.md


movie_2019-04-24.md


movie_2019-04-23.md


movie_2019-04-22.md


movie_2019-04-21.md


movie_2019-04-20.md


movie_2019-04-19.md


movie_2019-04-18.md


movie_2019-04-17.md


movie_2019-04-16.md


movie_2019-04-15.md


movie_2019-04-14.md


movie_2019-04-13.md


movie_2019-04-12.md


movie_2019-04-11.md


movie_2019-04-10.md


movie_2019-04-09.md


movie_2019-04-08.md


movie_2019-04-07.md


movie_2019-04-06.md


movie_2019-04-05.md


movie_2019-04-04.md


movie_2019-04-03.md


movie_2019-04-02.md


movie_2019-04-01.md


movie_2019-03-31.md


movie_2019-03-30.md


movie_2019-03-29.md


movie_2019-03-28.md


movie_2019-03-27.md


movie_2019-03-26.md


movie_2019-03-25.md


movie_2019-03-24.md


movie_2019-03-23.md


movie_2019-03-22.md


movie_2019-03-21.md


movie_2019-03-20.md


movie_2019-03-19.md


movie_2019-03-18.md


movie_2019-03-17.md


movie_2019-03-16.md


movie_2019-03-15.md


movie_2019-03-14.md


movie_2019-03-13.md


movie_2019-03-12.md


movie_2019-03-11.md


movie_2019-03-10.md


movie_2019-03-09.md


movie_2019-03-08.md


movie_2019-03-07.md


movie_2019-03-06.md


movie_2019-03-05.md


movie_2019-03-04.md


movie_2019-03-03.md


movie_2019-03-02.md


movie_2019-03-01.md


movie_2019-02-28.md


movie_2019-02-27.md


movie_2019-02-26.md


movie_2019-02-25.md


movie_2019-02-24.md


movie_2019-02-23.md


movie_2019-02-22.md


movie_2019-02-21.md


movie_2019-02-20.md


movie_2019-02-19.md


movie_2019-02-18.md


movie_2019-02-17.md


movie_2019-02-16.md


movie_2019-02-15.md


movie_2019-02-14.md


movie_2019-02-13.md


movie_2019-02-12.md


movie_2019-02-11.md


movie_2019-02-10.md


movie_2019-02-09.md


movie_2019-02-08.md


movie_2019-02-07.md


movie_2019-02-06.md


movie_2019-02-05.md


movie_2019-02-04.md


movie_2019-02-03.md


movie_2019-02-02.md


movie_2019-02-01.md


movie_2019-01-31.md


movie_2019-01-30.md


movie_2019-01-29.md


movie_2019-01-28.md


movie_2019-01-27.md


movie_2019-01-26.md


movie_2019-01-25.md


movie_2019-01-24.md


movie_2019-01-23.md


movie_2019-01-22.md


movie_2019-01-21.md


movie_2019-01-20.md


movie_2019-01-19.md


movie_2019-01-18.md


movie_2019-01-17.md


movie_2019-01-16.md


movie_2019-01-15.md


movie_2019-01-14.md


movie_2019-01-13.md


movie_2019-01-12.md


movie_2019-01-11.md


movie_2019-01-10.md


movie_2019-01-09.md


movie_2019-01-08.md


movie_2019-01-07.md


movie_2019-01-06.md


movie_2019-01-05.md


movie_2019-01-04.md


movie_2019-01-03.md


movie_2019-01-02.md


movie_2019-01-01.md


movie_2018-12-31.md


movie_2018-12-30.md


movie_2018-12-29.md


movie_2018-12-28.md


movie_2018-12-27.md


movie_2018-12-26.md


movie_2018-12-25.md


movie_2018-12-24.md


movie_2018-12-23.md


movie_2018-12-22.md


movie_2018-12-21.md


movie_2018-12-20.md


movie_2018-12-19.md


movie_2018-12-18.md


movie_2018-12-17.md


movie_2018-12-16.md


movie_2018-12-15.md


movie_2018-12-14.md


movie_2018-12-13.md


movie_2018-12-12.md


movie_2018-12-11.md


movie_2018-12-10.md


movie_2018-12-09.md


movie_2018-12-08.md


movie_2018-12-07.md


movie_2018-12-06.md


movie_2018-12-05.md


movie_2018-12-04.md


movie_2018-12-03.md


movie_2018-12-02.md


movie_2018-12-01.md


movie_2018-11-30.md


movie_2018-11-29.md


movie_2018-11-28.md


movie_2018-11-27.md


movie_2018-11-26.md


movie_2018-11-25.md


movie_2018-11-24.md


movie_2018-11-23.md


movie_2018-11-22.md


movie_2018-11-21.md


movie_2018-11-20.md


movie_2018-11-19.md


movie_2018-11-18.md


movie_2018-11-17.md


movie_2018-11-16.md


movie_2018-11-15.md


movie_2018-11-14.md


movie_2018-11-13.md


movie_2018-11-12.md


movie_2018-11-11.md


movie_2018-11-10.md


movie_2018-11-09.md


movie_2018-11-08.md


movie_2018-11-07.md


movie_2018-11-06.md


movie_2018-11-05.md


movie_2018-11-04.md


movie_2018-11-03.md


movie_2018-11-02.md


movie_2018-11-01.md


movie_2018-10-31.md


movie_2018-10-30.md


movie_2018-10-29.md


movie_2018-10-28.md


movie_2018-10-27.md


movie_2018-10-26.md


movie_2018-10-25.md


movie_2018-10-24.md


movie_2018-10-23.md


movie_2018-10-22.md


movie_2018-10-21.md


movie_2018-10-20.md


movie_2018-10-19.md


movie_2018-10-18.md


movie_2018-10-17.md


movie_2018-10-16.md


movie_2018-10-15.md


movie_2018-10-14.md


movie_2018-10-13.md


movie_2018-10-12.md


movie_2018-10-11.md


movie_2018-10-10.md


movie_2018-10-09.md


movie_2018-10-08.md


movie_2018-10-07.md


movie_2018-10-06.md


movie_2018-10-05.md


movie_2018-10-04.md


movie_2018-10-03.md


movie_2018-10-02.md


movie_2018-10-01.md


movie_2018-09-30.md


movie_2018-09-29.md


movie_2018-09-28.md


movie_2018-09-27.md


movie_2018-09-26.md


movie_2018-09-25.md


movie_2018-09-24.md


movie_2018-09-23.md


movie_2018-09-22.md


movie_2018-09-21.md


movie_2018-09-20.md


movie_2018-09-19.md


movie_2018-09-16.md


movie_2018-09-15.md


movie_2018-09-14.md


movie_2018-09-13.md


movie_2018-09-10.md


movie_2018-09-09.md


movie_2018-09-08.md


movie_2018-09-07.md


movie_2018-09-06.md


movie_2018-09-05.md


movie_2018-09-04.md


movie_2018-09-03.md


movie_2018-09-02.md


movie_2018-09-01.md


movie_2018-08-31.md


movie_2018-08-30.md


movie_2018-08-29.md


movie_2018-08-28.md


movie_2018-08-27.md


movie_2018-08-26.md


movie_2018-08-25.md


movie_2018-08-24.md


movie_2018-08-23.md


movie_2018-08-22.md


movie_2018-08-21.md


movie_2018-08-20.md


movie_2018-08-19.md


movie_2018-08-18.md


movie_2018-08-17.md


movie_2018-08-16.md


movie_2018-08-15.md


movie_2018-08-14.md


movie_2018-08-13.md


movie_2018-08-12.md


movie_2018-08-11.md


movie_2018-08-10.md


movie_2018-08-09.md


movie_2018-08-08.md


movie_2018-08-07.md


movie_2018-08-06.md


movie_2018-08-05.md


movie_2018-08-04.md


movie_2018-08-03.md


movie_2018-08-02.md


movie_2018-08-01.md


movie_2018-07-31.md


movie_2018-07-30.md


movie_2018-07-29.md


movie_2018-07-28.md


movie_2018-07-27.md


movie_2018-07-26.md


movie_2018-07-25.md


movie_2018-07-24.md


movie_2018-07-23.md


movie_2018-07-22.md


movie_2018-07-21.md


movie_2018-07-20.md


movie_2018-07-19.md


Hack News List


hacknews_2019-05-12.md


hacknews_2019-05-11.md


hacknews_2019-05-10.md


hacknews_2019-05-09.md


hacknews_2019-05-08.md


hacknews_2019-05-07.md


hacknews_2019-05-06.md


hacknews_2019-05-05.md


hacknews_2019-05-04.md


hacknews_2019-05-03.md


hacknews_2019-05-02.md


hacknews_2019-05-01.md


hacknews_2019-04-30.md


hacknews_2019-04-29.md


hacknews_2019-04-28.md


hacknews_2019-04-27.md


hacknews_2019-04-26.md


hacknews_2019-04-25.md


hacknews_2019-04-24.md


hacknews_2019-04-23.md


hacknews_2019-04-22.md


hacknews_2019-04-21.md


hacknews_2019-04-20.md


hacknews_2019-04-19.md


hacknews_2019-04-18.md


hacknews_2019-04-17.md


hacknews_2019-04-16.md


hacknews_2019-04-15.md


hacknews_2019-04-14.md


hacknews_2019-04-13.md


hacknews_2019-04-12.md


hacknews_2019-04-11.md


hacknews_2019-04-10.md


hacknews_2019-04-09.md


hacknews_2019-04-08.md


hacknews_2019-04-07.md


hacknews_2019-04-06.md


hacknews_2019-04-05.md


hacknews_2019-04-04.md


hacknews_2019-04-03.md


hacknews_2019-04-02.md


hacknews_2019-04-01.md


hacknews_2019-03-31.md


hacknews_2019-03-30.md


hacknews_2019-03-29.md


hacknews_2019-03-28.md


hacknews_2019-03-27.md


hacknews_2019-03-26.md


hacknews_2019-03-25.md


hacknews_2019-03-24.md


hacknews_2019-03-23.md


hacknews_2019-03-22.md


hacknews_2019-03-21.md


hacknews_2019-03-20.md


hacknews_2019-03-19.md


hacknews_2019-03-18.md


hacknews_2019-03-17.md


hacknews_2019-03-16.md


hacknews_2019-03-15.md


hacknews_2019-03-14.md


hacknews_2019-03-13.md


hacknews_2019-03-12.md


hacknews_2019-03-11.md


hacknews_2019-03-10.md


hacknews_2019-03-09.md


hacknews_2019-03-08.md


hacknews_2019-03-07.md


hacknews_2019-03-06.md


hacknews_2019-03-05.md


hacknews_2019-03-04.md


hacknews_2019-03-03.md


hacknews_2019-03-02.md


hacknews_2019-03-01.md


hacknews_2019-02-28.md


hacknews_2019-02-27.md


hacknews_2019-02-26.md


hacknews_2019-02-25.md


hacknews_2019-02-24.md


hacknews_2019-02-23.md


hacknews_2019-02-22.md


hacknews_2019-02-21.md


hacknews_2019-02-20.md


hacknews_2019-02-19.md


hacknews_2019-02-18.md


hacknews_2019-02-17.md


hacknews_2019-02-16.md


hacknews_2019-02-15.md


hacknews_2019-02-14.md


hacknews_2019-02-13.md


hacknews_2019-02-12.md


hacknews_2019-02-11.md


hacknews_2019-02-10.md


hacknews_2019-02-09.md


hacknews_2019-02-08.md


hacknews_2019-02-07.md


hacknews_2019-02-06.md


hacknews_2019-02-05.md


hacknews_2019-02-04.md


hacknews_2019-02-03.md


hacknews_2019-02-02.md


hacknews_2019-02-01.md


hacknews_2019-01-31.md


hacknews_2019-01-30.md


hacknews_2019-01-29.md


hacknews_2019-01-28.md


hacknews_2019-01-27.md


hacknews_2019-01-26.md


hacknews_2019-01-25.md


hacknews_2019-01-24.md


hacknews_2019-01-23.md


hacknews_2019-01-22.md


hacknews_2019-01-21.md


hacknews_2019-01-20.md


hacknews_2019-01-19.md


hacknews_2019-01-18.md


hacknews_2019-01-17.md


hacknews_2019-01-16.md


hacknews_2019-01-15.md


hacknews_2019-01-14.md


hacknews_2019-01-13.md


hacknews_2019-01-12.md


hacknews_2019-01-11.md


hacknews_2019-01-10.md


hacknews_2019-01-09.md


hacknews_2019-01-08.md


hacknews_2019-01-07.md


hacknews_2019-01-06.md


hacknews_2019-01-05.md


hacknews_2019-01-04.md


hacknews_2019-01-03.md


hacknews_2019-01-02.md


hacknews_2019-01-01.md


hacknews_2018-12-31.md


hacknews_2018-12-30.md


hacknews_2018-12-29.md


hacknews_2018-12-28.md


hacknews_2018-12-27.md


hacknews_2018-12-26.md


hacknews_2018-12-25.md


hacknews_2018-12-24.md


hacknews_2018-12-23.md


hacknews_2018-12-22.md


hacknews_2018-12-21.md


hacknews_2018-12-20.md


hacknews_2018-12-19.md


hacknews_2018-12-18.md


hacknews_2018-12-17.md


hacknews_2018-12-16.md


hacknews_2018-12-15.md


hacknews_2018-12-14.md


hacknews_2018-12-13.md


hacknews_2018-12-12.md


hacknews_2018-12-11.md


hacknews_2018-12-10.md


hacknews_2018-12-09.md


hacknews_2018-12-08.md


hacknews_2018-12-07.md


hacknews_2018-12-06.md


hacknews_2018-12-05.md


hacknews_2018-12-04.md


hacknews_2018-12-03.md


hacknews_2018-12-02.md


hacknews_2018-12-01.md


hacknews_2018-11-30.md


hacknews_2018-11-29.md


hacknews_2018-11-28.md


hacknews_2018-11-27.md


hacknews_2018-11-26.md


hacknews_2018-11-25.md


hacknews_2018-11-24.md


hacknews_2018-11-23.md


hacknews_2018-11-22.md


hacknews_2018-11-21.md


hacknews_2018-11-20.md


hacknews_2018-11-19.md


hacknews_2018-11-18.md


hacknews_2018-11-17.md


hacknews_2018-11-16.md


hacknews_2018-11-15.md


hacknews_2018-11-14.md


hacknews_2018-11-13.md


hacknews_2018-11-12.md


hacknews_2018-11-11.md


hacknews_2018-11-10.md


hacknews_2018-11-09.md


hacknews_2018-11-08.md


hacknews_2018-11-07.md


hacknews_2018-11-06.md


hacknews_2018-11-05.md


hacknews_2018-11-04.md


hacknews_2018-11-03.md


hacknews_2018-11-02.md


hacknews_2018-11-01.md


hacknews_2018-10-31.md


hacknews_2018-10-30.md


hacknews_2018-10-29.md


hacknews_2018-10-28.md


hacknews_2018-10-27.md


hacknews_2018-10-26.md


hacknews_2018-10-25.md


hacknews_2018-10-24.md


hacknews_2018-10-23.md


hacknews_2018-10-22.md


hacknews_2018-10-21.md


hacknews_2018-10-20.md


hacknews_2018-10-19.md


hacknews_2018-10-18.md


hacknews_2018-10-17.md


hacknews_2018-10-16.md


hacknews_2018-10-15.md


hacknews_2018-10-14.md


hacknews_2018-10-13.md


hacknews_2018-10-12.md


hacknews_2018-10-11.md


hacknews_2018-10-10.md


hacknews_2018-10-09.md


hacknews_2018-10-08.md


hacknews_2018-10-07.md


hacknews_2018-10-06.md


hacknews_2018-10-05.md


hacknews_2018-10-04.md


hacknews_2018-10-03.md


hacknews_2018-10-02.md


hacknews_2018-10-01.md


hacknews_2018-09-30.md


hacknews_2018-09-29.md


hacknews_2018-09-28.md


hacknews_2018-09-27.md


hacknews_2018-09-26.md


hacknews_2018-09-25.md


hacknews_2018-09-24.md


hacknews_2018-09-23.md


hacknews_2018-09-22.md


hacknews_2018-09-21.md


hacknews_2018-09-20.md


hacknews_2018-09-19.md


hacknews_2018-09-16.md


hacknews_2018-09-15.md


hacknews_2018-09-14.md


hacknews_2018-09-13.md


hacknews_2018-09-10.md


hacknews_2018-09-09.md


hacknews_2018-09-08.md


hacknews_2018-09-07.md


hacknews_2018-09-06.md


hacknews_2018-09-05.md


hacknews_2018-09-04.md


hacknews_2018-09-03.md


hacknews_2018-09-02.md


hacknews_2018-09-01.md


hacknews_2018-08-31.md


hacknews_2018-08-30.md


hacknews_2018-08-29.md


hacknews_2018-08-28.md


hacknews_2018-08-27.md


hacknews_2018-08-26.md


hacknews_2018-08-25.md


hacknews_2018-08-24.md


hacknews_2018-08-23.md


hacknews_2018-08-22.md


hacknews_2018-08-21.md


hacknews_2018-08-20.md


hacknews_2018-08-19.md


hacknews_2018-08-18.md


hacknews_2018-08-17.md


hacknews_2018-08-16.md


hacknews_2018-08-15.md


hacknews_2018-08-14.md


hacknews_2018-08-13.md


hacknews_2018-08-12.md


hacknews_2018-08-11.md


hacknews_2018-08-10.md


hacknews_2018-08-09.md


hacknews_2018-08-08.md


hacknews_2018-08-07.md


hacknews_2018-08-06.md


hacknews_2018-08-05.md


hacknews_2018-08-04.md


hacknews_2018-08-03.md


hacknews_2018-08-02.md


hacknews_2018-08-01.md


hacknews_2018-07-31.md


hacknews_2018-07-30.md


hacknews_2018-07-29.md


hacknews_2018-07-28.md


hacknews_2018-07-27.md


hacknews_2018-07-26.md


hacknews_2018-07-25.md


hacknews_2018-07-24.md


hacknews_2018-07-23.md


hacknews_2018-07-22.md


hacknews_2018-07-21.md


hacknews_2018-07-20.md


Feature

 Sendimg Logging Message To Dingding Talk
 Golang Redis Package Feature
 Xpath Selector By Golang
 Golang Html/Template
 Golang Executes Git Command

Github Project Created By Eric Zhou

 Golang Base64 Captcha
 Generating Hacknews Maoyan-Movie-Board Markdown Automatically 自动生成Hacknews新闻
 生活大爆炸-谢尔顿-剪刀石头布-百度智能音箱
 Golang Beego网站 mojotv.cn
 使用go标准库,log信息到钉钉群
 Forked from fogleman/primitive增加mp4导出功能
 Golang Gin框架RESTful APIs项目
 go+phantomjs网页图片截取微服务
 Python Scrapy+MongoDB+cnbeta爬虫样板
 Golang Geolocation Ip转换到省市

",2
eagleoflqj/p1a3_script,JavaScript,"p1a3_script
一亩三分地的油猴脚本
安装
自动（推荐）
点击油猴图标->管理面板，页面中点击实用工具，在URL文本框输入源码地址，点击导入
手动（不推荐）
点击油猴图标->添加新脚本，粘贴p1a3_script.js的源码，Ctrl+S
更新
为保证用户体验请及时更新
自动
若自动安装，则可点击油猴图标->用户脚本检查更新
手动
同手动安装
功能
论坛
自动签到
需要你绑定微信
定位贴和录取汇报贴
自动查看学校、三维
需要你有足够的积分
录取汇报版
自动折叠录取汇报版规
需要你自觉遵守
记录上次的录取汇报筛选条件
重置筛选条件
Doing
自动答题
题库维护中，目前共有29题
成功答题或题目不在题库，console会有提示
",8
kabir-shah/conode,HTML,"Conode
Collaborative project-based learning tool for programming.
Authors

Kabir Shah
Oleg Bychenkov
Jared Smith

Contributing

Make a new branch.
$ git checkout -b my-branch


Make changes and commit them.
$ git add .
$ git commit -m ""Commit message""


Push changes to your branch.
$ git push origin my-branch


Create a pull request on GitHub, and select the base branch as master and the compare branch as my-branch.
Wait for your changes to be approved and merged.

",3
tari/warcdedupe,Rust,"warcdedupe: a tool for deduplicating WARC files.
Given an input WARC file, output a copy with duplicate response records
rewritten to be revisit records pointing to the first instance of that response
in the same file.
Future improvements

More speed. Faster decompression (flate2 is faster than libflate for
decompression), parallel reading/parsing with some kind of exotic buffer
(or possibly iobuf).

",3
peachfuzz/spu2you,JavaScript,"Version Control Plan:
Currently we have two branches: dev branch and master branch. Our plan is to create 4 branches off of the dev branch for each team member, to ensure that we can each make commits without disrupting each other's work. Everyone will be working on separate project components so that we don't create rebasing issues. There is a rule in place that won't allow any members make a merge to a branch without another member reviewing the code first. Every member is responsible for fixing any rebasing issues they create. This helps to prevent merge conflicts.
All members are required to use concise but descriptive commit messages. Each message should clearly describe the changes that were made, as fits within the character count.
This project was bootstrapped with Create React App.
Available Scripts
In the project directory, you can run:
npm start
You will need to run
npm install nodemon -g

nodemon will run the node app and when it detects a change in the file, it will restart the server. No more ctrl+c and npm start manually!
npm run build
Builds the app for production to the build folder.
It correctly bundles React in production mode and optimizes the build for the best performance.
The build is minified and the filenames include the hashes.
Your app is ready to be deployed!
See the section about deployment for more information.

services: active-directory
platforms: nodejs
author: brandwe

Securing a web API with Azure AD
This Node.js server will give you a quick and easy way to set up a REST API Service using the OAuth2 protocol. Then this service is integrated with Azure Active Directory for API protection. The sample server included in the download is designed to run on any platform.
This REST API server is built using Restify and MongoDB with the following features:

A node.js server running an REST API interface with JSON using MongoDB as persistent storage
REST APIs leveraging OAuth2 API protection for endpoints using Azure Active Directory

We've released all of the source code for this example in GitHub under an Apache 2.0 license, so feel free to clone (or even better, fork!) and provide feedback on the forums.
Quick Start

[!Note] If you want to run this sample on Azure Government, navigate to the ""Azure Government Deviations"" section at the bottom of this page.

Getting started with the sample is easy. It is configured to run out of the box with minimal setup.
Step 1: Register a Azure AD Tenant
To use this sample you will need a Azure Active Directory Tenant. If you're not sure what a tenant is or how you would get one, read What is an Azure AD tenant? or Sign up for Azure as an organization. These docs should get you started on your way to using Azure AD.
Step 2: Register your Web API with your Azure AD Tenant
After you get your Azure AD tenant, add this sample app to your tenant so you can use it to protect your API endpoints. If you need help with this step, see: Register the REST API Service Azure Active Directory
Step 3: Download node.js for your platform
To successfully use this sample, you need a working installation of Node.js.
Install Node.js from http://nodejs.org.
Step 4: Install MongoDB on to your platform
To successfully use this sample, you must have a working installation of MongoDB. We will use MongoDB to make our REST API persistent across server instances.
Install MongoDB from http://mongodb.org.
NOTE: This walkthrough assumes that you use the default installation and server endpoints for MongoDB, which at the time of this writing is: mongodb://localhost. This should work locally without any configuration changes if you run this sample on the same machine as you've installed and ran mongodb.
Step 5: Download the Sample application and modules
Next, clone the sample repo and install the NPM.
From your shell or command line:

$ git clone https://github.com/Azure-Samples/active-directory-node-webapi.git
$ cd node-server
$ npm install

Step 6: Configure your server using config.js
You will need to update the sample to use your values for the metadata endpoint.
NOTE: If you wish to accept multiple tenants for this app, you'll want to use the common endpoint and you'll need to pass the issuer: and audience: value if you wish to validate that as well.
Step 7: Run the application

$ cd node-server
$ node app.js

Is the server output hard to understand?: We use bunyan for logging in this sample. The console won't make much sense to you unless you also install bunyan and run the server like above but pipe it through the bunyan binary:

$ npm install -g bunyan
$ node app.js | bunyan

You're done!
You will have a server successfully running on http://localhost:3000. Your REST / JSON API Endpoint will be http://localhost:3000/tasks
Azure Active Directory OIDC Web Sample



Library
Docs
Support
Protocol



This sample demonstrates how to set up OpenId Connect authentication in a web application built using Node.js with Express. The sample is designed to run on any platform.
Prerequisites
To run this sample you will need the following:


Install Node.js from http://nodejs.org/


An Azure AD tenant. If you're not sure what a tenant is or how you would get one, read How to get an Azure AD tenant.


Register the sample in your Azure AD tenant


Sign in to the Azure portal.


On the top bar, click on your account, and then on Switch Directory. Once the Directory + subscription pane opens, choose the Active Directory tenant where you wish to register your application.


Click on All services in the left-hand nav, and choose Azure Active Directory.


Click on App registrations and choose New application registration.


Enter a friendly name for the application, for example 'Webapp-Openidconnect' and select 'Web app / API' as the Application Type.


For the sign-on URL, enter the base URL for this sample which is http://localhost:3000/.


Click Create to create the application.


In the succeeding page, Find the Application ID value and record it for later. You'll need it to configure the client ID in the application.


Under Settings, choose Properties and update the App ID URI which is a unique identifier for your app. It is of the format 'https://<your_tenant_name>/<app_name>' replacing <your_tenant_name> with the name of your Azure AD tenant. For example: https://contoso.onmicrosoft.com/Webapp-Openidconnect


Under Settings, click on Reply URLs and set it to http://localhost:3000/auth/openid/return which this sample uses by default.


From the Settings menu, choose Keys and add a new entry in the Password section:

Type a key description (for instance 'app secret'),
Select a key duration of either In 1 year, In 2 years, or Never Expires.
When you save this page, the key value will be displayed. Copy, and save the value in a safe location.
You'll need this key later to configure the client secret in the app. This key value will not be displayed again, nor retrievable by any other means, so record it as soon as it is visible from the Azure portal.



Download the sample application and modules
Next, clone the sample repo and install the NPM modules.
From your shell or command line run:

$ git clone git@github.com:AzureADQuickStarts/WebApp-OpenIDConnect-NodeJS.git

or

$ git clone https://github.com/AzureADQuickStarts/WebApp-OpenIDConnect-NodeJS.git

From the project root directory, run the command:

$ npm install

Configure the application
Provide the parameters in exports.creds in config.js as instructed.

Update <tenant_name> in exports.identityMetadata with the Azure AD tenant name of the format *.onmicrosoft.com.
Update exports.clientID with the Application Id noted from app registration.
Update exports.clientSecret with the Application key noted from app registration.
Update exports.redirectUrl with the Reply URL noted from app registration.

Optional configuration for production apps:


Update exports.destroySessionUrl in config.js, if you want to use a different post_logout_redirect_uri.


Set exports.useMongoDBSessionStore in config.js to true, if you want to use use mongoDB or other compatible session stores.
The default session store in this sample is express-session. Note that the default session store is not suitable for production.


Update exports.databaseUri, if you want to use mongoDB session store and a different database URI.


Update exports.mongoDBSessionMaxAge. Here you can specify how long you want to keep a session in mongoDB. The unit is second(s).


Build and run the application


Start mongoDB service. If you are using mongoDB session store in this app, you have to install mongoDB and start the service first. If you are using the default session store, you can skip this step.


Run the app using the following command from your command line.
$ node app.js



Is the server output hard to understand?: We use bunyan for logging in this sample. The console won't make much sense to you unless you also install bunyan and run the server like above but pipe it through the bunyan binary:
$ npm install -g bunyan

$ node app.js | bunyan

You're done!
You will have a server successfully running on http://localhost:3000.
Community Help and Support
We use Stack Overflow with the community to provide support. We highly recommend you ask your questions on Stack Overflow first and browse existing issues to see if someone has asked your question before. Make sure that your questions or comments are tagged with [azure-active-directory].
If you find a bug or issue with this sample, please raise the issue on GitHub Issues.
For issues with the passport-azure-ad library, please raise the issue on the library GitHub repo.
Contributing
If you'd like to contribute to this sample, please follow the GitHub Fork and Pull request model.
This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.
Security Library
This library controls how users sign-in and access services. We recommend you always take the latest version of our library in your app when possible.
Security Reporting
If you find a security issue with our libraries or services please report it to secure@microsoft.com with as much detail as possible. Your submission may be eligible for a bounty through the Microsoft Bounty program. Please do not post security issues to GitHub Issues or any other public site. We will contact you shortly upon receiving the information. We encourage you to get notifications of when security incidents occur by visiting this page and subscribing to Security Advisory Alerts.
Acknowledgements
We would like to acknowledge the folks who own/contribute to the following projects for their support of Azure Active Directory and their libraries that were used to build this sample. In places where we forked these libraries to add additional functionality, we ensured that the chain of forking remains intact so you can navigate back to the original package. Working with such great partners in the open source community clearly illustrates what open collaboration can accomplish. Thank you!
About The Code
Copyright (c) Microsoft Corporation. All rights reserved. Licensed under the MIT License (the ""License"");
",4
PureDarwin/PureDarwin,Shell,"PureDarwin 

PureDarwin moved from https://code.google.com/p/puredarwin/
Darwin is the Open Source operating system from Apple that forms the basis for Mac OS X and PureDarwin. PureDarwin is a community project that aims to make Darwin more usable (some people think of it as the informal successor to OpenDarwin).
One current goal of this project is to provide a useful bootable ISO/VM of Darwin 10.x
Come Join our forum over at https:www.pd-devs.org/
See the Wiki for more information.
",739
DerWaldi/ReadMyStuff,Jupyter Notebook,"ReadMyStuff
Install Retinanet
cd main_pipeline
git clone https://github.com/fizyr/keras-retinanet.git
cd keras-retinanet/
pip install .
python setup.py build_ext --inplace

",2
Pokecube-Development/Pokecube-Core,Java,"Pokecube-Core
This is the core section of Pokecube, it contains the main framework needed for the Real-Time battle system and the automatic mob registry
Related/Contained projects
Contains a modifed version of NBTEdit which is currently used for debugging purposes.
Tabula Model loading code is a modified version of the one used for Showcase.
Contains JEP by Nathan Funk and Richard Morris for use with spawn logic, lisence here
",3
gosuri/uiprogress,Go,"uiprogress  
A Go library to render progress bars in terminal applications. It provides a set of flexible features with a customizable API.

Progress bars improve readability for terminal applications with long outputs by providing a concise feedback loop.
Features

Multiple Bars: uiprogress can render multiple progress bars that can be tracked concurrently
Dynamic Addition:  Add additional progress bars any time, even after the progress tracking has started
Prepend and Append Functions: Append or prepend completion percent and time elapsed to the progress bars
Custom Decorator Functions: Add custom functions around the bar along with helper functions

Usage
To start listening for progress bars, call uiprogress.Start() and add a progress bar using uiprogress.AddBar(total int). Update the progress using bar.Incr() or bar.Set(n int). Full source code for the below example is available at example/simple/simple.go
uiprogress.Start()            // start rendering
bar := uiprogress.AddBar(100) // Add a new bar

// optionally, append and prepend completion and elapsed time
bar.AppendCompleted()
bar.PrependElapsed()

for bar.Incr() {
  time.Sleep(time.Millisecond * 20)
}
This will render the below in the terminal

Using Custom Decorators
You can also add a custom decorator function in addition to default bar.AppendCompleted() and bar.PrependElapsed() decorators. The below example tracks the current step for an application deploy progress. Source code for the below example is available at example/full/full.go
var steps = []string{""downloading source"", ""installing deps"", ""compiling"", ""packaging"", ""seeding database"", ""deploying"", ""staring servers""}
bar := uiprogress.AddBar(len(steps))

// prepend the current step to the bar
bar.PrependFunc(func(b *uiprogress.Bar) string {
  return ""app: "" + steps[b.Current()-1]
})

for bar.Incr() {
  time.Sleep(time.Millisecond * 10)
}
Rendering Multiple bars
You can add multiple bars using uiprogress.AddBar(n). The below example demonstrates updating multiple bars concurrently and adding a new bar later in the pipeline. Source for this example is available at example/multi/multi.go
waitTime := time.Millisecond * 100
uiprogress.Start()

// start the progress bars in go routines
var wg sync.WaitGroup

bar1 := uiprogress.AddBar(20).AppendCompleted().PrependElapsed()
wg.Add(1)
go func() {
  defer wg.Done()
  for bar1.Incr() {
    time.Sleep(waitTime)
  }
}()

bar2 := uiprogress.AddBar(40).AppendCompleted().PrependElapsed()
wg.Add(1)
go func() {
  defer wg.Done()
  for bar2.Incr() {
    time.Sleep(waitTime)
  }
}()

time.Sleep(time.Second)
bar3 := uiprogress.AddBar(20).PrependElapsed().AppendCompleted()
wg.Add(1)
go func() {
  defer wg.Done()
  for i := 1; i <= bar3.Total; i++ {
    bar3.Set(i)
    time.Sleep(waitTime)
  }
}()

// wait for all the go routines to finish
wg.Wait()
This will produce

Incr counter
Bar.Incr() is an atomic counter and can be used as a general tracker, making it ideal for tracking progress of work fanned out to a lots of go routines. The source code for the below example is available at example/incr/incr.go
runtime.GOMAXPROCS(runtime.NumCPU()) // use all available cpu cores

// create a new bar and prepend the task progress to the bar and fanout into 1k go routines
count := 1000
bar := uiprogress.AddBar(count).AppendCompleted().PrependElapsed()
bar.PrependFunc(func(b *uiprogress.Bar) string {
  return fmt.Sprintf(""Task (%d/%d)"", b.Current(), count)
})

uiprogress.Start()
var wg sync.WaitGroup

// fanout into go routines
for i := 0; i < count; i++ {
  wg.Add(1)
  go func() {
    defer wg.Done()
    time.Sleep(time.Millisecond * time.Duration(rand.Intn(500)))
    bar.Incr()
  }()
}
time.Sleep(time.Second) // wait for a second for all the go routines to finish
wg.Wait()
uiprogress.Stop()
Installation
$ go get -v github.com/gosuri/uiprogress
Todos

 Resize bars and decorators by auto detecting window's dimensions
 Handle more progress bars than vertical screen allows

License
uiprogress is released under the MIT License. See LICENSE.
",1261
JiejayLan/CSC322_group_project,JavaScript,"Report and Documents
visit our Wiki Page

Getting started
I. clone repo
II. setup local and remote branch
III. using npm or yarn to install all dependencies on your machine
IV. setup .env.development and .env.test by heading to console of firebase

create .env.development and .env.test at root of project folder
open console of firebase
click Mini-eByMazon and click </>
setup key-value pair inside .env.development in following format

    FIREBASE_API=values copy from firebase without double quotes
    FIREBASE_AUTH_DOMAIN=values copy from firebase without double quotes
    FIREBASE_DATABASE_URL=values copy from firebase without double quotes
    FIREBASE_PROJECT_ID=values copy from firebase without double quotes
    FIREBASE_STORAGE_BUCKET=values copy from firebase without double quotes
    FIREBASE_MESSAGING_SENDER_ID=values copy from firebase without double quotes


click Mini-eByMazon Test and click </>
setup key-value pair inside .env.test in the same format in step 4 with values for Mini-eByMazon Test


Suggested Tools
React Developer Tools and Redux DevTools installed for Google Chrome

How to login
For login page, you dont't need to enter any username or password, because I set up a corrrect default username(""jay"") and password(""123"") in login-page react state.

Suggestion for development

""npm run deve"" for development //still can't set up the reload package to reload the page automatically
put component and pages into different folders
don't connet to firebase from client side.You should add a route controller on the server folder. You can take a look at how I write the login page

",4
ad-fidelitas/ctf-admin-website,Vue,"ctf-admin-website
Project setup
npm install

Compiles and hot-reloads for development
npm run serve

Compiles and minifies for production
npm run build

Run your tests
npm run test

Lints and fixes files
npm run lint

Customize configuration
See Configuration Reference.
",2
Tobybsmith/falppy_bird,Processing,"falppy_bird
",2
Pttn/rieMiner,Assembly,"rieMiner 0.9
rieMiner is a Riecoin miner supporting both solo and pooled mining. It was originally adapted and refactored from gatra's cpuminer-rminerd (https://github.com/gatra/cpuminer-rminerd) and dave-andersen's fastrie (https://github.com/dave-andersen/fastrie), though there is no remaining code from rminerd anymore.
Solo mining is done using the GetBlockTemplate protocol, while pooled mining is via the Stratum protocol. A benchmark mode is also proposed to compare more easily the performance between different computers.
Direct links to the latest official Windows x64 and Win32 standalone executables. Binaries built on Debian 9 with almost complete static linking also available (these should run on fresh Debian and Ubuntu installations): Deb64 and Deb32. Also note that 32 bits builds are much slower.
This README serves as manual for rieMiner, and you can also find a PDF version (without build instructions). I hope that this program will be useful for you!
The Riecoin community thanks you for your participation, you will be a contributor to the robustness of the Riecoin network. Happy mining!

I provide a Profitability Calculator here.
Minimum requirements
Only x64 systems with SSE are supported.

Windows 7 or later, or recent enough Linux;
x64 CPU with SSE instruction set;
1 GiB of RAM (the prime table limit must be manually set at a lower value in the options).

Recommended:

Windows 10 or Debian 9;
Intel Core i7 6700 or better, or AMD Ryzen R5 1600 or better;
8 GiB of RAM.

Compile this program
In Debian/Ubuntu x64
You can compile this C++ program with g++, as, m4 and make, install them if needed. Then, get if needed the following dependencies:

Jansson
cURL
libSSL
GMP

On a recent enough Debian or Ubuntu, you can easily install these by doing as root:
apt install g++ make m4 git libjansson-dev libcurl4-openssl-dev libssl-dev libgmp-dev
Then, just download the source files, go/cd to the directory, and do a simple make:
git clone https://github.com/Pttn/rieMiner.git
cd rieMiner
make
For other Linux, executing equivalent commands (using pacman instead of apt,...) should work.
If you get a warning after the compilation that there may be a conflict between libcrypto.so files, install libssl1.0-dev instead of libssl-dev.
In Windows x64
You can compile rieMiner in Windows, and here is one way to do this. First, install MSYS2 (follow the instructions on the website), then enter in the MSYS MinGW-w64 console, and install the tools and dependencies:
pacman -S make git
pacman -S mingw64/mingw-w64-x86_64-gcc
pacman -S mingw64/mingw-w64-x86_64-curl
Note that you must install the mingw64/mingw-w64-x86_64-... packages and not just gcc or curl.
Clone rieMiner with git like for Linux, go to its directory with cd, and compile with make.
Static building
The produced executable will only run in the MSYS console, or if all the needed DLLs are next to the executable. To obtain a standalone executable, you need to link statically the dependencies. Unfortunately, libcurl will give you a hard time, and you need to compile it yourself.
First, download the latest official libcurl code on their website, under ""Source Archives"", and decompress the folder somewhere (for example, next to the rieMiner's one).
In the MSYS MinGW-w64 console, cd to the libcurl directory. We will now configure it to not build unused features, then compile it:
./configure --disable-dict --disable-file --disable-ftp --disable-gopher --disable-imap --disable-ldap --disable-ldaps --disable-pop3 --disable-rtsp --disable-smtp --disable-telnet --disable-tftp --without-ssl --without-libssh2 --without-zlib --without-brotli --without-libidn2  --without-ldap  --without-ldaps --without-rtsp --without-psl --without-librtmp --without-libpsl --without-nghttp2 --disable-shared --disable-libcurl-option
make
Once done:

Create ""incs"" and ""libs"" folders in the rieMiner directory;
In the downloaded libcurl directory, go to the include directory and copy the ""curl"" folder to the ""incs"" folder;
Do the same with the file ""libcurl.a"" from the libs/.lib folder to the rieMiner's ""libs"" folder.

Now, you should be able to compile rieMiner with make static and produce a standalone executable.
Run and configure this program
You can finally run the newly created rieMiner executable using
./rieMiner
If no ""rieMiner.conf"" next to the executable was found, you will be assisted to configure rieMiner. Answer to its questions to start mining. If there is a ""rieMiner.conf"" file next to the executable with incorrect information that was read, you can delete this to get the assistant.
Alternatively, you can create or edit this ""rieMiner.conf"" file next to the executable yourself, in order to provide options to the miner. The rieMiner.conf syntax is very simple: each option is given by a line such
Option type = Option value

It is case sensitive, but spaces and invalid lines are ignored. A line starting with ""#"" will also be ignored. Do not put ; at the end or use other delimiters than = for each line, and do not confuse rieMiner.conf with riecoin.conf! If an option is missing, the default value(s) will be used. If there are duplicate lines, the last one will be used. Here is a sample configuration file for solo mining, with comments explaining the main available options.
# Mining mode: Solo for solo mining via GetBlockTemplate, Pool for pooled mining using Stratum, Benchmark for testing. Default: Benchmark
Mode = Solo

# IP and port of the Riecoin wallet/server or pool. Default: 127.0.0.1 (your computer), port 28332 (default port for Riecoin-Qt)
Host = 127.0.0.1
Port = 28332

# Username and password used to connect to the server (same as rpcuser and rpcpassword in riecoin.conf for solo mining).
# If using Stratum, the username includes the worker name (username.worker). Default: empty values
Username = user
Password = /70P$€CR€7/

# Custom payout address for solo mining (GetBlockTemplate only). Default: this donation address
PayoutAddress = RPttnMeDWkzjqqVp62SdG2ExtCor9w54EB

# Number of threads used for mining. Default: 8
Threads = 8

# The prime table used for mining will contain primes up to the given number.
# Use a bigger limit if you have 16 GiB of available RAM or more, as this will reduce the ratio between the n-tuple and (n + 1)-tuple counts (but also the 1-tuple find rate).
# Reduce if you have less than 8 GiB of RAM (or if you want to reduce memory usage).
# It can go up to 2^64 - 1, but setting this at more than 2^33 will usually be too much and decrease performance. Default: 2^31
PrimeTableLimit = 2147483648

# Refresh rate of the stats in seconds. 0 to disable them and only notify when a long enough tuple or share is found, or when the network finds a block. Default: 30
RefreshInterval = 60

# For solo mining, submit not only blocks (6-tuples) but also k-tuples of at least the given length.
# Additionally, the base prime of such tuple will be shown in the Benchmark Mode. Default: 6
TupleLengthMin = 4

# For solo mining, add consensus rules in the GetBlockTemplate RPC call, each separated by a comma.
# Useful for softforks, for example, to mine SegWit transactions, you would need the following line. Default: no rule
# Rules = segwit

# Other options
# BenchmarkDifficulty = 1600
# BenchmarkTimeLimit = 0
# Benchmark2tupleCountLimit = 100000
# SieveBits = 25
# SieveWorkers = 0
# ConstellationType = 0, 4, 2, 4, 2, 4
# PrimorialNumber = 40
# PrimorialOffsets = 4209995887, 4209999247, 4210002607, 4210005967, 7452755407, 7452758767, 7452762127, 7452765487, 8145217177, 8145220537, 8145223897, 8145227257
# Debug = 0

It is also possible to use custom configuration file paths, examples:
./rieMiner config/example.txt
./rieMiner ""config 2.conf""
./rieMiner /home/user/rieMiner/rieMiner.conf
Benchmark Mode options

BenchmarkDifficulty : sets the testing difficulty (must be from 265 to 32767). Default: 1600;
BenchmarkTimeLimit : sets the testing duration in s. 0 for no time limit. Default: 0;
Benchmark2tupleCountLimit : stops testing after finding this number of 2-tuples. 0 for no limit. Default: 50000.

Advanced/Tweaking/Dev options
They can be useful to get better performance depending on your computer.

SieveBits : size of the segment sieve is 2^SieveBits bits, e.g. 25 means the segment sieve size is 4 MiB. Choose this so that SieveWorkers*SieveBits fits in your L3 cache. Default: 25;
SieveWorkers : the number of threads to use for sieving. Increasing it may solve some CPU underuse problems, but will use more memory. 0 for choosing automatically based on number of Threads and PrimeTableLimit. Default: 0.

These ones should never be modified outside developing purposes and research for now.

ConstellationType : set your Constellation Type, i. e. the primes tuple offsets, each separated by a comma. Default: 0, 4, 2, 4, 2, 4 (values for Riecoin mining);
PrimorialNumber : Primorial Number for the Wheel Factorization. Default: 40;
PrimorialOffsets : list of Offsets from the Primorial for the first number in the prime tuple. Same syntax as ConsType. Default: carefully chosen offsets;
Debug : activate Debug Mode: rieMiner will print a lot of debug messages. Set to 1 to enable, 0 to disable. Other values may introduce some more specific debug messages. Default : 0.

Some possible constellations types (format: (type) -> offsets to put for ConstellationType ; 3 first constellations (n + 0) which can be used for PrimorialOffsets, though some might not work)

5-tuples

(0, 2, 6,  8, 12) -> 0, 2, 4, 2, 4 ; 5, 11, 101,...
(0, 4, 6, 10, 12) -> 0, 4, 2, 4, 2 ; 7, 97, 1867,...


6-tuples

(0, 4, 6, 10, 12, 16) -> 0, 4, 2, 4, 2, 4 (Riecoin) ; 7, 97, 16057,...


7-tuples

(0, 2, 6,  8, 12, 18, 20) -> 0, 2, 4, 2, 4, 6, 2 ; 11, 165701, 1068701,...
(0, 2, 8, 12, 14, 18, 20) -> 0, 2, 6, 4, 2, 4, 2 ; 5639, 88799, 284729,...


8-tuples

(0, 2, 6,  8, 12, 18, 20, 26) -> 0, 2, 4, 2, 4, 6, 2, 6 ; 11, 15760091, 25658441,...
(0, 2, 6, 12, 14, 20, 24, 26) -> 0, 2, 4, 6, 2, 6, 4, 2 ; 17, 1277, 113147,...
(0, 6, 8, 14, 18, 20, 24, 26) -> 0, 6, 2, 6, 4, 2, 4, 2 ; 88793, 284723, 855713,...



Also see the constellationsGen tool in my rieTools repository (https://github.com/Pttn/rieTools).
Memory problems
If you have memory errors (Unable to allocate... or Bad Allocs), try to lower the PrimeTableLimit value in the configuration file.
Statistics
rieMiner will regularly print some stats, and the frequency of this can be changed with the RefreshInterval parameter as said earlier.
For solo mining, rieMiner will regularly show the primes per second speed, and the 1 to 2-tuples/s ratio. From this, it will also estimate the average time to find a block (note that all the ratios are the same, and the estimation should be fairly precise). Of course, even if the average time to find a block is for example 2 days, you could find a block in the next hour as you could find nothing during a week. The number of 2 to 6-tuples found since the start of the mining is also shown.
For pooled mining, the shares per minute metric and the numbers of valid and total shares are shown instead. As it is hard to get a correct earnings estimation from k-shares, no other metric is shown. The Benchmark Mode (or solo mining) can be used to get better figures for comparisons.
rieMiner will also notify if it found a k-tuple (k >= Tuples option value) in solo mining or a share in pooled mining, and if the network found a new block. If it finds a block or a share, it will tell if the submission was accepted (solo mining only) or not. For solo mining, if the block was accepted, the reward will be generated for the address specified in the options. You can then spend it after 100 confirmations. Note that orphaned blocks will be shown as accepted.
Solo mining specific information
Note that other ways for solo mining (protocol proxies,...) were never tested with rieMiner. It was written specifically for the official wallet and the existing Riecoin pools.
Configure the Riecoin wallet for solo mining
We assume that Riecoin Core is already working and synced. To solo mine with it, you have to configure it.

Find the riecoin.conf configuration file. It should be located in /home/username/.riecoin or equivalent in Windows;
Do not confuse this file with the rieMiner.conf!
An example of riecoin.conf content suitable for mining is

rpcuser=(username)
rpcpassword=(password)
rpcport=28332
port=28333
rpcallowip=127.0.0.1
server=1
daemon=1

If you feel the need, you can add more nodes manually with connect=(nodeip), (nodeip) after connect being a node's IP. You can find a list of the nodes connected the last 24 h here: https://chainz.cryptoid.info/ric/#!network.
If you wish to mine from another computer, add another rpcallowip=ip.of.the.computer, or else the connection will be refused. Choose a username and a password and replace (username) and (password).
Work control
You might have to wait some consequent time before finding a block. What if something is actually wrong and then the time the miner finally found a block, the submission fails?
First, if for some reason rieMiner disconnects from the wallet (you killed it or its computer crashed), it will detect that it has not received the mining data and then just stop mining: so if it is currently mining, everything should be fine.
If you are worried about the fact that the block will be incorrectly submitted, here comes the TupleLengthMin option. Indeed, you can send invalid blocks to the wallet (after all, it is yours), and check if the wallet actually received them and if these submissions are properly processed. When such invalid block is submitted, you can check the debug.log file in the same location as riecoin.conf, and then, you should see something like
ERROR: CheckProofOfWork() : n+10 not prime

Remember that the miner searches numbers n such that n, n + 2, n + 6, n + 10, n + 12 and n + 16 are prime, so if you set the TupleLengthMin option to for example 3, rieMiner will submit a n such that n, n + 2 and n + 6 are prime, but not necessarily the other numbers, so you can conclude that the wallet successfully decoded the submission here, and that everything works fine. If you see nothing or another error message, then something is wrong (possible example would be an unstable overclock)...
Also watch regularly if the wallet is correctly syncing, especially if the message Blockheight = ... did not appear since a very long time (except if the network is mining the superblock). In Riecoin-Qt, this can be done by hovering the check at the lower right corner, and comparing the number with the latest block found in a Riecoin explorer. If something is wrong, try to change the nodes in riecoin.conf or check your connection.
Pooled mining specific information
Existing pools:

XPoolX

Host = mining.xpoolx.com
Port = 5000
Owner: xpoolx - info@xpoolx.com
They also support Solo mining via Stratum with a 5% fee


uBlock.it

Host = mine.ublock.it or mine.blockocean.com
Port = 5000
Owner: ziiip - netops.ublock.it@gmail.com



The miner will disconnect if it did not receive anything during 3 minutes (time out).
Benchmarking
rieMiner provides a way to test the performance of a computer, and compare with others. This feature can also be used to appreciate the improvements when trying to improve the miner algorithm. When sharing benchmark results, you must always communicate the difficulty, the prime table limit (PTL), the test duration, the CPU model, the memory speeds (frequency and CL), the miner version, and the OS. Also, do not forget to precise if you changed other options, like the SieveWorkers or Bits.
To compare two different platforms or settings, you must absolutely test with the same difficulty, during enough time. The proposed parameters, conditions and interpretations for serious benchmarking are:

Standard Benchmark

Difficulty of 1600;
PTL of 2^31 = 2147483648;
No time limit;
Stop after finding 50000 2-tuples or more;
The computer must not do anything else during testing;
The system must not swap. Else, the result would not make much sense. Ensure that you have enough memory when benchmarking.



The test will be fairly long, but similar to the real mining conditions. Once the benchmark finished itself (not by the user), it will print something like:
100000 2-tuples found, test finished. rieMiner 0.9, difficulty 1600, PTL 2147483648
BENCHMARK RESULTS: 233.354130 primes/s with ratio 28.955020 -> 0.990626 block(s)/day

Generally speaking, the block(s)/day metric is the one that should be shared or used to compare performance, though it is always good to also take in consideration the other ones. Moreover, for a given difficulty and PTL, the ratio should be the same, and the more precise primes/day metric can be used instead for comparisons.
The precision will be about 2 significant digits for the block(s)/day. To get 3 solid digits, about 1 million of 2-tuples would need to be found, which would be way too long to be practical for the Standard Benchmark.
A run with valid parameters for the Standard Benchmark will additionally print the message
VALID parameters for Standard Benchmark

Which should appear if you want to share your results.
You could stop before 50000 2-tuples, for example at 10000, if you just want a rough estimation of the performance. However, even after this long, the values are often still very imprecise, and can lead to confusion, like a slightly slower computer getting better results. This remark is critical for people wanting to optimize the miner.
A few results
Done with rieMiner 0.9, 100000 2-tuples except otherwise said. Unit: primes/s

AMD Ryzen R7 2700X @4 GHz, DDR4 3200 CL14, Debian 9: 235.856209
AMD Ryzen R7 2700X @4 GHz, DDR4 2400 CL15, Debian 9: 233.354130
AMD Ryzen R7 2700X @3 GHz, DDR4 2400 CL15, Debian 9: 177.234506
Intel Core i7 6700K @3 GHz, DDR4 2400 CL15, Debian 9: 89.288621
Intel Core 2 Quad Q9650 @3 GHz, DDR3 1067 CL6, Debian 9: 40.673097
Intel Pentium D 925 @3 GHz, DDR3 1000 CL6, Debian 9: 7.466797 (10000 2-tuples)

As said, we should use the primes/s metric for fixed difficulty and PTL. The ratio for the Standard Benchmark is about 28.9.
For a given architecture, the performance is basically proportional to the number of cores and frequency. However, we notice that much better RAM doesn't really matter.
Miscellaneous
Unless the weather is very cold, I do not recommend to overclock a CPU for mining, unless you can do that without increasing noticeably the power consumption. My 2700X computer would draw much, much more power at 4 GHz/1.2875 V instead of 3.7 GHz/1.08125 V, which is certainly absurd for a mere 8% increase. To get maximum efficiency, you might want to find the frequency with the best performance/power consumption ratio (which could also be obtained by underclocking the processor).
If you can, try to undervolt the CPU to reduce power consumption, heat and noise.
Developers and license

Pttn, author and maintainer, contact: dev at Pttn dot me

Parts coming from other projects and libraries are subject to their respective licenses. Else, this work is released under the MIT license. See the LICENSE or top of source files for details.
Notable contributors

Michael Bell: assembly optimizations, improvements of work management between threads, and some more.

Versioning
The version naming scheme is 0.9, 0.99, 0.999 and so on for major versions, analogous to 1.0, 2.0, 3.0,.... The first non 9 decimal digit is minor, etc. For example, the version 0.9925a can be thought as 2.2.5a. A perfect bug-free software will be version 1. No precise criteria have been decided about incrementing major or minor versions for now.
Contributing
Feel free to do a pull request or open an issue, and I will review it. I am open for adding new features, but I also wish to keep this project minimalist. Any useful contribution will be welcomed.
By contributing to rieMiner, you accept to place your code in the MIT license.
Donations welcome:

Bitcoin: 1PttnMeD9X6imTsRojmhHa1rjudW8Bjok5
Riecoin: RPttnMeDWkzjqqVp62SdG2ExtCor9w54EB
Gapcoin: GgCyVr6y6beBbTofmTLJHvGc1NCWynQyvw
Ethereum: 0x32de6b854b6a05448b4f25d4496990bece8a2862

Quick contributor's checklist

Your code must compile and work on recent Debian based distributions, and Windows using MSYS;
If modifying the miner, you must ensure that your changes do not cause any performance loss. You have to do proper and long enough before/after benchmarks;
rieMiner must work for any realistic setting, at least try these in the Benchmark Mode (and do some actual mining):

Difficulty 304, PTL 2^20 (Testnet mining conditions);
Difficulty 800, PTL 2^27;
Difficulty 1600, PTL 2^31 (Standard Benchmark, similar to real mining conditions);
Difficulty 3200, PTL 2^31 or more (we will eventually reach such Difficulties someday...).


Ensure that your changes did not break anything, even if it compiles. Examples (if applicable):

There should never be random (or not) segmentation faults or any other bug, try to do actual mining with Gdb, debugging symbols and Debug Mode enabled during hours or even days to catch possible bugs;
Ensure that valid work is produced (pools and Riecoin-Qt must not reject submissions);
Mining must stop completely while disconnected and restart properly when connection is established again.


Follow the style of the rest of the code (curly braces position, camelCase variable names, tabs and not spaces, spaces around + and - but not around * and /,...).

Resources

rieMiner thread on Riecoin-Community.com forum
My personal website about Riecoin
Get the Riecoin wallet
Fast prime cluster search - or building a fast Riecoin miner (part 1), nice article by dave-andersen explaining how Riecoin works and how to build an efficient miner and the algorithms. Unfortunately, he never published part 2...
Riecoin FAQ and technical aspects
Bitcoin Wiki - Getblocktemplate
BIP141 (Segwit)
Bitcoin Wiki - Stratum

",2
vinta/awesome-python,Python,"Awesome Python 
A curated list of awesome Python frameworks, libraries, software and resources.
Inspired by awesome-php.

Awesome Python

Admin Panels
Algorithms and Design Patterns
Audio
Authentication
Build Tools
Built-in Classes Enhancement
Caching
ChatOps Tools
CMS
Code Analysis
Command-line Tools
Compatibility
Computer Vision
Concurrency and Parallelism
Configuration
Cryptography
Data Analysis
Data Validation
Data Visualization
Database Drivers
Database
Date and Time
Debugging Tools
Deep Learning
DevOps Tools
Distributed Computing
Distribution
Documentation
Downloader
E-commerce
Editor Plugins and IDEs
Email
Environment Management
Files
Foreign Function Interface
Forms
Functional Programming
Game Development
Geolocation
GUI
Hardware
HTML Manipulation
HTTP
Image Processing
Implementations
Interactive Interpreter
Internationalization
Job Scheduler
Logging
Machine Learning
Miscellaneous
Natural Language Processing
Network Virtualization
Networking
News Feed
ORM
Package Management
Package Repositories
Permissions
Processes
Queue
Recommender Systems
RESTful API
Robotics
RPC Servers
Science
Search
Serialization
Serverless Frameworks
Specific Formats Processing
Static Site Generator
Tagging
Template Engine
Testing
Text Processing
Third-party APIs
URL Manipulation
Video
Web Asset Management
Web Content Extracting
Web Crawling & Web Scraping
Web Frameworks
WebSocket
WSGI Servers


Services

Code Quality
Continuous Integration


Resources

Podcasts
Twitter
Websites
Weekly


Other Awesome Lists
Contributing


Admin Panels
Libraries for administrative interfaces.

ajenti - The admin panel your servers deserve.
django-grappelli - A jazzy skin for the Django Admin-Interface.
django-suit - Alternative Django Admin-Interface (free only for Non-commercial use).
django-xadmin - Drop-in replacement of Django admin comes with lots of goodies.
flask-admin - Simple and extensible administrative interface framework for Flask.
flower - Real-time monitor and web admin for Celery.
wooey - A Django app which creates automatic web UIs for Python scripts.

Algorithms and Design Patterns
Python implementation of algorithms and design patterns.

algorithms - Minimal examples of data structures and algorithms in Python.
PyPattyrn - A simple yet effective library for implementing common design patterns.
python-patterns - A collection of design patterns in Python.
sortedcontainers - Fast, pure-Python implementation of SortedList, SortedDict, and SortedSet types.

Audio
Libraries for manipulating audio and its metadata.

Audio

audioread - Cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding.
dejavu - Audio fingerprinting and recognition.
mingus - An advanced music theory and notation package with MIDI file and playback support.
pyAudioAnalysis - Python Audio Analysis Library: Feature Extraction, Classification, Segmentation and Applications
pydub - Manipulate audio with a simple and easy high level interface.
TimeSide - Open web audio processing framework.


Metadata

beets - A music library manager and MusicBrainz tagger.
eyeD3 - A tool for working with audio files, specifically MP3 files containing ID3 metadata.
mutagen - A Python module to handle audio metadata.
tinytag - A library for reading music meta data of MP3, OGG, FLAC and Wave files.



Authentication
Libraries for implementing authentications schemes.

OAuth

authlib - JavaScript Object Signing and Encryption draft implementation.
django-allauth - Authentication app for Django that ""just works.""
django-oauth-toolkit - OAuth 2 goodies for Django.
oauthlib - A generic and thorough implementation of the OAuth request-signing logic.
python-oauth2 - A fully tested, abstract interface to creating OAuth clients and servers.
python-social-auth - An easy-to-setup social authentication mechanism.


JWT

pyjwt - JSON Web Token implementation in Python.
python-jose - A JOSE implementation in Python.
python-jwt - A module for generating and verifying JSON Web Tokens.



Build Tools
Compile software from source code.

BitBake - A make-like build tool for embedded Linux.
buildout - A build system for creating, assembling and deploying applications from multiple parts.
PlatformIO - A console tool to build code with different development platforms.
pybuilder - A continuous build tool written in pure Python.
SCons - A software construction tool.

Built-in Classes Enhancement
Libraries for enhancing Python built-in classes.

dataclasses - (Python standard library) Data classes.
attrs - Replacement for __init__, __eq__, __repr__, etc. boilerplate in class definitions.
bidict - Efficient, Pythonic bidirectional map data structures and related functionality..
Box - Python dictionaries with advanced dot notation access.
DottedDict - A library that provides a method of accessing lists and dicts with a dotted path notation.

CMS
Content Management Systems.

wagtail - A Django content management system.
django-cms - An Open source enterprise CMS based on the Django.
feincms - One of the most advanced Content Management Systems built on Django.
Kotti - A high-level, Pythonic web application framework built on Pyramid.
mezzanine - A powerful, consistent, and flexible content management platform.
plone - A CMS built on top of the open source application server Zope.
quokka - Flexible, extensible, small CMS powered by Flask and MongoDB.

Caching
Libraries for caching data.

beaker - A WSGI middleware for sessions and caching.
django-cache-machine - Automatic caching and invalidation for Django models.
django-cacheops - A slick ORM cache with automatic granular event-driven invalidation.
dogpile.cache - dogpile.cache is next generation replacement for Beaker made by same authors.
HermesCache - Python caching library with tag-based invalidation and dogpile effect prevention.
pylibmc - A Python wrapper around the libmemcached interface.
python-diskcache - SQLite and file backed cache backend with faster lookups than memcached and redis.

ChatOps Tools
Libraries for chatbot development.

errbot - The easiest and most popular chatbot to implement ChatOps.

Code Analysis
Tools of static analysis, linters and code quality checkers. See: awesome-static-analysis.

Code Analysis

coala - Language independent and easily extendable code analysis application.
code2flow - Turn your Python and JavaScript code into DOT flowcharts.
prospector - A tool to analyse Python code.
pycallgraph - A library that visualises the flow (call graph) of your Python application.


Code Linters

flake8 - A wrapper around pycodestyle, pyflakes and McCabe.
pylint - A fully customizable source code analyzer.
pylama - A code audit tool for Python and JavaScript.
Code Formatters
black - The uncompromising Python code formatter.
yapf - Yet another Python code formatter from Google.


Static Type Checkers

mypy - Check variable types during compile time.
pyre-check - Performant type checking.


Static Type Annotations Generators

MonkeyType - A system for Python that generates static type annotations by collecting runtime types



Command-line Tools
Libraries for building command-line application.

Command-line Application Development

cement - CLI Application Framework for Python.
click - A package for creating beautiful command line interfaces in a composable way.
cliff - A framework for creating command-line programs with multi-level commands.
clint - Python Command-line Application Tools.
docopt - Pythonic command line arguments parser.
python-fire - A library for creating command line interfaces from absolutely any Python object.
python-prompt-toolkit - A library for building powerful interactive command lines.


Terminal Rendering

asciimatics - A package to create full-screen text UIs (from interactive forms to ASCII animations).
bashplotlib - Making basic plots in the terminal.
colorama - Cross-platform colored terminal text.


Productivity Tools

cookiecutter - A command-line utility that creates projects from cookiecutters (project templates).
doitlive - A tool for live presentations in the terminal.
howdoi - Instant coding answers via the command line.
PathPicker - Select files out of bash output.
percol - Adds flavor of interactive selection to the traditional pipe concept on UNIX.
thefuck - Correcting your previous console command.
tmuxp - A tmux session manager.
try - A dead simple CLI to try out python packages - it's never been easier.


CLI Enhancements

httpie - A command line HTTP client, a user-friendly cURL replacement.
kube-shell - An integrated shell for working with the Kubernetes CLI.
mycli - A Terminal Client for MySQL with AutoCompletion and Syntax Highlighting.
pgcli - Postgres CLI with autocompletion and syntax highlighting.
saws - A Supercharged aws-cli.



Compatibility
Libraries for migrating from Python 2 to 3.

python-future - The missing compatibility layer between Python 2 and Python 3.
python-modernize - Modernizes Python code for eventual Python 3 migration.
six - Python 2 and 3 compatibility utilities.

Computer Vision
Libraries for computer vision.

OpenCV - Open Source Computer Vision Library.
pytesseract - Another wrapper for Google Tesseract OCR.
SimpleCV - An open source framework for building computer vision applications.

Concurrency and Parallelism
Libraries for concurrent and parallel execution. See awesome-asyncio.

concurrent.futures - (Python standard library) A high-level interface for asynchronously executing callables.
multiprocessing - (Python standard library) Process-based parallelism.
eventlet - Asynchronous framework with WSGI support.
gevent - A coroutine-based Python networking library that uses greenlet.
uvloop - Ultra fast implementation of asyncio event loop on top of libuv.
scoop - Scalable Concurrent Operations in Python.

Configuration
Libraries for storing and parsing configuration options.

configobj - INI file parser with validation.
configparser - (Python standard library) INI file parser.
profig - Config from multiple formats with value conversion.
python-decouple - Strict separation of settings from code.

Cryptography

cryptography - A package designed to expose cryptographic primitives and recipes to Python developers.
paramiko - A Python (2.6+, 3.3+) implementation of the SSHv2 protocol, providing both client and server functionality.
passlib - Secure password storage/hashing library, very high level.
pynacl - Python binding to the Networking and Cryptography (NaCl) library.

Data Analysis
Libraries for data analyzing.

Blaze - NumPy and Pandas interface to Big Data.
Open Mining - Business Intelligence (BI) in Pandas interface.
Orange - Data mining, data visualization, analysis and machine learning through visual programming or scripts.
Pandas - A library providing high-performance, easy-to-use data structures and data analysis tools.
Optimus - Cleansing, pre-processing, feature engineering, exploratory data analysis and easy Machine Learning with a PySpark backend.

Data Validation
Libraries for validating data. Used for forms in many cases.

Cerberus - A lightweight and extensible data validation library.
colander - Validating and deserializing data obtained via XML, JSON, an HTML form post.
Dash - Built on top of Flask, React and Plotly aimed at analytical web applications.

awesome-dash


jsonschema - An implementation of JSON Schema for Python.
schema - A library for validating Python data structures.
Schematics - Data Structure Validation.
valideer - Lightweight extensible data validation and adaptation library.
voluptuous - A Python data validation library.

Data Visualization
Libraries for visualizing data. See: awesome-javascript.

Altair - Declarative statistical visualization library for Python.
Bokeh - Interactive Web Plotting for Python.
bqplot - Interactive Plotting Library for the Jupyter Notebook
ggplot - Same API as ggplot2 for R.
Matplotlib - A Python 2D plotting library.
Pygal - A Python SVG Charts Creator.
PyGraphviz - Python interface to Graphviz.
PyQtGraph - Interactive and realtime 2D/3D/Image plotting and science/engineering widgets.
Seaborn - Statistical data visualization using Matplotlib.
VisPy - High-performance scientific visualization based on OpenGL.

Database
Databases implemented in Python.

pickleDB - A simple and lightweight key-value store for Python.
tinydb - A tiny, document-oriented database.
ZODB - A native object database for Python. A key-value and object graph database.

Database Drivers
Libraries for connecting and operating databases.

MySQL - awesome-mysql

mysqlclient - MySQL connector with Python 3 support (mysql-python fork).
PyMySQL - A pure Python MySQL driver compatible to mysql-python.


PostgreSQL - awesome-postgres

psycopg2 - The most popular PostgreSQL adapter for Python.
queries - A wrapper of the psycopg2 library for interacting with PostgreSQL.


Other Relational Databases

pymssql - A simple database interface to Microsoft SQL Server.


NoSQL Databases

cassandra-driver - The Python Driver for Apache Cassandra.
happybase - A developer-friendly library for Apache HBase.
kafka-python - The Python client for Apache Kafka.
py2neo - Python wrapper client for Neo4j's restful interface.
pymongo - The official Python client for MongoDB.
redis-py - The Python client for Redis.


Asynchronous Clients

motor - The async Python driver for MongoDB.
Telephus - Twisted based client for Cassandra.
txpostgres - Twisted based asynchronous driver for PostgreSQL.
txRedis - Twisted based client for Redis.



Date and Time
Libraries for working with dates and times.

Chronyk - A Python 3 library for parsing human-written times and dates.
dateutil - Extensions to the standard Python datetime module.
delorean - A library for clearing up the inconvenient truths that arise dealing with datetimes.
moment - A Python library for dealing with dates/times. Inspired by Moment.js.
Pendulum - Python datetimes made easy.
PyTime - A easy-use Python module which aims to operate date/time/datetime by string.
pytz - World timezone definitions, modern and historical. Brings the tz database into Python.
when.py - Providing user-friendly functions to help perform common date and time actions.
maya - Datetimes for Humans, Maya is mostly built around the headaches and use-cases around parsing datetime data from websites.

Debugging Tools
Libraries for debugging code.

pdb-like Debugger

ipdb - IPython-enabled pdb.
pdb++ - Another drop-in replacement for pdb.
pudb - A full-screen, console-based Python debugger.
wdb - An improbable web debugger through WebSockets.


Tracing

lptrace - strace for Python programs.
manhole - Debug service that will accept unix domain socket connections and present the stacktraces for all threads and an interactive prompt.
pyringe - Debugger capable of attaching to and injecting code into Python processes.
python-hunter - A flexible code tracing toolkit.


Profiler

line_profiler - Line-by-line profiling.
memory_profiler - Monitor Memory usage of Python code.
profiling - An interactive Python profiler.
py-spy - A sampling profiler for Python programs. Written in Rust.
pyflame - A ptracing profiler For Python.
vprof - Visual Python profiler.


Others

icecream - Inspect variables, expressions, and program execution with a single, simple function call.
django-debug-toolbar - Display various debug information for Django.
django-devserver - A drop-in replacement for Django's runserver.
flask-debugtoolbar - A port of the django-debug-toolbar to flask.
pyelftools - Parsing and analyzing ELF files and DWARF debugging information.



Deep Learning
Frameworks for Neural Networks and Deep Learning. See: awesome-deep-learning.

caffe - A fast open framework for deep learning..
keras - A high-level neural networks library and capable of running on top of either TensorFlow or Theano.
mxnet - A deep learning framework designed for both efficiency and flexibility.
pytorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration.
SerpentAI - Game agent framework. Use any video game as a deep learning sandbox.
tensorflow - The most popular Deep Learning framework created by Google.
Theano - A library for fast numerical computation.

DevOps Tools
Software and libraries for DevOps.

ansible - A radically simple IT automation platform.
cloudinit - A multi-distribution package that handles early initialization of a cloud instance.
cuisine - Chef-like functionality for Fabric.
docker-compose - Fast, isolated development environments using Docker.
fabric - A simple, Pythonic tool for remote execution and deployment.
fabtools - Tools for writing awesome Fabric files.
honcho - A Python clone of Foreman, for managing Procfile-based applications.
OpenStack - Open source software for building private and public clouds.
pexpect - Controlling interactive programs in a pseudo-terminal like GNU expect.
psutil - A cross-platform process and system utilities module.
saltstack - Infrastructure automation and management system.
supervisor - Supervisor process control system for UNIX.

Distributed Computing
Frameworks and libraries for Distributed Computing.

Batch Processing

PySpark - Apache Spark Python API.
dask - A flexible parallel computing library for analytic computing.
luigi - A module that helps you build complex pipelines of batch jobs.
mrjob - Run MapReduce jobs on Hadoop or Amazon Web Services.
Ray - A system for parallel and distributed Python that unifies the machine learning ecosystem.


Stream Processing

faust - A stream processing library, porting the ideas from Kafka Streams to Python.
streamparse - Run Python code against real-time streams of data via Apache Storm.



Distribution
Libraries to create packaged executables for release distribution.

dh-virtualenv - Build and distribute a virtualenv as a Debian package.
Nuitka - Compile scripts, modules, packages to an executable or extension module.
py2app - Freezes Python scripts (Mac OS X).
py2exe - Freezes Python scripts (Windows).
PyInstaller - Converts Python programs into stand-alone executables (cross-platform).
pynsist - A tool to build Windows installers, installers bundle Python itself.

Documentation
Libraries for generating project documentation.

sphinx - Python Documentation generator.

awesome-sphinxdoc


pdoc - Epydoc replacement to auto generate API documentation for Python libraries.
pycco - The literate-programming-style documentation generator.

Downloader
Libraries for downloading.

s3cmd - A command line tool for managing Amazon S3 and CloudFront.
s4cmd - Super S3 command line tool, good for higher performance.
you-get - A YouTube/Youku/Niconico video downloader written in Python 3.
youtube-dl - A small command-line program to download videos from YouTube.

E-commerce
Frameworks and libraries for e-commerce and payments.

alipay - Unofficial Alipay API for Python.
Cartridge - A shopping cart app built using the Mezzanine.
django-oscar - An open-source e-commerce framework for Django.
django-shop - A Django based shop system.
merchant - A Django app to accept payments from various payment processors.
money - Money class with optional CLDR-backed locale-aware formatting and an extensible currency exchange solution.
python-currencies - Display money format and its filthy currencies.
forex-python - Foreign exchange rates, Bitcoin price index and currency conversion.
saleor - An e-commerce storefront for Django.
shoop - An open source E-Commerce platform based on Django.

Editor Plugins and IDEs

Emacs

elpy - Emacs Python Development Environment.


Sublime Text

anaconda - Anaconda turns your Sublime Text 3 in a full featured Python development IDE.
SublimeJEDI - A Sublime Text plugin to the awesome auto-complete library Jedi.


Vim

jedi-vim - Vim bindings for the Jedi auto-completion library for Python.
python-mode - An all in one plugin for turning Vim into a Python IDE.
YouCompleteMe - Includes Jedi-based completion engine for Python.


Visual Studio

PTVS - Python Tools for Visual Studio.


Visual Studio Code

Python - An extension with rich support for the Python language, with features including linting, IntelliSense, formatting, refactoring, debugging, unit tests, and jupyter support.


IDE

PyCharm - Commercial Python IDE by JetBrains. Has free community edition available.
spyder - Open Source Python IDE.



Email
Libraries for sending and parsing email.

envelopes - Mailing for human beings.
flanker - A email address and Mime parsing library.
imbox - Python IMAP for Humans.
inbox.py - Python SMTP Server for Humans.
lamson - Pythonic SMTP Application Server.
Marrow Mailer - High-performance extensible mail delivery framework.
modoboa - A mail hosting and management platform including a modern and simplified Web UI.
Nylas Sync Engine - Providing a RESTful API on top of a powerful email sync platform.
yagmail - Yet another Gmail/SMTP client.

Environment Management
Libraries for Python version and environment management.

pipenv - Sacred Marriage of Pipfile, Pip, & Virtualenv.
poetry - Python dependency management and packaging made easy.
pyenv - Simple Python version management.
venv - (Python standard library in Python 3.3+) Creating lightweight virtual environments.
virtualenv - A tool to create isolated Python environments.

Files
Libraries for file manipulation and MIME type detection.

mimetypes - (Python standard library) Map filenames to MIME types.
path.py - A module wrapper for os.path.
pathlib - (Python standard library) An cross-platform, object-oriented path library.
PyFilesystem2 - Python's filesystem abstraction layer.
python-magic - A Python interface to the libmagic file type identification library.
Unipath - An object-oriented approach to file/directory operations.
watchdog - API and shell utilities to monitor file system events.

Foreign Function Interface
Libraries for providing foreign function interface.

cffi - Foreign Function Interface for Python calling C code.
ctypes - (Python standard library) Foreign Function Interface for Python calling C code.
PyCUDA - A Python wrapper for Nvidia's CUDA API.
SWIG - Simplified Wrapper and Interface Generator.

Forms
Libraries for working with forms.

Deform - Python HTML form generation library influenced by the formish form generation library.
django-bootstrap3 - Bootstrap 3 integration with Django.
django-bootstrap4 - Bootstrap 4 integration with Django.
django-crispy-forms - A Django app which lets you create beautiful forms in a very elegant and DRY way.
django-remote-forms - A platform independent Django form serializer.
WTForms - A flexible forms validation and rendering library.

Functional Programming
Functional Programming with Python.

Coconut - Coconut is a variant of Python built for simple, elegant, Pythonic functional programming.
CyToolz - Cython implementation of Toolz: High performance functional utilities.
fn.py - Functional programming in Python: implementation of missing features to enjoy FP.
funcy - A fancy and practical functional tools.
Toolz - A collection of functional utilities for iterators, functions, and dictionaries.

GUI
Libraries for working with graphical user interface applications.

curses - Built-in wrapper for ncurses used to create terminal GUI applications.
Eel - Little library for making simple Electron-like offline HTML/JS GUI apps, with full access to Python capabilities and libraries.
enaml - Creating beautiful user-interfaces with Declaratic Syntax like QML.
Flexx - Flexx is a pure Python toolkit for creating GUI's, that uses web technology for its rendering.
Gooey - Turn command line programs into a full GUI application with one line.
kivy - A library for creating NUI applications, running on Windows, Linux, Mac OS X, Android and iOS.
pyglet - A cross-platform windowing and multimedia library for Python.
PyGObject - Python Bindings for GLib/GObject/GIO/GTK+ (GTK+3).
PyQt - Python bindings for the Qt cross-platform application and UI framework, with support for both Qt v4 and Qt v5 frameworks.
PySide - Python bindings for the Qt cross-platform application and UI framework, supporting the Qt v4 framework.
PySimpleGUI - Wrapper for tkinter, Qt, WxPython and Remi that creates a unified, easy to understand & more Python-like interface for beginner and intermediate level custom GUIs.
pywebview - A lightweight cross-platform native wrapper around a webview component that allows to display HTML content in its own native dedicated window.
Tkinter - Tkinter is Python's de-facto standard GUI package.
Toga - A Python native, OS native GUI toolkit.
urwid - A library for creating terminal GUI applications with strong support for widgets, events, rich colors, etc.
wxPython - A blending of the wxWidgets C++ class library with the Python.

Game Development
Awesome game development libraries.

Cocos2d - cocos2d is a framework for building 2D games, demos, and other graphical/interactive applications. It is based on pyglet.
Harfang3D - Python framework for 3D, VR and game development. Manage and display complex 3D scenes, with physics, video, sound and music, access VR devices. All written in C++.
Panda3D - 3D game engine developed by Disney and maintained by Carnegie Mellon's Entertainment Technology Center. Written in C++, completely wrapped in Python.
Pygame - Pygame is a set of Python modules designed for writing games.
PyOgre - Python bindings for the Ogre 3D render engine, can be used for games, simulations, anything 3D.
PyOpenGL - Python ctypes bindings for OpenGL and it's related APIs.
PySDL2 - A ctypes based wrapper for the SDL2 library.
RenPy - A Visual Novel engine.

Geolocation
Libraries for geocoding addresses and working with latitudes and longitudes.

django-countries - A Django app that provides country choices for use with forms, flag icons static files, and a country field for models.
GeoDjango - A world-class geographic web framework.
GeoIP - Python API for MaxMind GeoIP Legacy Database.
geojson - Python bindings and utilities for GeoJSON.
geopy - Python Geocoding Toolbox.
pygeoip - Pure Python GeoIP API.

HTML Manipulation
Libraries for working with HTML and XML.

BeautifulSoup - Providing Pythonic idioms for iterating, searching, and modifying HTML or XML.
bleach - A whitelist-based HTML sanitization and text linkification library.
cssutils - A CSS library for Python.
html5lib - A standards-compliant library for parsing and serializing HTML documents and fragments.
lxml - A very fast, easy-to-use and versatile library for handling HTML and XML.
MarkupSafe - Implements a XML/HTML/XHTML Markup safe string for Python.
pyquery - A jQuery-like library for parsing HTML.
untangle - Converts XML documents to Python objects for easy access.
WeasyPrint - A visual rendering engine for HTML and CSS that can export to PDF.
xmldataset - Simple XML Parsing.
xmltodict - Working with XML feel like you are working with JSON.

HTTP
Libraries for working with HTTP.

grequests - requests + gevent for asynchronous HTTP requests.
httplib2 - Comprehensive HTTP client library.
requests - HTTP Requests for Humans™.
treq - Python requests like API built on top of Twisted's HTTP client.
urllib3 - A HTTP library with thread-safe connection pooling, file post support, sanity friendly.

Hardware
Libraries for programming with hardware.

ino - Command line toolkit for working with Arduino.
keyboard - Hook and simulate global keyboard events on Windows and Linux.
mouse - Hook and simulate global mouse events on Windows and Linux.
Pingo - Pingo provides a uniform API to program devices like the Raspberry Pi, pcDuino, Intel Galileo, etc.
PyUserInput - A module for cross-platform control of the mouse and keyboard.
scapy - A brilliant packet manipulation library.
wifi - A Python library and command line tool for working with WiFi on Linux.

Image Processing
Libraries for manipulating images.

hmap - Image histogram remapping.
imgSeek - A project for searching a collection of images using visual similarity.
nude.py - Nudity detection.
pagan - Retro identicon (Avatar) generation based on input string and hash.
pillow - Pillow is the friendly PIL fork.
pyBarcode - Create barcodes in Python without needing PIL.
pygram - Instagram-like image filters.
python-qrcode - A pure Python QR Code generator.
Quads - Computer art based on quadtrees.
scikit-image - A Python library for (scientific) image processing.
thumbor - A smart imaging service. It enables on-demand crop, re-sizing and flipping of images.
wand - Python bindings for MagickWand, C API for ImageMagick.

Implementations
Implementations of Python.

CLPython - Implementation of the Python programming language written in Common Lisp.
CPython - Default, most widely used implementation of the Python programming language written in C.
Cython - Optimizing Static Compiler for Python. Uses type mixins to compile Python into C or C++ modules resulting in large performance gains
Grumpy - More compiler than interpreter as more powerful CPython2.7 replacement (alpha).
IronPython - Implementation of the Python programming language written in C# targeting the .NET Framework and Mono.
Jython - Implementation of Python programming language written in Java for the Java virtual machine (JVM).
MicroPython - MicroPython - a lean and efficient Python programming language implementation for microcontrollers and constrained systems
Numba - Python JIT compiler to LLVM aimed at scientific Python.
PeachPy - x86-64 assembler embedded in Python. Can be used as inline assembler for Python or as a stand-alone assembler for Windows, Linux, OS X, Native Client and Go.
Pyjion - A JIT for Python based upon CoreCLR.
PyPy - Implementation of the Python programming language written in RPython and translated into C. PyPy focuses on speed, efficiency and compatibility with the original CPython interpreter. The interpreter uses black magic to make Python very fast without having to add in additional type information.
PySec - Hardened version of python that makes it easier for security professionals and developers to write applications more resilient to attacks and manipulations.
Pyston - A Python implementation built using LLVM and modern JIT techniques with the goal of achieving good performance.
Stackless Python - An enhanced version of the Python programming language which allows programmers to reap the benefits of thread-based programming without the performance and complexity problems associated with conventional threads.

Interactive Interpreter
Interactive Python interpreters (REPL).

bpython - A fancy interface to the Python interpreter.
Jupyter Notebook (IPython) - A rich toolkit to help you make the most out of using Python interactively.

awesome-jupyter


ptpython - Advanced Python REPL built on top of the python-prompt-toolkit.

Internationalization
Libraries for working with i18n.

Babel - An internationalization library for Python.
PyICU - A wrapper of International Components for Unicode C++ library (ICU).

Job Scheduler
Libraries for scheduling jobs.

APScheduler - A light but powerful in-process task scheduler that lets you schedule functions.
django-schedule - A calendaring app for Django.
doit - A task runner and build tool.
gunnery - Multipurpose task execution tool for distributed systems with web-based interface.
Joblib - A set of tools to provide lightweight pipelining in Python.
Plan - Writing crontab file in Python like a charm.
schedule - Python job scheduling for humans.
Spiff - A powerful workflow engine implemented in pure Python.
TaskFlow - A Python library that helps to make task execution easy, consistent and reliable.
Airflow - Airflow is a platform to programmatically author, schedule and monitor workflows.

Logging
Libraries for generating and working with logs.

Eliot - Logging for complex & distributed systems.
logbook - Logging replacement for Python.
logging - (Python standard library) Logging facility for Python.
raven - Python client for Sentry, a log/error tracking, crash reporting and aggregation platform for web applications.

Machine Learning
Libraries for Machine Learning. See: awesome-machine-learning.

H2O - Open Source Fast Scalable Machine Learning Platform.
Metrics - Machine learning evaluation metrics.
NuPIC - Numenta Platform for Intelligent Computing.
scikit-learn - The most popular Python library for Machine Learning.
Spark ML - Apache Spark's scalable Machine Learning library.
vowpal_porpoise - A lightweight Python wrapper for Vowpal Wabbit.
xgboost - A scalable, portable, and distributed gradient boosting library.

Microsoft Windows
Python programming on Microsoft Windows.

Python(x,y) - Scientific-applications-oriented Python Distribution based on Qt and Spyder.
pythonlibs - Unofficial Windows binaries for Python extension packages.
PythonNet - Python Integration with the .NET Common Language Runtime (CLR).
PyWin32 - Python Extensions for Windows.
WinPython - Portable development environment for Windows 7/8.

Miscellaneous
Useful libraries or tools that don't fit in the categories above.

blinker - A fast Python in-process signal/event dispatching system.
boltons - A set of pure-Python utilities.
itsdangerous - Various helpers to pass trusted data to untrusted environments.
pluginbase - A simple but flexible plugin system for Python.
tryton - A general purpose business framework.

Natural Language Processing
Libraries for working with human languages.

General

gensim - Topic Modelling for Humans.
langid.py - Stand-alone language identification system.
nltk - A leading platform for building Python programs to work with human language data.
pattern - A web mining module for the Python.
polyglot - Natural language pipeline supporting hundreds of languages.
pytext - A natural language modeling framework based on PyTorch.
PyTorch-NLP - A toolkit enabling rapid deep learning NLP prototyping for research.
spacy - A library for industrial-strength natural language processing in Python and Cython.
stanfordnlp - The Stanford NLP Group's official Python library, supporting 50+ languages.


Chinese

jieba - The most popular Chinese text segmentation library.
pkuseg-python - A toolkit for Chinese word segmentation in various domains.
snownlp - A library for processing Chinese text.
funNLP - A collection of tools and datasets for Chinese NLP.



Network Virtualization
Tools and libraries for Virtual Networking and SDN (Software Defined Networking).

mininet - A popular network emulator and API written in Python.
pox - A Python-based SDN control applications, such as OpenFlow SDN controllers.

Networking
Libraries for networking programming.

asyncio - (Python standard library) Asynchronous I/O, event loop, coroutines and tasks.

awesome-asyncio


pulsar - Event-driven concurrent framework for Python.
pyzmq - A Python wrapper for the ZeroMQ message library.
Twisted - An event-driven networking engine.
napalm - Cross-vendor API to manipulate network devices.

News Feed
Libraries for building user's activities.

django-activity-stream - Generating generic activity streams from the actions on your site.
Stream Framework - Building newsfeed and notification systems using Cassandra and Redis.

ORM
Libraries that implement Object-Relational Mapping or data mapping techniques.

Relational Databases

Django Models - A part of Django.
SQLAlchemy - The Python SQL Toolkit and Object Relational Mapper.

awesome-sqlalchemy


dataset - Store Python dicts in a database - works with SQLite, MySQL, and PostgreSQL.
orator -  The Orator ORM provides a simple yet beautiful ActiveRecord implementation.
peewee - A small, expressive ORM.
pony - ORM that provides a generator-oriented interface to SQL.
pydal - A pure Python Database Abstraction Layer.


NoSQL Databases

hot-redis - Rich Python data types for Redis.
mongoengine - A Python Object-Document-Mapper for working with MongoDB.
PynamoDB - A Pythonic interface for Amazon DynamoDB.
redisco - A Python Library for Simple Models and Containers Persisted in Redis.



Package Management
Libraries for package and dependency management.

pip - The Python package and dependency manager.

PyPI
pip-tools - A set of tools to keep your pinned Python dependencies fresh.


conda - Cross-platform, Python-agnostic binary package manager.

Package Repositories
Local PyPI repository server and proxies.

warehouse - Next generation Python Package Repository (PyPI).
bandersnatch - PyPI mirroring tool provided by Python Packaging Authority (PyPA).
devpi - PyPI server and packaging/testing/release tool.
localshop - Local PyPI server (custom packages and auto-mirroring of pypi).

Permissions
Libraries that allow or deny users access to data or functionality.

django-guardian - Implementation of per object permissions for Django 1.2+
django-rules - A tiny but powerful app providing object-level permissions to Django, without requiring a database.

Processes
Libraries for starting and communicating with OS processes.

delegator.py - Subprocesses for Humans™ 2.0.
sarge - Yet another wrapper for subprocess.
sh - A full-fledged subprocess replacement for Python.

Queue
Libraries for working with event and task queues.

celery - An asynchronous task queue/job queue based on distributed message passing.
huey - Little multi-threaded task queue.
mrq - Mr. Queue - A distributed worker task queue in Python using Redis & gevent.
rq - Simple job queues for Python.

Recommender Systems
Libraries for building recommender systems.

annoy - Approximate Nearest Neighbors in C++/Python optimized for memory usage.
fastFM - A library for Factorization Machines.
implicit - A fast Python implementation of collaborative filtering for implicit datasets.
libffm - A library for Field-aware Factorization Machine (FFM).
lightfm - A Python implementation of a number of popular recommendation algorithms.
spotlight - Deep recommender models using PyTorch.
Surprise - A scikit for building and analyzing recommender systems.
tensorrec - A Recommendation Engine Framework in TensorFlow.

RESTful API
Libraries for developing RESTful APIs.

Django

django-rest-framework - A powerful and flexible toolkit to build web APIs.
django-tastypie - Creating delicious APIs for Django apps.


Flask

eve - REST API framework powered by Flask, MongoDB and good intentions.
flask-api-utils - Taking care of API representation and authentication for Flask.
flask-api - Browsable Web APIs for Flask.
flask-restful - Quickly building REST APIs for Flask.
flask-restless - Generating RESTful APIs for database models defined with SQLAlchemy.


Pyramid

cornice - A RESTful framework for Pyramid.


Framework agnostic

apistar - A smart Web API framework, designed for Python 3.
falcon - A high-performance framework for building cloud APIs and web app backends.
hug - A Python3 framework for cleanly exposing APIs over HTTP and the Command Line with automatic documentation and validation.
restless - Framework agnostic REST framework based on lessons learned from Tastypie.
ripozo - Quickly creating REST/HATEOAS/Hypermedia APIs.
sandman - Automated REST APIs for existing database-driven systems.



Robotics
Libraries for robotics.

PythonRobotics - This is a compilation of various robotics algorithms with visualizations.
rospy - This is a library for ROS (Robot Operating System).

RPC Servers
RPC-compatible servers.

SimpleJSONRPCServer - This library is an implementation of the JSON-RPC specification.
SimpleXMLRPCServer - (Python standard library) Simple XML-RPC server implementation, single-threaded.
zeroRPC - zerorpc is a flexible RPC implementation based on ZeroMQ and MessagePack.

Science
Libraries for scientific computing.

astropy - A community Python library for Astronomy.
bcbio-nextgen - Providing best-practice pipelines for fully automated high throughput sequencing analysis.
bccb - Collection of useful code related to biological analysis.
Biopython - Biopython is a set of freely available tools for biological computation.
cclib - A library for parsing and interpreting the results of computational chemistry packages.
Colour - A colour science package implementing a comprehensive number of colour theory transformations and algorithms.
NetworkX - A high-productivity software for complex networks.
NIPY - A collection of neuroimaging toolkits.
NumPy - A fundamental package for scientific computing with Python.
Open Babel - A chemical toolbox designed to speak the many languages of chemical data.
ObsPy - A Python toolbox for seismology.
PyDy - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion.
PyMC - Markov Chain Monte Carlo sampling toolkit.
QuTiP - Quantum Toolbox in Python.
RDKit - Cheminformatics and Machine Learning Software.
SciPy - A Python-based ecosystem of open-source software for mathematics, science, and engineering.
statsmodels - Statistical modeling and econometrics in Python.
SymPy - A Python library for symbolic mathematics.
Zipline - A Pythonic algorithmic trading library.
SimPy -  A process-based discrete-event simulation framework.

Search
Libraries and software for indexing and performing search queries on data.

elasticsearch-py - The official low-level Python client for Elasticsearch.
elasticsearch-dsl-py - The official high-level Python client for Elasticsearch.
django-haystack - Modular search for Django.
pysolr - A lightweight Python wrapper for Apache Solr.
whoosh - A fast, pure Python search engine library.

Serialization
Libraries for serializing complex data types

marshmallow - A lightweight library for converting complex objects to and from simple Python datatypes.
pysimdjson - A Python bindings for simdjson.
python-rapidjson - A Python wrapper around RapidJSON.

Serverless Frameworks
Frameworks for developing serverless Python code.

python-lambda - A toolkit for developing and deploying Python code in AWS Lambda.
Zappa - A tool for deploying WSGI applications on AWS Lambda and API Gateway.

Specific Formats Processing
Libraries for parsing and manipulating specific text formats.

General

tablib - A module for Tabular Datasets in XLS, CSV, JSON, YAML.


Office

openpyxl - A library for reading and writing Excel 2010 xlsx/xlsm/xltx/xltm files.
pyexcel - Providing one API for reading, manipulating and writing csv, ods, xls, xlsx and xlsm files.
python-docx - Reads, queries and modifies Microsoft Word 2007/2008 docx files.
python-pptx - Python library for creating and updating PowerPoint (.pptx) files.
unoconv - Convert between any document format supported by LibreOffice/OpenOffice.
XlsxWriter - A Python module for creating Excel .xlsx files.
xlwings - A BSD-licensed library that makes it easy to call Python from Excel and vice versa.
xlwt / xlrd - Writing and reading data and formatting information from Excel files.


PDF

PDFMiner - A tool for extracting information from PDF documents.
PyPDF2 - A library capable of splitting, merging and transforming PDF pages.
ReportLab - Allowing Rapid creation of rich PDF documents.


Markdown

Mistune - Fastest and full featured pure Python parsers of Markdown.
Python-Markdown - A Python implementation of John Gruber’s Markdown.


YAML

PyYAML - YAML implementations for Python.


CSV

csvkit - Utilities for converting to and working with CSV.


Archive

unp - A command line tool that can unpack archives easily.



Static Site Generator
Static site generator is a software that takes some text + templates as input and produces HTML files on the output.

mkdocs - Markdown friendly documentation generator.
pelican - Static site generator that supports Markdown and reST syntax.
lektor - An easy to use static CMS and blog engine.
nikola - A static website and blog generator.

Tagging
Libraries for tagging items.

django-taggit - Simple tagging for Django.

Template Engine
Libraries and tools for templating and lexing.

Jinja2 - A modern and designer friendly templating language.
Genshi - Python templating toolkit for generation of web-aware output.
Mako - Hyperfast and lightweight templating for the Python platform.

Testing
Libraries for testing codebases and generating test data.

Testing Frameworks

pytest - A mature full-featured Python testing tool.
hypothesis - Hypothesis is an advanced Quickcheck style property based testing library.
nose2 - The successor to nose, based on `unittest2.
Robot Framework - A generic test automation framework.
unittest - (Python standard library) Unit testing framework.


Test Runners

green - A clean, colorful test runner.
mamba - The definitive testing tool for Python. Born under the banner of BDD.
tox - Auto builds and tests distributions in multiple Python versions


GUI / Web Testing

locust - Scalable user load testing tool written in Python.
PyAutoGUI - PyAutoGUI is a cross-platform GUI automation Python module for human beings.
Selenium - Python bindings for Selenium WebDriver.
sixpack - A language-agnostic A/B Testing framework.
splinter - Open source tool for testing web applications.


Mock

doublex - Powerful test doubles framework for Python.
freezegun - Travel through time by mocking the datetime module.
httmock - A mocking library for requests for Python 2.6+ and 3.2+.
httpretty - HTTP request mock tool for Python.
mock - (Python standard library) A mocking and patching library.
Mocket - Socket Mock Framework plus HTTP[S]/asyncio/gevent mocking library with recording/replaying capability.
responses - A utility library for mocking out the requests Python library.
VCR.py - Record and replay HTTP interactions on your tests.


Object Factories

factory_boy - A test fixtures replacement for Python.
mixer - Another fixtures replacement. Supported Django, Flask, SQLAlchemy, Peewee and etc.
model_mommy - Creating random fixtures for testing in Django.


Code Coverage

coverage - Code coverage measurement.


Fake Data

mimesis - is a Python library that help you generate fake data.
fake2db - Fake database generator.
faker - A Python package that generates fake data.
radar - Generate random datetime / time.


Error Handler

FuckIt.py - FuckIt.py uses state-of-the-art technology to make sure your Python code runs whether it has any right to or not.



Text Processing
Libraries for parsing and manipulating plain texts.

General

chardet - Python 2/3 compatible character encoding detector.
difflib - (Python standard library) Helpers for computing deltas.
ftfy - Makes Unicode text less broken and more consistent automagically.
fuzzywuzzy - Fuzzy String Matching.
Levenshtein - Fast computation of Levenshtein distance and string similarity.
pangu.py - Paranoid text spacing.
pyfiglet - An implementation of figlet written in Python.
pypinyin - Convert Chinese hanzi (漢字) to pinyin (拼音).
textdistance - Compute distance between sequences. 30+ algorithms, pure python implementation, common interface, optional external libs usage.
unidecode - ASCII transliterations of Unicode text.


Slugify

awesome-slugify - A Python slugify library that can preserve unicode.
python-slugify - A Python slugify library that translates unicode to ASCII.
unicode-slugify - A slugifier that generates unicode slugs with Django as a dependency.


Unique identifiers

hashids - Implementation of hashids in Python.
shortuuid - A generator library for concise, unambiguous and URL-safe UUIDs.


Parser

ply - Implementation of lex and yacc parsing tools for Python.
pygments - A generic syntax highlighter.
pyparsing - A general purpose framework for generating parsers.
python-nameparser - Parsing human names into their individual components.
python-phonenumbers - Parsing, formatting, storing and validating international phone numbers.
python-user-agents - Browser user agent parser.
sqlparse - A non-validating SQL parser.



Third-party APIs
Libraries for accessing third party services APIs. See: List of Python API Wrappers and Libraries.

apache-libcloud - One Python library for all clouds.
boto3 - Python interface to Amazon Web Services.
django-wordpress - WordPress models and views for Django.
facebook-sdk - Facebook Platform Python SDK.
google-api-python-client - Google APIs Client Library for Python.
gspread - Google Spreadsheets Python API.
twython - A Python wrapper for the Twitter API.

URL Manipulation
Libraries for parsing URLs.

furl - A small Python library that makes parsing and manipulating URLs easy.
purl - A simple, immutable URL class with a clean API for interrogation and manipulation.
pyshorteners - A pure Python URL shortening lib.
webargs - A friendly library for parsing HTTP request arguments, with built-in support for popular web frameworks, including Flask, Django, Bottle, Tornado, and Pyramid.

Video
Libraries for manipulating video and GIFs.

moviepy - A module for script-based movie editing with many formats, including animated GIFs.
scikit-video - Video processing routines for SciPy.

WSGI Servers
WSGI-compatible web servers.

bjoern - Asynchronous, very fast and written in C.
gunicorn - Pre-forked, partly written in C.
uWSGI - A project aims at developing a full stack for building hosting services, written in C.
waitress - Multi-threaded, powers Pyramid.
werkzeug - A WSGI utility library for Python that powers Flask and can easily be embedded into your own projects.

Web Asset Management
Tools for managing, compressing and minifying website assets.

django-compressor - Compresses linked and inline JavaScript or CSS into a single cached file.
django-pipeline - An asset packaging library for Django.
django-storages - A collection of custom storage back ends for Django.
fanstatic - Packages, optimizes, and serves static file dependencies as Python packages.
fileconveyor - A daemon to detect and sync files to CDNs, S3 and FTP.
flask-assets - Helps you integrate webassets into your Flask app.
webassets - Bundles, optimizes, and manages unique cache-busting URLs for static resources.

Web Content Extracting
Libraries for extracting web contents.

html2text - Convert HTML to Markdown-formatted text.
lassie - Web Content Retrieval for Humans.
micawber - A small library for extracting rich content from URLs.
newspaper - News extraction, article extraction and content curation in Python.
python-readability - Fast Python port of arc90's readability tool.
requests-html - Pythonic HTML Parsing for Humans.
sumy - A module for automatic summarization of text documents and HTML pages.
textract - Extract text from any document, Word, PowerPoint, PDFs, etc.
toapi - Every web site provides APIs.

Web Crawling & Web Scraping
Libraries to automate data extraction from websites.

cola - A distributed crawling framework.
feedparser - Universal feed parser.
grab - Site scraping framework.
MechanicalSoup - A Python library for automating interaction with websites.
portia - Visual scraping for Scrapy.
pyspider - A powerful spider system.
robobrowser - A simple, Pythonic library for browsing the web without a standalone web browser.
scrapy - A fast high-level screen scraping and web crawling framework.

Web Frameworks
Full stack web frameworks.

Django - The most popular web framework in Python.

awesome-django


Flask - A microframework for Python.

awesome-flask


Pyramid - A small, fast, down-to-earth, open source Python web framework.

awesome-pyramid


Sanic - Web server that's written to go fast.
Vibora - Fast, efficient and asynchronous Web framework inspired by Flask.
Tornado - A Web framework and asynchronous networking library.

WebSocket
Libraries for working with WebSocket.

autobahn-python - WebSocket & WAMP for Python on Twisted and asyncio.
crossbar - Open-source Unified Application Router (Websocket & WAMP for Python on Autobahn).
django-channels - Developer-friendly asynchrony for Django.
django-socketio - WebSockets for Django.
WebSocket-for-Python - WebSocket client and server library for Python 2 and 3 as well as PyPy.

Services
Online tools and APIs to simplify development.
Continuous Integration
See: awesome-CIandCD.

CircleCI - A CI service that can run very fast parallel testing. (GitHub only)
Travis CI - A popular CI service for your open source and private projects. (GitHub only)
Vexor CI - A continuous integration tool for private apps with pay-per-minute billing model.
Wercker - A Docker-based platform for building and deploying applications and microservices.

Code Quality

Codacy - Automated Code Review to ship better code, faster.
Codecov - Code coverage dashboard.
CodeFactor - Automated Code Review for Git.
Landscape - Hosted continuous Python code metrics.

Resources
Where to discover new Python libraries.
Podcasts

From Python Import Podcast
Podcast.init
Python Bytes
Python Testing
Radio Free Python
Talk Python To Me

Twitter

@codetengu
@getpy
@importpython
@planetpython
@pycoders
@pypi
@pythontrending
@PythonWeekly
@TalkPython
@realpython

Websites

/r/CoolGithubProjects
/r/Python
Awesome Python @LibHunt
Django Packages
Full Stack Python
PyPI Ranking
Python 3 Wall of Superpowers
Python Hackers
Python ZEEF
Python 开发社区
Real Python
Trending Python repositories on GitHub today

Weekly

CodeTengu Weekly 碼天狗週刊
Import Python Newsletter
Pycoder's Weekly
Python Weekly
Python Tricks

Contributing
Your contributions are always welcome! Please take a look at the contribution guidelines first.
I will keep some pull requests open if I'm not sure whether those libraries are awesome, you could vote for them by adding 👍 to them. Pull requests will be merged when their votes reach 20.

If you have any question about this opinionated list, do not hesitate to contact me @vinta on Twitter or open an issue on GitHub.
",67185
X-CASH-official/XCASH_proof_of_stake_consensus_node,C,"X-CASH Proof of stake - consensus node
More details will be released on the consensus node soon!
Installation
This program will only run on a Linux/Unix OS at this time. We recommend installing this on a Ubuntu VPS/Server (16.04 or 18.04) for the best compatibility.
You will also need to run an X-CASH Daemon and X-CASH RPC wallet on the server. You can either download the latest X-CASH release or build from source
Compiling X-CASH Proof of stake - consensus node from source
Dependencies
The following table summarizes the tools and libraries required to build.



Dependencies
Min. version
Ubuntu package




GCC
4.7.3
build-essential


CMake
3.0.0
cmake


pkg-config
any
pkg-config


OpenSSL
any
libssl-dev


Git
any
git


MongoDB
4.0.3
install from binaries


MongoDB C Driver (includes BSON libary)
1.13.1
build from source



Installing MongoDB from binaries
Visit https://www.mongodb.com/download-center/community
Then choose your OS, and make sure the version is the current version and the package is server. Then click on All version binaries. Now find the current version to download. You do not want the debug symbols or the rc version, just the regular current version.
Once you have downloaded the file move the file to a location where you want to keep the binaries, then run this set of commands
tar -xf mongodb-linux-x86_64-*.tgz && rm mongodb-linux-x86_64-*.tgz && sudo mkdir -p /data/db && sudo chmod 770 /data/db && sudo chown $USER /data/db
Building the MongoDB C driver from source
Visit the offical websites installation instructions at http://mongoc.org/libmongoc/current/installing.html
You will need to follow the instructions for Building from a release tarball or Building from git since you need the header files, not just the library files.
After you have built the MongoDB C driver from source, you will need to run
sudo ldconfig
Adding MongoDB to your PATH
You will probably want to add MongoDB to your path so you can run MonogDB by typing mongod at any terminal.
To add MongoDB to your PATH (replace ""MongoDB_folder"" with the location of the bin folder in the folder you installed MongoDB in
echo -e '\nexport PATH=MongoDB_folder:$PATH' >> ~/.profile && source ~/.profile
Cloning the repository
$ git clone https://github.com/X-CASH-official/XCASH_proof_of_stake_consensus_node.git
Build instructions
X-CASH Proof of stake - consensus node uses a Make file.
After cloning the repository, navigate to the folder
cd XCASH_proof_of_stake_consensus_node
Then use the make file to build the binary file
make clean ; make
Running MongoDB
To run MongoDB you will need to navigate to the folder you downloaded the binaries to, and in the bin folder run mongod by running
./mongod
If you have already added MongoDB to your path, you can just type in any terminal
mongod
Setting up the xcashd and xcash-wallet-RPC
First you will need to run xcashd in the background. Navigate to the folder that contains the xcash binaries, then run
./xcashd
Next you need to run a xcash-wallet-rpc. Depending on if this is the consensus node or the consensus backup node, you will need to the run the wallet that contains the public address in the Proof of stake for the CONSENSUS_NODE_PUBLIC_ADDRESS or CONSENSUS_BACKUP_NODE_PUBLIC_ADDRESS
To run the rpc wallet you can run
./xcash-wallet-rpc --wallet-file NAME_OF_WALLET_FILE --password WALLET_FILE_PASSWORD --rpc-bind-port 18285 --confirm-external-bind --daemon-port 18281 --disable-rpc-login --trusted-daemon
Just replace NAME_OF_WALLET_FILE with the name of your wallet file and WALLET_FILE_PASSWORD with the password of that wallet file. Make sure to use port 18285 as this is the port that is used in the program.
We suggest you use the screen command to run the program in the background, this way you can still maintenance the server. To do this run
screen -dmS Daemon ./xcashd
You can also run the RPC wallet this way as well
screen -dmS RPC-Wallet ./xcash-wallet-rpc --wallet-file NAME_OF_WALLET_FILE --password WALLET_FILE_PASSWORD --rpc-bind-port 18285 --confirm-external-bind --daemon-port 18281 --disable-rpc-login --trusted-daemon
To bring the screen from the bacground process to the active process run
screen -x NAME_OF_BACKGROUNDS_SCREEN
Where NAME_OF_BACKGROUNDS_SCREEN would be Daemon or RPC-Wallet in the above examples.
To exit a screen if it is the active process, you can press Control + C. To exit a screen that is a background process you can run
screen -XS NAME_OF_BACKGROUNDS_SCREEN quit
Running X-CASH Proof of stake - consensus node test
It is recomeneded to run the X-CASH Proof of stake test before you run the main program. The test will ensure that your system is compatbile, and that you have setup your system correctly.
To run the X-CASH Proof of stake test, Navigate to the folder that contains the binary, then run
./xcash_proof_of_stake_consensus_node --test
The test will return the number of passed and failed test on the bottom of the console. The failed test need to be 0 before you run the node. If the output is not showing 0 for failed test, then you need to scroll through the testing output and find what test failed (It will be red instead of green). If this is a system compatibility test, then you will need to fix the system. If this is a core test that has failed, then you need to possibly rebuild, or contact us with your OS version, and we can look into it.
Running X-CASH Proof of stake - consensus node
Then you will need to run the xcash_proof_of_stake_consensus_node. Navigate to the folder that contains the binary, then run
./xcash_proof_of_stake_consensus_node
We suggest you use the screen command to run the program in the background, this way you can still maintenance the server. To do this run
screen -dmS xcash_proof_of_stake_consensus_node ./xcash_proof_of_stake_consensus_node
To bring the screen from the bacground process to the active process run
screen -x NAME_OF_BACKGROUNDS_SCREEN
Where NAME_OF_BACKGROUNDS_SCREEN would be xcash_proof_of_stake_consensus_node in the above example.
To exit a screen if it is the active process, you can press Control + C. To exit a screen that is a background process you can run
screen -XS NAME_OF_BACKGROUNDS_SCREEN quit
",2
AngelKitty/review_the_national_post-graduate_entrance_examination,C++,"复习考研的那些事儿～～
这里我将记录我考研的全过程，包括看过的书，写过的笔记，读过的杂志，推荐的番剧，电影，以及我在生活中一些零碎的记录和思考。
也许这一切对你们可能一无是处，但对我而言，这将会是人生中最宝贵的一段回忆，我希望以这种方式记录下来，所以在 Github 上开了此项目。
网上很多up主都喜欢通过拍摄 vlog 这种形式来记录自己的日常，我就突发奇想，能不能在这个词基础上稍微修改一下。于是我就想到了一个非常 nice 的词，自己取一个名叫 plog (Page weblog，plog似乎指代的意思很多，但是大多都是和日志系统有关吧，所以我这个翻译应该不算很偏门吧23333)
当然也欢迎你们加入到我们的复习队伍中，有好看的电影、番剧推荐或者一些有意思的书籍，请 fork 本项目到您的仓库后，再进行 pull request。
本项目分为如下三个部分：

books_and_notes：存放着我复习时候看过的书以及笔记
exam：一些我复习的时候做过的一些题目
plog：记录着我每周的一些日常。

如何获取此项目？
本项目可以直接通过以下方式获取：
# clone
git clone git@github.com:AngelKitty/review_the_national_post-graduate_entrance_examination.git

版权声明

本作品采用知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议进行许可。
The Star And Thank Author License
",25
owlboy/greatpug-public,None,"
The Great Pug
A Bar in the Metaverse (VRChat)
thegreatpug.com
This Repository
This repository is the public sister repository to the private repository for The Great Pug.
Bug Reports and Feature Requests
Feel free to use the issues feature of this repository to report bugs or make feature requests relating to The Great Pug.
Support On-going Development
You can support on-going maintenance, events, and expansion to The Great Pug by joining my Patreon.
You can also donate crypto currency at the following addresses:

Etherium (ETH): 0xa2e7aBB300728afc7564874B12975D2f311687a6
Bitcoin (BTC): 16tWrVpZWJcw64aE5su8bRQJgyAVhBeNZQ
Litecoin (LTC): MCjYc1wm2r1f8uM5FPTyqZbKxnxwvNf7UQ

 
Change Log
5/11/19 (65mb)

Reduced draw called depending on your POV
Simplified some colliders
Rebaked lighting
Rebaked Occlusion
Fixed UV unwraps on some models
Adjusted some audio clip sizes

5/10/19 (71mb)

Reduced Draw Calls by a few depending on your POV
Updated calendar (10 days late!)
Rebaked lighting
Reduced material count by a few
Fixed a few incorrect materials
Safety and Security fixes

5/02/19 (69.82mb)

Removed 33.78mb from the build(!!!) (Huge thanks to TCL!)
Fixed an incorrect texture on the light over the notice board #18 (Thanks @HugoZink!)
Disabled the live audio player temporarily

04/27/19 (103.6 mb)

Fixed inconsistent and broken Pickup Respawners
Fixed the Pillows in The Roost
Fixed UV1 on meshes above main bar

04/24/19 (103.6 mb)

Possibly removed extranious refrences to unused objects
Adjusted texture sizes
Adjusted texture filtering to be trilinear on almost all textures
Removed extrainous material slots on some meshes
Removed extrainous some extrainous geometry
Rebaked Occlusion
Power Water and Kirito are back from holiday
(Also did lots of Quest work)

04/10/19 (104.98 mb)

Reduced overdraw on walls in main bar
Reduced overdraw on winndows in main bar
Updated the calendar (10 days late!)
Removed a few extranious materials

03/27/19 (106.89 mb)

Fixed the milky water (Thanks Meme_man!)
Fixed the Desaturated White Russian (Thanks Meme_man!)
Added photos from Saint Patrick's at The Pug 2019
Fixed texture on the lamp in The Roost

03/21/19 (106.78 mb)

Updated lamp and fixture emissive maps
Rebaked lighting
Rebaked Occlusion
Added bells to each floor

03/19/19 (106.52 mb)

Adjusted lighting down a bit in the main bar
Tweaked The Bucket
Adjusted PPV transition falloff for the kitchen coolers
Adjusted compression on some textures
Fixed the high gloss on the banners

03/18/19 (106.26 mb)

Took down the decorations
Updated lighting in many areas
Improved lightmap UVs on booth backs
Improved overdraw in the bathrooms

03/16/19 (108.52 mb) (444)

Saint Patrick's at The Pug 2019!
Thanks to Polopo for the help getting the Leprachaun avatar optimized!
Thanks to Zarniwoop and ShutUpSargent for suggesting hidden Leprecauns!

03/05/19 (105.32 mb)

Fixed the seat toggle for the chairs near the corner booth on the first floor (Thanks Zarniwoop!)
Turned off dithering in the material shaders. Dithering is still applied by the Post Processing Stack (Thanks Poplopo, HugoZink!)
Fixed an incompatibility between the liquid shader and an upcoming patch (Thanks TCL!)
Fixed the fireplace chimney being visible through the window in The Roost
Fixed the visible floating square in the sky
Fixed a gap behind the fireplace
Updated Calendar (5 days late!)
Put up Saint Patrick’s Day promotional decorations

02/21/19 (106.01 mb)

Fixed stage mic not respawning
Fixed misaligned collider near the lamp in The Roost
Rebaked lighting in the main hallway
Updated Patron flyers

02/15/19 (105.61 mb)

Stoves are ready for Udon 🍜
Updated Patron flyers

01/31/19 (104.5 mb)

Fixed the Night View bar lock
Updated Lightmaps on various objects in The Roost
Updated wood grain on various objects in The Roost
Fixed Z-Fighting on the table behind the couch in The Roost
Fixed Z-Fighting on Night View bar
Fixed low resolution texture on the sword in The Roost
Updated Light Probes in the main bar to be more consistent
Updated calendar (one day early!)
SDK Bump: VRCSDK-2018.12.19.17.03_Public

01/16/19 (103mb)

Rebaked Lighting
Updated Patron flyers
Updated Specular proxy objects

01/03/19 (103mb)

Adjustments to the live audio setup
Adjustments to textures and meshes to reduces the download size a bit
Fixes to reflection probes (Thanks Zarniwoop!)
Fixed house music

01/02/19 (107mb)

Took down holiday decorations
Adjusted bloom a bit
Fixed texture on the solo stool in The Roost
Adjusted collision on the stage edge

12/31/18 (113mb)

New Years decorations and Music

(Thanks CubedParadox for lending me your Record Player!)
(Radio Soulwax - Under the Covers)


Clamped bloom more aggressively
Drywall is now uniformly scaled and oriented
Updated the calendar
Misc fixes

12/28/18 (107mb)

Fixed the missing colliders in the women's bathroom (Thanks @SplitScream#8411!)
Removed the invisible collider in the first-floor hallway (Thanks @Sheppard#1998!)
Fixed missing collider along the doorway to the back stairs (Thanks @Sheppard#1998!)
Implemented a new ""lightmap method"" on some objects. Notably the bar in Night View.
Fixed hole in the ceiling near the kitchen door
Fixed floating baseboard in the back staircase
Improved lightmaps on various objects
Added a new lamp!
Padded the seat backs on the chairs in Night View (The Roost will follow later)
Rounded the Globes on the tables in Night View
Brightened up Night View a bit
Repainted the ceiling in Night View
Updated stools in the main bar so they can hold pickups
Misc fixes
Switched to Post Processing Stack v2

Flashing light from broken geometry should no longer happen
Bloom is clamped to prevent malicious emission values
Testing Post Processing Volumes with the kitchen coolers
Testing auto-exposure



12/24/18 (106mb)

Finished the refactor on the corner booth and near by booths
Fixed wood grain on the trim of the lower landing of the stairs to Night View
Added wood trim along the red wall on the first run of stairs to Night View
Added baseboard to wall near the main bar mirror
Added tiles to the walls that were missing them in the bathrooms
Improved light maps on many meshes
Cleaned up the geometry of some meshes

12/23/18 (108mb)

The Yule Goat has risen! 🐐
Fixed the floor from the hallway sticking into the womens bathroom

12/20/18 (105mb)

Refactored parts of the booths around the corner booth on the first floor to fix lightmapping and lower draw calls
Lowered the intensity of the specular light proxy objects (Thanks Korro Bravin!)
Adjusted the live stream audio sources
Fixed hazy materials on some objects

12/19/18 (105mb)

Tweaked reflection probles
Tweaked specular on many objects
Improved normals on beer taps
Improved normals on trash taps
Improved fake mirrors in light of the new specular profile
Improved light mapping on various small objects
Reduced download size a bit

12/18/18 (109mb)

Added a few more holiday decorations
Added a new sculpture to The Roost (Thanks Poplopo!)
Fixed dark table tops
Fixed lightmapping throughout The Pug
Improved specular response throughout The Pug. - Still needs tweaking (Thanks SeraRealm!)

12/12/18 (93mb)

Rebaked lightmap - fixed many issues/errors from the 5.6-2017.4 upgrade
Restored holiday banners

12/11/18 (91mb)

Fixed regressions (Reapplied the last update)
Updated patron flyers
Added a few holiday decorations to Night View and The Roost
Added new textures for the red phone
SDK Bump: 2018.12.04.10.25
Engine Bump: 2017.4.15f1

12/07/18 (92mb) - Final FIVE SIX update

Updated the Calendar (7 days late!)
Updated Patron flyers
Fixed the floor in Night View (Thanks laugexd!) [ Issue #10 ]
Started decorating for the holidays
Rebaked Occlusion

11/15/18

Unity 2017 shenanigans.

11/09/18 (96mb)

Adjusted a broken trigger in the Mr. Whiskers Puzzle to hopefully fix it (Thanks Naelstof)
Tweaked live stream playback component
Added a toggle to disable interaction with seats in The Roost
Adjusted some seats in The Roost so they are a bit easier to interact with for desktop users
Fixed missing seats on side couch in The Roost
Updated various materials
Rebaked Occlusion

11/08/18 (97mb)

Added a toggle to disable interaction with seats in the main bar - it is in the back room
Fixed ObjectRespawners on some more objects, including the Pillows in The Roost - Caused by this bug: http://bit.ly/vrc-teleportTo-hierarchy
Fixed missing Corner Booth seat stations
Fixed offset seat stations on stools near the bar mirror (Thanks Zarniwoop!)
Updated some parts of the Mr. Whiskers puzzle to use Custom Triggers
Shined up the booth table legs
Reduced drawcalls by 1 or 2

11/06/18 (98mb)

Fixed ObjectRespawners - Caused by this bug: http://bit.ly/vrc-teleportTo-hierarchy
Fixed Mr. Whiskers Puzzle - Caused by this bug: http://bit.ly/vrc-teleportTo-hierarchy
Fixed unsightly seams on the new booth models
Fixed a missing booth barrier in the Bar mirror (thanks Sheppard#1998!)
Fixed some lightmap issues on the Night View Bar (more need fixing)
Fixed some lightmap issues with the stools on the first floor
Fixed light leaks near the ceiling on the stairs to The Roost
Fixed light leak from the back staircase into the back hallway
Fixed tall baseboard along the tall windows in Night View - you can't walk on it anymore.
Fixed the taps, they were still hooked up to the Halloween kegs
Fixed disappearing White Russian liquid
Improved framing on Halloween 2018 Photo
Removed a draw call or two in Night View
Updated drink menu models
Updated textures the on the ceiling vents
Updated Patron flyers
Rebaked Occlusion
SDK Bump: 2018.11.05.17.42

11/02/18 (102mb)

Reduced a couple more draw calls throughout the map
Rebuilt the meshes for the booths near the mirror on the first floor
Updated all of the drywall materials
Improved colliders near corner stool on the first floor
Improved colliders along the base of the tall windows in Night View
Improved lightmap on the stairs to The Roost (Dark upper border should be gone)
Fixed missing light near the back exit
Added a group photo from Halloween at The Pug 2018 to the wall on the stairs
Readded tags: bar, stage, hangout, social, classic

11/01/18 - 2 (97mb)

Reduced draw calls on various objects around the main bar a bit.
Updated fishbowl water - hopefully fixing the flickering.
Rebaked occlusion
Removed a few stray Halloween remnants
Added tags: bar, stage, hangout, social, classic
SDK Bump: 2018.10.31.10.45

11/01/18 (96mb)

Removed Halloween Decor
Updated calendar
Updated patron flyers
Trigger adjustments to the Mr. Whiskers puzzle

10/27/18 (102mb)

🎃 Halloween at The Pug 2018

10/17/18 (90mb)

Adjusted liquid shaders some more
Fixed the missing colors from the red/blue/green pints (Thanks Hystericmikey!)
Removed additional superflus objects and materials

10/16/18 (90mb)

Reduced draw calls by 0-4 in main bar area and stage area
Fixed sorting issues with liquid and glass (Hopefully)
Fixed the shifted fireplace light
Removed some un-needed disabled objects

10/15/18 (90mb)

Updated Patron flyers
Updated liquid shader
Updated the glass material on the clocks

10/10/18 (91mb)

Adjusted the liquid shader (hopefully the weird refraction rendering is gone)
Removed a draw call on the rose in The Roost
Fixed a seat on the fireplace couch in The Roost that had a very long interaction distance (Thanks Pan Diman!)
Fixed the oversized interaction boxes on the fireplace couch in The Roost
Adjusted the glass on the mirrors
Removed a draw call on the clocks

10/09/18 (90mb)

Fixed the performance issue with the new glass shader (Thanks CubedParadox!)
Added table tents

10/08/18

New glass shader (Thanks CubedParadox!)
Banners for the Halloween Party are up

10/03/18 (350) (89mb)

Updated the SDK - VRCSDK-2018.10.02.10.29_Public
Updated the Calendar (3 days late)
Fixed some occlusion errors
Patron poster updates

09/20/18 (88mb)

Fixed a Patron poster
Fixed eject buttons in the bar (Thanks Meme Man!)
Fixed a phone receiver that was made of cloth
Fixed some reflection probe placements
Rebaked lighting

09/19/18 (88mb)

Adjusted some light probe placements
Updated/fixed respawn timers on a few objects
Updated Patron posters
Deleted some extranious objects that I found hiding in nooks and crannies

09/18/18 (87mb)

Cleaned the darkness off the Orchid on the welcome desk
Worked around a bug with onTimer triggers (hopefully)
Fixed weird geometry on the main staircase
Improved wood grain direction on the main staircase
Improved lightmap on walls on the main staircase
Improved wood grain direction on the sleeping platform in The Roost
Rounded up the plates
Patched some holes in The Roost ceiling (again)
Solidified the top of the stools in Night View (Thanks Poplopo!)
Removed a weird onInteract trigger near the entrance (Thanks Meme Man!)
Removed some errant animations on the Yellow Spotlight on the Stage
Updated Patron posters
Minor fixes

09/13/18 (86mb)

Updated patron posters
Added steamer pans
Minor fixes

09/9/18 (86mb)

Fixed weirdly shiny materials on main staircase and stage
Fixed leaky faucets in the men's bathroom
Fixed the issue with the sinks being missing in the bathroom mirrors
Added a lot more brushed steel in the kitchen

09/7/18 (85mb)

Loaded up a new Calendar (7 days late)
Sanded down the bathroom sinks to round them out, then re-polished them
Fixed the issue with the Whiskey being a vampire (Thanks Exiled!)
Fixed the issue with teleporting booth seats (Thanks Jordo!) [ Issue #8 ]
Fixed the issue with the missing wall collider near the bar mirror (Thanks Misaki and others!)
Updated TheArchitects poster - He does more than homeworlds now!

08/31/18 (89mb)

Added a new bottle label for Presence
Updates to stage controls for performers
Tweaked the tap triggers to be less square
Rebaked occlusion to fix up the stage
Tweaked toilet sounds so they should be audible again

08/30/18 (89mb)

Moved stage speakers off the stage and added monitors fulfilling Issue #3
Fixed the missing animation on the toilet water
Updated toilet and tap timers to (hopefully) work around a current bug with timers
Reduced range of toilet flush sound (hopefully)
Updated models and materials on stage equipment to reduce the draw calls a bit
Updated the live performer controls
Updated the meshes/materials on ceiling lights in Night View to reduce the draw calls a bit

08/29/18 (89mb)

Peformed some plumbing; The toilets may or may not ""work"" now
Hooked up the beer taps in the Night View bar
Added timers to the beer taps so they turn off after being left on - they were wasting so much beer!
Took down the open sign for Night View (it's always open these days!)
Adjusted the basement door so it is more logical when open, and when being handled
Adjusted lights near the main staircase and first-floor hallway entrance
Fixed the issue causing a teleport if you walked under the back stairs on the first floor (Thanks Exiled!)
Fixed texture tiling on toilets
Fixed more descended canister lights
Adjusted draw calls a tiny bit in the main bar area
Adjusted they way some sound effects play
Restored the MIP Maps on the posters

This exists now: The Great Pug - Steam Group
08/28/18 (91mb)

Made refinements to the Night View shelving meshes
Made refinements to the lightmap on the Night View bar
Made the main staircase a bit brighter at the first-floor landing (Thanks Exiled!)
Fixed the light canisters that were descending on the main staircase (Thanks Garret!)
Updated the Security Colliders; they should be a little more forgiving now (Thanks Korro!)

08/24/18 (91mb)

Fixed the flickering doors in the buffet in The Roost
Updated the textures on the clock and banners
Tweaked some baked lights
Made some minor draw call optimizations
Other minor tweaks

08/23/18 (91mb)

Removed some legacy VRC Chair scripts I found hiding around the map
Minor tweaks

08/22/18 (91mb)

That missing door frame returned home and apologized. It just needed some time away from all the people.
Re-jiggered texture compression on some things
Re-jiggered audio compression on some things
Made adjustments to the lighting
Further adjusted the Post Processing stack
A very Rigid Body was found in The Roost and removed.

08/21/18 (100mb)

Bloomified the fire in The Roost for a more cozy glow
Added a new menu model – more to come down the road here
Material adjustments
More draw call optimizations in the main bar area; 5-10 draw calls depending on the direction you are looking
Patched up the hole in the ceiling near the kitchen door

08/20/18 (97mb)

Fixed Basement occlusion issues
Fixed collider sticking into the main bar from the basement
More draw call optimizations in the main bar area; 1-12 draw calls depending on the direction you are looking
Adjusted the post-processing stack
Minor collider adjustments
The Devil Bucket should be easier to pick up now

08/18/18 (99mb)

Refactored the basement meshes

08/17/18 (99mb)

Fixed collider above the table behind the couch in The Roost
The VRCHAT ARCHIVES advert has gotten a bit dirty in the past year and a half. (Thanks Zarniwoop!)
Material updates
More materials are now using the Dithering Shader

08/16/18 (100mb)

Fixes/Adjustments for the live show audio
Adjustments to shadow casters in The Roost and on the stage
At least 1 draw call removed.

08/12/18

Fixed the missing/shifted colliders on the upstairs bar
Fixed the missing colliders on the downstairs bar cooler
Adjusted the Dithering Shader a bit

08/11/18

Updated the Dithering Shader to v.1.calm.0.0.pseudology.1534016371.7
Many wood materials updated
Bloom is back to its previous level
Light Probes should no longer bleed out of the cooler into the back stairway as easily
Fixed weird light fixture placements throughout the first floor
LOD adjustments for various signs
Fire extinguishers should no longer be inside the wall
Bar Two has slightly better UV unwrapping now
Reflection Probes adjusted down
Removed 3 draw calls from the bar cooler.

08/08/18

THE BELL WORKS AGAIN!
Fixed issues with the beta Dithering Shader by Xiexe (Thanks TCL!)
Fixed the weird sheen on the First Dollar plaque (Thanks Exiled!)
Shaved 1-2 draw calls off of a couple items
Fixed the weird ceilings in The Roost staircase
Fixed light baking issues in a few places
Fixed the ghost chairs by the Corner Booth on the first floor
Fixed the floor material in the bathrooms
Removed references to non-existent chair placement scripts by CubedParadox (Thanks Cubed!)

08/07/18

Calendar Update (7 days late)
Many shaders changed to a beta version of a Dithering Shader by Xiexe (Thanks Xiexe!)
Shaved 1-2 draw calls off stage props

07/27/18

Additions to the live streaming audio support
Minor fixes

07/19/18

SDK Bump - 2018.06.21.13.02
Added experimental live streaming audio support
Made some Minor LOD tweaks

07/03/18 (314)

LOD Tweaks
Reflection Probe fixes
Metalic tables fixed

07/02/18 (312)

Collider blocking stairs fixed

06/29/18 (311)

Bell should be fixed
Lightmap fixes
Gap below short wall at the top of the roost stairs fixed
New Brushed Metal material
Minor optimizations
LOD setup on many items, we will see how that goes.
Collision changes
Reflection probe adjustments

06/29/18 (310) (110mb)

Implemented minor draw call optimizations
Updated lightmaps on back hallway, no more light leaks near the exit sign
Greatly reduced lightmap artifacts on main stairs leading to Night View
Overall lightmap filesize dropped

06/28/18 (308) (116mb)

Implemented additional minor draw call optimizations
Updated the wine bottle labels
Reflection probe resolution changes to reduce download size

06/22/18 (307) (122mb)

Texture resolution increases
Texture compresion changes
Reduced overall download size by 6mb

06/08/18 (306)

Made draw call optimizations
Added missing baseboards in Night View
Reflection Probe Adjustments
Stools have shadows again!
Boxes under the Night View bar are now walkthrough (thanks Meme Man)

06/07/18 (305)

Updated the appearance of the lampshades
Made some minor draw call optimizations
Modified lightmap settings

06/05/18 (303)

Made some minor draw call optimizations
Updated some meshes to have better geometry and normals

06/04/18 (302)

Made some minor draw call optimizations
Fixed a missing collider near the bar mirror

06/03/18 (300)

Made a few draw calls optimizations
Changed the near clipping plane distance on the reference camera (the far clip was not changed)
Rebaked occlusion
Updated the meshes for the sink, Roost shelves, stair railings, booth base/backing, and other minor meshes
Made minor Trigger broadcast type adjustments
Re-painted the wall near the main bar
Added a new Patron flyer

Quick Fix (301)

Fixed some mis-aligned colliders that were out of place.

06/01/18 (298)

Made some draw call optimizations!
Made changes to the Object Respawing behavior to attempt to address lag when a new user joins the world.
Mixed Lights now are forced off when you are not in view of them. This is being done as a precaution because Occlusion Culling may not have been doing the job in all cases.
Rebuilt the shelves under the bar
Updated the Calendar
Tweaked the lighting
Changed the material on the dynamic towel
Fixed a gap in the ceiling in The Roost
Tweaks to various trigger broadcast types
Made some chair upgrades

05/24/18

House Music placement/falloff changes
Stage Lighting Updates
New Stage Lighting Control Board
Addition of Dynamic Event Posters for Spork of Love
Minor Updates to the Event Posters for MckMuze
Bulletin Board Updates

05/10/18

Administration controls added to lock and unlock the stage in a basic manner. (Issue #2 - MckMuze)

More work still needs to be done to fully satisfy the bounty to my satisfaction.


Stage lighting has been improved
Dynamic lights in Night View have been improved
Mesh updates for the stage - Rounder!
Material updates to the sage, Night View Floor, and Bars
This change log had dome embarrassing typoss

05/07/18

Reflection Probe Updates
SDK Bump - 2018.05.01.20.38
Flyers Added

04/24/18

New Bulletin boards

04/19/18

Bathrooms should be back to normal

04/14/18

You should no longer stick to the walls when using the main stairway or the stairs in The Roost
Mckmuze setlist lighting has been fixed
Missing lightmap on painting has been found and reapplied
New decorations in Night View and The Roost
New furniture in The Roost
Hopefully fixed some lag issues related to triggers

04/11/18

Weird, the basement door kept staying opened. - Fixed

04/08/18

Coasters added around The Pug to keep the finish on the wood nice
Eggs removed

03/30/18

Cleaned up straggling Saint Patrick's Day decorations
Improved Resolution on wireframe posters
Possible fix for the flickering hub portal
Eggs.

03/19/18

Removed Saint Patrick's Day decorations

03/17/18

Saint Patrick's at The Pug - 2018
UV fixes for the shelves in the wall behind the main bar
New wall art
Fixes to the glass roof in The Roost
Duplicated mesh fixes
Security Improvements

03/13/18

Calendar Update
Moon Fix

02/24/18

St. Patrick’s Day 2018 - Promotional Table Tents
Enhanced appearance of Night View stage spotlights and floor lights
Multiple spawns
Minor draw call optimizations
Lightmap fixes
Rose added in The Roost - Thanks Poplopo!
New shelves in Night View
Improved the readability of the bulletin board flyers
Fixed typo on bulletin board
Humoungously improved the wall near the back storage room
Security Improvements
SDK Bump

Thanks for the help testing Zarniwoop, Poplopo, and Zircronswift!
02/06/18

Removed birthday decorations
Removed portal to the prototype

02/02/18

Birthday decorations
Temporary portal to the prototype

02/01/18

Added a delightful painting above the fireplace. It was painted by Dicidius. Thanks, Dicidius!
A matching sword is on display
New shelf along the back wall in The Roost
Telephone ring volume lowered a bit
Calendar updated
Moon and city lights properly restored for real this time
Bulletin board updated

01/05/18 (268)

Added security colliders to prevent trolling from outside the map.
SDK Bump

01/02/18

New Years Decorations Removed
Moon and city lights restored
Phone Ring distance adjusted - hopefully, the beds are usable now

12/31/17

Far Back Staircase/Fire Escape
Most objects should Respawn when left lying on the floor, in weird places or outside of the map.
Exterior Meshes
Woodgrain direction fixes
Mesh improvements on the bar
Ambient Lighting Tweaks
Bathroom Stalls now have handles and latches
Skybox uses fewer draw calls
Hole in Phone base fixed
Material(s) consolidated on the phone base
General draw call optimizations
Lighting tweaks
Christmas Decor Removed
Calendar Updated
Disc for 2018 added
Champagne for New Years
Decorations for New Years

12/22/17

Seat Fixes

12/14/17

SDK Bump - VRCSDK-2017.12.12.13.36_Public
Martini added (941101501153505281)
Occlusion Settings Reverted
Lightmap tweaks
Material fixes

12/12/17

Downstairs beer taps should work correctly now
Christmas Decoration Updates
Draw Call Optimizations
Light Probe Improvements
Mesh Updates on the Night View bar
Material Tweaks
Toilet seat fixes
Dining chairs should be easier for desktop users to use
Occlusion changes

12/01/17

Calendar Updated
Lightmap Tweaks
Christmas Decorations
Material Optimizations
Thanksgiving meal put in storage

11/22/17

The Roost is Open
Added Thanksgiving Food
Improved Night View Hall sign (Thanks Poplopo)
Adjusted audio volume falloff on sink taps
Updated Meshes in The Roost
Added fireplace in The Roost
Added seating area in front of the fireplace in The Roost
Added table and chairs in The Roost
Fixed Grain Direction on various objects
Updated Proximity Dance Club Portal

11/16/17 (200)

Lighting Tweaks
UV Fixes on the 2nd Floor floor
Mesh Updates on Booth's backs
UV Fixes on Booth bases
Increased Red Phone ring frequency

11/15/17

Adjustments to the way pickups reset, hopefully fixing them
Patched over Z-Fighting at the top of the stairs
""Un-Fixed"" the Devil Bucket
The Red Phone should now randomly ring
Audio played from the phones should be easier to hear now

11/14/17

First attempt at making pickups reset when idle in undesirable locations.

11/09/17

Calendar Added
Red Phone Added
Various Materials Improved
Lighting Tweaks
Lightmap Resolution Changes
Minor Fixes
VRCSDK Updated to 2017.10.26.17.36

11/07/17

Material Updates
Material Fixes
Minor Fixes

11/06/17

Material Updates
Bar Mesh Updates
More Face Weighted Normals
Minor Fixes

11/04/17

Small Ceiling Vents Added
Face Weighted Normals on various objects
Faucets in the bathrooms now work
Toilets have been scrubbed
Minor Fixes

10/30/17

Post Halloween Party restore

10/24/17

Reflectiopn probe fixes
Halloween prep

10/13/17

VRCSDK Update to 2017.10.04.13.58

10/12/17

Halloween Promotional signs put up
WebPanel disabled
Minor Fixes

09/22/17

Lighting Tweaks
Material Updates (Albedo Checks)
Minor Fixes

09/19/17

Lighting Updates
Minor Fixes

09/12/17

Lighting Updates
Birthday Cake Optimizations
Minor Fixes

09/11/17

Bathroom Collider Fixes
Martial Swaps
Major Light Probe Overhaul
Minor Fixes

09/09/17

Bathroom ceiling now reflects in the bathroom mirrors

09/05/17

Minor Fixes

09/01/17

Bottle Liquid Fixes
Bathroom Walls Fixed

08/31/17

Updated Materials
Optimization for Bathroom Mirrors
Minor Fixes

08/30/17

Five Six

08/17/17

Karaoke added for Karaoke night

08/10/17

Karaoke functionality testing

07/12/17

SnailLock testing

06/30/17

Fireworks added
Minor fixes

06/22/17

Lighting updates
Trigger updates

06/21/17

Minor changes

06/20/17

Dance lights / floor lights added to Night View
Stage lights updates
Overall nightview light added

06/19/17

Sleeping roost updates
Minor fixes

06/14/17

Rounded edges on the lower bar
Added door latches

06/13/17

Refinements to the stairs
Fixes to the Night View floor

06/10/17

Basement optimizations
Sleeping Roost started

06/01/17

Additions for Contact
56 - Stage Lights
Reload button added for the YouTube panel

05/23/17

Texture size optimizations
Floor UV fixes
Adjusted seats for desktop users
Light probes reduced in overall count

05/19/17

Event board typo fixed

05/18/17

Collider fixes
Combined various meshes

05/17/17

Optimized Smörgåstårta
Added Smörgåstårta under glass to the back room
Texture fixes
Mesh fixes

05/16/17

Occlusion fixes
Baseboards added in various places
Coffee added
A large number of mesh colliders have been replaced with box colliders

05/15/17

Doorbell updates
Occlusion changes
Backroom additions
Security camera added and removed

05/12/17

Doorbell added
Cans of fish added
Lighting updates
Smörgåstårta.

04/21/17

White Russians
Light probe fixes
Booth fixes
Lighting changes
Tapster locks added
Switched to using Euan's video player

04/19/17

Eggs removed
Sink handles added
Event board updates

04/15/17

Minor fixes

04/14/17

Event board added
Easter eggs added for easter
Lighting changes
Wine bottles added
Wine glasses added

04/13/17

YouTube panels trigger fix
Stage Prop updates
Trigger updates
MckMuze setlist added
Stage updates
Occlusion fixes in the basement

04/12/17

Lightmap resolution adjustments

04/10/17

Corner booth fixes
Booth seat fixes
Upstairs gate fixes
Lighting fixes
Basement Door added
Basement wall issues fixed
Texture storage size optimizations

04/07/17

Basement
New chairs
YouTube panel fixes
Minor fixes

04/06/17

Mckmuze decor updates
Bar Two - Tigger and button fixes

04/05/17

Bar Two - Improvements
Trigger updates (groan)
Podium added to Night View

04/04/17

Mirror fixes
Metal material updates
Bar top fixes

04/03/17

Bar Two - shelves added
Lighting and light probe changes
Upstairs security changes

04/02/17

Video screen fixes
Brighter lighting

03/31/17

Second bar added in Night View
Stage lights added
New back panel controls

03/30/17

Foot pedals and amp added to the stage

03/29/17

YouTube panels added to the stage
Minor fixes

03/24/17

Corner booth implemented (it's too small!)
PhysSound added
Music fixed

03/23/17

Materials updated
LOD enabled on booth backs

03/22/17

Minor fixes

03/21/17

Decorations taken down
Minor mesh updates
Taps now dispense normal beer

03/17/17

St Patrick's Day Decorations
Taps are now interactable
Speakers
Party Music Player - Thanks Cubedparadox for the youtube playlist sync!

03/13/17

Trigger fixes (grumble)

03/12/17

bathroom Updates
Material atlasing
Security updates

03/10/17

Optimized Meshes
Added St Patrick's Day table tents
Minor Fixes

03/09/17

Added lights to the St Patrick's Day posters
Updated Materials

03/08/17

St Patrick's Day Posters added
Bar height adjusted
Measuring sticks added

03/07/17

Added a Clock
Updated those fancy liquid shaders
Minor fixes

03/06/17

Added fancy liquid shaders
Minor fixes

03/03/17

Added Your Favorite Beer Neon Sign
Updated Materials
Added lights above the bar top
Minor fixes

02/27/17

Implemented Security for the Bar
Added Your Favorite Beer
Optimized Meshes
Optimized Objects
Minor Fixes

02/24/17

Added the back room and it's keypad
Minor fixes

02/23/17

Added MckMuze signs
Minor fixes

02/21/17

Fixed the bar mirror
Optimized Geometry
Optimized Materials
Optimized Occlusion
Minor fixes

02/20/17

Smaller Light Maps
Lighting Changes
Added Gates to the bar
Added Staff Only Sign
Added more canister lights in the Ceiling
Added photo of Q sleeping

02/19/17

Fish Bowl Added
Light bake fixes

02/17/17

Posted my Liquor License
Mesh optimizations

02/01/17

Initial Release
VRCSDK version 2016.12.01.18.02

",12
jimboy3100/jimboy3100.github.io,JavaScript,"Legend Mod
Mod for Agar.io multiplayer action browser game.

Author: jimboy3100@hotmail.com
Website: www.legendmod.ml
<iframe width=""800"" height=""452"" src=""https://www.youtube.com/embed/CnIfNSpCf70?rel=0"" frameborder=""0"" allow=""autoplay; encrypted-media"" allowfullscreen></iframe>
Legend mod GitHub Library
Feature Highlights

10% extra zoom-out (see enemies from further)
Fast feed shortcut (hit viruses and feed team mates faster)
Double split shortcut
Triple split shortcut (for tricksplits)
Minimap (find your team mates, avoid getting cornered etc)
15 configurable shortcut keys to send messages to your team quickly

Other Features

Updates automatically
Unlimited FPS unlocked (quicker than Vanilla)
Old Skins
Animated Skins
UserScripts Manager (URL or pasted)
Language Packs
Direct PARTY / FFA / EXP / TEAM server by using tokens/sips or connector
Search engine for player name / clan / tag / leaderboard / ip / token
Integrated Chat, minimap and teamboard. Chat rooms per server/per team password(or public)
New Template / skins / animations / zoom / respawn / helpers / hud controls and many extras
Themes for quite all textures and map (Basic / Menu / Hud / chat / minimap / graphics and cursor)
Banners for many clans (Email me your symbol and weblink for updates)
60++ Macros / Events / Hotkeys (Script does many calculations)
Tools for quests / youtubers / timers / coin auto digger/youtube video player
Send message pictures, videos and also various message commands directly to teammate's script
Change various textures, add photos on huds and clan's pictures and url links
Dying Light Expansion
Discord webhook handler's for sending IP, and many more...

Legend Mod libraries
Github,
Greasyfiork,
Agarioscripts Chrome Extension
Installation

Install Tampermonkey browser extension on Chrome , Opera
Install Legend Express script here  

Screenshots
Welcome Screen - Copy Token, Leaderboard, IP

Searching

Youtube

Features

Big Names (visible to legend/ogario users only)

Banners and website anchors for many clans

Old Skins

Legend mod is based on many scripts (ogario, kitty, turtle clan scripts and others that can be found on greasyfork website).
",5
unknown321/mgsv_nuke_watcher,HTML,"https://unknown321.github.io/mgsv_nuke_watcher
A sample app using mgsv-emulator (https://github.com/unknown321/mgsv_emulator ).
",2
99designs/gqlgen,Go,"gqlgen  
What is gqlgen?
gqlgen is a Go library for building GraphQL servers without any fuss. gqlgen is:

Schema first — Define your API using the GraphQL Schema Definition Language.
Type safe — You should never see map[string]interface{} here.
Codegen — Let us generate the boring bits, so you can build your app quickly.

Feature Comparison
Getting Started
First work your way through the Getting Started tutorial.
If you can't find what your looking for, look at our examples for example usage of gqlgen.
Reporting Issues
If you think you've found a bug, or something isn't behaving the way you think it should, please raise an issue on GitHub.
Contributing
Read our Contribution Guidelines for information on how you can help out gqlgen.
Other Resources

Christopher Biscardi @ Gophercon UK 2018
Introducing gqlgen: a GraphQL Server Generator for Go
GraphQL workshop for Golang developers by Iván Corrales Solera

",2753
evsinev/grpc-java-long-polling,Java,"gRPC long polling implementation






Many web servers (ex. nginx), load balancers do not yet support HTTP/2 upstream.
This project implemented both gRPC server and client with long polling via HTTP/1.1
Client example
ManagedChannel channel = LongPollingChannelBuilder.forTarget(""http://localhost:9096/test"").build();
GreeterGrpc.GreeterBlockingStub service = GreeterGrpc
        .newBlockingStub(channel)
        .withDeadlineAfter(5, TimeUnit.SECONDS);

HelloRequest request = HelloRequest.newBuilder().setName(""hello"").build();
HelloReply reply = service.sayHello(request);
Server example
LongPollingServer pollingServer = new LongPollingServer();

Server grpcServer = LongPollingServerBuilder.forPort(-1)
        .longPollingServer(pollingServer)
        .addService(new GreeterImpl())
        .build();
grpcServer.start();

ServerListener serverListener = pollingServer.waitForServerListener();

HelloWorldServer server = new HelloWorldServer(9096, new LongPollingDispatcherServlet(serverListener));
server.start();
",3
fbieberly/orbcomm_decoder,Python,"orbcomm_decoder
A software receiver for ORBCOMM satellite transmissions.
Description
This is a software receiver for decoding packets from ORBCOMM satellites. I don't know what all of the various packets are for, but for the ones I do, I attempt to decode the packet data.
I am writing this decoder as an instructional personal project. Hopefully it
can be used by others to learn about designing satellite communication
receivers.
If you want a more full-featured ORBCOMM receiver please check out:
https://www.coaa.co.uk/orbcommplotter.htm
http://f6cte.free.fr/index_anglais.htm
Dependencies
Should work with either Python 2.X or 3.X
I use pyrtlsdr to record the RF signal with an RTLSDR receiver.
NumPy and SciPy are used for signal processing.
PyEphem is used to calculate Az/El and doppler shift of the satellites.
pip install pyrtlsdr, numpy, scipy, pyephem
Getting started
Offline recording and decoding

First run the update_orbcomm_tle.py script to get the latest two-line elements for the orbcomm satellites.
Update latitude and longitude of your receiver in record_orbcomm.py
Record IQ data by running record_orbcomm.py
Run file_decoder.py to decode a single recording file (defaults to the first file in the /data folder)

EXAMPLE OUTPUT  
Filename: ./data/1552071892p6.mat
Timestamp: 1552071892.6
Data collected on: 2019-03-08 19:04:52.600117
Satellites in recording: orbcomm fm114
SDR Sample rate: 1228800.0 Hz
SDR Center frequency: 137500000.0 Hz
Satellite frequencies: 137287500.0, 137737500.0
Remaining frequency offset after doppler compensation: -141.0 Hz
Number of possible packets: 100

List of packets: (### indicates checksum failed)
### Unrecognized packet: AF0C3958A56A1A7A9BE0ED2B
Fill: Data: 19F1D8528E1EF9701DED 
Fill: Data: 5A8C1A5E354CE775C6A3 
Message: Total length: 2 Part: 0 Data: 001F05CE01C0721828 
Message: Total length: 2 Part: 1 Data: 507102000000000032 
Message: Total length: 3 Part: 0 Data: A241000129687B035E 
Message: Total length: 3 Part: 1 Data: 921E026637E0228277 
Message: Total length: 3 Part: 2 Data: A236830000000000F7 
Unrecognized packet: 0B01FD24CCCCCC204501CF3A
Sync: Code: 65A8F9 Sat ID: 2C 
Downlink_info: Total length: 3 Part: 0 Data: 27310750A005640094 
Downlink_info: Total length: 3 Part: 1 Data: 7D000BB89010130195 
Downlink_info: Total length: 3 Part: 2 Data: 1D011400000000003F 
Network: Total length: 1 Part: 0 Data: 7800010000000000E2 
Ephemeris: Sat ID: 2C Data: 98E3D5043B9BC34CDDF04C3CE66F98D5A307FB07E1D8 
	Current satellite time: 2019-03-08 19:04:53 Z
	Lat/Lon:  36.9359, -119.0865, Altitude: 1041.4 km
Unrecognized packet: 0B011C2AEDEEEE409B02A761
Unrecognized packet: 0B01641D76989944450169D9

Real-time recording and decoding
Not implemented yet.
DSP Training
In the dsp_training folder are a number of scripts that I used to help me understand the DSP that I needed to decode the ORBCOMM signals. The scripts are simulation only and help understand phase recovery, timing recovery, creating symbols from bits, mixing, filtering, etc.
Scripts
Scripts include:

sat_db.py: just a dictionary of orbcomm satellites I know are active
helpers.py: a file with useful helper functions
mat_file_explorer.py: a script that shows what is in a .mat file
plot_recording_waterfall.py: plots a waterfall of recordings
update_orbcomm_tle.py: downloads the latest orbcomm tles from celestrack.com
record_orbcomm.py: records orbcomm satellites when they are overhead with an RTLSDR
file_decoder.py: If you have .mat files in the data folder, this script will attempt to decode one

References
I used these two resources as my primary references.
http://mdkenny.customer.netspace.net.au/Orbcomm.pdf
http://www.decodesystems.com/orbcomm.html
Data format
In the data folder is a couple files of samples that I have recorded.
The files are .mat files. They can be opened with MATLAB or Python (using SciPy's loadmat function).
The files include metadata:

fc: center frequency
fs: sample rate
sats: a list of the names of the satellites overhead
tles: a list of lists of the tle lines for each satellite (in the order of the sats list)
timestamp: unix time of the start of the recording
samples: a numpy complex64 array of the samples
lat: the latitude of the receiver when the samples were recorded
lon: the longitude of the receiver when the samples were recorded
alt: the elevation of the receiver when the samples were recorded

Look at the mat_file_explorer.py script to see an example of how to access the metadata.
",2
GoogleChromeLabs/confluence,JavaScript,"Web API Confluence Dashboard 
A web service and UI for describing API confluence metrics. These metrics are
intended to capture the how browser vendors
are
stretching the elastic band of
the web platform.
Stretching is good: Browsers should be implementing new APIs to add value to
the platform.
Breaking is bad: Implementing too many new APIs before other browsers
catch up, or failing to remove APIs other browsers don't intend to ship causes
fragmentation.
The purpose of API Confluence Metrics is to capture the ways in which
different browsers risk breaking the elastic band.

Data for this project is collected using BrowserStack.
Table of Contents generated with DocToc

The Catalog

Querying the catalog

Examples




The Metrics

API Count

Definition
Rationale


Lone Omission

Definition
Rationale


Lone Removal

Definition
Rationale


Browser-Specific

Definition
Rationale




Contributing

Filing issues and contributing code
Running locally
Collecting data



The Catalog
The dashboard contains an API catalog that lists attributes/properties and
operations/methods exposed on constructor functions and their prototypes. The
catalog constitutes the raw data from which aggregate API confluence metrics
are computed. See CatalogDataCollection.md for
details on how the catalog is created.
Querying the catalog
The catalog supports structured queries. Some query atoms apply to all
cataloged browser releases, while others apply to the releases currently
in view (i.e., the releases currently shown as columns in the table of APIs).
Query atoms may be joined by whitespace, conjunction (and or &), or
disjunction (or or |), with parentheses to disambiguate as needed. Atoms are
one of the following:

(Not-)in-releases clause: A phrase of the form in:release or
notin:release where release is identified by case-insensitive
[release-name-prefix][release-version-prefix][os-name-prefix][os-version-prefix].
Any of these, except [release-name-prefix] may be empty. For example,
in:fir59 describes APIs shipped in all releases of Firefox 59 (that are
included in the catalog). These atoms apply to all releases.
Count-of-releases clause: A phrase of the form count:n where n is a
non-negative integer describes APIs that are shipped in exactly n releases
currently in view.
Keyword: An atom matching the regular expression [a-zA-Z0-9_#-]+
describes APIs that contain the atom by case-insensitive substring match.

Examples
window# count:1: APIs on intefaces with the case-insensitive window suffix
that are shipped in exactly one of the releases in view.
count:1 or count:2 or count:3 or count:4: On a view with showing four or fewer
releases, APIs that are shipped by at least one release in view.
in:chrome65 and notin:chrome66: APIs removed in Chrome 66.
The Metrics
API confluence metrics are a count of “APIs” that meet specific criteria with
respect to browser releases that include these “APIs”.
Definition: API: For the purposes of these metrics, an “API” is an
interface name + attribute or operation pair.
Definition: The Browser: Each API Confluence Metric is computed with
respect to some particular browser; this is what’s meant by The Browser. E.g.,
the “Lone Removal metric for Safari on 2016-09-01” describes APIs that
Safari once provided (but no longer does) that where the latest release of all
other browsers a year later contains the APIs; in this case Safari is The
Browser.
Definition: Grace Period: Most metrics are deliberately calculated with
respect to releases of browsers other than The Browser sometime in the
past. This avoids penalizing The Browser for making a change (i.e., shipping
or removing an API) when other browsers respond in kind. Currently, the Grace
Period used for all metrics that have one is one year. The ""a year later"" in
the above example refers to the Lone Removal Grace Period.
API Confluence metrics are API counts assessed for a particular browser at a
particular point in time. Most metrics are computed on every date that any
browser has a major release. Some metrics are only computed on dates when The
Browser has a major release.
API Count
Definition
The API Count metric contains three values; the total number of APIs provided
as of the latest browser release, the number of APIs removed (since the previous
release) and the number of APIs added (since the previous release). This metric
is computed on dates when The Browser has a major release.
Rationale
When browsers move too slowly, it holds back the platform. When browsers move
too quickly, they risk “leaving other browsers behind”. Steady growth is good;
wild variation is bad.
Lone Omission
Definition
The Lone Omission metric indicates the number of APIs that The Browser does
not provide provide for the duration of the Grace Period, but all other
browsers do provide throughout the Grace Period.
Rationale
Failing to ship an API that other major vendors provide requires web
developers to use special code paths to remain interoperable. Smaller values
are good; larger values are bad.
Lone Removal
Definition
The Lone Removal metric indicates the number of APIs removed from a The
Browser prior to the Grace Period, that have not been added back in the
latest relase following the Grace Period, and that are provided in all other
browsers in the latest relase following the Grace Period.
Rationale
Removing an API from only one browser risks breaking existing sites that
(reasonably) assume that all browsers support the API. Smaller values are
good; larger values are bad.
Browser-Specific
Definition
The Browser-Specific metric indicates the number of APIs that The Browser
provides for the duration of the Grace Period, but all other browsers do not
provide throughout the Grace Period.
Rationale
Adding APIs that are provided by only one browser makes that browser more and
more like its own platform (rather than an implementation of a common web
platform). Smaller values are good; larger values are bad.
Contributing
Want to contribute to Web API Confluence? Great!
Filing issues and contributing code
Please use GitHub’s issue tracker and pull request features.
Running locally


Clone this repository.


Install: npm install


Launch the local server:


mkdir -p data/json
Then, either:

Copy the latest data:

cd data/json
curl https://storage.googleapis.com/web-api-confluence-data-cache/latest/json/org.chromium.apis.web.ApiCountData.json > org.chromium.apis.web.ApiCountData.json
curl https://storage.googleapis.com/web-api-confluence-data-cache/latest/json/org.chromium.apis.web.ReleaseWebInterfaceJunction.json > org.chromium.apis.web.ReleaseWebInterfaceJunction.json
curl https://storage.googleapis.com/web-api-confluence-data-cache/latest/json/org.chromium.apis.web.BrowserMetricData.json > org.chromium.apis.web.BrowserMetricData.json
curl https://storage.googleapis.com/web-api-confluence-data-cache/latest/json/org.chromium.apis.web.WebInterface.json > org.chromium.apis.web.WebInterface.json
curl https://storage.googleapis.com/web-api-confluence-data-cache/latest/json/org.chromium.apis.web.Release.json > org.chromium.apis.web.Release.json
cd ../..
or

Collect the data yourself.

Finally, use npm run serve to launch a local instance of the service. This
will load local data, which can take up to a minute to be ready to serve.

Hack away! npm run serve uses webpack --watch to observe local
changes. Making changes to server code will require a service restart, but
client-side changes will be reflected soon after they are saved.

Collecting data
NOTE: The current data collection process requires a
BrowserStack account, two separate git clones, and a whole lot of RAM. We hope to streamline and simplify this process
soon. If you have all the prerequisites, read on…


Clone mdittmer/web-apis and follow
the
data collection instructions for
historical data collection using BrowserStack.


Create /path/to/confluence/data/object-graph and copy
/path/to/web-apis/data/og/*.json into it.


Create /path/to/confluence/data/json and run
./scripts/og_to_confluence.sh to derive confluence data from the object
graphs.


To run the service locally based on your generated data invoke node main/serve.js ""LOCAL"" ""DEV"". If you want live reloading of client code,
change the parameters passed to main/serve.js in scripts/serve.sh and
start webpack alongside the service with `npm run serve.


Caveat: In order to serve the data you collect, you must ensure that a { <browser name}: { <browser version prefix>: <release date> } } for every
version you have imported appears in data/version_history.json.
",63
edufonseca/icassp19,Python,"Learning Sound Event Classifiers from Web Audio with Noisy Labels
This repository contains the code corresponding to the following ICASSP 2019 paper. If you use this code or part of it, please cite:

Eduardo Fonseca, Manoj Plakal, Daniel P. W. Ellis, Frederic Font, Xavier Favory, Xavier Serra, ""Learning Sound Event Classifiers from Web Audio with Noisy Labels"", In proceedings of ICASSP 2019, Brighton, UK

The framework comprises all the basic stages: feature extraction, training, inference and evaluation. After loading the FSDnoisy18k dataset, log-mel energies are computed and a CNN baseline is trained and evaluated. The code also allows to test four noise-robust loss functions. Please check our paper for more details. The system is implemented in Keras and TensorFlow.
The FSDnoisy18k dataset described in our ICASSP 2019 paper is available through Zenodo from its companion site: http://www.eduardofonseca.net/FSDnoisy18k/.
Dependencies
This framework is tested on Ubuntu 17.10 using a conda environment. To duplicate the conda environment:
conda create --name <envname> --file requirements.txt
Directories and files
config/ includes a *.yaml file with the parameters for the experiment
logs/ folder where to include output files per experiment
main.py is the main script
data.py contains the data generators
feat_extract.py contains feature extraction code
architectures.py contains the architecture for the baseline system
utils.py some basic utilities
eval.py evaluation code
losses.py definition of several loss functions
Usage
(0) Download the dataset:
Download FSDnoisy18k from Zenodo through the dataset companion site, unzip it and locate it in a given directory.
(1) Edit config/*.yaml file:
The goal is to define the parameters of the experiment. The file is structured with self-descriptive sections. The most important parameters are:
ctrl.dataset_path: path where the dataset is located, eg, /data/FSDnoisy18k/.
ctrl.train_data: define the subset of training data to consider in the experiment. To be decided among: ['all', 'noisy', 'noisy_small', 'clean'] (see paper)
loss.q_loss: this is an example of a hyper-parameter of a loss function, according to the paper. For example, q_loss corresponds to q in equation (3) of the paper and reed_beta corresponds to beta in equation (2).
loss.type: defines the loss function. To be decided among:

CCE: categorical_crossentropy aka cross entropy loss
lq_loss: L_q loss
CCE_max: CCE loss & discard loss values using maximum-based threshold
CCE_outlier: CCE loss & discard loss values using outlier-based threshold
bootstrapping: L_soft loss
lq_loss_origin: L_q loss applied selectively based on data origin*
CCE_max_origin: CCE_max applied selectively based on data origin*
CCE_outlier_origin: CCE_outlier applied selectively based on data origin*
bootstrapping_origin: L_soft loss applied selectively based on data origin*

*The selective application of the loss functions makes sense when training with the entire train set (that is, considering clean and noisy data), ie ctrl.train_data: all  (see paper).
The rest of the parameters should be rather intuitive.
(2) Execute the code by:

activating the conda env
run, for instance: CUDA_VISIBLE_DEVICES=0 KERAS_BACKEND=tensorflow python main.py -p config/params.yaml &> logs/output_file.out

In the first run, log-mel features are extracted and saved. In the following times, the code detects that there is a feature folder. It only checks the folder; not the content. If some feature extraction parameters are changed, the program won’t know it.
(3) See results:
You can check the logs/*.out. Results are shown in a table (you can search for the string ACCURACY - MICRO and it will take you to them).
Reproducing the baseline
(1) Edit config/*.yaml file

ctrl.train_data: all # (or any other train subset)
loss.type: CCE # this is standard cross entropy loss

(2) Execute the code.
Baseline system details
Incoming audio is transformed to 96-band, log-mel spectrogram as input representation.
To deal with the variable-length clips, we use time-frequency patches of 2s (which is equivalent to 100 frames of 40ms with 50% overlap). Shorter clips are replicated while longer clips are trimmed in several patches inheriting the clip-level label (this is the meaning of the parameter ctrl.load_mode = varup in the config/*.yaml file).
The model used is a CNN (3 conv layers + 1 dense layer) following that of this paper, with two main changes. First, we include Batch Normalization (BN) between each convolutional layer and ReLU non-linearity. Second, we use pre-activation, a technique initially devised in deep residual networks which essentially consists of applying BN and ReLU as pre-activation before each convolutional layer.
It was proved beneficial for acoustic scene classification in this paper, where it showed convenient generalization properties. Likewise, in preliminary experiments with FSDnoisy18k it was shown to slightly improve the classification accuracy. The baseline system has 531,624 weights and its architecture is summarized in the next figure.



As for the learning strategy, the default loss function is categorical cross-entropy (CCE), the batch size is 64, and we use Adam optimizer with initial learning rate of 0.001, which is halved whenever the validation accuracy plateaus for 5 epochs. The training samples are shuffled between epochs. Earlystopping is adopted with a patience of 15 epochs on the validation accuracy. To this end, a 15% validation set is split randomly from the training data of every class. This validation split is the random 15% of every class, considering both clean and noisy subsets together. Preliminary experiments revealed that this provides slightly better results if compared to using only the clean subset for validation (which amounts to roughly 10% of the training set, but it is highly imbalanced class-wise, from 6.1% to 22.4%).
On inference, the prediction for every clip is obtained by computing predictions at the patch level, and aggregating them with geometric mean to produce a clip-level prediction.
The goal of the baseline is to give a sense of the classification accuracy that a well-known architecture can attain and not to maximize the performance.
Extensive hyper-parameter tuning or additional model exploration was not conducted.
Contact
You are welcome to contact me privately should you have any question/suggestion or if you have any problems running the code at eduardo.fonseca@upf.edu. You can also create an issue.
",31
daizutabi/pheasant,HTML,"Pheasant






Description
Pheasant is a Markdown converter which is designed to be used as a plugin for static site generators, especially MkDocs. The one of the main features of Pheasant is auto Markdown generation of outputs after execution of any Python or other language codes written in a fenced code block of Markdown source. This process is executed by the Jupyter client functionality. In addition to the code execution, Pheasant can automatically number headers, figures, tables, etcs.
Document
See Pheasant document.
",2
zhkl0228/emulator,Java,"emulator
Allows you to emulate an Android ARM32 and/or ARM64 native library.
This is an educational project to learn more about the ELF file format and ARM assembly.
Usage
VM options: -Djava.library.path=prebuilt/os -Djna.library.path=prebuilt/os
Where os may: linux64, win32, win64, osx64
Simple tests under src/test directory

src/test/java/com/bytedance/frameworks/core/encrypt/TTEncrypt.java




src/test/java/com/sun/jna/JniDispatch32.java




src/test/java/com/sun/jna/JniDispatch64.java




src/test/java/org/telegram/messenger/Utilities32.java




src/test/java/org/telegram/messenger/Utilities64.java


Features

Emulation of the JNI Invocation API so JNI_OnLoad can be called.
Support JavaVM, JNIEnv.
Emulation of syscalls instruction.
Support ARM32 and ARM64 bit ELF.
Inline hook, thanks to HookZz.
Import hook, thanks to xHook.
Support simple debugger, instruction trace, memory read/write trace.

TODO

Working iOS emulation.
Support iOS objc.

Thanks

unicorn
HookZz
xHook
AndroidNativeEmu
usercorn
keystone
capstone
idaemu
jelf
whale
kaitai_struct

",159
MetacoSA/NBitcoin,C#,"NBitcoin



 
NBitcoin is the most complete Bitcoin library for the .NET platform. It implements all most relevant Bitcoin Improvement Proposals (BIPs). It also provides low level access to Bitcoin primitives so you can easily build your application on top of it. Join us in our gitter chat room.
It works on Windows, Mac and Linux with Xamarin, Unity, .NET Core or CLR. (Porting to Unity should not be that hard if you need it)
The best documentation available is our eBook, and the excellent unit tests. There are also some more resources below.
You can also browse the API easily through the API reference.
How to use ?
With NuGet :

Install-Package NBitcoin

Go on the NuGet website for more information.
The packages support:

With full features: Windows Desktop applications, Mono Desktop applications and platforms supported by .NET Standard 1.3 (.NET Core, Xamarin IOS, Xamarin Android, UWP and more).
With limited features: platforms supported by .NET Standard 1.1 (Windows Phone, Windows 8.0 apps).

To compile it by yourself, you can git clone, open the project and hit the compile button in Visual Studio.
How to get started ? Check out this article on CodeProject for some basic Bitcoin operations, or this Introduction to NBitcoin video.
How to use with Altcoins ?

Install-Package NBitcoin.Altcoins

Find more information here.
How to debug in NBitcoin source code?
When a new version of NBitcoin, NBitcoin.Altcoins or NBitcoin.TestFramework is released on Nuget, we also upload a separate symbol package (snupkg) with SourceLink enabled. This is enabled from version 4.1.1.73.
This means that it is possible to debug into NBitcoin code, and the source will be fetched transparently from github.
This works on both Visual Studio Code and Visual Studio for Windows.
Debug inside source with Visual Studio
You need to run at least Visual Studio 15.9.
Then, you need to:

Go in Tools / Options / Debugging / General and turn off Enable Just My Code.
Go in Tools / Options / Debugging / Symbols and add https://symbols.nuget.org/download/symbols to the Symbol file (.pdb) locations, make sure it is checked.

You should also check Microsoft Symbol Server or your debugging experience in visual studio will be slowed down.
Now you can Debug your project and step inside any call to NBitcoin.
Debug inside source with Visual Studio Code
Inside your launch.json, add the following to .NET Core Launch (console) configuration:
""justMyCode"": false,
""symbolOptions"": {
    ""searchPaths"": [ ""https://symbols.nuget.org/download/symbols"" ],
    ""searchMicrosoftSymbolServer"": true
},
Now you can Debug your project and step inside any call to NBitcoin.
How to use with my own blockchain?
Find more information here.
How to use in Unity?
You should use at least Unity 2018.2 using Script Runtime Version .NET 4.x Equivalent and Api Compatibility Level .NET Standard 2.0.
You can see more on this post.
Then you need to compile NBitcoin:
git clone https://github.com/MetacoSA/NBitcoin/
cd NBitcoin/NBitcoin
dotnet publish -c Release -f netstandard2.0
Remove-Item -Force -Recurse .\bin\Release\netstandard2.0\publish\runtimes\
Then put the libraries of .\bin\Release\netstandard2.0 into your asset folder.
If you need altcoins support, use the same step but with cd NBitcoin/NBitcoin.Altcoins instead.
How to use in .NET Core
If you want to use .NET Core, first install .NET Core as documented here.
Then:
mkdir MyProject
cd MyProject
dotnet new console
dotnet add package NBitcoin
dotnet restore

Then edit your Program.cs:
using System;
using NBitcoin;

namespace _125350929
{
    class Program
    {
        static void Main(string[] args)
        {
            Console.WriteLine(""Hello World! "" + new Key().GetWif(Network.Main));
        }
    }
}

You can then run with
dotnet run

We advise you to use Visual Studio Code as the editor for your project.
Description
NBitcoin notably includes:

A TransactionBuilder supporting Stealth, Open Asset, and all standard transactions
Full script evaluation and parsing
A RPC Client
A Rest Client
The parsing of standard scripts and creation of custom ones
The serialization of blocks, transactions and scripts
The signing and verification with private keys (with support for compact signatures) for proving ownership
Bloom filters and partial merkle trees
Segregated Witness (BIP 141, BIP 143, BIP 144)
Bech32 segwit address implementation with error detection BIP 173
Mnemonic code for generating deterministic keys (BIP 39), credits to Thasshiznets
Hierarchical Deterministic Wallets (BIP 32)
Payment URLs (BIP 21)
Full Bitcoin P2P implementation with SOCKS5 support for connecting through Tor

Please read our ebook to understand the capabilities.
NBitcoin is inspired by Bitcoin Core code but provides a simpler object oriented API (e.g., new Key().PubKey.Address.ToString() to generate a key and get the associated address). It relies on the BouncyCastle cryptography library instead of OpenSSL, yet replicates OpenSSL bugs to guarantee compatibility. NBitcoin also ports the integrality of Bitcoin Core unit tests with their original data in order to validate the compatibility of the two implementations.
NBitcoin is licensed under the MIT License and we encourage you to use it to explore, learn, debug, play, share and create software for Bitcoin and with other Metaco services.
How to connect use a SOCKS5 proxy to connect to a Bitcoin node?
Here an example which assume you run Tor with SOCKS5 proxy on port 9050.
var connectionParameters = new NodeConnectionParameters();
connectionParameters.TemplateBehaviors.Add(new SocksSettingsBehavior(Utils.ParseEndpoint(""localhost"", 9050)));
Node node = await Node.ConnectAsync(Network.Main, ""7xnmrhmkvptbcvpl.onion:8333"", connectionParameters);
node.VersionHandshake();
Some OSS projects using NBitcoin


Wasabi: Privacy focused, ZeroLink compliant Bitcoin wallet.


StratisBitcoinFullNode: Bitcoin full node in C# https://stratisplatform.com


Breeze: Breeze Wallet, the first full-block SPV bitcoin wallet


BlockExplorer: A set of projects that can index and query stratis blockchains on the fullnode.


BTCPay Server: A cross platform, self-hosted server compatible with Bitpay API


NTumbleBit: TumbleBit Implementation in .NET Core


BitPoker: Decentralised peer to peer poker, using bitcoin http://www.bitpoker.io


Zen-Wallet: Node and GUI for the Zen Protocol. https://www.zenprotocol.com


Metaco-Trader: Bitcoin Wallet for advanced user based on a NBitcoin.Server


Swarmops: Admin backend for any bitcoin-native or swarm organization http://sandbox.swarmops.com/


Nako: A Bitcoin and Altcoin server api that indexes blockchain transactions and addresses


NBXplorer: A minimalist UTXO tracker for HD Wallets with bitcoin based altcoin support


UnitCurrency: UnitCoin - a hybrid scrypt PoW + PoS based cryptocurrency.


Openchain: Openchain node reference implementation. https://www.openchain.org/


BreezeProject: Breeze Masternode and Wallet with Breeze Privacy Protocol


Geewallet: a minimalistic and pragmatist lightweight wallet for people that want to hold the most important cryptocurrencies in the same application without hassle


Useful doc :


Ebook Programming The Blockchain in C#


NBitcoin Github : https://github.com/NicolasDorier/NBitcoin


NBitcoin Nuget : https://www.nuget.org/packages/NBitcoin/


Intro: http://www.codeproject.com/Articles/768412/NBitcoin-The-most-complete-Bitcoin-port-Part-Crypt


Stealth Payment, and BIP38 : http://www.codeproject.com/Articles/775226/NBitcoin-Cryptography-Part


How to build transaction : http://www.codeproject.com/Articles/835098/NBitcoin-Build-Them-All


Using the NBitcoin Indexer : http://www.codeproject.com/Articles/819567/NBitcoin-Indexer-A-scalable-and-fault-tolerant-blo


How to Scan the blockchain : http://www.codeproject.com/Articles/784519/NBitcoin-How-to-scan-the-Blockchain (You can dismiss the ScanState for that, now I concentrate on the indexer)


Please, use github issues for questions or feedback. For confidential requests or specific demands, contact us on Metaco support.
Useful link for a free IDE :
Visual Studio Community Edition : https://www.visualstudio.com/products/visual-studio-community-vs
",1313
voteflux/flux-website-v2,HTML,"Instructions
Dependencies
Docker
If you have docker installed just run ./dev-docker.sh - that should get you developing straight away (well, minus the minutes required to build and install deps)
Manually
We presume your environment is OSX.

Install node, npm, ruby, yarn first
macOS: brew install ruby node npm yarn
ubuntu: sudo apt install ruby nodejs (aside: does this include npm?)
Ubuntu: Note: you'll need to install yarn yourself
Fedora: redhat-rpm-config
Install dependencies: ./dev-install-deps.sh or if that doesn't work: gem install bundle then bundle install then yarn install

Note: Node v11 doesn't seem to work building for some deps
Development

To run a development copy for everything run ./dev-watch-all.sh or yarn flux
To simulate a build run yarn build

Deployment

Deployments automatically happen via the master branch.
All merges require a PR.

Hints and Tips
brew install ruby for ruby
gem install bundle for bundle
bundle install to install dependencies
bundle exec jekyll serve --watch to run a dev server for just jekyll stuff.
React is used for the signup form but not for anything else.
Kip's notes on contributing
MK note: if you need to use sudo to run ./dev-docker.sh you should add yourself to the docker group (or google what to do for your OS); typically you shouldn't need sudo for docker, or at least it's good not to run it like that on your dev machine.

Install git
Install Docker
Get an IDE (I use Atom)
fork the flux-website
https://github.com/voteflux/flux-website-v2 - use the fork button
clone your copy onto your machine. For me in a terminal it's:
git clone https://github.com/KipCrossing/flux-website-v2
Read the Readme and run: sudo ./dev-docker.sh
Once the flie has run it will tell you the server address. for me it was: http://0.0.0.0:9000/
Paste that into your web browser
Open the flux-website repo as a project in your IDE
Make changes
in terminal:
git add .
git commit -m ""Write a commit message""
git push
Go to your repo on GitHub and make a pull request
This will help with the Pull Request https://github.com/voteflux/flux-docs/blob/master/docs/contributing/index.rst

",8
voteflux/flux-website-v2,HTML,"Instructions
Dependencies
Docker
If you have docker installed just run ./dev-docker.sh - that should get you developing straight away (well, minus the minutes required to build and install deps)
Manually
We presume your environment is OSX.

Install node, npm, ruby, yarn first
macOS: brew install ruby node npm yarn
ubuntu: sudo apt install ruby nodejs (aside: does this include npm?)
Ubuntu: Note: you'll need to install yarn yourself
Fedora: redhat-rpm-config
Install dependencies: ./dev-install-deps.sh or if that doesn't work: gem install bundle then bundle install then yarn install

Note: Node v11 doesn't seem to work building for some deps
Development

To run a development copy for everything run ./dev-watch-all.sh or yarn flux
To simulate a build run yarn build

Deployment

Deployments automatically happen via the master branch.
All merges require a PR.

Hints and Tips
brew install ruby for ruby
gem install bundle for bundle
bundle install to install dependencies
bundle exec jekyll serve --watch to run a dev server for just jekyll stuff.
React is used for the signup form but not for anything else.
Kip's notes on contributing
MK note: if you need to use sudo to run ./dev-docker.sh you should add yourself to the docker group (or google what to do for your OS); typically you shouldn't need sudo for docker, or at least it's good not to run it like that on your dev machine.

Install git
Install Docker
Get an IDE (I use Atom)
fork the flux-website
https://github.com/voteflux/flux-website-v2 - use the fork button
clone your copy onto your machine. For me in a terminal it's:
git clone https://github.com/KipCrossing/flux-website-v2
Read the Readme and run: sudo ./dev-docker.sh
Once the flie has run it will tell you the server address. for me it was: http://0.0.0.0:9000/
Paste that into your web browser
Open the flux-website repo as a project in your IDE
Make changes
in terminal:
git add .
git commit -m ""Write a commit message""
git push
Go to your repo on GitHub and make a pull request
This will help with the Pull Request https://github.com/voteflux/flux-docs/blob/master/docs/contributing/index.rst

",8
evanrelf/sort-imports,Haskell,"sort-imports
Sort Haskell import statements
Install
git clone https://github.com/evanrelf/sort-imports.git
cd sort-imports
stack install
Usage
Takes input on stdin or with --file 'path/to/file.hs', and outputs to stdout. For example:
sort-imports < input.hs > output.hs
input.hs:
module Main where

import qualified ModuleC as C
import ModuleD ((>>=), function, Type(..))
import ModuleA
import ModuleB hiding (aaa, ccc, bbb)

import AnotherModuleB
import AnotherModuleC
import AnotherModuleA

main :: IO ()
main = putStrLn ""Hello world""
output.hs:
module Main where

import ModuleA
import ModuleB hiding (aaa, bbb, ccc)
import qualified ModuleC as C
import ModuleD (Type(..), function, (>>=))

import AnotherModuleA
import AnotherModuleB
import AnotherModuleC

main :: IO ()
main = putStrLn ""Hello world""
Type sort-imports --help for more information.
Editor integration
Vim/Neovim

sbdchd/neoformat

",2
qword-os/qword,C,"qword - A KISS Unix-like operating system, written in C and Assembly for x86_64.


Talk to us!
We have a Discord server with all the developers for any question, support, contribution, or just chat!
Features

SMP (multicore) scheduler supporting thread scheduling.
Program loading with minimal userspace.
Fully functional VFS with support for several filesystems.
Support for AHCI/SATA.
ATA disk support.

Build requirements
In order to build qword, make sure to have the following installed:
wget, git, bash, make (gmake on *BSD), meson, ninja, gcc/g++ (8 or higher), nasm, xz, autoconf, and QEMU (to test it).
Building
# Clone repo wherever you like
git clone https://github.com/qword-os/qword.git
cd qword/host
# Let's first build and install the echfs-utils
git clone https://github.com/qword-os/echfs.git
cd echfs
make
# This will install echfs-utils in /usr/local
sudo make install
# Else specify a PREFIX variable if you want to install it elsewhere
#make PREFIX=<myprefix> install
# Now build the toolchain (this step will take a while)
cd ../toolchain
# You can replace the 4 in -j4 with your number of cores + 1
./make_toolchain.sh -j4
# Go back to the root of the tree
cd ../..
# Build the ports distribution
cd root/src
MAKEFLAGS=-j4 ./makeworld.sh
# Now to build qword itself
cd ../..
# You might need to use gmake instead of make here on FreeBSD
make clean && make img               # For a standard release build
make clean && make DBGOUT=qemu img   # For QEMU console debug output
make clean && make DBGOUT=tty img    # For kernel tty debug output
make clean && make DBGOUT=both img   # For both of the above
make clean && make DBGSYM=yes img    # For compilation with debug symbols and other debug facilities (can be used in combination with the other options)
You've now built qword, a flat qword.img disk image has been generated.
To run the OS in QEMU, use make run-img.
To run it with KVM enabled, use make run-img-kvm.
",115
godka/kurento-rtmp,JavaScript,"kurento-rtmp
Here is a simple demo which can provide a pipeline from kurento-webrtc to rtmp server (eg., srs, nginx-rtmp-module, etc.).
You can browse https://1029.mythkast.net to test this demo.

The pipeline of the work is described as follows:
[Browser] -> WebrtcEndpoint -> [Kurento] -> RtpEndpoint -> 
[FFmpeg] -> RTMP -> [Node_Media_Server(srs)] -> RTMP -> [Browser]

Build
1.install node && npm
2.git clone https://github.com/godka/kurento-rtmp
3.cd kurento-rtmp
4.npm install
5.node server.js
6.Open https://yourhost on Chrome or Firefox
7.Click Start button and have fun!

Notation
Before running this demo,you must have build FFmpeg and Kurento Media Server on your server.
Licensing and distribution
Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
",75
andrewrothstein/ansible-vault,None,"andrewrothstein.vault

Install's Hashicorp's Vault
Requirements
See meta/main.yml
Role Variables
See defaults/main.yml
Dependencies
See meta/main.yml
Example Playbook
- hosts: servers
  roles:
    - andrewrothstein.vault

License
MIT
Author Information
Andrew Rothstein andrew.rothstein@gmail.com
",2
deepgrace/giant,C++,"


Algorithms and Data Structures in Modern C++
Overview
permutation
#include <vector>
#include <algorithm>
using namespace std;

template <typename T>
bool next_permutation(vector<T>& A)
{
    int k = A.size() - 1;
    while (k > 0 && A[k-1] >= A[k])
           --k;
    if (k == 0)
        return false;

    swap(*find_if(A.rbegin(), A.rend(), [&](auto t){ return t > A[k-1]; }), A[k-1]);
    reverse(A.begin() + k, A.end());

    return true;
}

template <typename T>
bool prev_permutation(vector<T>& A)
{
    int k = A.size() - 1;
    while (k > 0 && A[k-1] <= A[k])
           --k;
    if (k == 0)
        return false;

    swap(*find_if(A.rbegin(), A.rend(), [&](auto t){ return t < A[k-1]; }), A[k-1]);
    reverse(A.begin() + k, A.end());

    return true;
}

template <typename T>
bool next_partial_permutation(vector<T>& A, int k)
{
   reverse(A.begin() + k, A.end());
   return next_permutation(A);
} 

template <typename T>
bool prev_partial_permutation(vector<T>& A, int k)
{
   bool result = prev_permutation(A);
   reverse(A.begin() + k, A.end());
   return result;
} 
combination
#include <algorithm>

template <typename Iterator>
bool combination(Iterator first1, Iterator last1, Iterator first2, Iterator last2, bool ASC)
{
    if (first1 == last1 || first2 == last2)
          return false;

    Iterator m1 = last1;
    Iterator m2 = last2;
    --m2;

    auto comp = [ASC](const auto& x, const auto& y){ return ASC ? x < y : x > y; };

    while (--m1 != first1 && !comp(*m1, *m2));

    bool result = m1 == first1 && !comp(*first1, *m2);
    if (! result)
    {
        while (first2 != m2 && !comp(*m1, *first2))
               ++first2;

        first1 = m1;
        std::iter_swap(first1, first2);
        ++first1;
        ++first2;
    }

    if (first1 != last1 && first2 != last2)
    {
        m1 = last1; 
        m2 = first2;

        while (m1 != first1 && m2 != last2)
        {
               std::iter_swap(--m1 , m2);
               ++m2;
        }

       std::reverse(first1, m1);
       std::reverse(first1, last1);
       std::reverse(m2, last2);
       std::reverse(first2, last2);
    }
    return !result;
}

template <typename Iterator>
bool next_combination(Iterator first, Iterator middle, Iterator last)
{
    return combination(first, middle, middle, last, true);
}

template <typename Iterator>
bool prev_combination(Iterator first, Iterator middle, Iterator last)
{
    return combination(first, middle, middle, last, false);
}
",3
arhat-dev/aranya,Go,"aranya 阿兰若
   
A Kubernetes operator for edge devices
(This project also includes arhat's source, which is the agent for edge device to communicate with aranya)
Purpose

Deploy and manage edge devices with ease.
Remove the boundry between Edge and Cloud.
Integrate every device with container runtime into your Kubernetes cluster.
Help everyone to share Kubernetes masters to others. (see docs/Multi-tenancy.md)

Non-Purpose
Simplify Kubernetes
State
EXPERIMENTAL, USE AT YOUR OWN RISK
Features

Pod modeled container management in edge device

Support Pod creation with Env, Volume

Sources: plain text, Secret, ConfigMap




Remote device management with kubectl (both container and host)

cp
log
exec
attach
port-forward


Built-in Prometheus node-exporter to collect and upload metrics efficiently

NOTE: For details of the host management, please refer to Maintenance #Host Management
Restrictions

Kubernetes cluster network not working for edge devices, see Roadmap #Networking

Build
see docs/Build.md
Deployment Prerequisites

Kubernetes cluster with RBAC enabled

Minimum cluster requirements: 1 master (must have) with 1 node (to deploy aranya)



Deployment Workflow


Deploy aranya to your Kubernetes cluster for evaluation with following commands (see docs/Maintenance.md for more deployment tips)
# set the namespace for edge devices, aranya will be deployed to this namespace
$ export NS=edge

# create the namespace
$ kubectl create namespace ${NS}

# create custom resource definitions (CRDs) used by aranya
$ kubectl apply -f https://raw.githubusercontent.com/arhat-dev/aranya/master/cicd/k8s/crds/aranya_v1alpha1_edgedevice_crd.yaml

# create service account for aranya (we will bind both cluster role and namespace role to it)
$ kubectl -n ${NS} create serviceaccount aranya

# create cluster role and namespace role for aranya
$ kubectl -n ${NS} apply -f https://raw.githubusercontent.com/arhat-dev/aranya/master/cicd/k8s/aranya-roles.yaml

# create role bindings for aranya
$ kubectl -n ${NS} create rolebinding aranya --role=aranya --serviceaccount=${NS}:aranya
$ kubectl create clusterrolebinding aranya --clusterrole=aranya --serviceaccount=${NS}:aranya

# deploy aranya to your cluster
$ kubectl -n ${NS} apply -f https://raw.githubusercontent.com/arhat-dev/aranya/master/cicd/k8s/aranya-deploy.yaml


Create EdgeDevice resource objects for each one of your edge devices (see sample-edge-devices.yaml for example)

aranya will create a node object with the same name for every EdgeDevice in your cluster
Configure the connectivity between aranya and your edge devices, depending on the connectivity method set in the spec (spec.connectivity.method):

grpc

A gRPC server will be created and served by aranya according to the spec.connectivity.grpcConfig, aranya also maintains an according service object for that server.
If you want to access the newly created gRPC service for your edge device outside the cluster, you need to setup Kubernetes Ingress using applications like ingress-nginx, traefik etc. at first. Then you need to create an Ingress object (see sample-ingress-traefik.yaml for example) for the gRPC service.
Configure your edge device's arhat to connect the gRPC server accoding to your Ingress's host


mqtt (WIP, see Roadmap #Connectivity)

aranya will try to talk to your mqtt broker accoding to the spec.connectivity.mqttConfig.
You need to configure your edge device's arhat to talk to the same mqtt broker or one broker in the same mqtt broker cluster depending on your own usecase, the config option messageNamespace must match to get arhat able to communicate with aranya.




Deploy arhat with configuration to your edge devices, start and wait to get connected to aranya

You can get araht by downloading from latest releases or build you own easily (see docs/Build.md).
For configuration references, please refer to config/arhat for configuration samples.
Run /path/to/arhat -c /path/to/arhat-config.yaml





aranya will create a virtualpod with the name of the EdgeDevice in the same namespace, kuebctl log/exec/attach/port-froward to the virtualpod will work in edge device host if allowed. (see design reasons at Maintenance #Host Management)


Create workloads with tolerations (taints for edge devices) and use label selectors or node affinity to assign to specific edge devices (see sample-workload.yaml for example)


Common Node Taints



Taint Key
Value




arhat.dev/namespace
Name of the namespace the edge device deployed to





Common Node Labels



Label Name
Value




arhat.dev/role
EdgeDevice


arhat.dev/name
The edge device name







Performance
Every EdgeDevice object needs to setup a kubelet server to serve kubectl commands which could execute into certain pods, thus we need to provision node certifications for each one of EdgeDevices' virtual node in cluster, which would take a lot of time for lage scale deployment. The performance test was taken on my own Kubernetes cluster described in my homelab after all the required node certifications has been provisioned.


Test Workload

1000 EdgeDevice using gRPC (generated with ./scripts/gen-deploy-script.sh 1000)

each requires a gRPC and kubelet server
each requires a Node and Service object





Resuts
---
Deployment Speed:   ~ 5 devices/s
Memory Usage:       ~ 280 MB
CPU Usage:          ~ 3 GHz

---
Delete Speed:       ~ 6 devices/s



However, after 1000 devices and node objects deployed and serving, my cluster shuts me out due to the kube-apiserver unable to handle more requests, but it's farely good result for my 4 core virtual machine serving both etcd and kube-apiserver.
Roadmap
see ROADMAP.md
Q&A

Why not k3s?

k3s is really awesome for some kind of edge devices, but still requires lots of work to be done to serve all kinds of edge devices right. One of the most significant problems is the splited networks with NAT or Firewall (such as homelab), and we don't think problems like that should be resolved in k3s project which would totally change the way k3s works.


Why not using virtual-kubelet?

virtual-kubelet is designed for cloud providers such as Azure, GCP, AWS to run containers at network edge. However, most edge device users aren't able to or don't want to setup such kind of network infrastructure.
A virtual-kubelet is deployed as a pod on behalf of a contaienr runtime, if this model is applied for edge devices, then large scale edge device cluster would claim a lot of pod resource, which requires a lot of node to serve, it's inefficient.


Why arhat and aranya (why not kubelet)?

kubelet is heavily dependent on http, maybe it's not a good idea for edge devices with poor network to communicate with each other via http.
aranya is the watcher part in kubelet, lots of kubelet's work such as cluster resource fetch and update is done by aranya, aranya resolves everything in cluster for arhat before any command was delivered to arhat.
arhat is the worker part in kubelet, it's an event driven agent and only tend to command execution.
Due to the design decisions above, we only need to transfer necessary messages between aranya and arhat such as pod creation command, container status update, node status update etc. Keeping the edge device's data usage for management as low as possible.



Thanks to

Kubernetes

Really eased my life with my homelab.


virtual-kubelet

This project was inspired by its idea, which introduced an cloud agent to run containers in network edge.


Buddhism

Which is the origin of the name aranya and arhat.



Authors

Jeffrey Stoke

I'm seeking for career opportunities (associate to junior level) in Deutschland



License

Copyright 2019 The arhat.dev Authors.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

",31
abelykh0/stm32f407-z80emu,C,"stm32f407-z80emu
Spectrum ZX 48K emulator (using STM32F407 microcontroller)

What it can do

Emulate Spectrum ZX 48K
Load snapshot in .Z80 format from SD card
Save snapshot in .Z80 format to SD card


Video
Installation
If you want to try my project, this is the only part that you need.



Hardware
Qty




Black F407VET6 board
1


VGA connector
1


PS/2 Keyboard
1


Power supply 5V DC for keyboard
1


Resistors 470 Ohm
3


Resistors 680 Ohm
3


Resistors 2.2 KOhm
2


Resistors 3.3 KOhm
2


Breadboard
1


Jumper wires
18


ST-Link v2 or clone
1



Software: Install free IDE System Workbench for STM32. I am using Windows 10, however STMicroelectronics claims that it also supports Linux and Mac.
How to connect wires:



PIN
Description
Connect To
Output




PE8
Red 1
Resistor 470 Ohm
VGA red (1)


PE9
Red 2
Resistor 680 Ohm
VGA red (1)


PE10
Green 1
Resistor 470 Ohm
VGA green (2)


PE11
Green 2
Resistor 680 Ohm
VGA green (2)


PE12
Blue 1
Resistor 470 Ohm
VGA blue (3)


PE13
Blue 2
Resistor 680 Ohm
VGA blue (3)


PD15
HSync

VGA HSync (13)


PD14
VSync

VGA VSync (14)


PB14
CLK
Resistor 2K2 to keyboard CLK and resistor 3K3 to GND



PB13
DATA
Resistor 2K2 to keyboard DATA and resistor 3K3 to GND



G
Ground

VGA Ground (5,6,7,8,10), '-' of passive speaker



Third party software
This project uses several libraries (in addition to HAL drivers from STMicroelectronics):

To display video using VGA: https://github.com/cbiffle/m4vgalib (which requires https://github.com/cbiffle/etl)
Z80 emulator: https://github.com/anotherlin/z80emu
FATFS for SD card: http://elm-chan.org/fsw/ff/00index_e.html

Plans for the future / issues

Flickering in some games
The speed is 12% faster than it is supposed to be
Sound

",10
ParaffinIoT/brokero,JavaScript,"brokero
Installer shell script for open source Paraffin microservice IoT platform. See Paraffin Platform
About
Paraffin is IoT platform based on node.js and mongodb with MQTT, HTTP and CoAP bridge.
Paraffin will enable you to put your IoT API services on your own server simply and painless in one command. It supports the popular MQTT and CoAP protocols in sync with HTTP. It is in javascript and by Parse Server api server will be able to authorize your device list so broker perform authentication by your entry data in MongoDB by api server.
Features

Simple and Scalable.
HTTP, MQTT and CoAP connections together as a bridge.
MQTT 3.1 and 3.1.1 compliant.
Sercured with authentication and JWT.

Install
Run the following command in your terminal to install the latest official Paraffin IoT Platform release.
sudo curl -o- https://raw.githubusercontent.com/ParaffinIoT/brokero/master/install.sh | bash
",2
bharti27/BestForMe,JavaScript,"BestForMe
BestForME is a Cross Domain Media Recommendation system. BFM recommend media based on users interest not just in one domain but in multiple domains like, movies, books, podcasts, video games
Available Scripts
In the project directory, you can run:
npm start
Runs the app in the development mode.
Open http://localhost:3000 to view it in the browser.
The page will reload if you make edits.
You will also see any lint errors in the console.
npm test
Launches the test runner in the interactive watch mode.
npm run build
Builds the app for production to the build folder.
It correctly bundles React in production mode and optimizes the build for the best performance.
The build is minified and the filenames include the hashes.
Your app is ready to be deployed!
Demo Link on S3
This is a demo link: http://bharti.project.s3-website-us-east-1.amazonaws.com/
Credentials
username: dexter27
password: password
username: Josephine88
password: password
or you can create a new user by clicking on registration.
",2
Lombiq/Pretty-Good-Bootstrap-Base-Theme,HTML,"Pretty Good Bootstrap Base Theme Orchard Theme Readme
Project Description
An Orchard base theme building on Twitter's Bootstrap framework.
We've also created a complete sample theme demonstrating what you can do with PGBBT.
PGBBT is the base for the themes of all Lombiq websites, including Lombiq.com, Orchard Dojo and DotNest.com.
The theme is also available for DotNest sites.
Documentation

What's the Great Purpose?
Structure
How to...
Updating Bootstrap

PGBBT currently includes the Bootstrap 3.3.4.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/pretty-good-bootstrap-base-theme (Mercurial repository)
https://github.com/Lombiq/Pretty-Good-Bootstrap-Base-Theme (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",6
DVE2000/Dogbone,Python,"Dogbone addin for fusion 360
Version: 2.0


Windows users:

You can download a self extracting file here



Mac users:


If you installed F360 directly from AD - download self extracting file here


If you installed F360 from Apple App Store - download self extracting file here


If you're having problems due to Apple Security, instead of clicking in the Downloads Dock icon Folder or Stack, click ""Open in Finder"" and then right-click the package and select ""Open"". You'll be able to install it then.


zip and tar files available (for both Mac and Windows) here

Description



Face selected and top face dogbones











Minimal dogbone
Long side mortise dogbone
Short side mortise dogbone










This addin includes 3 dogbone styles (normal, minimal and mortise) and allows dogbones to be cut from either the topface or selected face.  Both static and parametric options are available - however due to a number of bugs in F360, parametric will fail on mirrored components and some component copies.  These bugs have been reported (see here), but as of writing AD has not addressed them.
The interface has been improved, and specifically allows any face orientation to be chosen on any component.  The addin is based on the f360 primitive hole feature, and is as efficient as f360 single threaded engine allows.  In Static mode it will create 70 dogbones in about 7 seconds.  Parametric mode takes a little longer to calculate initially, but recalculation is very fast if you change your model parameters.  Of course, speed also depends on the processing power of your computer.

Notes:
This version should work with all static dogbones. Parameterized dogbones mostly work, but there are definite issues with the Fusion360 API that may cause problems when trying to create dogbones. If that happens, you can create a logfile and post it here to let us know. If it turns out that it is a Fusion360 bug, please report it to Autodesk.

Dogbones has been completed revamped. Some of the original utilities have remained, as well as some original mathematical formulaes, but mostly everything else has changed.
The original add-in was based on creating sketches and extruding - Peter found using this approach to be very heavy on processing resources, so this version has been designed to create dogbones directly by using a hole tool.

This version should work with all static dogbones. Parameterized dogbones mostly work, but there are definite issues with the Fusion360 API that may cause problems when trying to create dogbones. If that happens, you can create a logfile and post it here to let us know. If it turns out that it is a Fusion360 bug, please report it to Autodesk.

WARNING: use at your own risk.
The code provided is provided ""as is"" and with all faults. We specifically disclaim any implied warranty of merchantability or fitness for a particular use. The operation of the code provided is not warranted to be uninterrupted or error free.

Installation
See How to install sample Add-Ins and Scripts
Instructions
Note that you can hover your cursor over any Dogbone dialog item and you will get an explanatory popup in Fusion360.

Select the face(s) you want the dogbones to drop from. The add-in will only allow you to select appropriate and/or parallel faces for the body, once a primary face has been selected. The orientation of the primary face for unique components or bodies may be in any direction.

All edges associated with the selected face will be automatically selected. You can select the ""Dogbone Edges"" selector in the Dogbone popup, and that will allow you to deselect or reselect only internal edges. Note that only internal edges belonging to a selected face can be selected or deselected.

Specify a tool diameter and a radial offset.
Select the Mode - Static Dogbones or Parameterized Dogbones. Parameters are created for the second mode - dogbones will move with edge changes, and you can change diameter or offset from the normal ""Change Parameters"" dialog.
Choose the type of dogbone - Normal, Minimal or Mortise. See http://fablab.ruc.dk/more-elegant-cnc-dogbones/ for a description of minimal dogbones. Mortise dogbones place the dogbones along the sides, so that they can be hidden by a connecting piece with a cut tenon. Minimal and Mortise dogbones have their own option lines become visible when selected.  Note: In the minimal dogbone dialog, you can make the Percentage Reduction negative (eg -20), to inset the dogbone into the workpiece.
Decide if you'd like dogbones to be cut to the top. (Useful if you have steps, but can't do two sided machining.)
 
You can expand Settings and specify if you'd like to see benchmark time or do any logging.
Click ok.

The add-in will then create the specified dogbones. If you choose parameterized, the critical dimensions are maintained in the parameters - so you can change the dimensions as and when needed.

If you need dogbones in different orientations for the same body, you'll have to run the addin once for each direction.
The direction for egdes for a body is locked onve any face is selected. De-select all faces if you want to change edge selection direction.
Edges are selected down from a face. Generally, selecting a bottom face will not add any edges, but de-selecting one may remove some edges.

To do:

Handle acute angles (<90 degrees) by generating a slot.
Handle obtuse angles (>90 degrees)
... who knows

License
Samples are licensed under the terms of the MIT License. Please see the LICENSE file for full details.
Authors
Peter Ludikar (pludikar), Gary Singer (DVE2000), Casey Rogers (casycrogers)

Original version by Casey Rogers: https://github.com/caseycrogers/Dogbone/tree/cbe8f2c95317ae7eded43fee384171a492c6900e
Original version Modified by Patrick Rainsberry (Autodesk Fusion 360 Business Development)
Original version Modified by David Liu (http://github.com/iceboundflame/)

",72
zhuyizhuo/notes,JavaScript,"目录

notes [folder]

convention [folder]

版本号命名规范


database [folder]

mongodb
mysql
oracle sql
oracle 内置函数
oracle


java [folder]

annotations
cron
dubbo
freemarker
java 基础
java 调优
java-design-patterns
jsp
jstl
jvm
mybatis
questions [folder]

questions


spring
validation
项目管理 [folder]

maven plugin
maven




javascript [folder]

javascript
js [folder]


linux [folder]

linux-grep
linux-top
linux


pics [folder]
README
software-install [folder]

java 环境变量配置
mysql 安装
pics [folder]


tools-and-settings [folder]

eclipse 中的重构功能
eclipse 设置及快捷键
git
idea 快捷键
idea-settings
power-designer
sublime
svn version
tools
typora 快捷键


website [folder]

reference-materials
website


windows [folder]

windows-question





TIPS
1. 笔记属个人平时积累记录,如果觉得有用,欢迎各位Pull requests.
2. 欢迎指正笔记中的错误.
3. 欢迎issue留言交流.

联系方式
添加请注明:来自gitHub

QQ: 2361883887
个人微信:

",5
guanguans/favorite-link,Python,"favorite link
收集喜欢的网址
License
GNU General Public License v3.0
May 12, 2019

显示和控制您的Android设备
Tpay_Svr是微信和支付宝的个人免签 24小时全自动回调支付系统的php服务端程序 
微信/支付宝二维码支付监听器 - 免签接口 指母付/钉钉支付/个人二维码收款系统/微信支付支付宝支付二维码监听自动发货/个人免签系统/个人免签支付 微信.支付宝 个人支付监控 
欢迎使用 composer 免签约支付宝与微信 带监听
暗网导航

May 11, 2019

BeEF是The Browser Exploitation Framework的缩写。它是一种专注于Web浏览器的渗透测试工具。
Content Security Policy 入门教程
详细介绍如何使用 CSP 防止 XSS 攻击。
非常简单的点击复制CSS效果
英特尔公司
下一代对象关系数据库。一个@MagicStack项目。
PowerToys是一组用于高级用户调整和简化其Windows体验以提高工作效率的实用程序。
Overlord是哔哩哔哩基于Go语言编写的memcache和redis&cluster的代理及集群管理功能，致力于提供自动化高可用的缓存服务解决方案。 bilibili.com
Thruway是用于PHP的WAMP（Web应用程序消息传递协议）的开源客户端和路由器实现。Thruway使用事件驱动的非阻塞I / O模型（reactphp），非常适合现代实时应用。
适用于移动腾讯分析HTML5的PHP SDK
Symfony编码标准的开发存储库
使用php开发实现webdav协议的项目
🛁 PHP版的代码整洁之道 中文翻译
爆破字典
TensorFlow的Rust语言绑定
Yii2 Audit记录并显示web / cli请求，数据库更改，php / js错误和相关数据。
基于Docker快速搭建Gitlab与Gitlab CI/CD服务
Laravel Envoy提供了一种简洁的语法，用于定义您在远程服务器上运行的常见任务。使用Blade样式语法，您可以轻松设置部署任务，Artisan命令等。
V8 JavaScript引擎的PHP扩展
RESTful API 设计参考文献列表，可帮助你更加彻底的了解REST风格的接口设计。
JWT Framework
带有解析器的HTML5视频播放器，可以节省流量
字节跳动
Chrome运行时性能瓶颈分析
Vue.js剪贴板库（无依赖关系，小于2kb）
在浏览器中使用同类最佳的编解码器缩小图像。
K8s本机管道资源。
go程序热编译工具，提升开发效率
用React、Redux、Immutable做俄罗斯方块
用Vue、Vuex 做俄罗斯方块
PhpStorm Plugins-排序 use 包名称
使用Docker和docker-compose运行Symfony应用程序

May 10, 2019

OpenMV相机模块
在本笔记本中，我将创建一个预测股票价格变动的完整流程。 跟着我们，我们将取得一些非常好的结果。
HQ开放数据集的以主题为中心的列表。
✍️Fusuma轻松制作带有Markdown的幻灯片。
李笑来-挤挤都会有的 - 写给女生的性高潮指南
一个简单，轻量级的JavaScript API，用于处理浏览器cookie
将您的ascii图表涂鸦转换为快乐的小SVG
远程管理工具
本文原文由知名Hacker Eric S. Raymond 所撰写，教你如何成为一名黑客。
🖍终端字符串样式正确完成
使用盲水印保护创作者的知识产权
Golang的惯用HTTP中间件
社区Bash框架。
用于格式化和操作数字的JavaScript库。
这是在Node.js中构建CLI的框架。 此框架是使用Heroku CLI构建的，但通用于构建任何自定义CLI。
cvpr2019 papers，极市团队整理 
使用GitHub和Slack集成将您的代码带到您关心的对话中
Vim 从入门到精通
IPFS的桌面客户端。
PHP中IPFS API的客户端库
Generatedata是一个免费、开放源码的脚本，主要由javascript ， PHP和MySQL构成，它可以让您可以迅速生成大量各种格式的客户数据，用于测试软件，把数据输入数据库等。
创建使用虚假数据填充的自定义测试数据库
8个不错的随机生成数据库测试数据的利器 - 我就是snail啦 - OSCHINA
一个开源工具，用于从MySql数据库生成完整的后端。
Docker打包版本的benkeen / generatedata项目
😍Web前端助手--FeHelper

May 9, 2019

Cloud-Admin是国内首个基于Spring Cloud微服务化开发平台，具有统一授权、认证后台管理系统，其中包含具备用户管理、资源权限管理、网关API管理等多个模块，支持多业务系统并行开发，可以作为后端服务的开发脚手架。
使用Headless Chrome将javascript呈现的页面呈现为HTML的节点服务器。与prerender中间件一起使用。
一份网络安全入门的资料。
ARP+DNS欺骗工具，网络安全第三次实验，课堂演示用，严禁非法用途。ARPSpoof，wifi hijack，dns spoof
Kubernetes中文指南/云原生应用架构实践手册 
Vuido是一个基于Vue.js创建本机桌面应用程序的框架。使用Vuido的应用程序可以在Windows，OS X和Linux上运行，使用本机GUI组件，不需要Electron。
React in patterns 中文版 
🔍 让我帮你百度一下？
《Mastering GO》中文译本，暂时命名为《玩转 GO》。
Docker Machine的VMWare Workstation驱动程序
功能齐全的BitTorrent客户端软件包和实用程序
GitLab API客户端，使Go程序能够以简单统一的方式与GitLab交互
Golang的HTTP模拟
《GO专家编程》
将您的Flutter代码带到Web浏览器
一个真正可怕的异步网络聊天，在前端没有使用任何JS
使用React构建本机Windows应用程序的框架。
桌面自动化框架
m4b-tool是一个命令行实用程序，用于合并，拆分和分章有声读物文件，如mp3，ogg，flac，m4a或m4b
Laravel项目的自动代码格式
10 分钟掌握 MySQL 的索引查询优化技巧
我们发布了一个新的项目—Crypko。在Crypko中，你可以从以太坊区块链上获取并交易AI生成的二次元角色。
使用MakeGirlsMoe创建动画角色
MyFlash是由美团点评公司技术工程部开发维护的一个回滚DML操作的工具。该工具通过解析v4版本的binlog，完成回滚操作。相对已有的回滚工具，其增加了更多的过滤选项，让回滚更加容易。 该工具已经在美团点评内部使用
Laravel best practices
用于学习算法并在任何编程语言中实现它们

May 8, 2019

Github用户排名
CockroachDB  - 开源的云原生SQL数据库。
go.rice是一个Go包，它可以很容易地处理html，js，css，图像，模板等资源。
将本地服务器公开给外部网络
Laravel电子商务套餐，适用于专业，超快的在线商店，复杂的B2B应用程序和#gigacommerce
Rust通道的多进程直接替换
用于从Google Analytics检索综合浏览量和其他数据的Laravel程序包
微信官方 js-sdk CommonJS 版本
华丽的应用程序，纠正您以前的控制台命令。
免费购物车系统。OpenCart是一个基于PHP的开源在线电子商务解决方案。
史上最全的PyTorch学习资源汇总
分布式服务框架Zookeeper -- 管理分布式环境中的数据 
🚇 即刻 Ⓙ SDK 
优化即刻网页版体验的 Chrome 插件 
简单的错误处理原语
为方便WAF入库的项目 | 分享PHP免杀大马 | 菜是原罪 | 多姿势(假的就一个)
干将是一个自动化插件式网络漏洞检测系统
ThinkCMF电商版，基于ThinkCMF5.1开发
Honukai主题和颜色哦我的ZSH和iTerm
cos-php-sdk-v5
又拍云 SDK for PHPer

May 7, 2019

分布式知识图存储
phpEnv一款优雅强大的php集成环境
Symfony与Swoole集成以加速您的应用程序。
Php-webdriver库是Selenium WebDriver的PHP语言绑定，它允许您从PHP控制Web浏览器。
Golang优雅的刮板和抓取器框架
子豪兄的零基础树莓派教程，代码存放地及更新勘误
vue.js(element框架)+golang(beego框架)开发的运维发布系统,支持git,jenkins版本发布,go ssh,BT两种文件传输方式选择,支持部署前准备任务和部署后任务钩子函数
Wizard是基于Laravel开发框架开发的一款开源项目（API）文档管理工具。
提供在Linux上运行最新版腾讯QQ与TIM的解决方案
该存储库包含我的O'Reilly书籍Flask Web Development第二版的源代码示例。
AnyBar是您的菜单栏的一个小指示器，可以做一件简单的事情：它显示一个彩色圆点。点的含义以及何时更改它取决于您。
编写和优化Go代码
SQLFlow是连接SQL引擎的桥梁，例如MySQL，Hive，SparkSQL或SQL Server，带有TensorFlow和其他机器学习工具包。SQLFlow扩展了SQL语言，以支持模型训练，预测和推理。
工匠菜单 - 通过优雅的控制台GUI使用Artisan
ShopXO商城系统、国内领先企业级B2C免费开源电商系统，包含PC/wap/小程序
中国首个开源学校教务管理系统、网站布局自动化、学生/成绩/教师、成绩查询 
从供应商目录中删除测试和文档并缩小所有php文件
BladeOne是刀片模板引擎的独立版本，它使用单个PHP文件，可以移植并在不同的项目中使用。它允许您在Laravel外部使用刀片模板。
docker-slim：自动缩减 docker 镜像的体积的工具。大幅度缩减 docker 镜像的体积，方便分发，使用命令 docker-slim build --http-probe your-name/your-app。比如 Node.js 镜像缩减后的对比： from ubuntu:14.04 - 432MB => 14MB (缩减了 30.85 倍)
小霸王，其乐无穷 。红白机，FC在线游戏
简悦 ( SimpRead ) - 让你瞬间进入沉浸式阅读的扩展
gh-ost是MySQL的无触发在线模式迁移解决方案。
利用HTTP协议 远程加解密数据包，实现Burp一条龙服务。
api-standards
API设计指南
Standard Readme
中华人民共和国密码行业标准(GM/T)文本
支持国密SM2/SM3/SM4/SM9/ZUC/SSL的OpenSSL分支
pt-query-digest
README文件语法解读，即Github Flavored Markdown语法介绍

May 6, 2019

ntp包是基于RFC5905的Simple NTP（SNTP）客户端的实现。它允许您连接到远程NTP服务器并请求有关当前时间的信息。
将Word文档（.docx文件）转换为HTML
kunpeng是一个Golang编写的开源POC框架/库，以动态链接库的形式提供各种语言调用，通过此项目可快速开发漏洞检测类的系统。
vfsStream是虚拟文件系统的流包装器，可能有助于单元测试模拟真实文件系统。它可以与任何单元测试框架一起使用，如PHPUnit或SimpleTest。
一个PHP快速CGI客户端，用于同步向PHP-FPM发送请求（a）
不同行业的应用机器学习和数据科学笔记本和图书馆的精选列表。
即刻黄历macOS Saver
福利直通车
集合多家 API 的新一代图床 
UrlHum是一个使用PHP和Laravel Framework构建的现代，隐私和快速URL Shortener
laravel telescope 是Laravel框架的优雅调试助手。Telescope可深入了解进入应用程序的请求，异常，日志条目，数据库查询，排队作业，邮件，通知，缓存操作，计划任务，变量转储等。望远镜是您当地Laravel开发环境的绝佳伴侣。
commit 提交指南

May 5, 2019

各种xlsx的分析器和编写器。
使用Wordpress和WooCommerce构建高度可定制的电子商务网站，销售shadowsocks服务
FydeOS - 面向未来的云驱动操作系统 | 为中国用户打造的 Chrome OS
Quicker-window自动化软件
Quicker IOS 客户端
CC助手 - 超越剪贴板

May 4, 2019

GraphQL 开发原则
通过思维导图整理redis的重要知识点
用于从Google Analytics检索综合浏览量和其他数据的Laravel程序包
slimdump是一个小工具，可以帮助您创建大型MySQL数据库的可配置转储
首款微信 macOS 客户端撤回拦截与多开 
上最棒的开源 Win10 数字权利（数字许可证）激活工具！ 

May 3, 2019

易于上手的样品参考微服务和基于容器的应用。 Linux和Windows Docker容器上的跨平台，由.NET Core 2.2，Docker引擎以及Azure，Kubernetes或Service Fabric提供支持。
Surface Pro 6 黑苹果全球首发
一个静态网站生成器，用于使用Vue.js构建超快速的网站
供 Dash 使用的中文文档
Dash中文文档
go 语言框架 gin 的中文文档
快速开发macOS PHP开发环境
快速，评论，节点的极简主义Web框架。
深入理解Node.js：核心思想与源码分析
Vue.docset dash 文档 Vue 离线文档中文版
leetcode题解，记录自己的leetcode解题之路。
一个基本的游戏模拟器，支持终端“云游戏”

May 2, 2019

LeanCloud 发布的 Git Commit 日志风格指南。
动漫场景按图搜索
类似unix的逆向工程框架和命令行工具
测试可能是拖累。 AVA可以帮助您完成任务。 AVA是Node.js的测试运行器，具有简洁的API，详细的错误输出，新语言功能的拥抱和进程隔离，可让您更有效地编写测试。
FreeRDP是一个免费的远程桌面协议库和客户端
管理hosts的更好方法。
mysql注入,bypass的一些心得

May 1, 2019

精通以太坊 （中文版）
Cloud-Native API网关
从VS Code调试在Google Chrome中运行的JavaScript代码。
when当我键入kubectl run时会发生什么？
用于收集和报告指标的插件驱动的服务器代理。
一个经典的XSS渗透管理平台
Mac版微信的功能拓展
如何用GitHub Actions编译Golang项目

April 30, 2019

备忘单
200+ Mac菜单栏应用程序
一个简单的macOS应用程序，用于监控云服务的状态
Python中的简单区块链
Sortable是一个用于可重新排序的拖放列表的JavaScript库。
💬使用Google Translate，Bing Translator，Yandex.Translate等的命令行翻译器
引导您的用户浏览您的应用
Traefik是一个现代HTTP反向代理和负载均衡器，可以轻松部署微服务。Traefik与您现有的基础架构组件（Docker，Swarm模式，Kubernetes，Marathon，Consul，Etcd，Rancher，Amazon ECS ......）集成，并自动动态配置。在您的协调器上指向Traefik应该是您需要的唯一配置步骤。
SeaweedFS是一个简单且高度可扩展的分布式文件系统。有两个目标：存储数十亿个文件！快速提供文件！SeaweedFS实现了一个带有O（1）磁盘搜索的对象存储和一个带有POSIX接口的可选Filer，支持S3 API，FUSE挂载，兼容Hadoop。
Uber Technologies公开发布的分布式跟踪系统。 它可用于监视基于微服务的分布式系统
🎉全面的PHP客户端软件包，用于使用Hubtel Payment API
现代HTTP基准测试工具
Java资源大全中文版，包括开发库、开发工具、网站、博客、微信、微博等，由伯乐在线持续更新。
所有算法都在Python中实现
Cyber​​netically增强的网络应用程序
一个高级web目录扫描工具，功能将会强于DirBuster、Dirsearch、cansina、御剑。
一个外观漂亮且易于使用的照片管理系统，您可以在服务器上运行，管理和共享照片。
url2pdf基于中文支持wkhtmltopdf的docker镜像。
此程序包允许您管理数据库中的用户权限和组。
☁️LskyPro，您在云端的相册。
✨另一个OneDrive目录索引
Scoop bucket 用于开源/免费软件游戏和游戏相关工具
scoop-extras

April 29, 2019

PHP MySQL类的包装器，它使用MySQLi和预处理语句。
收集&推荐优秀的 Apps/硬件/技巧/周边等

April 28, 2019

一种查看和导航目录树的新方法
基于workerman的分布式实时消息传递框架。
Web安全学习笔记

April 27, 2019

微信支付宝个人免签收款Api系统，有了它对接再也不用担心我的业务不能支付了。
基于swoft-cloud的微服务架构，最小化拆分粒度，PHP7、多进程、协程、异步任务、mysql连接池、redi连接池、rpc连接池、服务治理、服务注册与发现、Aop切面、全注解
yq是一个便携式命令行YAML处理器
《深入解析Go》
这本书是来自walu的phpbook升级版
适用于macOS的更好的Cmd-Tab（OSX）
适用于您可爱网站的CSS唯一工具提示库。
在非Laravel应用程序中使用每个Illuminate组件的示例
🚀Chrome/ Firefox Extension可以从WebSite中搜索RSS源URL，支持RSSHub

April 26, 2019

文档和源代码PopClip扩展。
用C编写的通用数据结构和算法库。
编译器工具包。对于PHP（是的，我在命名事物上很有创意）......
如何搜索和阅读一篇论文 
qq群和soyun社工库的查询sql和索引存储过程
一个小型的较为简陋的社工库查询系统,使用Vue.js+Flask+MongoDB.可用52g, 本人不提供任何社工库资源. 
搜呼社工库
社工库查询系统
PHP的现代任务运行器
在 2019 成为一名 Go 开发者的路线图。为学习 Go 的人而准备。
💗在chrome扩展中写博客并推送到github pages

April 25, 2019

用于个人用途的部署机器人。
《Koa2进阶学习笔记》已完结🎄🎄🎄
小白入门学习vue和vue实例，vue总结
HTTP负载生成器，ApacheBench（ab）替换，以前称为rakyll / boom
基于Golang解决的长连接并发服务器框架
Go中快速，结构化，水平的日志记录。
electron builder 
切勿再次使用打印进行调试
用于辅助安全工程师漏洞挖掘、测试、复现，集合了mock、httplog、dns tools、xss，可用于测试各类无回显、无法直观判断或特定场景下的漏洞。
从生产中不需要的文件中清除PHP Composer供应商文件夹
tj/node-prune: Remove unnecessary files from node_modules (.md, .ts, ...)从node_modules（.md，.ts，...）中删除不必要的文件
Github 项目活跃度分析工具
lanproxy是一个将局域网个人电脑、服务器代理到公网的内网穿透工具，目前仅支持tcp流量转发，可支持任何tcp上层协议（访问内网网站、本地支付接口调试、ssh访问、远程桌面...）。
Windows 10的macOS Mojave Dynamic Desktop功能端口

April 24, 2019

最小的Markdown编辑器桌面应用程序
tshark的终端UI，灵感来自Wireshark
基于GitHub问题的评论系统。
分享一些黑苹果clover配置文件
黑客神器，谁用谁知道！
⛓区块链应用框架✨
Vue.js驱动的现代网站生成器
写代码时，持续思考这段代码能不能很容易的写出单元测试，能够大幅度提高最终产出的代码质量(即使实际上没写单元测试...)
用JavaScript编写的HTML to Markdown转换器
HackMD
Github味道Markdown编辑器。
创建基于Composer的Phar of PHP应用程序
bin依赖项没有冲突
Brook是一个跨平台（Linux / MacOS / Windows / Android / iOS）代理/ VPN软件

April 23, 2019

php导出导入excel
A LotServer KeyGen
Bluecherry监控系统（服务器应用）
Kratos是bilibili开源的一套Go微服务框架，包含大量微服务相关框架及工具。
swoole source reading 源码分析

April 22, 2019

wtfpython的中文翻译/施工结束/ 能力有限，欢迎帮我改进翻译
🖌docsify cli工具 - 一个神奇的文档生成器。
使用node.js的OAuth2.0的一个非常简单的演示
FastAPI框架，高性能，易学，快速编码，随时可用于制作
基于纯PHP实现的DH库，用于服务器端
dogeTV for macOS
Commit messages guide
一个专注于隐私，可扩展和美丽的Web浏览器
2019年成为Go开发者的路线图
一个带有调试工具的python vm注入器，基于gdb。
mathdroid (Odi)
将您的Python和Javascript代码转换为DOT流程图
轻松地将图像组合在一起，而不会弄乱画布

April 21, 2019

nicedoc.io是一个表示层，用于美化github.com上托管的任何doc文件。
利用思维导图整理的docker知识点
开发效率提升：Mac生产力工具链推荐
中国5级行政区域mysql库
msgpack.org [PHP]
Übersicht
一个漂亮的终端仿真器，模仿旧的阴极显示器......
Go设计模式的精选列表
 Full-featured code intelligence and smart autocomplete for Sublime Text
对象存储PHP简易代码，支持简单上传，删除，列表功能。支持网易，腾讯，七牛，UCloud四家API

April 20, 2019

Chrome扩展: 告别跨域，定制HTTP请求响应头
使用相同的Laravel安装运行多个网站，同时保持租户特定数据分离，以实现完全独立的多域设置

April 19, 2019

PHP中的BDD
wordpress theme rizhuti 日主题 不要钱 随意用

April 18, 2019

这是GitHub Actions main.workflow文件的语言规范和官方解析器。它作为GitHub Actions的一部分在生产中运行。它是用Go编写的。
使每个人都能够构建异步软件
z  - 跳来跳去
与GitHub API v3键入的交互
laravel-admin使用iframe-tab打开多页面
一个新鲜和轻量级的JavaScript游戏引擎
Laravel idcard验证器
ripgrep递归地在目录中搜索正则表达式模式
👊 Java Android 近几年最全面的技术点以及面试题 供自己学习使用
Laravel Eloquent模型的特性，可以让您克隆模型及其关系，包括文件。甚至到另一个数据库。
验证表单异步
一个全栈增长工程师的练手项目集. 
[包]用于包开发的Laravel Testing Helper
为Laravel开发测试助手
Frps 一键安装脚本，Frpc Windows 便捷脚本！Frp 远程桌面！
Linux 平台下基于 Rust + GTK 开发的网易云音乐播放器
Siler是一组通用的高级抽象，旨在用于PHP中的声明性编程API。
又一个 swoole
清理不值得信任的HTML用户输入
🐳 WeChat Playground - 开源微信调试工具
PHP设计模式的使用
PHP安全配置检查器

April 17, 2019

从混乱的网络中提取内容。
跨平台，GPU加速的终端仿真器
前端性能监控系统,消息队列,高可用,集群等相关架构
xhprof: PHP7 support
高效的基于令牌桶的速率限制器包。
用于构建Kubernetes应用程序的SDK。提供高级API，有用的抽象和项目脚手架。
Go包用于轻松呈现JSON，XML，二进制数据和HTML模板响应。
Beats是用Go编写的轻量级数据发送器，可以安装在服务器上以捕获各种操作数据（考虑日志，指标或网络数据包数据）。Beats直接或通过Logstash将操作数据发送到Elasticsearch，因此可以使用Kibana进行可视化。
Go编程语言的自动完成守护程序
适用于Laravel 5.x的SweetAlert2
基于Web的文件管理器在单个PHP文件中，使用Tiny File Manager高效，轻松地管理文件
以前所未有的速度构建Laravel Web应用程序，易于安装，无需定制。
姬长信API一个开源免费不限制提供生活常用,出行服务,开发工具,金融服务,通讯服务和公益大数据的平台 
《Chrome插件开发全攻略》配套完整Demo
一个轻量的工具集合
Python - 100天从新手到大师
SOLID原则 - 简单易懂的解释
使用Docker运行Laravel项目并使用Kubernetes进行部署
接口和抽象类 - 简单易懂的解释
✨用于macOS的Finder工具栏应用程序，用于打开Terminal，iTerm或Hyper中的当前目录。

April 16, 2019

PHP-FPM状态页面CLI
DH算法的API端，DH是一种利用非对称协商对称密钥的交换算法，他避免了对称密钥于公网来回传递的问题。
一个主要用于信息搜集的工具集，主要是用于对网站子域名、开放端口、端口指纹、c段地址、敏感目录等信息进行批量搜集。
用户友好的命令行shell。
💻 计算机速成课 | Crash Course 字幕组 (全40集 2018-5-1 精校完成)
自动完成软件发布的繁琐任务。愉快地发布和发布您的Git存储库，npm包，GitHub和GitLab版本，更改日志等等！
🙈实现过滤敏感词汇🔞，基于确定有穷自动机(DFA)算法，支持composer安装扩展
Golang的实战项目，学习笔记，代码例程汇总。
适用于Yii2应用程序的OAuth2包装器
Yii2 Webpack2资产管理
一组匹配中国大陆手机号码的正则表达式。
zipline 是开源量化平台，但是当前zipline 并不支持A股的测试，很多在线平台如优矿，聚宽等都是基于zipline，本项目改进zipline，使得zipline支持A股测试

April 15, 2019

一个极简内存池实现
 迅雷快鸟 Linux 版 
Squeezer Framework  - 构建无服务器的dApp
具有AutoCompletion和语法突出显示的MySQL终端客户端。
Swoole 开发的MySQL数据库连接池
基于爬虫的web漏洞扫描器
统计GitHub上文件的代码行数
saber / http库提供了处理http请求和响应的实用程序。
Laravel Repositories是Laravel 5的一个包，用于抽象数据库层。这使应用程序更容易维护。
依赖注入容器
用于InnoDB文件格式的解析器，在Ruby中

April 14, 2019

用于更改Rust的RFC
🌸命令行模糊查找器
Rust协议缓冲区的Rust实现
用于跟踪应用程序日志的artisan命令
💾用于Qiniu存储的Flysystem适配器。

April 13, 2019

微信支付宝个人免签收款系统
基于令牌桶算法和漏桶算法来实现的限速限流，Golang实现。
Parsedown的Markdown额外扩展
PHP DataMapper and ORM
Passbolt CE后端，一个用Cakephp编写的JSON API

April 12, 2019

关于视觉，文本，强化学习等中的pytorch的一组示例
人性化的API设计
ManaPHP Framework
用于使用CSS绘制图案的Web组件。
Google日志记录模块的C ++实现
DokuWiki开源Wiki引擎
基于MDX的演示文稿
通往计算机科学的免费自学教育之路！
Markdown 语法简体中文版
Rust每日新闻
此组件提供PHP 7.3之前的版本中不可用的功能。
PHP的FastCGI（FCGI）协议实现
🌈 Mix CLI — 让 PHP 像 Golang 一样开发命令行程序 (单执行文件)
【新】微信服务号+微信小程序+微信支付+支付宝支付
Git源代码镜像
Docker Desktop for Windows的错误报告

April 11, 2019

HTML5桌面应用程序开发通用工具
有关如何在keybase.io上创建GPG密钥的分步指南，将其添加到本地GPG设置并与Git和GitHub一起使用。
Keybase Go Library, Client, Service, OS X, iOS, Android, Electron
由前googlers，为前googlers  - 类似技术和服务的查找表
一个想帮你总结所有类型的上传漏洞的靶场
Laravel核心代码学习
连接，保护，控制和观察服务。
一个连接，管理和保护微服务的开放平台。
百宝门跨域单点登录(SSO)教程对应的示例完整代码
docker 教程 t.me/dockertutorial
Go语言实战: 编写可维护Go语言代码建议
996.ICU chrome 插件
Vanilla是一个功能强大的简单论坛，您可以轻松自定义，使其与您的社区一样独特。

April 10, 2019

轻松下载markdown文件中的所有图像
令人愉快且以性能为中心的纯CSS加载动画。
Crypto 101，关于密码学的入门书。
JavaScript 算法与数据结构
可以放在文档头部的所有内容的列表
一个基于Composer的简单PHP框架，看起来像一个小小的Laravel。
🔍基于一个唯一字符串生成数据库查询
人类的Python开发工作流程。
PHP中的一个简单的Podcast RSS编辑器
一个简单的PHP GitHub API客户端
go的现代文本索引库
✨强大、优雅的小程序异步库🚀 小程序promise
Apache Airflow
使用Vue CLI 3和Laravel的示例项目
饥人谷出品：一个会动的简历。
整理了一些在开发或学习过程中写的代码片段，并进行简单分类。
整理了一些在开发或学习过程中写的代码片段，并进行简单分类。
文件上传包含验证和存储策略
最初基于Carbon的独立DateTime库
允许轻松地将Ws Security标头添加到SOAP请求中
AutoRoute: 自动将HTTP请求映射到PHP操作类。
Query Translator是一个具有AST表示的搜索查询转换器
Jeff的算法手册，笔记等的错误跟踪
在GitHub上自动从标签，问题，标签和拉取请求生成更改日志。
如何维护更新日志
如果您构建软件，请保留更改日志。
PHP安全配置检查器
TinyLara/framework
JavaScript动画引擎

April 9, 2019

YApi 是一个可本地部署的、打通前后端及QA的、可视化的接口管理平台
YDoc 是一个更懂你的文档站构建工具，基于 markdown 轻松生成完整静态站点
WebAsssembly的独立JIT样式运行时，使用Cranelift
WebAsssembly的独立JIT样式运行时，使用Cranelift
构建Active Directory域并进行黑客攻击
Traefik是一个现代HTTP反向代理和负载均衡器，可以轻松部署微服务。Traefik与您现有的基础架构组件（Docker，Swarm模式，Kubernetes，Marathon，Consul，Etcd，Rancher，Amazon ECS ......）集成，并自动动态配置。在您的协调器上指向Traefik应该是您需要的唯一配置步骤。
Curl client for PHP HTTP
ThinkSNS的H5客户端文档。
自由门最新7.67版-无界19.02正式版下载
nginx源码中文注释版
机器学习》（西瓜书）公式推导解析
可能是让你受益匪浅的英语进阶指南
链家二手房租房在线数据，存量房交易服务平台数据，详细数据分析教程

April 8, 2019

《神经网络与深度学习》
GitLab API client for PHP 
印刷品般的漢字排版框架
Markdown 的 100 个骚操作（更新 100 年）
使用Biham和Kocher已知的明文攻击破解传统的zip加密。
自动更新正在运行的Docker容器
MySQL Log Analysis
爬取secwiki和xuanwu.github.io/sec.today,分析安全信息站点、安全趋势、提取安全工作者账号(twitter,weixin,github等)
用于WebShell Log Analysis的webshel​​l示例
v2ray的模板们
📄 中文排版 Composer 包
该软件包允许您使用固定窗口算法轻松创建和验证速率限制。
一个just-add-css集合的样式，使简单的网站更好一点
Fracker是一套工具，可以轻松跟踪和分析PHP函数调用，其目标是在PHP应用程序的手动安全评估期间协助研究人员。
Cacti是一个完整的网络图形解决方案，
Cacti是一个完整的网络图形解决方案
Kanboard是专注于看板方法的项目管理软件。
TypiCMS是一个使用Laravel 5.8构建的模块化多语言内容管理系统。开箱即用，您可以管理页面，事件，新闻，地点，菜单，翻译等。
laravel-source-analysis/
swoole source reading 源码分析
PHP中的M3U8解析器/转储器。

April 7, 2019

通过扫描MX记录，每日更新，清理和验证的一次性电子邮件域列表。
PHP扩展，以了解内存使用情况
Synology Audio Station / DS Audio的歌词插件
Yii 2扩展库，用于显示与Leaflet的交互式地图。
Laragon是一个可移植，隔离，快速且功能强大的通用开发环境，适用于PHP，Node.js，Python，Java，Go，Ruby。它快速，重量轻......
网络信息安全从业者面试指南
记录2019年社招面试过程中的一些问题，供大家参考，可以补充和指正，一起成长～
Editor.js的PHP后端
全栈Vue + Laravel + Axios CRUD示例

April 6, 2019

BootstrapVue为Vue.js提供了最全面的Bootstrap 4组件和网格系统实现之一，并且具有广泛的自动...
Laravel-Vue SPA入门项目模板。
一个高度自以为是的入门套件，用于使用Laravel和Vue.js构建单页应用程序
从PHP控制Chrome
PHP5.3 +路由类。轻巧但极其灵活。支持REST，动态和反向路由。
快速灵活的路由器
🎭Lava中的Laravel Translator课程！
🎩具有Lua，Markdown，HTTP / 2，QUIC，Redis和PostgreSQL支持的小型自包含pure-Go Web服务器
Iris是（THIS）地球上最快的社区驱动的网络框架。HTTP / 2，MVC等。为所有人提供无与伦比的免费支持。您的老式Web框架可以做到吗？
Iris 是一款超快、简洁高效的 Go 语言 Web开发框架。 Iris 功能强大、使用简单，它将会是你下一个网站、API 服务或者分布式应用基础框架的不二之选。
Standard Notes是一个简单的私人笔记应用程序，可在大多数平台上使用，包括Web，Mac，Windows，Linux，iOS和Android。
✨这包100多个gopher图片和元素将帮助您构建几乎任何与Go Programming Language相关的设计：演示文稿，博客或社交媒体中的帖子，课程，视频等等。
👾快速，简单，干净的视频下载器

April 5, 2019

FastDFS是一个开源高性能分布式文件系统（DFS）。它的主要功能包括：文件存储，文件同步和文件访问，以及高容量和负载平衡的设计。
qBittorrent BitTorrent客户端
完整而强大的PSR-14 EventDispatcher规范实现。
一个基于浏览器端 JS 实现的在线代理
一个用于图像中人脸检测的开源库。人脸检测速度可达1500FPS。
Shopify GraphQL设计教程
用于将标记化的PHP源代码转换为XML（以及可能的其他格式）的小型库
适合您项目的漂亮文档工具。

April 4, 2019

《React 学习之道》The Road to learn React (简体中文版)
用于PHP和Symfony的浏览器测试和Web爬网库
PHP项目/库的代码质量见解
通用第三方登录SDK，支持微信，微信扫码，QQ，微博登录，支付宝登录，Facebook，Line，Twitter，Google
用于与Ubiquiti的UniFi Controller API交互的PHP API客户端类
提供有关PHP对象图的有用操作
💱货币汇率库
Python脚本。模拟登录知乎， 爬虫，操作excel，微信公众号，远程开机
记录各种学习笔记(算法、Java、数据库、并发......)
用于运行WebAssembly二进制文件的PHP扩展。
应用程序仪表板和启动器
最先进的前端拖放页面构建器。以创纪录的速度创建高端，像素完美的网站。任何主题，任何页面，任何设计。
python爬虫教程，带你从零到一，包含js逆向，selenium, tesseract OCR识别,mongodb的使用，以及scrapy框架
用于生成Google站点地图XML文件的库
OOP代理包装器实用程序 - 生成和管理对象的代理
li 3是PHP的快速，灵活和最大的RAD开发框架。
 Go! AOP PHP - 面向切面编程的框架，用于新的软件开发水平
Parser Reflection API  - 提供源代码分析，无需将类加载到PHP内存中
允许反映对象属性，包括继承属性和非公共属性
微信开发者工具(微信小程序)linux完美支持
小米路由器Shell工具箱，本人自用，主要参考了小米的Misstar Tools制作，仅学习之用！
phar部署的例子
一个结构清晰的，易于维护的，现代的PHP Markdown解析器
GitHub 自动部署机器人
用于从PHP存档（PHAR）读取phar.io清单信息的组件
建立和管理Phars的申请。
Lisp到PHP编译器
PHP 依赖注入，从此不再考虑加载顺序

April 3, 2019

在触控栏中显示macOS Dock
聚合音乐Api，支持 node / android / ios / electron-render 调用
electron跨平台音乐播放器；可搜网易云、QQ音乐、虾米音乐；支持QQ、微博登录，云歌单; 支持一键导入音乐平台歌单
自己动手打造一个属于自己的直播间（视频直播、聊天室、弹幕、多端适配）
自己动手打造一个属于自己的直播间（视频直播、聊天室、弹幕、多端适配）
全栈Web开发笔记。
纯 Go 写的直播服务器
此项目用来提取收集以往泄露的密码中符合条件的强弱密码
C/C++面试基础知识总结 
下一代ls命令
大学生知识共享池
awesome window manager
TP生成静态站点类
基于Redis实现的延时队列服务，提供队列创建、删除及消息发送、接收、删除的操作。
phpstorm插件,用于thinkphp5框架的视图,配置,路由,数据库,模型智能提示和跳转
Web版中国菜刀
一天 30 秒 ⏱ 一段代码 ✍️ 一个场景 🖼
一个简洁优雅的图像识别转换文字的php类库, 须安装tesseract-ocr
Tesseract开源OCR引擎（主存储库）
数据字典自动生成文档 
📚 现代 Web 开发，现代 Web 开发导论 | 基础篇 | 进阶篇 | 架构优化篇 | React 篇 | Vue 篇
CrawlerDetect是一个PHP类，用于通过用户代理检测机器人/爬虫/蜘蛛

April 2, 2019

猫抓 chrome媒体嗅探插件
一个强大的Javascript库，用于捕获键盘输入。它没有依赖关系。
生成真正的随机用户代理
Mercure Component允许使用Mercure协议轻松地将更新推送到Web浏览器和其他HTTP客户端。
测试技术资源
Rust的Actor框架
运行维基百科的协作编辑软件。这是gerrit.wikimedia.org的一面镜子。
Flash OS映像到SD卡和USB驱动器，安全，轻松。
PHP中的Mustache实现。
Nmap扫描、漏洞利用脚本 
弱口令,敏感目录,敏感文件等渗透测试常用攻击字典
一个开源的网址导航网站项目，您可以拿来制作自己的网址导航。
根据关键字与 hosts 生成的关键词，利用 github 提供的 api，监控 git 泄漏。
暗网中文网监控爬虫
适用于PHP的腾讯云API 3.0 SDK
PHP中异步编程的资源列表
生成identicon头像,头像图片生成
PHP开发人员的一些工具。
 客户管理的前沿创新-悟空CRM
phpseclib  -  PHP安全通信库
爱百应，百度云网盘搜索引擎，爬虫+网站 

April 1, 2019

Markdown 简体中文与西文混排要点
Beanbun 是用 PHP 编写的多进程网络爬虫框架，具有良好的开放性、高可扩展性，基于 Workerman。
基于借方和贷方原则的余额会计（簿记）制度
swoole php多进程管理
缺少它的网站的RSS源
RSS-Bridge是一个PHP项目，能够为没有网站的网站生成RSS和Atom提要。
使用REST API包装自定义SQL数据库
十一月二十二日 题：在哈尔滨（一） 在这每一个寒冷又寂寞的夜晚 我窝藏在温暖沁人的被子里 那诞生的每一个轻浮的梦境中 我都能梦见一个美丽的花园 我梦见我的至亲们 我梦见我的挚友们 我梦见我的那个她 大家在花园里 都在听我的至亲们谈论着一个人 讲着他小时候的可爱 讲着他中学的稚嫩 到高中的叛逆以及大学的成熟 大家默默地听着 而她只是默默又含蓄地笑着 亲人们面向我 欢笑地讲述着过往 朋友们有说有笑地拍拍 我不是太宽大的肩膀 她伸手轻柔地捋捋 我被微风吹拂开的发丝 我一面微笑着 一手自然地靠在她的肩膀上 贴着她的脸颊 触碰着她的耳畔与发梢 我们愉悦地沐浴在晚霞的余晖之中 沉浸在多姿多彩的霞光下 只有我望着天空呀在发着呆 我看到远处红色与褐色相接的地方 有一轮红色的太阳 身后蓝的发黑的天际上 …
百度云/百度网盘Python客户端
各种脚本 -- 关于 虾米 xiami.com, 百度网盘 pan.baidu.com, 115网盘 115.com, 网易音乐 music.163.com, 百度音乐 music.baidu.com, 360网盘/云盘 yunpan.cn, 视频解析 flvxz.com, bt torrent ↔ magnet, ed2k 搜索, tumblr 图片下载, unzip
Babun  - 你会喜爱的Windows shell
restful-api风格接口 APP接口 APP接口权限 oauth2.0 接口版本管理 接口鉴权

March 31, 2019

Linux命令大全搜索工具，内容包含Linux命令手册、详解、学习、搜集。
当···时发生了什么？
learning-rust
一款轻量级、功能强大的内网穿透代理服务器。支持tcp、udp流量转发，支持内网http代理、内网socks5代理，同时支持snappy压缩、站点保护、加密传输、多路复用、header修改等。支持web图形化管理，集成多用户模式。
Wizard是基于Laravel开发框架开发的一款开源项目（API）文档管理工具。
一些常用的 docker 镜像 
一个PHPer的升级之路 

March 30, 2019

Linters Runner for Go。比gometalinter快5倍。漂亮的彩色输出。只能报告新问题。误报率较低。Yaml / toml配置。
将html转换为图像，pdf或字符串
php 资源文件管理
🔑 🔓免费开源的科学上网工具
用于reCAPTCHA的PHP客户端库，这是一项免费服务，可以保护您的网站免受垃圾邮件和滥用。
阿里巴巴nacos配置中心-PHP客户端
北京市预约挂号统一平台挂号小助手
Yasumi是一个简单的PHP库，用于计算国定假日
一个超级简单的PHP超全局变量管理扩展
shell代码部署系统
用C编写的简单，高性能的PHP框架

March 29, 2019

一款简单易用、功能强大的混沌实验注入工具
RedisLock for PHP是一种同步机制，用于在有许多exe线程的环境中强制限制对资源的访问...
RedisLock for PHP是一种同步机制，用于在存在许多执行线程的环境中强制限制对资源的访问。锁旨在实施互斥并发控制策略。
kubernetes1.13集群部署文档，包括kubernetes、dashboard、coredns、ingress、metrics、ceph rbd、helm、harbor、jenkins等相关组件部署文档
黑箱应用故障注入和资源发现的攻击模式和原语词典。
《把时间当作朋友》
《我也有话要说》—— 普通人的当众讲话技能
基于swoole开发的通信引擎，在线聊天平台，前端集成layerim框架，swoole基于eayswoole框架，异步连接池，多进程，异步任务，独立httperserver api，websocket推送，重构使用swoft-cloud 进行微服务架构
php实现的aes, des, 3des加密解密类
GitHub 上一些有趣的 HTML 小游戏进行了汉化
通过python脚本修改本机id破解teamviewer(tv版本需要是14以下)
黑苹果长期维护机型EFI及安装教程整理
Google Chrome扩展程序，用于修改Google Chrome 55+的页面默认编码。
轻松多个美丽的简历，创造你有史以来最好的简历！用Vue和LESS制作。
科学上网插件的离线安装包储存在这里
Goodby CSV是一种高内存高效，灵活且可扩展的开源CSV导入/导出库。

March 28, 2019

go 项目设计文件
JsonMapper  - 将嵌套的JSON结构映射到PHP类
一个还不错的图床工具，支持Mac和Win、支持压缩后上传、添加图片或文字水印、多张同时上传、同时上传到多个云、右击图片文件上传、快捷键上传剪贴板截图、提供Mweb接口，目前支持的云有：七牛、阿里、腾讯、网易、京东、百度、又拍、青云、Ucloud、sm.ms、Imgur！
帮助发现和安装工具
检测PHP代码中的设计模式
具有令牌桶算法的PHP速率限制库
本项目致力于收集网上公开来源的威胁情报，主要关注信誉类威胁情报（如IP/域名等），以及事件类威胁情报。
基于SQLite构建的轻量级分布式关系数据库。
Mojito 是一个基于 Laravel, Vue, Element构建的后台管理系统。
Node.js 应用线上/线下故障、压测问题和性能调优指南手册（更新中...）
异步PHP
一个开源的二次元向的社区程序
在blade模板中轻松使用过滤器。
网站分析应用程序
一个简化版的 man 手册。
code cola是一个chrome扩展，用于直观地编辑在线页面的CSS样式。
tldr alfred workflow
PHP Client for tldr
the only cheat sheet you need cheat.sh
955 不加班的公司名单
中文版 Awesome VS Code
PHP Excel Helper  - 基于PhpSpreadsheet以简单的方式编写和读取电子表格

March 27, 2019

通过与GitHub和GitLab的webhook集成增强Composer Satis
用于golang的socket.io库，一个实时应用程序框架。
Golang gRPC中间件：拦截器链接，身份验证，日志记录，重试等。
《Flutter实战》电子书
一本侧重于Go语言语法和语义的编程解释和指导书 
“996”工作制，即每天早9点到岗，一直工作到晚上9点。每周工作6天。

March 26, 2019

Pornhub模式标志生成器
本文介绍的是利用学生身份可以享受到的相关学生优惠权益，但也希望各位享受权利的同时不要忘记自己的义务，不要售卖、转手自己的学生优惠资格，使得其他同学无法受益。
CrazyCodes's blog
用于消费来自任何Broker的消息的lib
Twill是Laravel的开源CMS工具包，可帮助开发人员快速创建直观，强大且灵活的自定义管理控制台。在Spectrum上与我们和其他人聊天！
用于swarrot集成的symfony包
Debian，Ubuntu和CentOS的OpenVPN road warrior安装程序。
用于管理Kong网关的仪表板
脚本集，关于CSP（内容安全策略）的想法
Collections Abstraction Library
谷歌reCaptcha模块形成Magento2。
基于Flexbox的现代CSS框架
微小的WebSockets
php富文本过滤类，XSS Filter 
各种安全相关思维导图整理收集
laravel 即插即用的b2c商城扩展。
这是PHP CodeSniffer的一组嗅探，用于检查PHP跨版本兼容性。它将允许您分析代码以与PHP的更高版本和更低版本兼容。
带有可插拔后端的JWT登录微服务，如OAuth2，Google，Github，htpasswd，osiam，..
Netflix Eureka服务器的PHP客户端。支持所有Eureka REST操作。
（Spring Cloud）Netflix Eureka服务注册和发现的PHP客户端。http://hamid.work
图说设计模式
一个好玩的Web安全-漏洞测试平台
PHP7扩展开发 系列教程
当你ssh时带上你的.bashrc，.vimrc等
每周为你提供高质量的关于小程序、h5等前端领域的文章和项目
北京大学课程资料整理
北京邮电大学计算机考研信息汇总

March 25, 2019

phan是一个静态语法兼容性工具，它可以分析语法是否符合指定php版本，并将结果输出到指定文件。
一款离线，高颜值的🍅工作软件，二十五分钟专注做一件事⭐️。
用NodeJS解析纯真IP库(QQwry.dat) 支持IP段查询
前端小测答题收集专用
GoAccess是一个实时网络日志分析器和交互式查看器，可以在* nix系统中的终端或通过浏览器运行。
将Nginx log_format转换为goaccess配置文件
自动SQL注入和数据库接管工具
用于（反）序列化任何复杂数据的库（支持XML，JSON，YAML）
人人都能学会的 WordPress 实战课
在几秒钟内获得一个干净的，随时可用的Linux盒子。
记录自己学习TensorFlow的参考资料、笔记和代码
以图搜图
微信域名拦截检测、QQ域名拦截检测、360域名拦截检测、域名Whois查询
PhpBoot 是为快速开发微服务/RESTful API 设计的PHP框架。它可以帮助开发者更聚焦在业务本身, 而将原来开发中不得不做, 但又重复枯燥的事情丢给框架, 比如编写接口文档、参数校验和远程调用代码等。

March 24, 2019

Ludwig是一个基于TensorFlow构建的工具箱，可以训练和测试深度学习模型，而无需编写代码。
Web版中国菜刀
一个轻量级的包消息
微信mac/ipad协议，webapi封装好的实现方案，免IIS一键部署。 可实现微信80%功能；支持62数据登录、扫码登录、收发朋友圈、查看朋友圈、微信建群、微信拉人进群、微信公众号阅读、微信消息收发、微信附近的人定位、微信添加好友、微信红包接收、微信防撤回、分享小程序、微信加粉、微信收藏、微信标签等
轻松地异步运行代码
该项目的目的是在PHP中创建DNS记录的抽象对象表示。该项目包含代表DNS对象的各种类（如Zone，ResourceRecord和各种RData类型），用于将BIND样式文本文件转换为PHP对象的解析器，以及用于创建美观BIND记录的构建器。
此组件为Intl扩展（IDN功能）提供部分本机PHP实现。

March 23, 2019

mac 极简的开发环境 valet
OBS Studio  - 用于直播和屏幕录制的免费开源软件
适用于Linux，BSD和OSX的快速轻量级日志处理器和转发器
🦔快速，轻量级和无架构的搜索后端。Elasticsearch的替代方案，可在几MB的RAM上运行。
PHP徽章，使用包装信息为您的自述文件提供一些徽章。
收集&推荐优秀的 Apps/硬件/技巧/周边等
PHP的调试栏

March 22, 2019

Go 语言中文网 | Golang中文社区 | Go语言学习园地 源码
GCTT Go中文网翻译组。
阿里云容器服务持续交付
记录成长的过程
rust 协程库
从markdown文件创建书籍。像Gitbook一样，但在Rust中实现
这可能是yii2中最好用的微信SDK
在渗透测试中快速检测常见中间件、组件的高危漏洞。
在线子域名信息收集工具
在线子域名信息收集工具
Laravel GraphQL Server
这是ZipArchive方法的一个简单包装器，带有一些方便的功能
📦MacApp Store命令行界面
Hexo的管理界面
Beanstalk队列服务器的管理控制台
从Laravel应用程序创建静态站点包
您的基础架构作为GraphQL服务
根据Mix清单添加预加载和预取链接
XHProf是PHP的功能级分层分析器，具有简单的基于HTML的用户界面。
PHP非侵入式监控平台- 优化性能，定位Bug的神器，别再让你的PHP程序裸奔。
在MongoDB上构建的XHProf数据的图形界面
适用于PHP 7的现代XHProf兼容PHP Profiler
用于macOS上的v2ray-core的GUI
V2rayU,基于v2ray核心的mac版客户端,

March 21, 2019

下一代前端统一框架 - 支持桌面Web、移动H5和小程序
慢雾安全团队知识库
一种用于企业自托管的开源文档管理工具。
Python中的张量和动态神经网络，具有强大的GPU加速功能
中文近义词工具包
使用PHP Annotations声明GraphQL API
懒人博客，前后端分离，Vue+Beego Restful api 开箱即用，部署简单，后台管理系统简洁美观。
Golang实现的基于beego框架的接口在线文档管理系统
根据Mix清单添加预加载和预取链接
准确率99.9%的ip地址定位库
Sublime Text颜色方案已准备好用于下一代JavaScript语法
中文开源字体集 
轻松存储一些值
中国远程工作资料大全
PHP 5.3+的高度自以为是的模拟框架
Y分钟学习X种语言

March 20, 2019

PHP阿里巴巴云客户端的官方存储库
Go 每日阅读和 Go 夜读 
JIKEFM - 即刻电台📻
php开源商城系统，基于swoole、easyswoole框架开发
phalcon-oauth2-server
开源Spotify客户端库
easywechat for thinkphp支持
一个免费且符合道德标准的照片共享平台，由ActivityPub联合提供支持。
使用相同的Laravel安装运行多个网站，同时保持租户特定数据分离，以实现完全独立的多域设置。
建立在laravel for all上的电子商务框架，用于构建和扩展您的业务。

March 19, 2019

Caffe：深度学习的快速开放框架。
用户友好的命令行shell。
一个完整的PHP操作工具
*nix系统管理员测试问题和答案的集合。
微信公众号排版编辑器，转换 Markdown 到微信特制的 HTML
为比特币社区提供的一组资源。
📆使用HTML模板的jQuery日历插件
LaraCMS 后台管理系统
LaraCMS Framework。—— LaraCMS 核心基础框架，配合 LaraCMS 使用。
PHP高级工程师面试题汇总(2018.05)
基于swoole实现的微信机器人，依赖vbot和微信网页版的功能，帮助管理微信群/聊天/踢人等
在Composer运行时合并一个或多个其他composer.json文件
🍀本地git统计信息，包括类似GitHub的贡献日历。
油猴脚本 直接下载百度网盘和百度网盘分享的文件,直链下载超级加速
创建包含个人数据的zip文件
超级速查表 - 编程语言、框架和开发工具的速查表，单个文件包含一切你需要知道的东西 ⚡️
超赞的 Linux 软件
tree-ql是一个laravel扩展,通过简单的配置构建出一套极具描述性,可读性,且没有任何冗余的高性能API.
具有AutoCompletion和语法突出显示的MySQL终端客户端。
带有详细注释的 yii2 2.0.3 代码。喜欢的话请点star，欢迎大家一起来补充
构建自己的PHP框架
A simple PHP framework 构建自己的PHP 框架代码示例
NideShop：基于Node.js+MySQL开发的开源微信小程序商城（微信小程序） nideshop.com
企业仓库管理系统
php仓库进销存

March 18, 2019

ClickHouse是一个用于大数据的免费分析DBMS。
在浏览器中播放生成音乐的平台。
Godot Engine  - 多平台2D和3D游戏引擎
🚰用于检测代码和测试中的内存泄漏的PHPUnit插件
roave / dont是一个小型的PHP软件包，旨在实现设计防御性代码时的良好实践
Github信息泄漏监控系统
PHP7的简洁并行并发API
JSON-Schema +假数据生成器
多框架编写器库安装程序 
Composer安装程序扩展程序
支持（Laravel / Lumen / PSR-15 / Swoft / Slim / ThinkPHP） -  PHP CORS（跨源资源共享）中间件。
由libsodium提供支持的高级加密接口
apache/logging-log4php
用于检测用户浏览器的PHP类
一个轻量级的PHP分页器
通过生成包含所有自动加载文件的单个PHP文件来优化类加载性能。

March 17, 2019

输入检查JSX for Rust
小练习让你习惯阅读和编写Rust代码！
最近对Go语言很感兴趣，所以整理相关资料，呵呵
适合所有人的PHP数据库迁移

March 16, 2019

OPNsense GUI，API和系统后端
基于Lua的跨平台构建实用程序
Element UI 中国省市区级联数据
linux内核网络协议栈源码阅读分析注释--带详尽中文分析注释以及相关流程分析调用注释，对理解分析内核协议栈源码很有帮助
开源GraphQL CMS
拼多多API SDK【拼多多开放平台】
腾讯AI开放平台 【Tencent AI open platform】
该库提供了围绕数学运算的工具。
php的公式解释器
PHP Redis Cache缓存策略技术
A web framework for Rust.
draw.io是一个在线图表网站，提供此项目中的源代码。
一个用于图像中人脸检测的开源库。人脸检测速度可达1500FPS。
代码信息的统计程序 

March 15, 2019

自学是门手艺
WebSVN  - 在线subversion存储库浏览器
用于定义状态的轻量级库
使用Vega，您可以使用JSON格式描述数据可视化，并使用HTML5 Canvas或SVG生成交互式视图。
用于与Pusher Channels HTTP API交互的PHP库
💦 微博系统实现
基于 Laravel 可灵活自定义的的私人微信机器人，能够实现如：拜年群发自动回复、消息转发、防撤回、暗号加好友、甚至留言统计等功能
简单的闪光通知
将PHP数据转换为JavaScript。
Lua http restful api框架
MySQL入门教程（MySQL tutorial book）
PHP cron表达式解析器可以解析CRON表达式，确定它是否应该运行，计算表达式的下一个运行日期，以及计算表达式的上一个运行日期。您可以跳过n个匹配日期来计算远期或过去的日期。

March 14, 2019

终端的系统监控仪表板
Redis多线程叉
PHP 7+付款处理库。它提供了处理付款所需的一切：信用卡和异地购买，订阅，支付等 - 由Forma-Pro提供
用于PHP的HTML to Markdown转换器
一个关于人工智能渗透测试分析系列
weblogic 漏洞扫描工具
上传漏洞fuzz字典生成脚本
OpenCV的PHP扩展
OS X的全局鼠标手势
连接所有Kubernetes集群，无论它们在世界的哪个位置。
Markdown解析器，做得对。100％CommonMark支持，扩展，语法插件和高速
将任何使用STDIN / STDOUT的程序转换为WebSocket服务器。像inetd一样，但对于WebSockets。
Overlord是哔哩哔哩基于Go语言编写的memcache和redis&cluster的代理及集群管理功能，致力于提供自动化高可用的缓存服务解决方案。
github泄露扫描系统
为CentOS / Debian / Ubuntu自动安装Shadowsocks Server
关于交叉编译Rust程序需要了解的一切！
莲米粒是一个基于PHP+MySQL+微信小程序技术栈的、拥有用户登入、发布、修改、删除和转发信息、以及私信聊天模块的信息流应用。
Laravel 5 系列入门教程 https://github.com/johnlui/Learn-Lara…
基于php和bash的离线下载神器 http://goxz.gq
EleTeam开源项目-电商全套解决方案之PHP版-Shop-for-PHP-Yii2。一个类似京东/天猫/淘宝的商城，有对应的APP支持，由EleTeam团队维护！
覃健祥的学习笔记，各种几十分钟入门的文档
回到阅读的乐趣 -  Arc90的可读性PHP端口
基于GIT的应用程序的现代笔记，不需要本地GIT环境。gitnoteapp.com
《大话设计模式》php版本
纯PHP实现GraphQL协议
阿波罗11号制导计算机（AGC）中指令模块（Comanche055）和登月模块（Luminary099）原始代码。

March 13, 2019

SQL优化器和重写器
一个简单的腾讯企业邮箱SDK
QueryPHP 是一款现代化的高性能 PHP 7 常驻框架
rust 网络代理和隧道（VPN）
ngx_php7  - 用于nginx模块的嵌入式php7脚本语言。ngx_php的主线开发版本。
旨在通过分析企业信息安全建设过程中的心路历程
PSR-7和PSR-15 OpenAPI验证中间件
轻松安全地管理crontab文件
下一代ShadowsocksX
开源运维平台：帮助中小型企业完成主机、任务、发布部署、配置文件、监控、报警等管理(
Jamlee/coroutine: php nonpreemptive multipletasking
rust 一个模块化工具包，用于使用Rust和Wasm构建快速，可靠的Web应用程序和库
rust 从命令行轻松安全地共享文件。功能齐全的Firefox发送客户端。
TypeScript 入门教程
精通比特币（第二版）-- 区块链编程
使用Nginx+Lua实现自定义WAF

March 12, 2019

使用Yii 2的嵌套集的高级树管理模块。
保持应用程序设置同步（OS X / Linux）
shell十三问--shell教程（markdown 版本）
SecurityManageFramwork是一款适用于企业内网安全管理平台，包含资产管理，漏洞管理，账号管理，知识库管、安全扫描自动化功能模块，可用于企业内部的安全管理。 本平台旨在帮助安全人员少，业务线繁杂，周期巡检困难，自动化程度低的甲方，更好的实现企业内部的安全管理
rust style and philosophy
PHP的轻量级HTTP客户端
通过SSH擦除并重新安装正在运行的Linux系统，而无需重新启动。你知道你想。
大家一起被捕吧计划
编译器。对于PHP
PHP中的PHP VM实现
用于生成随机数和字符串的库
Easily manage git hooks in your composer config
在您的composer配置中轻松管理git钩子。此命令行工具可以轻松实现git挂钩的一致项目范围使用。
用于PHP的AST可视化工具
Medis是一款美观，易用的Redis数据库管理应用程序。

March 11, 2019

Google Chrome（和Chromium）的扩展程序，可以反转网站的亮度。
在本地运行Kubernetes
一个简单的服务器，用于每个Web套接字实时发送和接收消息。（包括时尚的web-ui）
精选的前端常见面试问题集
Box应用程序简化了PHAR构建过程。
CentOS 7 安装 LNMP 环境

March 10, 2019

将Figma帧转换为Google幻灯片演示文稿🍭
一个简单的包解析PHP中的Github Flavored Markdown
Lepton是一个基于GitHub Gist的精简代码片段管理器
具有多字节支持的PHP字符串操作库
3y原创技术文章导航
Acme PHP是一个简单但非常可扩展的CLI加密器CLI客户端，可帮助您获取和续订免费的HTTPS证书。
PHP多版本安装和管理
超级微信电脑客户端，支持多开、防消息撤销、语音消息备份...开放WeChatSDK
YOURLS是一组PHP脚本，允许您运行自己的URL Shortener。您可以完全控制数据，详细统计信息，分析，插件等。它是免费和开源的。
官方GitHub API的Ruby客户端。
GitHub Desktop
SketchI18N是Sketch.app的多语言插件

March 9, 2019

📒《Node.js实战：使用 Egg.js + Vue.js + Docker 构建渐进式、可持续集成与交付应用》 源码
第三方Jike app chrome扩展。
2018 JDDC对话大赛亚军解决方案
欢迎来到以太坊Wiki！
Opulence是一个PHP Web应用程序框架，它简化了创建和维护安全，可扩展的网站的难度部分。
简单快速的HTML解析器
网页版 PS
一个简单的静态http服务器
使用电子和vue.js制作的简单RSS阅读器应用程序
FEX 面试问题
GLPI是一个免费资产和IT管理软件包，ITIL服务台，许可跟踪和软件审计。
基于社区的GPL许可网络监控系统
Spala（SPA + Lalavel）。适用于Laravel和Vue开发人员的现代轻量级CMS（开源项目）。
The most awesome Powerline theme for ZSH around!

March 8, 2019

该软件包为Laravel 5.8提供了与FFmpeg的集成。文件的存储由Laravel的Filesystem处理。
最适合入门的laravel初级教程
用于动态编辑.env文件的Laravel包。
用于动态编辑.env文件的Laravel包。
python各大网站登陆方式与一些简单的爬虫
NGiИX config generator on steroids
分析正在运行的容器的资源使用情况和性能特征。
HttpClient组件提供了同步或异步获取HTTP资源的强大方法。
高仿书旗小说 Flutter版，支持iOS、Android
skl api，企业级后台API开发平台。使用beego语言架构。开发平台内嵌了用户、用户组、机构、角色、权限、多语言、枚举、OA引擎等功能模块。
Welcome！Hello YCY
MailEclipse⚡️轻松玩你的Laravel Mailables！
✏️了解如何用C编写哈希表
BetterAndBetter 是一款包含很多功能的 macOS 软件

March 7, 2019

纯bash脚本，用于测试和等待TCP主机和端口的可用性
各种滑动验证码识别 [腾讯云] [易盾] [Vaptcha] [Geetest] [极验] 各种网站破解 lengyue.me
一种开源可信云本机注册表项目，用于存储，签名和扫描内容。
CentOS7服务器的一些配置
维护的ctags实现
具有GPL许可证的高性能MySQL代理。
一个极简的基于swoole常驻内存框架，支持在fpm下运行
学习强国 懒人刷分工具 自动学习
一个视频播放器，开源版 potplayer ，用于学习和交流音视频技术
为通过Composer管理的每个PHP项目创建简单的phar
一个基于ThinkPHP5.1+和AmazeUI的快速后台开发框架
Googletest  -  Google测试和模拟框架
Lua + libUV + jIT = pure awesomesauce

March 6, 2019

正在搭架子 作为公共分享资料
README的艺术
PHP Protobuf  -  Google的PHP协议缓冲区
PHP 7的异步协程。
功能丰富的跨平台传输BitTorrent客户端。 比内置Web GUI更快，功能更多。
一个易于使用的库，用于使用InfluxDB和PHP。
一种向类动态添加方法的特性
htrace.sh是用于http / https故障排除和分析的shell脚本。它也是围绕几个开源安全工具的简单包装脚本。
记录一下SS的前世今生，以及一个简单的教程总结
记录一下SS的前世今生，以及一个简单的教程总结
swover 一个基于Swoole扩展的服务器框架
远程运行VS代码。coder.com
以开发人员为中心的HTTP客户端，针对大多数常见用例进行了优化。

March 5, 2019

🔥 让阅读变成一件有意义的事。Golang好文推荐；收录平时阅读到的一些Go相关写的比较好、质量较高的干货文章.
为互联网IT人打造的中文版awesome-go
适用于npm和GitHub的免费，快速，可靠的开源CDN
HTTP，HTTPS，WebSocket调试代理
机器学习100天
🚄用于PHP的快速生成对象Hydrator
为PHP实现XDG基本目录规范
根据网易云音乐的歌单, 下载flac无损音乐到本地.。
Go client for Redis
HTML5视频速度控制器（适用于谷歌浏览器）
Oracle数据库的应用程序和工具使用示例
PPGo_Job是一款可视化的、多人多权限的定时任务管理系统，采用golang开发，安装方便，资源消耗少，支持大并发，可同时管理多台服务器上的定时任务。
CamelCasePlugin for IDEA IDEs
EvaOAuth 1.0：统一接口的 OAuth 登录 PHP 类库
EvaOAuth 1.0：统一接口的 OAuth 登录 PHP 类库
EvaThumber : 基于URL的图片处理库 (可实现缩略图 | 二维码 | 水印 | 面部识别等,基于PHP
Vita：简单快速的VPN网关
队列论：软件开发简介
开源数字标牌解决方案
微信调试、API调试和AJAX的调试的工具，能将日志通过WebSocket输出到Chrome浏览器的console中

March 4, 2019

提供多款 Shadowrocket 规则，带广告过滤功能。用于 iOS 未越狱设备选择性地自动翻墙。
《大话设计模式》php版本
一个简单的脚本，用于在PHP中缓存第三方API调用
轻量级，简单但功能强大的PHP5缓存类，它使用文件系统进行缓存。
程序员技能图谱 
jSearch(聚搜) 是一款专注内容的chrome搜索扩展，一次搜索聚合多平台内容。
100行Python代码快速获得一个代理池，两分钟获得数千个有效代理

March 3, 2019

Webpack的演示与课程4
Go中的DNS客户端，通过HTTPS支持Google DNS
🥐一个Lua REPL和调试器
search and download music 从网易云音乐、QQ音乐、酷狗音乐、百度音乐、虾米音乐等搜索和下载歌曲
An official read-only mirror of http://hg.nginx.org/unit
Live2D 看板娘插件 (fghrsh.net/post/123.html) 上使用的后端 API live2d.fghrsh.net
Live2D 看板娘插件 (fghrsh.net/post/123.html) 的前端 HTML 源码 live2d.fghrsh.net
在Go中发送电子邮件的最佳方式。
库用于清理和清理网页以创建大量数据集。
用Lua编写的Nginx的Prometheus度量库
📦📦安装直接在浏览器中运行的npm依赖项。无需Browserify，Webpack或导入地图。
分布式系统讲座系列的课程材料
分布式系统讲座系列的课程材料
用于构建高效数据科学工作流的Python库
关于python的面试题
预制工厂采用faker方法建议，以提高生产力
The Hoa\Fastcgi library. 
该死的易受攻击的Web应用程序（DVWA）
mailcow: dockerized 搭建一个满分的自有邮箱服务

March 2, 2019

用Lua编写的Nginx的Prometheus度量库
PHP 最优秀资源的整理汇集
网络资产发现引擎
PHP的图像处理库
如果将markdown视作一门编程语言可以做哪些有趣的事情呢?
鲜亮的高饱和色彩，专注视觉的小程序组件库 
个人准备渗透测试和安全面试的经验，和部分厂商的面试题
个人准备渗透测试和安全面试的经验，和部分厂商的面试题
NexT 是一个高质量并且优雅的Hexo 主题。这是精心制作做出来的 hexo 主题。
这是一个webshel​​l开源项目
这是一个webshel​​l开源项目
中国运营商IP地址库-每日更新
Keybase Go Library, Client, Service, OS X, iOS, Android, Electron
打造优质前端博客，欢迎关注我的公众号：前端工匠
Delve是Go编程语言的调试器。
Elasticsearch 可视化DashBoard, 支持Es监控、实时搜索，Index template快捷替换修改，索引列表信息查看， SQL converts to DSL等
Frp配置面板 
HTPC / Homelab服务管理器 - 用PHP编写
报告PHPUnit测试套件中运行缓慢的测试
devdocs.io的Alfred工作流程
适用于DevDocs.io的全功能桌面应用

March 1, 2019

用于JavaScript的GitHub REST API客户端
📗[WIP] Laravel 应用部署上线课程系列。
程序猿成长计划
用于GraphQL订阅的WebSocket客户端+服务器
  redis-full-check是阿里云Redis&MongoDB团队开源的用于校验2个redis数据是否一致的工具，通常用于redis数据迁移（redis-shake）后正确性的校验。
HTTPlug，PHP的HTTP客户端抽象
Go中的快速键值DB。
Gitter for GitHub - 可能是目前颜值最高的GitHub小程序客户端
一款手势驱动的裁图插件
EventEmitter3是一个高性能的EventEmitter。它已针对各种代码路径进行了微优化，使其成为Node.js和浏览器中最快的EventEmitter之一。
certbot'renewing letencrypt证书插件 - 自动验证aliyun / tencentyun / godaddy dns
防止通过表单提交的垃圾邮件
用于配置组装的Composer插件
阿里妈妈前端团队出品的开源接口管理工具RAP第二代
这是一个用于检测和解码QR码的PHP库。这是第一个也是唯一一个无需扩展即可工作的QR码阅读器。
Swoft从入门到微服务课程代码
Symfony之上的开源电子商务框架
利用curlmulti内置的IO事件循环实现，具备高性能、高通用性、高扩展性，尤其适合复杂业务逻辑大批量请求的应用场景。
The best php curl library.

February 28, 2019

一款屏幕保护软件
Node.js REST开发的未来
一个广泛的JavaScript和Node.js数学库
定制kubernetes YAML配置
mageMagick 7
在不到30秒的时间内获得零编码的完整虚假REST API（严重）
Goji是一个简约的Web框架，它重视可组合性和简单性。
⚡️由Spray-can，Netty，underow，jetty，Vert.x，Grizzly，node.js和Go实现的高性能websocket服务器。它支持1,200,000个有效的websocket连接
Laravel插入重复键并插入忽略
gRPC PHP客户端库
直观的查找和替换CLI（sed替代）
Package Repository Website 
UPX  -  eXecutables的终极包装工具
Go服务器的平滑重启和零停机时间部署。
用Rust写的Javascript引擎
PHP到JavaScript转换器和用JavaScript编写的VM
iOS上的Native App over HTTP
使用kubeadm在AWS上真正便宜的Kubernetes集群
互联网广告的黑洞
轻量级Kubernetes。易于安装，内存的一半，所有二进制文件都小于40mb。
所有PHP函数，重写为抛出异常而不是返回false
PHPGGC是一个unserialize（）有效负载库，以及一个从命令行或以编程方式生成它们的工具。
用于商业用途的开源电子商务laravel框架
用于商业用途的开源电子商务laravel框架
imagecolormatch（）OOB堆写入漏洞
Laravel的文件管理器
vue-laravel-file-manager
缺少的Spotlight插件系统
Php JWT example.
PHP的一个小而强大的REPL。

February 27, 2019

用于富文本编辑的世界上最流行的JavaScript库。可用于React，Vue和Angular
程序员如何优雅的挣零花钱
DDos/DoS工具集
Zero是一个简化Web开发的Web服务器。
存储性能开发套件
Lumen和Laravel 5的发电机集合。
在本地运行Kubernetes
基于 Flutter & scoped_model 实现的视频类App客户端
✨轻量级依赖注入
让你自定义的方法也可以使用依赖注入.
由WebAssembly提供支持的通用二进制文件
通过Tensorflow JS在客户端进行NSFW检测

February 26, 2019

libp2p网络堆栈libp2p.io的技术规范
每个软件开发人员应该知道的（大多数）技术事项的集合
timeago.js 是一个非常简洁、轻量级、不到 1kb 的很简洁的 Javascript 库，用来将 datetime 时间转化成类似于*** 时间前的描述字符串，例如：“3 小时前”。
📅 中国农历（阴历）与阳历（公历）转换与查询工具
多框架编写器库安装程序http://composer.github.com/installers
OSS Browser 提供类似windows资源管理器功能。用户可以很方便的浏览文件，上传下载文件，支持断点续传等。
hexo主题的hub样式复制
ShowDoc是一个非常适合IT团队在线共享文档的工具
Rust箱提供有关电池的跨平台信息。
针对seniverse api的Api使用演示http://www.seniverse.com/doc
极客时间：nginx核心知识100讲配置文件与代码分享
Swoole 远程调试器
120个常见数据科学面试问题的答案。
Istio knowledge map 知识图谱
自动将字幕与视频同步。
PHP中的设计模式示例
Doctrine Inflector是一个小型库，可以对大写/小写和单数/复数形式的单词执行字符串操作。
PHP应用程序的即时升级getrector.org
PHP中强类型的枚举支持自动完成和重构
PHPGGC是一个unserialize（）有效负载库，以及一个从命令行或以编程方式生成它们的工具。
Html菜单生成器

February 25, 2019

rust stackful coroutine library
基于Bulma的Vue.js的轻量级UI组件
基于生成器的PHP集合
Swoole 提案
简单，灵活的多主机容器网络等。
适用于Linux的采样CPU分析器
IPv4和IPv6用户空间网络堆栈
键入时格式化输入文本内容...
适用于iOS 11.0  -  12.1.2的unc0ver越狱
一个框架不可知的PHP库，用于构建聊天机器人
🚀 一个为任何MySql数据库生成REST API的命令。
输出复杂，灵活的AJAX / RESTful数据结构。
将任何数据库转换为API平台。

February 24, 2019

一个基于Yii2高级框架的快速开发应用引擎 
🕹macOS http://openemu.org的复古视频游戏模拟
防止Mac进入睡眠状态。
macOS的通用纯文本编辑器。

February 23, 2019

基于Google Material Design的Bootstrap 4，React，Vue.js，React Native和Sketch的免费开源UI工具包
macOS的剪贴板扩展应用程序。
Spring Boot 教程、技术栈示例代码，快速简单上手教程。
70万条对联数据库。
每秒解析千兆字节的JSON
平滑滚动网页
常见数据结构的CRDT，如map，vecs，sets，text和JSON
结合实际PHP面试遇到的问题，尝试提供简洁准确的答案
检测未使用的编写器依赖项
用于创建Web应用程序的Rust框架
资源搜索型软件 macOS OSX magnet
小米手机内核开源
SmartisanOS kernel opensource contain T1Kernel, T2Kernel, U1Kernel(JianGuo), M1Kernel(M1 and M1L), U2ProKernel(JianGuo Pro)
可扩展的数据存储区，用于指标，事件和实时分析
⭐️一系列精彩的列表，手册，博客，黑客，单行，cli / web工具等等。
TypeScript Web服务器 - 比Deno快15倍
百度网盘下载神器
企业级持续交付和DevOps自动化开源平台
为任何Eloquent模型及其关系创建修订版
中国大陆 2018 年 12 月XX站访问百强榜单
The HTTP client for Vue.js
基于Vue和WeUI的移动UI组件
WebSockets的命令行客户端
Pretzel是Mac桌面应用程序，可根据您当前的应用程序显示和搜索键盘快捷键。

February 22, 2019

在几秒钟内创建HTML演示文稿
现代JavaScript教程
用于编程工具的增量解析系统
《Go2编程指南》开源图书
ngx_php - 用于nginx模块的嵌入式php脚本语言。
为药物发现，量子化学，材料科学和生物学进行民主化深度学习
编写理智，可维护和可扩展Sass的指南。
所有事情的黑暗主题！
Go的理想精炼Web框架。
Rust中一个相对简单的Datalog引擎
Google常用的Java，C ++和JavaScript库，用于解析，格式化和验证国际电话号码。
MarkDown在线简历工具，可在线预览、编辑和生成PDF。[此项目已不再维护，建议使用 cv.ftqq.com 替代 ]
精选的软件和架构相关设计模式列表。
设计模式的超简化解释
在javascript中实现的设计模式的超简化说明
使用JS插件转换样式
Rust的类型安全，编译类似Jinja的模板
每秒解析千兆字节的JSON
IntelliJ IDEA社区版
将设计模型转换为静态网站的神经网络
Rust中文社区 rustlang-cn.org
Rust的在线社区源码
Rustlang相关各种资料！！！
查看从Laravel中提取的PHP模板引擎
使用Python进行科学计算的基础包。
Rdebug  - 真正的调试器
为Laravel测试提供快速的数据库迁移。
一个简单的JSONP实现
jquery jsonp插件

February 21, 2019

Lua redis基于cosocket API的ngx_lua客户端驱动程序
Node.js 微信公众平台 API
工作日每天一道前端大厂面试题，祝大家天天进步，一年后会看到不一样的自己。
本仓库用于记录 B3log 系列站点被攻击的记录，不定期更新。
PHP异步编程: 基于 PHP 实(chao)现(xi) NODEJS web框架 KOA。
50 个有志向的开发者组成的精英圈 kebox.cn
TinyPNG client for Mac
快速Python 3.5+ HTTP工具包与基于uvloop和picohttpparser的流水线HTTP服务器集成。
Laravel中的模块管理
基于laravelS和layim的聊天系统
Go configuration with fangs
该项目基于CNN5 / DenseNet + BLSTM / LSTM + CTC实现验证码识别。
GPU加速C ++用户界面，具有WYSIWYG开发工具，XML支持，内置数据绑定和MVVM功能。
“用于云存储的rsync” -  Google Drive，Amazon Drive，S3，Dropbox，Backblaze B2，One Drive，Swift，Hubic，Cloudfiles，Google Cloud Storage，Yande ......
代码可以帮助您启动个人网站，展示您作为软件开发人员的工作。
Laravel代理包用于在负载均衡器或其他中介后面处理会话。
🔥 「干货集中营」是一款注重体验的 Gank.io 官方客户端
Gank api base △ next.js (react&ssr)
使用 Rust 创建 PHP 扩展
腾讯防水墙
Chrome插件英雄榜

February 20, 2019

渗透测试/APT模拟攻击
Yargs通过解析参数和生成优雅的用户界面来帮助您构建交互式命令行工具。
启动创始人的公开清单：“如果你被公共汽车撞到会怎么样？
平台不可知安全令牌
是安全无状态令牌的规范和参考实现。
由Laravel 5和Sentry提供支持的PHP CMS
Voten.co是一个开源，美丽，高度可定制但致命的简单，温暖的社区。
用于了解应用程序安全性的精选资源列表
PHP 5.x支持random_bytes（）和random_int（）
现代网络的安全内容管理 - “天空只是开始”
使用Slim Framework构建的仅用于公共附加的分类帐微服务
由libsodium提供支持的高级加密接口
PHP项目的快速，可搜索的字段级加密
全功能的反CSRF库
在经过身份验证的加密中包装Bcrypt-SHA2
GnuPG加密的电子邮件变得简单
用于phpseclib的简单安全包装器
易于使用的PDO包装器，适用于PHP项目。
异常和错误使用户更友好
安全API工具包
中文 man 手册页计划
无扩展PHP图形用户界面库
🌌LaravelNova的语言支持。随意提交您的语言或更新现有语言！
具有Web UI的跨平台http嗅探器
最权威最完整的中国省市县数据
CodeReview是一个Git GUI工具，用于执行用Python3和Qt5编写的代码审查（Diff Viewer）。
哪个是最快的Web框架？
将本地端点公开给Internet
蓝天采集器是一款免费的数据采集发布爬虫软件，采用php+mysql开发
Tracy：易于为酷开发人员调试PHP代码的令人上瘾的工具。 友好的设计，日志记录，分析器，高级功能，如调试AJAX调用或CLI支持。
ZoneMinder是一款免费的开源闭路电视软件应用程序，专为Linux开发，支持IP，USB和模拟摄像机。
SQLite通过Emscripten编译为JavaScript
Cordova / PhoneGap插件，可在Android，iOS和Windows上使用HTML5 / Web SQL API打开和使用sqlite数据库

February 19, 2019

easy-swoole/demo
Enterprise application cloud operating system(企业应用云操作系统)
用于C / C ++ / Golang的微型跨平台webview库。使用WebKit（Gtk / Cocoa）和MSHTML（Windows）
一个基于Laravel的开源论坛。
用于Spatie laravel许可库的Laravel Nova工具
Package Management for Golang
Quill是一个现代WYSIWYG编辑器，专为兼容性和可扩展性而构建。
WebRTC Web演示和示例
 一个小巧、轻量的浏览器内核，用来取代wke和libcef
用于生成和验证Google身份验证器双因素身份验证的PHP类
[全文]如何正确的学习Node.js
PHP Protobuf  -  Google的PHP协议缓冲区
该软件包提供了在Elasticsearch中搜索和过滤数据的高级功能。
概述机器学习概念的思维导图，从数据分析到深度学习。
这是一个 Nginx 极简教程，目的在于帮助新手快速入门 Nginx。
社工库半自动处理

February 18, 2019

nginx源码中文注释版
IPIP.net正式支持IP数据库ipdb格式解析库
使用docker快速搭建各大漏洞学习平台，目前可以一键搭建12个平台。
浏览器中增强的电子书。
.files，包括〜/ .macos  -  macOS的明智的黑客默认值
我的dotfiles（由LARBS部署）
PostgreSQL的过程语言PHP
📚 C/C++面试基础知识总结
[WIP] PHP Service Bus（发布 - 订阅模式）实现

February 17, 2019

一种利用深度学习技术识别和交换图片、视频中人物脸部图像的工具
交互式UI组件开发和测试：React，React Native，Vue，Angular，Ember
NAXSI是NGINX的开源，高性能，低规则维护WAF
用Go编写的概念证明OS内核
Keras模型从手绘网站模型生成HTML代码。实现图像字幕体系结构以绘制源图像。
Anki Vector  - 具有交互式AI技术的家庭机器人。
该项目通过python脚本从巨潮网络的服务器获取中国股市（sz,sh）的公告(上市公司和监管机构),把公告信息放到数据库，公告文件下载到本地，并支持网页查询和读取。
自动化检测小工具，主要实现了域名枚举、链接爬取、注入检测、主机扫描、目录枚举、敏感信息检测等功能~
Prometheus操作指南 
用于最佳安全实践的php.ini扫描程序
PoCBox - 漏洞测试验证辅助平台
解析和评估以字符串形式给出的数学公式。
适用于WordPress SEO的All in One SEO Pack插件
Laravel中的可排队行动
使用Google翻译自动翻译您的语言文件
将响应缓存为磁盘上的静态文件，以便快速加载页面。
Google 开源项目风格指南 (中文版) 

February 16, 2019

c++ 顺序表、链表、静态链表、队列、一元多项式、汉诺塔、火车调度问题、操作系统调度问题、背包问题、最大连续子列和问题、KMP算法、稀疏矩阵、广义表、并查集、无向图邻接表、有向图邻接表、Krusskal算法、Prim算法、最短路径Dijsktra算法、最短路径Bellman-Ford算法、最短路径Floyd算法、拓扑排序、关键路径、优化的冒泡排序、快速排序、直接插入排序、折半插入排序、闭散列实现、开散列实现
背包问题九讲
Go的Tiny WebSocket库。
用于Javascript的种子随机数生成器
API文档生成器
PHP的文档生成器
Git for Windows. 国内直接从官网下载比较困难，需要翻墙。这里提供一个国内的下载站，方便网友下载
一个简单而自以为是的包，用于向Laravel提供基于子域的多租户
示例云本机应用程序，包含10个微服务，展示了Kubernetes，Istio，gRPC和OpenCensus。提供用于说明和演示目的。
Mac应用程序，显示所有正在运行的进程正在使用的所有打开文件和套接字。很好的GUI用于lsof。
486行C ++：一个周末的老派FPS
网络范围的广告和跟踪器阻止DNS服务器
用于学习操作系统的简单内核
JavaScript中的x86虚拟化，在浏览器和NodeJS中运行
中文文案排版指北
PHP库允许从URL或html页面生成缩略图，快照或PDF。
使用基于HTTP的API，非常简单的按需图像处理库。
使用相同的Laravel安装运行多个网站，同时保持租户特定数据分离，以实现完全独立的多域设置。
Laravel 5  - 用于抽象数据库层的存储库
微博批量拉黑
Mockery是一个简单而灵活的PHP模拟对象框架，用于使用PHPUnit，PHPSpec或任何其他测试框架进行单元测试。
PHPUnit中文文档
用于API监控和管理的OpenResty / Nginx网关。
一个非常强大和友好的nginx基于lua-nginx-module（openresty），提供WAF，控制面板和仪表板。
VeryNginx 是一个功能强大而对人类友好的 Nginx 扩展程序.

February 15, 2019

在使用Laravel应用程序时修改变量
用于记录企业安全规划，建设，运营，攻防的相关资源 
背景音乐，macOS音频实用程序：自动暂停音乐，设置各个应用程序的音量和录制系统音频。
Kaggle 项目实战（教程） = 文档 + 代码 + 视频（欢迎参与）
数据结构和算法必知必会的50个代码实现
rust 程序设计语言 中文版
awesome-composer 
WDScanner平台目前实现了如下功能：分布式web漏洞扫描、客户管理、漏洞定期扫描、网站爬虫、暗链检测、坏链检测、网站指纹搜集、专项漏洞检测、代理搜集及部署、密码定向破解、社工库查询等功能。
📦 A composer package builder. http://overtrue.me/package-builder
Go的快速脚本语言
GeoLocation限制了Laravel的路线
PHP的轻量级HTTP客户端
目前实现了网络空间资产探测、指纹检索、漏洞检测、漏洞全生命周期管理、poc定向检测、暗链检测、挂马监测、敏感字检测、DNS监测、网站可用性监测、漏洞库管理、安全预警等等~
Webpack的优雅包装，适用于80％的用例。

February 14, 2019

Docker + Node = Dockerode（Docker远程API的Node.js模块）
Hprose Server for Symfony
nginx cheatsheet
TensorFlow教程和最新API初学者示例
用于物联网的超轻量级JavaScript引擎
💯后端面试进阶指南
自己提炼的关于《HTTP权威指南》每章的知识点总结！
用于代码生成的Laravel组件
历史上最伟大的软件工程师列表
有趣的注释
精选的黑客教程，工具和资源的精选列表
程序员最值得关注的10个C开源项目
管理员资源的精选列表。
Google 全球 IP 地址库
开发者工具箱， free-for-dev
GitHub秘籍
Git 风格指南
Octicons是由GitHub为GitHub构建的一组SVG图标。
github上的lighttpd2更易于协作 - 主要的回购仍然在lighttpd.net上

February 13, 2019

C++包管理器
《Go语言四十二章经》
在GO中处理1M WebSockets连接
简单可靠的网站分析。与Golang&Preact一起构建。
机器学习 (CS 229 Stanford)
即时消息服务器；Go中的后端；Android、Web命令行客户端；聊天机器人
sourcerer应用程序从您的GitHub和Git存储库生成一个可视配置文件。
openresty/openresty: Turning Nginx into a Full-Fledged Scriptable Web Platform
说明描述了如何提高nginx性能、安全性和其他重要事项；
用于MacOS的MySQL/Mariadb数据库管理
隐藏MacOS菜单栏项目
一款功能齐全的客户端（iOS、Android）研发助手，你值得拥有。
网易云音乐命令行版本
Theia是一个用TypeScript实现的云和桌面IDE框架。
一个又酷又高效的命令行 GitHub 工具
用于在MacOS的VSCode上隐藏标题栏并内联交通灯（=窗口控件）的扩展。
PouchDB是一个口袋大小的数据库。
Web的实时数据库
用于功能强大的React和React Native应用程序的高性能反应数据库
javascript嵌入/内存数据库
⚡️lowledb是一个由Lodash支持的小型本地JSON数据库（支持Node，Electron和浏览器）
浏览器的无情键值存储。

February 12, 2019

编写和优化Go代码
REST API application generator for Yii2, openapi 3.0 YAML -> Yii2
一个不断发展的如何保护Linux服务器的指南。
在学院的书架上发现了一本不带脑子就能看懂的书《Python数据挖掘与实战》
ElasticSearch的官方Go客户端
用PHP编写的CSS文件的分析器。允许将CSS文件提取到数据结构中，操作所述结构并输出为（优化的）CSS
轻松创建Alfred工作流
Chrome扩展，在使用应用程序时生成Laravel集成测试。
上传图片到终端公共cdn
使网站页面在1分钟内即时更新，并将转换率提高1%
客户没有付款？增加不透明度的身体标签，并减少它每天直到他们的网站完全消失。
下一个用于Web浏览器的开源文件上载程序
💩🚀 Windows 95 in Electron. Runs on macOS, Linux, and Windows.
大规模中文自然语言处理语料
社区驱动的内容聚合器
Global key-value store in the database
Immutable base object and value objects.
语义化版本控制规范（SemVer）
语义化版本控制规范（SemVer）

February 11, 2019

用深度学习对对联。

February 10, 2019

快速浏览任何GitHub文件的历史记录GitHistory.xyz

February 6, 2019

FastHub是Android的终极Github客户端。
基于现代网络的可扩展桌面邮件应用程序。

February 5, 2019

video.js单元库。
kubernetes cli以风格管理集群！
中国5级行政区域mysql库
A Node.js style checker and lint tool for Markdown/CommonMark files.
Collection of awesome podcasts
Helper functions I find super-duper handy

February 1, 2019

基于PHP的全功能颠覆革命性框架，大道至简、大有若无。本框架钦定组件库：packagist.org

January 31, 2019

[已弃用] Dockerfile，包含安装Magento 2所需的扩展，配置和命令
Go（Golang）假结构数据生成器
适用于Web社区的AI OS

January 30, 2019

全语自动填充器：tabnine.com
Empire客户端应用程序
适用于PHP 7的低开销采样分析器
写在19年初的后端社招面试经历(两年经验): 蚂蚁 头条 PingCAP
draw.io是一个在线图表网站，提供此项目中的源代码。
去死吧！996 godie996.com
带有Windows API的最小无边框窗口
面向对象的PHP驱动程序，用于FFMpeg二进制文件
将curl命令转换为python，javascript，php，R
用golang编写的迷你SMTP服务器
一些内网渗透TIPS
Hexo七牛同步插件
PHP的通用SOAP客户端
收集的一些国外能提供提供威胁情报的公司，涵盖网络安全、工控安全、终端安全、移动安全等领域
用Go（Golang）编写的轻量级MVC框架。
Gitter for GitHub - 可能是目前颜值最高的GitHub小程序客户端
尝试解析出知乎官方未开放的 OAuth2 接口，并提供优雅的使用方式，作为 zhihu-py3 项目的替代者，目前还在实验阶段
golang提示
🛁 PHP版的代码整洁之道 中文翻译

January 29, 2019

日更的FlutterDemo合集，今天你fu了吗
PHP非侵入式监控平台- 优化性能，定位Bug的神器，别再让你的PHP程序裸奔。
git commit --fixup，但是自动的
Flutter嵌入API的桌面实现
PHP SSO Platform 生蚝科技统一身份认证平台
open-source-mac-os-apps 
JavaScript web server
迈向 Tech Lead 之路。
elasticsearch-cn/elasticsearch-definitive-guide
看到女装的项目的issue建议妹子建一个男装的项目，但是考虑到github的女性用户 数量貌似并不能达到女装的效果2333总之先建一个。
基于CLI和REST接口的自托管和基于PHP的URL缩短程序
用于大数据的分布式SQL查询引擎
依赖注入系统
发现需要重构的文件。
API平台的服务器组件：超媒体和GraphQL API，只需几分钟
PHP链接检查器
饿了么蜂鸟配送php开发包
华丽的应用程序，纠正您以前的控制台命令。
🐸ASCII在线视频流搭建脚本
可视化GitHub配置文件的工具
HTML5 / JavaScript多人游戏实验
给不了你梦中情人，至少还有硬盘女神
软件版本控制可视化
12306 图片验证码识别测试

January 28, 2019

PHP MySQL类的包装器，它使用MySQLi和预处理语句。
“在互联网上寻找栖息之地”
macOS文件存档
Lanyrd's MySQL to PostgreSQL conversion script
按其属性或关系对Eloquent模型记录进行排序
GitHub和GitLab缺少IntelliSense提示
[Chrome扩展程序]在github.com活动信息中心上过滤活动。

January 27, 2019

一些 CSS 常用样式
将任何网站转为无服务器API（支持SPA！）
Chrome 插件，查看开源中国软件更新资讯，文档导航，GitHub 趋势榜，linux命令索引，浏览历史记录和时钟页面。
从网易云音乐、QQ音乐、酷狗音乐、百度音乐、虾米音乐等搜索和下载歌曲
程序员找工作黑名单
 论文阅读笔记（分布式，虚拟化，容器，自动机器学习）
Odoo。开源应用程序以拓展您的业务。
用于探索各种解析器生成的AST的Web工具。
使用Mac，iOS，tvOS和watchOS的原生API桥接.NET世界。
一个http api网关

January 26, 2019

使用Docker在CI中运行Lighthouse
文件共享实验
我们一起来还原微信。希望通过 iWeChat 这个项目能过勾勒出微信的设计，使用到的技术手段等
A Go (golang) Custom Flutter Engine Embedder for desktop
wudi/PHP-Interview-Best-Practices-in-China: 📙 PHP 面试知识点汇总📙 PHP 面试知识点汇总
Suitable for Work (NSFW) classification

January 25, 2019

各主要城市的互联网公司黑名单
一个非常固执的，高度个性化的脚本来设置一台新的Mac机器，就像我喜欢它一样！
2018/2019/校招/春招/秋招/算法/机器学习(Machine Learning)/深度学习(Deep Learning)/自然语言处理(NLP)/C/C++/Python/面试笔记
网页版微信API，包含终端版微信及微信机器人
这是我为php访谈准备的信息。笔记包括php，mysql，linux等。
Composer 免签约支付宝与微信，直接到个人账户，持续维护，PHP实现免签约支付接口，判断订单号与备注，自带监听。
我大学两年来的笔记，希望对大家有些些帮助（以后会持续更新）
A keygen for Navicat
PHP安全通信库
极简主义的Vim插件管理器

January 24, 2019

在紧急情况下保存您的代码
fastai深度学习库，以及课程和教程
原生，高性能，跨平台的桌面应用程序 - 使用Reason构建！
Futurice开发人员对Android开发的注意事项和注意事项
中文版 《微服务：从设计到部署》
通过预览，编译，自动完成，着色等提高LaTeX排版效率。
构建GitHub应用程序的框架，用于自动化和改进您的工作流程
简单，可扩展的状态管理。
斗图神器 收集了成千上万的撕逼斗图表情包，在这里你可以快速找到想要的表情
Go by Example

January 23, 2019

用于实时可视化的JavaScript库
用于macOS，Windows，Linux和最终Android的下一代Brave浏览器
终端游戏测试git技能
声明用很好的错误消息验证方法输入/输出。
PHP扩展开发及内核应用
显示和控制您的Android设备
编辑器中的真实浏览器预览，您可以调试。
在Laravel应用程序中捕获传入的电子邮件
应用程序仪表板和启动器
基于laravel + reactjs的问题跟踪工具，适用于中小型企业，开源和免费，类似于Jira。
一个渐进而全面的框架，用于构建基于组件的网站，跨越前端和后端
黑箱应用故障注入和资源发现的攻击模式和原语词典。
在线测试驱动器编程字体
用于运行OAuth2服务器的演示应用程序
用于实现OAuth2服务器的包装器
oauth2-server-laravel 

January 22, 2019

任务运行/简单使用Go编写替代
适用于Android应用的字节码优化器
工程师知识管理系统：基于golang go语言（beego框架）。每个行业都有自己的知识管理系统，EngineerCMS旨在为土木工程师们打造一款适用的基于web的知识管理系统。
自动编译js + css + html
基于运营转换（OT）的实时数据库后端
git-subrepo
golang数据库/ sql的通用扩展
建立在自然节点上的NLP库，具有实体提取，情感分析，自动语言识别等功能
PHP的Kafka客户端
小红书通信协议签名
抖音通信协议签名
快手通信协议签名
Google API的公共接口定义。
一个基于golang的web应用快速开发框架，提供了开放平台及相关open api的封装，可以快速的开发微服务、web应用、微信公众号、企业微信、钉钉、云之家等第三方平台应用
GitPython是一个用于与Git存储库交互的python库。
为 Sketch 准备的模拟数据中文版，包含：中文姓名，手机号，省份，城市，地区，公司名，银行名，星期几，详情地址，邮编，邮箱，颜色，广告词等。
令人愉快的JavaScript测试。
PHP源码加密模块
Go的极简主义websocket框架
MISP（核心软件） - 开源威胁情报平台（以前称为恶意软件信息共享平台）
Siler是一组通用的高级抽象，旨在用于PHP中的声明性编程API。
开源图像托管脚本
PHP Link Checker
将云计算，数据和服务无缝扩展到边缘设备。
使Chrome能够将markdown文件呈现为HTML

January 21, 2019

程序员的 macOS 搭建指南
V2Ray 基于 Nginx 的 vmess+ws+tls 一键安装脚本
ThinkPHP5 社会化登录组件
适用于Linux-Gnome桌面的Mac OS主题
微信、支付宝、QQ 三合一收款二维码（单页版）
使用压缩和经过身份验证的加密对归档程序进行重复数据删除。
为PHP 7提供多进程的扩展
🌴上传组件，可让您节省更多播放LOL的时间。
真正专注于让一套代码运行多端的开发框架，提供标准的MVVM架构开发模式统一各类终端
一系列可打印的单页备忘单，由Markdown使用Pandoc和LaTeX生成
这是来自麻省理工学院，斯坦福大学和普林斯顿等知名大学的免费课程的精选清单。
Python开源Web, CMF，可做微信小程序后端, 网站后端等.Restful Api 
使用Go语言开发的版本发布系统
vue.js(element框架)+golang(beego框架)开发的运维发布系统,支持git,jenkins版本发布,go ssh,BT两种文件传输方式选择,支持部署前准备任务和部署后任务钩子函数
Mysql web端sql审核平台 
TCP数据包监控和统计工具
 Layx 新一代Web弹窗组件。
在线协作与文档管理系统
基于G6和React的可视化图形编辑器
MM-Wiki 一个轻量级的企业知识分享与团队协同软件，可用于快速构建企业 Wiki 和团队知识分享平台。
一个记笔记的应用程序，更好地了解程序员和Markdown。
 Teamcat 软件工程团队协作平台！
参考百度文库，使用Beego（Golang）开发的开源文库系统
TeaWeb-可视化的Web代理服务。
基于Lua的跨平台终端ui库
通过简单的面向对象的类似dom的API在画布上绘制图形。支持Vue＆React / Preact。
使用Three.js构建的声明式3D Globe数据可视化库
RedisPlus是为Redis可视化管理开发的一款开源免费的桌面客户端软件
在Go中快速开发微服务的微服务框架
阿布量化交易系统(股票，期权，期货，比特币，机器学习) 基于python的开源量化交易，量化投资架构 
由TypeScript提供支持的阿里巴巴代表的可管理，可衡量和可追踪的Node.js应用程序管理器
金链盟区块链底层平台
scrapy

January 20, 2019

A Go (golang) Custom Flutter Engine Embedder for desktop
golangci-lint
用于在浏览器中制作交互式音乐的Web Audio框架。
macOS的开源Markdown编辑器。
phpspy 

January 19, 2019

Greenplum数据库
实时性能监控
在JavaScript中实现的算法和数据结构，包含解释和进一步读数的链接
JavaScript 算法与数据结构
Prettier是一个固定的代码格式化程序。
Prometheus的Elasticsearch统计数据导出器
ORM for TypeScript和JavaScript（ES7，ES6，ES5）。支持MySQL，PostgreSQL，MariaDB，SQLite，MS SQL Server，Oracle，WebSQL数据库。适用于NodeJS，浏览器，Ionic，Cordova和Electron平台。
Go的快速脚本语言
一个简单的PHP文件管理器。代码是一个单独的php文件。
A Lua VM in Go
基于搜狗微信搜索的微信公众号爬虫接口
Upload big files for Laravel 上传大文件的Laravel扩展
超过400家易于申请的软件工程公司
用Golang编写的高性能PHP应用程序服务器，负载均衡器和进程管理器
swoft课程代码
material-ui 

January 18, 2019

一个用于渗透透测试演练的WEB系统,用于提升寻找网站能力,也可以用于web安全教学
通过监控wifi信号来计算你周围的人数
深度学习书中文翻译
PHP底层内核源码分析和扩展开发
Git快速统计是一种访问git存储库中各种统计信息的简单而有效的方法。
高性能后台系统构建工具, 使用极少代码即可构建出功能完善的后台系统
一个适用于iOS的原生网络调试工具
程序员的全栈资源集合。
一个基于Yii2高级框架的快速开发应用引擎
WeChat SDK for Yii2 , 基于 overtrue/wechat 4.x
我是木易杨，网易高级前端工程师，跟着我每周重点攻克一个前端面试重难点。
生产级集装箱调度和管理
妈妈再也不用担心我的网易云音乐变灰了
比特币核心集成

January 17, 2019

从网易云音乐、虾米音乐、QQ音乐、酷狗音乐、酷我音乐等搜索和下载歌曲
Popcorn Time for music
Dead simple Object schema validation
Docker速查表
go结构和字段验证，包括跨字段、跨结构、映射、切片和数组潜水
将compose中描述的应用程序部署到kubernetes集群上
内存中的键：用于go的值存储/缓存（类似于memcached）库，适用于单机器应用程序。
您的即时Emacs开发环境。
一种限速器中间件
阿里云翻译小组，为社区输出优质的技术文章。
蓝眼系列软件之《蓝眼云盘》
A RESTful Search Engine 
简单、强大、跨平台的SQLite客户端和ORM
一个浏览器内的IDE，用于浏览GRAPHQL。
基于Webkit/Firefox浏览器，辅助用于12306订票的助手软件。
为国内网站分享的网盘下载提供便利的工具
FastD Swoole 基础组件
一个高性能的PHP API框架。
Moby项目-集装箱生态系统组装基于集装箱系统的协作项目
PHP代码审计分段讲解
1000个PHP代码审计案例(2016.7以前乌云公开漏洞)
自动化运维平台: 代码及应用部署CI/CD、资产管理CMDB、计划任务管理平台、SQL审核|回滚、任务调度、站内WIKI
基于放弃的官方应用程序的Android Github客户端
Thor HTTPS 抓包分析，开发调试利器 for iOS
这些是phpsorm中用于代码完成的助手文件，我使用的是一些开源软件。
高度定制的chrome黑色主题
tensorflow
EXLcode - VS Code-based Online IDE Chrome Extension 
由Visual Studio代码支持的在线IDE
为Web应用程序开发量身定制的在线代码编辑器
描述HTTP / 3和QUIC协议的文档
将终端会话记录为SVG动画
Flink Forward China 2018 Slides
P编程语言。
百度AI开放平台 PHP SDK. 
简体中文的逐步区块链教程
TypeScript的权威指南
MIME组件允许操作MIME类型。
QOR是一组用Go编写的库，它抽象了业务应用程序，CMS和电子商务系统所需的常用功能。
 这是一个很酷炫的前端网站搜集器，导航网
用任何编程语言构建强大的管道。
Ansible是一个极其简单的IT自动化平台，可使您的应用程序和系统更易于部署。
Node.js最佳实践中排名最高的内容的总结和分享

January 16, 2019

Go 学习之路：Go 开发者博客、Go 微信公众号、Go 学习资料（文档、书籍、视频）
Arduino framework for node.js
在本地运行您的GitHub操作
appsync的无服务器插件
📜 33 concepts every JavaScript developer should know.
🚀他妈的快速文件管理器（用bash编写）
Laravel 5.4+内容管理框架
Laravel 5.4+内容管理框架
PHP Telegram Bot.
与用于学习与Node.js相比，Golang的示例
TablePlus
Redis Desktop Manager For Mac OSX DMG 

January 15, 2019

A lightweight MVC framework written in Go (Golang).
您的NoSQL数据库由Golang提供支持
麻省理工学院深度学习相关课程的教程，作业和比赛。
开源免费的简易中文分词系统，PHP分词的上乘之选！
TiDB是与MySQL协议兼容的分布式HTAP数据库
api-problem规范的简单实现
基于Swoole扩展开发游戏服务器框架，示例实现h5游戏开发
rust 全文搜索引擎
用go语言编写的Microsoft SQL服务器驱动程序
syncd是一款开源的代码部署工具，它具有简单、高效、易用等特点，可以提高团队的工作效率.
在不到30s内得到一个干净的开箱即用的临时linux系统.
在Mac上计算你写了多少行代码
用于更改React的RFC
Jupyter笔记本用于“深度学习Python”一书的代码示例
Babel是编写下一代JavaScript的编译器。
💻PHPUnit的并行测试
LiteSpeed Cache for WordPress 
LiteSpeed QUIC Client Library
litespeed - 高性能，轻量级，开源的HTTP服务器
12306
强大，无处不在且可大规模扩展的Jabber / XMPP即时消息平台
🧀将图像上传到公共CDN
中英文敏感词、语言检测、中外手机/电话归属地/运营商查询、名字推断性别、手机号抽取、身份证抽取、邮箱抽取、中日文人名库、中文缩写库、拆字词典、词汇情感值、停用词、反动词表、暴恐词表、繁简体转换、英文模拟中文发音、汪峰歌词生成器、职业名称词库、同义词库、反义词库、否定词库、汽车品牌词库、汽车零件词库、连续英文切割、各种中文词向量、公司名字大全、古诗词库、IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库、中文聊天语料、中文谣言数据、百度中文问答数据集、句子相似度匹配算法集合、bert资源、文本生成&摘要相关工具、cocoNLP信息抽取工具
灵活的Zsh插件管理器，具有干净的fpath，报告，完成管理，turbo模式，服务
Go的基于反射的依赖注入工具包。
PHP中的一个简单的Podcast RSS编辑器
用于语义UI的日历模块

January 14, 2019

现代，疯狂快速，可靠，简单且功能强大的平面文件CMS
高效微信公众号历史文章和阅读数据爬虫powered by scrapy
一款专为 deepin 打造的小飞机
🍎笔记本
js 功能函数库
一系列Android进阶文章
在C中从头开始编写sqlite克隆
Fast3kB React采用相同的现代API替代品。 组件和虚拟DOM。
flutter 开发者帮助 APP，包含 flutter 常用 130+ 组件的中文文档与 demo 演示
LeetCode练习, Go语言版本
一种简单快捷的图像处理工具。
ImgBot在GitHub中抓取所有图像文件，并在应用无损压缩后提交拉取请求。这将使文件大小下降，但保持尺寸和质量同样好。
Elasticsearch Web UI
使用简单的HTML和CSS构建漂亮的Electron应用程序的最快方法
伪装115浏览器

January 13, 2019

一种有效的浏览器扩展，可以阻止整个网络上基于浏览器的加密货币挖掘者。
用Go编写的YouTube下载库和CLI
Go（Golang）中的指数退避算法。
一款入门级的人脸、视频、文字检测以及识别的项目。
基于aria2的轻量级多线程下载器。
帮助115导出下载链接到aria2-rpc
遵循gRPC HTTP规范的gRPC到JSON代理生成器
将github贡献图表嵌入图像
flutter 开发者帮助 APP，包含 flutter 常用 130+ 组件的中文文档与 demo 演示
学习PHP的在线书籍

January 12, 2019

用于OpenTracing的NGINX插件
“玄魂工作室--安全圈” 知识星球内资源汇总
📖「一个」、「Time 时光」、「开眼」、「一席」、「梨视频」、「微软必应词典」、「金山词典」、「豆瓣电影」、「中央天气」、「魅族天气」、「每日一文」、「12306」、「途牛」、「快递100」、「快递」应用 Api。
前端决策树
无它术，唯勤读书而多为之，自工
软件版本控制可视化
Powerline是vim的状态行插件，并为其他几个应用程序提供状态和提示，包括zsh，bash，tmux，IPython，Awesome和Qtile。
机器学习备忘录
Tivi是一款正在进行中的电视节目跟踪Android应用程序，它连接到Trakt.tv。
QQ 音乐接口 api
Guzzle Swoole Handler
Electron for PHP. Desktop GUI framework with HTML5 Chrome/IE engine. 
Electron for PHP. Desktop GUI framework with HTML5 Chrome/IE engine. 
php_desktop 像开发网站一样开发桌面应用软件
CRMEB是一款微信公众号和小程序的电商系统，带分销、拼团、秒杀、砍价、优惠券、积分等功能

January 11, 2019

a cron library for go
A theme that adds the dark Incognito Mode colour scheme to the normal mode of Chrome.
Python和CLI的快速，可扩展的进度条。
一个窗口切换器，应用程序启动器和dmenu替换
专为程序员编写的英语学习指南。v1.0
用于移动网页的轻量级，可扩展的前端开发人员工具。
算法实现在GoLang
一个基于Laravel的发布平台
kafka php client
中华人民共和国国家标准 GB/T 2260 行政区划代码
记录前端开发中的技巧以及算法知识
python版本：领域细分的中文分词工具，简单易用，跟现有开源工具相比提高了分词的准确率。
CSS Inspiration，在这里找到写 CSS 的灵感
使用直接SQL查询编写API没有麻烦，让我们重新思考SQL
一个PHP包，用于向用户显示读取内容所需的时间。

January 10, 2019

caliber是一名电子书经理。 它可以查看，转换，编辑和编目所有主要电子书格式的电子书。
Python最佳实践指南
一个轻量级库，用于将复杂对象转换为简单的Python数据类型。
Java 8 Jar和Android APK逆向工程套件
pytorch handbook是一本开源的书籍，目标是帮助那些希望和使用PyTorch进行深度学习开发和研究的朋友快速入门，其中包含的Pytorch教程全部通过测试保证可以成功运行
JavaScript中的PFS实现
Perun是一款主要适用于乙方安服、渗透测试人员和甲方RedTeam红队人员的网络资产漏洞扫描器/扫描框架
Go学习笔记
机器学习初学者公众号作品
查找向项目添加新依赖项的成本
将博客网站转换为合并pdf的示例程序。
功能全面的php命令行应用库。提供控制台参数解析, 命令运行，颜色风格输出, 用户信息交互, 特殊格式信息显示 
100天的ML编码
用于DOM操作的最小独立JS库
一个静态博客写作客户端 
🚂 12306 购票助手，支持分布式，多账号，多任务购票

January 9, 2019

Windows 10的macOS Mojave Dynamic Desktop功能端口
一款用 Java 实现的现代化社区（论坛/BBS/社交网络/博客）平台。
离线存储，改进。 使用简单但功能强大的API包装IndexedDB，WebSQL或localStorage。
在持久性引擎之间同步数据，就像ETL一样，只是不稳定
新浪微博图床 Chrome 扩展，支持同步到微相册
使用libgmp对大整数进行算术运算
用于使用URL语法传输数据的命令行工具和库，支持HTTP，HTTPS，FTP，FTPS，GOPHER，TFTP，SCP，SFTP，SMB，TELNET，DICT，LDAP，LDAPS，FILE，IMAP，SMTP，POP3，RTSP和 RTMP。 libcurl提供了无数强大的功能
通过设计稿一键智能生成视图代码，目前支持生成 Vue、React、Html5、Weex Rax 等常见 DSL。
Swoole支持Expressive应用程序
轻量、可靠的小程序 UI 组件库
XPay个人免签收款支付系统 完全免费 资金直接到达本人账号 无需备案 无需签约支付宝微信 无需挂机APP 无需插件 无需第三方支付SDK 无需营业执照身份证 只需收款码 搞定支付流程 现已支持移动端支付
用于构建优秀社区的简单论坛软件。
网络终端
Matomo是Google Analytics的领先开放替代品，可让您完全控制数据。 Matomo可让您轻松收集来自网站，应用和物联网的数据，并可视化这些数据并提取见解。
以快速，可扩展的方式读写电子表格文件（CSV，XLSX和ODS）
禅道

January 8, 2019

现代复制到剪贴板。
Linuxlinuxbrew.sh的Homebrew包管理器
ScreenToGif允许您录制屏幕的选定区域，编辑并将其保存为gif或视频。
PHP 获取快递物流信息
README文件语法解读，即Github Flavored Markdown语法介绍
秋招面试总结
Apache Dubbo（孵化）是一个基于Java的高性能开源RPC框架。
一个简单的守护进程，允许会话软件更新固件
现代JavaScript日期实用程序库
基于复制和翻译的国外纸质阅读和翻译助手。
websocket命令行工具
记录并重播网络
傳承字形標準化文件
🏝钉钉 SDK • 👷‍♂️2.0 开发中
eSpeak NG是一个开源语音合成器，支持101种语言和口音。
一组匹配中国大陆手机号码的正则表达式。
Pika是与redis兼容的nosql，由Qihoo的DBA和基础架构团队开发
微信小程序开发资源汇总 💯
使用GTK + 3的Linux平铺终端仿真器
使用开放式Web技术构建令人惊叹的原生和渐进式Web应用 一个应用程序在所有东西上运行🎉
Python 艺术二维码生成器
收集整理远程工作相关的资料
一个简单而优雅的客户端，用于访问和控制Kubernetes集群
终端会话记录器
自由·负责·克制 去广告 Hosts 项目
免费中文字体
根据Dice的系数找出两个字符串之间的相似程度，这个系数大多优于Levenshtein距离。
根据Dice的系数找出两个字符串之间的相似程度，这个系数大多优于Levenshtein距离。
可靠的USB格式化实用程序
Darling是OS X应用程序的运行时环境。
收集iOS应用程序中最常见的漏洞
使用Trilium Notes构建您的个人知识库
用于软件和Web开发的免费API的集合列表。

January 6, 2019

一个小型JavaScript库，用于计算太阳/月亮位置和阶段。
AVH版的git扩展，为Vincent Driessen的分支模型提供高级存储库操作
The Kubernetes Package Manager 
GirlsInAI 是一个面向编程零基础女孩子的AI算法工程师养成计划。
Go package captcha实现了图像和音频CAPTCHA的生成和验证。
Here Music 一个 使用 Electron + React 开发的音乐客户端
使用Github GIST在多台计算机上同步Visual Studio代码设置

January 5, 2019

适用于iOS 11.4.1-iOS 12.1的漏洞利用
walle - 瓦力 开源项目代码部署平台
Go to JavaScript中的编译器，用于在浏览器中运行Go代码
技能树
laravel5.5和vue.js结合的前后端分离项目模板。
微服务的标准库。gokit.io
物联网设备的WebUI仪表板喜欢raspberry pi。
12306智能刷票，订票
PhpStorm的PHP运行时和扩展头文件
JSON，CSV，XML和Yaml的世界国家。

January 4, 2019

极客挚爱的在线技术平台
🔎在社交网络中查找用户名
将使用STDIN / STDOUT的任何程序转换为WebSocket服务器。像inetd一样，但对于WebSockets。
JavaScript国际化框架
用Golang编写的高性能PHP应用程序服务器，负载均衡器和进程管理器
WordPress.com for Desktop
明星和感谢作者许可证（SATA许可证）
Zipkin是一个分布式跟踪系统
论文与代码。按星星排序。每周更新。
关于Detour App规则配置的简单介绍
wingy-announcement
💾 Flysystem adapter for the oss storage.
一个实验性的点对点Web浏览器beakerbrowser.com
跨平台HTTP和GraphQL客户端
A Simplenote React app packaged in Electron
通用剪贴板管理应用程序，可以从任何设备上的任何位置轻松访问剪贴板。
Mac和Windows上的Visual Docker容器管理
ZeroNet  - 使用比特币加密和BitTorrent网络zeronet.io的分散式网站

January 3, 2019

以91％的准确率击败最新版本的
有助于管理Git托管的PHP项目版本号的库
带有解析器的HTML5视频播放器可以节省流量
整个百度使用的工业级RPC框架，拥有1,000,000多个实例和数千种服务，在百度内部称为“baidu-rpc”。
tmux源代码
Oss storage filesystem for Laravel.
golang123 是使用 vue、nuxt、node.js 和 golang 开发的知识分享系统 golang123.com
zsh的插件管理器。
一个简单的脚本来创建github toc
使用相同的现代API替代Moment.js的不可变日期库
基于Raft构建的分布式MySQL binlog存储系统
官方redis集群的Python集群客户端。 Redis 3.0+。
一系列精选的面试问题列表。
🔐了解如何使用JSON Web Token（JWT）来保护您的下一个Web应用程序！（教程/测试示例!!）
利用现代浏览器所提供的强大 API 录制并回放任意 web 界面中的用户操作。
成为2019年Web开发人员的路线图

January 2, 2019

polarphp
PHP应用程序的即时升级
Redis集群的Openresty lua客户端。
排名前200位的深度学习Github存储库按星数排序。
按特定日期获得的星数排序的前100个趋势深度学习资源库。
Golang命令教程中文。
我的专栏“Core Golang  -  36课”的示例项目
awesome-go-China
高性能，极简主义的Go web框架echo.labstack.com

January 1, 2019

中国省市区数据
Python和命令行的世界上最简单的面部识别API
GitHubDaily 分享内容定期整理与分类。欢迎推荐、自荐项目，让更多人知道你的项目。
Visual Studio Code的UNOFFICIAL网易音乐扩展
适用于Composer的快速，可靠且安全的NPM / Yarn桥接器
科学上网的有趣项目集锦，欢迎大家pr自己喜欢的项目到这里。
WebTorrent
webtorrent-desktop 
GitHub Dark as a userscript 
Stylus - Userstyles Manager 

December 31, 2018

一个composer包，用于验证以前是否使用Have I Been Pwned API在密码中使用了密码。
微信公众号管理系统，也是一套微信公众号开发框架。支持移动管理，几乎集合微信功能，简洁、快速上手、快速开发微信各种各样应用。
PrestaShop提供完全可扩展的开源电子商务解决方案。
🐘 A PHP prober (一款精美的 PHP 探針, 又名X探針、劉海探針)
建立和管理Phars的申请。
提问的智慧
科学上网的有趣项目集锦。

December 30, 2018

功能齐全的下载管理器。
一个用PHP和Redis编写的Twitter玩具克隆，在早期用于介绍Redis数据类型。
用于golang的socket.io库，一个实时应用程序框架。
丰富的表情符号包资源。
Sublime Text 2和3中PHP项目的智能代码完成。
在一个地方管理你的git存储库。
用于动态生成PDF文档的PHP库。
aveo基于Laravel框架的开源票务系统。
一个小型JavaScript库，可以从数字生成类似YouTube的ID。 当您不希望向用户公开数据库ID时使用它。

December 29, 2018

Spring Boot教程。
💰 微信/支付宝收款监控，个人收款无需签约支付宝、微信支付。为支付宝、微信支付的个人账户，提供即时到账收款服务。
常用交互式命令行用户界面的集合。
JavaScript 代码规范，自带 linter & 代码自动修正。
从HTML表单中提取的PHP表单验证。在同一个地方写一次表格和验证！
一个Web代理工具。
将ArchLinux安装为WSL实例。 支持多重安装。
Laravel核心代码学习。
一个阮一峰博客风格的Hexo主题。
用于长期短期内存网络（LSTM）的可视化工具箱。
Tinker in your browser。
Gonum是Go编程语言的一组数字库。它包含用于矩阵，统计，优化和更多。
基于 PAYJS 微信支付个人接口开发的 Laravel Package，可直接用于生产环境。
基于 PAYJS 微信支付个人接口开发的 Package，可直接用于生产环境。
用于快速文本表示和分类的库。
一个集审核、执行、备份及生成回滚语句于一身的MySQL自动化运维工具之手册部分
Go模板的有用模板函数。
PostgreSQL数据库的跨平台客户端。
macOS的简单SSH快捷菜单。
务实地搜索模型和其他来源。
 iBrand EC 是一个免费的开源电子商务解决方案，使用 PHP 基于 Laravel 框架进行编写。
Laravel的Web安装程序。

December 28, 2018

生辰八字，五行，算命。
用于安全和可扩展的网络流量分析的框架。
一个检测移动设备的简单JS库。
Laravel 5的数据清理程序和表单请求输入卫生。
具有GPL许可证的高性能MySQL代理。
中国省/自治区/直辖市、市/自治州、区/县/旗数据，包含名称、拼音、拼音首字母、行政代码、区号。
用于构建JSON API的规范。
AliSQL是一个源自阿里巴巴集团的MySQL分支。
Laravel + go-micro + grpc + Zipkin。
Laravel + go-micro + grpc + Zipkin
在Symfony sylius.com之上的开源电子商务框架。
Spree是一个完整的，模块化的，API驱动的开源电子商务解决方案，适用于Ruby on Rails。
微信个人号接口、微信机器人及命令行微信，三十行即可自定义个人号机器人。
VM and compiler for Lua in Go。

December 27, 2018

 Segment Fault 在线讲堂 代码工程。
「小马哥技术周报」

December 26, 2018

PHP TensorFlow绑定。
一个字体系列，为程序员提供了一个很好的等宽变体。
📚学习机器学习的实用方法。
异步WebSocket客户端。
Événement是一个非常简单的PHP事件调度库。
开箱即用的中台前端/设计解决方案。
ant-design-mobile
深入理解PHP内核。
GitUp 快速，安全，无头痛地工作。 你终生 失去的Git界面 终于到来了。
具有交互式TLS功能的拦截HTTP代理，适用于渗透测试人员和软件开发人员。
htop是Unix系统的交互式文本模式进程查看器。
go-internals 本书是关于 Go 程序设计语言内部实现原理的阐释，当前正在进行中。
Repo for gRPC PHP。

December 25, 2018

由Visual Studio Code提供支持的在线IDE。
将终端记录转换为GIF动画。
在 Windows 上用 WSL 优雅开发。
Lua和OpenResty的验证库（输入验证和过滤）。
Hprose基于swoole的异步客户端和独立服务器。
Cloud-Native API网关和服务网格。
PHP client/server for the telegram MTProto protocol 
🐜 A UI Design Language。

December 24, 2018

功能强大的免费软件，也恰好是开源Python。
乡村信息系统（SID）。
 ss-panel-v3-mod是一款专为shadowsocks设计的web前端面板。
一个各种方式突破Disable_functions达到命令执行的shell。
收集一些小型实用的工具。
从零开始内网渗透学习。
自动化收集linux信息。
Gin是一个用Go（Golang）编写的HTTP Web框架。
python-web入坑指南。

December 23, 2018

Flexihash是一个小型PHP库，可实现一致的hashing。
基于iView的Vue 2.0管理系统模板。
通过动画QR码传输数据。
PHP制作了加密货币。
Supervisord监控工具。
用Golang编写的高性能PHP应用程序服务器，负载均衡器和进程管理器。
这是ZipArchive方法的简单包装器，带有一些方便的功能。
用于Git repos和npm包的CLI发布工具。
PHP的事件驱动，非阻塞I/O。

December 22, 2018

通过LD_PRELOA绕过disable_functions
Rubix ML是一个高级机器学习库，可让您构建使用PHP语言从数据中学习的程序。
一个方便实用的工具，用于在redis组之间迁移数据。
基于Laravel的API服务端架构代码。
时尚的CLI提示用户友好，直观且易于创建。
RedisDesktopManager-Windows 安装包和编译教程。
建立在yaf的基础上，集成了Smarty引擎，加入了封装好的各种功能类。
开源Node.js无头CMS，轻松构建可定制的API。
学习Python 3示例代码。
一个类似jquery的python库。
持续收集国内免费优质API。
ip2region - 最自由的ip地址查询库，ip到地区的映射库，提供Binary,B树和纯内存三种查询算法，妈妈再也不用担心我的ip地址定位。
JavaScript的“警报”的美丽替代品。

December 21, 2018

oh-my-posh。
适用于WordPress的GraphQL API。
麻省理工学院6.824分布式系统类的基本资源。
序列化闭包（匿名函数）。
精通以太坊 （中文版）。
用于创建彩色控制台输出的简单库。
一个美丽的hexo博客主题与材料设计和响应设计。
电子表格分析器和编写器。
Java 编程思想。

December 20, 2018

Rails Girls Guides。
输入SQL，输出索引优化建议。
f-admin是一套基于Laravel框架开发的基础权限后台系统。
用于#golang（WIP）的实验性新HTTP客户端API。
包 goconfig 是一个易于使用，支持注释的 Go 语言配置文件解析器，该文件的书写格式和 Windows 下的 INI 文件一样。
💫一系列精彩的列表，手册，博客，黑客，单行，cli / web工具等等。特别是对于系统和网络管理员，DevOps，Pentesters或安全研究人员。
NumPy和Pandas与大数据的接口。
PHP 中文工具类，支持汉字转拼音、拼音分词、简繁互转。
Docker  - 初学者|中级|高级。
基于TP3.1的多用户BT离线下载。
ftl-desktop 下载器。
MongoDB到Elasticsearch连接器。

December 19, 2018

一个更新鲜的“在GitHub上叉我”标注。
将markdown文档可视化为思维导图。
用纯Python编写的计算机代数系统。
Golang好文推荐；收录平时阅读到的一些Go相关写的比较好、质量较高的干货文章。
memcached和redis的快速，轻量级代理。

December 18, 2018

GRPC服务的GUI客户端。
Github iOS客户端用RxSwift和MVVM编写的干净架构。
PHP的Diff实现，从PHPUnit中分解为一个独立的组件。
Soli PHP框架。
MIT课程《Distributed Systems 》学习和翻译。
中国低线城市（或称三四线城市）的机会。
Phar安装和验证环境（PHIVE）。
对文件/目录中的所有PHP命名空间进行前缀，以隔离PHAR中捆绑的代码。
Navicat Keygen。
Laravel 社区门户网站。
纯Go中的高度可扩展的Git实现。
Laravel Dusk 基于laravel测试的漂亮仪表板。
Awesome English。
一种更快，更简单的方式来驱动支持Chrome DevTools协议的浏览器。
Package Repo Search。
php-lisp is a Lisp dialect written in PHP. 
GitHub Workflow for Alfred 3.
用于防止睡眠的macOS应用程序。
小明VPN。
一个可以观看国内主流视频平台所有视频的客户端（Mac、Windows、Linux）

December 17, 2018

TinySSH是小型服务器（少于100000字的代码）。
在Node.js和浏览器中生成大量真实的假数据。
广告过滤Adblock，uBlock Origin，Adguard。
Linux内核源代码。
用于Graphite，InfluxDB和Prometheus的漂亮监控和度量分析和仪表板的工具。
快速轻松地管理和切换多个代理。
Go By Example 中文版。
徽章。
基于 Vue.js 的小程序开发框架，从底层支持 Vue.js 语法和构建工具体系。
Tideland GoLib。
一个小型PHP库，用于从数字生成类似YouTube的ID。 当您不希望向用户公开数据库ID时使用它。
MinTTY的一些配色方案。

December 16, 2018

Docker官方映像包装PHP。
由Firebase提供支持的协作文本编辑器。
秒杀系统设计与实现.互联网工程师进阶与分析。
为PHP代码覆盖率信息提供收集，处理和呈现功能的库。
Dillinger是一款支持云端，移动就绪的离线存储，AngularJS支持的HTML5 Markdown编辑器。
Win32 port of OpenSSH。

December 15, 2018

HTTP负载测试工具和库。
自动生成 ppt。
基于vegeta和boom的http基准web应用程序。
录制终端并生成动画gif图像或共享网络播放器。
go 支持移动设备。
node.js命令行界面变得简单。
CKEditor 5的开发环境 - 最好的基于浏览器的富文本编辑器。

December 14, 2018

一个通用字幕查找器，可以查找字幕并下载。
检查中文 markdown 编写格式规范的命令行工具，基于 AST，方便集成 ci，写博客 / 文档必备。
在macOS和Linux上更好的微信。用Electron制造。
一套完整的学习手册帮助自己准备 Google 的面试。
适用于Windows的软件发行版和构建平台。
JSON-RPC 1/2传输实现。 支持python 2/3和pypy。
在Android上安装并运行GNU / Linux。
WeHalo 简约风 的微信小程序版博客。

December 13, 2018

GitLab CE Mirror。
Windows的命令行安装程序。
一个更现代化的终端。
在PHP中实现Token Bucket算法。
Debian，Ubuntu和CentOS的OpenVPN road warrior安装程序。
分布式可靠键值存储，用于分布式系统的最关键数据。
TiDB是与MySQL协议兼容的分布式HTAP数据库。
DevHub: TweetDeck for GitHub - Android, iOS and Web。
基于DuerOS的个人的智能语音助手。
在切换开关中转动复选框和单选按钮。
适用于Google翻译的免费且无限制的API。
Node.js的可移植Unix shell命令。
学习Golang。
准备好使用JSONP端点/有效负载来帮助绕过不同网站的内容安全策略。
HookPHP一款基于C扩展搭建支持AI在线编程的PHP框架-安全秒杀ThinkPHP-性能秒Laravel-功能秒YAF-易用秒Symfony-入门秒Zend-组件秒Yii-耦合秒Phalcon 。

December 12, 2018

Python代码的静态分析器。
Hoa是一个模块化，可扩展和结构化的PHP库集。
Hoa是一个模块化，可扩展和结构化的PHP库集。
此工具可帮助您测试socket.io服务器。
ClickHouse是一个用于大数据的免费分析DBMS。
公众号 swoole4.0之打造自己的web开发框架 代码。
1 kB用于构建声明性Web应用程序的JavaScript微框架。
awesome design systems。
向你的贡献者展示一些爱！您的repo README的小部件。每小时刷新一次。
Python开发工作流程。
jsliang 的文档库. 里面包含了所有的前端文章，例如 vue、react,、angular、微信小程序、设计模式等……
⚡️通过在空闲时间预取视口内链接来加载后续页面加载。
有史以来为PHP创建的最棒的验证引擎。
收集工具和改进使PhpStorm更好一点。
Atom文件特定的图标。

December 11, 2018

基于NodeJS的跨平台，免费和开源密码管理器。
A proxyee-down extension for baiduyun。
Proxyee Down扩展存储库。
演示React项目如何接入Fundebug错误监控服务。
Go的简单文件嵌入器。
The official Go package for NSQ。
For macOS.百度网盘 破解SVIP、下载速度限制~
OpenTracing API for PHP。
将任何脚本或程序的输出放在Mac OS X菜单栏中。
这个javascript库解析PHP代码并将其转换为AST。
Chrome扩展以树格式显示Gitlab代码。
一个授权库，支持Golang中的ACL，RBAC，ABAC等访问控制模型。
专为ThinkPHP5.1定制的Casbin的扩展包，Casbin是一个功能强大，高效的开源访问控制库。
TomatoIDC虚拟主机销售系统。
快速，高效且易于使用的JSON pull解析器。
Composer更新后显示更好的摘要。
一个授权库，支持PHP casbin.org中的 ACL，RBAC，ABAC等访问控制模型。
一个授权库，支持PHP casbin.org中的 ACL，RBAC，ABAC等访问控制模型。
PHP电子邮件验证器库受到。
检查以下RFC的电子邮件地址：3696,1123,4291,5321,5322 isemail.info
Spotify Web API的Go包装器。
《Github 帮助文档》 中文翻译。

December 10, 2018

为每个人提供客户端JavaScript PDF生成。
重新编写，使用react ，babel，webpack和其他现代东西。
phpstorm插件,用于thinkphp5框架的视图,配置,路由,数据库,模型智能提示和跳转。
输入git open打开浏览器中存储库的GitHub页面或网站。

December 9, 2018

构建OAuth和OpenID Connect服务器的终极Python库。 包括JWS，JWE，JWK，JWA，JWT。
基于workerman的PHP中socket.io的服务器端替代实现。
使用一个命令设置git，vim，zsh，SublimeText，tmux等。
验证表单异步。
eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee
在HOME中以纯PHP形式存储和管理PHP版本。

December 8, 2018

适用于Golang的类型安全的Redis客户端。
JavaScript的Base64实现。
谷歌访问助手破解版。
php框架基准测试（包括laravel，symfony，silex，lumen，slim，yii2，tastphp等）
为GitHub README.md轻松创建TOC。
用动画的形式呈现解LeetCode题目的思路。
Tampermonkey脚本从Stack Overflow等复制代码。
aria2是一个轻量级的多协议和多源，跨平台下载实用程序，在命令行中运行。
webui-aria2。
更优雅的驾车体验。
Charles 破解工具。
listen1 desktop。

December 7, 2018

Larastan - 在不运行代码的情况下发现代码中的错误。
Go设计模式，食谱和习语的精选列表。
PHP的简单多进程管理器，基于pcntl和posix。
PHP的可扩展微框架。
我的Python示例 http://www.thegeekblog.co.uk
基于workerman的shadowocks的php端口。
Flutter可以轻松快速地构建漂亮的移动应用程序。
PHP的基本CURL包装器。
easyProxy是一款轻量级、高性能、功能最为强大的内网穿透代理服务器。

December 6, 2018

用于处理Tumblr博客的工具，Tumblr备份。
JSBox 扩展 demo。
jsbox、pin 使用技巧。 
在iOS设备上无需越狱即可检索InfoPlist。
应用程序中开放系统设置的演示（iOS 10.2）。
命令行的艺术。
基于浏览器的代码编辑器。
一个简单的本地图像，用PHP编写的缩略图生成脚本.
我的ZSH配置和dotfiles。
One for all free music in china for Windows with fluent UI. https://github.com/oyrx/listen1_desktop。

December 5, 2018

golang库用于读写Microsoft Excel。
Dan Abramov 的个人博客。
Rules / 规则：Surge / Shadowrocket / Quantumult。
You-Dont-Know-JS中文版。
计算机操作系统慕课笔记。
Laravel专用OSS扩展包。
基于C的gRPC（C ++，Python，Ruby，Objective-C，PHP，C＃）。
将JSON数据转换为PHP类对象。
开源书籍：《Shell 编程范例》，面向操作对象学 Shell！
廖雪峰 learn java。
极客时间：nginx核心知识100讲配置文件与代码分享。
Laravel 的 Websockets。
项目管理系统接口。
墙外到墙内搬运工。
暗网网址大全TOR。
Tor Browser。
中文暗网爬虫。
一个简单的交互式Go解释器。

December 4, 2018

基于Web技术构建的终端。
基于UWP和Web技术的终端。
谷歌蜻蜓计划。
CLI爱好者的终端框架，插件和资源的精选列表。
使用Hexo构建您自己的网站。
将MySQL binlog解析为您想要的SQL。
一个和Laravel的dd一样方便调试的包。
Golang 高效编码引擎。
《The Way to Go》中文译本，中文正式名《Go 入门指南》
Training for Golang。
如何使用golang构建Web应用。
中文版 awesome-go。
Golang标准库。

December 3, 2018

微信里面的黑科技。
了解如何将动画引入您的Web项目。

December 2, 2018

👄小程序And公众号商城，外加后台，功能齐全！
🎬豆瓣电影传送门。

December 1, 2018

微信支付单文件版。一个PHP文件搞定微信支付系列。包括原生支付（扫码支付），H5支付，公众号支付，现金红包、企业付款到零钱等。
一个PHP文件搞定支付宝支付系列，包括电脑网站支付，手机网站支付，扫码支付，JSAPI支付、单笔转账到支付宝账户、交易结算（分账、分润）、网页授权获取用户信息等。
Laravel最佳实践。

November 30, 2018

Go Web的Go-Mega教程。
免费的Kindle电子书资源。
精选的博客列表。
Laravel Nova资源的精选列表。
开源php加密运行扩展，基于screw二次开发。
golang 学习笔记。
Go语言学习笔记。
《Go Web 编程》 
《Go 入门指南》
beego是Go编程语言的开源，高性能Web框架。

November 29, 2018

🛁适用于JavaScript的Clean Code概念。
为PhpStorm和IntelliJ添加PHP注释支持。
Node.js CMS和Web应用程序框架。

November 28, 2018

Chrome 扩展：麻麻再也不用担心 Google API 抽风了。
这是PHP的一个实现，用纯Go编写（尽可能，现在pcre在pure go中不存在并且需要使用libpcre）。
功能强大且易于使用的PHP微框架，旨在帮助您快速构建动态，强大的Web应用程序！
此PHP类使用FastCGI协议处理与FastCGI（FCGI）应用程序的通信。
用于处理二进制和十六进制数据的工具箱。与NodeJS Buffer类似。
将Laravel，LaraDock [Laravel + Docker]和PHPStorm连接在一起。
由微信开发的高效，小型移动键值存储框架。适用于iOS，macOS和Android。
跨平台异步I / O http://libuv.org/。
用 electron 生成的 Shadowsocksr客户端。
MongoDB对象文档映射器（ODM）
用于在PHP中使用MongoDb。

November 27, 2018

91云服务器一键测试包。
用Go（golang）编写的完整比特币解决方案。
Go的执行日志。
⏲️定时任务脚本，推送前端资讯到微信/Telegram。
煎鱼的博客，啊（golang 的一些文章）。
区块链钱包技术指南。
锁定库提供PHP代码的序列化执行。
Yii3 web application template。
Yii Framework 3.0 core。

November 26, 2018

Deepin wine for ubuntu。
Flash OS映像到SD卡和USB驱动器，安全，轻松。
Chinese Identity Card package （中国大陆）公民身份证类。
Linux 内核揭密。
带有详细注释的 Redis 3.0 代码（annotated Redis 3.0 source code）。

November 25, 2018

淘气字符串的大清单是一个字符串列表，当用作用户输入数据时很可能导致问题。
“不要编写你的UI代码，画它！”
一个到处运行的科幻桌面。

November 24, 2018

一套用于Laravel的高级Eloquent。
延迟队列。
我的油猴脚本。
查看 Github 其他用户的时间表。
如何成为一名程序员 中文版。

November 23, 2018

Tipask是一款开放源码的PHP问答系统，基于Laravel框架开发，容易扩展，具有强大的负载能力和稳定性。
NGiИX配置生成器。
京价保（京价宝）—— 一个帮助你自动申请京东价格保护的chrome拓展。

November 22, 2018

标准图书馆。
kafka php客户端。
The Elements of Statistical Learning (ESL)的中文翻译、代码实现及其习题解答。
最好用的PHP汉字转拼音类，支持获取汉字的拼音以及拼音的缩写，能准确匹配6千多个汉字。
GitHub code tree。
macOS hosts 文件管理器。
 Go 语言高性能分词。
CLI用于将各种网站的流提取到您选择的视频播放器。
Go Web服务器的实时重新加载实用程序。
GraphQL是一种与任何后端服务相关联的查询语言和执行引擎。
一个浏览器扩展，为GitHub，Gitlab，Bitbucket，gitea和gogs提供不同的文件类型。
兼容的Redis协议NoSQL数据库。
Kubernetes中文指南/云原生应用架构实践手册。
学习正则。
一个命令行工具，用于派生bip32地址和私钥。
在云端发送您的项目。
网站讨论平台composer包。
swoole队列。

November 21, 2018

直接在CSV或TSV文件上运行SQL。
多集群 Kubernetes 的Web UI。
WeUI的轻量级JavaScript库。
golang的算法和数据结构。
使JSON / JSONP易于阅读。
一个JavaScript / Python / PHP加密货币交易库，支持超过100个比特币/ altcoin交换。
📚 W3School 教程整理 http://www.w3cschool.cc
中英文敏感词、语言检测、中外手机/电话归属地/运营商查询、名字推断性别、手机号抽取、身份证抽取、邮箱抽取、中日文人名库、中文缩写库、拆字词典、词汇情感值、停用词、反动词表、暴恐词表、繁简体转换、英文模拟中文发音、汪峰歌词生成器、职业名称词库、同义词库、反义词库、否定词库、汽车品牌&零件词库、时间抽取、连续英文切割、中文词向量大全、公司名字大全、古诗词库、IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库、中文聊天语料、中文谣言数据。
灵活而强大的通用路由解决方案。
开源看板（用Meteor建造）。
开放中文知识图谱的schema。

November 20, 2018

公共维护的Pholcus爬虫规则库。
Swagger 2.0实现go。
一个开源的自托管持续集成和部署系统。
后台技术栈/全栈开发/架构师之路，秋招/春招/校招/面试。
""结巴""中文分词的Node.js版本。
微信ipad、微信mac协议，可实现微信80%功能；支持62数据登录、扫码登录、收发朋友圈、查看朋友圈、微信建群、微信拉人进群、微信公众号阅读、微信消息收发、微信附近的人定位、微信添加好友、微信红包接收、微信防撤回、分享小程序、微信加粉、微信收藏、微信标签等。
淘气字符串的大清单是一个字符串列表，当用作用户输入数据时很可能导致问题。
在浏览器中运行SQL。
支付宝（蚂蚁金服）开放平台第三方 PHP SDK，基于官方 3.3.0 版本，助力支付宝小程序后端开发。
PHP中的简单加密。
Spring Boot 教程、技术栈示例代码，快速简单上手教程。
基于Swift的iTu​​nes插件，用于在桌面上显示歌词。
适用于Mac的TinyPNG客户端.
作曲家并行安装插件-加速包安装。
Caddy是一款可立即投入生产的开源Web服务器，它快速，易用，并且可以提高您的工作效率。
Certbot是EFF的工具，用于从Let's Encrypt获取证书，并且（可选）在您的服务器上自动启用HTTPS。它还可以充当使用ACME协议的任何其他CA的客户端。
Laravel Echo的Socket.io服务器。
总结关于科学上网的概念方法及工具。
这里唯一一个gfwlist。
微小版本的gfwlist，仅关注常见网站。
实时网络的开源数据库。
thumbor是一个开源的照片缩略图服务。

November 19, 2018

股票期权，RSU，税收阅读。
HTTP 相关的 RFC 中文翻译（中英文对照）。
静态网页生成器大合集汇总网站。
一个用于在 MacOS 上平滑你的鼠标滚动效果或单独设置滚动方向的小工具, 让你的滚轮爽如触控板 。
使用vue构建electron应用程序。
Postman中文使用说明。

November 18, 2018

使用Go + HTML5构建跨平台的现代桌面应用程序。
iOS 12 捷径创建者。

November 17, 2018

Cloudflare CNAME接入。
小米笔记本PRO安装macOS Mojave & High Sierra 使用说明。
Go的微型和可插拔Web框架。
PHP静态分析工具 - 发现代码中的错误而不运行它！
可组合Docker管理。
Compose setup for Portainer。

November 16, 2018

Golang实现JSON网络令牌（JWT）。
轻松的文档。
灵活且可扩展的CMS，可在网络上及以后创建定制的数字体验。

November 15, 2018

Docker UI管理器。
用于构建代理以绕过网络限制的平台。
最好用的 V2Ray 一键安装脚本 & 管理脚本。
机器人/机器人/爬虫/刮刀/蜘蛛使用的HTTP用户代理的语法模式。
符合规范，默认情况下是安全的PHP OAuth 2.0服务器。
swoole 开发的mysql数据库连接池。
Go的解析器库。

November 14, 2018

MySQL JSON Explain Analyzer。
将Lua的强大功能嵌入到NGINX HTTP服务器中。
PHP图像处理。
Laravel的实时信使。
快速而强大的Web服务器和应用程序服务器。
强大的音乐API框架。
为您的Electron应用程序创建一个Windows包。
Zephir是一种编译的高级语言，旨在为PHP创建C扩展。
Go的快速HTTP包。调整为高性能。热路径中的零内存分配。比net / http快10倍。
Redis在PHP中分布锁。
基于Node.js的中文分词模块。
下一代ShadowsocksX。

November 13, 2018

跨平台Go日志库。
用 go 实现 laravel。
zsh的下一代插件管理器。
awesome-zsh-plugins。
google 的 ggrc-core。
Go的结构化可插入日志记录。

November 12, 2018

Vue.js的移动UI元素。
PHP的一个面向对象的多进程管理器。
PHP中基于Web的文件管理器，使用Tiny File Manager高效，轻松地管理文件。
JetBrains 系列软件汉化包。

November 11, 2018

go的依赖工具。
闲耘的 rime 输入法配置。
【鼠鬚管】Rime for macOS。

November 10, 2018

iHosts非常适合在Mac OS X上编辑/etc/hosts。
host管理chrome插件。
📝开发工具，用于记录laravel应用程序的所有查询。
《精通比特币2》中文版。
Docker的简单管理UI。
在Laravel应用程序中记录活动。
Photopea是在线图像编辑器。
一个面向全平台的代理客户端。
一个Clash的Windows用户图形界面。

November 9, 2018

与KeePass兼容的免费跨平台密码管理器。
macOS  KeePass 客户端。
KeePass插件通过HTTP安全地公开密码条目。
archiver。
命令行JSON处理工具🔥
for PHP的感知图像散列https://jenssegers.com

November 8, 2018

一刻社区后端 API 源码。
一刻社区前端源码。
Chatter是一个简单的Laravel论坛包。
docker中文文档。

November 7, 2018

飞冰 - 让前端开发简单而友好，海量可复用物料，配套桌面工具极速构建前端应用，效率提升 100% 。
Yii的依赖注入。
Gitbook 的高亮插件。
SQL优化器和重写器。
Go中基于设计的API和微服务。
简单的PHP版本管理。
一个简单的HTML5，YouTube和Vimeo播放器。
宁皓网课程的学习路径。
PHP API 文档。
用于本地开发的现代Docker LAMP堆栈和MEAN堆栈。
多端统一开发框架，支持用 React 的开发方式编写一次代码，生成能运行在微信小程序/百度智能小程序/支付宝小程序、H5、React Native 等的应用。
将macOS“快速查看”功能带到Window。
🚥 从MongoDB到Elasticsearch的数据集迁移工具，反之亦然。
为MongoDB生成随机数据。
《一起学 Node.js》
百度网盘客户端 - Go语言编写。
为云音乐客户端解决不可用的歌曲。
网易云音乐第三方。

November 6, 2018

JSS是CSS的创作工具，它使用JavaScript作为宿主语言。
PHP Curl Class可以轻松发送HTTP请求并与Web API集成。
PHP的快速请求路由器。
Next Generation of ShadowsocksX。

November 5, 2018

HTTP API 设计指南。
基于Vue.js 2.0构建的高质量UI工具包。
开源项目挣钱实用手册。
静态网站生成器Hexo的简单，精致和现代主题。
使用tensorflow.js在浏览器中进行面部检测和面部识别的JavaScript API。
🤘Alfred3工作流程的集合，将震撼您的世界。
Alfred工作流程的公共集合。http://www.alfredworkflow.com
shortcuts workflow 集合。
What-happens-when 的中文翻译。
使任何网页成为桌面应用程序。
简约的Vue驱动的静态站点生成器。
一个神奇的文档站点生成器。

November 3, 2018

深度学习500问，以问答形式对常用的概率知识、线性代数、机器学习、深度学习、计算机视觉等热点问题进行阐述，以帮助自己及有需要的读者。
覃健祥的学习笔记，各种几十分钟入门的文档。
中国运营商IP地址库-每日更新。
使用JavaScript，HTML和CSS构建跨平台桌面应用程序。
最快的shell插件管理器。
一种使用Web技术构建的开源屏幕录像机。
在终端中获取Linux桌面截图的系统/主题信息。

November 2, 2018

PHP 7中的正确舍入，任意精度的十进制浮点运算。
PHP终端NES模拟器。
Sentry是跨平台应用程序监控，专注于错误报告。
浏览器中的Markdown编辑器。
Biny是一个用于Web应用程序的小型高性能PHP框架。
将MySQL binlog解析为您想要的SQL。
输入SQL，输出索引优化建议。
适用于macOS的现代视频播放器。
使我的macOS体验更加惊人的应用程序和工具列表。
使我的iOS体验更加惊人的应用程序和工具列表。

November 1, 2018

程序员应该访问的最佳网站中文版。
使用Golang实现PHP的常见内置函数。
【新】微信开（微信服务号+微信小程序+微信支付）。
MIT-18.06-线性代数-完整笔记。
《Redis Command Reference》全文的中文翻译版。
PHP非侵入式监控平台- 优化性能，定位Bug的神器，别再让你的PHP程序裸奔。
Surge、Quantumult、Kitsunebi、Shadowrocket、Pepi(ShadowRay)、Surfboard 的配置规则文件 。
Swagger整合到Laravel 5。
API接口辅助库包，生成和校验api签名。
以开发人员为中心的HTTP客户端，针对大多数常见用例进行了优化。

October 30, 2018

 收集 PHP 最佳实践、编码规范和权威学习指南，方便 PHP 开发者阅读和查找。
🔗一些有用的程序员网站。
用 vue 写小程序，基于 mpvue 框架重写 weui。
CSS重置的现代替代方案。
立即将JSON转换为浏览器中的Go类型（原始）。
学习如何设计大型系统。
一种自动修复PHP编码标准问题的工具。
确保你的项目没有依赖一些已知易受攻击的依赖。
用于在PHP应用程序中查找错误的静态分析工具。
易于使用的PDO包装器，适用于PHP项目。
密码哈希Argon2，PHC的获胜者。
兼容PHP 5.5附带的password_ *函数。
PHP项目的自动cacert.pem管理。

October 29, 2018

一个简单的Jekyll主题。
SparkPHP框架。
PHP中函数式编程。
百度网盘命令行工具。
PHP的有效，快速，稳定的日志扩展。
📖 原则 · 中文版。
用于将PHP变量记录到Google Chrome控制台。
在Google Chrome中远程执行PHP代码，处理PHP错误，转储变量。
一个适用于的OpenWRT全的平台个人文库翻墙路由方案。
互联网公司技术架构，微信/淘宝/微博/腾讯/阿里/美团点评/百度/Google/Facebook/Amazon/eBay的架构。
后端架构师技术图谱。

October 26, 2018

vuejs Database Manager数据管理系统——前端。
vuejs Database Manager数据管理系统——后端。
A PHP framework for console artisans。
收集整理一些常用的PHP类库，资源以及技巧。

October 25, 2018

吾爱破解论坛 爱盘 down.52pojie.cn 页面的源代码。
微信个人号接口、微信机器人及命令行微信，三十行即可自定义个人号机器人。
基于yaf开发的免费、安全、稳定、高效的发卡系统，值得拥有! 
基于yaf开发的全新、免费、开源、高效好用的知识付费平台（自用型）。
基于Swoole的PHP中的高性能Web框架和应用程序服务器。
适用于Web客户端的gRPC。
类似与PHP Simple HTML DOM Parser，但是比它快好几倍。

October 24, 2018

Vagrant Manager for Windows。
SQL优化器和重写器。
将实时段落，单词和字符计数添加到HTML元素。
让自己轻松选择阿里巴巴风味名称（又名，花名）。
所有算法都在Python中实现。
个人网站即时到账收款解决方案。
适用于PayPal RESTful API的PHP SDK。
百度网盘不限速下载 支持Windows和Mac。
Laravel框架的优雅调试助手。 Telescope可深入了解进入应用程序的请求，异常，日志条目，数据库查询，排队作业，邮件，通知，缓存操作，计划任务，变量转储等。

October 23, 2018

Nginx开发从入门到精通。
opcache状态页面。
dcloudio 支付相关。
DCloud开源项目集锦 http://www.dcloud.io。
每个 JavaScript 工程师都应懂的33个概念。
swoft 框架文件。
macOS的MySQL / MariaDB数据库管理。

October 22, 2018

区块链3.0 -> 超级账本hyperledger fabirc教程 v1.1。
Google Hosts。

October 21, 2018

一些经典且高质量的电子书分享。
中国程序员容易发音错误的单词。
中华人民共和国行政区划：省级（省份直辖市自治区）、 地级（城市）、 县级（区县）、 乡级（乡镇街道）、 村级（村委会居委会） ，中国省市区镇村二级三级四级五级联动地址数据 Node.js 爬虫。
🌈 An elegant dashboard https://d2-projects.github.io/d2-admin/。
.vimrc简单配置，没有插件。

October 20, 2018

在macOS上安装开发环境。
加盐密码哈希：如何正确使用。
Metabase是一个开源的BI工具，最大的特点是具有可视化操作界面的数据分析和查询功能，让不懂SQL得用户可能够快速掌握业务数据，支持团队共享业务数据，并且支持MySQL、Postgresql等多种数据源，部署方便，为企业提供了一个很不错的BI解决方案。
在 Windows 上用 WSL 优雅开发。
适用于开发人员的有用Quick Look插件列表。
Mysql web端sql审核平台。
WordPress 主题 Puma。
这是书籍《深度学习框架PyTorch：入门与实践》的对应代码，但是也可以作为一个独立的PyTorch入门指南和教程。
以各种方式使用RabbitMQ的教程。
纯Python MySQL客户端。

October 19, 2018

Shadowsocks for Windows。
Nginx安装维护入门学习笔记，以及各种实例。
中国最大的API接口管理平台。
A PHP terminal NES emulator。
护网杯2018 easy_laravel Docker环境。
萌音影视 - 在线影视应用 http://www.moeins.cn。
一个简单的图书 SDK，你可以使用它用于获取指定书籍的基本信息。
php开源商城系统，基于swoole、easyswoole框架开发 https://www.fashop.cn。

October 18, 2018

算法学习 Golang 版。
阿里巴巴mysql数据库binlog的增量订阅&消费组件 。
和我一步步部署 kubernetes 集群。
PHPConChina 相关资源。
Redis、Lua、Nginx、OpenResty笔记。
MeepoPS是Meepo PHP Socket的缩写，旨在提供稳定的Socket服务。可以轻松构建在线实时聊天、即时游戏、视频流媒体播放等。

October 17, 2018

Git 常见问题、用法。
国家标准的软件开发文档。
阮一峰 - 技术分享周刊，每周五发布。

October 16, 2018

平常学习中收集的教程整理。
Twitter 的 Snowflake 的PHP版。
基于有赞云和有赞微小店实现个人收款解决方案。
PHPForker是一个PHP多进程编程骨架，借鉴了Workerman诸多优良编程思想，剥离了其中的网络事件库抽象部分，集中围绕多进程编程，为了便于直观的调试以及保持最轻的多进程骨架，所以简单的内嵌了一个基于select多路复用技术的 TCP & UDP Server。
lnmp 一键安装包。

October 15, 2018

Yaf MVC框架集成了一些常用类库。
Valitron是一个简单，优雅，独立的验证库，没有依赖关系。
复制GitHub Markdown样式的最小CSS量。

October 14, 2018

一个结构清晰的，易于维护的，现代的PHP Markdown解析器。
OpenCV-Python-Tutorial。
跨域本地存储，具有权限。
通过缓存整个响应来加速Laravel应用程序。
redis web 客户端。

October 12, 2018

后端架构师技术图谱。
🔴蓝灯最新版本下载。

October 11, 2018

IPsec VPN 服务器一键安装脚本。
搬瓦工一键搭建酸酸 Shad0ws0cks 图文教程。
一个可体验 Windows 95 的app。
从0到1构建分布式秒杀系统，脱离案例讲架构都是耍流氓（数据库，服务器文章）。
找到一个好用的验证码程序(5种验证码)。
Luosimao 创新开发的人机验证，免去了复杂的输入过程，具有更加优秀的操作体验，更加美观的设计，可更好地融入到您的网站中。
一个基于PHP的jQuery中文点击验证码插件 （php, jquery, captcha）。
ckplayer (超酷网页视频播放器),支持http协议下的flv,f4v,mp4,支持rtmp视频流和rtmp视频回放,支持m3u8格式,是你做视频直播,视频点播的理想播放器 http://www.ckplayer.com
🌻 HTML5播放器、M3U8直播/点播、RTMP直播、低延迟、推流/播流地址鉴权、优化浏览器兼容性，HLS+扩展 http://github.tinywan.com/html5-dash-…
一个支持自定义UI布局,流式API, 加密,直播 ,亮度,音量,快进等手势 ,广告视频预览,多种加载模式 ,多种分辨率切换 ,多种封面图, 自定义数据源,列表播放,倍数播放,边播变缓存不是使用AndroidVideoCache,离线播放,神奇的播放器。
SGPlayer 是一款基于 AVPlayer、FFmpeg 的媒体资源播放器框架。支持360°全景视频，VR视频，RTMP、RTSP 等直播流；同时支持 iOS、macOS、tvOS 三个平台。
云教务，摩码创想开源版云教务系统主要由教学、系统、账户三个大模块组成 http://www.yunjiaowu.cn。

October 10, 2018

一款功能强大的 macOS 版微信小助手。
网易云音乐 Node.js API service。
我个人曾经做过的技术分享... http://xiaorui.cc
中文版 《微服务：从设计到部署》。
基于laravel免费的开源IT资产/许可证管理系统。
腾讯云COS对象存储 V5。
运用swoole在浏览器更友好的实现vmstat。

October 9, 2018

Laravel为Sentry整合。
中文人名语料库。中文姓名,姓氏,名字,称呼,日本人名,翻译人名,英文人名。
Wooyun知识库，乌云知识库。
一个能够 Hook 绝大多数函数/类、部分 opcode 的 PHP7 扩展。

October 8, 2018

提供用于规范化composer.json的composer插件。

October 7, 2018

终端主题配色。
spf13-vim。

October 6, 2018

Vue.js的国际化插件。

October 5, 2018

收集那些优秀的软件（Windows & Mac）。

October 4, 2018

《Redis 设计与实现》（网络版）的书稿源码。
Aria2GUI for macOS。

October 3, 2018

Laravel 深入浅出指南 —— Laravel 5.7 源代码解析，新手进阶指南。

October 2, 2018

GitHub 虚假 Star 净网行动。
Go实战开发。

October 1, 2018

hitokoto本地源。

September 30, 2018

在Blade视图中使用自定义html组件。
我的macOS配置：Zsh, Karabiner, VS Code, Sublime, Neovim, Nix, Hammerspoon。
Fira代码：具有编程连字的等宽字体。
具有编程连字的等宽字体。
基于Laravel开发的在线点播系统。

September 29, 2018

蓝灯最新版本下载。
来自Laravel生态系统的精选资源大全，包括书签、包、教程、视频以及其它诸多很酷的资源。
用于备份Laravel应用程序的软件包。
一个用vue写的后台模板

September 28, 2018

PHP开发知识结构。

September 27, 2018

前端笔试面试题题库。
快应用的例子。

September 26, 2018

使用开源Laravel Envoy工具提供基本的“零停机”部署。
QR码生成器。
VPN Chrome是基于Google Chromium的浏览器，具有内置的VPN功能，可让用户以安全和私密的方式上网。
此存储库包含使用大多数纯PHP的比特币实现。
Minera是一个管理和监控比特币采矿硬件的完整系统。
用于与blockchain.info API交互的官方PHP库。
BitWasp是一个开源PHP项目，允许任何人建立一个独立于其他集中服务的安全比特币市场。
10 个你应该知道的 PHP 比特币开源项目。

September 25, 2018

前端发展很快，现代浏览器原生 API 已经足够好用。我们并不需要为了操作 DOM、Event 等再学习一下 jQuery 的 API。同时由于 React、Angular、Vue 等框架的流行，直接操作 DOM 不再是好的模式，jQuery 使用场景大大减少。本项目总结了大部分 jQuery API 替代的方法，暂时只支持 IE10 以上浏览器
ClickHouse是一个面向开源列的数据库管理系统，可以实时生成分析数据报告。
社会需求收集器 - 一些非技术文章。

September 24, 2018

基于PHP的反病毒反木马反恶意软件解决方案。
一个简单的图书 SDK，你可以使用它用于获取指定书籍的基本信息。

September 23, 2018

重新定义微信小程序的开发。
一个安全的私有聊天软件。
微信/支付宝监控个人收款，无需签约支付宝、微信支付。为支付宝、微信支付的个人账户提供即时到账服务。
独立的qrcode生成（不依赖于外部服务）。

September 22, 2018

PHP Server Monitor是一个脚本，用于检查您的网站和服务器是否已启动并正在运行。它带有一个基于Web的用户界面，您可以在其中管理您的服务和网站，还可以使用手机号码和电子邮件地址管理每个服务器的用户。
php-interview-2018: 面试总结。
YApi 是一个可本地部署的、打通前后端及QA的、可视化的接口管理平台。
swoole内核分析，基于swoole2.0.13。
基于 Chrome 和 Vue.js 开发的第三方即刻通知插件。
github徽章服务。

September 21, 2018

Leetcode 题解 (跟随思路一步一步撸出代码) 及经典算法实现。
SimPic是一个开源的PHP图床。
谈谈一些有趣的 CSS 话题。
PHP比特币开发详解：本课程面向初学者，内容即涵盖比特币的核心概念。
Laravel 的中大型專案架構 | 點燈坊。
Gitlab 安装和配置。

September 20, 2018

Serialize closures (anonymous functions) https://opis.io/closure。

September 19, 2018

Laravel 5.7 blog application with Vue.js, Docker, Redis, Horizon and Pusher。
独立开发/自由职业/远程工作资源列表。

September 18, 2018

Laravel 5 系列入门教程。
网页微信PHP登录的实现。
PHP底层内核源码分析和扩展开发。

September 17, 2018

PHP代码职业生涯中的一些小技巧🐘http://easy-tips.tigerb.cn。

September 16, 2018

深度有趣 - 人工智能实战项目合集。
为互联网IT人打造的中文版awesome-go。
开源书籍大搜罗。

September 15, 2018

基于 Swoole 开发的协程 PHP 开发框架，常驻内存、协程异步。
Golang标准库。对于程序员而言，标准库与语言本身同样重要，它好比一个百宝箱，能为各种常见的任务提供完美的解决方案。以示例驱动的方式讲解Golang的标准库。

September 14, 2018

又一个Linux VPS测评脚本。
高性能, 并发抢占锁, 并发队列锁。
支持多家云存储的云盘系统。

September 13, 2018

一个网站部署包。
RSSHub 是一个轻量、易于扩展的 RSS 生成器，可以给任何奇奇怪怪的内容生成 RSS 订阅源。

September 12, 2018

搜索互动地图以了解任何内容 https://learn-anything.xyz。
一堆初中生写的类库、框架。
前端精读周刊。

September 11, 2018

2018前端常见题汇总，不定时更新。

September 10, 2018

Node.js API 中文文档。

September 9, 2018

eoLinker是国内最大的在线API接口管理平台，提供自动生成API文档、API自动化测试、Mock测试、团队协作等功能。
V2EX 撕逼大战。
一款功能强大的 macOS 版微信小助手。

September 8, 2018

📚 免费的计算机编程类中文书籍。
可能是让你受益匪浅的英语进阶指南。
用 PHP 像开发网站一样开发桌面应用软件。

September 7, 2018

中华人民共和国居民身份证号码验证工具。

September 6, 2018

php7.x 新特性。
Go语言高级编程 (Advanced Go Programming)。

September 4, 2018

《Go语言高级编程》开源图书，涵盖CGO、Go汇编语言、RPC实现、Protobuf插件实现、Web框架实现、分布式系统等高阶主题。
PHP 面试知识点汇总。
PHP工程师面试题目。
笔记、Laravel、PHP、面试题、HTML、CSS。

September 3, 2018

收集&推荐优秀的 Apps/硬件/技巧/周边等。

September 2, 2018

一个收集在 GitHub 上作弊用户的黑名单项目
中华新华字典数据库。包括歇后语，成语，词语，汉字。提供新华字典API。
大型系统设计的基础知识
当···时发生了什么？
GitHub 上最神奇的项目之一应该是这个了，一行代码都没，但有 2w+ stars。
Awesome macOS open source applications
使用Microsoft Style的GitHub主题。

August 30, 2018

awesome-chrome-devtools
clockwork-chrome - Clockwork是一个浏览器扩展，提供调试和分析PHP应用程序的工具，包括请求数据，应用程序日志，数据库查询，路由，应用程序运行时的可视化等。
remotedebug-gateway - 允许您一次将客户端连接到多个浏览器
AwesomeList top

August 29, 2018

浅谈常见的NoSQL技术方案和选型
A collection of awesome browser extensions for GitHub
composer 范例包
PHP 开发者该知道的 5 个 Composer 小技巧
github star 整理
pt-query-digest: 从日志，进程列表和 tcpdump 分析 MySQL 查询
MySQL 生成千万级的测试数据

",170
gosuri/uiprogress,Go,"uiprogress  
A Go library to render progress bars in terminal applications. It provides a set of flexible features with a customizable API.

Progress bars improve readability for terminal applications with long outputs by providing a concise feedback loop.
Features

Multiple Bars: uiprogress can render multiple progress bars that can be tracked concurrently
Dynamic Addition:  Add additional progress bars any time, even after the progress tracking has started
Prepend and Append Functions: Append or prepend completion percent and time elapsed to the progress bars
Custom Decorator Functions: Add custom functions around the bar along with helper functions

Usage
To start listening for progress bars, call uiprogress.Start() and add a progress bar using uiprogress.AddBar(total int). Update the progress using bar.Incr() or bar.Set(n int). Full source code for the below example is available at example/simple/simple.go
uiprogress.Start()            // start rendering
bar := uiprogress.AddBar(100) // Add a new bar

// optionally, append and prepend completion and elapsed time
bar.AppendCompleted()
bar.PrependElapsed()

for bar.Incr() {
  time.Sleep(time.Millisecond * 20)
}
This will render the below in the terminal

Using Custom Decorators
You can also add a custom decorator function in addition to default bar.AppendCompleted() and bar.PrependElapsed() decorators. The below example tracks the current step for an application deploy progress. Source code for the below example is available at example/full/full.go
var steps = []string{""downloading source"", ""installing deps"", ""compiling"", ""packaging"", ""seeding database"", ""deploying"", ""staring servers""}
bar := uiprogress.AddBar(len(steps))

// prepend the current step to the bar
bar.PrependFunc(func(b *uiprogress.Bar) string {
  return ""app: "" + steps[b.Current()-1]
})

for bar.Incr() {
  time.Sleep(time.Millisecond * 10)
}
Rendering Multiple bars
You can add multiple bars using uiprogress.AddBar(n). The below example demonstrates updating multiple bars concurrently and adding a new bar later in the pipeline. Source for this example is available at example/multi/multi.go
waitTime := time.Millisecond * 100
uiprogress.Start()

// start the progress bars in go routines
var wg sync.WaitGroup

bar1 := uiprogress.AddBar(20).AppendCompleted().PrependElapsed()
wg.Add(1)
go func() {
  defer wg.Done()
  for bar1.Incr() {
    time.Sleep(waitTime)
  }
}()

bar2 := uiprogress.AddBar(40).AppendCompleted().PrependElapsed()
wg.Add(1)
go func() {
  defer wg.Done()
  for bar2.Incr() {
    time.Sleep(waitTime)
  }
}()

time.Sleep(time.Second)
bar3 := uiprogress.AddBar(20).PrependElapsed().AppendCompleted()
wg.Add(1)
go func() {
  defer wg.Done()
  for i := 1; i <= bar3.Total; i++ {
    bar3.Set(i)
    time.Sleep(waitTime)
  }
}()

// wait for all the go routines to finish
wg.Wait()
This will produce

Incr counter
Bar.Incr() is an atomic counter and can be used as a general tracker, making it ideal for tracking progress of work fanned out to a lots of go routines. The source code for the below example is available at example/incr/incr.go
runtime.GOMAXPROCS(runtime.NumCPU()) // use all available cpu cores

// create a new bar and prepend the task progress to the bar and fanout into 1k go routines
count := 1000
bar := uiprogress.AddBar(count).AppendCompleted().PrependElapsed()
bar.PrependFunc(func(b *uiprogress.Bar) string {
  return fmt.Sprintf(""Task (%d/%d)"", b.Current(), count)
})

uiprogress.Start()
var wg sync.WaitGroup

// fanout into go routines
for i := 0; i < count; i++ {
  wg.Add(1)
  go func() {
    defer wg.Done()
    time.Sleep(time.Millisecond * time.Duration(rand.Intn(500)))
    bar.Incr()
  }()
}
time.Sleep(time.Second) // wait for a second for all the go routines to finish
wg.Wait()
uiprogress.Stop()
Installation
$ go get -v github.com/gosuri/uiprogress
Todos

 Resize bars and decorators by auto detecting window's dimensions
 Handle more progress bars than vertical screen allows

License
uiprogress is released under the MIT License. See LICENSE.
",1261
JiejayLan/CSC322_group_project,JavaScript,"Report and Documents
visit our Wiki Page

Getting started
I. clone repo
II. setup local and remote branch
III. using npm or yarn to install all dependencies on your machine
IV. setup .env.development and .env.test by heading to console of firebase

create .env.development and .env.test at root of project folder
open console of firebase
click Mini-eByMazon and click </>
setup key-value pair inside .env.development in following format

    FIREBASE_API=values copy from firebase without double quotes
    FIREBASE_AUTH_DOMAIN=values copy from firebase without double quotes
    FIREBASE_DATABASE_URL=values copy from firebase without double quotes
    FIREBASE_PROJECT_ID=values copy from firebase without double quotes
    FIREBASE_STORAGE_BUCKET=values copy from firebase without double quotes
    FIREBASE_MESSAGING_SENDER_ID=values copy from firebase without double quotes


click Mini-eByMazon Test and click </>
setup key-value pair inside .env.test in the same format in step 4 with values for Mini-eByMazon Test


Suggested Tools
React Developer Tools and Redux DevTools installed for Google Chrome

How to login
For login page, you dont't need to enter any username or password, because I set up a corrrect default username(""jay"") and password(""123"") in login-page react state.

Suggestion for development

""npm run deve"" for development //still can't set up the reload package to reload the page automatically
put component and pages into different folders
don't connet to firebase from client side.You should add a route controller on the server folder. You can take a look at how I write the login page

",4
ad-fidelitas/ctf-admin-website,Vue,"ctf-admin-website
Project setup
npm install

Compiles and hot-reloads for development
npm run serve

Compiles and minifies for production
npm run build

Run your tests
npm run test

Lints and fixes files
npm run lint

Customize configuration
See Configuration Reference.
",2
Tobybsmith/falppy_bird,Processing,"falppy_bird
",2
Pttn/rieMiner,Assembly,"rieMiner 0.9
rieMiner is a Riecoin miner supporting both solo and pooled mining. It was originally adapted and refactored from gatra's cpuminer-rminerd (https://github.com/gatra/cpuminer-rminerd) and dave-andersen's fastrie (https://github.com/dave-andersen/fastrie), though there is no remaining code from rminerd anymore.
Solo mining is done using the GetBlockTemplate protocol, while pooled mining is via the Stratum protocol. A benchmark mode is also proposed to compare more easily the performance between different computers.
Direct links to the latest official Windows x64 and Win32 standalone executables. Binaries built on Debian 9 with almost complete static linking also available (these should run on fresh Debian and Ubuntu installations): Deb64 and Deb32. Also note that 32 bits builds are much slower.
This README serves as manual for rieMiner, and you can also find a PDF version (without build instructions). I hope that this program will be useful for you!
The Riecoin community thanks you for your participation, you will be a contributor to the robustness of the Riecoin network. Happy mining!

I provide a Profitability Calculator here.
Minimum requirements
Only x64 systems with SSE are supported.

Windows 7 or later, or recent enough Linux;
x64 CPU with SSE instruction set;
1 GiB of RAM (the prime table limit must be manually set at a lower value in the options).

Recommended:

Windows 10 or Debian 9;
Intel Core i7 6700 or better, or AMD Ryzen R5 1600 or better;
8 GiB of RAM.

Compile this program
In Debian/Ubuntu x64
You can compile this C++ program with g++, as, m4 and make, install them if needed. Then, get if needed the following dependencies:

Jansson
cURL
libSSL
GMP

On a recent enough Debian or Ubuntu, you can easily install these by doing as root:
apt install g++ make m4 git libjansson-dev libcurl4-openssl-dev libssl-dev libgmp-dev
Then, just download the source files, go/cd to the directory, and do a simple make:
git clone https://github.com/Pttn/rieMiner.git
cd rieMiner
make
For other Linux, executing equivalent commands (using pacman instead of apt,...) should work.
If you get a warning after the compilation that there may be a conflict between libcrypto.so files, install libssl1.0-dev instead of libssl-dev.
In Windows x64
You can compile rieMiner in Windows, and here is one way to do this. First, install MSYS2 (follow the instructions on the website), then enter in the MSYS MinGW-w64 console, and install the tools and dependencies:
pacman -S make git
pacman -S mingw64/mingw-w64-x86_64-gcc
pacman -S mingw64/mingw-w64-x86_64-curl
Note that you must install the mingw64/mingw-w64-x86_64-... packages and not just gcc or curl.
Clone rieMiner with git like for Linux, go to its directory with cd, and compile with make.
Static building
The produced executable will only run in the MSYS console, or if all the needed DLLs are next to the executable. To obtain a standalone executable, you need to link statically the dependencies. Unfortunately, libcurl will give you a hard time, and you need to compile it yourself.
First, download the latest official libcurl code on their website, under ""Source Archives"", and decompress the folder somewhere (for example, next to the rieMiner's one).
In the MSYS MinGW-w64 console, cd to the libcurl directory. We will now configure it to not build unused features, then compile it:
./configure --disable-dict --disable-file --disable-ftp --disable-gopher --disable-imap --disable-ldap --disable-ldaps --disable-pop3 --disable-rtsp --disable-smtp --disable-telnet --disable-tftp --without-ssl --without-libssh2 --without-zlib --without-brotli --without-libidn2  --without-ldap  --without-ldaps --without-rtsp --without-psl --without-librtmp --without-libpsl --without-nghttp2 --disable-shared --disable-libcurl-option
make
Once done:

Create ""incs"" and ""libs"" folders in the rieMiner directory;
In the downloaded libcurl directory, go to the include directory and copy the ""curl"" folder to the ""incs"" folder;
Do the same with the file ""libcurl.a"" from the libs/.lib folder to the rieMiner's ""libs"" folder.

Now, you should be able to compile rieMiner with make static and produce a standalone executable.
Run and configure this program
You can finally run the newly created rieMiner executable using
./rieMiner
If no ""rieMiner.conf"" next to the executable was found, you will be assisted to configure rieMiner. Answer to its questions to start mining. If there is a ""rieMiner.conf"" file next to the executable with incorrect information that was read, you can delete this to get the assistant.
Alternatively, you can create or edit this ""rieMiner.conf"" file next to the executable yourself, in order to provide options to the miner. The rieMiner.conf syntax is very simple: each option is given by a line such
Option type = Option value

It is case sensitive, but spaces and invalid lines are ignored. A line starting with ""#"" will also be ignored. Do not put ; at the end or use other delimiters than = for each line, and do not confuse rieMiner.conf with riecoin.conf! If an option is missing, the default value(s) will be used. If there are duplicate lines, the last one will be used. Here is a sample configuration file for solo mining, with comments explaining the main available options.
# Mining mode: Solo for solo mining via GetBlockTemplate, Pool for pooled mining using Stratum, Benchmark for testing. Default: Benchmark
Mode = Solo

# IP and port of the Riecoin wallet/server or pool. Default: 127.0.0.1 (your computer), port 28332 (default port for Riecoin-Qt)
Host = 127.0.0.1
Port = 28332

# Username and password used to connect to the server (same as rpcuser and rpcpassword in riecoin.conf for solo mining).
# If using Stratum, the username includes the worker name (username.worker). Default: empty values
Username = user
Password = /70P$€CR€7/

# Custom payout address for solo mining (GetBlockTemplate only). Default: this donation address
PayoutAddress = RPttnMeDWkzjqqVp62SdG2ExtCor9w54EB

# Number of threads used for mining. Default: 8
Threads = 8

# The prime table used for mining will contain primes up to the given number.
# Use a bigger limit if you have 16 GiB of available RAM or more, as this will reduce the ratio between the n-tuple and (n + 1)-tuple counts (but also the 1-tuple find rate).
# Reduce if you have less than 8 GiB of RAM (or if you want to reduce memory usage).
# It can go up to 2^64 - 1, but setting this at more than 2^33 will usually be too much and decrease performance. Default: 2^31
PrimeTableLimit = 2147483648

# Refresh rate of the stats in seconds. 0 to disable them and only notify when a long enough tuple or share is found, or when the network finds a block. Default: 30
RefreshInterval = 60

# For solo mining, submit not only blocks (6-tuples) but also k-tuples of at least the given length.
# Additionally, the base prime of such tuple will be shown in the Benchmark Mode. Default: 6
TupleLengthMin = 4

# For solo mining, add consensus rules in the GetBlockTemplate RPC call, each separated by a comma.
# Useful for softforks, for example, to mine SegWit transactions, you would need the following line. Default: no rule
# Rules = segwit

# Other options
# BenchmarkDifficulty = 1600
# BenchmarkTimeLimit = 0
# Benchmark2tupleCountLimit = 100000
# SieveBits = 25
# SieveWorkers = 0
# ConstellationType = 0, 4, 2, 4, 2, 4
# PrimorialNumber = 40
# PrimorialOffsets = 4209995887, 4209999247, 4210002607, 4210005967, 7452755407, 7452758767, 7452762127, 7452765487, 8145217177, 8145220537, 8145223897, 8145227257
# Debug = 0

It is also possible to use custom configuration file paths, examples:
./rieMiner config/example.txt
./rieMiner ""config 2.conf""
./rieMiner /home/user/rieMiner/rieMiner.conf
Benchmark Mode options

BenchmarkDifficulty : sets the testing difficulty (must be from 265 to 32767). Default: 1600;
BenchmarkTimeLimit : sets the testing duration in s. 0 for no time limit. Default: 0;
Benchmark2tupleCountLimit : stops testing after finding this number of 2-tuples. 0 for no limit. Default: 50000.

Advanced/Tweaking/Dev options
They can be useful to get better performance depending on your computer.

SieveBits : size of the segment sieve is 2^SieveBits bits, e.g. 25 means the segment sieve size is 4 MiB. Choose this so that SieveWorkers*SieveBits fits in your L3 cache. Default: 25;
SieveWorkers : the number of threads to use for sieving. Increasing it may solve some CPU underuse problems, but will use more memory. 0 for choosing automatically based on number of Threads and PrimeTableLimit. Default: 0.

These ones should never be modified outside developing purposes and research for now.

ConstellationType : set your Constellation Type, i. e. the primes tuple offsets, each separated by a comma. Default: 0, 4, 2, 4, 2, 4 (values for Riecoin mining);
PrimorialNumber : Primorial Number for the Wheel Factorization. Default: 40;
PrimorialOffsets : list of Offsets from the Primorial for the first number in the prime tuple. Same syntax as ConsType. Default: carefully chosen offsets;
Debug : activate Debug Mode: rieMiner will print a lot of debug messages. Set to 1 to enable, 0 to disable. Other values may introduce some more specific debug messages. Default : 0.

Some possible constellations types (format: (type) -> offsets to put for ConstellationType ; 3 first constellations (n + 0) which can be used for PrimorialOffsets, though some might not work)

5-tuples

(0, 2, 6,  8, 12) -> 0, 2, 4, 2, 4 ; 5, 11, 101,...
(0, 4, 6, 10, 12) -> 0, 4, 2, 4, 2 ; 7, 97, 1867,...


6-tuples

(0, 4, 6, 10, 12, 16) -> 0, 4, 2, 4, 2, 4 (Riecoin) ; 7, 97, 16057,...


7-tuples

(0, 2, 6,  8, 12, 18, 20) -> 0, 2, 4, 2, 4, 6, 2 ; 11, 165701, 1068701,...
(0, 2, 8, 12, 14, 18, 20) -> 0, 2, 6, 4, 2, 4, 2 ; 5639, 88799, 284729,...


8-tuples

(0, 2, 6,  8, 12, 18, 20, 26) -> 0, 2, 4, 2, 4, 6, 2, 6 ; 11, 15760091, 25658441,...
(0, 2, 6, 12, 14, 20, 24, 26) -> 0, 2, 4, 6, 2, 6, 4, 2 ; 17, 1277, 113147,...
(0, 6, 8, 14, 18, 20, 24, 26) -> 0, 6, 2, 6, 4, 2, 4, 2 ; 88793, 284723, 855713,...



Also see the constellationsGen tool in my rieTools repository (https://github.com/Pttn/rieTools).
Memory problems
If you have memory errors (Unable to allocate... or Bad Allocs), try to lower the PrimeTableLimit value in the configuration file.
Statistics
rieMiner will regularly print some stats, and the frequency of this can be changed with the RefreshInterval parameter as said earlier.
For solo mining, rieMiner will regularly show the primes per second speed, and the 1 to 2-tuples/s ratio. From this, it will also estimate the average time to find a block (note that all the ratios are the same, and the estimation should be fairly precise). Of course, even if the average time to find a block is for example 2 days, you could find a block in the next hour as you could find nothing during a week. The number of 2 to 6-tuples found since the start of the mining is also shown.
For pooled mining, the shares per minute metric and the numbers of valid and total shares are shown instead. As it is hard to get a correct earnings estimation from k-shares, no other metric is shown. The Benchmark Mode (or solo mining) can be used to get better figures for comparisons.
rieMiner will also notify if it found a k-tuple (k >= Tuples option value) in solo mining or a share in pooled mining, and if the network found a new block. If it finds a block or a share, it will tell if the submission was accepted (solo mining only) or not. For solo mining, if the block was accepted, the reward will be generated for the address specified in the options. You can then spend it after 100 confirmations. Note that orphaned blocks will be shown as accepted.
Solo mining specific information
Note that other ways for solo mining (protocol proxies,...) were never tested with rieMiner. It was written specifically for the official wallet and the existing Riecoin pools.
Configure the Riecoin wallet for solo mining
We assume that Riecoin Core is already working and synced. To solo mine with it, you have to configure it.

Find the riecoin.conf configuration file. It should be located in /home/username/.riecoin or equivalent in Windows;
Do not confuse this file with the rieMiner.conf!
An example of riecoin.conf content suitable for mining is

rpcuser=(username)
rpcpassword=(password)
rpcport=28332
port=28333
rpcallowip=127.0.0.1
server=1
daemon=1

If you feel the need, you can add more nodes manually with connect=(nodeip), (nodeip) after connect being a node's IP. You can find a list of the nodes connected the last 24 h here: https://chainz.cryptoid.info/ric/#!network.
If you wish to mine from another computer, add another rpcallowip=ip.of.the.computer, or else the connection will be refused. Choose a username and a password and replace (username) and (password).
Work control
You might have to wait some consequent time before finding a block. What if something is actually wrong and then the time the miner finally found a block, the submission fails?
First, if for some reason rieMiner disconnects from the wallet (you killed it or its computer crashed), it will detect that it has not received the mining data and then just stop mining: so if it is currently mining, everything should be fine.
If you are worried about the fact that the block will be incorrectly submitted, here comes the TupleLengthMin option. Indeed, you can send invalid blocks to the wallet (after all, it is yours), and check if the wallet actually received them and if these submissions are properly processed. When such invalid block is submitted, you can check the debug.log file in the same location as riecoin.conf, and then, you should see something like
ERROR: CheckProofOfWork() : n+10 not prime

Remember that the miner searches numbers n such that n, n + 2, n + 6, n + 10, n + 12 and n + 16 are prime, so if you set the TupleLengthMin option to for example 3, rieMiner will submit a n such that n, n + 2 and n + 6 are prime, but not necessarily the other numbers, so you can conclude that the wallet successfully decoded the submission here, and that everything works fine. If you see nothing or another error message, then something is wrong (possible example would be an unstable overclock)...
Also watch regularly if the wallet is correctly syncing, especially if the message Blockheight = ... did not appear since a very long time (except if the network is mining the superblock). In Riecoin-Qt, this can be done by hovering the check at the lower right corner, and comparing the number with the latest block found in a Riecoin explorer. If something is wrong, try to change the nodes in riecoin.conf or check your connection.
Pooled mining specific information
Existing pools:

XPoolX

Host = mining.xpoolx.com
Port = 5000
Owner: xpoolx - info@xpoolx.com
They also support Solo mining via Stratum with a 5% fee


uBlock.it

Host = mine.ublock.it or mine.blockocean.com
Port = 5000
Owner: ziiip - netops.ublock.it@gmail.com



The miner will disconnect if it did not receive anything during 3 minutes (time out).
Benchmarking
rieMiner provides a way to test the performance of a computer, and compare with others. This feature can also be used to appreciate the improvements when trying to improve the miner algorithm. When sharing benchmark results, you must always communicate the difficulty, the prime table limit (PTL), the test duration, the CPU model, the memory speeds (frequency and CL), the miner version, and the OS. Also, do not forget to precise if you changed other options, like the SieveWorkers or Bits.
To compare two different platforms or settings, you must absolutely test with the same difficulty, during enough time. The proposed parameters, conditions and interpretations for serious benchmarking are:

Standard Benchmark

Difficulty of 1600;
PTL of 2^31 = 2147483648;
No time limit;
Stop after finding 50000 2-tuples or more;
The computer must not do anything else during testing;
The system must not swap. Else, the result would not make much sense. Ensure that you have enough memory when benchmarking.



The test will be fairly long, but similar to the real mining conditions. Once the benchmark finished itself (not by the user), it will print something like:
100000 2-tuples found, test finished. rieMiner 0.9, difficulty 1600, PTL 2147483648
BENCHMARK RESULTS: 233.354130 primes/s with ratio 28.955020 -> 0.990626 block(s)/day

Generally speaking, the block(s)/day metric is the one that should be shared or used to compare performance, though it is always good to also take in consideration the other ones. Moreover, for a given difficulty and PTL, the ratio should be the same, and the more precise primes/day metric can be used instead for comparisons.
The precision will be about 2 significant digits for the block(s)/day. To get 3 solid digits, about 1 million of 2-tuples would need to be found, which would be way too long to be practical for the Standard Benchmark.
A run with valid parameters for the Standard Benchmark will additionally print the message
VALID parameters for Standard Benchmark

Which should appear if you want to share your results.
You could stop before 50000 2-tuples, for example at 10000, if you just want a rough estimation of the performance. However, even after this long, the values are often still very imprecise, and can lead to confusion, like a slightly slower computer getting better results. This remark is critical for people wanting to optimize the miner.
A few results
Done with rieMiner 0.9, 100000 2-tuples except otherwise said. Unit: primes/s

AMD Ryzen R7 2700X @4 GHz, DDR4 3200 CL14, Debian 9: 235.856209
AMD Ryzen R7 2700X @4 GHz, DDR4 2400 CL15, Debian 9: 233.354130
AMD Ryzen R7 2700X @3 GHz, DDR4 2400 CL15, Debian 9: 177.234506
Intel Core i7 6700K @3 GHz, DDR4 2400 CL15, Debian 9: 89.288621
Intel Core 2 Quad Q9650 @3 GHz, DDR3 1067 CL6, Debian 9: 40.673097
Intel Pentium D 925 @3 GHz, DDR3 1000 CL6, Debian 9: 7.466797 (10000 2-tuples)

As said, we should use the primes/s metric for fixed difficulty and PTL. The ratio for the Standard Benchmark is about 28.9.
For a given architecture, the performance is basically proportional to the number of cores and frequency. However, we notice that much better RAM doesn't really matter.
Miscellaneous
Unless the weather is very cold, I do not recommend to overclock a CPU for mining, unless you can do that without increasing noticeably the power consumption. My 2700X computer would draw much, much more power at 4 GHz/1.2875 V instead of 3.7 GHz/1.08125 V, which is certainly absurd for a mere 8% increase. To get maximum efficiency, you might want to find the frequency with the best performance/power consumption ratio (which could also be obtained by underclocking the processor).
If you can, try to undervolt the CPU to reduce power consumption, heat and noise.
Developers and license

Pttn, author and maintainer, contact: dev at Pttn dot me

Parts coming from other projects and libraries are subject to their respective licenses. Else, this work is released under the MIT license. See the LICENSE or top of source files for details.
Notable contributors

Michael Bell: assembly optimizations, improvements of work management between threads, and some more.

Versioning
The version naming scheme is 0.9, 0.99, 0.999 and so on for major versions, analogous to 1.0, 2.0, 3.0,.... The first non 9 decimal digit is minor, etc. For example, the version 0.9925a can be thought as 2.2.5a. A perfect bug-free software will be version 1. No precise criteria have been decided about incrementing major or minor versions for now.
Contributing
Feel free to do a pull request or open an issue, and I will review it. I am open for adding new features, but I also wish to keep this project minimalist. Any useful contribution will be welcomed.
By contributing to rieMiner, you accept to place your code in the MIT license.
Donations welcome:

Bitcoin: 1PttnMeD9X6imTsRojmhHa1rjudW8Bjok5
Riecoin: RPttnMeDWkzjqqVp62SdG2ExtCor9w54EB
Gapcoin: GgCyVr6y6beBbTofmTLJHvGc1NCWynQyvw
Ethereum: 0x32de6b854b6a05448b4f25d4496990bece8a2862

Quick contributor's checklist

Your code must compile and work on recent Debian based distributions, and Windows using MSYS;
If modifying the miner, you must ensure that your changes do not cause any performance loss. You have to do proper and long enough before/after benchmarks;
rieMiner must work for any realistic setting, at least try these in the Benchmark Mode (and do some actual mining):

Difficulty 304, PTL 2^20 (Testnet mining conditions);
Difficulty 800, PTL 2^27;
Difficulty 1600, PTL 2^31 (Standard Benchmark, similar to real mining conditions);
Difficulty 3200, PTL 2^31 or more (we will eventually reach such Difficulties someday...).


Ensure that your changes did not break anything, even if it compiles. Examples (if applicable):

There should never be random (or not) segmentation faults or any other bug, try to do actual mining with Gdb, debugging symbols and Debug Mode enabled during hours or even days to catch possible bugs;
Ensure that valid work is produced (pools and Riecoin-Qt must not reject submissions);
Mining must stop completely while disconnected and restart properly when connection is established again.


Follow the style of the rest of the code (curly braces position, camelCase variable names, tabs and not spaces, spaces around + and - but not around * and /,...).

Resources

rieMiner thread on Riecoin-Community.com forum
My personal website about Riecoin
Get the Riecoin wallet
Fast prime cluster search - or building a fast Riecoin miner (part 1), nice article by dave-andersen explaining how Riecoin works and how to build an efficient miner and the algorithms. Unfortunately, he never published part 2...
Riecoin FAQ and technical aspects
Bitcoin Wiki - Getblocktemplate
BIP141 (Segwit)
Bitcoin Wiki - Stratum

",2
vinta/awesome-python,Python,"Awesome Python 
A curated list of awesome Python frameworks, libraries, software and resources.
Inspired by awesome-php.

Awesome Python

Admin Panels
Algorithms and Design Patterns
Audio
Authentication
Build Tools
Built-in Classes Enhancement
Caching
ChatOps Tools
CMS
Code Analysis
Command-line Tools
Compatibility
Computer Vision
Concurrency and Parallelism
Configuration
Cryptography
Data Analysis
Data Validation
Data Visualization
Database Drivers
Database
Date and Time
Debugging Tools
Deep Learning
DevOps Tools
Distributed Computing
Distribution
Documentation
Downloader
E-commerce
Editor Plugins and IDEs
Email
Environment Management
Files
Foreign Function Interface
Forms
Functional Programming
Game Development
Geolocation
GUI
Hardware
HTML Manipulation
HTTP
Image Processing
Implementations
Interactive Interpreter
Internationalization
Job Scheduler
Logging
Machine Learning
Miscellaneous
Natural Language Processing
Network Virtualization
Networking
News Feed
ORM
Package Management
Package Repositories
Permissions
Processes
Queue
Recommender Systems
RESTful API
Robotics
RPC Servers
Science
Search
Serialization
Serverless Frameworks
Specific Formats Processing
Static Site Generator
Tagging
Template Engine
Testing
Text Processing
Third-party APIs
URL Manipulation
Video
Web Asset Management
Web Content Extracting
Web Crawling & Web Scraping
Web Frameworks
WebSocket
WSGI Servers


Services

Code Quality
Continuous Integration


Resources

Podcasts
Twitter
Websites
Weekly


Other Awesome Lists
Contributing


Admin Panels
Libraries for administrative interfaces.

ajenti - The admin panel your servers deserve.
django-grappelli - A jazzy skin for the Django Admin-Interface.
django-suit - Alternative Django Admin-Interface (free only for Non-commercial use).
django-xadmin - Drop-in replacement of Django admin comes with lots of goodies.
flask-admin - Simple and extensible administrative interface framework for Flask.
flower - Real-time monitor and web admin for Celery.
wooey - A Django app which creates automatic web UIs for Python scripts.

Algorithms and Design Patterns
Python implementation of algorithms and design patterns.

algorithms - Minimal examples of data structures and algorithms in Python.
PyPattyrn - A simple yet effective library for implementing common design patterns.
python-patterns - A collection of design patterns in Python.
sortedcontainers - Fast, pure-Python implementation of SortedList, SortedDict, and SortedSet types.

Audio
Libraries for manipulating audio and its metadata.

Audio

audioread - Cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding.
dejavu - Audio fingerprinting and recognition.
mingus - An advanced music theory and notation package with MIDI file and playback support.
pyAudioAnalysis - Python Audio Analysis Library: Feature Extraction, Classification, Segmentation and Applications
pydub - Manipulate audio with a simple and easy high level interface.
TimeSide - Open web audio processing framework.


Metadata

beets - A music library manager and MusicBrainz tagger.
eyeD3 - A tool for working with audio files, specifically MP3 files containing ID3 metadata.
mutagen - A Python module to handle audio metadata.
tinytag - A library for reading music meta data of MP3, OGG, FLAC and Wave files.



Authentication
Libraries for implementing authentications schemes.

OAuth

authlib - JavaScript Object Signing and Encryption draft implementation.
django-allauth - Authentication app for Django that ""just works.""
django-oauth-toolkit - OAuth 2 goodies for Django.
oauthlib - A generic and thorough implementation of the OAuth request-signing logic.
python-oauth2 - A fully tested, abstract interface to creating OAuth clients and servers.
python-social-auth - An easy-to-setup social authentication mechanism.


JWT

pyjwt - JSON Web Token implementation in Python.
python-jose - A JOSE implementation in Python.
python-jwt - A module for generating and verifying JSON Web Tokens.



Build Tools
Compile software from source code.

BitBake - A make-like build tool for embedded Linux.
buildout - A build system for creating, assembling and deploying applications from multiple parts.
PlatformIO - A console tool to build code with different development platforms.
pybuilder - A continuous build tool written in pure Python.
SCons - A software construction tool.

Built-in Classes Enhancement
Libraries for enhancing Python built-in classes.

dataclasses - (Python standard library) Data classes.
attrs - Replacement for __init__, __eq__, __repr__, etc. boilerplate in class definitions.
bidict - Efficient, Pythonic bidirectional map data structures and related functionality..
Box - Python dictionaries with advanced dot notation access.
DottedDict - A library that provides a method of accessing lists and dicts with a dotted path notation.

CMS
Content Management Systems.

wagtail - A Django content management system.
django-cms - An Open source enterprise CMS based on the Django.
feincms - One of the most advanced Content Management Systems built on Django.
Kotti - A high-level, Pythonic web application framework built on Pyramid.
mezzanine - A powerful, consistent, and flexible content management platform.
plone - A CMS built on top of the open source application server Zope.
quokka - Flexible, extensible, small CMS powered by Flask and MongoDB.

Caching
Libraries for caching data.

beaker - A WSGI middleware for sessions and caching.
django-cache-machine - Automatic caching and invalidation for Django models.
django-cacheops - A slick ORM cache with automatic granular event-driven invalidation.
dogpile.cache - dogpile.cache is next generation replacement for Beaker made by same authors.
HermesCache - Python caching library with tag-based invalidation and dogpile effect prevention.
pylibmc - A Python wrapper around the libmemcached interface.
python-diskcache - SQLite and file backed cache backend with faster lookups than memcached and redis.

ChatOps Tools
Libraries for chatbot development.

errbot - The easiest and most popular chatbot to implement ChatOps.

Code Analysis
Tools of static analysis, linters and code quality checkers. See: awesome-static-analysis.

Code Analysis

coala - Language independent and easily extendable code analysis application.
code2flow - Turn your Python and JavaScript code into DOT flowcharts.
prospector - A tool to analyse Python code.
pycallgraph - A library that visualises the flow (call graph) of your Python application.


Code Linters

flake8 - A wrapper around pycodestyle, pyflakes and McCabe.
pylint - A fully customizable source code analyzer.
pylama - A code audit tool for Python and JavaScript.
Code Formatters
black - The uncompromising Python code formatter.
yapf - Yet another Python code formatter from Google.


Static Type Checkers

mypy - Check variable types during compile time.
pyre-check - Performant type checking.


Static Type Annotations Generators

MonkeyType - A system for Python that generates static type annotations by collecting runtime types



Command-line Tools
Libraries for building command-line application.

Command-line Application Development

cement - CLI Application Framework for Python.
click - A package for creating beautiful command line interfaces in a composable way.
cliff - A framework for creating command-line programs with multi-level commands.
clint - Python Command-line Application Tools.
docopt - Pythonic command line arguments parser.
python-fire - A library for creating command line interfaces from absolutely any Python object.
python-prompt-toolkit - A library for building powerful interactive command lines.


Terminal Rendering

asciimatics - A package to create full-screen text UIs (from interactive forms to ASCII animations).
bashplotlib - Making basic plots in the terminal.
colorama - Cross-platform colored terminal text.


Productivity Tools

cookiecutter - A command-line utility that creates projects from cookiecutters (project templates).
doitlive - A tool for live presentations in the terminal.
howdoi - Instant coding answers via the command line.
PathPicker - Select files out of bash output.
percol - Adds flavor of interactive selection to the traditional pipe concept on UNIX.
thefuck - Correcting your previous console command.
tmuxp - A tmux session manager.
try - A dead simple CLI to try out python packages - it's never been easier.


CLI Enhancements

httpie - A command line HTTP client, a user-friendly cURL replacement.
kube-shell - An integrated shell for working with the Kubernetes CLI.
mycli - A Terminal Client for MySQL with AutoCompletion and Syntax Highlighting.
pgcli - Postgres CLI with autocompletion and syntax highlighting.
saws - A Supercharged aws-cli.



Compatibility
Libraries for migrating from Python 2 to 3.

python-future - The missing compatibility layer between Python 2 and Python 3.
python-modernize - Modernizes Python code for eventual Python 3 migration.
six - Python 2 and 3 compatibility utilities.

Computer Vision
Libraries for computer vision.

OpenCV - Open Source Computer Vision Library.
pytesseract - Another wrapper for Google Tesseract OCR.
SimpleCV - An open source framework for building computer vision applications.

Concurrency and Parallelism
Libraries for concurrent and parallel execution. See awesome-asyncio.

concurrent.futures - (Python standard library) A high-level interface for asynchronously executing callables.
multiprocessing - (Python standard library) Process-based parallelism.
eventlet - Asynchronous framework with WSGI support.
gevent - A coroutine-based Python networking library that uses greenlet.
uvloop - Ultra fast implementation of asyncio event loop on top of libuv.
scoop - Scalable Concurrent Operations in Python.

Configuration
Libraries for storing and parsing configuration options.

configobj - INI file parser with validation.
configparser - (Python standard library) INI file parser.
profig - Config from multiple formats with value conversion.
python-decouple - Strict separation of settings from code.

Cryptography

cryptography - A package designed to expose cryptographic primitives and recipes to Python developers.
paramiko - A Python (2.6+, 3.3+) implementation of the SSHv2 protocol, providing both client and server functionality.
passlib - Secure password storage/hashing library, very high level.
pynacl - Python binding to the Networking and Cryptography (NaCl) library.

Data Analysis
Libraries for data analyzing.

Blaze - NumPy and Pandas interface to Big Data.
Open Mining - Business Intelligence (BI) in Pandas interface.
Orange - Data mining, data visualization, analysis and machine learning through visual programming or scripts.
Pandas - A library providing high-performance, easy-to-use data structures and data analysis tools.
Optimus - Cleansing, pre-processing, feature engineering, exploratory data analysis and easy Machine Learning with a PySpark backend.

Data Validation
Libraries for validating data. Used for forms in many cases.

Cerberus - A lightweight and extensible data validation library.
colander - Validating and deserializing data obtained via XML, JSON, an HTML form post.
Dash - Built on top of Flask, React and Plotly aimed at analytical web applications.

awesome-dash


jsonschema - An implementation of JSON Schema for Python.
schema - A library for validating Python data structures.
Schematics - Data Structure Validation.
valideer - Lightweight extensible data validation and adaptation library.
voluptuous - A Python data validation library.

Data Visualization
Libraries for visualizing data. See: awesome-javascript.

Altair - Declarative statistical visualization library for Python.
Bokeh - Interactive Web Plotting for Python.
bqplot - Interactive Plotting Library for the Jupyter Notebook
ggplot - Same API as ggplot2 for R.
Matplotlib - A Python 2D plotting library.
Pygal - A Python SVG Charts Creator.
PyGraphviz - Python interface to Graphviz.
PyQtGraph - Interactive and realtime 2D/3D/Image plotting and science/engineering widgets.
Seaborn - Statistical data visualization using Matplotlib.
VisPy - High-performance scientific visualization based on OpenGL.

Database
Databases implemented in Python.

pickleDB - A simple and lightweight key-value store for Python.
tinydb - A tiny, document-oriented database.
ZODB - A native object database for Python. A key-value and object graph database.

Database Drivers
Libraries for connecting and operating databases.

MySQL - awesome-mysql

mysqlclient - MySQL connector with Python 3 support (mysql-python fork).
PyMySQL - A pure Python MySQL driver compatible to mysql-python.


PostgreSQL - awesome-postgres

psycopg2 - The most popular PostgreSQL adapter for Python.
queries - A wrapper of the psycopg2 library for interacting with PostgreSQL.


Other Relational Databases

pymssql - A simple database interface to Microsoft SQL Server.


NoSQL Databases

cassandra-driver - The Python Driver for Apache Cassandra.
happybase - A developer-friendly library for Apache HBase.
kafka-python - The Python client for Apache Kafka.
py2neo - Python wrapper client for Neo4j's restful interface.
pymongo - The official Python client for MongoDB.
redis-py - The Python client for Redis.


Asynchronous Clients

motor - The async Python driver for MongoDB.
Telephus - Twisted based client for Cassandra.
txpostgres - Twisted based asynchronous driver for PostgreSQL.
txRedis - Twisted based client for Redis.



Date and Time
Libraries for working with dates and times.

Chronyk - A Python 3 library for parsing human-written times and dates.
dateutil - Extensions to the standard Python datetime module.
delorean - A library for clearing up the inconvenient truths that arise dealing with datetimes.
moment - A Python library for dealing with dates/times. Inspired by Moment.js.
Pendulum - Python datetimes made easy.
PyTime - A easy-use Python module which aims to operate date/time/datetime by string.
pytz - World timezone definitions, modern and historical. Brings the tz database into Python.
when.py - Providing user-friendly functions to help perform common date and time actions.
maya - Datetimes for Humans, Maya is mostly built around the headaches and use-cases around parsing datetime data from websites.

Debugging Tools
Libraries for debugging code.

pdb-like Debugger

ipdb - IPython-enabled pdb.
pdb++ - Another drop-in replacement for pdb.
pudb - A full-screen, console-based Python debugger.
wdb - An improbable web debugger through WebSockets.


Tracing

lptrace - strace for Python programs.
manhole - Debug service that will accept unix domain socket connections and present the stacktraces for all threads and an interactive prompt.
pyringe - Debugger capable of attaching to and injecting code into Python processes.
python-hunter - A flexible code tracing toolkit.


Profiler

line_profiler - Line-by-line profiling.
memory_profiler - Monitor Memory usage of Python code.
profiling - An interactive Python profiler.
py-spy - A sampling profiler for Python programs. Written in Rust.
pyflame - A ptracing profiler For Python.
vprof - Visual Python profiler.


Others

icecream - Inspect variables, expressions, and program execution with a single, simple function call.
django-debug-toolbar - Display various debug information for Django.
django-devserver - A drop-in replacement for Django's runserver.
flask-debugtoolbar - A port of the django-debug-toolbar to flask.
pyelftools - Parsing and analyzing ELF files and DWARF debugging information.



Deep Learning
Frameworks for Neural Networks and Deep Learning. See: awesome-deep-learning.

caffe - A fast open framework for deep learning..
keras - A high-level neural networks library and capable of running on top of either TensorFlow or Theano.
mxnet - A deep learning framework designed for both efficiency and flexibility.
pytorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration.
SerpentAI - Game agent framework. Use any video game as a deep learning sandbox.
tensorflow - The most popular Deep Learning framework created by Google.
Theano - A library for fast numerical computation.

DevOps Tools
Software and libraries for DevOps.

ansible - A radically simple IT automation platform.
cloudinit - A multi-distribution package that handles early initialization of a cloud instance.
cuisine - Chef-like functionality for Fabric.
docker-compose - Fast, isolated development environments using Docker.
fabric - A simple, Pythonic tool for remote execution and deployment.
fabtools - Tools for writing awesome Fabric files.
honcho - A Python clone of Foreman, for managing Procfile-based applications.
OpenStack - Open source software for building private and public clouds.
pexpect - Controlling interactive programs in a pseudo-terminal like GNU expect.
psutil - A cross-platform process and system utilities module.
saltstack - Infrastructure automation and management system.
supervisor - Supervisor process control system for UNIX.

Distributed Computing
Frameworks and libraries for Distributed Computing.

Batch Processing

PySpark - Apache Spark Python API.
dask - A flexible parallel computing library for analytic computing.
luigi - A module that helps you build complex pipelines of batch jobs.
mrjob - Run MapReduce jobs on Hadoop or Amazon Web Services.
Ray - A system for parallel and distributed Python that unifies the machine learning ecosystem.


Stream Processing

faust - A stream processing library, porting the ideas from Kafka Streams to Python.
streamparse - Run Python code against real-time streams of data via Apache Storm.



Distribution
Libraries to create packaged executables for release distribution.

dh-virtualenv - Build and distribute a virtualenv as a Debian package.
Nuitka - Compile scripts, modules, packages to an executable or extension module.
py2app - Freezes Python scripts (Mac OS X).
py2exe - Freezes Python scripts (Windows).
PyInstaller - Converts Python programs into stand-alone executables (cross-platform).
pynsist - A tool to build Windows installers, installers bundle Python itself.

Documentation
Libraries for generating project documentation.

sphinx - Python Documentation generator.

awesome-sphinxdoc


pdoc - Epydoc replacement to auto generate API documentation for Python libraries.
pycco - The literate-programming-style documentation generator.

Downloader
Libraries for downloading.

s3cmd - A command line tool for managing Amazon S3 and CloudFront.
s4cmd - Super S3 command line tool, good for higher performance.
you-get - A YouTube/Youku/Niconico video downloader written in Python 3.
youtube-dl - A small command-line program to download videos from YouTube.

E-commerce
Frameworks and libraries for e-commerce and payments.

alipay - Unofficial Alipay API for Python.
Cartridge - A shopping cart app built using the Mezzanine.
django-oscar - An open-source e-commerce framework for Django.
django-shop - A Django based shop system.
merchant - A Django app to accept payments from various payment processors.
money - Money class with optional CLDR-backed locale-aware formatting and an extensible currency exchange solution.
python-currencies - Display money format and its filthy currencies.
forex-python - Foreign exchange rates, Bitcoin price index and currency conversion.
saleor - An e-commerce storefront for Django.
shoop - An open source E-Commerce platform based on Django.

Editor Plugins and IDEs

Emacs

elpy - Emacs Python Development Environment.


Sublime Text

anaconda - Anaconda turns your Sublime Text 3 in a full featured Python development IDE.
SublimeJEDI - A Sublime Text plugin to the awesome auto-complete library Jedi.


Vim

jedi-vim - Vim bindings for the Jedi auto-completion library for Python.
python-mode - An all in one plugin for turning Vim into a Python IDE.
YouCompleteMe - Includes Jedi-based completion engine for Python.


Visual Studio

PTVS - Python Tools for Visual Studio.


Visual Studio Code

Python - An extension with rich support for the Python language, with features including linting, IntelliSense, formatting, refactoring, debugging, unit tests, and jupyter support.


IDE

PyCharm - Commercial Python IDE by JetBrains. Has free community edition available.
spyder - Open Source Python IDE.



Email
Libraries for sending and parsing email.

envelopes - Mailing for human beings.
flanker - A email address and Mime parsing library.
imbox - Python IMAP for Humans.
inbox.py - Python SMTP Server for Humans.
lamson - Pythonic SMTP Application Server.
Marrow Mailer - High-performance extensible mail delivery framework.
modoboa - A mail hosting and management platform including a modern and simplified Web UI.
Nylas Sync Engine - Providing a RESTful API on top of a powerful email sync platform.
yagmail - Yet another Gmail/SMTP client.

Environment Management
Libraries for Python version and environment management.

pipenv - Sacred Marriage of Pipfile, Pip, & Virtualenv.
poetry - Python dependency management and packaging made easy.
pyenv - Simple Python version management.
venv - (Python standard library in Python 3.3+) Creating lightweight virtual environments.
virtualenv - A tool to create isolated Python environments.

Files
Libraries for file manipulation and MIME type detection.

mimetypes - (Python standard library) Map filenames to MIME types.
path.py - A module wrapper for os.path.
pathlib - (Python standard library) An cross-platform, object-oriented path library.
PyFilesystem2 - Python's filesystem abstraction layer.
python-magic - A Python interface to the libmagic file type identification library.
Unipath - An object-oriented approach to file/directory operations.
watchdog - API and shell utilities to monitor file system events.

Foreign Function Interface
Libraries for providing foreign function interface.

cffi - Foreign Function Interface for Python calling C code.
ctypes - (Python standard library) Foreign Function Interface for Python calling C code.
PyCUDA - A Python wrapper for Nvidia's CUDA API.
SWIG - Simplified Wrapper and Interface Generator.

Forms
Libraries for working with forms.

Deform - Python HTML form generation library influenced by the formish form generation library.
django-bootstrap3 - Bootstrap 3 integration with Django.
django-bootstrap4 - Bootstrap 4 integration with Django.
django-crispy-forms - A Django app which lets you create beautiful forms in a very elegant and DRY way.
django-remote-forms - A platform independent Django form serializer.
WTForms - A flexible forms validation and rendering library.

Functional Programming
Functional Programming with Python.

Coconut - Coconut is a variant of Python built for simple, elegant, Pythonic functional programming.
CyToolz - Cython implementation of Toolz: High performance functional utilities.
fn.py - Functional programming in Python: implementation of missing features to enjoy FP.
funcy - A fancy and practical functional tools.
Toolz - A collection of functional utilities for iterators, functions, and dictionaries.

GUI
Libraries for working with graphical user interface applications.

curses - Built-in wrapper for ncurses used to create terminal GUI applications.
Eel - Little library for making simple Electron-like offline HTML/JS GUI apps, with full access to Python capabilities and libraries.
enaml - Creating beautiful user-interfaces with Declaratic Syntax like QML.
Flexx - Flexx is a pure Python toolkit for creating GUI's, that uses web technology for its rendering.
Gooey - Turn command line programs into a full GUI application with one line.
kivy - A library for creating NUI applications, running on Windows, Linux, Mac OS X, Android and iOS.
pyglet - A cross-platform windowing and multimedia library for Python.
PyGObject - Python Bindings for GLib/GObject/GIO/GTK+ (GTK+3).
PyQt - Python bindings for the Qt cross-platform application and UI framework, with support for both Qt v4 and Qt v5 frameworks.
PySide - Python bindings for the Qt cross-platform application and UI framework, supporting the Qt v4 framework.
PySimpleGUI - Wrapper for tkinter, Qt, WxPython and Remi that creates a unified, easy to understand & more Python-like interface for beginner and intermediate level custom GUIs.
pywebview - A lightweight cross-platform native wrapper around a webview component that allows to display HTML content in its own native dedicated window.
Tkinter - Tkinter is Python's de-facto standard GUI package.
Toga - A Python native, OS native GUI toolkit.
urwid - A library for creating terminal GUI applications with strong support for widgets, events, rich colors, etc.
wxPython - A blending of the wxWidgets C++ class library with the Python.

Game Development
Awesome game development libraries.

Cocos2d - cocos2d is a framework for building 2D games, demos, and other graphical/interactive applications. It is based on pyglet.
Harfang3D - Python framework for 3D, VR and game development. Manage and display complex 3D scenes, with physics, video, sound and music, access VR devices. All written in C++.
Panda3D - 3D game engine developed by Disney and maintained by Carnegie Mellon's Entertainment Technology Center. Written in C++, completely wrapped in Python.
Pygame - Pygame is a set of Python modules designed for writing games.
PyOgre - Python bindings for the Ogre 3D render engine, can be used for games, simulations, anything 3D.
PyOpenGL - Python ctypes bindings for OpenGL and it's related APIs.
PySDL2 - A ctypes based wrapper for the SDL2 library.
RenPy - A Visual Novel engine.

Geolocation
Libraries for geocoding addresses and working with latitudes and longitudes.

django-countries - A Django app that provides country choices for use with forms, flag icons static files, and a country field for models.
GeoDjango - A world-class geographic web framework.
GeoIP - Python API for MaxMind GeoIP Legacy Database.
geojson - Python bindings and utilities for GeoJSON.
geopy - Python Geocoding Toolbox.
pygeoip - Pure Python GeoIP API.

HTML Manipulation
Libraries for working with HTML and XML.

BeautifulSoup - Providing Pythonic idioms for iterating, searching, and modifying HTML or XML.
bleach - A whitelist-based HTML sanitization and text linkification library.
cssutils - A CSS library for Python.
html5lib - A standards-compliant library for parsing and serializing HTML documents and fragments.
lxml - A very fast, easy-to-use and versatile library for handling HTML and XML.
MarkupSafe - Implements a XML/HTML/XHTML Markup safe string for Python.
pyquery - A jQuery-like library for parsing HTML.
untangle - Converts XML documents to Python objects for easy access.
WeasyPrint - A visual rendering engine for HTML and CSS that can export to PDF.
xmldataset - Simple XML Parsing.
xmltodict - Working with XML feel like you are working with JSON.

HTTP
Libraries for working with HTTP.

grequests - requests + gevent for asynchronous HTTP requests.
httplib2 - Comprehensive HTTP client library.
requests - HTTP Requests for Humans™.
treq - Python requests like API built on top of Twisted's HTTP client.
urllib3 - A HTTP library with thread-safe connection pooling, file post support, sanity friendly.

Hardware
Libraries for programming with hardware.

ino - Command line toolkit for working with Arduino.
keyboard - Hook and simulate global keyboard events on Windows and Linux.
mouse - Hook and simulate global mouse events on Windows and Linux.
Pingo - Pingo provides a uniform API to program devices like the Raspberry Pi, pcDuino, Intel Galileo, etc.
PyUserInput - A module for cross-platform control of the mouse and keyboard.
scapy - A brilliant packet manipulation library.
wifi - A Python library and command line tool for working with WiFi on Linux.

Image Processing
Libraries for manipulating images.

hmap - Image histogram remapping.
imgSeek - A project for searching a collection of images using visual similarity.
nude.py - Nudity detection.
pagan - Retro identicon (Avatar) generation based on input string and hash.
pillow - Pillow is the friendly PIL fork.
pyBarcode - Create barcodes in Python without needing PIL.
pygram - Instagram-like image filters.
python-qrcode - A pure Python QR Code generator.
Quads - Computer art based on quadtrees.
scikit-image - A Python library for (scientific) image processing.
thumbor - A smart imaging service. It enables on-demand crop, re-sizing and flipping of images.
wand - Python bindings for MagickWand, C API for ImageMagick.

Implementations
Implementations of Python.

CLPython - Implementation of the Python programming language written in Common Lisp.
CPython - Default, most widely used implementation of the Python programming language written in C.
Cython - Optimizing Static Compiler for Python. Uses type mixins to compile Python into C or C++ modules resulting in large performance gains
Grumpy - More compiler than interpreter as more powerful CPython2.7 replacement (alpha).
IronPython - Implementation of the Python programming language written in C# targeting the .NET Framework and Mono.
Jython - Implementation of Python programming language written in Java for the Java virtual machine (JVM).
MicroPython - MicroPython - a lean and efficient Python programming language implementation for microcontrollers and constrained systems
Numba - Python JIT compiler to LLVM aimed at scientific Python.
PeachPy - x86-64 assembler embedded in Python. Can be used as inline assembler for Python or as a stand-alone assembler for Windows, Linux, OS X, Native Client and Go.
Pyjion - A JIT for Python based upon CoreCLR.
PyPy - Implementation of the Python programming language written in RPython and translated into C. PyPy focuses on speed, efficiency and compatibility with the original CPython interpreter. The interpreter uses black magic to make Python very fast without having to add in additional type information.
PySec - Hardened version of python that makes it easier for security professionals and developers to write applications more resilient to attacks and manipulations.
Pyston - A Python implementation built using LLVM and modern JIT techniques with the goal of achieving good performance.
Stackless Python - An enhanced version of the Python programming language which allows programmers to reap the benefits of thread-based programming without the performance and complexity problems associated with conventional threads.

Interactive Interpreter
Interactive Python interpreters (REPL).

bpython - A fancy interface to the Python interpreter.
Jupyter Notebook (IPython) - A rich toolkit to help you make the most out of using Python interactively.

awesome-jupyter


ptpython - Advanced Python REPL built on top of the python-prompt-toolkit.

Internationalization
Libraries for working with i18n.

Babel - An internationalization library for Python.
PyICU - A wrapper of International Components for Unicode C++ library (ICU).

Job Scheduler
Libraries for scheduling jobs.

APScheduler - A light but powerful in-process task scheduler that lets you schedule functions.
django-schedule - A calendaring app for Django.
doit - A task runner and build tool.
gunnery - Multipurpose task execution tool for distributed systems with web-based interface.
Joblib - A set of tools to provide lightweight pipelining in Python.
Plan - Writing crontab file in Python like a charm.
schedule - Python job scheduling for humans.
Spiff - A powerful workflow engine implemented in pure Python.
TaskFlow - A Python library that helps to make task execution easy, consistent and reliable.
Airflow - Airflow is a platform to programmatically author, schedule and monitor workflows.

Logging
Libraries for generating and working with logs.

Eliot - Logging for complex & distributed systems.
logbook - Logging replacement for Python.
logging - (Python standard library) Logging facility for Python.
raven - Python client for Sentry, a log/error tracking, crash reporting and aggregation platform for web applications.

Machine Learning
Libraries for Machine Learning. See: awesome-machine-learning.

H2O - Open Source Fast Scalable Machine Learning Platform.
Metrics - Machine learning evaluation metrics.
NuPIC - Numenta Platform for Intelligent Computing.
scikit-learn - The most popular Python library for Machine Learning.
Spark ML - Apache Spark's scalable Machine Learning library.
vowpal_porpoise - A lightweight Python wrapper for Vowpal Wabbit.
xgboost - A scalable, portable, and distributed gradient boosting library.

Microsoft Windows
Python programming on Microsoft Windows.

Python(x,y) - Scientific-applications-oriented Python Distribution based on Qt and Spyder.
pythonlibs - Unofficial Windows binaries for Python extension packages.
PythonNet - Python Integration with the .NET Common Language Runtime (CLR).
PyWin32 - Python Extensions for Windows.
WinPython - Portable development environment for Windows 7/8.

Miscellaneous
Useful libraries or tools that don't fit in the categories above.

blinker - A fast Python in-process signal/event dispatching system.
boltons - A set of pure-Python utilities.
itsdangerous - Various helpers to pass trusted data to untrusted environments.
pluginbase - A simple but flexible plugin system for Python.
tryton - A general purpose business framework.

Natural Language Processing
Libraries for working with human languages.

General

gensim - Topic Modelling for Humans.
langid.py - Stand-alone language identification system.
nltk - A leading platform for building Python programs to work with human language data.
pattern - A web mining module for the Python.
polyglot - Natural language pipeline supporting hundreds of languages.
pytext - A natural language modeling framework based on PyTorch.
PyTorch-NLP - A toolkit enabling rapid deep learning NLP prototyping for research.
spacy - A library for industrial-strength natural language processing in Python and Cython.
stanfordnlp - The Stanford NLP Group's official Python library, supporting 50+ languages.


Chinese

jieba - The most popular Chinese text segmentation library.
pkuseg-python - A toolkit for Chinese word segmentation in various domains.
snownlp - A library for processing Chinese text.
funNLP - A collection of tools and datasets for Chinese NLP.



Network Virtualization
Tools and libraries for Virtual Networking and SDN (Software Defined Networking).

mininet - A popular network emulator and API written in Python.
pox - A Python-based SDN control applications, such as OpenFlow SDN controllers.

Networking
Libraries for networking programming.

asyncio - (Python standard library) Asynchronous I/O, event loop, coroutines and tasks.

awesome-asyncio


pulsar - Event-driven concurrent framework for Python.
pyzmq - A Python wrapper for the ZeroMQ message library.
Twisted - An event-driven networking engine.
napalm - Cross-vendor API to manipulate network devices.

News Feed
Libraries for building user's activities.

django-activity-stream - Generating generic activity streams from the actions on your site.
Stream Framework - Building newsfeed and notification systems using Cassandra and Redis.

ORM
Libraries that implement Object-Relational Mapping or data mapping techniques.

Relational Databases

Django Models - A part of Django.
SQLAlchemy - The Python SQL Toolkit and Object Relational Mapper.

awesome-sqlalchemy


dataset - Store Python dicts in a database - works with SQLite, MySQL, and PostgreSQL.
orator -  The Orator ORM provides a simple yet beautiful ActiveRecord implementation.
peewee - A small, expressive ORM.
pony - ORM that provides a generator-oriented interface to SQL.
pydal - A pure Python Database Abstraction Layer.


NoSQL Databases

hot-redis - Rich Python data types for Redis.
mongoengine - A Python Object-Document-Mapper for working with MongoDB.
PynamoDB - A Pythonic interface for Amazon DynamoDB.
redisco - A Python Library for Simple Models and Containers Persisted in Redis.



Package Management
Libraries for package and dependency management.

pip - The Python package and dependency manager.

PyPI
pip-tools - A set of tools to keep your pinned Python dependencies fresh.


conda - Cross-platform, Python-agnostic binary package manager.

Package Repositories
Local PyPI repository server and proxies.

warehouse - Next generation Python Package Repository (PyPI).
bandersnatch - PyPI mirroring tool provided by Python Packaging Authority (PyPA).
devpi - PyPI server and packaging/testing/release tool.
localshop - Local PyPI server (custom packages and auto-mirroring of pypi).

Permissions
Libraries that allow or deny users access to data or functionality.

django-guardian - Implementation of per object permissions for Django 1.2+
django-rules - A tiny but powerful app providing object-level permissions to Django, without requiring a database.

Processes
Libraries for starting and communicating with OS processes.

delegator.py - Subprocesses for Humans™ 2.0.
sarge - Yet another wrapper for subprocess.
sh - A full-fledged subprocess replacement for Python.

Queue
Libraries for working with event and task queues.

celery - An asynchronous task queue/job queue based on distributed message passing.
huey - Little multi-threaded task queue.
mrq - Mr. Queue - A distributed worker task queue in Python using Redis & gevent.
rq - Simple job queues for Python.

Recommender Systems
Libraries for building recommender systems.

annoy - Approximate Nearest Neighbors in C++/Python optimized for memory usage.
fastFM - A library for Factorization Machines.
implicit - A fast Python implementation of collaborative filtering for implicit datasets.
libffm - A library for Field-aware Factorization Machine (FFM).
lightfm - A Python implementation of a number of popular recommendation algorithms.
spotlight - Deep recommender models using PyTorch.
Surprise - A scikit for building and analyzing recommender systems.
tensorrec - A Recommendation Engine Framework in TensorFlow.

RESTful API
Libraries for developing RESTful APIs.

Django

django-rest-framework - A powerful and flexible toolkit to build web APIs.
django-tastypie - Creating delicious APIs for Django apps.


Flask

eve - REST API framework powered by Flask, MongoDB and good intentions.
flask-api-utils - Taking care of API representation and authentication for Flask.
flask-api - Browsable Web APIs for Flask.
flask-restful - Quickly building REST APIs for Flask.
flask-restless - Generating RESTful APIs for database models defined with SQLAlchemy.


Pyramid

cornice - A RESTful framework for Pyramid.


Framework agnostic

apistar - A smart Web API framework, designed for Python 3.
falcon - A high-performance framework for building cloud APIs and web app backends.
hug - A Python3 framework for cleanly exposing APIs over HTTP and the Command Line with automatic documentation and validation.
restless - Framework agnostic REST framework based on lessons learned from Tastypie.
ripozo - Quickly creating REST/HATEOAS/Hypermedia APIs.
sandman - Automated REST APIs for existing database-driven systems.



Robotics
Libraries for robotics.

PythonRobotics - This is a compilation of various robotics algorithms with visualizations.
rospy - This is a library for ROS (Robot Operating System).

RPC Servers
RPC-compatible servers.

SimpleJSONRPCServer - This library is an implementation of the JSON-RPC specification.
SimpleXMLRPCServer - (Python standard library) Simple XML-RPC server implementation, single-threaded.
zeroRPC - zerorpc is a flexible RPC implementation based on ZeroMQ and MessagePack.

Science
Libraries for scientific computing.

astropy - A community Python library for Astronomy.
bcbio-nextgen - Providing best-practice pipelines for fully automated high throughput sequencing analysis.
bccb - Collection of useful code related to biological analysis.
Biopython - Biopython is a set of freely available tools for biological computation.
cclib - A library for parsing and interpreting the results of computational chemistry packages.
Colour - A colour science package implementing a comprehensive number of colour theory transformations and algorithms.
NetworkX - A high-productivity software for complex networks.
NIPY - A collection of neuroimaging toolkits.
NumPy - A fundamental package for scientific computing with Python.
Open Babel - A chemical toolbox designed to speak the many languages of chemical data.
ObsPy - A Python toolbox for seismology.
PyDy - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion.
PyMC - Markov Chain Monte Carlo sampling toolkit.
QuTiP - Quantum Toolbox in Python.
RDKit - Cheminformatics and Machine Learning Software.
SciPy - A Python-based ecosystem of open-source software for mathematics, science, and engineering.
statsmodels - Statistical modeling and econometrics in Python.
SymPy - A Python library for symbolic mathematics.
Zipline - A Pythonic algorithmic trading library.
SimPy -  A process-based discrete-event simulation framework.

Search
Libraries and software for indexing and performing search queries on data.

elasticsearch-py - The official low-level Python client for Elasticsearch.
elasticsearch-dsl-py - The official high-level Python client for Elasticsearch.
django-haystack - Modular search for Django.
pysolr - A lightweight Python wrapper for Apache Solr.
whoosh - A fast, pure Python search engine library.

Serialization
Libraries for serializing complex data types

marshmallow - A lightweight library for converting complex objects to and from simple Python datatypes.
pysimdjson - A Python bindings for simdjson.
python-rapidjson - A Python wrapper around RapidJSON.

Serverless Frameworks
Frameworks for developing serverless Python code.

python-lambda - A toolkit for developing and deploying Python code in AWS Lambda.
Zappa - A tool for deploying WSGI applications on AWS Lambda and API Gateway.

Specific Formats Processing
Libraries for parsing and manipulating specific text formats.

General

tablib - A module for Tabular Datasets in XLS, CSV, JSON, YAML.


Office

openpyxl - A library for reading and writing Excel 2010 xlsx/xlsm/xltx/xltm files.
pyexcel - Providing one API for reading, manipulating and writing csv, ods, xls, xlsx and xlsm files.
python-docx - Reads, queries and modifies Microsoft Word 2007/2008 docx files.
python-pptx - Python library for creating and updating PowerPoint (.pptx) files.
unoconv - Convert between any document format supported by LibreOffice/OpenOffice.
XlsxWriter - A Python module for creating Excel .xlsx files.
xlwings - A BSD-licensed library that makes it easy to call Python from Excel and vice versa.
xlwt / xlrd - Writing and reading data and formatting information from Excel files.


PDF

PDFMiner - A tool for extracting information from PDF documents.
PyPDF2 - A library capable of splitting, merging and transforming PDF pages.
ReportLab - Allowing Rapid creation of rich PDF documents.


Markdown

Mistune - Fastest and full featured pure Python parsers of Markdown.
Python-Markdown - A Python implementation of John Gruber’s Markdown.


YAML

PyYAML - YAML implementations for Python.


CSV

csvkit - Utilities for converting to and working with CSV.


Archive

unp - A command line tool that can unpack archives easily.



Static Site Generator
Static site generator is a software that takes some text + templates as input and produces HTML files on the output.

mkdocs - Markdown friendly documentation generator.
pelican - Static site generator that supports Markdown and reST syntax.
lektor - An easy to use static CMS and blog engine.
nikola - A static website and blog generator.

Tagging
Libraries for tagging items.

django-taggit - Simple tagging for Django.

Template Engine
Libraries and tools for templating and lexing.

Jinja2 - A modern and designer friendly templating language.
Genshi - Python templating toolkit for generation of web-aware output.
Mako - Hyperfast and lightweight templating for the Python platform.

Testing
Libraries for testing codebases and generating test data.

Testing Frameworks

pytest - A mature full-featured Python testing tool.
hypothesis - Hypothesis is an advanced Quickcheck style property based testing library.
nose2 - The successor to nose, based on `unittest2.
Robot Framework - A generic test automation framework.
unittest - (Python standard library) Unit testing framework.


Test Runners

green - A clean, colorful test runner.
mamba - The definitive testing tool for Python. Born under the banner of BDD.
tox - Auto builds and tests distributions in multiple Python versions


GUI / Web Testing

locust - Scalable user load testing tool written in Python.
PyAutoGUI - PyAutoGUI is a cross-platform GUI automation Python module for human beings.
Selenium - Python bindings for Selenium WebDriver.
sixpack - A language-agnostic A/B Testing framework.
splinter - Open source tool for testing web applications.


Mock

doublex - Powerful test doubles framework for Python.
freezegun - Travel through time by mocking the datetime module.
httmock - A mocking library for requests for Python 2.6+ and 3.2+.
httpretty - HTTP request mock tool for Python.
mock - (Python standard library) A mocking and patching library.
Mocket - Socket Mock Framework plus HTTP[S]/asyncio/gevent mocking library with recording/replaying capability.
responses - A utility library for mocking out the requests Python library.
VCR.py - Record and replay HTTP interactions on your tests.


Object Factories

factory_boy - A test fixtures replacement for Python.
mixer - Another fixtures replacement. Supported Django, Flask, SQLAlchemy, Peewee and etc.
model_mommy - Creating random fixtures for testing in Django.


Code Coverage

coverage - Code coverage measurement.


Fake Data

mimesis - is a Python library that help you generate fake data.
fake2db - Fake database generator.
faker - A Python package that generates fake data.
radar - Generate random datetime / time.


Error Handler

FuckIt.py - FuckIt.py uses state-of-the-art technology to make sure your Python code runs whether it has any right to or not.



Text Processing
Libraries for parsing and manipulating plain texts.

General

chardet - Python 2/3 compatible character encoding detector.
difflib - (Python standard library) Helpers for computing deltas.
ftfy - Makes Unicode text less broken and more consistent automagically.
fuzzywuzzy - Fuzzy String Matching.
Levenshtein - Fast computation of Levenshtein distance and string similarity.
pangu.py - Paranoid text spacing.
pyfiglet - An implementation of figlet written in Python.
pypinyin - Convert Chinese hanzi (漢字) to pinyin (拼音).
textdistance - Compute distance between sequences. 30+ algorithms, pure python implementation, common interface, optional external libs usage.
unidecode - ASCII transliterations of Unicode text.


Slugify

awesome-slugify - A Python slugify library that can preserve unicode.
python-slugify - A Python slugify library that translates unicode to ASCII.
unicode-slugify - A slugifier that generates unicode slugs with Django as a dependency.


Unique identifiers

hashids - Implementation of hashids in Python.
shortuuid - A generator library for concise, unambiguous and URL-safe UUIDs.


Parser

ply - Implementation of lex and yacc parsing tools for Python.
pygments - A generic syntax highlighter.
pyparsing - A general purpose framework for generating parsers.
python-nameparser - Parsing human names into their individual components.
python-phonenumbers - Parsing, formatting, storing and validating international phone numbers.
python-user-agents - Browser user agent parser.
sqlparse - A non-validating SQL parser.



Third-party APIs
Libraries for accessing third party services APIs. See: List of Python API Wrappers and Libraries.

apache-libcloud - One Python library for all clouds.
boto3 - Python interface to Amazon Web Services.
django-wordpress - WordPress models and views for Django.
facebook-sdk - Facebook Platform Python SDK.
google-api-python-client - Google APIs Client Library for Python.
gspread - Google Spreadsheets Python API.
twython - A Python wrapper for the Twitter API.

URL Manipulation
Libraries for parsing URLs.

furl - A small Python library that makes parsing and manipulating URLs easy.
purl - A simple, immutable URL class with a clean API for interrogation and manipulation.
pyshorteners - A pure Python URL shortening lib.
webargs - A friendly library for parsing HTTP request arguments, with built-in support for popular web frameworks, including Flask, Django, Bottle, Tornado, and Pyramid.

Video
Libraries for manipulating video and GIFs.

moviepy - A module for script-based movie editing with many formats, including animated GIFs.
scikit-video - Video processing routines for SciPy.

WSGI Servers
WSGI-compatible web servers.

bjoern - Asynchronous, very fast and written in C.
gunicorn - Pre-forked, partly written in C.
uWSGI - A project aims at developing a full stack for building hosting services, written in C.
waitress - Multi-threaded, powers Pyramid.
werkzeug - A WSGI utility library for Python that powers Flask and can easily be embedded into your own projects.

Web Asset Management
Tools for managing, compressing and minifying website assets.

django-compressor - Compresses linked and inline JavaScript or CSS into a single cached file.
django-pipeline - An asset packaging library for Django.
django-storages - A collection of custom storage back ends for Django.
fanstatic - Packages, optimizes, and serves static file dependencies as Python packages.
fileconveyor - A daemon to detect and sync files to CDNs, S3 and FTP.
flask-assets - Helps you integrate webassets into your Flask app.
webassets - Bundles, optimizes, and manages unique cache-busting URLs for static resources.

Web Content Extracting
Libraries for extracting web contents.

html2text - Convert HTML to Markdown-formatted text.
lassie - Web Content Retrieval for Humans.
micawber - A small library for extracting rich content from URLs.
newspaper - News extraction, article extraction and content curation in Python.
python-readability - Fast Python port of arc90's readability tool.
requests-html - Pythonic HTML Parsing for Humans.
sumy - A module for automatic summarization of text documents and HTML pages.
textract - Extract text from any document, Word, PowerPoint, PDFs, etc.
toapi - Every web site provides APIs.

Web Crawling & Web Scraping
Libraries to automate data extraction from websites.

cola - A distributed crawling framework.
feedparser - Universal feed parser.
grab - Site scraping framework.
MechanicalSoup - A Python library for automating interaction with websites.
portia - Visual scraping for Scrapy.
pyspider - A powerful spider system.
robobrowser - A simple, Pythonic library for browsing the web without a standalone web browser.
scrapy - A fast high-level screen scraping and web crawling framework.

Web Frameworks
Full stack web frameworks.

Django - The most popular web framework in Python.

awesome-django


Flask - A microframework for Python.

awesome-flask


Pyramid - A small, fast, down-to-earth, open source Python web framework.

awesome-pyramid


Sanic - Web server that's written to go fast.
Vibora - Fast, efficient and asynchronous Web framework inspired by Flask.
Tornado - A Web framework and asynchronous networking library.

WebSocket
Libraries for working with WebSocket.

autobahn-python - WebSocket & WAMP for Python on Twisted and asyncio.
crossbar - Open-source Unified Application Router (Websocket & WAMP for Python on Autobahn).
django-channels - Developer-friendly asynchrony for Django.
django-socketio - WebSockets for Django.
WebSocket-for-Python - WebSocket client and server library for Python 2 and 3 as well as PyPy.

Services
Online tools and APIs to simplify development.
Continuous Integration
See: awesome-CIandCD.

CircleCI - A CI service that can run very fast parallel testing. (GitHub only)
Travis CI - A popular CI service for your open source and private projects. (GitHub only)
Vexor CI - A continuous integration tool for private apps with pay-per-minute billing model.
Wercker - A Docker-based platform for building and deploying applications and microservices.

Code Quality

Codacy - Automated Code Review to ship better code, faster.
Codecov - Code coverage dashboard.
CodeFactor - Automated Code Review for Git.
Landscape - Hosted continuous Python code metrics.

Resources
Where to discover new Python libraries.
Podcasts

From Python Import Podcast
Podcast.init
Python Bytes
Python Testing
Radio Free Python
Talk Python To Me

Twitter

@codetengu
@getpy
@importpython
@planetpython
@pycoders
@pypi
@pythontrending
@PythonWeekly
@TalkPython
@realpython

Websites

/r/CoolGithubProjects
/r/Python
Awesome Python @LibHunt
Django Packages
Full Stack Python
PyPI Ranking
Python 3 Wall of Superpowers
Python Hackers
Python ZEEF
Python 开发社区
Real Python
Trending Python repositories on GitHub today

Weekly

CodeTengu Weekly 碼天狗週刊
Import Python Newsletter
Pycoder's Weekly
Python Weekly
Python Tricks

Contributing
Your contributions are always welcome! Please take a look at the contribution guidelines first.
I will keep some pull requests open if I'm not sure whether those libraries are awesome, you could vote for them by adding 👍 to them. Pull requests will be merged when their votes reach 20.

If you have any question about this opinionated list, do not hesitate to contact me @vinta on Twitter or open an issue on GitHub.
",67185
X-CASH-official/XCASH_proof_of_stake_consensus_node,C,"X-CASH Proof of stake - consensus node
More details will be released on the consensus node soon!
Installation
This program will only run on a Linux/Unix OS at this time. We recommend installing this on a Ubuntu VPS/Server (16.04 or 18.04) for the best compatibility.
You will also need to run an X-CASH Daemon and X-CASH RPC wallet on the server. You can either download the latest X-CASH release or build from source
Compiling X-CASH Proof of stake - consensus node from source
Dependencies
The following table summarizes the tools and libraries required to build.



Dependencies
Min. version
Ubuntu package




GCC
4.7.3
build-essential


CMake
3.0.0
cmake


pkg-config
any
pkg-config


OpenSSL
any
libssl-dev


Git
any
git


MongoDB
4.0.3
install from binaries


MongoDB C Driver (includes BSON libary)
1.13.1
build from source



Installing MongoDB from binaries
Visit https://www.mongodb.com/download-center/community
Then choose your OS, and make sure the version is the current version and the package is server. Then click on All version binaries. Now find the current version to download. You do not want the debug symbols or the rc version, just the regular current version.
Once you have downloaded the file move the file to a location where you want to keep the binaries, then run this set of commands
tar -xf mongodb-linux-x86_64-*.tgz && rm mongodb-linux-x86_64-*.tgz && sudo mkdir -p /data/db && sudo chmod 770 /data/db && sudo chown $USER /data/db
Building the MongoDB C driver from source
Visit the offical websites installation instructions at http://mongoc.org/libmongoc/current/installing.html
You will need to follow the instructions for Building from a release tarball or Building from git since you need the header files, not just the library files.
After you have built the MongoDB C driver from source, you will need to run
sudo ldconfig
Adding MongoDB to your PATH
You will probably want to add MongoDB to your path so you can run MonogDB by typing mongod at any terminal.
To add MongoDB to your PATH (replace ""MongoDB_folder"" with the location of the bin folder in the folder you installed MongoDB in
echo -e '\nexport PATH=MongoDB_folder:$PATH' >> ~/.profile && source ~/.profile
Cloning the repository
$ git clone https://github.com/X-CASH-official/XCASH_proof_of_stake_consensus_node.git
Build instructions
X-CASH Proof of stake - consensus node uses a Make file.
After cloning the repository, navigate to the folder
cd XCASH_proof_of_stake_consensus_node
Then use the make file to build the binary file
make clean ; make
Running MongoDB
To run MongoDB you will need to navigate to the folder you downloaded the binaries to, and in the bin folder run mongod by running
./mongod
If you have already added MongoDB to your path, you can just type in any terminal
mongod
Setting up the xcashd and xcash-wallet-RPC
First you will need to run xcashd in the background. Navigate to the folder that contains the xcash binaries, then run
./xcashd
Next you need to run a xcash-wallet-rpc. Depending on if this is the consensus node or the consensus backup node, you will need to the run the wallet that contains the public address in the Proof of stake for the CONSENSUS_NODE_PUBLIC_ADDRESS or CONSENSUS_BACKUP_NODE_PUBLIC_ADDRESS
To run the rpc wallet you can run
./xcash-wallet-rpc --wallet-file NAME_OF_WALLET_FILE --password WALLET_FILE_PASSWORD --rpc-bind-port 18285 --confirm-external-bind --daemon-port 18281 --disable-rpc-login --trusted-daemon
Just replace NAME_OF_WALLET_FILE with the name of your wallet file and WALLET_FILE_PASSWORD with the password of that wallet file. Make sure to use port 18285 as this is the port that is used in the program.
We suggest you use the screen command to run the program in the background, this way you can still maintenance the server. To do this run
screen -dmS Daemon ./xcashd
You can also run the RPC wallet this way as well
screen -dmS RPC-Wallet ./xcash-wallet-rpc --wallet-file NAME_OF_WALLET_FILE --password WALLET_FILE_PASSWORD --rpc-bind-port 18285 --confirm-external-bind --daemon-port 18281 --disable-rpc-login --trusted-daemon
To bring the screen from the bacground process to the active process run
screen -x NAME_OF_BACKGROUNDS_SCREEN
Where NAME_OF_BACKGROUNDS_SCREEN would be Daemon or RPC-Wallet in the above examples.
To exit a screen if it is the active process, you can press Control + C. To exit a screen that is a background process you can run
screen -XS NAME_OF_BACKGROUNDS_SCREEN quit
Running X-CASH Proof of stake - consensus node test
It is recomeneded to run the X-CASH Proof of stake test before you run the main program. The test will ensure that your system is compatbile, and that you have setup your system correctly.
To run the X-CASH Proof of stake test, Navigate to the folder that contains the binary, then run
./xcash_proof_of_stake_consensus_node --test
The test will return the number of passed and failed test on the bottom of the console. The failed test need to be 0 before you run the node. If the output is not showing 0 for failed test, then you need to scroll through the testing output and find what test failed (It will be red instead of green). If this is a system compatibility test, then you will need to fix the system. If this is a core test that has failed, then you need to possibly rebuild, or contact us with your OS version, and we can look into it.
Running X-CASH Proof of stake - consensus node
Then you will need to run the xcash_proof_of_stake_consensus_node. Navigate to the folder that contains the binary, then run
./xcash_proof_of_stake_consensus_node
We suggest you use the screen command to run the program in the background, this way you can still maintenance the server. To do this run
screen -dmS xcash_proof_of_stake_consensus_node ./xcash_proof_of_stake_consensus_node
To bring the screen from the bacground process to the active process run
screen -x NAME_OF_BACKGROUNDS_SCREEN
Where NAME_OF_BACKGROUNDS_SCREEN would be xcash_proof_of_stake_consensus_node in the above example.
To exit a screen if it is the active process, you can press Control + C. To exit a screen that is a background process you can run
screen -XS NAME_OF_BACKGROUNDS_SCREEN quit
",2
AngelKitty/review_the_national_post-graduate_entrance_examination,C++,"复习考研的那些事儿～～
这里我将记录我考研的全过程，包括看过的书，写过的笔记，读过的杂志，推荐的番剧，电影，以及我在生活中一些零碎的记录和思考。
也许这一切对你们可能一无是处，但对我而言，这将会是人生中最宝贵的一段回忆，我希望以这种方式记录下来，所以在 Github 上开了此项目。
网上很多up主都喜欢通过拍摄 vlog 这种形式来记录自己的日常，我就突发奇想，能不能在这个词基础上稍微修改一下。于是我就想到了一个非常 nice 的词，自己取一个名叫 plog (Page weblog，plog似乎指代的意思很多，但是大多都是和日志系统有关吧，所以我这个翻译应该不算很偏门吧23333)
当然也欢迎你们加入到我们的复习队伍中，有好看的电影、番剧推荐或者一些有意思的书籍，请 fork 本项目到您的仓库后，再进行 pull request。
本项目分为如下三个部分：

books_and_notes：存放着我复习时候看过的书以及笔记
exam：一些我复习的时候做过的一些题目
plog：记录着我每周的一些日常。

如何获取此项目？
本项目可以直接通过以下方式获取：
# clone
git clone git@github.com:AngelKitty/review_the_national_post-graduate_entrance_examination.git

版权声明

本作品采用知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议进行许可。
The Star And Thank Author License
",25
owlboy/greatpug-public,None,"
The Great Pug
A Bar in the Metaverse (VRChat)
thegreatpug.com
This Repository
This repository is the public sister repository to the private repository for The Great Pug.
Bug Reports and Feature Requests
Feel free to use the issues feature of this repository to report bugs or make feature requests relating to The Great Pug.
Support On-going Development
You can support on-going maintenance, events, and expansion to The Great Pug by joining my Patreon.
You can also donate crypto currency at the following addresses:

Etherium (ETH): 0xa2e7aBB300728afc7564874B12975D2f311687a6
Bitcoin (BTC): 16tWrVpZWJcw64aE5su8bRQJgyAVhBeNZQ
Litecoin (LTC): MCjYc1wm2r1f8uM5FPTyqZbKxnxwvNf7UQ

 
Change Log
5/11/19 (65mb)

Reduced draw called depending on your POV
Simplified some colliders
Rebaked lighting
Rebaked Occlusion
Fixed UV unwraps on some models
Adjusted some audio clip sizes

5/10/19 (71mb)

Reduced Draw Calls by a few depending on your POV
Updated calendar (10 days late!)
Rebaked lighting
Reduced material count by a few
Fixed a few incorrect materials
Safety and Security fixes

5/02/19 (69.82mb)

Removed 33.78mb from the build(!!!) (Huge thanks to TCL!)
Fixed an incorrect texture on the light over the notice board #18 (Thanks @HugoZink!)
Disabled the live audio player temporarily

04/27/19 (103.6 mb)

Fixed inconsistent and broken Pickup Respawners
Fixed the Pillows in The Roost
Fixed UV1 on meshes above main bar

04/24/19 (103.6 mb)

Possibly removed extranious refrences to unused objects
Adjusted texture sizes
Adjusted texture filtering to be trilinear on almost all textures
Removed extrainous material slots on some meshes
Removed extrainous some extrainous geometry
Rebaked Occlusion
Power Water and Kirito are back from holiday
(Also did lots of Quest work)

04/10/19 (104.98 mb)

Reduced overdraw on walls in main bar
Reduced overdraw on winndows in main bar
Updated the calendar (10 days late!)
Removed a few extranious materials

03/27/19 (106.89 mb)

Fixed the milky water (Thanks Meme_man!)
Fixed the Desaturated White Russian (Thanks Meme_man!)
Added photos from Saint Patrick's at The Pug 2019
Fixed texture on the lamp in The Roost

03/21/19 (106.78 mb)

Updated lamp and fixture emissive maps
Rebaked lighting
Rebaked Occlusion
Added bells to each floor

03/19/19 (106.52 mb)

Adjusted lighting down a bit in the main bar
Tweaked The Bucket
Adjusted PPV transition falloff for the kitchen coolers
Adjusted compression on some textures
Fixed the high gloss on the banners

03/18/19 (106.26 mb)

Took down the decorations
Updated lighting in many areas
Improved lightmap UVs on booth backs
Improved overdraw in the bathrooms

03/16/19 (108.52 mb) (444)

Saint Patrick's at The Pug 2019!
Thanks to Polopo for the help getting the Leprachaun avatar optimized!
Thanks to Zarniwoop and ShutUpSargent for suggesting hidden Leprecauns!

03/05/19 (105.32 mb)

Fixed the seat toggle for the chairs near the corner booth on the first floor (Thanks Zarniwoop!)
Turned off dithering in the material shaders. Dithering is still applied by the Post Processing Stack (Thanks Poplopo, HugoZink!)
Fixed an incompatibility between the liquid shader and an upcoming patch (Thanks TCL!)
Fixed the fireplace chimney being visible through the window in The Roost
Fixed the visible floating square in the sky
Fixed a gap behind the fireplace
Updated Calendar (5 days late!)
Put up Saint Patrick’s Day promotional decorations

02/21/19 (106.01 mb)

Fixed stage mic not respawning
Fixed misaligned collider near the lamp in The Roost
Rebaked lighting in the main hallway
Updated Patron flyers

02/15/19 (105.61 mb)

Stoves are ready for Udon 🍜
Updated Patron flyers

01/31/19 (104.5 mb)

Fixed the Night View bar lock
Updated Lightmaps on various objects in The Roost
Updated wood grain on various objects in The Roost
Fixed Z-Fighting on the table behind the couch in The Roost
Fixed Z-Fighting on Night View bar
Fixed low resolution texture on the sword in The Roost
Updated Light Probes in the main bar to be more consistent
Updated calendar (one day early!)
SDK Bump: VRCSDK-2018.12.19.17.03_Public

01/16/19 (103mb)

Rebaked Lighting
Updated Patron flyers
Updated Specular proxy objects

01/03/19 (103mb)

Adjustments to the live audio setup
Adjustments to textures and meshes to reduces the download size a bit
Fixes to reflection probes (Thanks Zarniwoop!)
Fixed house music

01/02/19 (107mb)

Took down holiday decorations
Adjusted bloom a bit
Fixed texture on the solo stool in The Roost
Adjusted collision on the stage edge

12/31/18 (113mb)

New Years decorations and Music

(Thanks CubedParadox for lending me your Record Player!)
(Radio Soulwax - Under the Covers)


Clamped bloom more aggressively
Drywall is now uniformly scaled and oriented
Updated the calendar
Misc fixes

12/28/18 (107mb)

Fixed the missing colliders in the women's bathroom (Thanks @SplitScream#8411!)
Removed the invisible collider in the first-floor hallway (Thanks @Sheppard#1998!)
Fixed missing collider along the doorway to the back stairs (Thanks @Sheppard#1998!)
Implemented a new ""lightmap method"" on some objects. Notably the bar in Night View.
Fixed hole in the ceiling near the kitchen door
Fixed floating baseboard in the back staircase
Improved lightmaps on various objects
Added a new lamp!
Padded the seat backs on the chairs in Night View (The Roost will follow later)
Rounded the Globes on the tables in Night View
Brightened up Night View a bit
Repainted the ceiling in Night View
Updated stools in the main bar so they can hold pickups
Misc fixes
Switched to Post Processing Stack v2

Flashing light from broken geometry should no longer happen
Bloom is clamped to prevent malicious emission values
Testing Post Processing Volumes with the kitchen coolers
Testing auto-exposure



12/24/18 (106mb)

Finished the refactor on the corner booth and near by booths
Fixed wood grain on the trim of the lower landing of the stairs to Night View
Added wood trim along the red wall on the first run of stairs to Night View
Added baseboard to wall near the main bar mirror
Added tiles to the walls that were missing them in the bathrooms
Improved light maps on many meshes
Cleaned up the geometry of some meshes

12/23/18 (108mb)

The Yule Goat has risen! 🐐
Fixed the floor from the hallway sticking into the womens bathroom

12/20/18 (105mb)

Refactored parts of the booths around the corner booth on the first floor to fix lightmapping and lower draw calls
Lowered the intensity of the specular light proxy objects (Thanks Korro Bravin!)
Adjusted the live stream audio sources
Fixed hazy materials on some objects

12/19/18 (105mb)

Tweaked reflection probles
Tweaked specular on many objects
Improved normals on beer taps
Improved normals on trash taps
Improved fake mirrors in light of the new specular profile
Improved light mapping on various small objects
Reduced download size a bit

12/18/18 (109mb)

Added a few more holiday decorations
Added a new sculpture to The Roost (Thanks Poplopo!)
Fixed dark table tops
Fixed lightmapping throughout The Pug
Improved specular response throughout The Pug. - Still needs tweaking (Thanks SeraRealm!)

12/12/18 (93mb)

Rebaked lightmap - fixed many issues/errors from the 5.6-2017.4 upgrade
Restored holiday banners

12/11/18 (91mb)

Fixed regressions (Reapplied the last update)
Updated patron flyers
Added a few holiday decorations to Night View and The Roost
Added new textures for the red phone
SDK Bump: 2018.12.04.10.25
Engine Bump: 2017.4.15f1

12/07/18 (92mb) - Final FIVE SIX update

Updated the Calendar (7 days late!)
Updated Patron flyers
Fixed the floor in Night View (Thanks laugexd!) [ Issue #10 ]
Started decorating for the holidays
Rebaked Occlusion

11/15/18

Unity 2017 shenanigans.

11/09/18 (96mb)

Adjusted a broken trigger in the Mr. Whiskers Puzzle to hopefully fix it (Thanks Naelstof)
Tweaked live stream playback component
Added a toggle to disable interaction with seats in The Roost
Adjusted some seats in The Roost so they are a bit easier to interact with for desktop users
Fixed missing seats on side couch in The Roost
Updated various materials
Rebaked Occlusion

11/08/18 (97mb)

Added a toggle to disable interaction with seats in the main bar - it is in the back room
Fixed ObjectRespawners on some more objects, including the Pillows in The Roost - Caused by this bug: http://bit.ly/vrc-teleportTo-hierarchy
Fixed missing Corner Booth seat stations
Fixed offset seat stations on stools near the bar mirror (Thanks Zarniwoop!)
Updated some parts of the Mr. Whiskers puzzle to use Custom Triggers
Shined up the booth table legs
Reduced drawcalls by 1 or 2

11/06/18 (98mb)

Fixed ObjectRespawners - Caused by this bug: http://bit.ly/vrc-teleportTo-hierarchy
Fixed Mr. Whiskers Puzzle - Caused by this bug: http://bit.ly/vrc-teleportTo-hierarchy
Fixed unsightly seams on the new booth models
Fixed a missing booth barrier in the Bar mirror (thanks Sheppard#1998!)
Fixed some lightmap issues on the Night View Bar (more need fixing)
Fixed some lightmap issues with the stools on the first floor
Fixed light leaks near the ceiling on the stairs to The Roost
Fixed light leak from the back staircase into the back hallway
Fixed tall baseboard along the tall windows in Night View - you can't walk on it anymore.
Fixed the taps, they were still hooked up to the Halloween kegs
Fixed disappearing White Russian liquid
Improved framing on Halloween 2018 Photo
Removed a draw call or two in Night View
Updated drink menu models
Updated textures the on the ceiling vents
Updated Patron flyers
Rebaked Occlusion
SDK Bump: 2018.11.05.17.42

11/02/18 (102mb)

Reduced a couple more draw calls throughout the map
Rebuilt the meshes for the booths near the mirror on the first floor
Updated all of the drywall materials
Improved colliders near corner stool on the first floor
Improved colliders along the base of the tall windows in Night View
Improved lightmap on the stairs to The Roost (Dark upper border should be gone)
Fixed missing light near the back exit
Added a group photo from Halloween at The Pug 2018 to the wall on the stairs
Readded tags: bar, stage, hangout, social, classic

11/01/18 - 2 (97mb)

Reduced draw calls on various objects around the main bar a bit.
Updated fishbowl water - hopefully fixing the flickering.
Rebaked occlusion
Removed a few stray Halloween remnants
Added tags: bar, stage, hangout, social, classic
SDK Bump: 2018.10.31.10.45

11/01/18 (96mb)

Removed Halloween Decor
Updated calendar
Updated patron flyers
Trigger adjustments to the Mr. Whiskers puzzle

10/27/18 (102mb)

🎃 Halloween at The Pug 2018

10/17/18 (90mb)

Adjusted liquid shaders some more
Fixed the missing colors from the red/blue/green pints (Thanks Hystericmikey!)
Removed additional superflus objects and materials

10/16/18 (90mb)

Reduced draw calls by 0-4 in main bar area and stage area
Fixed sorting issues with liquid and glass (Hopefully)
Fixed the shifted fireplace light
Removed some un-needed disabled objects

10/15/18 (90mb)

Updated Patron flyers
Updated liquid shader
Updated the glass material on the clocks

10/10/18 (91mb)

Adjusted the liquid shader (hopefully the weird refraction rendering is gone)
Removed a draw call on the rose in The Roost
Fixed a seat on the fireplace couch in The Roost that had a very long interaction distance (Thanks Pan Diman!)
Fixed the oversized interaction boxes on the fireplace couch in The Roost
Adjusted the glass on the mirrors
Removed a draw call on the clocks

10/09/18 (90mb)

Fixed the performance issue with the new glass shader (Thanks CubedParadox!)
Added table tents

10/08/18

New glass shader (Thanks CubedParadox!)
Banners for the Halloween Party are up

10/03/18 (350) (89mb)

Updated the SDK - VRCSDK-2018.10.02.10.29_Public
Updated the Calendar (3 days late)
Fixed some occlusion errors
Patron poster updates

09/20/18 (88mb)

Fixed a Patron poster
Fixed eject buttons in the bar (Thanks Meme Man!)
Fixed a phone receiver that was made of cloth
Fixed some reflection probe placements
Rebaked lighting

09/19/18 (88mb)

Adjusted some light probe placements
Updated/fixed respawn timers on a few objects
Updated Patron posters
Deleted some extranious objects that I found hiding in nooks and crannies

09/18/18 (87mb)

Cleaned the darkness off the Orchid on the welcome desk
Worked around a bug with onTimer triggers (hopefully)
Fixed weird geometry on the main staircase
Improved wood grain direction on the main staircase
Improved lightmap on walls on the main staircase
Improved wood grain direction on the sleeping platform in The Roost
Rounded up the plates
Patched some holes in The Roost ceiling (again)
Solidified the top of the stools in Night View (Thanks Poplopo!)
Removed a weird onInteract trigger near the entrance (Thanks Meme Man!)
Removed some errant animations on the Yellow Spotlight on the Stage
Updated Patron posters
Minor fixes

09/13/18 (86mb)

Updated patron posters
Added steamer pans
Minor fixes

09/9/18 (86mb)

Fixed weirdly shiny materials on main staircase and stage
Fixed leaky faucets in the men's bathroom
Fixed the issue with the sinks being missing in the bathroom mirrors
Added a lot more brushed steel in the kitchen

09/7/18 (85mb)

Loaded up a new Calendar (7 days late)
Sanded down the bathroom sinks to round them out, then re-polished them
Fixed the issue with the Whiskey being a vampire (Thanks Exiled!)
Fixed the issue with teleporting booth seats (Thanks Jordo!) [ Issue #8 ]
Fixed the issue with the missing wall collider near the bar mirror (Thanks Misaki and others!)
Updated TheArchitects poster - He does more than homeworlds now!

08/31/18 (89mb)

Added a new bottle label for Presence
Updates to stage controls for performers
Tweaked the tap triggers to be less square
Rebaked occlusion to fix up the stage
Tweaked toilet sounds so they should be audible again

08/30/18 (89mb)

Moved stage speakers off the stage and added monitors fulfilling Issue #3
Fixed the missing animation on the toilet water
Updated toilet and tap timers to (hopefully) work around a current bug with timers
Reduced range of toilet flush sound (hopefully)
Updated models and materials on stage equipment to reduce the draw calls a bit
Updated the live performer controls
Updated the meshes/materials on ceiling lights in Night View to reduce the draw calls a bit

08/29/18 (89mb)

Peformed some plumbing; The toilets may or may not ""work"" now
Hooked up the beer taps in the Night View bar
Added timers to the beer taps so they turn off after being left on - they were wasting so much beer!
Took down the open sign for Night View (it's always open these days!)
Adjusted the basement door so it is more logical when open, and when being handled
Adjusted lights near the main staircase and first-floor hallway entrance
Fixed the issue causing a teleport if you walked under the back stairs on the first floor (Thanks Exiled!)
Fixed texture tiling on toilets
Fixed more descended canister lights
Adjusted draw calls a tiny bit in the main bar area
Adjusted they way some sound effects play
Restored the MIP Maps on the posters

This exists now: The Great Pug - Steam Group
08/28/18 (91mb)

Made refinements to the Night View shelving meshes
Made refinements to the lightmap on the Night View bar
Made the main staircase a bit brighter at the first-floor landing (Thanks Exiled!)
Fixed the light canisters that were descending on the main staircase (Thanks Garret!)
Updated the Security Colliders; they should be a little more forgiving now (Thanks Korro!)

08/24/18 (91mb)

Fixed the flickering doors in the buffet in The Roost
Updated the textures on the clock and banners
Tweaked some baked lights
Made some minor draw call optimizations
Other minor tweaks

08/23/18 (91mb)

Removed some legacy VRC Chair scripts I found hiding around the map
Minor tweaks

08/22/18 (91mb)

That missing door frame returned home and apologized. It just needed some time away from all the people.
Re-jiggered texture compression on some things
Re-jiggered audio compression on some things
Made adjustments to the lighting
Further adjusted the Post Processing stack
A very Rigid Body was found in The Roost and removed.

08/21/18 (100mb)

Bloomified the fire in The Roost for a more cozy glow
Added a new menu model – more to come down the road here
Material adjustments
More draw call optimizations in the main bar area; 5-10 draw calls depending on the direction you are looking
Patched up the hole in the ceiling near the kitchen door

08/20/18 (97mb)

Fixed Basement occlusion issues
Fixed collider sticking into the main bar from the basement
More draw call optimizations in the main bar area; 1-12 draw calls depending on the direction you are looking
Adjusted the post-processing stack
Minor collider adjustments
The Devil Bucket should be easier to pick up now

08/18/18 (99mb)

Refactored the basement meshes

08/17/18 (99mb)

Fixed collider above the table behind the couch in The Roost
The VRCHAT ARCHIVES advert has gotten a bit dirty in the past year and a half. (Thanks Zarniwoop!)
Material updates
More materials are now using the Dithering Shader

08/16/18 (100mb)

Fixes/Adjustments for the live show audio
Adjustments to shadow casters in The Roost and on the stage
At least 1 draw call removed.

08/12/18

Fixed the missing/shifted colliders on the upstairs bar
Fixed the missing colliders on the downstairs bar cooler
Adjusted the Dithering Shader a bit

08/11/18

Updated the Dithering Shader to v.1.calm.0.0.pseudology.1534016371.7
Many wood materials updated
Bloom is back to its previous level
Light Probes should no longer bleed out of the cooler into the back stairway as easily
Fixed weird light fixture placements throughout the first floor
LOD adjustments for various signs
Fire extinguishers should no longer be inside the wall
Bar Two has slightly better UV unwrapping now
Reflection Probes adjusted down
Removed 3 draw calls from the bar cooler.

08/08/18

THE BELL WORKS AGAIN!
Fixed issues with the beta Dithering Shader by Xiexe (Thanks TCL!)
Fixed the weird sheen on the First Dollar plaque (Thanks Exiled!)
Shaved 1-2 draw calls off of a couple items
Fixed the weird ceilings in The Roost staircase
Fixed light baking issues in a few places
Fixed the ghost chairs by the Corner Booth on the first floor
Fixed the floor material in the bathrooms
Removed references to non-existent chair placement scripts by CubedParadox (Thanks Cubed!)

08/07/18

Calendar Update (7 days late)
Many shaders changed to a beta version of a Dithering Shader by Xiexe (Thanks Xiexe!)
Shaved 1-2 draw calls off stage props

07/27/18

Additions to the live streaming audio support
Minor fixes

07/19/18

SDK Bump - 2018.06.21.13.02
Added experimental live streaming audio support
Made some Minor LOD tweaks

07/03/18 (314)

LOD Tweaks
Reflection Probe fixes
Metalic tables fixed

07/02/18 (312)

Collider blocking stairs fixed

06/29/18 (311)

Bell should be fixed
Lightmap fixes
Gap below short wall at the top of the roost stairs fixed
New Brushed Metal material
Minor optimizations
LOD setup on many items, we will see how that goes.
Collision changes
Reflection probe adjustments

06/29/18 (310) (110mb)

Implemented minor draw call optimizations
Updated lightmaps on back hallway, no more light leaks near the exit sign
Greatly reduced lightmap artifacts on main stairs leading to Night View
Overall lightmap filesize dropped

06/28/18 (308) (116mb)

Implemented additional minor draw call optimizations
Updated the wine bottle labels
Reflection probe resolution changes to reduce download size

06/22/18 (307) (122mb)

Texture resolution increases
Texture compresion changes
Reduced overall download size by 6mb

06/08/18 (306)

Made draw call optimizations
Added missing baseboards in Night View
Reflection Probe Adjustments
Stools have shadows again!
Boxes under the Night View bar are now walkthrough (thanks Meme Man)

06/07/18 (305)

Updated the appearance of the lampshades
Made some minor draw call optimizations
Modified lightmap settings

06/05/18 (303)

Made some minor draw call optimizations
Updated some meshes to have better geometry and normals

06/04/18 (302)

Made some minor draw call optimizations
Fixed a missing collider near the bar mirror

06/03/18 (300)

Made a few draw calls optimizations
Changed the near clipping plane distance on the reference camera (the far clip was not changed)
Rebaked occlusion
Updated the meshes for the sink, Roost shelves, stair railings, booth base/backing, and other minor meshes
Made minor Trigger broadcast type adjustments
Re-painted the wall near the main bar
Added a new Patron flyer

Quick Fix (301)

Fixed some mis-aligned colliders that were out of place.

06/01/18 (298)

Made some draw call optimizations!
Made changes to the Object Respawing behavior to attempt to address lag when a new user joins the world.
Mixed Lights now are forced off when you are not in view of them. This is being done as a precaution because Occlusion Culling may not have been doing the job in all cases.
Rebuilt the shelves under the bar
Updated the Calendar
Tweaked the lighting
Changed the material on the dynamic towel
Fixed a gap in the ceiling in The Roost
Tweaks to various trigger broadcast types
Made some chair upgrades

05/24/18

House Music placement/falloff changes
Stage Lighting Updates
New Stage Lighting Control Board
Addition of Dynamic Event Posters for Spork of Love
Minor Updates to the Event Posters for MckMuze
Bulletin Board Updates

05/10/18

Administration controls added to lock and unlock the stage in a basic manner. (Issue #2 - MckMuze)

More work still needs to be done to fully satisfy the bounty to my satisfaction.


Stage lighting has been improved
Dynamic lights in Night View have been improved
Mesh updates for the stage - Rounder!
Material updates to the sage, Night View Floor, and Bars
This change log had dome embarrassing typoss

05/07/18

Reflection Probe Updates
SDK Bump - 2018.05.01.20.38
Flyers Added

04/24/18

New Bulletin boards

04/19/18

Bathrooms should be back to normal

04/14/18

You should no longer stick to the walls when using the main stairway or the stairs in The Roost
Mckmuze setlist lighting has been fixed
Missing lightmap on painting has been found and reapplied
New decorations in Night View and The Roost
New furniture in The Roost
Hopefully fixed some lag issues related to triggers

04/11/18

Weird, the basement door kept staying opened. - Fixed

04/08/18

Coasters added around The Pug to keep the finish on the wood nice
Eggs removed

03/30/18

Cleaned up straggling Saint Patrick's Day decorations
Improved Resolution on wireframe posters
Possible fix for the flickering hub portal
Eggs.

03/19/18

Removed Saint Patrick's Day decorations

03/17/18

Saint Patrick's at The Pug - 2018
UV fixes for the shelves in the wall behind the main bar
New wall art
Fixes to the glass roof in The Roost
Duplicated mesh fixes
Security Improvements

03/13/18

Calendar Update
Moon Fix

02/24/18

St. Patrick’s Day 2018 - Promotional Table Tents
Enhanced appearance of Night View stage spotlights and floor lights
Multiple spawns
Minor draw call optimizations
Lightmap fixes
Rose added in The Roost - Thanks Poplopo!
New shelves in Night View
Improved the readability of the bulletin board flyers
Fixed typo on bulletin board
Humoungously improved the wall near the back storage room
Security Improvements
SDK Bump

Thanks for the help testing Zarniwoop, Poplopo, and Zircronswift!
02/06/18

Removed birthday decorations
Removed portal to the prototype

02/02/18

Birthday decorations
Temporary portal to the prototype

02/01/18

Added a delightful painting above the fireplace. It was painted by Dicidius. Thanks, Dicidius!
A matching sword is on display
New shelf along the back wall in The Roost
Telephone ring volume lowered a bit
Calendar updated
Moon and city lights properly restored for real this time
Bulletin board updated

01/05/18 (268)

Added security colliders to prevent trolling from outside the map.
SDK Bump

01/02/18

New Years Decorations Removed
Moon and city lights restored
Phone Ring distance adjusted - hopefully, the beds are usable now

12/31/17

Far Back Staircase/Fire Escape
Most objects should Respawn when left lying on the floor, in weird places or outside of the map.
Exterior Meshes
Woodgrain direction fixes
Mesh improvements on the bar
Ambient Lighting Tweaks
Bathroom Stalls now have handles and latches
Skybox uses fewer draw calls
Hole in Phone base fixed
Material(s) consolidated on the phone base
General draw call optimizations
Lighting tweaks
Christmas Decor Removed
Calendar Updated
Disc for 2018 added
Champagne for New Years
Decorations for New Years

12/22/17

Seat Fixes

12/14/17

SDK Bump - VRCSDK-2017.12.12.13.36_Public
Martini added (941101501153505281)
Occlusion Settings Reverted
Lightmap tweaks
Material fixes

12/12/17

Downstairs beer taps should work correctly now
Christmas Decoration Updates
Draw Call Optimizations
Light Probe Improvements
Mesh Updates on the Night View bar
Material Tweaks
Toilet seat fixes
Dining chairs should be easier for desktop users to use
Occlusion changes

12/01/17

Calendar Updated
Lightmap Tweaks
Christmas Decorations
Material Optimizations
Thanksgiving meal put in storage

11/22/17

The Roost is Open
Added Thanksgiving Food
Improved Night View Hall sign (Thanks Poplopo)
Adjusted audio volume falloff on sink taps
Updated Meshes in The Roost
Added fireplace in The Roost
Added seating area in front of the fireplace in The Roost
Added table and chairs in The Roost
Fixed Grain Direction on various objects
Updated Proximity Dance Club Portal

11/16/17 (200)

Lighting Tweaks
UV Fixes on the 2nd Floor floor
Mesh Updates on Booth's backs
UV Fixes on Booth bases
Increased Red Phone ring frequency

11/15/17

Adjustments to the way pickups reset, hopefully fixing them
Patched over Z-Fighting at the top of the stairs
""Un-Fixed"" the Devil Bucket
The Red Phone should now randomly ring
Audio played from the phones should be easier to hear now

11/14/17

First attempt at making pickups reset when idle in undesirable locations.

11/09/17

Calendar Added
Red Phone Added
Various Materials Improved
Lighting Tweaks
Lightmap Resolution Changes
Minor Fixes
VRCSDK Updated to 2017.10.26.17.36

11/07/17

Material Updates
Material Fixes
Minor Fixes

11/06/17

Material Updates
Bar Mesh Updates
More Face Weighted Normals
Minor Fixes

11/04/17

Small Ceiling Vents Added
Face Weighted Normals on various objects
Faucets in the bathrooms now work
Toilets have been scrubbed
Minor Fixes

10/30/17

Post Halloween Party restore

10/24/17

Reflectiopn probe fixes
Halloween prep

10/13/17

VRCSDK Update to 2017.10.04.13.58

10/12/17

Halloween Promotional signs put up
WebPanel disabled
Minor Fixes

09/22/17

Lighting Tweaks
Material Updates (Albedo Checks)
Minor Fixes

09/19/17

Lighting Updates
Minor Fixes

09/12/17

Lighting Updates
Birthday Cake Optimizations
Minor Fixes

09/11/17

Bathroom Collider Fixes
Martial Swaps
Major Light Probe Overhaul
Minor Fixes

09/09/17

Bathroom ceiling now reflects in the bathroom mirrors

09/05/17

Minor Fixes

09/01/17

Bottle Liquid Fixes
Bathroom Walls Fixed

08/31/17

Updated Materials
Optimization for Bathroom Mirrors
Minor Fixes

08/30/17

Five Six

08/17/17

Karaoke added for Karaoke night

08/10/17

Karaoke functionality testing

07/12/17

SnailLock testing

06/30/17

Fireworks added
Minor fixes

06/22/17

Lighting updates
Trigger updates

06/21/17

Minor changes

06/20/17

Dance lights / floor lights added to Night View
Stage lights updates
Overall nightview light added

06/19/17

Sleeping roost updates
Minor fixes

06/14/17

Rounded edges on the lower bar
Added door latches

06/13/17

Refinements to the stairs
Fixes to the Night View floor

06/10/17

Basement optimizations
Sleeping Roost started

06/01/17

Additions for Contact
56 - Stage Lights
Reload button added for the YouTube panel

05/23/17

Texture size optimizations
Floor UV fixes
Adjusted seats for desktop users
Light probes reduced in overall count

05/19/17

Event board typo fixed

05/18/17

Collider fixes
Combined various meshes

05/17/17

Optimized Smörgåstårta
Added Smörgåstårta under glass to the back room
Texture fixes
Mesh fixes

05/16/17

Occlusion fixes
Baseboards added in various places
Coffee added
A large number of mesh colliders have been replaced with box colliders

05/15/17

Doorbell updates
Occlusion changes
Backroom additions
Security camera added and removed

05/12/17

Doorbell added
Cans of fish added
Lighting updates
Smörgåstårta.

04/21/17

White Russians
Light probe fixes
Booth fixes
Lighting changes
Tapster locks added
Switched to using Euan's video player

04/19/17

Eggs removed
Sink handles added
Event board updates

04/15/17

Minor fixes

04/14/17

Event board added
Easter eggs added for easter
Lighting changes
Wine bottles added
Wine glasses added

04/13/17

YouTube panels trigger fix
Stage Prop updates
Trigger updates
MckMuze setlist added
Stage updates
Occlusion fixes in the basement

04/12/17

Lightmap resolution adjustments

04/10/17

Corner booth fixes
Booth seat fixes
Upstairs gate fixes
Lighting fixes
Basement Door added
Basement wall issues fixed
Texture storage size optimizations

04/07/17

Basement
New chairs
YouTube panel fixes
Minor fixes

04/06/17

Mckmuze decor updates
Bar Two - Tigger and button fixes

04/05/17

Bar Two - Improvements
Trigger updates (groan)
Podium added to Night View

04/04/17

Mirror fixes
Metal material updates
Bar top fixes

04/03/17

Bar Two - shelves added
Lighting and light probe changes
Upstairs security changes

04/02/17

Video screen fixes
Brighter lighting

03/31/17

Second bar added in Night View
Stage lights added
New back panel controls

03/30/17

Foot pedals and amp added to the stage

03/29/17

YouTube panels added to the stage
Minor fixes

03/24/17

Corner booth implemented (it's too small!)
PhysSound added
Music fixed

03/23/17

Materials updated
LOD enabled on booth backs

03/22/17

Minor fixes

03/21/17

Decorations taken down
Minor mesh updates
Taps now dispense normal beer

03/17/17

St Patrick's Day Decorations
Taps are now interactable
Speakers
Party Music Player - Thanks Cubedparadox for the youtube playlist sync!

03/13/17

Trigger fixes (grumble)

03/12/17

bathroom Updates
Material atlasing
Security updates

03/10/17

Optimized Meshes
Added St Patrick's Day table tents
Minor Fixes

03/09/17

Added lights to the St Patrick's Day posters
Updated Materials

03/08/17

St Patrick's Day Posters added
Bar height adjusted
Measuring sticks added

03/07/17

Added a Clock
Updated those fancy liquid shaders
Minor fixes

03/06/17

Added fancy liquid shaders
Minor fixes

03/03/17

Added Your Favorite Beer Neon Sign
Updated Materials
Added lights above the bar top
Minor fixes

02/27/17

Implemented Security for the Bar
Added Your Favorite Beer
Optimized Meshes
Optimized Objects
Minor Fixes

02/24/17

Added the back room and it's keypad
Minor fixes

02/23/17

Added MckMuze signs
Minor fixes

02/21/17

Fixed the bar mirror
Optimized Geometry
Optimized Materials
Optimized Occlusion
Minor fixes

02/20/17

Smaller Light Maps
Lighting Changes
Added Gates to the bar
Added Staff Only Sign
Added more canister lights in the Ceiling
Added photo of Q sleeping

02/19/17

Fish Bowl Added
Light bake fixes

02/17/17

Posted my Liquor License
Mesh optimizations

02/01/17

Initial Release
VRCSDK version 2016.12.01.18.02

",12
jimboy3100/jimboy3100.github.io,JavaScript,"Legend Mod
Mod for Agar.io multiplayer action browser game.

Author: jimboy3100@hotmail.com
Website: www.legendmod.ml
<iframe width=""800"" height=""452"" src=""https://www.youtube.com/embed/CnIfNSpCf70?rel=0"" frameborder=""0"" allow=""autoplay; encrypted-media"" allowfullscreen></iframe>
Legend mod GitHub Library
Feature Highlights

10% extra zoom-out (see enemies from further)
Fast feed shortcut (hit viruses and feed team mates faster)
Double split shortcut
Triple split shortcut (for tricksplits)
Minimap (find your team mates, avoid getting cornered etc)
15 configurable shortcut keys to send messages to your team quickly

Other Features

Updates automatically
Unlimited FPS unlocked (quicker than Vanilla)
Old Skins
Animated Skins
UserScripts Manager (URL or pasted)
Language Packs
Direct PARTY / FFA / EXP / TEAM server by using tokens/sips or connector
Search engine for player name / clan / tag / leaderboard / ip / token
Integrated Chat, minimap and teamboard. Chat rooms per server/per team password(or public)
New Template / skins / animations / zoom / respawn / helpers / hud controls and many extras
Themes for quite all textures and map (Basic / Menu / Hud / chat / minimap / graphics and cursor)
Banners for many clans (Email me your symbol and weblink for updates)
60++ Macros / Events / Hotkeys (Script does many calculations)
Tools for quests / youtubers / timers / coin auto digger/youtube video player
Send message pictures, videos and also various message commands directly to teammate's script
Change various textures, add photos on huds and clan's pictures and url links
Dying Light Expansion
Discord webhook handler's for sending IP, and many more...

Legend Mod libraries
Github,
Greasyfiork,
Agarioscripts Chrome Extension
Installation

Install Tampermonkey browser extension on Chrome , Opera
Install Legend Express script here  

Screenshots
Welcome Screen - Copy Token, Leaderboard, IP

Searching

Youtube

Features

Big Names (visible to legend/ogario users only)

Banners and website anchors for many clans

Old Skins

Legend mod is based on many scripts (ogario, kitty, turtle clan scripts and others that can be found on greasyfork website).
",5
ZkHaider/Figure,Swift,"Figure
Figure is a elegant declarative UI library, utilizing the Final Tagless solution to the expression problem.
Installation
Carthage
Add github ""ZkHaider/Figure"" ""master"" to your Cartfile, and run Carthage.
Requirements

Deployment target iOS 10.0+
Swift 4+
Xcode 10+

Usage
A view is declared simply by:
let view: iOSRenderer = .view()
You can then render that view like so:
let renderedView: UIView = view.render()
You can simply create a var in your UIViewController as your root view, like so:
import FigureiOS

final class ViewController: UIViewController {

    // MARK: - View 

    var rootView: iOSRenderer {
        return .view()
    }
    
    // MARK: - Init
    
    // ...
    
    // MARK: - Lifecycle 
    
    override func loadView() {
        self.view = self.rootView.render()
    }

}
Notice we declaratively create the view in rootView and then render it by calling render() in loadView(). Alternatively you can subclass your UIViewController with RenderViewController and override the rootView property. RenderViewController handles your view rendering automatically, so even less boilerplate to write.
import FigureiOS

final class ViewController: RenderViewController {

    // MARK: - View 

    override var rootView: iOSRenderer {
        return .view()
    }
    
}
Declare view hierarchies like so:
let myNestedView: iOSRenderer = .view(
    config: [.backgroundColor(.red)],
    .view(
        layout: [.set(width: 100.0, height: 100.0)],
        .view(layout: [.fill]),
        .view(),
        .view()
    )
)

let view: UIView = myNestedView.render()
The above will create a root view which has a red background with 1 subview which has a width of 100.0 and a height of 100.0 which itself should have 3 subviews where 1 of those subviews fills the frame of it's parent view.
Contributions
Figure welcomes both fixes, improvements, and feature additions. If you'd like to contribute, open a pull request with a detailed description of your changes.
As a rule of thumb, if you're proposing an API breaking change or a change to existing functionality, consider proposing it by opening an issue, rather than a pull request; we'll use the issue as a public forum for discussing whether the proposal makes sense or not.
Maintainers
Haider Khan

https://github.com/ZkHaider

Timothy Kautz

https://github.com/littleowl

",2
zacanger/z,Shell,"$HOME
Please feel free to use anything you like!
Unless otherwise noted, everything here is under the MIT license.
I keep this repo at /home/z/Dropbox/z and symlink a lot of stuff to /home/z,
so there may be a few references to those paths scattered around.
Notes:

Professionaly, I write Node, shell, and lots and lots of config. A lot of my
setup is oriented around quick editing of text and quick navigation.
This is shared between some laptops running Ubuntu, Debian, and (work) macOS.
I use nvim, but init.vim config probably works fine with Vim 8 (if renamed to .vimrc)
dwm on Linux, chunkwm on Mac
Bash 4 and 5, Python 3 mostly, Node latest, Neovim.
The files called *.list (under /misc) are to keep track of what I need on a fresh computer.

apt.list: generated with apt-mark showmanual
npm.list: npm i -g all these things, generated with global-packages-cli
pip.list: Python 3
go.list: go get -u this stuff


On fresh computers, I sync Dropbox first, then run either new-mac.sh (and
symlink all the things) or new-linux.sh.

",7
TomYang1993/javacs,Java,"Data structures and algorithm code
",6
bergerhealer/BKCommonLib,Java,"BKCommonLib
Spigot Resource Page | Dev Builds
To build BKCommonLib you will (probably) need to run Build Tools beforehand.
Otherwise tests will fail and maven will complain. No actual server code is linked during compiling, hence the dependency is type test.
This is a library-plugin system, introducing a lot of utility classes
It also simplifies coding:

PluginBase: allows quick registering and monitoring of plugins being enabled
(Async)Task: allows quick task starting and dynamic in-code creation
Utilities for virtually every needed task ahead, from mathematics to obtaining an entity by UUID
Custom node-based configuration extension
BlockLocation, BlockMap and BlockSet to safely use blocks in maps and sets
ItemParser class to generate a parser from user and use it during item transactions
Nanosecond StopWatch class for performance monitoring
Operation class to handle entities, players, chunks and worlds the way you want it

License
Copyright (C) 2013-2015 bergerkiller
Copyright (C) 2016-2017 Berger Healer
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, and/or sublicense the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
",41
mchehab/zbar,C,"ZBAR BAR CODE READER
ZBar Bar Code Reader is an open source software suite for reading bar
codes from various sources, such as video streams, image files and raw
intensity sensors. It supports EAN-13/UPC-A, UPC-E, EAN-8, Code 128,
Code 93, Code 39, Codabar, Interleaved 2 of 5, QR Code and SQ Code.
Included with the library are basic applications for decoding captured bar
code images and using a video device (eg, webcam) as a bar code scanner.
For application developers, language bindings are included for C, C++,
Python 2 and Perl as well as GUI widgets for Qt, GTK and PyGTK 2.0.
Zbar also supports sending the scanned codes via dbus, allowing its
integration with other applications.
Check the ZBar home page for the latest release, mailing lists, etc.:

https://github.com/mchehab/zbar

License information can be found in COPYING.
BUILDING
See INSTALL.md for generic configuration and build instructions.
The scanner/decoder library itself only requires a few standard
library functions which should be available almost anywhere.
The zbarcam program uses the video4linux API (v4l1 or v4l2) to access
the video device.  This interface is part of the linux kernel, a 3.16
kernel or upper is recommended for full support.  More information is
available at:

http://www.linuxtv.org/wiki/

pkg-config is used to locate installed libraries.  You should have
installed pkg-config if you need any of the remaining components.
pkg-config may be obtained from:

http://pkg-config.freedesktop.org/

The zbarimg program uses ImageMagick to read image files in many
different formats.  You will need at least ImageMagick version 6.2.6
if you want to scan image files.  ImageMagick may be obtained from:

http://www.imagemagick.org/

The Qt widget requires Qt4 or Qt5. You will need Qt if you would like to
use or develop a Qt GUI application with an integrated bar code
scanning widget. Qt4 may be obtained from:

https://www.qt.io/

The GTK+ widget requires GTK+-2.x.  You will need GTK+ if you would
like to use or develop a GTK+ GUI application with an integrated bar
code scanning widget.  GTK+ may be obtained from:

http://www.gtk.org/

The PyGTK 2.0 wrapper for the GTK+ widget requires Python 2, PyGTK.
You will need both if you would like to use or develop a PyGTK GUI
application with an integrated bar code scanning widget.  PyGTK may be
obtained from:

http://www.pygtk.org/

The Python bindings require Python 2.  You will need Python and PIL
if you would like to scan images or video directly using Python.
Python is available from:

http://python.org/

The Perl bindings require Perl (version?).  You will need Perl if you
would like to scan images or video directly using Perl.  Perl is
available from:

http://www.perl.org/

If required libraries are not available you may disable building for
the corresponding component using configure (see configure --help).
The Perl bindings must be built separately after installing the
library.  see:

perl/README

RUNNING
make install will install the library and application programs.  Run
zbarcam-qt or zbarcam to start the video scanner. Use zbarimg <file>
to decode a saved image file.
Check the manual to find specific options for each program.
DBUS TESTING
In order to test if dbus is working, you could use:
$ dbus-monitor --system interface=org.linuxtv.Zbar1.Code

or build the test programs with:
$ make test_progs

And run:
$ ./test/test_dbus
With that, running this command on a separate shell:
$ ./zbarimg/zbarimg examples/code-128.png
CODE-128:https://github.com/mchehab/zbar
scanned 1 barcode symbols from 1 images in 0.01 seconds

Will produce this output at test_dbus shell window:
Waiting for Zbar events
Type = CODE-128
Value = https://github.com/mchehab/zbar

REPORTING BUGS
Bugs can be reported on the project page:

https://github.com/mchehab/zbar

Please include the ZBar version number and a detailed description of
the problem.  You'll probably have better luck if you're also familiar
with the concepts from:

http://www.catb.org/~esr/faqs/smart-questions.html

",22
wobblui/wobblui,Python,"wobblui
(experimental project)
Wobblui is a versatile & easy-to-use UI framework for Python 3!
Why wobblui is awesome:

Cross-platform: Works on Windows, Linux, and
Android
Easy: simple API & versatile auto-scaling box layouts
Efficient: 3d accelerated, on-demand redraw/relayout ('lazy') and more!

It also has a consistent look on all platforms and supports styling,
including freeform UI scaling!
This is how easy wobblui is to use:
from wobblui import event_loop
from wobblui.label import Label
from wobblui.window import Window

w = Window()
w.add(Label(""Hello World! This is a wobblui example!""))

event_loop()

See the Quickstart Guide for more!
Installation
You'll need SDL2 and some SDL2-related libraries as prerequisite,
see the Installation Guide.
Afterwards, just install from pip:
pip install --user -U wobblui

(make sure to use a Python 3.X pip! Python 2 is NOT supported)
Documentation
Jump into the documentation here!
License
Wobblui is open-source under various licenses (due to some included
3rd-party components), but most of it is zlib-licensed.
See the LICENSE.md document for full details!
",3
ronalfy/highlight-and-share,PHP,"Highlight and Share for WordPress
A WordPress plugin for highlighting text and sharing it via Twitter or Facebook (Now available on WordPress.org!).
SA WordPress plugin for highlighting text and sharing it via Twitter and Facebook and other services including LinkedIn, Email, Xing, and WhatsApp.
Sharing selectable text is only possible via Twitter, Facebook, and WhatsApp. However an option to share the post via LinkedIn, Pinterest, and E-mail are also present when highlighting text as a convenience.
If you have a feature request, please add an issue.
Features

Override which content is selectable (using jQuery class notation without the dots).
Enable or disable Facebook sharing.
Enable or disable Twitter sharing.
Customize the Twitter username used.

Advanced customization is allowed via hooks.  See the Plugin Filters section.
Recommended Plugins

Better Font Awesome - Enables Twitter/Facebook sharing icons
WordPress SEO - For Facebook OpenGraph data
JetPack - for URL Shortlinks

Installation
Simply install as a WordPress plugin.
Head to the plugin settings and customize the content area and the Twitter username to use.
Tested on:
The latest versions of Firefox, Chrome, and Safari.  IE testing is coming soon.
Themes tested on:

TwentyFourteen
TwentyThirteen

Plugin Filters
The plugin filters are demonstrated in the code below.
//Example filter usage for highlight and share
//https://github.com/ronalfy/highlight-and-share

/* The following filters take and return booleans (true, false)*/
/* Call WordPress functions __return_false or __return_true */
add_filter( 'has_show_facebook', '__return_true' ); //Disable or enable facebook sharing
add_filter( 'has_show_twitter', '__return_true' ); //Disable or enable twitter sharing
add_filter( 'has_load_css', '__return_true' ); //Disable or enable plugin's CSS - Use your own
add_filter( 'has_enable_content', '__return_true' ); //Disable or enable main post content
add_filter( 'has_enable_excerpt', '__return_true' ); //Disable or enable excerpt content
add_filter( 'has_enable_mobile', '__return_true' ); //Disable or enable on mobile devices

/* Override the Facebook share text (default is Share) */
add_filter( 'has_facebook_text', 'has_override_facebook_text' );
function has_override_facebook_text( $default ) {
	return 'Facebook';
}

/* Override the Twitter share text (default is Tweet) */
add_filter( 'has_twitter_text', 'has_override_twitter_text' );
function has_override_twitter_text( $default ) {
	return 'Twitter';
}

/* Override the JavaScript classes (assuming jQuery class format with no periods) */
add_filter( 'has_js_classes', 'has_override_js_classes' );
function has_override_js_classes( $content ) {
	return 'entry-content,type-page,type-post';
}

/* Add JS IDs */
add_filter( 'has_js_ids', 'has_override_js_ids' );
function has_override_js_ids( $content = array() ) {
	if ( !is_array( $content ) ) $content = array();
	$new_arr = array( 'content', 'comments' );
	$content = array_merge( $content, $new_arr );
	return $content;
}

/* Add JS elements */
add_filter( 'has_js_elements', 'has_override_js_elements' );
function has_override_js_elements( $content = array() ) {
	if ( !is_array( $content ) ) $content = array();
	$new_arr = array( 'blockquote' );
	$content = array_merge( $content, $new_arr );
	return $content;
}

/* Override the Twitter username (no @ symbol needed) */
add_filter( 'has_twitter_username', 'has_override_twitter_username' );
function has_override_twitter_username( $username ) {
	return 'wordpress';
}
Some examples are below:
Disable sharing on a static front page.
 add_action( 'wp', function() {
	if ( is_front_page() ) {
		add_filter( 'has_show_facebook', '__return_false' );
		add_filter( 'has_show_twitter', '__return_false' );
	}
} );
Modify the Content URL
add_filter( 'has_content_url', function( $url, $post_id ) {
	return 'https://wordpress.org';
}, 10, 2 );
Modifying jQuery Selectors
The following demonstrates how to override the jQuery selectors used in choosing which content to share:
//Demonstrates how to select paragraph text only

add_filter( 'has_js_selectors', 'hs_custom_selectors', 10, 5 );
function hs_custom_selectors( $selectors, $content = array(), $classes = array(), $ids = array(), $elements = array() ) {
	//With $content, $classes, $ids, $elements, you can build your own selectors
	//Or just override $selectors (a string) with your own custom ones
	return '.has-content-area p, .has-excerpt-area p';
}
Credit
This script was originally observed on Vogue.com and was ported over for use in WordPress.
",12
Lombiq/Hastlayer-Hardware-Framework---Xilinx,VHDL,"Hastlayer Hardware Framework - Xilinx readme
This document is a guideline which provides a brief description of the Hastlayer Hardware Framework for Xilinx FPGAs. The aim of this document is to help the reader to reconstruct and test the Hastlayer FPGA firmware design and to give a hand when you run into a problem.
If you're not familiar with Hastlayer take a look at https://hastlayer.com/.
Table of contents

Prerequisite requirements
Getting started
Running hardware designs
Release notes
Version control
Upgrading the project to the latest Vivado version
Design reproduction steps
Testing custom IP cores
AXI Lite interface slave registers
Adding custom library functions to the design
Debugging with an ILA core

",2
SquirtleSquadProgramming/ScarkSource,C#,"Scark
Scark is a text-based roleplaying game.
A stable and completed release of the game isn't fully completed as we are in early stages of development. If anyone passing by wishes to play the game, bare in mind bugs and loopholes are inevitable and the story isn't very long.
Scark is programmed in C# using the layout of a C# console app.
You can view the wiki for more information on game dynamics and storyline.
Please review the code of conduct for information about what our community values are for contributing.
Please also read the formatting guidelines for information regarding how code should be formatted within the project.
Programmed, written and created by Squirtle Squad.
Credit to https://www.asciiart.eu/ for use of some great ASCII art. A list of all peoples' artwork featured within this project can be found here.
Also thanks to patorjk's brilliant text to ASCII program which was used in the title screen.

",4
IgoRode/QuimicaEZ,HTML,"QuimicaEZ
trabalho de tcc meu e do meu querido amigo Ronan orientado por nada mais nada menos que o grande Arvaro(Alvaro)
https://igorode.github.io/QuimicaEZ/
",2
Holo-Host/DeepKey,Rust,"DeepKey

Circle CI: 
Summary
This is a core hApp. The DeepKey is used for Key Management for hApps in Holochain.
Tests:
hc test
Documentation


DeepKey hApp specs


Built With

Holochain v0.0.13-alpha1

Authors

Joel Ulahanna - zo-el

License

Copyright (C) 2017, The MetaCurrency Project (Eric Harris-Braun, Arthur Brock, et. al.)
This program is free software: you can redistribute it and/or modify it under the terms of the license provided in the LICENSE file (GPLv3). This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
Note: We are considering other 'looser' licensing options (like MIT license) but at this stage are using GPL while we're getting the matter sorted out.
",4
gentoo/musl,Shell,"This is the overlay for Gentoo build with musl.  musl is a new C Standard Library
which aims to be lightweight, fast, simple, free, and strives to be correct in the
sense of standards-conformance and safety.
http://www.musl-libc.org/
Maintainers

Anthony G. Basile blueness@gentoo.org
Aric Belsito lluixhi@gmail.com
Felix Janda felix.janda@posteo.de

Repoman Status

",52
gentoo/musl,Shell,"This is the overlay for Gentoo build with musl.  musl is a new C Standard Library
which aims to be lightweight, fast, simple, free, and strives to be correct in the
sense of standards-conformance and safety.
http://www.musl-libc.org/
Maintainers

Anthony G. Basile blueness@gentoo.org
Aric Belsito lluixhi@gmail.com
Felix Janda felix.janda@posteo.de

Repoman Status

",52
simon300000/vtbs.moe,Vue,"vtbs.moe
你好呀→_→
欢迎来到 https://vtbs.moe 的 Github 项目主页
前后端包括数据库都在这个 repository
介绍
这是我自娱自乐做出来的 Bilibili 虚拟主播状态记录页面，目前只在NGA宣传过
现在 vtb.simon3k.moe 和 vtbs.moe 两个地址都能用，内容没有区别；推荐用 vtbs.moe
网站用到的部分开源软件:

前端架构: Vue.js

Vue CLI
Vuex


组件库: Element
图表: ECharts

v-charts


数据库: level
数据采集: Bili-api
前后端API通讯: socket.io
万能的: Node.js

开发
安装依赖:
npm install

前端
Compiles and hot-reloads for development
npm run serve

Compiles and minifies for production
npm run build

Customize configuration
See Configuration Reference.
后端API
node index


Socket 服务端口: 8001
Vtuber/Vup 列表 见 api/vtbs.js

其他


检查 vtbs.js 列表有没有重复的指令: npm run repeat


把数据库导出为 json 文件: node script/db2json


贡献
想要加什么大功能可以先发 issue 讨论讨论，其他的比如vtb列表补全，修BUG什么的可以直接 Pull request
有什么问题可以开 issue
聊天也可以开 issue →_→
",13
mstubinis/Serenity,C++,"Serenity
A game engine focused on producing stunning visual effects and supporting custom user made rendering and logic code.
Features


Physically based rendering using deferred rendering
Several post-processing effects (Depth of Field, HDR, SSAO, Bloom, Fog, God Rays
Entity-Component based game logic
Custom logic and rendering code via functors
Bullet Physics world
Multi-threading using boost::asio worker pool and functor based jobs
Resource loading (3d meshes, textures, sounds, fonts
Optimized render calls using Render Graphs


Installing & Building - Visual Studio
The project uses several library dependencies. The current solution file is designed to be used in Visual Studio 2017 with the libraries being built statically and not dynamically, on a Windows OS. The dependencies are:


Assimp
Bullet Physics
SFML
freeglut
GLEW
glm
Boost (filesystem, iostreams, systems)


Most of these libs are already included in the dependencies folder. The project will point to use them, but if you have them already you can point to your own. Boost is not included (for obvious reasons), you will have to have that installed yourself. For more information on how to build boost, visit The getting started guide
The solution contains 3 projects: the engine itself, which will be built into Serenity.lib, and 2 sample applications that will be built into .exe's.
In order for the exe's to run, the SerenityLib/Engine/data folder will have to be copied over to SerenityLib/Builds folder, which will be generated upon compiling the solution.
In the future, the engine will receive an overhaul with the intention of opening it up to client usage. Part of this endeavour will be a wiki section with API documentation as well as continued refactoring of the engine code for readability and understanding.
",3
azu/parse-github-event,TypeScript,"parse-github-event 
Small library to parse Event Types from Github API response.
Feature

Parse event json and built message and html_url without addtional request
Create human-readable message like GitHub's timeline from event json

Installation
npm install parse-github-event
Usage
Response json object of GitHub Events API.
{
    ""id"": ""2070416128"",
    ""type"": ""PullRequestEvent"",
    ""actor"": {
        ""id"": 1062518,
        ""login"": ""pivotal-brian-croom"",
        ""gravatar_id"": ""92d36bd6d9b53539fcec282452872710"",
        ""url"": ""https://api.github.com/users/pivotal-brian-croom"",
        ""avatar_url"": ""https://avatars.githubusercontent.com/u/1062518?""
    },
    ""repo"": {
        ""id"": 708684,
        ""name"": ""pivotal/cedar"",
        ""url"": ""https://api.github.com/repos/pivotal/cedar""
    },
    ""payload"": {
        ""action"": ""opened"",
        ""number"": 231,
        ""pull_request"": {
            ""url"": ""https://api.github.com/repos/pivotal/cedar/pulls/231"",
            ""body"": ""- Common code consolidated into CDROTestRunner and CDROTestIPhoneRunner\r\n- CDROTestIPhoneRunner subclasses CDROTestRunner\r\n[#67878220]\r\n\r\nThoughts?\r\n@idoru, @jeffh"",
            ""created_at"": ""2014-04-24T05:01:39Z"",
            ""updated_at"": ""2014-04-24T05:01:39Z"",
    ...
}
Parse response
var parseGithubEvent = require(""parse-github-event"");
// responseJSON is come from https://developer.github.com/v3/activity/events/
var parsed = parseGithubEvent.parse(responseJSON);
/*
{
    text: 'opened issue on %%repository%%',
    data: { repository: 'pivotal/cedar' },
    html_url : 'https://github.com/pivotal/cedar/pull/231'
}
*/
Create message
It's bonus method.
var parseGithubEvent = require(""parse-github-event"");
var result = parseGithubEvent.compile(json);
// pivotal-brian-croom opened issue on pivotal/cedar
UseCase
Create one-line message and html_url from event response.

azu/github-to-twitter-lambda: Lambda bot that fetch own GitHub notifications/events and post to Twitter.
azu/faao: Faao is a GitHub issue/pull-request client on Electron.
lawvs/buddy-github-events: View broadcast/received GItHub events from other people or organizations.

Contributing

Fork it!
Create your feature branch: git checkout -b my-new-feature
Commit your changes: git commit -am 'Add some feature'
Push to the branch: git push origin my-new-feature
Submit a pull request :D

License
MIT
Thanks for alindeman/github-timeline-widget.
Use these as a reference

https://github.com/FenrirUnbound/github-feed/tree/FixPath/test/mocks
https://github.com/limbo0312/gitBox/tree/master/ioctocat2/iOctocatUnitTests/Fixtures
https://github.com/octokit/go-octokit/tree/master/fixtures
https://github.com/chamerling/QuickHubApp
https://github.com/linyows/octospy/tree/master/spec/fixtures
https://github.com/octokit/octokit.objc/tree/master/OctoKitTests/Stubs
https://github.com/octokit/octokit.net/tree/master/Octokit.Tests/Fixtures

",5
maticzav/gimb-events,TypeScript,"Gimb Dogodki

Ticketing system for Gimnazija Bežigrad

Installation
npm install
npm run bootstrap
npm run dev
Required Environment variables

APP_SECRET
PRISMA_ENDPOINT
PRISMA_SECRET
SENDGRID_KEY

To see the page in action, go to http://localhost:3001
",3
VeNoMouS/cloudscraper,Python,"cloudscraper




A simple Python module to bypass Cloudflare's anti-bot page (also known as ""I'm Under Attack Mode"", or IUAM), implemented with Requests. Cloudflare changes their techniques periodically, so I will update this repo frequently.
This can be useful if you wish to scrape or crawl a website protected with Cloudflare. Cloudflare's anti-bot page currently just checks if the client supports Javascript, though they may add additional techniques in the future.
Due to Cloudflare continually changing and hardening their protection page, cloudscraper requires a JavaScript interpreter to solve Javascript challenges. This allows the script to easily impersonate a regular web browser without explicitly deobfuscating and parsing Cloudflare's Javascript.
Note: This only works when regular Cloudflare anti-bots is enabled (the ""Checking your browser before accessing..."" loading page). If there is a reCAPTCHA challenge, you're out of luck (At this stage... however we will be adding in Anti-CAPTCHA 3rd party support). Thankfully, the Javascript check page is much more common.
For reference, this is the default message Cloudflare uses for these sorts of pages:
Checking your browser before accessing website.com.

This process is automatic. Your browser will redirect to your requested content shortly.

Please allow up to 5 seconds...

Any script using cloudscraper will sleep for ~5 seconds for the first visit to any site with Cloudflare anti-bots enabled, though no delay will occur after the first request.
Installation
Simply run pip install cloudscraper. The PyPI package is at https://pypi.python.org/pypi/cloudscraper/
Alternatively, clone this repository and run python setup.py install.
Dependencies

Python 2.7 - 3.x
Requests >= 2.9.2
Brotli >= 1.0.7
requests_toolbelt >= 0.9.1

Have the ability to choose between Javascript Interpreters.

js2py >=0.60
Node.js

Your computer or server may already have it (check with node -v). If not, you can install it with apt-get install nodejs on Ubuntu. Debian requires nodejs-legacy. Otherwise, please read Node's installation instructions.



python setup.py install will install the Python dependencies automatically. Node is the only application you need to install yourself.
Updates
Cloudflare modifies their anti-bot protection page occasionally, So far it has changed maybe once per year on average.
If you notice that the anti-bot page has changed, or if this module suddenly stops working, please create a GitHub issue so that I can update the code accordingly.

Many issues are a result of users not updating to the latest release of this project. Before filing an issue, please run the following command:

pip show cloudscraper

If the value of the version field is not the latest release, please run the following to update your package:
pip install cloudscraper -U

If you are still encountering a problem, open an issue and please include:

The full exception and stack trace.
The URL of the Cloudflare-protected page which the script does not work on.
A Pastebin or Gist containing the HTML source of the protected page.
The version number from pip show cloudscraper.

Usage
The simplest way to use cloudscraper is by calling create_scraper().
import cloudscraper

scraper = cloudscraper.create_scraper()  # returns a CloudScraper instance
# Or: scraper = cloudscraper.CloudScraper()  # CloudScraper inherits from requests.Session
print scraper.get(""http://somesite.com"").content  # => ""<!DOCTYPE html><html><head>...""
That's it...
Any requests made from this session object to websites protected by Cloudflare anti-bot will be handled automatically. Websites not using Cloudflare will be treated normally. You don't need to configure or call anything further, and you can effectively treat all websites as if they're not protected with anything.
You use cloudscraper exactly the same way you use Requests. CloudScraper works identically to a Requests Session object, just instead of calling requests.get() or requests.post(), you call scraper.get() or scraper.post().
Consult Requests' documentation for more information.
Options
Existing session
If you already have an existing Requests session, you can pass it to create_scraper() to continue using that session.
session = requests.session()
scraper = cloudscraper.create_scraper(sess=session)
Unfortunately, not all of Requests' session attributes are easily transferable, so if you run into problems with this, you should replace your initial sess = requests.session() call with sess = cloudscraper.create_scraper().

Debug
scraper = cloudscraper.create_scraper(debug=True)
Or
scraper = cloudscraper.create_scraper()
scraper.debug = True

Delays
Normally, when a browser is faced with a Cloudflare IUAM challenge page, Cloudflare requires the browser to wait ~5 seconds before submitting the challenge answer. If a website is under heavy load, sometimes this may fail. One solution is to increase the delay (perhaps to 10 or 15 seconds, depending on the website). If you would like to override this delay, pass the delay keyword argument to create_scraper() or CloudScraper().
There is no need to override this delay unless cloudscraper generates an error recommending you increase the delay.
scraper = cloudscraper.create_scraper(delay=10)
or
scraper = cloudscraper.create_scraper()
scraper.delay = 10

JavaScript Interpreters
Cloudscraper currently supports two JavaScript Interpreters

js2py
Node.js

The default interpreter is set to js2py,  you can set which to use by defining the interpreter parameter.
scraper = cloudscraper.create_scraper(interpreter='nodejs')
or
scraper = cloudscraper.create_scraper()
scraper.interpreter = 'nodejs'

Brotli Support
We have added in Brotli decompression support in, and it is enabled by default, the only way to disable it, is by passing the allow_brotli parameter set toFalse to create_scraper()
scraper = cloudscraper.create_scraper(allow_brotli=False)
Integration
It's easy to integrate cloudscraper with other applications and tools. Cloudflare uses two cookies as tokens: one to verify you made it past their challenge page and one to track your session. To bypass the challenge page, simply include both of these cookies (with the appropriate user-agent) in all HTTP requests you make.
To retrieve just the cookies (as a dictionary), use cloudscraper.get_tokens(). To retrieve them as a full Cookie HTTP header, use cloudscraper.get_cookie_string().
get_tokens and get_cookie_string both accept Requests' usual keyword arguments (like get_tokens(url, proxies={""http"": ""socks5://localhost:9050""})).
Please read Requests' documentation on request arguments for more information.

User-Agent Handling
The two integration functions return a tuple of (cookie, user_agent_string).
You must use the same user-agent string for obtaining tokens and for making requests with those tokens, otherwise Cloudflare will flag you as a bot.
That means you have to pass the returned user_agent_string to whatever script, tool, or service you are passing the tokens to (e.g. curl, or a specialized scraping tool), and it must use that passed user-agent when it makes HTTP requests.

Integration examples
Remember, you must always use the same user-agent when retrieving or using these cookies. These functions all return a tuple of (cookie_dict, user_agent_string).

Retrieving a cookie dict through a proxy
get_tokens is a convenience function for returning a Python dict containing Cloudflare's session cookies. For demonstration, we will configure this request to use a proxy. (Please note that if you request Cloudflare clearance tokens through a proxy, you must always use the same proxy when those tokens are passed to the server. Cloudflare requires that the challenge-solving IP and the visitor IP stay the same.)
If you do not wish to use a proxy, just don't pass the proxies keyword argument. These convenience functions support all of Requests' normal keyword arguments, like params, data, and headers.
import cloudscraper

proxies = {""http"": ""http://localhost:8080"", ""https"": ""http://localhost:8080""}
tokens, user_agent = cloudscraper.get_tokens(""http://somesite.com"", proxies=proxies)
print tokens
# => {
         'cf_clearance': 'c8f913c707b818b47aa328d81cab57c349b1eee5-1426733163-3600',
         '__cfduid': 'dd8ec03dfdbcb8c2ea63e920f1335c1001426733158'
     }

Retrieving a cookie string
get_cookie_string is a convenience function for returning the tokens as a string for use as a Cookie HTTP header value.
This is useful when crafting an HTTP request manually, or working with an external application or library that passes on raw cookie headers.
import cloudscraper

cookie_value, user_agent = cloudscraper.get_cookie_string('http://somesite.com')

print 'GET / HTTP/1.1\r\nCookie: {}\r\nUser-Agent: {}\r\n'.format(cookie_value, user_agent)

# GET / HTTP/1.1\r\n
# Cookie: cf_clearance=c8f913c707b818b47aa328d81cab57c349b1eee5-1426733163-3600; __cfduid=dd8ec03dfdbcb8c2ea63e920f1335c1001426733158
# User-Agent: Some/User-Agent String

curl example
Here is an example of integrating cloudscraper with curl. As you can see, all you have to do is pass the cookies and user-agent to curl.
import subprocess
import cloudscraper

# With get_tokens() cookie dict:

# tokens, user_agent = cloudscraper.get_tokens(""http://somesite.com"")
# cookie_arg = 'cf_clearance={}; __cfduid={}'.format(tokens['cf_clearance'], tokens['__cfduid'])

# With get_cookie_string() cookie header; recommended for curl and similar external applications:

cookie_arg, user_agent = cloudscraper.get_cookie_string('http://somesite.com')

# With a custom user-agent string you can optionally provide:

# ua = ""Scraping Bot""
# cookie_arg, user_agent = cloudscraper.get_cookie_string(""http://somesite.com"", user_agent=ua)

result = subprocess.check_output(
    [
        'curl',
        '--cookie',
        cookie_arg,
        '-A',
        user_agent,
        'http://somesite.com'
    ]
)
Trimmed down version. Prints page contents of any site protected with Cloudflare, via curl.
Warning: shell=True can be dangerous to use with subprocess in real code.
url = ""http://somesite.com""
cookie_arg, user_agent = cloudscraper.get_cookie_string(url)
cmd = ""curl --cookie {cookie_arg} -A {user_agent} {url}""
print(
    subprocess.check_output(
        cmd.format(
            cookie_arg=cookie_arg,
            user_agent=user_agent,
            url=url
        ), 
        shell=True
    )
)
",53
rustlang-cn/rustlang-cn,None,"Rustlang-cn  
一、参与 Rust 中文阅读投稿

Rust 中文致力于 Rust 编程语言中文网络建设，期待你的参与向 Rust 中文阅读投稿.

说明：文章投稿可以直接PR本仓库，也可以投递文章链接（请选用易使用的格式，如：Markdown）。本专栏默认 MIT 协议，如你的文章有其他要求可注明。
二、参与 Rust 中文网站建设

因为本仓库的修改会自动发布到 rustlang-cn 网站，请参与时遵循以下步骤，并确保构建为成功状态。

A. 参与文档
如果你只想修改文件，不用操作下面添加文件的步骤，你可以修改 docs 目录内的任何 .md 文件。
B. 参与网站
如果你想添加更多文件或改变主题结构布局，请遵循以下步骤。


Fork 并克隆本仓库：
$ git clone https://github.com/<YOUR_GITHUB_ID>/rustlang-cn
$ cd rustlang-cn
$ npm install


测试，可以在终端查看测试输出：
$ npm run dev
打开浏览器 http://localhost:8080 查看页面效果。


修改/添加 docs 目录内的文件, 保证步骤二测试运行没有错误。


Push 到你的 GitHub 仓库，然后提交 PR 到本仓库。


",130
bergerhealer/Mountiplex,Java,"Mountiplex
Dev Builds
General Purpose Java Reflection Library
Mountiplex delivers a two-hit solution for accessing the internals of a hidden implementation in Java.
A type conversion engine allows dynamic type changing, which can be used to translate between hidden and API
types. A reflection template engine allows a way to declare the internal structure of the implementation.
An example template declaration that shows most of what is possible:
package some.secret.project;

import my.api.wrapper.Secret;

class SecretService {
#if version >= 1.1
    private int secretCount;
#else
    private int secretCount:nSecrets;
#endif

    public (Secret) TSecret getSecret(String name);
    public String getStatus();

    public String getSecretName(String name) {
        return instance.getSecret(name).getName();
    }

    <code>
    public void clearSecretCount() {
        setSecretCount(0);
    }
    </code>
}
At runtime this same declaration file will be parsed, preprocessing macros are executed and then the whole thing is loaded into a compiletime generated abstract model of the SecretService called SecretServiceHandle. Using this handle it is possible to interact with the implementation safely, using the name/type translations as declared in the template file.
The handle implementation is fully generated at runtime, avoiding reflection where possible to allow for maximum performance code execution.
Mountiplex is primarily used by BKCommonLib to interact with the Minecraft Server internals.
This product includes software developed by the Apache Software Foundation (http://www.apache.org/)
",2
jeffrey-hokanson/PSDR,Jupyter Notebook,"PSDR: Parameter Space Dimension Reduction Toolbox



Author: Jeffrey M. Hokanson, Postdoctoral Fellow at the University of Colorado Boulder (jeffrey@hokanson.us)
Introduction
Given a function mapping some subset of an m-dimensional space to a scalar value



parameter space dimension reduction seeks to identify a low-dimensional manifold
of the input along which this function varies the most.
Frequently we will choose to use a linear manifold
and consequently identify linear combinations of input variables along
which the function varies the most.
We emphasize that this library is for parameter space dimension reduction
as the term 'dimension reduction' often appears in other contexts.
For example, model reduction is often referred to as dimension reduction
because it reduces the state-space dimension of a set of differential equations,
yielding a smaller set of differential equations.
Simple example
One basic use of the library is to identify an active subspace using
the outer product of gradients:
import psdr, psdr.demos
fun = psdr.demos.Borehole()    # load a test problem
X = fun.domain.sample(1000)    # sample points from the domain with uniform probabilty
grads = fun.grad(X)            # evaluate the gradient at the points in X
act = psdr.ActiveSubspace()    # initialize a class to find the Active Subspace
act.fit(grads)                 # estimate the active subspace using these Monte-Carlo samples
print(act.U[:,0])              # print the most important linear combination of variables

>>> array([ 9.19118904e-01, -2.26566967e-03,  2.90116247e-06,  2.17665629e-01,
        2.78485430e-03, -2.17665629e-01, -2.21695479e-01,  1.06310937e-01])
We can then create a shadow plot showing the projection of the input to this function
onto a one-dimensional subspace spanned by the important linear combination identified above
import matplotlib.pyplot as plt
fX = fun(X)                    # evaluate the function at the points X
act.shadow_plot(X, fX)         # generate the shadow plot
plt.show()                     # draw the results



We say this function is has low-dimensional structure since the output of the function
is well described by the value of this one linear combination of its input parameters.
Documentation
For further documentation, please see our page on Read the Docs:
Documentation.
Contributing
I welcome contributions to this library,
particularly of test functions similar to those in psdr.demos.
Please submit a pull request along with unit tests for the proposed code.
If you are submitting a complex test function that requires calling code outside of Python,
please submit a Docker image along with a docker file generating that image
(see the OpenAeroStruct demo function for an example of how to do this).
",3
TheIllusiveC4/Curios,Java,"Curios API   
Overview
Curios is a flexible and expandable accessory/equipment API for users and developers. The purpose is to provide functionality for developers to add extra accessory/equipment slots in a convenient and compatible manner, as well as to give users the ability to configure these slots to their preferences. By default, Curios does not add any content except for an inventory GUI. There are no slots and only two items, the latter only being available through the Creative menu and primarily serving as examples for developers to use when coding their own integration.
Features

Expandable equipment slots through a central library. New equipment slots can be made and managed easily through an identifier registry. Identical identifiers will be merged together to avoid functional redundancies and provide maximum compatibility to potential items, while unique identifiers can still be used to mark special types when appropriate.
Slots are only made on-demand. There are no slots included by default, all slots are created only as needed. This reduces instances where one or more superfluous slots are present without any suitable items to go into the slot.
Slots are completely customizable and manipulable. Slots can have custom backgrounds, different sizes, and can even be disabled or hidden by default. But how would a player even access disabled slots? Through the API, developers can access functions to enable/disable a player's slots or add/remove a certain number of slots of a given type.
Flexible item->curio relations using the vanilla tag system. Potential curios are selected through the vanilla tag system, which means that categorizing items into curio types is as easy as creating a .json file in the data/curios/tags folder. Items can be categorized into as many curio types as you want as long as they're tagged in the appropriate files, and these settings can even be overridden entirely. For more information, see the vanilla tag system.
Complete integration with other inventory mechanics. Mending and Curses will work with all applicable items equipped in the curio slots. There are also various minor features for developers that make it simpler to integrate their current items or mechanics into the curio system.
Accessible from a single GUI. Curios comes with its own GUI accessible from the inventory that shows all of the available slots to a player. This allows players to see all of the extended equipment slots in a central location without needing to access different inventory GUIs. However, developers can still provide their own GUIs for their mod-specific slots if they want. The default keybinding for the GUI is 'g'.

Documentation

How to Use: Users
How to Use: Developers
Commands

Adding to Your Project:
Add the following to your build.gradle file:
repositories {
    maven {
        url = ""https://maven.theillusivec4.top/""
    }
}

dependencies {
    compile fg.deobf(""top.theillusivec4.curios:curios:${version}"")
}

Replace ${version} with the version of Curios that you want to use.
",3
JordanMartinez/learn-halogen,PureScript,"Learn Halogen

Learn purescript-halogen, (v5.0.0-rc.4) from a bottom-up approach
Requirements
Before learning Halogen via this project, you will need to install the following. (If you don't have them already installed, see my purescript learning repo's Install Guide

purescript (v0.12.5)
spago (v0.7.5.0)
parcel (v1.12.3)
dhall-to-json

Instructions

Git clone this project
Run spago build
Run spago docs (and refer to the docs via the ./generated-docs/index.html file)
Read through each folder using the same rules that I use in my learning repo (described in the third bullet point here

License
This project is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license: (Human-readable version), (Actual License)

",20
bqi343/USACO,C++,"README
Introduction
In competitive programming contests, one must solve well-defined problems by writing computer programs under specified constraints (Wikipedia). Typically, the most popular language is C++, followed by Java and Python.
Notes
I am currently not updating the following pages:

README's for most implementations
tables and solutions for non-platinum USACO contests

If you would like to contribute, provide feedback, or encourage me to update something, please email me at bqi343@gmail.com or submit a pull request. If you have a question, it may be answered by another file in this folder.
Getting Started
I recommend that you use C++, even if you already know some other language such as Java or Python (see notes). A solid foundation in math (ex. AIME qualification) can help greatly.

C++ Tutorial.
CodeFights

good way to practice syntax


Schedule for Beginners
E869120 Tutorial

Assuming you know how C++ input/output works, you should soon be able to advance from USACO Bronze. After that, you can start doing past USACO Silver problems as well as the USACO training pages. Also, participate in other contests such as AtCoder Beginner.
How do you train for USACO Platinum?

Do as many (high-quality) contests as possible.

Make sure to upsolve after the contest ends!


Work through past problems. See Contests -> README:Problems.
Use the tags to learn new topics as necessary.

see implementation / USACO categories
A2OJ
Problem Topics (Morass)



",289
ZDfordream/FlutterTianYue,Dart,"Language: English | 中文简体
Flutter开发一款跨平台的开源漫画App (放慢脚步，给个star)
仿腾讯动漫，抖音视频播放效果，小说阅读，我猜想，你所需要的效果，此项目大致都有，项目持续更新中...
screen shot






Setup

Clone the repo

$ git clone https://github.com/ZDfordream/FlutterTianYue.git
$ cd FlutterTianYue
$ flutter packages get


Running:

$ flutter run
$ 提示：如果出现build失败，再执行一次

主要实现的功能有：

动漫：仿腾讯动漫m站，嵌套滑动
加载状态视图
flutter 与 native 双向通信
启屏页-轮播图
下拉刷新-上拉加载更多
AndroidView嵌入原生视图
仿抖音视频播放
其他flutter常见效果，此项目都有

第三方框架



库
功能




dio
网络框架


shared_preferences
本地数据缓存


fluttertoast
toast


device_info
设备信息


iconfont
字库图标


share
系统分享


flutter_webview_plugin
全屏的webview


video_player
视频播放



如果您觉得还可以的话，给个Star白~
默认条约
此项目仅供大家交流沟通使用，不得用于任何商业以及利益活动。
LICENSE
licensed under the Apache License 2.0

A permissive license whose main conditions require preservation of copyright and license notices.
Contributors provide an express grant of patent rights.
Licensed works, modifications, and larger works may be distributed under different terms and without source code.

",30
hzuapps/web-wechat-2019,CSS,"高级网页设计（微信小程序开发讲解）
实验要求
https://github.com/hzuapps/web-wechat-2019/issues?q=is%3Aissue+is%3Aopen+label%3ALab
在线查看实验结果
https://infoaas.com/web-wechat-2019/
微信小程序开发文档
https://mp.weixin.qq.com/debug/wxadoc/dev/index.html?t=2017830
WeUI-wxss库
https://github.com/Tencent/weui-wxss
微信小程序注册网站
https://mp.weixin.qq.com
课程网站
http://zeng.shaoning.net/web/
通过审核并发布的小程序列表
https://github.com/hzuapps/web-wechat-2017/list.md
https://github.com/hzuapps/html5-2018/labels/Example
联系老师

",4
aws-quickstart/quickstart-microsoft-wapadfs,PowerShell,"quickstart-microsoft-wapadfs
Web Application Proxy on the AWS Cloud
This Quick Start deploys Web Application Proxy and Active Directory Federation Services (AD FS) on the AWS Cloud.
AD FS is a Windows Server role that authenticates users and provides security tokens to applications or federated partner applications that trust AD FS.
The Web Application Proxy role on Windows Server makes AD FS accessible to external users by proxying requests without requiring VPN connectivity. You can also use Web Application Proxy to selectively publish and pre-authenticate connections to internal web applications, allowing users outside your organization to access those applications over the Internet.
The Quick Start offers two deployment options:

Deploying Web Application Proxy and AD FS into a new virtual private cloud (VPC) on AWS
Deploying Web Application Proxy and AD FS into an existing VPC on AWS

You can also use the AWS CloudFormation templates as a starting point for your own implementation.

For architectural details, best practices, step-by-step instructions, and customization options, see the
deployment guide.
To post feedback, submit feature ideas, or report bugs, use the Issues section of this GitHub repo.
If you'd like to submit code for this Quick Start, please review the AWS Quick Start Contributor's Kit.
",5
anhad13/F1testing,Python,"F1testing
testing F1 from ON_LSTM on different datasets
",2
Auralcat/my-dotfiles,Emacs Lisp,"My dotfiles.
Influences:

https://github.com/shiroyasha/dotfiles
https://github.com/thoughtbot/dotfiles

Why share your dotfiles
",3
gocn/news,None,"news
GoCN 每日新闻
",317
chan-sccp/chan-sccp,C,"Welcome to Chan_SCCP







.
Chan_SCCP is free software. Please see the file COPYING for details.
For documentation, please see the files in the doc subdirectory.
For building and installation instructions please see the INSTALL file.
Prerequisites
Make sure you have the following installed on your system:

c-compiler:

gcc >= 4.4  (note: older not supported, higher advised)
clang >= 3.6  (note: older not supported, higher advised)


gnu make
libraries:

libxml2-dev
libxslt1-dev
gettext


pbx:

asterisk >= 1.6.2 (absolute minimum)
asterisk >= 11.21 or asterisk >= 13.7 recommended
including source headers and debug symbols (asterisk-dev and asterisk-dbg / asterisk-devel and asterisk-debug-info)
chan_skinny module is prevented from loading in /etc/asterisk/modules.conf


standard posix compatible applications like sed, awk, tr

Building from source
Using git (recommended)
Clone github repository (once)
git clone https://github.com/chan-sccp/chan-sccp.git chan-sccp
cd chan-sccp

Update to latest state
cd chan-sccp
git fetch
git pull

Using Released tar.gz
retrieve the tar.gz from latest release and save it to /tmp/chan-sccp_latest.tar.gz
mkdir chan-sccp
cd chan-sccp
tar xvfz /tmp/chan-sccp_latest.tar.gz

Configuring
./configure [....configure flags you prefer...]

Note: For more information about the possible configure flags, check:
./configure --help
Note: When you are making changes to configure.ac, autoconf / or Makefile.am files you should run:
./tools/bootstrap.sh
Build and Install
make -j2 && make install && make reload

Required Asterisk Modules
Make sure you have the following asterisk modules loaded before loading the chan_sccp
module:

app_voicemail
bridge_simple
bridge_native_rtp
bridge_softmix
bridge_holding
res_stasis
res_stasis_device_state

Binaries
We also provide prebuild binaries for:

Ubuntu Lauchpad (PPA)
Asterisk-11

Debian-8.0


Asterisk-13

Debian-9.0
Fedora-23
Fedora-24
Fedora-25
Fedora-26
Ubuntu-16.04
Ubuntu-16.10
Ubuntu-17.04


Asterisk-16

OpenSuSE-Leap_15.0
OpenSuSE-Leap_42.2
OpenSuSE-Leap_42.3
OpenSuSE-Factory
OpenSuSE-Factory_ARM



Wiki
You can find more information and documentation on our 
Mailinglist


chan-sccp Discussion:

Subscribe
Archive
Search



chan-sccp Releases:

Subscribe
Archive
Search



Chat

Donate
If you like our project, then please consider to

License

",76
CraftingAsAService/FFXIVCrafting,PHP,"FFXIV Crafting
Crafting As A Service
An online tool to help crafters in Final Fantasy XIV: A Realm Reborn.
Routine Updates
Image for Footer: http://na.finalfantasyxiv.com/lodestone/special/patchnote_log/
",30
webprofusion/OpenAudio,None,"Open Source Audio Plugins & Apps
A list of open source VST (and other format) plugin/app projects. The intention of this list is to catalog open source plugins or apps which are fully featured or are useful examples which have non-trivial features.
The main benefit of having Open Source plugins/apps is that the code itself is preserved for the future, so when the author(s) stop updating it the community can continue using and developing the software. Open Source projects are also a great way to learn how different audio FX/instruments are created.
https://openaudio.webprofusion.com
Please contribute links!
Audio Plugins



Plugin
Description
Type
Framework




airwindows
Various small and experimental effect plugins
Effect



Argotlunar
Real-time delay-line granulator
Effect
JUCE


Cocoa Delay
Warm and lively delay
Effect
wdl-ol


Dexed
DX7 FM plugin synth
Instrument
JUCE


Digits
Phase-distortion synth inspired by Casio CZ series
Instrument
VSTGUI


Dragonfly Reverb
Hall-style reverb based on freeverb3 algorithms
Effect
DPF


Eurorack
Diverse set of physical modeling sources, organic processors, wavetable oscillators, waveshapers, granular synthesizers, and utility modules
Misc



Flutterbird
Simple pitch fluctuation
Effect
iPlug 2


Helm
Polyphonic synth with lots of modulation
Instrument
JUCE


keithhearne/VSTPlugins
A collection of VST plugins
Effect
JUCE


LameVST
LameMP3 as an effect
Effect



mda
FX and virtual instruments for PC and Mac
Misc



Mika Micro
Simple subtractive synth
Instrument
wdl-ol


OwlBass
Additive bass synth
Instrument
JUCE


regrader
Degenerative delay
Effect
VSTGUI


Roth-AIR
Mixing tool for easily adding airy, crispy presence to audio
Effect
JUCE


ScorchCrafter Guitar FX
Audio DSP FX and plug-ins, mostly for guitar (amp sim) and other FX
Effect



Surge
Subtractive wavetable synth
Instrument
VSTGUI


Synister
Subtractive software synth
Instrument
JUCE


VCVRack
Virtual modular synthesizer
Misc
rtaudio



Collections

VCRack Library - a library of plugins compatible with VCV Plugin Manager

Open Source Audio Apps



Software
Source
Description




Ardour
Ardour/ardour
DAW


ASIO2WASAPI
levmin/ASIO2WASAPI
Universal ASIO driver for Windows


Audacity
audacity/audacity
Audio editor


FlexASIO
dechamps/FlexASIO
Universal ASIO driver for Windows


Guitarix
SourceForge → guitarix
GNU/Linux Virtual Amplifier


Helio Workstation
helio-fm/helio-workstation
Sequencer


OwlPlug
DropSnorz/OwlPlug
Audio plugin manager


VCV Rack
VCVRack/Rack
Modular synthesizer



Open Source Software Development Libraries



Name
Source




Cabbage
https://github.com/rorywalsh/cabbage


Csound
https://github.com/csound/csound


Faust
https://github.com/grame-cncm/faust


iPlug2
https://github.com/iplug2/iplug2


JUCE
https://github.com/WeAreROLI/JUCE


rtaudio
https://github.com/thestk/rtaudio


PortAudio
https://app.assembla.com/spaces/portaudio/git/source


wdl-ol
https://github.com/olilarkin/wdl-ol



Code Samples

KlangFalter — a convolution audio plugin (e.g. for usage as convolution reverb)
FFTConvolver — an audio convolution algorithm in C++ for real time audio processing

Open Data Resources
OpenAIR — the Open Acoustic Impulse Response Library (Convolution Reverb Impulse Responses to recreate reverb character of space and equipment/recordings)
",89
dry-python/stories,Python,"
 
  
 
 



The business transaction DSL

Source Code
Issue Tracker
Documentation
Discussion


Installation
All released versions are hosted on the Python Package Index.  You can
install this package with following command.
pip install stories

Usage
stories provide a simple way to define a complex business scenario
that include many processing steps.
from stories import story, arguments, Success, Failure, Result

class Subscribe:

    @story
    @arguments('category_id', 'user_id')
    def buy(I):

        I.find_category
        I.find_profile
        I.check_balance
        I.persist_subscription
        I.show_subscription

    def find_category(self, ctx):

        category = Category.objects.get(id=ctx.category_id)
        return Success(category=category)

    def find_profile(self, ctx):

        profile = Profile.objects.get(user_id=ctx.user_id)
        return Success(profile=profile)

    def check_balance(self, ctx):

        if ctx.category.cost < ctx.profile.balance:
            return Success()
        else:
            return Failure()

    def persist_subscription(self, ctx):

        subscription = Subscription(ctx.category, ctx.profile)
        subscription.save()
        return Success(subscription=subscription)

    def show_subscription(self, ctx):

        return Result(ctx.subscription)
>>> Subscribe().buy(category_id=1, user_id=1)
<Subscription object>
>>> _
This code style allow you clearly separate actual business scenario
from implementation details.

License
Stories library is offered under the two clause BSD license.
",64
DataBiosphere/leonardo,Scala," 
Leonardo
Leo provisions Spark clusters through Google Dataproc and installs Jupyter notebooks and Hail on them. It can also proxy end-user connections to the Jupyter interface in order to provide authorization for particular users.
For more information and an overview, see the wiki.
Swagger API documentation: https://notebooks.firecloud.org/
Project status
This project is under active development. It is not yet ready for independent production deployment. See the roadmap section of the wiki for details.
Configurability
Documentation on how to configure Leo is Coming Soon™. Until then, a brief overview: there are two points at which Leonardo is pluggable.
Authorization provider
Leo provides two modes of authorization out of the box:

By whitelist
Through Sam, the Workbench IAM service

Users wanting to roll their own authorization mechanism can do so by subclassing LeoAuthProvider and setting up the Leo configuration file appropriately.
Service account provider
There are (up to) three service accounts used in the process of spinning up a notebook cluster:

The Leo service account itself, used to make the call to Google Dataproc
The service account passed to dataproc clusters create via the --service-account parameter, whose credentials will be used to set up the instance and localized into the GCE metadata server
The service account that will be localized into the user environment and returned when any application asks for application default credentials.

Currently, Leo uses its own SA for #1, and the same per-user project-specific SA for #2 and #3, which it fetches from Sam. Users wanting to roll their own service account provision mechanism by subclassing ServiceAccountProvider and setting up the Leo configuration file appropriately.
Building and running Leonardo
Clone the repo.
$ git clone https://github.com/DataBiosphere/leonardo.git 
$ cd leonardo

Run Leonardo unit tests
Ensure docker is running. Spin up MySQL locally:
$ ./docker/run-mysql.sh start leonardo  

Build Leonardo and run tests.
export SBT_OPTS=""-Xmx2G -Xms1G -Dmysql.host=localhost -Dmysql.port=3311""
sbt clean compile test

Once you're done, tear down MySQL.
./docker/run-mysql.sh stop leonardo

Building Leonardo docker image
To install git-secrets
brew install git-secrets

To ensure git hooks are run
cp -r hooks/ .git/hooks/
chmod 755 .git/hooks/apply-git-secrets.sh

To build jar, leonardo docker image, and leonardo-notebooks docker image
./docker/build.sh jar -d build

To build jar, leonardo docker image, and leonardo-notebooks docker image
and push to repos broadinstitute/leonardo and broadinstitute/leonardo-notebooks
tagged with git hash
./docker/build.sh jar -d push

To build the leonardo-notebooks docker image with a given tag
bash ./jupyter-docker/build.sh build <TAG NAME>

To push the leonardo-notebooks docker image you built
to repo broadinstitute/leonardo-notebooks
bash ./jupyter-docker/build.sh push <TAG NAME>

",16
JoepVanlier/JSFX,Lua,"JSFX
This is a small bundle of JSFX and scripts for reaper.
You can install
these by adding the link:
https://raw.githubusercontent.com/JoepVanlier/JSFX/master/index.xml
to your reapack (https://reapack.com/) list of repositories. If you run
into issues with these, feel free to open an issue here on github.
Tight Compressor

This peak compressor is based on a paper by Giannoulis et al, ""Digital Dynamic Range Compressor Design—A Tutorial and Analysis"", Journal of the Audio Engineering Society 60(6). It seems to be a pretty decent at tight style compression, with pretty aggressive attack. The compression is continuously visualized to help you dial in the appropriate settings.
Stereo Bub II

A fairly basic stereo widening tool. Widens the sound, but makes sure that the mono-mix stays unaffected (unlike Haas). The crossover is basically a 12 pole HPF that cuts the bass of the widening to avoid widening the bass too much. The last slider allows you to mix in the original side channel (which can optionally also be run through the 12-pole highpass).
There are two basic modes of operation:

You can either add stereo sound from nothing, using the Strength slider. This adds a comb filtered version of the average signal with opposite polarity to the different channels. Be careful not to overdo it, or you get a flangey sound (unless that is what you want).
You can manipulate the existing side channel that's in the input. The gain of the original side channel is scaled by the old ""Old side"" knob. Depending on the button ""HP original side"" this signal route will be highpassed (mono-izing the low frequencies).

Filther, a waveshaping filter / distortion plugin with dynamic processing.

Filther is a waveshaping / filterbank plugin that allows for some dynamic processing as well.
You can find a full manual for Filther here: https://joepvanlier.github.io/FiltherManual/
What does it sound like?
All the distortion/filtering on that track was done with this filter (mostly nonlin Kr0g and Rezzy):
https://soundcloud.com/saike/ohnoesitsaboss2/s-zYCOt
It can also sound pretty destructive:
https://soundcloud.com/saike/sine/s-mbHJL
https://soundcloud.com/saike/fm-modes-filther/s-KXwEQ
The more experimental filters (such as ""Experimental"" and ""Phase Mangler"") can be used on pads to make eerie soundscapes: https://soundcloud.com/saike/filter-ambience/s-UxdLO
Here's a short tutorial on how to use it: https://www.youtube.com/watch?v=jtc8kp57xpI
For more information, or to contact the author, see the forum thread here: https://forum.cockos.com/showthread.php?t=213269
Waveshaping
Filther supports saturating soft clipping as well as drawing custom voltage curves using a spline. For the simpler filters, the distortion is simply applied before the filtering stage, but for some the filter is located in the filter scheme. In these cases, the distortion is either applied on the delayed or during solving the implicit equations for the supplied zero delay feedback filters (ZDF).
Filters
Filther contains two filter modules which can be automated by dynamics and LFO. The routing of the A and B filter can be altered (serial, parallel modes, plus control over the number of times the waveshaper is applied),
Filther contains a large variety of filters, each with their own advantages and drawbacks. Most of the filters behave non-ideal and are intended for creative purposes rather than fidelity to specification. Note that not all filters are stable for all combinations of resonance and waveshaping. Using very sharp transitions in the spline waveshaper can result in filter instability for the filters where waveshaping is part of the filter. Filther contains a large array of filters listed below:

Feedback section
There is an additional feedback section, which can be activated.  Feedback can be used to fatten up filters and in some cases regain control of the resonance. If you want some fatness/resonance fighting, keep the delay firmly placed at zero. The feedback delay chain has the exact opposite polarity of the resonance in most chains, so in this mode, it will fight with the resonance to sort of choke in on itself (see diode ladder or ms-20 for this effect). This can make the resonance less ringey, more chunky and a lot more pleasant to listen to. Note that the global feedback is not ZDF. Also note that using feedback, reduces the maximum number of spline nodes by two.
For phasey effects, use feedback with larger delays. Note however that then you're in the danger zone, because once resonance starts boosting resonance, things get real dicey. I would always recommend playing with this only if you have AGC on.
Automatic Gain Control
When tweaking, enable Automatic Gain Control to protect your ears from resonance issues. This rescales the volume so that the RMS value post filter is the same as the input level (meaning that you can leave the post fader at 0 dB). You can transfer the estimated gain to the post-gain fader with the outer mouse once you've honed in on a preset you like.
Dynamics
Filther also supports dynamically modifying the filter and waveshaping settings, by checking ""Filter"" and/or ""Shaping"" in the Dynamics section. Dynamics can be monitored in the dynamics window. Here you will see the input RMS (red curve), output RMS (blue curve), dynamic variable and threshold (click and drag to zoom). The dynamic variable (yellow curve) will start accumulating when the input RMS is above the threshold. The threshold can be dragged with the mouse or set in the dynamics panel. Averaging can be increased by modifying the RMS time. This will smoothen out the RMS values that you see (and the dynamics will respond accordingly). Alternatively, dynamics can be triggered by MIDI note events.
Waveshaping Dynamics
For waveshaping, Filther will interpolate between the non-waveshaped and waveshaped voltage response (1 being the fully waveshaped version).
Filter Dynamics
The extent of modulation on the filter can be set with the outer mouse button. This will showed a greyed area that will show the extent of the dynamics being applied. When the dynamics are at maximum, the parameter value will be at the full extent of this greyed area.
Tone Stacks

Based on the work of jatalahd and ~arph from diystompboxes.com forum.
See their plugin here: http://www.guitarscience.net/tsc/info.htm
I've made some bi-linearly transformed versions of these filters which emulate classic tone stacks.
Multi-channel spectral analyser with sonogram and time window
I needed a plugin that I could keep open on one screen to monitor things.
Hence I modified the stock Reaper spectral analyzer to allow for
multi-channel analysis and combine it with a sonogram and time window.

The JSFX comes with a lua script which sets up the routing appropriately
on a new FX track.
White/Black
Chooses background color.
Smoothing
Chooses size of spectral smoothing. Spectral smoothing is performed in
the frequency domain,
using larger smoothing for higher values. Note
that this is not an unbiased smoother.
More smoothing means that peaks
get wider and the spectrum becomes less accurate.  The noise
is also
suppressed however, which makes it easier to read when there are multiple
spectra.
Color map
Specifies colormap for spectral analyzer.
Scale
Scale indicators the zoom factor on the spectrum analyzer.
Integrate
Integrate spectrum over time. This makes the spectrum less noisy, but
less sensitive to short transients. Smoothness is a tradeoff between
smoothing (width), integration time (transients)
and noise (no smoothing
or integration time).
Floor
Specify where to put the noise floor.
Window
Window function. Defaults to Blackman-Harris for its resolution.
FFT
FFT window size. 8192 is pretty good. Higher is heavier on the CPU.
Log(Sonogram)
Enabling this shows the sonogram with a logarithmic frequency axis.
Disabling it means linear.
Sonogram/Time toggle
Determine whether you want to see the waveform or the sonogram.
Waveform is good for studying
transients. Sonogram is good for
studying frequencies over time.
Channel buttons
The next buttons indicate what channels are visualized. Enabling
or disabling them can be done
by clicking them with the left
mouse button. Clicking them with the right mouse button will make
them the active channel in the sonogram or time window. This way,
you can study the sonograms of  the channels separately.
Sum
Indicates the sum of the signal. This will show the left and right
channel in black and grey in the main graph. Enabling or disabling
can be done by left clicking. Clicking this with the outer mouse
button will show the signal in the sonogram or time window.
Ch1 - Ch16
The channels that are routed to the spectral analyser. Enabling or
disabling can be done by left
clicking. Clicking this with the outer
mouse button will show the signal in the sonogram or time
window.
Sonogram mode
Double-clicking the sonogram will toggle its size. Clicking and
dragging with the left mouse button
will change how bright it is.
Clicking with the right mouse button will switch colormap. The channel
you're viewing and the scale are shown on the top left. The colormap
on the bottom left. Switch with outer mouse button on the channel
button in the second row on the top. Mousewheel will change the
scaling
w.r.t. the frequency axis. Doubleclicking alters the sonogram size.
Time mode
Clicking and dragging or using the mouse wheel  will change the scale
of the graph. The channel you're
viewing is shown on the top left. Switch
with outer mouse button on the channel button in the second row on the top.
Doubleclicking alters the signal window size.
SideSpectrum Meter
A stereo spectral analyzer to study how much the left and right channel
differ.
StereoManipulator
A stereo width manipulator with a large number of filters.
Splits the channel into two via crossover filter. Both channels can then
be mono-ified separately. Use FIR filter for strong transients, but note
that this incurs N/2 delay of the signal. Larger filters are required for
cutting lower freq. Larger filters will also reduce aliasing. Use high
order IIR for less transient heavy stuff. This incurs no global delay
but may alter transients. FIRs are much more expensive than IIRs.
If you hear phase cancellation, set use channel to left or right rather
than mix. Note that widths other than 0 or 100% in this setting is not
recommended since this will create volume differences between left and right.
",6
Ferocia/snek,Ruby,"
🐍✨Snek
Just like on the Nokia 3210

What
A programming game made with love for Railscamp in the tradition of treasure wars, brains, ant wars etc etc.

You are snake (snek).  Your want to be biggest snek.  Avoid other snek.  Walls too.


Rules
You spawn at a random location.
Every 1 second you get a chance to submit a move [""N"", ""S, ""E"", ""W""].  If you don't submit a move, you will move forward.
Every 5 ticks you will grown by one
If you hit a wall, your snek dies.  If you crash into yourself, your snek dies.  If you hit another snek, your snek dies.  If another snek hits you, that snek dies.  If you head on collide with another snek, both sneks die.
Become the biggest snek.
How to do it
You connect to the server via websockets - there's a sample client and utility code in /client
Quick start
You need to run 3 processes: the server, the client, and the game.
cd server
bundle && yarn && rake db:create db:schema:load
bundle exec rails server

[new tab]
cd server && bundle exec rake game:run

[new tab]
cd client
bundle && yarn
bundle exec ruby runner.rb

PR's Welcome!
If you feel like chipping in there's loads of things you could do.  Maybe:

Add some better obstacles to the map
Improve the styling of the front end
Make the snakes look better
Add food
Improve the error handling of the client code
Surely it needs sound effects right?

",2
polarphp/polarphp,C++,"
Read the English version of this document: English version Readme
阅读本文档其他语言版本: English, 简体中文.
为什么要做 polarphp 项目
随着Go和NodeJS的强势崛起，PHP的市场份额逐渐被蚕食，而PHP官方仍然坚守在Web编程领域，有些东西越是想守住就越守不住。polarphp借鉴NodeJS和Go的相关特性对zendVM重新封装，去掉PHP一些古老弃用的特性和强Web属性，通过实现一套新的运行时框架libpdk，将PHP语言打造成为一门真正的通用性脚本语言，赋能PHP，让其拥有异步编程，协程，线程，内置的unicode支持，标准的文件IO等等特性，让PHP程序员不仅仅能做web应用，也能从容面对真正的服务端应用。polarphp不是一门新的语言，而是PHP语言的一种运行时容器。
主要特性

 兼容最新的PHP语言标准，移除废弃语言特性
 内置unicode字符标准支持
 全功能型运行时库支持，支持异步编程，多线程和协程等等编程模式
 内置包管理器
 内置文档生成器

开发计划
因为开发资源有限，开发计划暂定如下：

使用cmake对zend VM进行编译，生成polarphp定制版的PHP语言虚拟机
语言支持项目，语言测试框架，移植LLVM项目的lit测试框架
实现polarphp驱动程序，实现从命令行执行PHP代码
对polarphp虚拟机进行回归测试，暂定跑通PHP的语言虚拟机相关回归测试
实现polarphp的内置函数
发布核心虚拟机的docker镜像
整合libpdk运行时框架
实现人性化安装，尽量以最少的步骤进行polarphp的安装
实现包管理器
实现语言配套小工具，比如文档生成工具等等

开始体验
克隆polarphp项目库
git clone https://github.com/polarphp/polarphp.git
cd polarphp
git submodule init
git submodule update
git checkout v0.0.1-alpha

运行脚本
./devtools/scripts/build_polarphp.sh

这个时候脚本开始编译相关镜像，耗时比较长，请您耐心等待。等待编译完成，您运行：
docker images

这个时候请确认在输出中有如下镜像：

polarphp_base_env
polarphp_debug

如果没有问题，我们开始测试polarphp是否在镜像中正常运行。
docker run --rm -it polarphp_debug

进入容器后，输入我们的polarphp命令行程序
polar --version

如果您得到下面的输出：
polarphp 0.0.1-git (built: 2019-01-27 12:22)
Copyright (c) 2016-2018 The polarphp foundation (https://polar.foundation)
Zend Engine v3.3.0-dev, Copyright (c) 1998-2018 Zend Technologies

恭喜您，您已经成功编译了polarphp运行时环境。
在编译镜像的时候，我们在~/temp/文件夹中放入了一个测试脚本
if (function_exists('\php\retrieve_version_str')) {
    echo ""version str: "" . \php\retrieve_version_str() . ""\n"";
}

if (function_exists('\php\retrieve_major_version')) {
    echo ""major version: "" . \php\retrieve_major_version() . ""\n"";
}

if (function_exists('\php\retrieve_minor_version')) {
    echo ""minor version: "" . \php\retrieve_minor_version() . ""\n"";
}

if (function_exists('\php\retrieve_patch_version')) {
    echo ""patch version: "" . \php\retrieve_patch_version() . ""\n"";
}

您可以运行一下命令：
polar ~/temp/main.php

如果没有错误，您将得到下面的输出：
version str: polarphp 0.0.1-git
major version: 0
minor version: 0
patch version: 1

感谢您测试polarphp，有什么问题，请扫描下面的微信二维码进群交流。
社区
目前我们暂时只针对中国的用户，所以采用了微信和QQ群的交流方式，下面是二维码，有兴趣的同学可以扫码加入：

PS：扫码请注明来意，比如：学习polarphp或者PHP爱好者

















目前有以下工作组

语言核心团队
标准库团队
生态链项目团队
文档团队
官方网站维护团队

授权协议
polarphp在php语言项目之上进行二次开发，遵守php项目的协议，详情请看：项目协议
贡献代码引导
===========================

CODING_STANDARDS
README.GIT-RULES
README.MAILINGLIST_RULES
README.RELEASE_PROCESS

特别感谢





















",804
RT-Thread/rt-thread,C,"RT-Thread
中文页 |






RT-Thread is an open source IoT operating system from China, which has strong scalability: from a tiny kernel running on a tiny core, for example ARM Cortex-M0, or Cortex-M3/4/7, to a rich feature system running on MIPS32, ARM Cortex-A8, ARM Cortex-A9 DualCore etc.
Overview
RT-Thread RTOS like a traditional real-time operating system. The kernel has real-time multi-task scheduling, semaphore, mutex, mail box, message queue, signal etc. However, it has three different things:

Device Driver;
Component;
Dynamic Module

The device driver is more like a driver framework, UART, IIC, SPI, SDIO, USB device/host, EMAC, MTD NAND etc. The developer can easily add low level driver and board configuration, then combined with the upper framework, he/she can use lots of features.
The Component is a software concept upon RT-Thread kernel, for example a shell (finsh/msh shell), virtual file system (FAT, YAFFS, UFFS, ROM/RAM file system etc), TCP/IP protocol stack (lwIP), POSIX (thread) interface etc. One component must be a directory under RT-Thread/Components and one component can be descripted by a SConscript file (then be compiled and linked into the system).
The Dynamic Module, formerly named as User Applicaion (UA) is a dynamic loaded module or library, it can be compiled standalone without Kernel. Each Dynamic Module has its own object list to manage thread/semaphore/kernel object which was created or initialized inside this UA. More information about UA, please visit another git repo.
Board Support Package
RT-Thread RTOS can support many architectures:

ARM Cortex-M0
ARM Cortex-M3/M4/7
ARM Cortex-R4
ARM Cortex-A8/A9
ARM920T/ARM926 etc
MIPS32
x86
Andes
C-Sky
RISC-V
PowerPC

License
RT-Thread is Open Source software under the Apache License 2.0 since RT-Thread v3.1.1. License and copyright information can be found within the code.
/*
 * Copyright (c) 2006-2018, RT-Thread Development Team
 *
 * SPDX-License-Identifier: Apache-2.0
 */

Since 9th of September 2018, PRs submitted by the community may be merged into the main line only after signing the Contributor License Agreement(CLA).
Usage
RT-Thread RTOS uses scons as building system. Therefore, please install scons and Python 2.7 firstly.
So far, the RT-Thread scons building system support the command line compile or generate some IDE's project. There are some option varaibles in the scons building script (rtconfig.py):

CROSS_TOOL the compiler which you want to use, gcc/keil/iar.
EXEC_PATH the path of compiler.

In SConstruct file:
RTT_ROOT This variable is the root directory of RT-Thread RTOS. If you build the porting in the bsp directory, you can use the default setting. Also, you can set the root directory in RTT_ROOT environment variable and not modify SConstruct files.
When you set these variables correctly, you can use command:
scons

under BSP directory to simplely compile RT-Thread RTOS.
If you want to generate the IDE's project file, you can use command:
scons --target=mdk/mdk4/mdk5/iar/cb -s

to generate the project file.
NOTE: RT-Thread scons building system will tailor the system according to your rtconfig.h configuration header file. For example, if you disable the lwIP in the rtconfig.h by commenting the #define RT_USING_LWIP, the generated project file should have no lwIP related files.
Contribution
Please refer the contributors in the github. Thank all of RT-Thread Developers.
",2553
Bistua/flutter_shop,Dart,"##1
flutter android ios google hybrid方案
需要flutter最新版1.2
或者使用dev1.19版也行
##2
其它版本不确定是否支持hybrid
新建目录‘ bristuaftshop’

在‘ bristuaftshop’ 中 clone本项目

或者：
修改settings.gradle

将'./bristuaftshop/flutter_lib/.android/include_flutter.groovy'中的

bristuaftshop改成你的父类文件夹目录

##3执行以下命令
cd flutter_lib
flutter build apk
google hybrid方案：https://github.com/flutter/flutter/wiki/Add-Flutter-to-existing-apps
原型图：
https://fbw50t.axshare.com/#g=1
密码：201902
蓝图地址：
https://lanhuapp.com/url/tzWeO
百度云地址：
https://pan.baidu.com/s/12gGDI8O5WicXhgkXaEdFMQ
密码: nxms
禅道 项目跟进系统：
pm.bristua.com
初始密码：bristua123456
账号：个人手机号
",5
RT-Thread/rt-thread,C,"RT-Thread
中文页 |






RT-Thread is an open source IoT operating system from China, which has strong scalability: from a tiny kernel running on a tiny core, for example ARM Cortex-M0, or Cortex-M3/4/7, to a rich feature system running on MIPS32, ARM Cortex-A8, ARM Cortex-A9 DualCore etc.
Overview
RT-Thread RTOS like a traditional real-time operating system. The kernel has real-time multi-task scheduling, semaphore, mutex, mail box, message queue, signal etc. However, it has three different things:

Device Driver;
Component;
Dynamic Module

The device driver is more like a driver framework, UART, IIC, SPI, SDIO, USB device/host, EMAC, MTD NAND etc. The developer can easily add low level driver and board configuration, then combined with the upper framework, he/she can use lots of features.
The Component is a software concept upon RT-Thread kernel, for example a shell (finsh/msh shell), virtual file system (FAT, YAFFS, UFFS, ROM/RAM file system etc), TCP/IP protocol stack (lwIP), POSIX (thread) interface etc. One component must be a directory under RT-Thread/Components and one component can be descripted by a SConscript file (then be compiled and linked into the system).
The Dynamic Module, formerly named as User Applicaion (UA) is a dynamic loaded module or library, it can be compiled standalone without Kernel. Each Dynamic Module has its own object list to manage thread/semaphore/kernel object which was created or initialized inside this UA. More information about UA, please visit another git repo.
Board Support Package
RT-Thread RTOS can support many architectures:

ARM Cortex-M0
ARM Cortex-M3/M4/7
ARM Cortex-R4
ARM Cortex-A8/A9
ARM920T/ARM926 etc
MIPS32
x86
Andes
C-Sky
RISC-V
PowerPC

License
RT-Thread is Open Source software under the Apache License 2.0 since RT-Thread v3.1.1. License and copyright information can be found within the code.
/*
 * Copyright (c) 2006-2018, RT-Thread Development Team
 *
 * SPDX-License-Identifier: Apache-2.0
 */

Since 9th of September 2018, PRs submitted by the community may be merged into the main line only after signing the Contributor License Agreement(CLA).
Usage
RT-Thread RTOS uses scons as building system. Therefore, please install scons and Python 2.7 firstly.
So far, the RT-Thread scons building system support the command line compile or generate some IDE's project. There are some option varaibles in the scons building script (rtconfig.py):

CROSS_TOOL the compiler which you want to use, gcc/keil/iar.
EXEC_PATH the path of compiler.

In SConstruct file:
RTT_ROOT This variable is the root directory of RT-Thread RTOS. If you build the porting in the bsp directory, you can use the default setting. Also, you can set the root directory in RTT_ROOT environment variable and not modify SConstruct files.
When you set these variables correctly, you can use command:
scons

under BSP directory to simplely compile RT-Thread RTOS.
If you want to generate the IDE's project file, you can use command:
scons --target=mdk/mdk4/mdk5/iar/cb -s

to generate the project file.
NOTE: RT-Thread scons building system will tailor the system according to your rtconfig.h configuration header file. For example, if you disable the lwIP in the rtconfig.h by commenting the #define RT_USING_LWIP, the generated project file should have no lwIP related files.
Contribution
Please refer the contributors in the github. Thank all of RT-Thread Developers.
",2553
Bistua/flutter_shop,Dart,"##1
flutter android ios google hybrid方案
需要flutter最新版1.2
或者使用dev1.19版也行
##2
其它版本不确定是否支持hybrid
新建目录‘ bristuaftshop’

在‘ bristuaftshop’ 中 clone本项目

或者：
修改settings.gradle

将'./bristuaftshop/flutter_lib/.android/include_flutter.groovy'中的

bristuaftshop改成你的父类文件夹目录

##3执行以下命令
cd flutter_lib
flutter build apk
google hybrid方案：https://github.com/flutter/flutter/wiki/Add-Flutter-to-existing-apps
原型图：
https://fbw50t.axshare.com/#g=1
密码：201902
蓝图地址：
https://lanhuapp.com/url/tzWeO
百度云地址：
https://pan.baidu.com/s/12gGDI8O5WicXhgkXaEdFMQ
密码: nxms
禅道 项目跟进系统：
pm.bristua.com
初始密码：bristua123456
账号：个人手机号
",5
jidroid404/MLiterature,None,"Getting a taste of Research Papers-💯

Repo to track my progress on New Resolution of reading, understanding and tinkering with Machine Learning Research Papers.



Category




Computer Vision


Convolutional Neural Networks


Federated Learning


Generative Models


Geometric Deep Learning


Initialization And Optimization


Miscellaneous


Natural Language Processing


Reinforcement Learning



Computer Vision




Title
Tags




1.
Panoptic Feature Pyramid Networks



2.
Unsupervised Data Augmentation



3.
Fast AutoAugment



4.
DeViSE: A Deep Visual-Semantic Embedding Model



5.
S4L: Self-Supervised Semi-Supervised Learning



6.
Processing Megapixel Images with Deep Attention-Sampling Models



7.
LEDNet: A Lightweight Encoder-Decoder Network for Real-Time Semantic Segmentation




Convolutional Neural Networks




Title
Tags




1.
Bag of Tricks for Image Classification with Convolutional Neural Networks



2.
Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution



3.
Local Relation Networks for Image Recognition



4.
Invertible Residual Networks



5.
Kervolutional Neural Networks



6.
4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks



7.
Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet



8.
Making Convolutional Networks Shift-Invariant Again



9.
Attention Augmented Convolutional Networks



10.
Rethinking Atrous Convolution for Semantic Image Segmentation




Federated Learning




Title
Tags




1.
Towards Federated Learning at Scale



2.
Federated Learning for Mobile Keyboard Prediction



3.
Federated Reinforcement Learning



4.
Federated Learning: Strategies for Improving Communication Efficiency



5.
Gaussian Differential Privacy




Generative Models




Title
Tags




1.
A Style-Based Generator Architecture for Generative Adversarial Networks 



2.
Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks



3.
Controllable Artistic Text Style Transfer via Shape-Matching GAN



4.
SinGAN: Learning a Generative Model from a Single Natural Image



5.
Sketchforme: Composing Sketched Scenes from Text Descriptions for Interactive Applications



6.
Few-Shot Unsupervised Image-to-Image Translation



7.
Semantic Image Synthesis with Spatially-Adaptive Normalization



8.
Self-Attention Generative Adversarial Networks




Deep Learning on Graphs




Title
Tags




1.
Deep Learning on Graphs : A Survey



2.
Graph Neural Networks: A Review of Methods and Applications



3.
A Comprehensive Survey on Graph Neural Networks



4.
Graph Matching Networks for Learning the Similarity of Graph Structured Objects




Initialization And Optimization




Title
Tags




1.
Fixup Initialization: Residual Learning Without Normalization



2.
A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay



3.
L4: Practical loss-based stepsize adaptation for deep learning



4.
A Mean Field Theory of Batch Normalization



5.
The Lottery Ticket Hypothesis:Finding Sparse,Trainable Neural Networks



6.
Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask



7.
The Lottery Ticket Hypothesis at Scale



8.
REGAL: Transfer Learning For Fast Optimization of Computation Graphs




Miscellaneous




Title
Tags




1.
EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Task



2.
Attentive Neural Processes



3.
TensorLy: Tensor Learning in Python



4.
Meta-Sim: Learning to Generate Synthetic Datasets



5.
Low-Memory Neural Network Training: A Technical Report



6.
Using Deep Learning to Annotate the Protein Universe



7.
Initialized Equilibrium Propagation for Backprop-Free Training



8.
MixMatch: A Holistic Approach to Semi-Supervised Learning



9.
Adversarial Examples Are Not Bugs, They Are Features



10.
Style Transfer by Relaxed Optimal Transport and Self-Similarity




Natural Language Processing




Title
Tags




1.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding



2.
Parameter-Efficient Transfer Learning for NLP



3.
Attention Is All You Need



4.
Visualizing Attention in Transformer-Based Language Representation Models



5.
Language Models with Transformers



6.
Generating Long Sequences with Sparse Transformers



7.
Neural Networks for Modeling Source Code Edits



8.
Sample Efficient Adaptive Text-to-Speech



9.
Unified Language Model Pre-training for Natural Language Understanding and Generation



10.
Controllable Neural Story Plot Generation via Reward Shaping




Reinforcement Learning




Title
Tags




1.
Scalable agent alignment via reward modeling: a research direction



2.
Reinforcement Learning with Attention that Works: A Self-Supervised Approach



3.
Challenges of Real-World Reinforcement Learning



4.
An Introduction to Deep Reinforcement Learning




",31
IdleLands/IdleLands4,TypeScript,"IdleLands4
🎉
Requirements

Git
MongoDB (not tested against other DBs)
Node 10.x

MongoDB Note
The game is only tested against MongoDB. TypeORM currently lacks support for joins, etc and these are done manually. Additionally, the seed process requires using MongoDB. This could be cleaned up in the future, but is not a priority right now.
Getting Started

Clone the repo
npm install
If you do not have an assets folder, run npm run postinstall
npm run seed
Create .env file (see Environment Variables)

Environment Variables
Create a .env file in the root of the cloned project and fill it with these values.
Required

TYPEORM_CONNECTION - the DB type (you probably want to use mongodb)
TYPEORM_URL - the URL to connect to the DB
TYPEORM_SYNCHRONIZE - set to true
TYPEORM_ENTITIES - set to src/shared/models/entity/**/*.ts

Optional

SCC_BROKER_REDIS_HOST - the URL to the Redis instance
SCC_BROKER_REDIS_PORT - the port of the Redis instance
GAME_DELAY - the game loop delay. Default: 5000ms.
GRACE_PERIOD_DISCONNECT - the delay between disconnect and character exiting game. Default: 30000ms.
FIREBASE_ADMIN_DATABASE - the admin database URL for firebase. Should be in the format https://<DATABASE_NAME>.firebaseio.com.
FIREBASE_ADMIN_JSON - the JSON blob (stringified) for a service account private key. You can read how to do that here.
DISCORD_SECRET - the Discord API secret for your created Discord bot
DISCORD_GUILD_ID - the Discord guild ID
DISCORD_CHANNEL_ID - the Discord channel ID

Useful Commands

npm run start:server - start the server
npm run start:client - start the client

",3
zpoint/CPython-Internals,None,"Cpython Internals

简体中文
watching this repo so that you will be notified when there's update

This repository is my notes/blog for cpython source code
Trying to illustrate every detail of cpython implementation
# based on version 3.8.0a0
cd cpython
git reset --hard ab54b9a130c88f708077c2ef6c4963b632c132b3

Table of Contents

Objects
Modules
Lib
Interpreter
learning material

Objects

 dict
 long/int
 unicode/str
 set
 list
 tuple
 bytes
 bytearray
 float
 func(user-defined method)
 method(builtin method)
 iter
 gen
 class(bound method/classmethod/staticmethod)
 complex
 enum

Modules

 io

 fileio



Lib

 re
 asyncio

Interpreter

 frame
 code
 descr
 exception
 module
 namespace
 GIL
 gc
 memory management

learning material
I will only recommend what I've read

rushter
YET ANOTHER PYTHON INTERNALS BLOG
CPython internals - Interpreter and source code overview
< < Inside The Python Virtual Machine > >
< < Python源码剖析 > >

",236
scummvm/scummvm,C++,"ScummVM README ·   
For more information, compatibility lists, details on donating, the
latest release, progress reports and more, please visit the ScummVM home
page at: https://www.scummvm.org/
Table of Contents:

1.0) Introduction

1.1) About ScummVM
1.2) Quick start
1.3) F.A.Q.


2.0) Contact

2.1) Reporting Bugs


3.0) Supported Games

3.1) Copy Protection
3.2) Datafiles
3.3) Multi-CD games notes
3.4) Known Problems
3.5) Extra Data Files
3.6) Broken Sword games notes

3.6.1) Broken Sword
3.6.2) Broken Sword II
3.6.3) Broken Sword games
cutscenes
3.6.4) Broken Sword games cutscenes, in
retrospect


3.7) Day of the Tentacle notes
3.8) Dragon History notes
3.9) Flight of the Amazon Queen
notes
3.10) Gobliiins notes
3.11) Inherit the Earth: Quest for the Orb
notes
3.12) Mickey's Space Adventure
notes
3.13) Might and Magic Xeen games
notes
3.14) Myst game notes
3.15) Quest for Glory notes
3.16) Riven game notes
3.17) Simon the Sorcerer games
notes
3.18) Starship Titanic game
notes
3.19) The Curse of Monkey Island
notes
3.20) The Feeble Files notes
3.21) The Legend of Kyrandia
notes
3.22) Troll's Tale notes
3.23) Winnie the Pooh notes
3.24) Sierra AGI games: Predictive Input
Dialog
3.25) Sierra SCI games: Simultaneous speech and
subtitles
3.26) Zork games notes

3.26.1) Zork Nemesis: The Forbidden
Lands
3.26.2) Zork: Grand
Inquisitor


3.27) Commodore64 games notes
3.28) Macintosh games notes


4.0) Supported Platforms
5.0) Running ScummVM

5.1) Command Line Options
5.2) Global Menu
5.3) Graphics filters
5.4) Hotkeys
5.5) Language options


6.0) Saved Games

6.1) Autosaves
6.2) Converting Saved Games
6.3) Viewing/Loading saved games from the command
line


7.0) Music and Sound

7.1) AdLib emulation
7.2) FluidSynth MIDI emulation
7.3) MT-32 emulation
7.4) MIDI emulation
7.5) Native MIDI support

7.5.1) Using MIDI options to customize Native MIDI
output


7.6) UNIX native, ALSA and dmedia sequencer
support

7.6.1) ALSA sequencer [UNIX
ONLY]
7.6.2) IRIX dmedia sequencer: [UNIX
ONLY]


7.7) TiMidity++ MIDI server
support
7.8) Using compressed audio
files

7.8.1) Using MP3 files for CD
audio
7.8.2) Using Ogg Vorbis files for CD
audio
7.8.3) Using Flac files for CD
audio
7.8.4) Compressing MONSTER.SOU with
MP3
7.8.5) Compressing MONSTER.SOU with Ogg
Vorbis
7.8.6) Compressing MONSTER.SOU with
Flac
7.8.7) Compressing music/sfx/speech in AGOS
games
7.8.8) Compressing speech/music in Broken
Sword
7.8.9) Compressing speech/music in Broken Sword
II


7.9) Output sample rate


8.0) Configuration file

8.1) Recognized configuration
keywords
8.2) Custom game options that can be toggled via the
GUI


9.0) Screenshots (SDL backend
only)
10.0) Compiling
11.0) Changelog
12.0) Credits

1.0) Introduction
1.1) About ScummVM
ScummVM is a program which allows you to run certain classic graphical
point-and-click adventure games, provided you already have their data
files. The clever part about this: ScummVM just replaces the executables
shipped with the game, allowing you to play them on systems for which
they were never designed!
Originally it was designed to run LucasArts' SCUMM games, such as Maniac
Mansion, Monkey Island, Day of the Tentacle or Sam and Max. SCUMM stands
for 'Script Creation Utility for Maniac Mansion', which was the first
game for which LucasArts designed this system. And much later it gave
its name to ScummVM ('VM' meaning Virtual Machine).
Over time support for a lot of non-SCUMM games has been added, and
ScummVM now also supports many of Sierra's AGI and SCI games (such as
King's Quest 1-6, Space Quest 1-5, ...), Discworld 1 and 2, Simon the
Sorcerer 1 and 2, Beneath A Steel Sky, Lure of the Temptress, Broken
Sword I and II, Flight of the Amazon Queen, Gobliiins 1-3, The Legend of
Kyrandia series, many of Humongous Entertainment's children's SCUMM
games (including Freddi Fish and Putt Putt games) and many more. You can
find a full list with details on which adventures are supported and how
well on the compatibility page. ScummVM is continually improving, so
check back often.
Among the systems on which you can play those games are regular desktop
computers (running Windows, Linux, Mac OS X, ...), game consoles
(Dreamcast, Nintendo DS & Wii, PS2, PSP, ...), smartphones (Android,
iPhone, PocketPC, Symbian ...) and more.
At this time ScummVM is still under heavy development. Be aware that
whilst we attempt to make sure that many games can be completed with few
major bugs, crashes can happen and we offer no warranty. That being
said, some of the games have been supported for a long time and should
work fine with any recent stable release. You can get a feeling of how
well each game is working in ScummVM by looking at the compatibility
page. Actually, if you browse a bit around you might discover that
ScummVM is even being used commercially to re-release some of the
supported games on modern platforms. This shows that several companies
are happy with the quality of the software and how well it can run some
of the games.
If you enjoy ScummVM feel free to donate using the PayPal button on the
ScummVM homepage. This will help us buy utilities needed to develop
ScummVM easier and quicker. If you cannot donate, help and contribute a
patch!
1.2) Quick start
For the impatient among you, here is how to get ScummVM running in five
simple steps.


Download ScummVM from https://www.scummvm.org/downloads/ and
install it.


Create a directory on your hard drive and copy the game datafiles
from the original media to this directory. Repeat this for every
game you want to play.


Start ScummVM, choose 'Add game', select the directory with the game
datafiles (do not try to select the datafiles themselves!) and
press Choose.


A dialog should pop up allowing you to configure various settings if
you wish to (it should be just fine to leave everything at its
default, though). Confirm the dialog.


Select the game you want to play in the list, and press Start.


In the future, you should be able to directly skip to step 5, unless you
want to add more games.
Hint: If you want to add multiple games in one go, try pressing and
holding the shift key before clicking 'Add game' -- its label will
change to 'Mass Add' and if you press it, you are again asked to select
a directory, only this time ScummVM will search through all
subdirectories for supported games.
1.3) F.A.Q.
We've compiled a list of F.A.Q. at:
https://www.scummvm.org/faq/
2.0) Contact
The easiest way to contact the ScummVM team is by submitting bug reports
(see section 2.1) or by using our forums at https://forums.scummvm.org.
You can also join and e-mail the scummvm-devel mailing list, or chat
with us on IRC (#scummvm on irc.freenode.net) Please do not ask us to
support an unsupported game -- read the FAQ on our web site first.
2.1) Reporting Bugs
To report a bug, please follow the ""Bug Tracker"" link from our homepage
and log in with your GitHub account. Please make sure the bug is
reproducible, and still occurs in the latest git/Daily build version.
Also check the known problems list (below) and the compatibility list on
our website for that game, to ensure the issue is not already known:
https://www.scummvm.org/compatibility/
Please do not report bugs for games that are not listed as being
completeable in the 'Supported Games' section, or compatibility list. We
know those games have bugs.
Please include the following information:

ScummVM version (PLEASE test the latest git/Daily build)
Bug details, including instructions on reproducing
Language of game (English, German, ...)
Version of game (talkie, floppy, ...)
Platform and Compiler (Win32, Linux, FreeBSD, ...)
Attach a saved game if possible - If this bug only occurred
recently, please note the last version without the bug, and the
first version including the bug. That way we can fix it quicker by
looking at the changes made.

Finally, please report each issue separately; do not file multiple
issues on the same ticket. (Otherwise, it gets difficult to track the
status of each individual bug).
3.0) Supported Games
At the moment the following games have been reported to work, and should
be playable to the end: A more detailed compatibility list of the
supported games can be found here:
https://www.scummvm.org/compatibility/



LucasArts (SCUMM) Games:





Maniac Mansion
[maniac]


Zak McKracken and the Alien Mindbenders
[zak]


Indiana Jones and the Last Crusade
[indy3]


Loom
[loom]


Passport to Adventure
[pass]


The Secret of Monkey Island
[monkey]


Monkey Island 2: LeChuck's Revenge
[monkey2]


Indiana Jones and the Fate of Atlantis
[atlantis]


Day of the Tentacle
[tentacle]


Sam & Max Hit the Road
[samnmax]


Full Throttle
[ft]


The Dig
[dig]


The Curse of Monkey Island
[comi]






Activision (MADE) Games:





Leather Goddesses of Phobos 2
[lgop2]


The Manhole
[manhole]


Return to Zork
[rtz]


Rodney's Funscreen
[rodney]






Adventuresoft/Horrorsoft (AGOS) Games:





Elvira - Mistress of the Dark
[elvira1]


Elvira II - The Jaws of Cerberus
[elvira2]


Personal Nightmare
[pn]


Simon the Sorcerer 1
[simon1]


Simon the Sorcerer 2
[simon2]


Simon the Sorcerer's Puzzle Pack - Demon In My Pocket
[dimp]


Simon the Sorcerer's Puzzle Pack - Jumble
[jumble]


Simon the Sorcerer's Puzzle Pack - NoPatience
[puzzle]


Simon the Sorcerer's Puzzle Pack - Swampy Adventures
[swampy]


The Feeble Files
[feeble]


Waxworks
[waxworks]






Coktel Vision (GOB) Games:





Bargon Attack
[bargon]


Fascination
[fascination]


Geisha
[geisha]


Gobliiins
[gob1]


Gobliins 2
[gob2]


Goblins 3
[gob3]


Lost in Time
[lostintime]


Once Upon A Time: Little Red Riding Hood
[littlered]


Playtoons: Bambou le sauveur de la jungle
[bambou]


The Bizarre Adventures of Woodruff and the Schnibble
[woodruff]


Urban Runner
[urban]


Ween: The Prophecy
[ween]






Revolution Software (Various) Games:





Beneath a Steel Sky
[sky]


Broken Sword: The Shadow of the Templars
[sword1]


Broken Sword II: The Smoking Mirror
[sword2]


Lure of the Temptress
[lure]






Sierra (AGI/preAGI) Games:





The Black Cauldron
[bc]


Gold Rush!
[goldrush]


King's Quest I
[kq1]


King's Quest II
[kq2]


King's Quest III
[kq3]


King's Quest IV
[kq4]


Leisure Suit Larry in the Land of the Lounge Lizards
[lsl1]


Mixed-Up Mother Goose
[mixedup]


Manhunter 1: New York
[mh1]


Manhunter 2: San Francisco
[mh2]


Police Quest I: In Pursuit of the Death Angel
[pq1]


Space Quest I: The Sarien Encounter
[sq1]


Space Quest II: Vohaul's Revenge
[sq2]


Fanmade Games
[agi-fanmade]


Mickey's Space Adventure
[mickey]


Troll's Tale
[troll]


Winnie the Pooh in the Hundred Acre Wood
[winnie]






Sierra (SCI) Games:





Castle of Dr. Brain
[castlebrain]


Codename: ICEMAN
[iceman]


Conquests of Camelot
[camelot]


Conquests of the Longbow
[longbow]


EcoQuest: The Search for Cetus
[ecoquest]


EcoQuest 2: Lost Secret of the Rainforest
[ecoquest2]


Freddy Pharkas: Frontier Pharmacist
[freddypharkas]


Gabriel Knight: Sins of the Fathers
[gk1]


Hoyle's Book of Games 1
[hoyle1]


Hoyle's Book of Games 2
[hoyle2]


Hoyle's Book of Games 3
[hoyle3]


Hoyle Classic Card Games
[hoyle4]


Jones in the Fast Lane
[jones]


King's Quest I
[kq1sci]


King's Quest IV
[kq4sci]


King's Quest V
[kq5]


King's Quest VI
[kq6]


King's Quest VII
[kq7]


King's Questions
[kquestions]


Laura Bow: The Colonel's Bequest
[laurabow]


Laura Bow 2: The Dagger of Amon Ra
[laurabow2]


Leisure Suit Larry 1
[lsl1sci]


Leisure Suit Larry 2
[lsl2]


Leisure Suit Larry 3
[lsl3]


Leisure Suit Larry 5
[lsl5]


Leisure Suit Larry 6
[lsl6]


Leisure Suit Larry 6 (hires)
[lsl6hires]


Leisure Suit Larry 7
[lsl7]


Lighthouse: The Dark Being
[lighthouse]


Mixed-up Fairy Tales
[fairytales]


Mixed-up Mother Goose
[mothergoose]


Mixed-up Mother Goose Deluxe
[mothergoosehires]


Pepper's Adventures in Time
[pepper]


Phantasmagoria
[phantasmagoria]


Phantasmagoria 2: A Puzzle of Flesh
[phantasmagoria2]


Police Quest 1
[pq1sci]


Police Quest 2
[pq2]


Police Quest 3
[pq3]


Police Quest 4
[pq4]


Quest for Glory 1/Hero's Quest
[qfg1]


Quest for Glory 1
[qfg1vga]


Quest for Glory 2
[qfg2]


Quest for Glory 3
[qfg3]


RAMA
[rama]


Slater & Charlie Go Camping
[slater]


Shivers
[shivers]


Space Quest I
[sq1sci]


Space Quest III
[sq3]


Space Quest IV
[sq4]


Space Quest V
[sq5]


Space Quest 6
[sq6]


The Island of Dr. Brain
[islandbrain]


The Beast Within: A Gabriel Knight Mystery
[gk2]


Torin's Passage
[torin]






Other Games:





3 Skulls of the Toltecs
[toltecs]


Amazon: Guardians of Eden
[access]


Beavis and Butt-head in Virtual Stupidity
[bbvs]


Blue Force
[blueforce]


Broken Sword: The Return of the Templars
[sword25]


Bud Tucker in Double Trouble
[tucker]


Chivalry is Not Dead
[chivalry]


Cruise for a Corpse
[cruise]


DreamWeb
[dreamweb]


Discworld
[dw]


Discworld 2: Missing Presumed ...!?
[dw2]


Dragon History
[draci]


Drascula: The Vampire Strikes Back
[drascula]


Eye of the Beholder
[eob]


Eye of the Beholder II: The Legend of Darkmoon
[eob2]


Flight of the Amazon Queen
[queen]


Future Wars
[fw]


Hopkins FBI
[hopkins]


Hugo's House of Horrors
[hugo1]


Hugo 2: Whodunit?
[hugo2]


Hugo 3: Jungle of Doom
[hugo3]


I Have No Mouth, and I Must Scream
[ihnm]


Inherit the Earth: Quest for the Orb
[ite]


Lands of Lore: The Throne of Chaos
[lol]


Mortville Manor
[mortevielle]


Myst / Myst: Masterpiece Edition
[myst]


Nippon Safes Inc.
[nippon]


Rex Nebular and the Cosmic Gender Bender
[nebular]


Ringworld: Revenge Of The Patriarch
[ringworld]


Riven: The Sequel to Myst
[riven]


Return to Ringworld
[ringworld2]


Sfinx
[sfinx]


Soltys
[soltys]


Starship Titanic
[titanic]


The Journeyman Project: Pegasus Prime
[pegasus]


The Labyrinth of Time
[lab]


The Legend of Kyrandia
[kyra1]


The Legend of Kyrandia: The Hand of Fate
[kyra2]


The Legend of Kyrandia: Malcolm's Revenge
[kyra3]


The Lost Files of Sherlock Holmes: The Case of the Serrated Scalpel
[scalpel]


The Lost Files of Sherlock Holmes: The Case of the Rose Tattoo
[rosetattoo]


The Neverhood
[neverhood]


The 7th Guest
[t7g]


TeenAgent
[teenagent]


Toonstruck
[toon]


Tony Tough and the Night of Roasted Moths
[tony]


Touche: The Adventures of the Fifth Musketeer
[touche]


U.F.O.s / Gnap: Der Schurke aus dem All
[gnap]


Voyeur
[voyeur]


Zork: Grand Inquisitor
[zgi]


Zork Nemesis: The Forbidden Lands
[znemesis]






Humongous Entertainment (SCUMM) Games:





Backyard Baseball
[baseball]


Backyard Baseball 2001
[baseball2001]


Backyard Baseball 2003
[baseball2003]


Backyard Football
[football]


Backyard Football 2002
[football2002]


Bear Stormin'
[brstorm]


Big Thinkers First Grade
[thinker1]


Big Thinkers Kindergarten
[thinkerk]


Blue's 123 Time Activities
[Blues123Time]


Blue's ABC Time Activities
[BluesABCTime]


Blue's Art Time Activities
[arttime]


Blue's Birthday Adventure
[BluesBirthday]


Blue's Reading Time Activities
[readtime]


Fatty Bear's Birthday Surprise
[fbear]


Fatty Bear's Fun Pack
[fbpack]


Freddi Fish 1: The Case of the Missing Kelp Seeds
[freddi]


Freddi Fish 2: The Case of the Haunted Schoolhouse
[freddi2]


Freddi Fish 3: The Case of the Stolen Conch Shell
[freddi3]


Freddi Fish 4: The Case of the Hogfish Rustlers of Briny Gulch
[freddi4]


Freddi Fish 5: The Case of the Creature of Coral Cove
[freddicove]


Freddi Fish and Luther's Maze Madness
[maze]


Freddi Fish and Luther's Water Worries
[water]


Let's Explore the Airport with Buzzy
[airport]


Let's Explore the Farm with Buzzy
[farm]


Let's Explore the Jungle with Buzzy
[jungle]


Pajama Sam: Games to Play on Any Day
[pjgames]


Pajama Sam 1: No Need to Hide When It's Dark Outside
[pajama]


Pajama Sam 2: Thunder and Lightning Aren't so Frightening
[pajama2]


Pajama Sam 3: You Are What You Eat From Your Head to Your Feet
[pajama3]


Pajama Sam's Lost & Found
[lost]


Pajama Sam's Sock Works
[socks]


Putt-Putt Enters the Race
[puttrace]


Putt-Putt Goes to the Moon
[puttmoon]


Putt-Putt Joins the Circus
[puttcircus]


Putt-Putt Joins the Parade
[puttputt]


Putt-Putt Saves the Zoo
[puttzoo]


Putt-Putt Travels Through Time
[putttime]


Putt-Putt and Pep's Balloon-O-Rama
[balloon]


Putt-Putt and Pep's Dog on a Stick
[dog]


Putt-Putt & Fatty Bear's Activity Pack
[activity]


Putt-Putt's Fun Pack
[funpack]


SPY Fox 1: Dry Cereal
[spyfox]


SPY Fox 2: Some Assembly Required
[spyfox2]


SPY Fox 3: Operation Ozone
[spyozon]


SPY Fox in Cheese Chase
[chase]


SPY Fox in Hold the Mustard
[mustard]




The following games should load but are not yet fully playable. Play
these at your own risk, and please do not file bug reports about them.
If you want the latest updates on game compatibility, visit our web
site and view the compatibility chart.










Backyard Soccer
[soccer]


Backyard Soccer MLS
[soccermls]


Backyard Soccer 2004
[soccer2004]


Blue's Treasure Hunt
[BluesTreasureHunt]






Animation Magic (Composer) Games:





Darby the Dragon
[darby]


Gregory and the Hot Air Balloon
[gregory]


Magic Tales: Liam Finds a Story
[liam]


The Princess and the Crab
[princess]


Sleeping Cub's Test of Courage
[sleepingcub]






Living Books Games:





Aesop's Fables: The Tortoise and the Hare
[tortoise]


Arthur's Birthday
[arthurbday]


Arthur's Teacher Trouble
[arthur]


Dr. Seuss's ABC
[seussabc]


Green Eggs and Ham
[greeneggs]


Harry and the Haunted House
[harryhh]


Just Grandma and Me
[grandma]


Little Monster at School
[lilmonster]


Ruff's Bone
[ruff]


Sheila Rae, the Brave
[sheila]


Stellaluna
[stellaluna]


The Berenstain Bears Get in a Fight
[bearfight]


The Berenstain Bears in the Dark
[beardark]


The New Kid on the Block
[newkid]



The following games are based on the SCUMM engine, but NOT supported by
ScummVM (yet):
Moonbase Commander

Please be aware that the engines may contain bugs and unimplemented
features that sometimes make it impossible to finish the game. Save
often, and please file a bug report (instructions on submitting bug
reports are above) if you encounter such a bug in a 'supported' game.
3.1) Copy Protection
The ScummVM team does not condone piracy. However, there are cases where
the game companies (such as LucasArts) themselves bundled 'cracked'
executables with their games -- in these cases the data files still
contain the copy protection scripts, but the interpreter bypasses them
(similar to what an illegally cracked version might do, only that here
the producer of the game did it). There is no way for us to tell the
difference between legitimate and pirated data files, so for the games
where we know that a cracked version of the original interpreter was
sold at some point, ScummVM will always have to bypass the copy
protection.
In some cases ScummVM will still show the copy protection screen. Try
entering any answer. Chances are that it will work.
ScummVM will skip copy protection in the following games:

Beneath a Steel Sky

bypassed with kind permission from Revolution Software.


Dreamweb

a list of available commands in the in-game terminals is now
shown when the player uses the help command


Inherit the Earth: Quest for the Orb (Floppy version)

bypassed with kind permission from Wyrmkeep Entertainment, since
it was bypassed in all CD releases of the game.


Loom (EGA DOS)
Lure of the Temptress
Maniac Mansion
Might and Magic: World of Xeen
Monkey Island 2: LeChuck's Revenge
Rex Nebular and The Cosmic Gender Bender
Simon the Sorcerer 1 (Floppy version)
Simon the Sorcerer 2 (Floppy version)

bypassed with kind permission from Adventure Soft, since it was
bypassed in all CD releases of the game.


The Secret of Monkey Island (VGA)
Voyeur
Waxworks
Zak McKracken and the Alien Mindbenders

3.2) Datafiles
For a comprehensive list of required Datafiles for supported games
visit:
https://wiki.scummvm.org/index.php/Datafiles
3.3) Multi-CD games notes
In general, ScummVM does not deal very well with Multi-CD games. This is
because ScummVM assumes everything about a game can be found in one
directory. Even if ScummVM does make some provisions for asking the user
to change CD, the original game executables usually installed a small
number of files to the hard disk. Unless these files can be found on all
the CDs, ScummVM will be in trouble.
Fortunately, ScummVM has no problems running the games entirely from
hard disk, if you create a directory with the correct combination of
files. Usually, when a file appears on more than one CD you can pick
either of them.
3.4) Known Problems
This release has the following known problems. There is no need to
report them, although patches to fix them are welcome. If you discover a
bug that is not listed here, nor in the compatibility list on the web
site, please see the section on reporting bugs.
CD Audio Games:

When playing games that use CD Audio (FM-TOWNS games, Loom CD, etc)
users of Microsoft Windows 2000/XP may experience random crashes.
This is due to a long-standing Windows bug, resulting in corrupt
game files being read from the CD. Please copy the game data to your
hard disk to avoid this.

FM-TOWNS versions:

The Kanji versions require the FM-TOWNS Font ROM.

Loom:

Turning off the subtitles via the config file does not work reliably
as the Loom scripts automatically turn them on again.
MIDI support in the EGA version requires the Roland update from
LucasArts.
The PC-Engine Kanji version requires the system card rom.

The Secret of Monkey Island:

MIDI support in the EGA version requires the Roland update from
LucasArts.

Beneath a Steel Sky:

Amiga versions aren't supported.
Floppy demos aren't supported.
Not a bug: CD version is missing speech for some dialogs, this is
normal.

Elvira - Mistress of the Dark:

No music in the Atari ST version.

Elvira II - The Jaws of Cerberus

No music in the Atari ST version.
No sound effects in the PC version.
Palette issues in the Atari ST version.

Inherit the Earth: Quest for the Orb:

Amiga versions aren't supported.

Lure of the Temptress:

No Roland MT-32 support.
Sound support is incomplete and doesn't sound like original.

Simon the Sorcerer 1:

Subtitles aren't available in the English and German CD versions as
they are missing the majority of subtitles.

Simon the Sorcerer 2:

Combined speech and subtitles will often cause speech to be cut off
early, this is a limitation of the original game.
Only default language (English) of data files is supported in Amiga
and Macintosh versions.

Simon the Sorcerer's Puzzle Pack:

No support for displaying, entering, loading or saving high scores.
No support for displaying names of items, when hovering over them in
Swampy Adventures.

The Feeble Files:

Subtitles are often incomplete, they were always disabled in the
original game.

The Legend of Kyrandia:

No music or sound effects in the Macintosh floppy versions.
Macintosh CD is using included DOS music and sound effects.

Humongous Entertainment games:

Only the original load and save interface can be used.
No support for multiplayer or printing images.

3.5) Extra Data Files
Some games require additional files that are not part of the original data. Those files can generally be found in our Downloads page.
Games that require additional data:

Beneath a Steel Sky (sky.cpt)
Flight of the Amazon Queen
Kyrandia Series (kyra.dat)
Lands of Lore Series (kyra.dat)
Lure of the Temptress (lure.dat)

The most up to date list of Engine data files can be found in our source code repository
3.6) Broken Sword games notes
The instructions for the Broken Sword games are for the Sold-Out
Software versions, with each game on two CDs, since these were the
versions most easily available at the time ScummVM gained support for
them. Hopefully they are general enough to be useful to other releases
as well.
3.6.1) Broken Sword
For this game, you will need all of the files from the clusters
directories on both CDs. For the Windows and Macintosh versions, you
will also need the speech.clu files from the speech directories, but
since they are not identical you will need to rename them speech1.clu
and speech2.clu for CD 1 and 2 respectively. The PlayStation version
requires the speech.tab, speech.dat, speech.lis, and speech.inf.
In addition, the Windows and Macintosh versions require a music
subdirectory with all of the files from the music subdirectories on both
CDs. Some of these files appear on both CDs, but in these cases they are
either identical or, in one case, so nearly identical that it makes
little difference. The PlayStation version requires tunes.dat and
tunes.tab.
3.6.2) Broken Sword II
For this game, you will need all of the files from the clusters
directories on both CDs. (Actually, a few of them may not be strictly
necessary, but the ones that I'm uncertain about are all fairly small.)
You will need to rename the speech.clu and music.clu files speech1.clu,
speech2.clu, music1.clu and music2.clu so that ScummVM can tell which
ones are from CD 1 and which ones are from CD 2. Any other files that
appear in both cluster directories are identical. Use whichever you
like.
In addition, you will need the cd.inf and, optionally, the startup.inf
files from the sword2 directory on CD 1.
3.6.3) Broken Sword games cutscenes
The cutscenes for the Broken Sword games have a bit of a history (see
the next section, if you are interested), but in general all you need to
do is to copy the .SMK files from the ""SMACKS"" or ""SMACKSHI"" directories
on the CDs to the same directory as the other game data files. (Broken
Sword has a ""SMACKSLO"" directory with the same cutscenes, but these are
of lower quality.) You can put them in a subdirectory called ""video"" if
you find that neater.
For the PlayStation versions, you can dump the original videos off the
disc. For each of the files ending in an ""STR"" extension, you should
dump them as raw sectors off the disc (all 2352 bytes per sector). You
may also use the re-encoded cutscenes mentioned below instead, but this
will not work for all videos in Broken Sword II. For more information,
see:
https://wiki.scummvm.org/index.php/HOWTO-PlayStation_Videos
Some re-releases of the games, as well as the PlayStation version, do
not have Smacker videos. Revolution Software has kindly allowed us to
provide re-encoded cutscenes for download on our website:
https://www.scummvm.org/downloads/
These cutscenes are provided in DXA format with FLAC audio. Their
quality is equal to the original games due to the use of lossless
compression. Viewing these cutscenes requires a version of ScummVM
compiled with both FLAC and zlib support.
For systems that are too slow to handle the decoding of FLAC audio, the
audio for these cutscenes is also provided separately as OGG Vorbis
audio. Viewing these cutscenes with OGG Vorbis audio requires a version
of ScummVM compiled with both libVorbis and zlib support.
For Broken Sword, we also provide a subtitles add-on. Simply unpack it
and follow the instructions in its readme.txt file. The subtitle pack
currently does not work when running PlayStation videos. (Broken Sword
II already has subtitles; no extra work is needed for them.)
3.6.4) Broken Sword games cutscenes, in retrospect
The original releases of the Broken Sword games used RAD Game Tools's
Smacker(tm) format. As RAD was unwilling to open the older legacy
versions of this format to us, and had requested we not reverse engineer
it, an alternative solution had to be found.
In Broken Sword II, it was possible to play back the voice-over without
playing the video itself. This remained a fallback until ScummVM 1.0.0,
but was never the only solution for any stable release.
In ScummVM 0.6.0 we used MPEG, which provided a reasonable trade-off
between size and quality. In ScummVM 0.10.0 this was superseded by DXA
(originally added for AdventureSoft's ""The Feeble Files""). This gave us
a way of providing the cutscenes in the exact same quality as the
originals, at the cost of being larger.
Finally, in early 2006, the Smacker format was reverse engineered for
the FFmpeg project. Thanks to their hard work, ScummVM 1.0.0 now
supports the original cutscenes. At the same time, MPEG support was
dropped. From a technical standpoint, this was a good thing since
decoding MPEG movies added a lot of complexity, and they didn't look as
good as the Smacker and DXA versions anyway.
3.7) Day of the Tentacle notes
At one point in the game, you come across a computer that allows you to
play the original Maniac Mansion as an easter egg. ScummVM supports
this, with a few caveats:
ScummVM will scan your configuration file for a game that's in a
Maniac sub-folder of your Day of the Tentacle folder. If you've copied
the data files from the CD version, this should already be the case but
you have to add the game to ScummVM as well.
To return to Day of the Tentacle, press F5 and select ""Return to
Launcher"".
This means that you could in theory use any game as the easter egg.
Indeed, there is a ""secret"" configuration setting, easter_egg, to
override the ID of the game to run. Be aware, though, that not all games
support returning to the launcher, and setting it up to use Day of the
Tentacle itself as the easter egg game is not recommended.
3.8) Dragon History notes
There are 4 language variants of the game: Czech, English, Polish and
German. Each of them is distributed in a separate archive. The only
official version is the Czech one, and the English, Polish and German
ports have always been work in progress and never officially released.
Although all texts are fully translated, it is known that some of them
contain typos.
There exists an optional Czech dubbing for the game. For bandwidth
reasons, you can download it separately and then unpack it to the
directory of the game. You can listen to the Czech dubbing with all
language variants of the game, while reading the subtitles.
All game files and the walkthrough can be downloaded from:
http://www.ucw.cz/draci-historie/index-en.html
3.9) Flight of the Amazon Queen notes
Only the original non-freeware version of Flight of the Amazon Queen
(from original CD), requires the queen.tbl datafile (available from the Downloads page on our website) in either the
directory containing the queen.1 game data file, in your extrapath, or
in the directory where your ScummVM executable resides.
Alternatively, you can use the compress_queen tool from the tools
package to 'rebuild' your FOTAQ data file to include the table for that
specific version, and thus removing the run-time dependency on the
queen.tbl file. This tool also allows you to compress the speech and
sound effects with MP3, OGG or FLAC.
3.10) Gobliiins notes
The CD versions of the Gobliiins series contain one big audio track
which you need to rip (see the section on using compressed audio files)
and copy into the game directory if you want to have in-game music
without the CD in the drive all the time. The speech is also in that
track and its volume is therefore changed with the music volume control
as well.
3.11) Inherit the Earth: Quest for the Orb notes
In order to run the Mac OS X Wyrmkeep re-release of the game you will
need to copy over data from the CD to your hard disk. If you're on a PC
then consult:
https://wiki.scummvm.org/index.php/HOWTO-Mac_Games
Although it primarily talks about SCUMM games, it mentions the
""HFSExplorer"" utility which you need to extract the files. Note that you
have to put the speech data ""Inherit the Earth Voices"" in the same
directory as the game data which is stored in:
Inherit the Earth.app/Contents/Resources
For the old Mac OS 9 release you need to copy the files in MacBinary
format, as they should include both resource and data forks. Copy all
'ITE *' files.
3.12) Mickey's Space Adventure notes
To run Mickey's Space Adventure under ScummVM, the original executable
of the game (mickey.exe) is needed together with the game's data files.
There is extensive mouse support for the game under ScummVM, even though
there wasn't any mouse support in the original game. Menu items can be
selected using the mouse, and it is possible to move to other locations
using the mouse as well. When the mouse cursor is hovered on the edges
of the screen, it changes color to red if it is possible to walk towards
that direction. The player can then simply click on the edges of the
game's screen to change location, similar to many adventure games, which
is simpler and more straightforward than moving around using the menu.
3.13) Might and Magic Xeen games notes
To properly play the World of Xeen CD Talkie using original discs, use
LAME or some other encoder to rip the cd audio tracks to files, either
mp3 or ogg. Whichever you choose, the tracks of the first CD should be
named from track02 to track31, whereas the second CD's audio tracks
should be encoded and renamed as track32 through to track60.
For the GOG Might and Magic 4-5 installation, install the game to your
computer, and do the following steps:

The game1.inst (CUE) and game1.gog (BIN) file from the game folder
is a CD image. Use software like Virtual CloneDrive to mount it as a
drive. Linux and MacOS users can use bchunk to convert it to an ISO.
Copy all the .cc files from the subfolder in the mounted drive to a
new empty game folder that you create for the game.
Copy all the music/*.ogg files from the GOG installation to your
game folder. You'll then need to rename all of them from xeen??.ogg
to track??.ogg
You should then be able to point ScummVM to this new game folder,
and the CD talkie version should be detected.

Savegames from either Clouds or Darkside of Xeen games can be
transferred across to World of Xeen (that combines both games) simply by
setting up and detecting World of Xeen (either by manually combining the
two games or using the GOG World of Xeen installer), and then renaming
the savegames to use the World of Xeen savegame format, by default
'worldofxeen.*'
The Xeen engine also offers two custom options in the Engine tab for the
games in the ScummVM launcher. They are:

To change the threshold armor breaks at for characters from -10HP to
-80HP
To show values for inventory items, even outside of the blacksmith,
allowing the relative strength/value of armor and weapons to be
compared.

3.14) Myst game notes
Left Click: Move/action
Space: Pause the game
Esc: Skip cutscene
F5: Menu
Myst will autosave to slot 0 if no save or an autosave is present in
slot 0.
3.15) Quest for Glory notes
It is possible to import characters, beginning with Quest for Glory II,
from past games to future games and continue from the stats earned from
those games.
For example, a character can be imported from Quest for Glory I directly
to Quest for Glory III without having to necessarily play Quest for
Glory II.
Characters cannot be imported from future games to past games, nor can a
character be imported to the same game that was just completed. In other
words, a character from Quest for Glory II cannot be imported into Quest
for Glory II.
If you want to use a saved character from the original Sierra
interpreter, you will need to rename the character file to
""qfg[game-number]-[character-filename].sav"" and place it in the
ScummVM save path (see section 6.0), otherwise the file won't get listed
on the import screen.
Example: qfg2-thief.sav
3.16) Riven game notes
Left Click: Move/action
Arrow Keys: Movement
Page Up: Look up
Page Down: Look down
Space: Pause the game
Esc: Skip cutscene
F5: Menu
Ctrl-o: Load game
Ctrl-s: Save game
Riven will autosave to slot 0 if no save or an autosave is present in
slot 0.
3.17) Simon the Sorcerer games notes
If you have the dual version of Simon the Sorcerer 1 or 2 on CD, you
will find the Windows version in the main directory of the CD and the
DOS version in the DOS directory of the CD.
3.18) Starship Titanic game notes
For the purposes of solving the starfield puzzle, only mouse clicks, L
and Tab are really needed, though the action glyph in the PET can be
used instead of Tab.
3.19) The Curse of Monkey Island notes
For this game, you will need the comi.la0, comi.la1 and comi.la2 files.
The comi.la0 file can be found on either CD, but since they are
identical it doesn't matter which one of them you use.
In addition, you will need to create a ""resource"" subdirectory
containing all of the files from -both- ""resource"" subdirectories on the
two CDs. Some of the files appear on both CDs, but again they're
identical.
3.20) The Feeble Files notes
Amiga/Macintosh: You need to install a small pack of cutscenes that are
missing in both of these versions of The Feeble Files. It's called ""The
Feeble Files - Omni TV and epilogue cutscenes for the Amiga and
Macintosh versions"" and you can get it here:
https://www.scummvm.org/games/#feeble
Windows: If you have the Windows version of The Feeble Files, there are
several things to note.
Many of the files necessary for the game are stored in an InstallShield
file called data1.cab, which ScummVM is unable to unpack. You will need
to use the original installer or i5comp to unpack the contents of this
file. The i5comp decompression tool, can be found via a search on the
internet.
To use the speech files with ScummVM, they need to be renamed as
follows:

Rename voices.wav on CD1 to voices1.wav
Rename voices.wav on CD2 to voices2.wav
Rename voices.wav on CD3 to voices3.wav
Rename voices.wav on CD4 to voices4.wav

3.21) The Legend of Kyrandia notes
To run The Legend of Kyrandia under ScummVM you need the kyra.dat
file. The file should already be included in official ScummVM packages.
In case ScummVM complains that the file is missing you can find it on
the Downloads page of the ScummVM website. Note that the current
Windows release of ScummVM should contain the file embedded into the
executable, thus you only need to grab it in case ScummVM complains
about the file being missing.
3.22) Troll's Tale notes
The original game came in a PC booter disk, therefore it is necessary to
dump the contents of that disk in an image file and name it ""troll.img""
to be able to play the game under ScummVM.
3.23) Winnie the Pooh notes
It is possible to import saved games from the original interpreter of
the game into ScummVM.
There is extensive mouse support for the game under ScummVM, even though
there wasn't any mouse support in the original game. Menu items can be
selected using the mouse, and it is possible to move to other locations
using the mouse as well. When the mouse cursor is hovered on the edges
of the screen, it changes color to red if it is possible to walk towards
that direction. The player can then simply click on the edges of the
game's screen to change location, similar to many adventure games, which
is simpler and more straightforward than moving around using the menu.
3.24) Sierra AGI games: Predictive Input Dialog
The Predictive Input Dialog is a ScummVM aid for running AGI engine
games (which notoriously require command line input) on devices with
limited keyboard support. In these situations, since typing with
emulated keyboards is quite tedious, commands can be entered quickly and
easily via the Predictive Input Dialog.
In order to enable predictive input in AGI games, you need to copy the
pred.dic file in the ScummVM extras directory or the directory of the
game you wish to play. This dictionary has been created by parsing
through all known AGI games and contains the maximum set of common
words.
If the dictionary is detected, the Predictive Input Dialog is displayed
either when you click on the command line area (wherever keyboard input
is required, even in dialog boxes), or in some ports by pressing a
designated hot key.
The predictive input dialog operates in three modes, switchable by the
(*)Pre/123/Abc button. The primary input method is the predictive mode
(Pre) which resembles the way ""fast typing"" is performed at phones. The
alphabet is divided into 9 sets which naturally map to the 9 number keys
of the numeric keypad (0 is space). To type in a word, you press once
the number of the set which contains the letter of the word you intend
to type, then move on to the next. For example, to type the command
look, you should press 5665. As you gradually type the intended word's
numeric code, the dictionary is accessed for known words matching your
input up to that point. As you press more keys, the prediction converges
to the correct word. This is why the printed word may change
dramatically between key presses. There exist situations though where
more than one words share the same numeric representation. For example
the words quit and suit map to the same number, namely 7848. In
these cases the (#)next button lights up. By pressing it, you can cycle
through the list of words sharing the same code and finally accept the
correct one by pressing (0)space or the Ok button.
The second input method (123) is the numeric input: Each key you press
is entered verbatim as a number.
The third input method (Abc) is the Multi-tap Alpha input mode. This
mode is intended for entering free text, without assistance from the
dictionary scheme of predictive (Pre) mode. The text is entered one
letter at the time. For each letter first press the number of the set
which contains the letter you want, then use the (#)next button to
cycle through the letters and repeat with another number. For example,
to enter the word look you must press the following:
5##6##6##5#
The dialog is fully usable with the mouse, but a few provisions have
been made in some ScummVM ports to make its use more comfortable by
naturally mapping the functionality to the numeric keypad. Also, the
dialog's buttons can be navigated with the arrow and the enter keys.
3.25) Sierra SCI games: Simultaneous speech and subtitles
Certain CD versions of Sierra SCI games had both speech and text
resources. Some have an option to toggle between the two, but there are
some cases where there wasn't any option to enable both simultaneously.
In ScummVM, it is possible to enjoy a combined mode, where both speech
and text are shown at the same time. This mode can be toggled in the
ScummVM audio options, but each game has different behavior in-game
regarding speech and text toggling.
The CD games where speech and subtitles can be shown simultaneously are:

EcoQuest 1 CD
Freddy Pharkas CD
Gabriel Knight CD
King's Quest 6 CD
King's Quest VII CD
Laura Bow 2 CD
Leisure Suit Larry 6 CD
Leisure Suit Larry 6 (hires) CD
Police Quest 4 CD
Shivers CD
Space Quest 4 CD
Space Quest 6 CD Torin's Passage CD

EcoQuest 1 CD: Speech and text can be toggled via the game's ""Mode""
option in the options dialog, or via ScummVM's audio options.
Freddy Pharkas CD: There is no in-game option to toggle speech and
text. Only ScummVM's audio options can be used to toggle this feature.
Note that some spoken dialog is missing from the game texts.
Gabriel Knight CD: Speech and text can be toggled via the ""Text"" and
""Voice"" buttons in the game's settings dialog, or via ScummVM's audio
options.
King's Quest 6 CD: Speech and text can be toggled via the ""Mode""
button in the options dialog (with an extra ""Dual"" setting added in
ScummVM), or via ScummVM's audio options.
King's Quest VII CD: There is no in-game option to toggle speech and
text. Only ScummVM's audio options can be used to toggle this feature.
Note that the subtitles were disabled in the official release of this
game, so some subtitles may be wrong or missing.
Laura Bow 2 CD: Speech and text can be toggled via the ""Mode"" button
in the options dialog (with an extra ""Dual"" setting added in ScummVM),
or via ScummVM's audio options.
Leisure Suit Larry 6 CD: Either speech only or speech and text can
be selected. There is no in-game option to toggle text only. Only
ScummVM's audio options can be used to enable the text only mode.
Leisure Suit Larry 6 (hires) CD: Text can be toggled by selecting
the ""Text On/Off"" option from the in-game ""Game"" menu, or via ScummVM's
audio options. Speech cannot be disabled.
Police Quest 4 CD: Either speech only or text only can be selected
from the game's settings dialog. Only ScummVM's audio options can be
used to enable text+speech mode.
Shivers CD: Text can be toggled by selecting the ""Text"" option from
the game's settings dialog, or via ScummVM's audio options. Note that
only videos have subtitles in this game.
Space Quest 4 CD: Speech and text can be toggled via the ""Display
Mode"" button in the options dialog, or via ScummVM's audio options.
Space Quest 6 CD: Speech and text can be toggled via the ""Speech""
and ""Text"" buttons in the game's settings dialog, or via ScummVM's audio
options.
Torin's Passage CD: Text can be toggled by selecting ""Closed
Captioning"" from the in-game ""Game"" menu. Speech can be disabled by
selecting ""Audio Mixer"" from the in-game ""Game"" menu and setting the
speech volume to zero.
3.26) Zork games notes
To run the supported Zork games (Zork Nemesis: The Forbidden Lands and
Zork: Grand Inquisitor) you need to copy some (extra) data to its
corresponding destination.
3.26.1) Zork Nemesis: The Forbidden Lands
Download the Liberation(tm) fonts package
https://releases.pagure.org/liberation-fonts/liberation-fonts-ttf-2.00.1.tar.gz
and unpack all the ttf files into your ScummVM extras directory.
Alternatively, download the GNU FreeFont TTF package
https://ftp.gnu.org/gnu/freefont/freefont-ttf.zip and unzip all the
ttf files from the sfd directory into your ScummVM extras directory,
though at the time of writing these fonts cause some text rendering
issues. Download the subtitles patch
https://www.thezorklibrary.com/installguides/znpatch.zip and unzip the
addon directory into the game root directory
3.26.2) Zork: Grand Inquisitor
Download the Liberation(tm) fonts package
https://releases.pagure.org/liberation-fonts/liberation-fonts-ttf-2.00.1.tar.gz
and unpack all the ttf files into your ScummVM extras directory.
Alternatively, download the GNU FreeFont TTF package
https://ftp.gnu.org/gnu/freefont/freefont-ttf.zip and unzip all the
ttf files from the sfd directory into your ScummVM extras directory,
though at the time of writing these fonts cause some text rendering
issues.
3.27) Commodore64 games notes
Both Maniac Mansion and Zak McKracken run but Maniac Mansion is not yet
playable. Simply name the D64 disks ""maniac1.d64"" and ""maniac2.d64""
respectively ""zak1.d64"" and ""zak2.d64"", then ScummVM should be able to
automatically detect the game if you point it at the right directory.
Alternatively, you can use extract_mm_c64 from the tools package to
extract the data files. But then the game will not be properly
autodetected by ScummVM, and you must make sure that the platform is set
to Commodore64. We recommend using the much simpler approach described
in the previous paragraph.
3.28) Macintosh games notes
All LucasArts SCUMM based adventures, except COMI, also exist in
versions for the Macintosh. ScummVM can use most (all?) of them,
however, in some cases some additional work is required. First off, if
you are not using a Macintosh for this, accessing the CD/floppy data
might be tricky. The reason for this is that the mac uses a special disk
format called HFS which other systems usually do not support. However,
there are various free tools which allow reading such HFS volumes. For
example HFSExplorer for Windows and hfsutils for Linux and other
Unix-like operating systems.
Most of the newer games on the Macintosh shipped with only a single data
file (note that in some cases this data file was made invisible, so you
may need extra tools in order to copy it). ScummVM is able to directly
use such a data file; simply point ScummVM at the directory containing
it, and it should work (just like with every other supported game).
We also provide a tool called extract_scumm_mac in the tools package
to extract the data from these data files, but this is neither required
nor recommended.
For further information on copying Macintosh game files to your hard
disk see:
https://wiki.scummvm.org/index.php/HOWTO-Mac_Games
4.0) Supported Platforms
ScummVM has been ported to run on many platforms and operating systems.
Links to these ports can be found either on the ScummVM web page or by a
Google search. Many thanks to our porters for their efforts. If you have
a port of ScummVM and wish to commit it into the master git, feel free
to contact us!
Supported platforms include (but are not limited to):

UNIX (Linux, Solaris, IRIX, *BSD, ...)
Windows
Windows CE
Windows Mobile (including Smartphones and PocketPCs)
Mac OS X
AmigaOS
Android
Atari/FreeMiNT
BeOS
Dreamcast
GP2x
Haiku
iPhone (also includes iPod Touch and iPad)
Maemo (Nokia Internet tablet N810)
Nintendo 64
Nintendo DS
Nintendo GameCube
Nintendo Wii
OpenPandora
OS/2
PlayStation 2
PlayStation 3
PlayStation Portable
PlayStation Vita
Raspberry Pi
RISC OS
Symbian
WebOS

The Dreamcast port does not support The Curse of Monkey Island, nor The
Dig. The Nintendo DS port does not support Full Throttle, The Dig, or
The Curse of Monkey Island. For more platform specific limitations,
please refer to our Wiki:
https://wiki.scummvm.org/index.php/Platforms
In the Macintosh port, the right mouse button is emulated via Cmd-Click
(that is, you click the mouse button while holding the
Command/Apple/Propeller key).
There are unofficial ports to a variety of platforms, including the
Xbox, and Xbox 360. Please note that these are not made
by us, so we neither endorse nor can we support them. Use at your own
risk!
5.0) Running ScummVM
Please note that by default, ScummVM will save games in the directory it
is executed from, so you should refrain from running it from more than
one location. Further information, including how to specify a specific
save directory to avoid this issue, are in section 6.0.
ScummVM can be launched directly by running the executable. In this
case, the built-in launcher will activate. From this, you can add games
(click 'Add Game'), or launch games which have already been configured.
Games can also be added in mass quantities. By pressing shift + 'Add
Game' (Note that the image turns to 'Mass Add'), you can then specify a
directory to start in, and ScummVM will attempt to detect games in all
subdirectories of that directory.
ScummVM can also be launched into a game directly using Command Line
arguments -- see the next section.
5.1) Command Line Options
Usage: scummvm [OPTIONS]... [GAME]

[GAME]                   Short name of game to load. For example, 'monkey'
                          for Monkey Island. This can be either a built-in
                          gameid, or a user configured target.

-v, --version            Display ScummVM version information and exit
-h, --help               Display a brief help text and exit
-z, --list-games         Display list of supported games and exit
-t, --list-targets       Display list of configured targets and exit
--list-saves             Display a list of saved games for the target specified
                          with --game=TARGET, or all targets if none is specified
-a, --add                Add all games from current or specified directory.
                          If --game=ID is passed only the game with id ID is
                          added. See also --detect.
                          Use --path=PATH to specify a directory.
--detect                 Display a list of games with their ID from current or
                          specified directory without adding it to the config.
                          Use --path=PATH to specify a directory.
--game=ID                In combination with --add or --detect only adds or attempts to
                          detect the game with id ID.
--auto-detect            Display a list of games from current or specified directory
                          and start the first one. Use --path=PATH to specify
                          a directory.
--recursive              In combination with --add or --detect recurse down all
                          subdirectories
--console                Enable the console window (default: enabled) (Windows only)

-c, --config=CONFIG      Use alternate configuration file
-p, --path=PATH          Path to where the game is installed
-x, --save-slot[=NUM]    Saved game slot to load (default: autosave)
-f, --fullscreen         Force full-screen mode
-F, --no-fullscreen      Force windowed mode
-g, --gfx-mode=MODE      Select graphics scaler (see also section 5.3)
--stretch-mode=MODE      Select stretch mode (center, integral, fit, stretch)
--filtering              Force filtered graphics mode
--no-filtering           Force unfiltered graphics mode


--gui-theme=THEME        Select GUI theme (default, modern, classic)
--themepath=PATH         Path to where GUI themes are stored
--list-themes            Display list of all usable GUI themes
-e, --music-driver=MODE  Select music driver (see also section 7.0)
--list-audio-devices     List all available audio devices
-q, --language=LANG      Select game's language (see also section 5.5)
-m, --music-volume=NUM   Set the music volume, 0-255 (default: 192)
-s, --sfx-volume=NUM     Set the sfx volume, 0-255 (default: 192)
-r, --speech-volume=NUM  Set the voice volume, 0-255 (default: 192)
--midi-gain=NUM          Set the gain for MIDI playback, 0-1000 (default: 100)
                          (only supported by some MIDI drivers)
-n, --subtitles          Enable subtitles (use with games that have voice)
-b, --boot-param=NUM     Pass number to the boot script (boot param)
-d, --debuglevel=NUM     Set debug verbosity level
--debugflags=FLAGS       Enable engine specific debug flags
                          (separated by commas)
-u, --dump-scripts       Enable script dumping if a directory called 'dumps'
                          exists in the current directory

--cdrom=NUM              CD drive to play CD audio from (default: 0 = first
                          drive)
--joystick[=NUM]         Enable joystick input (default: 0 = first joystick)
--platform=WORD          Specify platform of game (allowed values: 2gs, 3do,
                          acorn, amiga, atari, c64, fmtowns, mac, nes, pc,
                          pce, segacd, windows)
--savepath=PATH          Path to where saved games are stored
--extrapath=PATH         Extra path to additional game data
--soundfont=FILE         Select the SoundFont for MIDI playback (Only
                          supported by some MIDI drivers)
--multi-midi             Enable combination of AdLib and native MIDI
--native-mt32            True Roland MT-32 (disable GM emulation)
--enable-gs              Enable Roland GS mode for MIDI playback
--output-rate=RATE       Select output sample rate in Hz (e.g. 22050)
--opl-driver=DRIVER      Select AdLib (OPL) emulator (db, mame, nuked)
--aspect-ratio           Enable aspect ratio correction
--render-mode=MODE       Enable additional render modes (hercGreen, hercAmber,
                          cga, ega, vga, amiga, fmtowns, pc9821, pc9801, 2gs,
                          atari, macintosh)

--alt-intro              Use alternative intro for CD versions of Beneath a
                          Steel Sky and Flight of the Amazon Queen
--copy-protection        Enable copy protection in games, when
                          ScummVM disables it by default.
--talkspeed=NUM          Set talk delay for SCUMM games, or talk speed for
                          other games (default: 60)
--demo-mode              Start demo mode of Maniac Mansion (Classic version)
--tempo=NUM              Set music tempo (in percent, 50-200) for SCUMM games
                          (default: 100)

The meaning of most long options (that is, those options starting with a
double-dash) can be inverted by prefixing them with ""no-"". For example,
--no-aspect-ratio will turn aspect ratio correction off. This is
useful if you want to override a setting in the configuration file.
The short game name ('game target') you see at the end of the command
line specifies which game is started. It either corresponds to an
arbitrary user defined target (from the configuration file), or to a
built-in gameid. A brief list of the latter can be found in section 3.0.
Examples:


Win32:
Running Monkey Island, fullscreen, from a hard disk:
C:\Games\LucasArts\scummvm.exe -f -pC:\Games\LucasArts\monkey\ monkey
Running Full Throttle from CD, fullscreen and with subtitles
enabled:
C:\Games\LucasArts\scummvm.exe -f -n -pD:\resource\ ft


Unix:
Running Monkey Island, fullscreen, from a hard disk:
/path/to/scummvm -f -p/games/LucasArts/monkey/ monkey
Running Full Throttle from CD, fullscreen and with subtitles
enabled:
/path/to/scummvm -f -n -p/cdrom/resource/ ft


5.2) Global Menu
The Global Menu is a general menu which is available to all of the game
engines by pressing Ctrl-F5. From this menu there are the following
buttons: Resume, Options, About, Return to Launcher, and Quit. Selecting
Options will display a dialog where basic audio settings, such as
volume levels, can be adjusted. Selecting 'Return to Launcher' will
close the current game and return the user back to the ScummVM Launcher,
where another game may be selected to play.
Note: Returning to the Launcher is not supported by all of the engines,
and the button will be disabled in the Global Menu if it is not
supported.
Engines which currently support returning to the Launcher are:
AGI
AGOS
CINE
COMPOSER
CRUISE
DRACI
DRASCULA
GOB
GROOVIE
HUGO
KYRA
LURE
MADE
MOHAWK
PARALLACTION
QUEEN
SAGA
SCI
SCUMM
SKY
SWORD1
SWORD2
TEENAGENT
TITANIC
TOUCHE
TSAGE
TUCKER
ZVISION

5.3) Graphics filters
ScummVM offers several anti-aliasing filters to attempt to improve
visual quality. These are the same filters used in many other emulators,
such as MAME. These filters take the original game graphics, and scale
it by a certain fixed factor (usually 2x or 3x) before displaying them
to you. So for example, if the game originally run at a resolution of
320x200 (typical for most of the SCUMM games), then using a filter with
scale factor 2x will effectively yield 640x400 graphics. Likewise with a
3x filter you will get 960x600.
They are:
1x         - No filtering, no scaling. Fastest.
2x         - No filtering, factor 2x (default for non 640x480 games).
3x         - No filtering, factor 3x.
2xsai      - 2xSAI filter, factor 2x.
super2xsai - Enhanced 2xSAI filtering, factor 2x.
supereagle - Less blurry than 2xSAI, but slower. Factor 2x.
advmame2x  - Doesn't rely on blurring like 2xSAI, fast. Factor 2x.
advmame3x  - Doesn't rely on blurring like 2xSAI, fast. Factor 3x.
hq2x       - Very nice high quality filter but slow. Factor 2x.
hq3x       - Very nice high quality filter but slow. Factor 3x.
tv2x       - Interlace filter, tries to emulate a TV. Factor 2x.
dotmatrix  - Dot matrix effect. Factor 2x.

To select a graphics filter, select it in the Launcher, or pass its name
via the '-g' option to scummvm, for example:
scummvm -gadvmame2x monkey2

Note #1: Not all backends support all (or even any) of the filters
listed above; some may support additional ones. The filters listed above
are those supported by the default SDL backend.
Note #2: Filters can be very slow when ScummVM is compiled in a debug
configuration without optimizations. And there is always a speed impact
when using any form of anti-aliasing/linear filtering.
Note #3: The FM-TOWNS version of Zak McKracken uses an original
resolution of 320x240, hence for this game scalers will scale to 640x480
or 960x720. Likewise, games that originally were using 640x480 (such as
Curse of Monkey Island or Broken Sword) will be scaled to 1280x960 and
1920x1440.
5.4) Hotkeys
ScummVM supports various in-game hotkeys. They differ between SCUMM
games and other games.
  Common:
    Ctrl-F5                - Displays the Global Menu
    Cmd-q                  - Quit (Mac OS X)
    Ctrl-q                 - Quit (other unices including Linux)
    Alt-F4                 - Quit (Windows)
    Ctrl-z                 - Quit (other platforms)
    Ctrl-u                 - Mute all sounds
    Ctrl-m                 - Toggle mouse capture
    Ctrl-Alt 1-8           - Switch between graphics filters
    Ctrl-Alt + and -       - Increase/Decrease the scale factor
    Ctrl-Alt a             - Toggle aspect-ratio correction on/off
                             Most of the games use a 320x200 pixel
                             resolution, which may look squashed on
                             modern monitors. Aspect-ratio correction
                             stretches the image to use 320x240 pixels
                             instead, or a multiple thereof
    Ctrl-Alt f             - Enable/disable graphics filtering
    Ctrl-Alt s             - Cycle through scaling modes
    Alt-Enter              - Toggles full screen/windowed
    Alt-s                  - Make a screenshot (SDL backend only)
    Ctrl-F7                - Open virtual keyboard (if enabled)
                             This can also be triggered by a long press
                             of the middle mouse button or wheel.

  SCUMM:
    Alt-x                  - Quit
    Ctrl 0-9 and Alt 0-9   - Load and save game state
    Ctrl-d                 - Starts the debugger
    Ctrl-f                 - Toggle fast mode
    Ctrl-g                 - Runs in really REALLY fast mode
    Ctrl-t                 - Switch between 'Speech only',
                             'Speech and Subtitles' and 'Subtitles only'
    Tilde (~)              - Show/hide the debugging console
    [ and ]                - Music volume, down/up
    - and +                - Text speed, slower/faster
    F5                     - Displays a save/load box
    Alt-F5                 - Displays the original save/load box, if the
                             game has one. You can save and load games using
                             this, however it is not intended for this purpose,
                             and may even crash ScummVM in some games.
    i                      - Displays IQ points (Indiana Jones and the Last
                             Crusade, and Indiana Jones and the Fate of
                             Atlantis)
    Space                  - Pauses
    Period (.)             - Skips current line of text in some games
    Enter                  - Simulate left mouse button press
    Tab                    - Simulate right mouse button press

  Beneath a Steel Sky:
    Ctrl-d                 - Starts the debugger
    Ctrl-f                 - Toggle fast mode
    Ctrl-g                 - Runs in really REALLY fast mode
    F5                     - Displays a save/load box
    Escape                 - Skips the game intro
    Period (.)             - Skips current line of text

  Broken Sword:
    F5 or Escape           - Displays save/load box

  Broken Sword II:
    Ctrl-d                 - Starts the debugger
    Ctrl-f                 - Toggle fast mode
    p                      - Pauses

  Dragon History:
    F5                     - Displays the Global Menu
    left click             - Walk, explore
    right click            - Use, talk
    move mouse up, i       - Inventory
    move mouse down, m     - Map
    Escape                 - Skip the intro, exit map/inventory
    any click              - Skip the currently dubbed sentence
    q                      - Quick walking on/off

  Flight of the Amazon Queen:
    Ctrl-d                 - Starts the debugger
    Ctrl-f                 - Toggle fast mode
    F1                     - Use Journal (saving/loading)
    F11                    - Quicksave
    F12                    - Quickload
    Escape                 - Skips cutscenes
    Space                  - Skips current line of text

  Future Wars:
    F1                     - Examine
    F2                     - Take
    F3                     - Inventory
    F4                     - Use
    F5                     - Activate
    F6                     - Speak
    F9                     - ""Activate"" menu
    F10                    - ""Use"" menu
    Escape                 - Bring on command menu

  Nippon Safes:
    Ctrl-d                 - Starts the debugger
    l                      - Load game
    s                      - Save game

  Simon the Sorcerer 1 and 2:
    Ctrl 0-9 and Alt 0-9   - Load and save game state
    Ctrl-d                 - Starts the debugger
    Ctrl-f                 - Toggle fast mode
    F1 - F3                - Text speed, faster - slower
    F10                    - Shows all characters and objects you can
                             interact with
    Escape                 - Skip cutscenes
    - and +                - Music volume, down/up
    m                      - Music on/off
    s                      - Sound effects on/off
    b                      - Background sounds on/off
                             [Simon the Sorcerer 2 only]
    Pause                  - Pauses
    t                      - Switch between speech only and
                             combined speech and subtitles
                             [Simon the Sorcerer 1 CD (other than
                             English and German) and Simon the
                             Sorcerer 2 CD (all languages)]
    v                      - Switch between subtitles only and
                             combined speech and subtitles
                             [Simon the Sorcerer 2 CD only]

  Simon the Sorcerer's Puzzle Pack:
    Ctrl-d                 - Starts the debugger
    Ctrl-f                 - Toggle fast mode
    F12                    - High speed mode on/off in Swampy Adventures
    - and +                - Music volume, down/up
    m                      - Music on/off
    s                      - Sound effects on/off
    Pause                  - Pauses

  Starship Titanic:    
    Ctrl-c                 - Open up the developer's cheat room
    Ctrl-d                 - Open up the ScummVM Debugger
    Left click             - Move action
    Shift-Left click       - Edit room glyph chevrons and 
                              quick movement transitions
    Right click            - Edit room glyph chevrons 
                              and quick transitions
    Mouse wheel            - Scroll through items (inventory, etc) 
                              and conversation log
    Arrow keys             - Movement. Down arrow/back is only available if the
                              given view explicitly has a backwards movement 
                              available.        
    F1                     - Switch to Chat-O-Mat
    F2                     - Switch to Personal Baggage
    F3                     - Switch to Remote Thingummy
    F4                     - Switch to Designer Room Numbers (chevron list)
    F5                     - GMM save menu
    F6                     - Switch to Real Life
    F7                     - GMM restore menu

  Starship Titanic (Starfield Puzzle):
    Tab                    - Toggle between starmap and skyscape
    Mouse click:           - skyscape star selection and
                              starmap star fast travel
    Mouse movement         - starmap orientation
    SPACE                  - starmap stop movement
    z                      - starmap turn left
    x                      - starmap turn right
    Single quote (')       - starmap turn up
    Forward slash (/)      - starmap turn down
    Semicolon (;)          - starmap move forward
    Period (.)             - starmap move backward
    l                      - starmap lock coordinate
    d                      - starmap unlock coordinate
    
  The Feeble Files:
    Ctrl-d                 - Starts the debugger
    Ctrl-f                 - Toggle fast mode
    F7                     - Switch characters
    F9                     - Hitbox names on/off
    s                      - Sound effects on/off
    Pause                  - Pauses
    t                      - Switch between speech only and
                             combined speech and subtitles
    v                      - Switch between subtitles only and
                             combined speech and subtitles

  The Legend of Kyrandia:
    Ctrl 0-9 and Alt 0-9   - Load and save game state
    Ctrl-d                 - Starts the debugger

  TeenAgent
    F5                     - Displays the Global Menu

  Touche: The Adventures of the Fifth Musketeer:
    Ctrl-f                 - Toggle fast mode
    F5                     - Displays options
    F9                     - Turn fast walk mode on
    F10                    - Turn fast walk mode off
    Escape                 - Quit
    Space                  - Skips current line of text
    t                      - Switch between 'Voice only',
                             'Voice and Text' and 'Text only'

  Zork: Grand Inquisitor:
    Ctrl-s                 - Save
    Ctrl-r                 - Restore
    Ctrl-q                 - Quit
    Ctrl-p                 - Preferences
    F1                     - Help
    F5                     - Inventory
    F6                     - Spellbook
    F7                     - Score
    F8                     - Put away current object/forget spell
    F9                     - Extract coin (must have the coin bag)
    Space                  - Skips movies

  Zork Nemesis: The Forbidden Lands:
    Ctrl-s                 - Save
    Ctrl-r                 - Restore
    Ctrl-q                 - Quit
    Ctrl-p                 - Preferences
    Space                  - Skips movies

Note that using Ctrl-f or Ctrl-g is not recommended: games can crash
when being run faster than their normal speed, as scripts will lose
synchronisation.
Note for WinCE users: Due to the limited keyboard input in most devices,
a small subset of these hot keys are supported via key remapping and/or
panel actions. Please consult the README-WinCE.txt file.
5.5) Language options
ScummVM includes a language option for Maniac Mansion, Zak McKracken,
The Dig, The Curse of Monkey Island, Beneath a Steel Sky and Broken
Sword.
Note that with the exception of Beneath a Steel Sky, Broken Sword,
multilanguage versions of Goblins games and Nippon Safes Inc., using
this option does not change the language of the game (which usually is
hardcoded), but rather is only used to select the appropriate font (e.g.
for a German version of a game, one containing umlauts).
An exception are The Dig and The Curse of Monkey Island -- non-English
versions can be set to 'English.' This however only affects subtitles;
game speech will remain the same.
Maniac Mansion and Zak McKracken
    en  - English (default)
    de  - German
    fr  - French
    it  - Italian
    es  - Spanish

The Dig
    jp  - Japanese
    zh  - Chinese
    kr  - Korean

The Curse of Monkey Island
    en  - English (default)
    de  - German
    fr  - French
    it  - Italian
    pt  - Portuguese
    es  - Spanish
    jp  - Japanese
    zh  - Chinese
    kr  - Korean

Beneath a Steel Sky
    gb  - English (Great Britain) (default)
    en  - English (USA)
    de  - German
    fr  - French
    it  - Italian
    pt  - Portuguese
    es  - Spanish
    se  - Swedish

Broken Sword
    en  - English (default)
    de  - German
    fr  - French
    it  - Italian
    es  - Spanish
    pt  - Portuguese
    cz  - Czech

6.0) Saved Games
Saved games are by default put in the current directory on some
platforms and preset directories on others. You can specify the save in
the config file by setting the savepath parameter. See the example
config file later in this README.
The platforms that currently have a different default directory are:
Mac OS X:
$HOME/Documents/ScummVM Savegames/
Other unices:
We follow the XDG Base Directory Specification. This means by default
saved games can be found in: $XDG_DATA_HOME/scummvm/saves/
If XDG_DATA_HOME is not defined or empty, ~/.local/share will be
used as value of XDG_DATA_HOME in accordance with the specification.
If an earlier version of ScummVM was installed on your system, the
previous default location of ~/.scummvm will be kept. This is detected
based on the presence of the path ~/.scummvm.
Windows Vista/7:
\Users\username\AppData\Roaming\ScummVM\Saved games\
Windows 2000/XP:
\Documents and Settings\username\Application Data\ScummVM\Saved games\
Windows NT4:
<windir>\Profiles\username\Application Data\ScummVM\Saved games\
Saved games are stored under a hidden area in Windows
NT4/2000/XP/Vista/7, which can be accessed by running
%APPDATA%\ScummVM\Saved Games or by enabling hidden files in Windows
Explorer.
Note for Windows NT4/2000/XP/Vista/7 users: The default saved games
location changed in ScummVM 1.5.0. The migration batch file can be used
to copy saved games from the old default location, to the new default
location.
6.1) Autosaves
For some games ScummVM will by default automatically save the current
state every five minutes (adjustable via the autosave_period config
setting). The default autosave slot for many engines is slot 0.
The games/engines listed below have autosave support.

AGI games
Beneath a Steel Sky
Bud Tucker in Double Trouble
COMPOSER games
Flight of the Amazon Queen
Myst
Riven
SCUMM games
The Legend of Kyrandia I (slot 999)
ZVISION games

For the SCUMM engine, this saved game can then be loaded again via
Ctrl-0, or the F5 menu.
6.2) Converting Saved Games
Using saved games from original versions isn't supported by all game
engines. Only the following games can use saved games from their
original versions.


Elvira 1

Add 8 bytes (saved game name) to the start of the saved game
file
Rename the saved game to elvira1.xxx



Elvira 2

Add 8 bytes (saved game name) to the start of the saved game
file
Rename the saved game to elvira2-pc.xxx (DOS version) or
elvira2.xxx (Other versions)



Myst

Rename the saved game to myst-xxx.mys
Saves from the masterpiece edition and the regular edition are
interchangeable



Riven

Rename the saved game to riven-xxx.rvn
Saves from the CD and DVD edition are not interchangeable



Simon the Sorcerer 1

Rename the saved game to simon1.xxx



Simon the Sorcerer 2

Rename the saved game to simon2.xxx



Starship Titanic

Rename the saved game to titanic-win.xxx for saves from the
English version and titanic-win-de.xxx for saves from the
German version
Saved games between different languages are not interchangeable



The Feeble Files

Rename the saved game to feeble.xxx



Waxworks

Add 8 bytes (saved game name) to the start of the saved game
file
Rename the saved game to waxworks-pc.xxx (DOS version) or
waxworks.xxx (Other versions)



Where xxx is exact the saved game slot (i.e., 001) under ScummVM
6.3) Viewing/Loading saved games from the command line
--list-saves
This switch may be used to display a list of the current saved games of
the specified target game and their corresponding save slots. If no
target is specified, it lists saved games for all known target.
Usage: --list-saves --game=[TARGET], where [TARGET] is the target
game.
Engines which currently support --list-saves are:

AGI
AGOS
CGE
CINE
CRUISE
DRACI
GROOVIE
HUGO
KYRA
LURE
MOHAWK
PARALLACTION
QUEEN
SAGA
SCI
SCUMM
SKY
SWORD1
SWORD2
TEENAGENT
TINSEL
TITANIC
TOON
TOUCHE
TSAGE
TUCKER
ZVISION

--save-slot/-x
This switch may be used to load a saved game directly from the command
line.
Usage: --save-slot[SLOT] or -x[SLOT], where [SLOT] is the save
slot number.
Engines which currently support --save-slot / -x are:

AGI
CGE
CINE
CRUISE
DRACI
GROOVIE
HUGO
KYRA
LURE
MOHAWK
QUEEN
SAGA
SCI
SCUMM
SKY
SWORD1
SWORD2
TEENAGENT
TINSEL
TITANIC
TOON
TOUCHE
TSAGE
TUCKER
ZVISION

7.0) Music and Sound
On most operating systems and for most games, ScummVM will by default
use MT-32 or AdLib emulation for music playback. MIDI may not be
available on all operating systems or may need manual configuration. If
you want to use MIDI, you have several different choices of output,
depending on your operating system and configuration.
null       - Null output. Don't play any music.

adlib      - Internal AdLib emulation
fluidsynth - FluidSynth MIDI emulation
mt32       - Internal MT-32 emulation
pcjr       - Internal PCjr emulation (only usable in SCUMM games)
pcspk      - Internal PC Speaker emulation
towns      - Internal FM-TOWNS YM2612 emulation
             (only usable in SCUMM FM-TOWNS games)

alsa       - Output using ALSA sequencer device. See below.
core       - CoreAudio sound, for Mac OS X users.
coremidi   - CoreMIDI sound, for Mac OS X users. Use only if you have
             a hardware MIDI synthesizer.
seq        - Use /dev/sequencer for MIDI, *nix users. See below.
timidity   - Connect to TiMidity++ MIDI server. See below.
windows    - Windows MIDI. Uses built-in sequencer, for Windows users

To select a sound driver, select it in the Launcher, or pass its name
via the -e option to scummvm, for example:
scummvm -eadlib monkey2
7.1) AdLib emulation
By default an AdLib card will be emulated and ScummVM will output the
music as sampled waves. This is the default mode for several games, and
offers the best compatibility between machines and games.
7.2) FluidSynth MIDI emulation
If ScummVM was build with libfluidsynth support it will be able to play
MIDI music through the FluidSynth driver. You will have to specify a
SoundFont to use, however.
Since the default output volume from FluidSynth can be fairly low,
ScummVM will set the gain by default to get a stronger signal. This can
be further adjusted using the --midi-gain command-line option, or the
midi_gain config file setting.
The setting can take any value from 0 through 1000, with the default
being 100. (This corresponds to FluidSynth's gain settings of 0.0
through 10.0, which are presumably measured in decibel.)
NOTE: The processor requirements for FluidSynth can be fairly high in
some cases. A fast CPU is recommended.
7.3) MT-32 emulation
Some games which contain MIDI music data also have improved tracks
designed for the MT-32 sound module. ScummVM can now emulate this
device, however you must provide original MT-32 ROMs to make it work:
MT32_PCM.ROM - IC21 (512KB)
MT32_CONTROL.ROM - IC26 (32KB) and IC27 (32KB), interleaved byte-wise
Place these ROMs in the game directory, in your extrapath, or in the
directory where your ScummVM executable resides.
You don't need to specify --native-mt32 with this driver, as it
automatically gets turned on.
NOTE: The processor requirements for the emulator are quite high; a fast
CPU is strongly recommended.
7.4) MIDI emulation
Some games (such as Sam & Max) only contain MIDI music data. This once
prevented music for these games from working on platforms that do not
support MIDI, or soundcards that do not provide MIDI drivers (e.g. many
soundcards will not play MIDI under Linux). ScummVM can now emulate MIDI
mode using sampled waves and AdLib, FluidSynth MIDI emulation or MT-32
emulation using the -eadlib, -efluidsynth or -emt32 options
respectively. However, if you are capable of using native MIDI, we
recommend using one of the MIDI modes below for best sound.
7.5) Native MIDI support
Use the appropriate -e<mode> command line option from the list above
to select your preferred MIDI device. For example, if you wish to use
the Windows MIDI driver, use the -ewindows option.
7.5.1) Using MIDI options to customize Native MIDI output
ScummVM supports a variety of MIDI modes, depending on the capabilities
of your MIDI device.
If --native-mt32 is specified, ScummVM will treat your device as a
real MT-32. Because the instrument mappings and system exclusive
commands of the MT-32 vary from those of General MIDI devices, you
should only enable this option if you are using an actual Roland MT-32,
LAPC-I, CM-64, CM-32L, CM-500, or GS device with an MT-32 map.
If --enable-gs is specified, ScummVM will initialize your
GS-compatible device with settings that mimic the MT-32's reverb, (lack
of) chorus, pitch bend sensitivity, etc. If it is specified in
conjunction with --native-mt32, ScummVM will select the
MT-32-compatible map and drumset on your GS device. This setting works
better than default GM or GS emulation with games that do not have
custom instrument mappings (Loom and Monkey1). You should only specify
both settings if you are using a GS device that has an MT-32 map, such
as an SC-55, SC-88, SC-88 Pro, SC-8820, SC-8850, etc. Please note that
--enable-gs is automatically disabled in both DOTT and Samnmax, since
they use General MIDI natively.
If neither of the above settings is enabled, ScummVM will initialize
your device in General MIDI mode and use GM emulation in games with
MT-32 soundtracks.
Some games contain sound effects that are exclusive to the AdLib
soundtrack. For these games, you may wish to specify --multi-midi in
order to combine MIDI music with AdLib sound effects.
7.6) UNIX native, ALSA and dmedia sequencer support
If your soundcard driver supports a sequencer, you may set the
environment variable SCUMMVM_MIDI to your sequencer device -- for
example, to /dev/sequencer
If you have problems with not hearing audio in this configuration, you
may need to set the environment variable SCUMMVM_MIDIPORT to 1 or 2.
This selects the port on the selected sequencer to use. Then start
scummvm with the -eseq parameter. This should work on several cards,
and may offer better performance and quality than AdLib emulation.
However, for those systems where sequencer support does not work, you
can always fall back on AdLib emulation.
7.6.1) ALSA sequencer [UNIX ONLY]
If you have installed the ALSA driver with sequencer support, then you
may set the environment variable SCUMMVM_PORT or the config file
variable alsa_port to specify your sequencer port. If neither is set,
the default behavior is to try both ""65:0"" and ""17:0"".
Here is a brief guide on how to use the ALSA sequencer with your
soundcard. In all cases, to obtain a list of all the sequencer ports you
have, try the command aconnect -o -l. This should give output similar
to:
    client 14: 'Midi Through' [type=kernel]
        0 'Midi Through Port-0'
    client 16: 'SBLive! Value [CT4832]' [type=kernel]
        0 'EMU10K1 MPU-401 (UART)'
    client 17: 'Emu10k1 WaveTable' [type=kernel]
        0 'Emu10k1 Port 0  '
        1 'Emu10k1 Port 1  '
        2 'Emu10k1 Port 2  '
        3 'Emu10k1 Port 3  '
    client 128: 'TiMidity' [type=user]
        0 'TiMidity port 0 '
        1 'TiMidity port 1 '
        2 'TiMidity port 2 '
        3 'TiMidity port 3 '

The most important bit here is that there are four WaveTable MIDI
outputs located at 17:0, 17:1, 17:2 and 17:3, and four TiMidity ports
located at 128:0, 128:1, 128:2 and 128:3.
If you have a FM-chip on your card, like the SB16, then you have to load
the SoundFonts using the sbiload software. Example:
sbiload -p 17:0 /etc/std.o3 /etc/drums.o3

If you have a WaveTable capable sound card, you have to load a sbk or
sf2 SoundFont using the sfxload or asfxload software. Example:
sfxload /path/to/8mbgmsfx.sf2

If you don't have a MIDI capable soundcard, there are two options:
FluidSynth and TiMidity. We recommend FluidSynth, as on many systems
TiMidity will 'lag' behind music. This is very noticeable in
iMUSE-enabled games, which use fast and dynamic music transitions.
Running TiMidity as root will allow it to setup real time priority,
which may reduce music lag.
Asking TiMidity to become an ALSA sequencer:
timidity -iAqqq -B2,8 -Os1S -s 44100 &

(If you get distorted output with this setting, you can try dropping the
-B2,8 or changing the value.)
Asking FluidSynth to become an ALSA sequencer (using SoundFonts):
fluidsynth -m alsa_seq /path/to/8mbgmsfx.sf2

Once either TiMidity or FluidSynth are running, use the 'aconnect -o -l'
command as described earlier in this section.
7.6.2) IRIX dmedia sequencer: [UNIX ONLY]
If you are using IRIX and the dmedia driver with sequencer support, you
can set the environment variable SCUMMVM_MIDIPORT or the config file
variable dmedia_port to specify your sequencer port. The default is to
use the first port.
To get a list of configured midi interfaces on your system, run
""startmidi"" without parameters. Example output:
  2 MIDI interfaces configured:
          Serial Port 2
          Software Synth

In this example, you can configure ScummVM to use the ""Software Synth""
instead of the default ""Serial Port 2"" by adding a line
dmedia_port=Software Synth

to your configuration file in the section [scummvm], or setting
SCUMMVM_PORT=Software Synth in your environment.
7.7) TiMidity++ MIDI server support
If your system lacks any MIDI sequencer, but you still want better MIDI
quality than default AdLib emulation can offer, you can try the
TiMidity++ MIDI server. See http://timidity.sourceforge.net/ for
download and install instructions.
First, you need to start a daemon:
timidity -ir 7777

Now you can start ScummVM and try selection TiMidity music output. By
default, it will connect to localhost:7777, but you can change host/port
via the TIMIDITY_HOST environment variable. You can also specify a
""device number"" using the SCUMMVM_MIDIPORT environment variable.
7.8) Using compressed audio files
7.8.1) Using MP3 files for CD audio
Use LAME or some other MP3 encoder to rip the cd audio tracks to files.
Name the files track1.mp3 track2.mp3 etc. ScummVM must be compiled with
MAD support to use this option. You will need to rip the file from the
CD as a WAV file, then encode the MP3 files in constant bit rate. This
can be done with the following LAME command line:
lame -t -q 0 -b 96 track1.wav track1.mp3

7.8.2) Using Ogg Vorbis files for CD audio
Use oggenc or some other vorbis encoder to encode the audio tracks to
files. Name the files track1.ogg track2.ogg etc. ScummVM must be
compiled with vorbis support to use this option. You will need to rip
the files from the CD as a WAV file, then encode the vorbis files. This
can be done with the following oggenc command line with the value after
q specifying the desired quality from 0 to 10:
oggenc -q 5 track1.wav

7.8.3) Using Flac files for CD audio
Use flac or some other flac encoder to encode the audio tracks to files.
Name the files track1.flac track2.flac etc. If your filesystem only
allows three letter extensions, name the files track1.fla track2.fla
etc. ScummVM must be compiled with flac support to use this option. You
will need to rip the files from the CD as a WAV file, then encode the
flac files. This can be done with the following flac command line:
flac --best track1.wav

Remember that the quality is always the same, varying encoder options
will only affect the encoding time and resulting filesize.
7.8.4) Compressing MONSTER.SOU with MP3
You need LAME, and our compress_scumm_sou utility from the
scummvm-tools package to perform this task, and ScummVM must be compiled
with MAD support.
compress_scumm_sou monster.sou

Eventually you will have a much smaller monster.so3 file, copy this file
to your game directory. You can safely remove the monster.sou file.
7.8.5) Compressing MONSTER.SOU with Ogg Vorbis
As above, but ScummVM must be compiled with OGG support. Run:
compress_scumm_sou --vorbis monster.sou

This should produce a smaller monster.sog file, which you should copy to
your game directory. Ogg encoding may take a considerable longer amount
of time than MP3, so have a good book handy.
7.8.6) Compressing MONSTER.SOU with Flac
As above, but ScummVM must be compiled with Flac support. Run:
compress_scumm_sou --flac monster.sou

This should produce a smaller monster.sof file, which you should copy to
your game directory. Remember that the quality is always the same,
varying encoder options will only affect the encoding time and resulting
file size. Playing with the blocksize (-b <value>), has the biggest
impact on the resulting file size -- 1152 seems to be a good value for
those kind of soundfiles. Be sure to read the encoder documentation
before you use other values.
7.8.7) Compressing music/sfx/speech in AGOS games
Use our compress_agos utility from the scummvm-tools package to
perform this task. You can choose between multiple target formats, but
note that you can only use each if ScummVM was compiled with the
respective decoder support enabled.
  compress_agos effects     (For Acorn CD version of Simon 1)
  compress_agos simon       (For Acorn CD version of Simon 1)
  compress_agos effects.voc (For DOS CD version of Simon 1)
  compress_agos simon.voc   (For DOS CD version of Simon 1)
  compress_agos simon.wav   (For Windows CD version of Simon 1)
  compress_agos simon2.voc  (For DOS CD version of Simon 2)
  compress_agos simon2.wav  (For Windows CD version of Simon 2)
  compress_agos mac         (For Macintosh version of Simon 2)

  compress_agos voices1.wav (For Windows 2CD/4CD version of Feeble)
  compress_agos voices2.wav (For Windows 2CD/4CD version of Feeble)
  compress_agos voices3.wav (For Windows 4CD version of Feeble)
  compress_agos voices4.wav (For Windows 4CD version of Feeble)

  compress_agos Music       (For Windows version of Puzzle Pack)

For Ogg Vorbis add --vorbis to the options, i.e.
compress_agos --vorbis

For Flac add --flac and optional parameters, i.e.
compress_agos --flac

Eventually you will have a much smaller *.mp3, *.ogg or *.fla file,
copy this file to your game directory. You can safely remove the old
file.
7.8.8) Compressing speech/music in Broken Sword
The compress_sword1 tool from the scummvm-tools package can encode
music and speech to MP3, Ogg Vorbis as well as Flac. The easiest way to
encode the files is simply copying the executable into your BS1
directory (together with the lame encoder) and run it from there. This
way, it will automatically encode everything to MP3. Afterwards, you can
manually remove the SPEECH?.CLU files and the wave music files.
Running compress_sword1 --vorbis will compress the files using Ogg
Vorbis instead of MP3.
Running compress_sword1 --flac will compress the files using Flac
instead of MP3.
Use compress_sword1 --help to get a full list of the options.
7.8.9) Compressing speech/music in Broken Sword II
Use our compress_sword2 utility from the scummvm-tools package to
perform this task. You can choose between multiple target formats, but
note that you can only use each if ScummVM was compiled with the
respective decoder support enabled.
  compress_sword2 speech1.clu
  compress_sword2 music1.clu

For Ogg Vorbis add --vorbis to the options, i.e.
compress_sword2 --vorbis

Eventually you will have a much smaller *.cl3 or *.clg file, copy this
file to your game directory. You can safely remove the old file.
It is possible to use Flac compression by adding the --flac option.
However, the resulting *.clf file will actually be larger than the
original.
Please note that compress_sword2 will only work with the four
speech/music files in Broken Sword II. It will not work with any of the
other *.clu files, nor will it work with the speech files from Broken
Sword.
7.9) Output sample rate
The output sample rate tells ScummVM how many sound samples to play per
channel per second. There is much that could be said on this subject,
but most of it would be irrelevant here. The short version is that for
most games 22050 Hz is fine, but in some cases 44100 Hz is preferable.
On extremely low-end systems you may want to use 11025 Hz, but it is
unlikely that you have to worry about that.
To elaborate, most of the sounds ScummVM has to play were sampled at
either 22050 Hz or 11025 Hz. Using a higher sample rate will not
magically improve the quality of these sounds. Hence, 22050 Hz is fine.
Some games use CD audio. If you use compressed files for this, they are
probably sampled at 44100 Hz, so for these games that may be a better
choice of sample rate.
When using the AdLib, FM Towns, PC Speaker or IBM PCjr music drivers,
ScummVM is responsible for generating the samples. Usually 22050 Hz will
be plenty for these, but there is at least one piece of AdLib music in
Beneath a Steel Sky that will sound a lot better at 44100 Hz.
Using frequencies in between is not recommended. For one thing, your
sound card may not support it. In theory, ScummVM should fall back on a
sensible frequency in that case, but don't count on it. More
importantly, ScummVM has to resample all sounds to its output frequency.
This is much easier to do well if the output frequency is a multiple of
the original frequency.
8.0) Configuration file
By default, the configuration file is saved in, and loaded from:
Windows Vista/7:
\Users\username\AppData\Roaming\ScummVM\scummvm.ini
Windows 2000/XP:
\Documents and Settings\username\Application Data\ScummVM\scummvm.ini
Windows NT4:
<windir>\Profiles\username\Application Data\ScummVM\scummvm.ini
Windows 95/98/ME:
<windir>\scummvm.ini
If an earlier version of ScummVM was installed under Windows, the
previous default location of <windir>\scummvm.ini will be kept.
Unix:
We follow the XDG Base Directory Specification. This means our
configuration can be found in: $XDG_CONFIG_HOME/scummvm/scummvm.ini
If XDG_CONFIG_HOME is not defined or empty, ~/.config will be used
as value for XDG_CONFIG_HOME in accordance with the specification.
If an earlier version of ScummVM was installed on your system, the
previous default location of ~/.scummvmrc will be kept.
Mac OS X:
~/Library/Preferences/ScummVM Preferences (here, ~ refers to your
home directory)
Others:
scummvm.ini in the current directory
An example config file looks as follows:
    [scummvm]
    gfx_mode=supereagle
    fullscreen=true
    savepath=C:\saves\

    [sky]
    path=C:\games\SteelSky\

    [germansky]
    gameid=sky
    language=de
    path=C:\games\SteelSky\
    description=Beneath a Steel Sky w/ German subtitles

    [germandott]
    gameid=tentacle
    path=C:\german\tentacle\
    description=German version of DOTT

    [tentacle]
    path=C:\tentacle\
    subtitles=true
    music_volume=40
    sfx_volume=255

    [loomcd]
    cdrom=1
    path=C:\loom\
    talkspeed=5
    savepath=C:\loom\saves\

    [monkey2]
    path=C:\amiga_mi2\
    music_driver=windows

8.1) Recognized configuration keywords
The following keywords are recognized:
path               string   The path to where a game's data files are
autosave_period    number   The seconds between autosaving (default: 300)
save_slot          number   The saved game number to load on startup.
savepath           string   The path to where a game will store its
                            saved games.
screenshotpath     string   The path to where screenshots are saved.
iconpath           string   The path to where to look for icons to use as
                            overlay for the ScummVM icon in the Windows
                            taskbar or macOS X Dock when running a game.
                            The icon files should be named after the game
                            ids and be in ico format on Windows or png
                            format on macOS X.
versioninfo        string   The version of the ScummVM that created the
                            configuration file.

gameid             string   The real id of a game. Useful if you have
                            several versions of the same game, and want
                            different aliases for them. See the example.
description        string   The description of the game as it will appear
                            in the launcher.

language           string   Specify language (en, us, de, fr, it, pt, es,
                            jp, zh, kr, se, gb, hb, cz, ru)
speech_mute        bool     If true, speech is muted
subtitles          bool     Set to true to enable subtitles.
talkspeed          number   Text delay in SCUMM games, or text speed in
                            other games.

fullscreen         bool     Fullscreen mode
aspect_ratio       bool     Enable aspect ratio correction
gfx_mode           string   Graphics mode (normal, 2x, 3x, 2xsai,
                            super2xsai, supereagle, advmame2x, advmame3x,
                            hq2x, hq3x, tv2x, dotmatrix, opengl)
filtering          bool     Enable graphics filtering

confirm_exit       bool     Ask for confirmation by the user before
                            quitting (SDL backend only).
console            bool     Enable the console window (default: enabled)
                            (Windows only).
cdrom              number   Number of CD-ROM unit to use for audio. If
                            negative, don't even try to access the CD-ROM.
joystick_num       number   Number of joystick device to use for input
controller_map_db  string   A custom controller mapping file to load to
                            complete default database (SDL backend only).
                            Otherwise, file gamecontrollerdb.txt will be
                            loaded from extrapath.
music_driver       string   The music engine to use.
opl_driver         string   The AdLib (OPL) emulator to use.
output_rate        number   The output sample rate to use, in Hz. Sensible
                            values are 11025, 22050 and 44100.
audio_buffer_size  number   Overrides the size of the audio buffer. The
                            value must be one of: 256 512 1024 2048 4096
                            8192 16384 32768. The default value is
                            calculated based on the output_rate to keep
                            audio latency below 45ms.
alsa_port          string   Port to use for output when using the
                            ALSA music driver.
music_volume       number   The music volume setting (0-255)
multi_midi         bool     If true, enable combination AdLib and native
                            MIDI.
soundfont          string   The SoundFont to use for MIDI playback. (Only
                            supported by some MIDI drivers.)
native_mt32        bool     If true, disable GM emulation and assume that
                            there is a true Roland MT-32 available.
enable_gs          bool     If true, enable Roland GS-specific features to
                            enhance GM emulation. If native_mt32 is also
                            true, the GS device will select an MT-32 map
                            to play the correct instruments.
sfx_volume         number   The sfx volume setting (0-255)
tempo              number   The music tempo (50-200) (default: 100)
speech_volume      number   The speech volume setting (0-255)
midi_gain          number   The MIDI gain (0-1000) (default: 100) (Only
                            supported by some MIDI drivers.)

copy_protection    bool     Enable copy protection in certain games, in
                            those cases where ScummVM disables it by
                            default.
demo_mode          bool     Start demo in Maniac Mansion
alt_intro          bool     Use alternative intro for CD versions of
                            Beneath a Steel Sky and Flight of the Amazon
                            Queen

boot_param         number   Pass this number to the boot script

Sierra games using the AGI engine add the following non-standard keywords:
originalsaveload   bool     If true, the original save/load screens are
                            used instead of the enhanced ScummVM ones
altamigapalette    bool     Use an alternative palette, common for all
                            Amiga games. This was the old behavior
mousesupport       bool     Enables mouse support. Allows to use mouse
                            for movement and in game menus

Sierra games using the SCI engine add the following non-standard keywords:
disable_dithering  bool     Remove dithering artifacts from EGA games
prefer_digitalsfx  bool     If true, digital sound effects are preferred
                            instead of synthesized ones
originalsaveload   bool     If true, the original save/load screens are
                            used instead of the enhanced ScummVM ones
native_fb01        bool     If true, the music driver for an IBM Music
                            Feature card or a Yamaha FB-01 FM synth module
                            is used for MIDI output
use_cdaudio        bool     Use CD audio instead of in-game audio,
                            when available
windows_cursors    bool     Use the Windows cursors (smaller and monochrome)
                            instead of the DOS ones (King's Quest 6)
silver_cursors     bool     Use the alternate set of silver cursors,
                            instead of the normal golden ones (Space Quest 4)

Broken Sword II adds the following non-standard keywords:
gfx_details        number   Graphics details setting (0-3)
music_mute         bool     If true, music is muted
object_labels      bool     If true, object labels are enabled
reverse_stereo     bool     If true, stereo channels are reversed
sfx_mute           bool     If true, sound effects are muted

Flight of the Amazon Queen adds the following non-standard keywords:
music_mute         bool     If true, music is muted
sfx_mute           bool     If true, sound effects are muted

Hopkins FBI adds the following non-standard keyword:
enable_gore        bool     If true, enable some optional gore content in
                            the game

Jones in the Fast Lane adds the following non-standard keyword:
music_mute         bool     If true, CD audio is used, if available,
                            instead of in-game audio

King's Quest VI Windows adds the following non-standard keyword:
windows_cursors    bool     If true, the original unscaled black and white
                            Windows cursors are used instead of the DOS
                            ones. If false, the DOS cursors are used in the
                            Windows version, upscaled to match the rest of
                            the upscaled graphics

Lands of Lore: The Throne of Chaos adds the following non-standard keywords:
smooth_scrolling   bool     If true, scrolling is smoother when changing
                            from one screen to another
floating_cursors   bool     If true, the cursor changes when it floats to
                            the edge of the screen to a directional arrow.
                            The player can then click to walk towards that
                            direction.

Space Quest IV CD adds the following non-standard keyword:
silver_cursors     bool     If true, an alternate set of silver mouse
                            cursors is used instead of the original golden
                            ones

Simon the Sorcerer 1 and 2 add the following non-standard keywords:
music_mute         bool     If true, music is muted
sfx_mute           bool     If true, sound effects are muted

Soltys adds the following non-standard keyword:
enable_color_blind bool     If true, original colors are replaced by a set
                            of greys

The Legend of Kyrandia adds the following non-standard keyword:
walkspeed          number   The walk speed (0-4)

The Legend of Kyrandia: The Hand of Fate adds the following non-standard
keyword:
walkspeed          number   The walk speed (3 or 5, resp. fast or
                            slow)

The Legend of Kyrandia: Malcolm's Revenge adds the following non-standard
keywords:
walkspeed          number   The walk speed (3 or 5, resp. fast or
                            slow)
studio_audience    bool     If true, applause and cheering sounds are heard
                            whenever Malcolm makes a joke
skip_support       bool     If true, the player can skip text and cutscenes
helium_mode        bool     If true, people sound like they've inhaled
                            Helium

The Neverhood adds the following non-standard keywords:
originalsaveload   bool     If true, the original save/load screens are
                            used instead of the enhanced ScummVM ones
skiphallofrecordsscenes     bool
                            If true, allows the player to skip
                            past the Hall of Records storyboard scenes
scalemakingofvideos  bool   If true, the making of videos are scaled, so that
                            they use the whole screen

The 7th Guest adds the following non-standard keyword:
fast_movie_speed   bool     If true, movies are played at an increased
                            speed, matching the speed of the iOS version.
                            Movies without sound are still played at their
                            normal speed, to avoid music synchronization
                            issues

Zork Nemesis: The Forbidden Lands adds the following non-standard keywords:
originalsaveload   bool     If true, the original save/load screens are
                            used instead of the enhanced ScummVM ones
doublefps          bool     If true, game FPS are increased from 30 to 60
venusenabled       bool     If true, the in-game Venus help system is
                            enabled
noanimwhileturning bool     If true, animations are disabled while turning
                            in panoramic mode

Zork: Grand Inquisitor adds the following non-standard keywords:
originalsaveload   bool     If true, the original save/load screens are
                            used instead of the enhanced ScummVM ones
doublefps          bool     If true, game FPS are increased from 30 to 60
noanimwhileturning bool     If true, animations are disabled while turning
                            in panoramic mode
mpegmovies         bool     If true, the hires MPEG movies are used in the
                            DVD version of the game, instead of the lowres
                            AVI ones

8.2) Custom game options that can be toggled via the GUI
A lot of the custom game options in the previous section can be toggled
via the GUI. If a custom option is available for a specific game, a new
tab called ""Engine"" will appear when adding or editing the configuration
of that game. If the custom options are not shown, the games in question
will need to be run once or readded in the ScummVM launcher's game list.
This will update the configuration of each entry, allowing the custom
options to be shown.
9.0) Screenshots (SDL backend only)
On systems using the SDL backend (for example Windows, Mac or Linux) you
can use alt+s to take snapshots (see section 5.4 - Hotkeys).
You can specify the directory in which you want the screenshots to be
created in the config file. To do so add a screenshotpath value under
the [scummvm] section:
[scummvm]
screenshotpath=/path/to/screenshots/

The default location, when no screenshot path is defined in the config
file, depends on the OS:

Windows: In Users\username\My Pictures\ScummVM Screenshots.
macOS X: On the Desktop.
Other unices: In the XDG Pictures user directory,
e.g. ~/Pictures/ScummVM Screenshots
Any other OS: In the current directory.

10.0) Compiling
For an up-to-date overview on how to compile ScummVM for various
platforms, please consult our Wiki, in particular this page:
https://wiki.scummvm.org/index.php/Compiling_ScummVM
If you are compiling for Windows, Linux or Mac OS X, you need SDL-1.2.2
or newer (older versions may work, but are unsupported), and a supported
compiler. Several compilers, including GCC, mingw and recent versions of
Microsoft Visual C++ are supported. If you wish to use MP3-compressed CD
tracks or .SOU files, you will need to install the MAD library; likewise
you will need the appropriate libraries for Ogg Vorbis and FLAC
compressed sound. For compressed save states, zlib is required.
Some parts of ScummVM, particularly scalers, have highly optimized
versions written in assembler. If you wish to use this option, you will
need to install nasm assembler (see https://www.nasm.us/). Note that
we currently only have x86 MMX optimized versions, and they will not
compile on other processors.
On Windows, you can define USE_WINDBG and attach WinDbg to browse
debug messages (see
https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/index).


Windows:


MinGW:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/MinGW



Visual Studio (MSVC):

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/Visual_Studio





Linux:


GCC:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/GCC





AmigaOS4:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/AmigaOS4



Apple iPhone:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/iPhone



Atari/FreeMiNT:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/Atari/FreeMiNT



Bada/Tizen:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/Bada/Tizen



BeOS/ZETA/Haiku:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/BeOS/ZETA/Haiku



Google Android:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/Android



HP webOS:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/WebOS



Mac OS:


Mac OS X:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/macOS



Mac OS X 10.2.8:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/Mac_OS_X_10.2.8



Mac OS X Crosscompiling:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/Mac_OS_X_Crosscompiling





Maemo:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/Maemo



Nintendo Wii and Gamecube:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/Wii



Raspberry Pi:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/RPI



Sega Dreamcast:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/Dreamcast



Sony Playstation:


Sony PlayStation 2:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/PlayStation_2



Sony PlayStation 3:

Please refer to:
https://wiki.scummvm.org/index.php/PlayStation_3#Building_from_source



Sony PlayStation Portable:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/PlayStation_Portable





Symbian:

Please refer to:
https://wiki.scummvm.org/index.php/Compiling_ScummVM/Symbian



11.0) Changelog
Please refer to our extensive Changelog here.
12.0) Credits
Please refer to our extensive Credits list here.


Good Luck and Happy Adventuring!
The ScummVM team.
https://www.scummvm.org/

",1065
TheCjw/scoop-retools,PowerShell,"scoop-retools
   
Scoop bucket for reverse engineering tools.
Usage


Install scoop


Add this bucket to scoop:


scoop bucket add retools https://github.com/TheCjw/scoop-retools.git

Install tools via scoop install:

scoop install smali baksmali apktool

Done.

Using DynamoRIO
Bulid WinAFL
scoop install dynamorio
git clone https://github.com/googleprojectzero/winafl
cd winafl
# Compile 32bit tool
mkdir build32
cd build32
cmake .. -DDynamoRIO_DIR=""$env:DYNAMORIO_DIR""
cmake --build . --config Release
# or 64bit
mkdir build64
cd build64
cmake .. -DDynamoRIO_DIR=""$env:DYNAMORIO_DIR""
cmake --build . --config Release
",21
geekhch/homework,Jupyter Notebook,"homework
",6
vromero/activemq-artemis-docker,XSLT,"     
1. What is ActiveMQ Artemis?
Apache ActiveMQ Artemis is an open source project to build a multi-protocol, embeddable, very high performance, clustered, asynchronous messaging system. Apache ActiveMQ Artemis is an example of Message Oriented Middleware (MoM).

2. Tags and Dockerfile links



Debian Based
Alpine Based




latest
latest-alpine


2.7.0
2.7.0-alpine


2.6.4
2.6.4-alpine


2.6.3
2.6.3-alpine


2.6.2
2.6.2-alpine


2.6.1
2.6.1-alpine


2.6.0
2.6.0-alpine


2.5.0
2.5.0-alpine


2.4.0
2.4.0-alpine


2.3.0
2.3.0-alpine


2.2.0
2.2.0-alpine


2.1.0
2.1.0-alpine


2.0.0
2.0.0-alpine


1.5.6
1.5.6-alpine


1.5.5
1.5.5-alpine


1.5.4
1.5.4-alpine


1.5.3
1.5.3-alpine


1.5.2
1.5.2-alpine


1.5.1
1.5.1-alpine


1.5.0
1.5.0-alpine


1.4.0
1.4.0-alpine


1.3.0
1.3.0-alpine


1.2.0
1.2.0-alpine


1.1.0
1.1.0-alpine


1.0.0
1.0.0-alpine



3. About this image
The ActiveMQ Artemis images come in two flavors, both equally supported :

Debian based: the default one.
Alpine based: much lighter.

All versions of ActiveMQ Artemis are provided for the time being but versions previous to 1.5.5 shall be considered deprecated and could be removed at any time.
This image shall not be considered production ready as is. If you plan to use this image in a production environment, fork the image in order to maintain stability as
the build is reproducible in a best effort basis. Then at each rebase, make sure you tests the changes you are importing.
4. How to use this image
You can find how to run this image in the section Running the image. Beware as the default
configuration is not recommended for production usage, at the very least you'll want to set your own
login and password. This is described with detail in section Setting the username and password.
In case you also want to set some customized memory limits, this is described in
Setting the memory values.
ActiveMQ Artemis typically persists the queue state to disk. In order to leverage the most of your
disk ActiveMQ artemis might require some fine-tuning. The good news is that this process is
fully automated and its described in Performing a performance journal test.
JMX uses RMI and therefore random ports. This is extremely bad for automatization in Docker and
in general. For that reason its not supported for most of the use cases. However, when using this
image in orchestrators like Kubernetes you might want to connect from a sidecar where it
does make sense. How to enable JMX is described in section Enabling JMX.
The Jolokia console CORS header won't be a problem by default as it set to *, however if you want to
narrow it down for improved security don't miss the section Settings the console's allow origin.
In rare ocassions you might find the need of running ActiveMQ Artemis without security. This
is described in section Disabling security.
Some of the configurations mentioned above are scripted automations that modify the
configuration files. You might have your own configuration that you want to provide as a whole.
In that case disregard the aforementioned sections and find how to pass your own
configuration in section Using external configuration files.
If instead you want to use the configuration parameters and make some non-mayor changes to the
configuration you could use the mechanisms to apply some small transformations using XSLT
as described in section Overriding parts of the configuration.
5. Running the image
There are different methods to run a Docker image, from interactive Docker to Kubernetes and Docker
Compose. This documentation will cover only Docker with an interactive terminal mode. You should
refer to the appropriate documentation for more information around other execution methods.
To run ActiveMQ with AMQP, JMS and the web console open (if your are running 2.3.0 or later),
run the following command:
docker run -it --rm \
  -p 8161:8161 \
  -p 61616:61616 \
  vromero/activemq-artemis
After a few seconds you'll see in the output a block similar to:
_        _               _
/ \  ____| |_  ___ __  __(_) _____
/ _ \|  _ \ __|/ _ \  \/  | |/  __/
/ ___ \ | \/ |_/  __/ |\/| | |\___ \
/_/   \_\|   \__\____|_|  |_|_|/___ /
Apache ActiveMQ Artemis x.x.x

HH:mm:ss,SSS INFO  [...] AMQ101000: Starting ActiveMQ Artemis Server

At this point you can open the web server port at 8161 and check the web console using
the default username and password of artemis / simetraehcapa.
5.1 Setting the username and password
If you wish to change the default username and password of artemis / simetraehcapa, you can do so with the ARTEMIS_USERNAME and ARTEMIS_PASSWORD environment variables:
docker run -it --rm \
  -e ARTEMIS_USERNAME=myuser \
  -e ARTEMIS_PASSWORD=otherpassword \
  vromero/activemq-artemis
5.2 Setting the memory values
By default this image does leverage the new features that came in Java 8u131 related to memory ergonomics in containerized environments, more information about it here.
It does use a -XX:MaxRAMFraction=2 meaning that half of the memory made avaiable to the container will be used by the Java heap, leaving the other half for other types of Java memory and other OS purposes. However, in some
circumstances it might be advisable to fine tune the memory to manual values, in that case you can set the memory that you application needs by using the parameters ARTEMIS_MIN_MEMORY and ARTEMIS_MAX_MEMORY:
docker run -it --rm \
  -e 'ARTEMIS_MIN_MEMORY=1512M' \
  -e 'ARTEMIS_MAX_MEMORY=3048M' \
  vromero/activemq-artemis
The previous example will launch Apache ActiveMQ Artemis in docker with 1512 MB of memory, with a maximum usage of 3048 MB of memory.
The format of the values passed is the same than the format used for the Java -Xms and -Xmx parameters and its documented here.
5.3 Performing a performance journal test
Different kinds of volumes need different values in fine tuning. In ActiveMQ Artemis the journal-buffer-timeout is oftentimes configured for this purpose.
Since 1.5.3 it is possible to calculate the optimal value automatically. This image supports this automation using the environment variable: ARTEMIS_PERF_JOURNAL with one of the following values:



Value
Description




AUTO (default)
Checks for the existence of a .perf-journal-completed file in the data volume, if it doesn't exist performs the calculation, applies the configuration and creates the file.


NEVER
Never do the performance journal configuration


ALWAYS
Always do the performance journal configuration



It is safe to leave it as AUTO even for the casual usage of this image given that the image already have
incorporated a .perf-journal-completed for its internal directory used when no volume is mounted.
One example of execution with the performance journal calibration set to be executed always can be found
in the next listing:
docker run -it --rm \
  -e ARTEMIS_PERF_JOURNAL=ALWAYS \
  vromero/activemq-artemis
5.4 Enabling JMX
Due to the JMX's nature, often with dynamics ports for RMI and the need having configure the public IP address to reach the RMI server.
It is discouraged to use JMX in Docker. Although in certain scenarios, it could be advisable, as when deploying in a
container orchestrator such as Kubernetes or Mesos, and deploying along side this container a side car. For such cases
the following environment variable could be used: ENABLE_JMX.
It is also possible to set the JMX port and the JMX RMI port with these two environment variables respectively: JMX_PORT (default: 1099) and JMX_RMI_PORT (default: 1098).
Given that JMX is intended for side cars, it is attached only to localhost and not protected with SSL. Likewise, its ports are not declared in the Dockerfile.
docker run -it --rm \
  -e ENABLE_JMX=true \
  -e JMX_PORT=1199 \
  -e JMX_RMI_PORT=1198 \
  vromero/activemq-artemis
5.5 Prometheus metrics
When using this image in a orchestrated environmnet like in Kubernetes. It is often useful to have metrics endpoints compatible
with prometheus to ease monitoring.
This image can export such metrics in port 9404 thanks to the integration with the Prometheus JMX exporter. In order to enable it the environmnet variable ENABLE_JMX_EXPORTER should
be present, it will also inderectly enable JMX as if ENABLE_JMX was set.
To see what is exported just:
docker run -it --rm \
  -p9404:9404 \
  -e ENABLE_JMX_EXPORTER=true \
  vromero/activemq-artemis
And then in a different terminal run:
curl http://127.0.0.1:9404
To obtain the following and more:
# HELP artemis_disk_scan_period How often to check for disk space usage, in milliseconds (org.apache.activemq.artemis<broker=""0.0.0.0""><>DiskScanPeriod)
# TYPE artemis_disk_scan_period counter
artemis_disk_scan_period 5000.0
# HELP artemis_durable_delivering_count number of durable messages that this queue is currently delivering to its consumers (org.apache.activemq.artemis<broker=""0.0.0.0"", component=addresses, address=""DLQ"", subcomponent=queues, routing-type=""anycast"", queue=""DLQ""><>DurableDeliveringCount)
# TYPE artemis_durable_delivering_count counter
artemis_durable_delivering_count{queue=""DLQ"",address=""DLQ"",} 0.0
artemis_durable_delivering_count{queue=""ExpiryQueue"",address=""ExpiryQueue"",} 0.0
# HELP artemis_journal_min_files Number of journal files to pre-create (org.apache.activemq.artemis<broker=""0.0.0.0""><>JournalMinFiles)
# TYPE artemis_journal_min_files counter
artemis_journal_min_files 2.0
# HELP artemis_message_expiry_thread_priority Priority of the thread used to scan message expiration (org.apache.activemq.artemis<broker=""0.0.0.0""><>MessageExpiryThreadPriority)
# TYPE artemis_message_expiry_thread_priority counter
artemis_message_expiry_thread_priority 3.0
# HELP artemis_messages_killed number of messages removed from this queue since it was created due to exceeding the max delivery attempts (org.apache.activemq.artemis<broker=""0.0.0.0"", component=addresses, address=""DLQ"", subcomponent=queues, routing-type=""anycast"", queue=""DLQ""><>MessagesKilled)
# TYPE artemis_messages_killed counter
artemis_messages_killed{queue=""DLQ"",address=""DLQ"",} 0.0
artemis_messages_killed{queue=""ExpiryQueue"",address=""ExpiryQueue"",} 0.0
# HELP artemis_address_memory_usage_percentage Memory used by all the addresses on broker as a percentage of global maximum limit (org.apache.activemq.artemis<broker=""0.0.0.0""><>AddressMemoryUsagePercentage)
# TYPE artemis_address_memory_usage_percentage counter
artemis_address_memory_usage_percentage 0.0
# HELP artemis_journal_sync_non_transactional Whether the journal is synchronized when receiving non-transactional datar (org.apache.activemq.artemis<broker=""0.0.0.0""><>JournalSyncNonTransactional)
# TYPE artemis_journal_sync_non_transactional counter
artemis_journal_sync_non_transactional 1.0
# HELP artemis_journal_buffer_size Size of the internal buffer on the journal (org.apache.activemq.artemis<broker=""0.0.0.0""><>JournalBufferSize)
# TYPE artemis_journal_buffer_size counter
artemis_journal_buffer_size 501760.0
# HELP artemis_journal_max_io Maximum number of write requests that can be in the AIO queue at any given time (org.apache.activemq.artemis<broker=""0.0.0.0""><>JournalMaxIO)
# TYPE artemis_journal_max_io counter
artemis_journal_max_io 4096.0

In case you need more control over the metrics that are exported, you can mount a jmx-exporter
configuration file in /opt/jmx-exporter/etc-override with the file name jmx-exporter-config.yaml.
5.6 Settings the console's allow origin
ActiveMQ Artemis console uses Jolokia. In the default vanilla non-docker installation Jolokia does set a CORS header to
allow only localhost. In the docker image this create problems as things are rarely accesed as localhost.
Therefore the docker image does set the CORS header to * by default. However there is a mechanism to narrow it
down to whatever value is best suited to you for improved security through the environmnet property: JOLOKIA_ALLOW_ORIGIN.
docker run -it --rm \
  -e JOLOKIA_ALLOW_ORIGIN=192.168.1.1 \
  vromero/activemq-artemis
5.7 Overriding parts of the configuration
ActiveMQ Artemis support disabling the security using the element <security-enabled>false</security-enabled>
as described in the official documentation.
This docker image makes it simple to set that element using the environment property: DISABLE_SECURITY:
docker run -it --rm \
  -e DISABLE_SECURITY=true \
  vromero/activemq-artemis
Please keep in mind no production system, possible no environment at all, should ever disable security.
Make sure you read the falacy number one of the falacies of the distributed computing before disabling the security.
5.8 Using external configuration files
It is possible to mount a whole artemis etc directory in this image in the volume /var/lib/artemis/etc.
Be careful as this might be an overkill for many situations where only small tweaks are necessary.
When using this technique be aware that the configuration files of Artemis might change from version to version.
Generally speaking, when in need to configure Artemis beyond what it is offered by this image using environment
variables, it is recommended to use the partial override mechanism described in the next section.
5.9 Overriding parts of the configuration
The default ActiveMQ Artemis configuration can be partially modified, instead of completely replaced as in the previous section, using three mechanisms. Merge snippets, XSLT tranformations and entrypoint overrides.
Merging snippets
Multiple files with snippets of configuration can be dropped in the /var/lib/artemis/etc-override volume. Those configuration files must be named following the name convention broker-{{num}}.xml where num is a numeric representation of the snippet.
The configuration files will be merged with the default configuration. An alphabetical precedence of the file names will be considered for the merge and in case of collision the latest change will be treated as final.
For instance lets say that you want to add a diverts section, you could have a local directory, lets say /var/artemis-data/etc-override
where you could place a broker-00.xml file that looks like the following listing:
<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?>

<configuration xmlns=""urn:activemq"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""urn:activemq /schema/artemis-configuration.xsd"">
   <!-- from 1.0.0 to 1.5.5 the following line should be : <core xmlns=""urn:activemq:core""> -->
   <core xmlns=""urn:activemq:core"" xsi:schemaLocation=""urn:activemq:core "">
      <diverts>
         <divert name=""order-divert"">
            <routing-name>order-divert</routing-name>
            <address>orders</address>
            <forwarding-address>spyTopic</forwarding-address>
            <exclusive>false</exclusive>
         </divert>
      </diverts>
   </core>
</configuration>
Please notice the core element change along with the versions:

1.0.0 up to 1.5.5: <core xmlns=""urn:activemq:core"">
2.0.0 onwards: <core xmlns=""urn:activemq:core"" xsi:schemaLocation=""urn:activemq:core "">

Configuration transformations
For the use cases where instead of merging, the desired outcome is a deletion or some other kind of advanced transformation a file named broker-00.xslt
in /var/lib/artemis/etc-override is supported. For instance to delete the jms definitions that is present by default in the broker.xml file shown below:
<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?>

<configuration xmlns=""urn:activemq"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""urn:activemq /schema/artemis-configuration.xsd"">
  ...
  <jms xmlns=""urn:activemq:jms"">
    <queue name=""myfancyqueue""/>
    <queue name=""myotherqueue""/>
  </jms>
  ...
</configuration>
A file name broker-00.xslt with content like the following listing, could be used:
<xsl:stylesheet version=""1.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform""
  xmlns:activemq=""urn:activemq"" xmlns:jms=""urn:activemq:jms"">

 <xsl:output omit-xml-declaration=""yes""/>

    <xsl:template match=""node()|@*"">
      <xsl:copy>
         <xsl:apply-templates select=""node()|@*""/>
      </xsl:copy>
    </xsl:template>

    <xsl:template match=""*[local-name()='jms']""/>
</xsl:stylesheet>
Entrypoint Overrides
Multiple shell scripts can be dropped in the /var/lib/artemis/etc-override volume. Those shell files must be named following the name convention entrypoint-{{num}}.sh where num is a numeric representation of the snippet.
The shell scripts will be executed in alphabetical precedence of the file names on startup of the docker container.
A typical use case for using entrypoint overrides would be if you want to make a minor modification to a file which cannot be overriden using the 2 methods above and you do not want to expose the etc volume.
If you would like to see the final result of your transformations, execute the following:
docker run -it --rm \
  -v /var/artemis-data/override:/var/lib/artemis/etc-override \
  vromero/activemq-artemis \
  cat ../etc/broker.xml

5.10 Broker Config
ActiveMQ allows you to override key configuration values using System properties.
This docker image has built in support to set these values by passing environment variables prefixed with BROKER_CONFIG to the docker image.
Below is an example which overrides the global-max-size and disk-scan-period values
docker run -it --rm   -p 8161:8161 \
    -e BROKER_CONFIG_GLOBAL_MAX_SIZE=50000 \
    -e BROKER_CONFIG_DISK_SCAN_PERIOD=6000 \
    vromero/activemq-artemis

5.11 Environment Variables
Additionally, the following environment variables are supported



Env Var
Default
Description




JAVA_OPTS

Will pass additional java options to the artemis runtime



5.12 Mount points



Mount point
Description




/var/lib/artemis/data
Holds the data files used for storing persistent messages


/var/lib/artemis/etc
Holds the instance configuration files


/var/lib/artemis/etc-override
Holds the instance configuration files


/var/lib/artemis/lock
Holds the command line locks (typically not useful to mount)


/opt/jmx-exporter/etc-override
Holds the configuration file for jmx-exporter jmx-exporter-config.yaml



5.13 Exposed ports



Port
Description




8161
Web Server


9404
JMX Exporter


61616
Core,MQTT,AMQP,HORNETQ,STOMP,Openwire


5445
HORNETQ,STOMP


5672
AMQP


1883
MQTT


61613
STOMP



6. Running in orchestrators
At the moment only docker is directly supported for this image. However there is an attempt to create
a helm chart for Kubernetes and some configuration tuning for OpenShift.
6.1 Running in Kubernetes
ActiveMQ Artemis can leverage JGroups to discover the members of the cluster. And JGroups
can be extended with a plugin called jgroups-kubernetes
that allows JGroups to discover using Kubernetes.
jgroups-kubernetes version 0.9.3 is included in the classpath of
this image, however everything about the configuration of jgroups and jgroups-kubernetes is left to the user.
If you rather prefer a easier solution to run a cluster of ActiveMQ Artemis nodes, there is an attempt to create a Helm chart
by the same author of this image. It can be found here. It
does leverage jgroups-kubernetes in a transparent way.
6.2 OpenShift
OpenShift has diverted a bit from Kubernetes (e.g: automounts empty volumes in all declared volumes without
the user asking for it at all) and Docker (e.g: runs on an random user).
The biggest problem to run this image is the automount of empty directories because it empties the etc directory.
In order to restore it the environment variable RESTORE_CONFIGURATION has been created. It can be used as follows:
oc new-app --name=artemis vromero/activemq-artemis -e RESTORE_CONFIGURATION=true
7. License
View license information for the software contained in this image.
8. User Feedback
8.1 Issues
If you have any problems with or questions about this image, please contact us through a GitHub issue.
8.2 Contributing
You are invited to contribute new features, fixes, or updates, large or small; we are always thrilled to receive pull requests, and do our best to process them as fast as we can.
Before you start to code, we recommend discussing your plans through a GitHub issue, especially for more ambitious contributions. This gives other contributors a chance to point you in the right direction, give you feedback on your design, and help you find out if someone else is working on the same thing.
",57
d-e-s-o/cell,Rust,"



cell

Documentation
Changelog

cell is a crate providing a revised RefCell implementation with
additional mapping capabilities. It can be used as a drop-in replacement
for std::cell::RefCell where this additional functionality is required.
The Problem
A borrowed RefCell is represented as a
Ref. Such a Ref contains a reference to the data
contained in the RefCell. To extend the borrow to a different member
in the original data, the map method can be used.
Using this method a new Ref object can be created that contains a
reference to a member in the borrowed data.
While having a direct reference to such a data member is appropriate in
many cases, there are some where this is insufficient, and an actual
object that contains such a reference is required.
Example
The most prominent example is an iterator. While an iterator internally
keeps a reference to the object being iterated over, it is more than
just a reference to it: it contains state about the progress of the
iteration.
If such an iterator is to be exposed for an object contained in a
RefCell that is currently borrowed, the Ref::map function is
insufficient:
struct RefStrings(RefCell<Vec<String>>);

impl RefStrings {
    fn iter(&self) -> Ref<Iter<String>> {
        Ref::map(self.0.borrow(), |x| x.iter())
    }
}
error[E0308]: mismatched types
 |  Ref::map(self.0.borrow(), |x| x.iter())
 |                                ^^^^^^^^ expected reference, found struct `std::slice::Iter`
 |
 = note: expected type `&_`
            found type `std::slice::Iter<'_, std::string::String>`

(Note that required lifetimes have been elided in the example for brevity)
A Solution
This crate provides alternative RefCell and Ref implementations that
solve this problem by introduction of another mapping method: map_val.
This method returns a RefVal object. RefVal is a new type that is
similar to Ref but, instead of embedding a reference to its T (a
type parameter), it embeds a value of it. T in turn would contain the
actual reference to the borrowed object.
In the above example the only changes that need to happen are the
replacement of std::cell::RefCell with cell::RefCell, that of
std::cell::Ref with cell::Ref, and the usage of Ref::map instead
of Ref::map_val.
--- test.rs
+++ test.rs
@@ -1,13 +1,14 @@
-use std::cell::Ref;
-use std::cell::RefCell;
+use cell::Ref;
+use cell::RefCell;
+use cell::RefVal;
 use std::slice::Iter;


 struct RefStrings(RefCell<Vec<String>>);

 impl RefStrings {
-    fn iter<'t, 's: 't>(&'s self) -> Ref<'t, Iter<String>> {
-        Ref::map(self.0.borrow(), |x| x.iter())
+    fn iter<'t, 's: 't>(&'s self) -> RefVal<'t, Iter<String>> {
+        Ref::map_val(self.0.borrow(), |x| x.iter())
     }
 }

Alternative Implementations
The possibility of providing this functionality by means of a trait has
been investigated but no viable solution has been identified. The main
problem stems from the fact that we require access to Ref internals in
order to provide this functionality. Such a trait would alleviate the
need for providing alternative RefCell and Ref implementations.
No other existing solutions for this problem have been found.
A discussion around the upstreaming of this functionality is tracked by
Rust issue #54776.
",3
vromero/activemq-artemis-docker,XSLT,"     
1. What is ActiveMQ Artemis?
Apache ActiveMQ Artemis is an open source project to build a multi-protocol, embeddable, very high performance, clustered, asynchronous messaging system. Apache ActiveMQ Artemis is an example of Message Oriented Middleware (MoM).

2. Tags and Dockerfile links



Debian Based
Alpine Based




latest
latest-alpine


2.7.0
2.7.0-alpine


2.6.4
2.6.4-alpine


2.6.3
2.6.3-alpine


2.6.2
2.6.2-alpine


2.6.1
2.6.1-alpine


2.6.0
2.6.0-alpine


2.5.0
2.5.0-alpine


2.4.0
2.4.0-alpine


2.3.0
2.3.0-alpine


2.2.0
2.2.0-alpine


2.1.0
2.1.0-alpine


2.0.0
2.0.0-alpine


1.5.6
1.5.6-alpine


1.5.5
1.5.5-alpine


1.5.4
1.5.4-alpine


1.5.3
1.5.3-alpine


1.5.2
1.5.2-alpine


1.5.1
1.5.1-alpine


1.5.0
1.5.0-alpine


1.4.0
1.4.0-alpine


1.3.0
1.3.0-alpine


1.2.0
1.2.0-alpine


1.1.0
1.1.0-alpine


1.0.0
1.0.0-alpine



3. About this image
The ActiveMQ Artemis images come in two flavors, both equally supported :

Debian based: the default one.
Alpine based: much lighter.

All versions of ActiveMQ Artemis are provided for the time being but versions previous to 1.5.5 shall be considered deprecated and could be removed at any time.
This image shall not be considered production ready as is. If you plan to use this image in a production environment, fork the image in order to maintain stability as
the build is reproducible in a best effort basis. Then at each rebase, make sure you tests the changes you are importing.
4. How to use this image
You can find how to run this image in the section Running the image. Beware as the default
configuration is not recommended for production usage, at the very least you'll want to set your own
login and password. This is described with detail in section Setting the username and password.
In case you also want to set some customized memory limits, this is described in
Setting the memory values.
ActiveMQ Artemis typically persists the queue state to disk. In order to leverage the most of your
disk ActiveMQ artemis might require some fine-tuning. The good news is that this process is
fully automated and its described in Performing a performance journal test.
JMX uses RMI and therefore random ports. This is extremely bad for automatization in Docker and
in general. For that reason its not supported for most of the use cases. However, when using this
image in orchestrators like Kubernetes you might want to connect from a sidecar where it
does make sense. How to enable JMX is described in section Enabling JMX.
The Jolokia console CORS header won't be a problem by default as it set to *, however if you want to
narrow it down for improved security don't miss the section Settings the console's allow origin.
In rare ocassions you might find the need of running ActiveMQ Artemis without security. This
is described in section Disabling security.
Some of the configurations mentioned above are scripted automations that modify the
configuration files. You might have your own configuration that you want to provide as a whole.
In that case disregard the aforementioned sections and find how to pass your own
configuration in section Using external configuration files.
If instead you want to use the configuration parameters and make some non-mayor changes to the
configuration you could use the mechanisms to apply some small transformations using XSLT
as described in section Overriding parts of the configuration.
5. Running the image
There are different methods to run a Docker image, from interactive Docker to Kubernetes and Docker
Compose. This documentation will cover only Docker with an interactive terminal mode. You should
refer to the appropriate documentation for more information around other execution methods.
To run ActiveMQ with AMQP, JMS and the web console open (if your are running 2.3.0 or later),
run the following command:
docker run -it --rm \
  -p 8161:8161 \
  -p 61616:61616 \
  vromero/activemq-artemis
After a few seconds you'll see in the output a block similar to:
_        _               _
/ \  ____| |_  ___ __  __(_) _____
/ _ \|  _ \ __|/ _ \  \/  | |/  __/
/ ___ \ | \/ |_/  __/ |\/| | |\___ \
/_/   \_\|   \__\____|_|  |_|_|/___ /
Apache ActiveMQ Artemis x.x.x

HH:mm:ss,SSS INFO  [...] AMQ101000: Starting ActiveMQ Artemis Server

At this point you can open the web server port at 8161 and check the web console using
the default username and password of artemis / simetraehcapa.
5.1 Setting the username and password
If you wish to change the default username and password of artemis / simetraehcapa, you can do so with the ARTEMIS_USERNAME and ARTEMIS_PASSWORD environment variables:
docker run -it --rm \
  -e ARTEMIS_USERNAME=myuser \
  -e ARTEMIS_PASSWORD=otherpassword \
  vromero/activemq-artemis
5.2 Setting the memory values
By default this image does leverage the new features that came in Java 8u131 related to memory ergonomics in containerized environments, more information about it here.
It does use a -XX:MaxRAMFraction=2 meaning that half of the memory made avaiable to the container will be used by the Java heap, leaving the other half for other types of Java memory and other OS purposes. However, in some
circumstances it might be advisable to fine tune the memory to manual values, in that case you can set the memory that you application needs by using the parameters ARTEMIS_MIN_MEMORY and ARTEMIS_MAX_MEMORY:
docker run -it --rm \
  -e 'ARTEMIS_MIN_MEMORY=1512M' \
  -e 'ARTEMIS_MAX_MEMORY=3048M' \
  vromero/activemq-artemis
The previous example will launch Apache ActiveMQ Artemis in docker with 1512 MB of memory, with a maximum usage of 3048 MB of memory.
The format of the values passed is the same than the format used for the Java -Xms and -Xmx parameters and its documented here.
5.3 Performing a performance journal test
Different kinds of volumes need different values in fine tuning. In ActiveMQ Artemis the journal-buffer-timeout is oftentimes configured for this purpose.
Since 1.5.3 it is possible to calculate the optimal value automatically. This image supports this automation using the environment variable: ARTEMIS_PERF_JOURNAL with one of the following values:



Value
Description




AUTO (default)
Checks for the existence of a .perf-journal-completed file in the data volume, if it doesn't exist performs the calculation, applies the configuration and creates the file.


NEVER
Never do the performance journal configuration


ALWAYS
Always do the performance journal configuration



It is safe to leave it as AUTO even for the casual usage of this image given that the image already have
incorporated a .perf-journal-completed for its internal directory used when no volume is mounted.
One example of execution with the performance journal calibration set to be executed always can be found
in the next listing:
docker run -it --rm \
  -e ARTEMIS_PERF_JOURNAL=ALWAYS \
  vromero/activemq-artemis
5.4 Enabling JMX
Due to the JMX's nature, often with dynamics ports for RMI and the need having configure the public IP address to reach the RMI server.
It is discouraged to use JMX in Docker. Although in certain scenarios, it could be advisable, as when deploying in a
container orchestrator such as Kubernetes or Mesos, and deploying along side this container a side car. For such cases
the following environment variable could be used: ENABLE_JMX.
It is also possible to set the JMX port and the JMX RMI port with these two environment variables respectively: JMX_PORT (default: 1099) and JMX_RMI_PORT (default: 1098).
Given that JMX is intended for side cars, it is attached only to localhost and not protected with SSL. Likewise, its ports are not declared in the Dockerfile.
docker run -it --rm \
  -e ENABLE_JMX=true \
  -e JMX_PORT=1199 \
  -e JMX_RMI_PORT=1198 \
  vromero/activemq-artemis
5.5 Prometheus metrics
When using this image in a orchestrated environmnet like in Kubernetes. It is often useful to have metrics endpoints compatible
with prometheus to ease monitoring.
This image can export such metrics in port 9404 thanks to the integration with the Prometheus JMX exporter. In order to enable it the environmnet variable ENABLE_JMX_EXPORTER should
be present, it will also inderectly enable JMX as if ENABLE_JMX was set.
To see what is exported just:
docker run -it --rm \
  -p9404:9404 \
  -e ENABLE_JMX_EXPORTER=true \
  vromero/activemq-artemis
And then in a different terminal run:
curl http://127.0.0.1:9404
To obtain the following and more:
# HELP artemis_disk_scan_period How often to check for disk space usage, in milliseconds (org.apache.activemq.artemis<broker=""0.0.0.0""><>DiskScanPeriod)
# TYPE artemis_disk_scan_period counter
artemis_disk_scan_period 5000.0
# HELP artemis_durable_delivering_count number of durable messages that this queue is currently delivering to its consumers (org.apache.activemq.artemis<broker=""0.0.0.0"", component=addresses, address=""DLQ"", subcomponent=queues, routing-type=""anycast"", queue=""DLQ""><>DurableDeliveringCount)
# TYPE artemis_durable_delivering_count counter
artemis_durable_delivering_count{queue=""DLQ"",address=""DLQ"",} 0.0
artemis_durable_delivering_count{queue=""ExpiryQueue"",address=""ExpiryQueue"",} 0.0
# HELP artemis_journal_min_files Number of journal files to pre-create (org.apache.activemq.artemis<broker=""0.0.0.0""><>JournalMinFiles)
# TYPE artemis_journal_min_files counter
artemis_journal_min_files 2.0
# HELP artemis_message_expiry_thread_priority Priority of the thread used to scan message expiration (org.apache.activemq.artemis<broker=""0.0.0.0""><>MessageExpiryThreadPriority)
# TYPE artemis_message_expiry_thread_priority counter
artemis_message_expiry_thread_priority 3.0
# HELP artemis_messages_killed number of messages removed from this queue since it was created due to exceeding the max delivery attempts (org.apache.activemq.artemis<broker=""0.0.0.0"", component=addresses, address=""DLQ"", subcomponent=queues, routing-type=""anycast"", queue=""DLQ""><>MessagesKilled)
# TYPE artemis_messages_killed counter
artemis_messages_killed{queue=""DLQ"",address=""DLQ"",} 0.0
artemis_messages_killed{queue=""ExpiryQueue"",address=""ExpiryQueue"",} 0.0
# HELP artemis_address_memory_usage_percentage Memory used by all the addresses on broker as a percentage of global maximum limit (org.apache.activemq.artemis<broker=""0.0.0.0""><>AddressMemoryUsagePercentage)
# TYPE artemis_address_memory_usage_percentage counter
artemis_address_memory_usage_percentage 0.0
# HELP artemis_journal_sync_non_transactional Whether the journal is synchronized when receiving non-transactional datar (org.apache.activemq.artemis<broker=""0.0.0.0""><>JournalSyncNonTransactional)
# TYPE artemis_journal_sync_non_transactional counter
artemis_journal_sync_non_transactional 1.0
# HELP artemis_journal_buffer_size Size of the internal buffer on the journal (org.apache.activemq.artemis<broker=""0.0.0.0""><>JournalBufferSize)
# TYPE artemis_journal_buffer_size counter
artemis_journal_buffer_size 501760.0
# HELP artemis_journal_max_io Maximum number of write requests that can be in the AIO queue at any given time (org.apache.activemq.artemis<broker=""0.0.0.0""><>JournalMaxIO)
# TYPE artemis_journal_max_io counter
artemis_journal_max_io 4096.0

In case you need more control over the metrics that are exported, you can mount a jmx-exporter
configuration file in /opt/jmx-exporter/etc-override with the file name jmx-exporter-config.yaml.
5.6 Settings the console's allow origin
ActiveMQ Artemis console uses Jolokia. In the default vanilla non-docker installation Jolokia does set a CORS header to
allow only localhost. In the docker image this create problems as things are rarely accesed as localhost.
Therefore the docker image does set the CORS header to * by default. However there is a mechanism to narrow it
down to whatever value is best suited to you for improved security through the environmnet property: JOLOKIA_ALLOW_ORIGIN.
docker run -it --rm \
  -e JOLOKIA_ALLOW_ORIGIN=192.168.1.1 \
  vromero/activemq-artemis
5.7 Overriding parts of the configuration
ActiveMQ Artemis support disabling the security using the element <security-enabled>false</security-enabled>
as described in the official documentation.
This docker image makes it simple to set that element using the environment property: DISABLE_SECURITY:
docker run -it --rm \
  -e DISABLE_SECURITY=true \
  vromero/activemq-artemis
Please keep in mind no production system, possible no environment at all, should ever disable security.
Make sure you read the falacy number one of the falacies of the distributed computing before disabling the security.
5.8 Using external configuration files
It is possible to mount a whole artemis etc directory in this image in the volume /var/lib/artemis/etc.
Be careful as this might be an overkill for many situations where only small tweaks are necessary.
When using this technique be aware that the configuration files of Artemis might change from version to version.
Generally speaking, when in need to configure Artemis beyond what it is offered by this image using environment
variables, it is recommended to use the partial override mechanism described in the next section.
5.9 Overriding parts of the configuration
The default ActiveMQ Artemis configuration can be partially modified, instead of completely replaced as in the previous section, using three mechanisms. Merge snippets, XSLT tranformations and entrypoint overrides.
Merging snippets
Multiple files with snippets of configuration can be dropped in the /var/lib/artemis/etc-override volume. Those configuration files must be named following the name convention broker-{{num}}.xml where num is a numeric representation of the snippet.
The configuration files will be merged with the default configuration. An alphabetical precedence of the file names will be considered for the merge and in case of collision the latest change will be treated as final.
For instance lets say that you want to add a diverts section, you could have a local directory, lets say /var/artemis-data/etc-override
where you could place a broker-00.xml file that looks like the following listing:
<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?>

<configuration xmlns=""urn:activemq"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""urn:activemq /schema/artemis-configuration.xsd"">
   <!-- from 1.0.0 to 1.5.5 the following line should be : <core xmlns=""urn:activemq:core""> -->
   <core xmlns=""urn:activemq:core"" xsi:schemaLocation=""urn:activemq:core "">
      <diverts>
         <divert name=""order-divert"">
            <routing-name>order-divert</routing-name>
            <address>orders</address>
            <forwarding-address>spyTopic</forwarding-address>
            <exclusive>false</exclusive>
         </divert>
      </diverts>
   </core>
</configuration>
Please notice the core element change along with the versions:

1.0.0 up to 1.5.5: <core xmlns=""urn:activemq:core"">
2.0.0 onwards: <core xmlns=""urn:activemq:core"" xsi:schemaLocation=""urn:activemq:core "">

Configuration transformations
For the use cases where instead of merging, the desired outcome is a deletion or some other kind of advanced transformation a file named broker-00.xslt
in /var/lib/artemis/etc-override is supported. For instance to delete the jms definitions that is present by default in the broker.xml file shown below:
<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?>

<configuration xmlns=""urn:activemq"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""urn:activemq /schema/artemis-configuration.xsd"">
  ...
  <jms xmlns=""urn:activemq:jms"">
    <queue name=""myfancyqueue""/>
    <queue name=""myotherqueue""/>
  </jms>
  ...
</configuration>
A file name broker-00.xslt with content like the following listing, could be used:
<xsl:stylesheet version=""1.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform""
  xmlns:activemq=""urn:activemq"" xmlns:jms=""urn:activemq:jms"">

 <xsl:output omit-xml-declaration=""yes""/>

    <xsl:template match=""node()|@*"">
      <xsl:copy>
         <xsl:apply-templates select=""node()|@*""/>
      </xsl:copy>
    </xsl:template>

    <xsl:template match=""*[local-name()='jms']""/>
</xsl:stylesheet>
Entrypoint Overrides
Multiple shell scripts can be dropped in the /var/lib/artemis/etc-override volume. Those shell files must be named following the name convention entrypoint-{{num}}.sh where num is a numeric representation of the snippet.
The shell scripts will be executed in alphabetical precedence of the file names on startup of the docker container.
A typical use case for using entrypoint overrides would be if you want to make a minor modification to a file which cannot be overriden using the 2 methods above and you do not want to expose the etc volume.
If you would like to see the final result of your transformations, execute the following:
docker run -it --rm \
  -v /var/artemis-data/override:/var/lib/artemis/etc-override \
  vromero/activemq-artemis \
  cat ../etc/broker.xml

5.10 Broker Config
ActiveMQ allows you to override key configuration values using System properties.
This docker image has built in support to set these values by passing environment variables prefixed with BROKER_CONFIG to the docker image.
Below is an example which overrides the global-max-size and disk-scan-period values
docker run -it --rm   -p 8161:8161 \
    -e BROKER_CONFIG_GLOBAL_MAX_SIZE=50000 \
    -e BROKER_CONFIG_DISK_SCAN_PERIOD=6000 \
    vromero/activemq-artemis

5.11 Environment Variables
Additionally, the following environment variables are supported



Env Var
Default
Description




JAVA_OPTS

Will pass additional java options to the artemis runtime



5.12 Mount points



Mount point
Description




/var/lib/artemis/data
Holds the data files used for storing persistent messages


/var/lib/artemis/etc
Holds the instance configuration files


/var/lib/artemis/etc-override
Holds the instance configuration files


/var/lib/artemis/lock
Holds the command line locks (typically not useful to mount)


/opt/jmx-exporter/etc-override
Holds the configuration file for jmx-exporter jmx-exporter-config.yaml



5.13 Exposed ports



Port
Description




8161
Web Server


9404
JMX Exporter


61616
Core,MQTT,AMQP,HORNETQ,STOMP,Openwire


5445
HORNETQ,STOMP


5672
AMQP


1883
MQTT


61613
STOMP



6. Running in orchestrators
At the moment only docker is directly supported for this image. However there is an attempt to create
a helm chart for Kubernetes and some configuration tuning for OpenShift.
6.1 Running in Kubernetes
ActiveMQ Artemis can leverage JGroups to discover the members of the cluster. And JGroups
can be extended with a plugin called jgroups-kubernetes
that allows JGroups to discover using Kubernetes.
jgroups-kubernetes version 0.9.3 is included in the classpath of
this image, however everything about the configuration of jgroups and jgroups-kubernetes is left to the user.
If you rather prefer a easier solution to run a cluster of ActiveMQ Artemis nodes, there is an attempt to create a Helm chart
by the same author of this image. It can be found here. It
does leverage jgroups-kubernetes in a transparent way.
6.2 OpenShift
OpenShift has diverted a bit from Kubernetes (e.g: automounts empty volumes in all declared volumes without
the user asking for it at all) and Docker (e.g: runs on an random user).
The biggest problem to run this image is the automount of empty directories because it empties the etc directory.
In order to restore it the environment variable RESTORE_CONFIGURATION has been created. It can be used as follows:
oc new-app --name=artemis vromero/activemq-artemis -e RESTORE_CONFIGURATION=true
7. License
View license information for the software contained in this image.
8. User Feedback
8.1 Issues
If you have any problems with or questions about this image, please contact us through a GitHub issue.
8.2 Contributing
You are invited to contribute new features, fixes, or updates, large or small; we are always thrilled to receive pull requests, and do our best to process them as fast as we can.
Before you start to code, we recommend discussing your plans through a GitHub issue, especially for more ambitious contributions. This gives other contributors a chance to point you in the right direction, give you feedback on your design, and help you find out if someone else is working on the same thing.
",57
d-e-s-o/cell,Rust,"



cell

Documentation
Changelog

cell is a crate providing a revised RefCell implementation with
additional mapping capabilities. It can be used as a drop-in replacement
for std::cell::RefCell where this additional functionality is required.
The Problem
A borrowed RefCell is represented as a
Ref. Such a Ref contains a reference to the data
contained in the RefCell. To extend the borrow to a different member
in the original data, the map method can be used.
Using this method a new Ref object can be created that contains a
reference to a member in the borrowed data.
While having a direct reference to such a data member is appropriate in
many cases, there are some where this is insufficient, and an actual
object that contains such a reference is required.
Example
The most prominent example is an iterator. While an iterator internally
keeps a reference to the object being iterated over, it is more than
just a reference to it: it contains state about the progress of the
iteration.
If such an iterator is to be exposed for an object contained in a
RefCell that is currently borrowed, the Ref::map function is
insufficient:
struct RefStrings(RefCell<Vec<String>>);

impl RefStrings {
    fn iter(&self) -> Ref<Iter<String>> {
        Ref::map(self.0.borrow(), |x| x.iter())
    }
}
error[E0308]: mismatched types
 |  Ref::map(self.0.borrow(), |x| x.iter())
 |                                ^^^^^^^^ expected reference, found struct `std::slice::Iter`
 |
 = note: expected type `&_`
            found type `std::slice::Iter<'_, std::string::String>`

(Note that required lifetimes have been elided in the example for brevity)
A Solution
This crate provides alternative RefCell and Ref implementations that
solve this problem by introduction of another mapping method: map_val.
This method returns a RefVal object. RefVal is a new type that is
similar to Ref but, instead of embedding a reference to its T (a
type parameter), it embeds a value of it. T in turn would contain the
actual reference to the borrowed object.
In the above example the only changes that need to happen are the
replacement of std::cell::RefCell with cell::RefCell, that of
std::cell::Ref with cell::Ref, and the usage of Ref::map instead
of Ref::map_val.
--- test.rs
+++ test.rs
@@ -1,13 +1,14 @@
-use std::cell::Ref;
-use std::cell::RefCell;
+use cell::Ref;
+use cell::RefCell;
+use cell::RefVal;
 use std::slice::Iter;


 struct RefStrings(RefCell<Vec<String>>);

 impl RefStrings {
-    fn iter<'t, 's: 't>(&'s self) -> Ref<'t, Iter<String>> {
-        Ref::map(self.0.borrow(), |x| x.iter())
+    fn iter<'t, 's: 't>(&'s self) -> RefVal<'t, Iter<String>> {
+        Ref::map_val(self.0.borrow(), |x| x.iter())
     }
 }

Alternative Implementations
The possibility of providing this functionality by means of a trait has
been investigated but no viable solution has been identified. The main
problem stems from the fact that we require access to Ref internals in
order to provide this functionality. Such a trait would alleviate the
need for providing alternative RefCell and Ref implementations.
No other existing solutions for this problem have been found.
A discussion around the upstreaming of this functionality is tracked by
Rust issue #54776.
",3
Erisa/gittag-testing-i-think,HTML,"gittag-testing-i-think
",3
pvaiko/course-generator,Lua,"Fieldwork Course Generator
This is an extended course generator for Courseplay. It can be used in standalone mode or from within the game.
Using from the Game

You can now select 'Vehicle location' in the HUD as
starting corner. If you do so, Starting Direction is set to
automatic too and one headland is selected (can't generate
without headland)
As usual you can set the number of headlands, clockwise/counterclockwise
and headland/center order.
If you select anything other than 'Vehicle location' for the
starting corner, the old generator is invoked.
Using the Standalone Version
Since this generator is now part of the Courseplay release, the only point of
having a standalone version is to provide an efficient development environment
for the course generator outside of the game engine. This allows shorter development
cycles and some automated tests as well.
Standalone mode requires that you embed the Courseplay repository under the
courseplay folder.
The rest of this guide is outdated and startCourseGenerator.lua is not maintained
anymore (it still may work).

In standalone mode it'll loada saved field or a fieldwork course previously
generated in Courseplay, then you can make some adjustments and save the
customized course.
Note that if you load fieldwork courses the course must have
a headland track as the tool uses the outermost headland track
to find the field boundary.
You can then load this customized course in the game just
as a Courseplay generated course.
What this preview does:

generates tracks in any direction
finds the optimum angle, that is the direction the parallel
tracks must run in order to have the minimum number of turns.
makes a best effort to find an angle for non-convex fields
which results in continuous tracks
start the headland tracks at any location.
link the headland tracks with the parallel track (remember,
the parallel tracks start location changes with the angle)
generate course starting in the center and finishing with
the headland for sowing and any fieldwork other than
harvesting
interleaved parallel tracks (sorry, don't know the exact term)
where you work every second (or 3rd, 4th...) track to prevent Y turns.
non-convex fields: can generate path for non-convex fields.
This is an experimental feature.
If there is no convex solution for a field or the non-convex
solution results in significantly less tracks it'll choose the
non-convex solution.
A non-convex solution will always have the middle of the field
divided in two or more blocks which will be worked one by one.
Once a block is finished, the course will lead to the next block
on the innermost headland path. This is not optimal but will
cover the entire field and the course will always remain within
the field.
In general, the algorithm will try to create the minimum amount
of blocks and avoids creating blocks with just a few tracks.

Upcoming features:

integration with Courseplay so courses can be generated
in game
optimized headland track generation to prevent skipping
fruit at corners.

Installation
The package is for Windows and contains everything needed
to run, including a lua interpreter and the LOVE graphical
framework.
This is the Windows version, I don't have access to Mac
computers. If you are willing to install lua and LOVE,
you can run the scripts on a Mac too.
Just unzip the latest release from https://github.com/pvajko/course-generator/releases
to any folder you like. The release has all the required
tools bundled.
Alternatively, if you have lua and LOVE (love2d.org) installed
and the executables in the PATH you can clone or unzip the
source from git.
Usage


Find your game folder. On Windows, this will most likely
be under My Documents\My Games\FarmingSimulator2017


Under that, there is a CoursePlay_Courses folder with
a subfolder for each installed map. This is where
the Courseplay courses are stored for each map.
Note the full path to these folders.


MAKE A BACKUP OF YOUR CoursePlay_Courses FOLDER! This is
a beta version, there may be bugs in there I don't want
you to lose your saved courses!


Now switch to the folder where you unzipped the course
generator and type:
lua.exe startCourseGenerator.lua 
for example, on my computer this would be:
lua53.exe startCourseGenerator.lua ""c:\Users\Peter\Documents\My Games\FarmingSimulator2017\CoursePlay_Courses\FS17_coldboroughParkFarm.SampleModMap""
for the Coldborough Farm map. Make sure there is no \ at the very
end of the folder name just before the "".


Alternatively, you can load a previously saved field with:
lua53.exe startCourseGenerator.lua 
(Courseplay saves the fields into the savegame folder)
for example, on Savegame 8 on my computer would be:
lua53.exe startCourseGenerator.lua ""c:\Users\Peter\Documents\My Games\FarmingSimulator2017\savegame8


You should now see a list which contains either the
the saved courses of the map (if you started it with a map folder)
or the saved fields of the savegame.
Has limited support for course folders: when a new course is created
from an existing one it'll be in the same folder as the original.
If created from a field, it'll be in the root folder.


Select a fieldwork course or a field by typing in the number of the course
and pressing enter. The course generator will use this course
or field to generate the course.
If you selected a course, the course generator will use the
outermost headland pass of the course without alterations and
build everything based on that.
Remember, the course must have a headland track!
Selecting non-fieldwork courses or fieldwork courses with no
headland tracks will result in errors.
If you selected a field, the course generator uses the field
boundary to generate the headland tracks.


Next, you have to select the course where you want to save
the generated course. Or, you can create a new course and
type in the name.


After you confirm the creation/overwrite of the course,
the course generator window appears showing the outline of the
field.


Set the width and number of passes using the w/W and p/P keys.


Next, you'll have to define where to start the headland track.
Use the right mouse button to place a marker where your want to
start it, just outside the field boundary.


The course is now generated. Green is the headland, blueish
are the tracks in the center of the field and the red line is
the path between the two.
The green dot is where the course starts, the red is where it ends.


By pressing 'c' you can toggle between clockwise or counterclockwise
headland tracks.


If you want you can reverse the course so it starts in the middle
and ends with the headland passes. This is great for sowing and
other, non harvesting fieldwork.


You can experiment with the various settings (you may need to
press g to regenerate the course).


If you like what you see, press s to save the course.


If you are in a game, you'll have to quit it and restart to be
able to see the new course, otherwise it won't show up.


Load the new course and test.


Thanks for giving it a try. If you have any problems, report it on
github https://github.com/pvajko/course-generator/issues or
at courseplay@vajko.name.
Peter
",13
christopher4lis/canvas-boilerplate,JavaScript,"Canvas Boilerplate is the go-to solution for quickly creating modern canvas pieces using ES6 and webpack.
Getting Started


Clone the repo:
git clone https://github.com/christopher4lis/canvas-boilerplate.git



Install dependencies:
yarn

or
npm install



Run webpack:
npm start



Your canvas piece should open up automatically at http://localhost:3000 and you should see 'HTML CANVAS BOILERPLATE' on hover.
",187
chenjian158978/chenjian.github.io,CSS,"ChenJian Blog
2017.04.01
博客今日开通，ChenJian Blog
从开始搭建到上线，大概有半年有余。
需要做


找一个好的模版，我才用Huxpro/huxpro.github.io，认真阅读里面的帮助说明；


学习一些基本知识，例如markdown, photoshop, git，jekyll, ruby等等。如果你不是IT的话，还要了解基本的html，javascript等；


项目clone下来后，一个字：改。图片注意大小，尺寸，颜色，这里建议会用ps的actions，把操作记录下来；改各种链接，例如github,zhihu账户等等；


给博文md文件添加前缀，规范命名，配图等等。这步弄完后，基本博客上没啥药弄的了；


评论使用disque插件，这个要注册disque账户，同时注册shortname;


购买域名。买的阿里云的com域名，注意多买几年的，后续续费很贵。绑定博客与域名这方面内容查查就知晓了。


2017.04.05
已经完成以下内容


完成https。采用Cloudflare方式


图片大小的修改。让加载更快


添加搜索功能


改变md文件中的""年""的问题


2017.04.06
已经完成以下内容


进一步修改博客内容error


增加侧边的目录栏，与搜索，并是其始终保持在一个页面jQuery-One-Page-Nav


添加所有博文添加侧边框选项


添加底部的""前后""翻动键


修改FRIENDS一栏


再次缩小图片大小，更新icon


2017.04.07
已经完成以下内容


更新icon


修改about内容


序列化部分文章


添加RSS订阅,又取消掉了


添加下栏分享功能，插件share.js


博文末尾添加cc4协议


2017.04.08
已经完成以下内容


长文章序列化完成


修改cc4部分错误


打算做的


进一步修改博客的问题


博客的运维


至此，整个博客基本完成
2017.04.13

给README添加参考博文

2017.04.20

添加google search console文件

2017.05.02


修改404链接错误


添加新博文


提高可读性将题目改为中文，副标题改为英文


2017.05.03


添加网站地图


添加baidu认证文件


2017.05.27


修改博文中代码片段的高亮问题，使用highlight.js中的styles:solarized-dark.css


修改部分css文件


2017.08.30


更新网站地图


添加代码下载链接


联系gmail更换点击模式


2018.06.06


添加html5的音乐播放


更新部分""歌词翻译""博文


2018.06.15


更换为aplayer+metingjs在线播放音乐


更换博文头文件


2018.06.16

解决yaml的头信息（front matter）问题，主要是换行格式问题，应该为LF格式！

参考

使用jekyll和hexo搭建免费博客
Hexo搭建博客教程
Hexo theme制作笔记-长期更新
一步步在GitHub上创建博客主页-1

",2
lrusso/3DObjectMaker,HTML,"3D Object Maker
3D Object Maker compatible with models in STL, OBJ and 3DS format. You can export your work ready to print in 3D (STL format) or to keep working on it later (SCENE format).

Web version
https://lrusso.github.io/3DObjectMaker/3DObjectMaker.htm
App version
https://play.google.com/store/apps/details?id=ar.com.lrusso.dobjectmaker
HOW TO USE THIS SOFTWARE
Add geometric shapes (from the right panel) to the plataform to create your own object. Also you can import STL, OBJ and 3DS models to the plataform. Later, export the object as STL file (for 3D printing) or as a SCENE file (to keep working on it later).
HOW TO CUT OBJECTS

Add object A to the plaform.
Add object B to the platform.
Select object B.
Select the material 'Hollow' (from the right panel).
Export the work as a STL file (the object B will erase every object, partially or entirely, that is within it's space). Depending of how complex are the objects, the device may take a few minutes to perform the task.


HOW TO FUSION OBJECTS

Add object A to the plaform.
Add object B to the platform.
Select object B.
Select any material (except 'Hollow') from the right panel.
Export the work as a STL file.


HOW TO MOVE AROUND THE PLATFORM


In the App: One finger to rotate, two fingers to zoom in and out and three fingers to move the camera.


In the Web: With your mouse, hold left-click and move the mouse to rotate, use the mouse wheel to zoom in and out and hold the right-click and move the mouse to move the camera.


",2
tkhamez/neucore,PHP,"

Neucore
An application for managing access for EVE Online players to external services
of an alliance.
Overview
Objectives:

Management of groups for players.
An API for applications to read these groups (and more).
Access to ESI data of all members.
Login via EVE SSO.

For more information, see the doc directory, including Documentation,
an API overview, and some screenshots.
This project consists of two applications, the Backend
and the Frontend.
A preview/demo installation is available at https://neucore.herokuapp.com.
Installation
EVE API Setup

Visit https://developers.eveonline.com or https://developers.testeveonline.com
Create a new application (e.g.: Neucore DEV)
Connection Type: ""Authentication & API Access"", add the required scopes. Scopes for the backend
are configured with the environment variable BRAVECORE_EVE_SCOPES.
Set the callback to https://your.domain/login-callback

App Setup
Server Requirements

PHP 7.1+ with Composer, see backend/composer.json for necessary extensions
Node.js 8 or 10, npm 6 (other versions may work, but are not tested)
MariaDB or MySQL Server
Apache or another HTTP Server

Set the document root to the web directory.
A sample Apache configuration is included in the Vagrantfile file and there
is a .htaccess file in the web directory.
A sample Nginx configuration can be found in the doc directory nginx.conf


Java 8+ runtime (only for openapi-generator)

Install/Update
Clone the repository or download the distribution
(the distribution does not require Composer, Node.js or Java).
Copy backend/.env.dist file to backend/.env and adjust values or
set the required environment variables accordingly.
Make sure that the web server can write in backend/var/logs and backend/var/cache.
Please note that both the web server and console user write the same files to backend/var/cache,
so make sure they can override each other's files, e. g. by putting them into each other's group
(the app uses umask 0002 when writing files and directories).
If available, the app uses the APCu cache in production mode. This must be cleared during an update
(depending on the configuration, restart the web server or php-fpm).
Archive file
If you downloaded the .tar.gz file, you only need to run the database migrations and seeds and,
depending on the update method, clear the cache:
cd backend
rm -rf var/cache/{di,http,proxies}
vendor/bin/doctrine-migrations migrations:migrate --no-interaction
bin/console doctrine-fixtures-load

Git
If you have cloned the repository, you must install the dependencies and build the backend and frontend:
./install.sh or
./install.sh prod
Cron Job
Set up necessary cron jobs, e. g. 3 times daily (adjust user and paths):
0 4,12,20 * * * neucore /var/app/backend/bin/run-jobs.sh

The output is logged to backend/var/logs.
First login and Customization
Read the backend documentation on how to make yourself an admin,
then you can navigate to ""Admin"" -> ""Settings"" and change texts, links and images that are specific to your
installation.
Using Vagrant
Only tested with Vagrant 2 + libvirt.

vagrant up creates and configures the virtual machine.
If the Vagrant file changes, run vagrant provision to update the VM.
vagrant destroy will completely remove the VM.

Please note that the rsync synchronization method used is a one-way synchronization from host to virtual
machine that is performed each time vagrant up or vagrant reload is executed.
The Vagrant setup will create the file backend/.env with correct values for the database connection.
The values for the EVE application must be adjusted.
Deploy on Heroku
You can deploy the application on a free Heroku account.

Create a new app
Add a compatible database, e. g. JawsDB Maria.
Add the necessary config vars (see backend/.env.dist file)
Add build packs in this order:

heroku buildpacks:add heroku/java
heroku buildpacks:add heroku/nodejs
heroku buildpacks:add heroku/php

Logs are streamed to stderr instead of being written to files.
Final notes
Origin
The software was originally developed for the Brave Collective,
when CCP shut down the old API and we had to replace our Core system.
This is also where the name ""Neucore"" comes from.
Related Software
Clients for the application API are available on the Brave Collective GitHub for PHP and Python:

neucore-api
neucore-api-python

Contact
If you have any questions or feedback, you can contact Tian Khamez on Tweetfleet Slack
(get invites here).
",2
alcros33/MHW-Costume-Armor,C++,"
MHW Costume Armor
MHW Costume Armor is a Monster Hunter World MOD which includes graphic user interface to customize the layered armor equipped.
It is a C++ implementaion of the original MHW Transmog MOD based on a decompilation of it.
Available also at Nexus Mods
Some Benchmarks
I tested out against original Transmog on My computer (i7 6700, GTX 970M, 8GB RAM) on what I considered ""Normal"" workload.
AKA: MHW open, Firefox playing a youtube video and Discord Running in the back.
I measured the time of the ""data retreival"" part and memory usage.

Original Transmog -> 23 seconds (Average) and 250MB RAM (Peak)
Costume Armor -> 4 seconds (Average) and 60MB RAM (Peak)

Release!
Checkout the compiled binaries on the latest Release !
State of Development

Memory reading (working properly).
Memory writing (working properly).
GUI Basic Functions (working properly).
GUI Design and vanity (A E S T H E T I C).
GUI Additional features :

Select ComboBox Instead of ID Input (working properly).
Save and Load ArmorSets by Name (working beta).
""Unsafe Mode"" (working beta).
Delete and Rename Saved Sets(pending...)



GUI Preview


Dependencies To Build

CMake # Download Link!
QT5 (Select MinGW 7.30)# Download Link!
MinGW 7.30 # When installing QT5 pick the actual MinGW compiler from the ""tools"" section
Python 3 and openpyxl (pip install openpyxl)

Add the following folders to Path C:\Qt\Tools\mingw730_64\bin and C:\Qt\5.12.0\mingw73_64\bin (Guide Info).
Extra Libraries that I include
Logging powered by EasyLogging++ Available here
Json powered by nholmannJson Available her
Sorry I'm to lazy to do the git submodule sutff.
Building Using MinGW
$ mkdir build
$ cd build
$ cmake .. -DCMAKE_BUILD_TYPE=[Debug | Release] -G ""MinGW Makefiles""
$ mingw32-make
Building Using Visual Studio

Visual Studio 2017
Have MSVC x64 in your Qt install

Build either with CMake directly, or use File > Open > CMake in Visual
Studio, then select Debug or Release, and press build.
Directly with CMake
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=[Debug | Release] -G ""Visual Studio 15 2017 Win64""
cmake --build . --config [Debug | Release]
",13
Donaldliu94/DonaldLiu94.github.io,CSS,"Welcome to the GitHub page of Donald Liu's Personal Website!
Live Site
LinkedIn
Github
Introduction
I am a Software Engineer who is committed and driven about the future of autonomous technology, and wants to bridge the gap of technology and human experiences. With my detailed-oriented experience in web development using React/Redux, Ruby on Rails, JavaScript, PostgreSQL, HTML5, and CSS3, I am excited and capable of resolving challenges individually as well as collaboratively.
Projects


Pixel 800

Single-page 500px-clone web application, a photography platform with emphasis on improving user's experience on uploading photos, liking photos, and searching photo processes.
Stacks: React.js, Redux, Ruby on Rails, PostgreSQL, NPM
Live Site
GitHub



Floppy Duck

An interactive 2D-platform game where a player navigates a duck and dodges pipe and missle obstacles.
Technologies: JavaScript, HTML5, NPM
Live Site
GitHub



Visit the live site to find out more!
",3
known/Known,C#,"Known
Known is a .NET framework.
",2
iceiix/stevenarella,Rust,"Stevenarella

Multi-protocol Minecraft-compatible client written in Rust
Don't expect it to go anywhere, just doing this for fun.
Images


In action: http://gfycat.com/NeedyElaborateGypsymoth
Community chatroom
We have a chatroom on EsperNet: irc.esper.net server, #stevenarella channel.
Join with your favorite IRC client or Matrix.
Protocol support



Game version
Protocol version
Supported?




1.14
477
✓


19w02a
452
✓


18w50a
451
✓


1.13.2
404
✓


1.12.2
340
✓


1.11.2
316
✓


1.11
315
✓


1.10.2
210
✓


1.9.2
109
✓


1.9
107
✓


15w39c
74
✓


1.8.9
47
✓


1.7.10 + Forge
5
✓



Stevenarella is designed to support multiple protocol versions, so that client
development is not in lock-step with the server version. The level of
support varies, but the goal is to support major versions from 1.7.10
up to the current latest major version. Occasionally, snapshots are also supported.
Support for older protocols will not be dropped as newer protocols are added.
Credits
Thanks to @thinkofname for
the original Steven (Rust),
which Stevenarella is an updated and enhanced version of.
Downloads
Windows users can download pre-compiled builds from here: https://ci.appveyor.com/project/iceiix/stevenarella
(Select your platform, Click the artifacts tab and download Steven.zip)
The Visual Studio 2017 Redistributable is required to run these builds.
Building
Requires Rust stable version 1.34.1 or newer to build.
Compile and run:
cargo run --release
Just compile:
cargo build --release
For progress on web support, see www/.
Running
Standalone
Just running Stevenarella via a double click (Windows) or ./stevenarella (everything else)
will bring up a login screen followed by a server list which you can select a server
from.
License
Dual-licensed MIT and ApacheV2
",92
microsoft/BuildXL,C#,"Microsoft Build Accelerator

Introduction
Build Accelerator, BuildXL for short, is a build engine originally developed for large internal teams at Microsoft, and owned by the Tools for Software Engineers team, part of the Microsoft One Engineering System internal engineering group. Internally at Microsoft, BuildXL runs 30,000+ builds per day on monorepo  codebases up to a half-terabyte in size with a half-million process executions per build, using distribution to thousands of datacenter machines and petabytes of source code, package, and build output caching. Thousands of developers use BuildXL on their desktops for faster builds even on mega-sized codebases.
BuildXL accelerates multiple build languages, including:

MSBuild (using new features under development in MSBuild 16 which will ship in future versions of Visual Studio 2019 and the .NET Core SDK)
CMake (under development)
Its own internal scripting language, DScript, an experimental TypeScript based format used as an intermediate language by a small number of teams inside Microsoft

BuildXL has a command-line interface. There are currently no plans to integrate it into Visual Studio. The project is open source in the spirit of transparency of our engineering system. You may find our technology useful if you face similar issues of scale. Note that BuildXL is not intended as a replacement for MSBuild or to indicate any future direction of build languages from Microsoft.
Documentation
The BuildXL documentation main page is here.
Examples and Demos
See the Examples/ folder for basic project examples. See the Demos page for information about various technical demos like using the process sandboxing code.
Building the Code
Build Status - Azure DevOps Pipelines

Command Line Build and Test
This repo uses DScript files for its own build. From the root of the enlistment run: bxl.cmd which will:

Download the latest self-host engine release.
Pull all needed packages from NuGet.org and other package sources.
Run a debug build as well as the unit tests locally.
Deploy a runnable bxl.exe to: out\bin\debug\net472.

Note you do not need administrator (elevated) privileges for your console window.
If you just want to do a 'compile' without running tests you can use: bxl.cmd -minimal after which you can find the binaries in out\bin\debug\net472.
Other build types can be performed as well:

bxl -deployConfig release : Retail build
bxl /vs : Converts DScript files into MSBuild .proj files and generates a .sln for the project at out\vs\BuildXL\BuildXL.sln

Windows
You should use Windows 10 with BuildXL. You do not need to install Visual Studio to get a working build, but see the section below on using VS with BuildXL for developing in the BuildXL codebase.
macOS
To run BuildXL on macOS you need to install:

Microsoft .NET Core SDK for macOS
The latest Mono runtime
If you want to run and load the sandbox to enable fully observed and cacheable builds, you also have to turn off System Integrity Protection (SIP) on macOS. SIP blocks the installation of the unsigned kernel extension (or Kext) produced by the build.

To start building, go to the root of the repository and run ./bxl.sh --minimal in your preferred terminal. Just like bxl.cmd, this bash script also supports several flags for your convenience.
Using BuildXL With Visual Studio
Because we don't have deep VS integration for BuildXL at this time, you can use bxl /vs which will convert the DScript files into MSBuild .proj files and generates a .sln for the project under out\vs\BuildXL\ with a base filename matching the top-level directory of your enlistment. So for example if your enlistment directory is c:\enlist\BuildXL, the generated solution file will be out\vs\BuildXL\BuildXL.sln.
Contributing
See CONTRIBUTING.
",463
johnynek/bosatsu,Scala,"The Bosatsu Programming Language

Bosatsu (菩薩) is the transliteration in Japanese of the sanskrit bodhisattva.
A bodhisattva is someone who can reach enlightenment but decides not to, to
help others achieve that goal.  -- Wikipedia
Bosatsu is a simple, non-turing complete language designed for configuration, queries and scripting. It
borrows from Python, Haskell,
Dhall and Rust.
An example of Bosatsu
Here is a working Bosatsu program to solve the first Project Euler problem:
package Euler/One

# see:
# https://projecteuler.net/problem=1
# Find the sum of all the multiples of 3 or 5 below 1000.

operator == = eq_Int
operator % = mod_Int

def operator ||(x, y):
  True if x else y

def keep(i):
  (i % 3 == 0) || (i % 5 == 0)

def sum(as): as.foldLeft(0, add)

# >>> sum(i for i in xrange(1000) if keep_fn(i))
# 233168
computed = [i for i in range(1000) if keep(i)].sum

test = Assertion(computed == 233168, ""expected 233168"")

For more details see the language guide in particular the section on syntax
Use cases
Currently we have only implemented type-checking, the package system, and an interpreter to evalute expressions. This could
already be useful if you want to give some programmability to configuration that can load, type-check and evaluate the configuration
before executing the rest of the scala or java code.
As a JSON templating engine
Along with Bazel Bosatsu can be used as a JSON generation
system, which could be useful for generating complex configurations in a way that has type-checking
and ability to compose. For a working example see this example.
cd test_workspace
bazel build ...
cat bazel-build/testjson.json

Future features
We would like to implement a number of features:

a REPL
a java backend and bazel rules which can call java and scala functions
a skylark backend to allow writing strongly typed bazel rules compiling to untyped skylark

",134
emacs-ess/ESS,Emacs Lisp,"


ESS
Git development branch of Emacs Speaks Statistics: ESS.
For more info, see our web page at https://ess.r-project.org/
",465
emacs-ess/ESS,Emacs Lisp,"


ESS
Git development branch of Emacs Speaks Statistics: ESS.
For more info, see our web page at https://ess.r-project.org/
",465
kids-first/kf-portal-etl,Scala,"


kf-portal-etl
The Kids-First ETL is built on Scala, Spark, Elasticsearch, HDFS, Postgresql, MySQL etc.
Dependencies
Before building this application, the following dependencies must be built and added to your local maven (.m2) directory.
Shaded json4s and jackson


Clone from repository
git clone git@github.com:kids-first/scalapb-json4s-shade.git



Publish Locally with sbt
cd scalapb-json4s-shade
sbt "";clean;assembly;publishLocal""



ES Model


Clone from repository
git clone git@github.com:kids-first/kf-es-model.git



Maven install
cd kf-es-model
mvn install



Build
To build the application, run the following from the command line in the root directory of the project
sbt "";clean;assembly""

Then get the application jar file from
${root of the project}/kf-portal-etl-pipeline/target/scala-2.11/kf-portal-etl.jar

Configuration
The Kids-First ETL uses lightbend/config as the configuration library. kf_etl.conf.sample defines all of the configuration objects used by ETL.
The top namespace of the configuration is io.kf.etl. To refer to any specific configuration object, please prefix the namespace string.

spark defines how the ETL connects to Spark environment
postgresql defines how the ETL connects to the PostgreSQL server, where the raw data to be transformed is stored
elasticsearch defines how the ETL connects to the Elasticsearch environment
hdfs defines how the ETL connects to the HDFS cluster. The HDFS cluster is used by the ETL for intermediate data writing

root: defines the root path of the entire ETL dataset under HDFS


processors defines all of the processors. Processors are the basic execution units of the ETL. Each processor has its own configurations, which means a processor could refer to the top level configuration objects or could also customize extra configuration objects to complete its specific job. At runtime, the ETL Context will pass the specific configlet to each processor.

download dumps clinical data from PostgreSQL and HPO data from MySQL to dump_path
file_centric transforms and generates the data for the file-centric index in Elasticsearch.

data_path defines where the processor stores both intermediate and output data
write_intermediate_data defines if the processor will write the intermediate data into data_path. It is optional; if not available in the file, false will be the default value


participant_entric transforms and generates the data for the participant-centric index in Elasticsearch
index stores the generated data from the above processors in Elasticsearch

release_tag defines how to generate a release tag as the suffix of the index name. It is format-free, which means one could define any necessary configuration objects needed for the current implementation

release_tag_class_name is a full class name of the release_tag implementation class and is required. At runtime, the ETL will pass the configlet to this class.







The ETL has a system environment variable called kf.etl.config which accept a URL string. The following are supported as values:

classpath:///.../${file_name}, URL scheme classpath:// is not defined in JDK, however theETL defines it. Remember to mixin ClasspathURLEnabler trait
file:///.../${file_name}
http://${http_server_ip}/.../${file_name}

If kf.etl.config is not provided when the application is submitted to Spark, the ETL will search the root of the class path for the default file with the name kf_etl.conf, otherwise the application will quit.
Running the Application
There are some dependencies to run Kids-First ETL, refer to submit.sh.example

PostgreSQL: primary storage for Kids-First data models
MySQL: contains HPO reference data. The reference DB dump file is found here
Spark 2.3.0
Configuration file: refer to Configuration for the format and contents of the file

To submit the application to Spark, run the following in the command line
${SPARK_HOME}/bin/spark-submit --master spark://${Spark master node name or IP}:7077 --deploy-mode cluster --class io.kf.etl.ETLMain --driver-java-options ""-Dkf.etl.config=${URL string for configuration file}"" --conf ""spark.executor.extraJavaOptions=-Dkf.etl.config=${URL string for configuration file}"" ${path to kf-portal-etl.jar} ${command-line arguments}
Command line arguments
Kids-First ETL supports command-line argument -study_id id1 id2. In this case, ETL will filter the dataset retrieved from data service through study_ids.
The second command-line argument is -study_id_file ${url of file}, for exmaple: -study_id_file file://${path} or -study_id_file s3://${bucket}/${key}, etc
The third command-line argument is -release_id rid
To submit the application with study_ids, run the following:
${SPARK_HOME}/bin/spark-submit --master spark://${Spark master node name or IP}:7077 --deploy-mode cluster --class io.kf.etl.ETLMain --driver-java-options ""-Dkf.etl.config=${URL string for configuration file}"" --conf ""spark.executor.extraJavaOptions=-Dkf.etl.config=${URL string for configuration file}"" ${path to kf-portal-etl.jar} -study_id id1 id2 -release_id rid

",5
Bystroushaak/tinySelf,Python,"
tinySelf is an experimental programming language inspired by the Self lang, with the emphasis on the word experimental.
I would like something like the glasswork rack you know from the chemistry pictures, but for computation.

The point of the experiment is not in the language itself, but in all kind of different stuff that can be done with it. tinySelf should be lightweight, compact, collapsible, interchangeable, universal computational apparatus for anything I can possibly hope to make.

Articles

Github Wiki
Articles about tinySelf


Planning
See

https://github.com/Bystroushaak/tinySelf/projects/1
https://github.com/Bystroushaak/tinySelf/projects/2

",9
facebook/facebook-android-sdk,Java,"Facebook SDK for Android


This open-source library allows you to integrate Facebook into your Android app.
Learn more about the provided samples, documentation, integrating the SDK into your app, accessing source code, and more at https://developers.facebook.com/docs/android
TRY IT OUT

Check-out the tutorials available online at https://developers.facebook.com/docs/android/getting-started
Start coding! Visit https://developers.facebook.com/docs/android/ for tutorials and reference documentation.

FEATURES

Login
Sharing
Places
Messenger
App Links
Analytics
Graph API
Marketing

STRUCTURE
The SDK is separated into modules with the following structure. Each box represents a module with an
estimated size when included into an app (when included using proguard and supporting a single language).
+--------------------------------------------------------------+
|                                                              |
|  Facebook-android-sdk : 367.95 kb                            |
|                                                              |
+--------------------------------------------------------------+
+----------+ +----------+ +----------+ +----------+ +----------+
|          | |          | |          | |          | |          |
|          | |          | |          | |          | |          |
| Facebook | | Facebook | | Facebook | | Facebook | | Facebook |
| -Login : | | -Share : | | -Places :| |-Messenger: |-Applinks:|
|          | |          | |          | |          | |          |
| 276.94 kb| | 282.46 kb| | 53.76 kb | | 91.63 kb | | 65.96 kb |
+----------+ +----------+ |          | |          | |          |
+-----------------------+ |          | |          | |          |
|                       | |          | |          | |          |
| Facebook-Common : N/A | |          | |          | |          |
|                       | |          | |          | |          |
+-----------------------+ +----------+ +----------+ +----------+
+--------------------------------------------------------------+
|                                                              |
| Facebook-Core : 52.61 kb                                     |
|                                                              |
+--------------------------------------------------------------+

Example: Including the Facebook-Login module (which depends on Facebook-Common and Facebook-Core) would
increase your app's size by an estimated 276.94 kb and including just Facebook-Core would only increase
your size by 52.61 kb.
USAGE
Facebook SDKs are broken up into separate modules as shown above. To ensure the most optimized use of
space only install the modules that you intend to use. To get started, see the Installation section below.
INSTALLATION
Facebook SDKs are published to Maven as independent modules. To utilize a feature listed above
include the appropriate dependency (or dependencies) listed below in your app/build.gradle file.
dependencies {
    // Facebook Core only (Analytics)
    implementation 'com.facebook.android:facebook-core:4.33.0'

    // Facebook Login only
    implementation 'com.facebook.android:facebook-login:4.33.0'

    // Facebook Share only
    implementation 'com.facebook.android:facebook-share:4.33.0'

    // Facebook Places only
    implementation 'com.facebook.android:facebook-places:4.33.0'

    // Facebook Messenger only
    implementation 'com.facebook.android:facebook-messenger:4.33.0'

    // Facebook App Links only
    implementation 'com.facebook.android:facebook-applinks:4.33.0'

    // Facebook Android SDK (everything)
    implementation 'com.facebook.android:facebook-android-sdk:4.33.0'
}

You may also need to add the following to your project/build.gradle file.
buildscript {
    repositories {
        mavenCentral()
    }
}

GIVE FEEDBACK
Please report bugs or issues to https://developers.facebook.com/bugs/
You can also join the Facebook Developers Group on Facebook (https://www.facebook.com/groups/fbdevelopers/) or ask questions on Stack Overflow (http://facebook.stackoverflow.com)
CONTRIBUTING
We are able to accept contributions to the Facebook SDK for Android. To contribute please do the following.

Follow the instructions in the CONTRIBUTING.mdown.
Submit your pull request to the master branch. This allows us to merge your change into our internal master and then push out the change in the next release.

LICENSE
Except as otherwise noted, the Facebook SDK for Android is licensed under the Facebook Platform License (https://github.com/facebook/facebook-android-sdk/blob/master/LICENSE.txt).
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.
DEVELOPER TERMS


By enabling Facebook integrations, including through this SDK, you can share information with Facebook, including information about people’s use of your app. Facebook will use information received in accordance with our Data Use Policy (https://www.facebook.com/about/privacy/), including to provide you with insights about the effectiveness of your ads and the use of your app.  These integrations also enable us and our partners to serve ads on and off Facebook.


You may limit your sharing of information with us by updating the Insights control in the developer tool (https://developers.facebook.com/apps/[app_id]/settings/advanced).


If you use a Facebook integration, including to share information with us, you agree and confirm that you have provided appropriate and sufficiently prominent notice to and obtained the appropriate consent from your users regarding such collection, use, and disclosure (including, at a minimum, through your privacy policy). You further agree that you will not share information with us about children under the age of 13.


You agree to comply with all applicable laws and regulations and also agree to our Terms (https://www.facebook.com/policies/), including our Platform Policies (https://developers.facebook.com/policy/) and Advertising Guidelines, as applicable (https://www.facebook.com/ad_guidelines.php).


By using the Facebook SDK for Android you agree to these terms.
",4811
AutoGentoo/AutoGentoo,C,"AutoGentoo
A gentoo portage binhost manager
Why use AutoGentoo
Overview
AutoGentoo is a socket server that can run on any Linux kernel. It will create a chroot enviroment to your specification (pretty much a make.conf and a profile).
Instead of downloading a stage3, autogentoo will emerge the basic profile packages into a new chroot folder. This will
give you binary packages with the compile flags (CFLAGS, CXXFLAGS etc.) you want from the start.
The advantage of a binhost
is that you can have one computer (build server) compiling packages for many different systems. This way all package building
will be kept separate from the client computer. This means if an update breaks your system (very common with Gentoo) it won't
affect your client computer, only the chroot environment on the build server.
How it works

Runtime dependencies

pcre2
curl
libarchive

",5
Hammermaps-DEV/SOHL-V1.9-Opposing-Force-Edition,C++,"Spirit of Half Life V1.9 - Opposing-Force Edition
Spirit of Half Life V1.9 with Opposing-Force Monsters, Weapons..
",2
yashha/wp-nuxt,JavaScript,"wp-nuxt







📖 Release Notes
Features
The module adds WP-API to your nuxt application.
The size of the library is 49,8 kB minified (14 kB gzipped). 
Setup


Add wp-nuxt dependency using yarn or npm to your project


Add wp-nuxt to modules section of nuxt.config.js


{
  modules: [
    // Simple usage
    'wp-nuxt',

    // With options
    ['wp-nuxt', {
      endpoint: 'https://wp.kmr.io/wp-json'
      /* other options of WP-API */
    }],
 ]
}
Usage
You can use the API of WP-API using the injected 'app.$wp'. (s. example)
Example
<script>
export default {
  async asyncData ({ app, store, params }) {
    return { articles: await app.$wp.posts().perPage(10) }
  }
}
</script>
License
MIT License
Copyright (c) yashha

",69
darekf77/morphi,TypeScript,"

Morphi v2 (BETA)

    Isomorphic framework in TypeScript for NodeJS back-end and browser (web/mobile) front-end. 
    
 Do no repeat yourself anymore, never !!! 



-NO MORE SEPARATION BETWEEN BACKEND AND FRONTEND !
- Isomorphic classes as Angular/Ionic Services and ExpressJS controllers. 
One node_modules folder for browser, mobile and server.
- Write everything in TypeScript 
      all the time and  automaticly strip off server code for browser/ionic versions.
- Use power of  TypeORM
       framwork to write awesome, robust, clean NodeJS backend 
       connected to SQLite, Mysql, WebSQL, MongoDB and many others... 
- Keep amazing code consistency
        thanks to isomorphic entities classes, that you can use to
        create backend tables and also inside frontend-angular templates with type checking.        
      
- Change business logic in the fastest possible way!
- generate  formly objects from entites
        - have typescript typechecking from entity in db to html template!
        - set default values to entities
       
Support project to develop only amazing, creative things
      in the future... 
      
 Project is under development, but every 
        juicy features just works... try example.
      



        TODO: 
        - firebase like... realtime update of backend/frontend (in progress) 
        - extended authentication based on isomorphic decorators metadata and db roles (in progress) 
        - isomoprhic unit tests (with inheritance) in mocha/jasmine  
        - vscode extension to support @backend, @backendFunc #regions 




Instalation
Global CLI
First install global tool:
npm install -g morphi

Create new Isomorphic Workspace project ( mobile, web, server + sqlite db + basic rest authentication )
morphi new:workspace myAwesomeIsomrphicApp

OR create Isomorphic Single File  backend/frontend project
morphi new:simple myAwesomeIsomrphicApp

Visual Studio Code (recommended editor)
Open your project in VSCode to get the maximum of development experience.
code myAwesomeIsomrphicApp

Installation - Isomorphi Workspace
Link one version of node_modules
Once you have your app opened...

run:
npm install
npm run link

to install and link node_module folder for each subproject.
Build and run sub-projects with auto-reload

isomorphic-lib: npm run build:watch #or morphi:watch   + F5 to run server
angular-client: npm run build:watch + open browser http:\\localhost:4200
ionic-client: npm run build:watch + open browser http:\\localhost:8100

Instead of npm run build:watch you can also open each sub-project in separated vscode window code <sub-project-name>
and press: ctrl(cmd) + shift + b.
Installation - Isomorphi Single File
Install dependencies
Once you have your app opened... run:
npm install

Run isomorphic build
morphi build:watch

Run frontend client
npm run build:watch

Backend controllers, entities directly in the frontend
Isomorphic TypeScript Classes
The main reason why this framework has huge potential is that you can use your backend code
( usualy ExpressJS, REST controllers ) as Anguar 2+ (or any other js framework)
services, to access your RESTfull backend without dealing with backend patches and unessery source code.
This will allow you to change business login very quickly, without confusion and keep
no separation between your frontend/backend application.
Morphi CLI tool
Is responsible for magic behing stripping of backend code for browser version ( web app or ionic mobile app).
Regions @backend,@backendFunc
The difference between @backend and @backendFunc is that @backendFunc will replace code with 'return undefined' (it is needed for typescript compilation) and @backend
will precisely delete all lines between.
HOW IT WORKS:
+ Isomorphi initilization
Initialization for backend and frontend
import { Morphi } from 'morphi';


@Morphi.init({
  controllers: [ /* Your controllers clases here */ UserController  ],
  entites: [ /* Your entites clases here */  User ],
  host: 'http://localhost:4000' // host for backend and frontend,
  //#region @backend
  config: { /* Your db config clases here */  }
  //#endregion
})

To inject providers you can use
import { Morphi } from 'morphi';

...
  providers: [  ...Morphi.Providers  ]
...

+ Isomorphi backend
Typeorm isomorphic ENTITY in NodeJS backend:
import { Morphi } from 'morphi';

@Morphi.Entity()
export class User {

    //#region @backend
    @Morphi.Orm.Column.Primary()
    //#endregion
    id: number;

    //#region @backend
    @Morphi.Orm.Column.Custom()
    //#endregion
    name: string;

    //#region @backend
    @Morphi.Orm.Column.Custom()
    //#endregion
    surname: string;

    fullName() {
      return `${this.name} ${this.surname}`
    }
    
    //#region @backend 
    password: string
    //#endregion

}
Morphi isomorphic CONTROLLER in NodeJS backend:
import { Morphi } from 'morphi'

@Morphi.Controller() 
class UserController {
		
	@Morphi.Http.GET()
	getAllUser() {
		//#region @backendFunc 
		const  repository  =  this.connection.getRepository(User) as  any;
		return  async (req, res) => {
			return await  this.repository.findAll();
		}
		//#endregion
	}	
}

After isomorphic compilation by morphi;
morphi build

or (incremental watch build)
morphi build:watch

will be generated browser version.
+ Generated borwser version
The result for browser client will be like below:
Typeorm isomorphic ENTITY in browser version:
import { Morphi } from 'morphi/browser';

@Morphi.Entity()
export class User {

    id: number;

    name: string;

    surname: string;

    fullName() {
      return `${this.name} ${this.surname}`
    }
}
Morphi isomorphic CONTROLLER in browser version:
import { Morphi } from 'morphi/browser'

@Morphi.Controller()
class UserController {
	 // 'return undefined' is for purpose on the browser side
	 // The function body will be replaced through decorate
	 // to access REST endpoint
	@Morphi.Http.GET()
	getAllUser() { 
    return undefined; 
    }	
}
Angular 2+ services
Morphi.Controller(s) you can use as Angular 2+ services. If you
used Morphi.Entity your browser <-> backend REST communication will
keep entity type and automaticly reproduce it.
@Component({
	selector:  'app-test',
	templateUrl:'app-test.html'
})
class  AppTestComponent  implements  OnInit {

	// Inject isomorphic class as service into component
	constructor(public users: UserController) { } 
	
	async ngOnInit() {
          const data = await this.users.getAllUsers().received;
          const users = data.body.json;
          const firstUser = users[0]
          console.log( firstUser instanceOf User ) // true
	}
}
Directly in html template
To simplify object receiving from backend you can use async pipes (available with Angular4+)
and really make you MVVM amazing.
Morphi and Angular4+ lets you use backend functions in html frontend template.
app-test.html
Users:
<ul   *ngIf=""users.getAllUsers().received.observable | async; else loader; let users"" >

  <li  *ngFor=""let user of users""> 
  		{{user.id}} {{user.fullName()}} <!-- BACKEND FUNCTION IN FRONTEND TEMPLATE ! -->
		  <br>
		<input type=""name"" [(NgModel)]=""user.name"" >
  </li>

</ul>

<ng-template #loader> loading users...  </ng-template>

Of course Angular services can be used inside Angular web and Ionic mobile apps.
",7
instagrambot/instabotai,Python,"
Website | Read the Docs | Contribute | Buy Instagram Expert Marketing

InstabotAi 🤖
Instabotai is an instagram bot with face detection that uses the undocumented Web API. Instabotai can reupload photo to feed, reupload photo to stories, comment, like and DM users if a face is detected on image.
Unlike other bots, Instabotai does not require Selenium or a WebDriver. Instead, it interacts with the API over simple HTTP Requests. It runs on most systems.

Requirements

Python 3.6+
Min 20-30 Profiles to scrape or it will repost same image when no new image is posted in list.

Face detection at work on a live webcam

This script scrapes images from users and then repost, like and comment their images if face is detected with your own tags.
Demo:
https://www.instagram.com/siliconeheaven/
The script does not work with new accounts. If you know how to fix, send me a message.
To install script with Docker:
docker pull reliefs/instabotai

docker run [imageid] -u username -p password -l josephineskriver,wolfiecindy -t ""#like4like#follow4follow""

To install script on Windows:
Install Cmake
download link : https://github.com/Kitware/CMake/releases/download/v3.14.1/cmake-3.14.1.zip
Install Dblib
Download dlib ‘.wheel’ file as ur system requirnment (use link bellow)
download link : https://pypi.python.org/simple/dlib/
Open cmd navigate to dlib wheel file path and hit command
pip install dlib_file_name.wheel

Then run
git clone https://github.com/instagrambot/instabotai.git --recursive
cd instabotai/

pip install -r requirements.txt

python example.py -u yourusername -p password -l josephineskriver,wolfiecindy -t ""#like4like#follow4follow""


And press Enter.
To install script on Linux:
Ubuntu:
apt-get install python-dev python3-dev
sudo apt install g++
sudo apt install cmake
sudo apt install python3-pip

Arch Linux:
sudo pacman -S python3-dev
sudo pacman -S cmake
sudo pacman -S python3-pip

First, make sure you have dlib already installed with Python bindings:

How to install dlib from source on macOS or Ubuntu

Then do
git clone https://github.com/instagrambot/instabotai.git

cd instabotai

sudo pip install -r requirements.txt

python example.py -u yourusername -p password -l josephineskriver,wolfiecindy -t ""#like4like#follow4follow""


Troubleshoot
If you are getting Illegal Instruction with face_recognition follow this guide:
https://github.com/ageitgey/face_recognition/issues/11#issuecomment-475482716
AttributeError: 'module' object has no attribute 'face_recognition_model_v1'
Solution: The version of dlib you have installed is too old. You need version 19.7 or newer. Upgrade dlib.
For Dlib install error run
python3 setup.py install --no DLIB_USE_CUDA
DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE TERMS AND CONDITIONS FOR COPYING, DISTRIBIUTION AND MODIFICATION.

You just do WHAT THE FUCK YOU WANT TO.

Technology is destructive only in the hands of people who do not realize that they are one and the same process as the universe.
",62
LPGhatguy/rbx-dom,Rust,"rbx-dom

rbx-dom is a collection of crates to help represent, serialize, and deserialize Roblox DOMs. The goal of rbx-dom is to have a common format that projects like Rojo can use for handling Instances efficiently.
rbx_dom_weak


This crate was recently renamed from rbx_tree to rbx_dom_weak.
Weakly-typed Roblox DOM implementation. Defines types for representing instances and properties on them.
rbx_dom_lua
Roblox Lua implementation of DOM APIs, allowing Instance reflection from inside Roblox. Uses a data format that's compatible with rbx_dom_weak to facilitate communication with applications outside Roblox about instances.
rbx_xml


Serializer and deserializer for for Roblox's XML model and place formats, rbxmx and rbxlx.
rbx_binary


Serializer and deserializer for for Roblox's binary model and place formats, rbxm and rbxl.
rbx_reflection


Roblox reflection information for working with Instances in external tooling.
Property Type Coverage



Property Type
Example Property
rbx_dom_weak
rbx_dom_lua
rbx_xml
rbx_binary




Axes
ArcHandles.Axes
❌
❌
❌
❌


BinaryString
Terrain.MaterialColors
✔
➖
✔
❌


Bool
Part.Anchored
✔
✔
✔
✔


BrickColor
Part.BrickColor
✔
❌
❌
❌


CFrame
Camera.CFrame
✔
✔
✔
❌


Color3
Lighting.Ambient
✔
✔
✔
❌


Color3uint8
N/A
✔
✔
✔
❌


ColorSequence
Beam.Color
✔
✔
✔
❌


Content
Decal.Texture
✔
✔
✔
❌


Enum
Part.Shape
✔
✔
✔
❌


Faces
BasePart.ResizableFaces
❌
❌
❌
❌


Float32
Players.RespawnTime
✔
✔
✔
❌


Float64
Sound.PlaybackLoudness
✔
✔
✔
❌


Int32
Frame.ZIndex
✔
✔
✔
❌


Int64
Player.UserId
✔
✔
✔
❌


NumberRange
ParticleEmitter.Lifetime
✔
✔
✔
❌


NumberSequence
Beam.Transparency
✔
✔
✔
❌


PhysicalProperties
Part.CustomPhysicalProperties
✔
✔
✔
❌


ProtectedString
ModuleScript.Source
✔¹
✔
✔¹
❌


Ray
RayValue.Value
✔
❌
❌
❌


Rect
ImageButton.SliceCenter
✔
✔
✔
❌


Ref
Model.PrimaryPart
✔
✔
✔
❌


Region3
N/A
❌
✔
❌
❌


Region3int16
Terrain.MaxExtents
❌
✔
❌
❌


SharedString
N/A
❌
❌
❌
❌


String
Instance.Name
✔
✔
✔
✔


UDim
UIListLayout.Padding
✔
✔
✔
❌


UDim2
Frame.Size
✔
✔
✔
❌


Vector2
ImageLabel.ImageRectSize
✔
✔
✔
❌


Vector2int16
N/A
✔
✔
✔
❌


Vector3
Part.Size
✔
✔
✔
❌


Vector3int16
N/A
✔
✔
✔
❌


QDir
Studio.Auto-Save Path
⛔
⛔
⛔
⛔


QFont
Studio.Font
⛔
⛔
⛔
⛔



✔ Implemented | ❌ Unimplemented | ➖ Partially Implemented | ⛔ Never

ProtectedString is deserialized as String, which is technically lossy but does not change semantics in practice

Outcome
This project has unveiled a handful of interesting bugs and quirks in Roblox!

GuiMain.DisplayOrder is uninitialized, so its default value isn't stable
MaxPlayersInternal and PreferredPlayersInternal on Players are scriptable and accessible by the command bar
Instantiating a NetworkClient will turn your edit session into a game client and stop you from sending HTTP requests
ContentProvider.RequestQueueSize is mistakenly marked as serializable
Trying to invoke game:GetService(""Studio"") causes a unique error: singleton Studio already exists
Color3 properties not serialized as Color3uint8 would have their colors mistakenly clamped in the XML place format. This was bad for properties on Lighting.
ColorSequence's XML serialization contains an extra value per keypoint that was intended to be used as an envelope value, but was never implemented.

License
rbx-dom is available under the MIT license. See LICENSE.txt for details.
",9
LPGhatguy/rbx-dom,Rust,"rbx-dom

rbx-dom is a collection of crates to help represent, serialize, and deserialize Roblox DOMs. The goal of rbx-dom is to have a common format that projects like Rojo can use for handling Instances efficiently.
rbx_dom_weak


This crate was recently renamed from rbx_tree to rbx_dom_weak.
Weakly-typed Roblox DOM implementation. Defines types for representing instances and properties on them.
rbx_dom_lua
Roblox Lua implementation of DOM APIs, allowing Instance reflection from inside Roblox. Uses a data format that's compatible with rbx_dom_weak to facilitate communication with applications outside Roblox about instances.
rbx_xml


Serializer and deserializer for for Roblox's XML model and place formats, rbxmx and rbxlx.
rbx_binary


Serializer and deserializer for for Roblox's binary model and place formats, rbxm and rbxl.
rbx_reflection


Roblox reflection information for working with Instances in external tooling.
Property Type Coverage



Property Type
Example Property
rbx_dom_weak
rbx_dom_lua
rbx_xml
rbx_binary




Axes
ArcHandles.Axes
❌
❌
❌
❌


BinaryString
Terrain.MaterialColors
✔
➖
✔
❌


Bool
Part.Anchored
✔
✔
✔
✔


BrickColor
Part.BrickColor
✔
❌
❌
❌


CFrame
Camera.CFrame
✔
✔
✔
❌


Color3
Lighting.Ambient
✔
✔
✔
❌


Color3uint8
N/A
✔
✔
✔
❌


ColorSequence
Beam.Color
✔
✔
✔
❌


Content
Decal.Texture
✔
✔
✔
❌


Enum
Part.Shape
✔
✔
✔
❌


Faces
BasePart.ResizableFaces
❌
❌
❌
❌


Float32
Players.RespawnTime
✔
✔
✔
❌


Float64
Sound.PlaybackLoudness
✔
✔
✔
❌


Int32
Frame.ZIndex
✔
✔
✔
❌


Int64
Player.UserId
✔
✔
✔
❌


NumberRange
ParticleEmitter.Lifetime
✔
✔
✔
❌


NumberSequence
Beam.Transparency
✔
✔
✔
❌


PhysicalProperties
Part.CustomPhysicalProperties
✔
✔
✔
❌


ProtectedString
ModuleScript.Source
✔¹
✔
✔¹
❌


Ray
RayValue.Value
✔
❌
❌
❌


Rect
ImageButton.SliceCenter
✔
✔
✔
❌


Ref
Model.PrimaryPart
✔
✔
✔
❌


Region3
N/A
❌
✔
❌
❌


Region3int16
Terrain.MaxExtents
❌
✔
❌
❌


SharedString
N/A
❌
❌
❌
❌


String
Instance.Name
✔
✔
✔
✔


UDim
UIListLayout.Padding
✔
✔
✔
❌


UDim2
Frame.Size
✔
✔
✔
❌


Vector2
ImageLabel.ImageRectSize
✔
✔
✔
❌


Vector2int16
N/A
✔
✔
✔
❌


Vector3
Part.Size
✔
✔
✔
❌


Vector3int16
N/A
✔
✔
✔
❌


QDir
Studio.Auto-Save Path
⛔
⛔
⛔
⛔


QFont
Studio.Font
⛔
⛔
⛔
⛔



✔ Implemented | ❌ Unimplemented | ➖ Partially Implemented | ⛔ Never

ProtectedString is deserialized as String, which is technically lossy but does not change semantics in practice

Outcome
This project has unveiled a handful of interesting bugs and quirks in Roblox!

GuiMain.DisplayOrder is uninitialized, so its default value isn't stable
MaxPlayersInternal and PreferredPlayersInternal on Players are scriptable and accessible by the command bar
Instantiating a NetworkClient will turn your edit session into a game client and stop you from sending HTTP requests
ContentProvider.RequestQueueSize is mistakenly marked as serializable
Trying to invoke game:GetService(""Studio"") causes a unique error: singleton Studio already exists
Color3 properties not serialized as Color3uint8 would have their colors mistakenly clamped in the XML place format. This was bad for properties on Lighting.
ColorSequence's XML serialization contains an extra value per keypoint that was intended to be used as an envelope value, but was never implemented.

License
rbx-dom is available under the MIT license. See LICENSE.txt for details.
",9
dx7/ruby-bitly,Ruby,"
A simple bit.ly ruby client.
 
Configuration
You need to load the gem:

require 'ruby-bitly'

Set global configuration:

Bitly.config do |c|
  c.login   = 'login-here'
  c.api_key = 'api-key-here'
  c.use_ssl = false # read more below
  c.proxy   = 'http://localhost:8888' # read more below
end

Or set them individualy:

Bitly.login   = 'login-here'
Bitly.api_key = 'api-key-here'
Bitly.use_ssl = false # read more below
Bitly.proxy   = 'http://localhost:8888' # read more below

Or set them on methods if you prefer (see it below).
Shorten
bitly = Bitly.shorten(long_url: ""https://dx7.github.io/"")

# setting credentials
bitly = Bitly.shorten(long_url: ""https://dx7.github.io/"", domain: ""my.do"", login: ""login-here"", api_key: ""api-key-here"")

# result
bitly.short_url   #=> ""http://bit.ly/2dAjjfo""
bitly.long_url    #=> ""https://dx7.github.io/""
bitly.new_hash?   #=> true
bitly.global_hash #=> ""2dAkyet""
bitly.user_hash   #=> ""2dAjjfo""
bitly.success?    #=> true
Expand
bitly = Bitly.expand(short_url: ""http://bit.ly/2dAjjfo"")

# setting credentials
bitly = Bitly.expand(short_url: ""http://bit.ly/2dAjjfo"", login: ""login-here"", api_key: ""api-key-here"")

# result
bitly.short_url   #=> ""http://bit.ly/2dAjjfo""
bitly.long_url    #=> ""https://dx7.github.io/""
bitly.global_hash #=> ""2dAkyet""
bitly.user_hash   #=> ""2dAjjfo""
bitly.success?    #=> true
Get Clicks
bitly = Bitly.get_clicks(short_url: ""http://bit.ly/2dAjjfo"")

# setting credentials
bitly = Bitly.get_clicks(short_url: ""http://bit.ly/2dAjjfo"", login: ""login-here"", api_key: ""api-key-here"")

# result
bitly.short_url      #=> ""http://bit.ly/2dAjjfo""
bitly.user_hash      #=> ""2dAjjfo""
bitly.global_hash    #=> ""2dAkyet""
bitly.user_clicks    #=> 0
bitly.global_clicks  #=> 1
bitly.success?       #=> true
Error handling
# if something goes wrong you can check
bitly.success? #=> false
bitly.error    #=> 'INVALID_LOGIN'
Proxy
All calls will use the proxy specified by environment variable ""http_proxy"" by default.

You can set the proxy directly if you prefer:

Bitly.proxy = 'http://localhost:8888'
SSL
All calls will use SSL by default. You can disable it:

Bitly.use_ssl = false
Command Line
Usage: bitly [options] URL
  -l, --login LOGIN                You need a free Bitly login and api key. Sign up here: http://bit.ly/a/sign_up.
  -k, --api-key KEY                You can find your api key here: http://bit.ly/a/your_api_key.

  -d, --domain DOMAIN              The short domain to use: either bit.ly, j.mp, bitly.com or a custom short domain.
                                   This option will override the default short domain selected in your Bitly account settings.

  -s, --shorten                    Given a long URL, returns a Bitlink.
  -e, --expand                     Given a Bitlink, hash or custom path, returns the target (long) URL.
  -u, --user-clicks                The total count of clicks to this user's Bitlink.
  -g, --global-clicks              The total count of the corresponding Bitly aggregate hash.

  -h, --help                       Print this help.
  -v, --version                    Print version.

Basic examples:
  bitly -s http://dx7.github.io
  bitly -e http://bit.ly/2dAjjfo
  bitly --user-clicks http://bit.ly/2dAjjfo
  bitly --global-clicks http://bit.ly/2dAjjfo
Authentication
bit.ly API requires authentication credentials.

Using commmand line you can supply credentials as parameters. For example:
  bitly -l <login-here> -k <api-key-here> -s http://dx7.github.io

Or you can create the file ~/.bitly (YAML format) with that content:
  login: <login-here>
  api_key: <api-key-here>
Author
dx7 ~ dx7(a)protonmail.ch
Copyright
Copyright (c) 2010 dx7. Licensed under the MIT License:
http://www.opensource.org/licenses/mit-license.php
",14
romanblanco/KaktusBOT,Python,"Telegram bot notifying about news from mobile operator Kaktus

pip install -r requirements.txt --user
Give it a try!
",3
Qiskit/qiskit-terra,Python,"Qiskit Terra

Qiskit is an open-source framework for working with Noisy Intermediate-Scale Quantum (NISQ) computers at the level of pulses, circuits, and algorithms.
Qiskit is made up of elements that work together to enable quantum computing. This element is Terra and is the foundation on which the rest of Qiskit is built.
Installation
We encourage installing Qiskit via the pip tool (a python package manager), which installs all Qiskit elements, including Terra.
pip install qiskit
PIP will handle all dependencies automatically and you will always install the latest (and well-tested) version.
To install from source, follow the instructions in the contribution guidelines.
Creating Your First Quantum Program in Qiskit Terra
Now that Qiskit is installed, it's time to begin working with Terra.
We are ready to try out a quantum circuit example, which is simulated locally using
the Qiskit BasicAer element. This is a simple example that makes an entangled state.
$ python

>>> from qiskit import *
>>> q = QuantumRegister(2)
>>> c = ClassicalRegister(2)
>>> qc = QuantumCircuit(q, c)
>>> qc.h(q[0])
>>> qc.cx(q[0], q[1])
>>> qc.measure(q, c)
>>> backend_sim = BasicAer.get_backend('qasm_simulator')
>>> result = execute(qc, backend_sim).result()
>>> print(result.get_counts(qc))
In this case, the output will be:
{'00': 513, '11': 511}
A script is available here, where we also show how to
run the same program on a real quantum computer via IBMQ.
Executing your code on a real quantum chip
You can also use Qiskit to execute your code on a
real quantum chip.
In order to do so, you need to configure Qiskit for using the credentials in
your IBM Q account:
Configure your IBMQ credentials


Create an IBM Q > Account if you haven't already done so.


Get an API token from the IBM Q website under My Account > Advanced > API Token.


Take your token from step 2, here called MY_API_TOKEN, and run:
>>> from qiskit import IBMQ
>>> IBMQ.save_account('MY_API_TOKEN')


If you have access to the IBM Q Network features, you also need to pass the
URL listed on your IBM Q account page to save_account.


After calling IBMQ.save_account(), your credentials will be stored on disk.
Once they are stored, at any point in the future you can load and use them
in your program simply via:
>>> from qiskit import IBMQ
>>> IBMQ.load_accounts()
Those who do not want to save their credentials to disk should use instead:
>>> from qiskit import IBMQ
>>> IBMQ.enable_account('MY_API_TOKEN')
and the token will only be active for the session. For examples using Terra with real
devices we have provided a set of examples in examples/python and we suggest starting with using_qiskit_terra_level_0.py and working up in
the levels.
Contribution Guidelines
If you'd like to contribute to Qiskit Terra, please take a look at our
contribution guidelines. This project adheres to Qiskit's code of conduct. By participating, you are expected to uphold this code.
We use GitHub issues for tracking requests and bugs. Please
join the Qiskit Slack community
and use our Qiskit Slack channel for discussion and simple questions.
For questions that are more suited for a forum we use the Qiskit tag in the Stack Exchange.
Next Steps
Now you're set up and ready to check out some of the other examples from our
Qiskit Tutorials repository.
Authors and Citation
Qiskit Terra is the work of many people who contribute
to the project at different levels. If you use Qiskit, please cite as per the included BibTeX file.
License
Apache License 2.0
",2488
StevenUpForever/leetcode-java,Java,"leetcode-java
Introduction
The legacy_code implementations in Java sorted by solving method.
problem's title as the class name.
With as many possible solutions which from brute force to optimized solutions (by time complexity and real runtime).
Include explanations and time/space complexity analysis.
Coding style
Followed Google Java Coding style.
Source code
Under the root/src folder ./src/
Clone from this repo:
git clone https://github.com/StevenUpForever/LeetCode_Java.git
pull requests and Issues are welcome
中文
这个repo是以解题方法为序的LeetCode Java的解法，类名是当前问题的标题。
提供从蛮力法到优化解法的尽可能多种的循序渐进的优化解法 （排序方式按照时间复杂度和实际运行时间）。
包含了时间/空间复杂度的分析。
编码风格
遵循 Google Java Coding style。
源代码
在当前目录的src文件夹下 ./src/
复制当前repo:
git clone https://github.com/StevenUpForever/LeetCode_Java.git
欢迎提出pull requests and Issues
",2
JavaDominicano/conference,JavaScript,"conference
Contents behind the https://jconfdominicana.org web site
This site is baked with JBake, a static site generator.
The idea behind JBake is very simple: contents is written using a markup language and ""baked"" with template engines into actual HTML. Everything is generated statically and you can upload the generated site wherever you want.
How to contribute
There are a few guidelines that we need contributors to follow so that we can have a chance of keeping on
top of things.
Prerequisites

A Github account
Gradle

Getting Started

Fork the repository
Clone your fork repository

Example:
git clone https://github.com/ecabrerar/conference.git
cd conference
Exploring the project structure
The directory src/jbake contains the classic JBake folder contents:
src
 |-- jbake
       |-- assets    : static assets (images, css, ...)
       |-- content   : blog posts, ...
       |-- templates : HTML templates (by default, uses FreeMarker, but we are using Thymeleaf)

To do any change, you have to explore JBake and Thymeleaf a template engine for Java.
Generating the output
you can generate the site by running the following command:
./gradlew -i jbake
after the rendering step, you should now have a new directory:
build
  |-- jbake
into which you will find the generated HTML contents.
Running the site
./gradlew bakePreview
Browse to http://localhost:8080
Submitting Changes

Push your changes to your fork.
Submit a pull request.

Additional Resources

General GitHub documentation
GitHub pull request documentation

",2
andrewcooke/choochoo,Jupyter Notebook,"Choochoo (ch2)
An open, hackable and free training diary.
Please see the full
documentation.  This page
contains only some images and a Technical
Overview.
The following plots are generated automatically from the diary -
clicking a ""link"" starts Jupyter and pushes the page to the browser.
The pages can be edited and serve as an introduction to accessing the
data manually.


Technical Overview
The system includes:


An SQLite3 database containing time series data.


An interface to move data between the database and Pandas
DataFrames.


A FIT reader to import new data.


Algorithms to derive new statistics from the data (using Pandas for
efficiency).


Pipelines to apply the algorithms to new data on import (in parallel
processes for efficiency).


An embedded Jupyter server to explore the data.


Pre-written scripts to present graphical data views via Jupyter.


A ""diary"" to present textual data and allow data entry.


The database has an SQLAlchemy ORM interface.  The schema separates
""statistics"" (named time series data) from the source (which might be
direct entry, read from a FIT file, or calculated from pre-existing
values).  SQL tracks dependencies to avoid stale values.
The pipelines are Python classes whose class names are also configured
in the database.
The data are stored in an ""open"" format, directly accessible by third
party tools, and easily backed-up (eg by copying the database file).
When the database format changes scripts are provided to migrate
existing data (see package ch2.migraine).  Data extracted from FIT
files are not migrated - they must be re-imported.
Support libraries include: FIT file parsing; spatial R-Trees; reading
elevation data from SRTM files; estimating power from elevation and
speed; Fitness / Fatigue models; detection of pre-defined segments;
clustering of routes; climb detection.
The ""diary"" view, where the user enters data, is also configured via
the database.  So the fields displayed (and the statistics collected)
can be customized.  This configuration can include ""schedules"" which
control when information is displayed (eg: weekdays only; every other
day; second Sunday in the month).
The combination of customizable diary fields and scheduling allows
training plans to be entered and displayed.  This presents a steep
learning curve but is ultimately very flexible - ""any"" training plan
can be accommodated.  Python code for generating example plans is
included (see package ch2.config.plan).
Currently the program is single-user (ie the data in the database are
not grouped by user).  Multiple users can co-exist using separate
database files.
Choochoo collects and organizes time-series data using
athlete-appropriate interfaces.  It facilitates calculations of
derived statistics and extraction of data for further analysis using
Python's rich data science tools.  Both data and code are open and
extensible.
",29
KeyboardCowboy/iawriter-templates,CSS,"iawriter-templates
Custom templates for IA Writer
",3
KeyboardCowboy/iawriter-templates,CSS,"iawriter-templates
Custom templates for IA Writer
",3
krestaino/themer.js,JavaScript,"Themer.js
Spice up your app with themes. Themer.js features include:

Automatic night/day theme switching
System prefers-color-scheme support
Android meta theme-color support
Custom themes
Manual control over everything

Demo
https://themer.js.kmr.io
Quick Start
Install
# using yarn
$ yarn add themer.js

# using npm
$ npm install themer.js

Define the light and dark themes
To use the auto or system themes, you must define a light and dark Theme object.
import Themer from ""themer.js"";

const config = {
  ""light"": {
    ""styles"": {
      ""--app-background-color"": ""#f1f1f1"",
      ""--primary-text-color"": ""#555""
    }
  },
  ""dark"": {
    ""styles"": {
      ""--app-background-color"": ""#242835"",
      ""--primary-text-color"": ""#f1f1f1""
    }
  }
}

// instantiate Themer.js
const themer = new Themer(config);

Setting a theme
import Themer from ""themer.js"";
import { light, dark } from ""./themes/index.js"";

const themer = new Themer({ light, dark });

// set theme to dark
themer.setTheme(dark);

// set theme to auto
themer.setAuto();

// set theme to system
themer.setSystem();

Setting a custom theme
Pass a valid Theme object to setTheme().
import Themer from ""themer.js"";

const custom = {
  ""styles"": {
    ""--app-background-color"": ""#f1f1f1"",
    ""--primary-text-color"": ""#555""
  }
};

const themer = new Themer();

themer.setTheme(custom);

API
Themer(config)


Arguments:

{Object} config



Details: Instantiate Themer.js.


Usage:
import { light, dark } from ""./themes/index.js"";

const config = {
  light,
  dark,
  debug: true,
  onUpdate: (theme) => console.log(theme)
};

const themer = new Themer(config);



See also: Config object


Themer.setAuto()


Details: Sets the active theme to light during the day and dark during the night.


Restrictions:

light and dark themes must be defined.
Requires user geolocation consent.



Usage:
Themer.setAuto();



Themer.setSystem()


Details: Sets the active theme to system.


Restriction:

light and dark themes must be defined.
The browser must support prefers-color-scheme.



Usage:
Themer.setSystem();

See also: Themer.systemThemeSupport()


Themer.setTheme( theme )


Arguments:

{Object | string} theme



Details: Sets the active theme.


Usage:
const dark = {
  ""android"": ""#242835"",
  ""styles"": {
    ""--app-background-color"": ""#242835""
  }
};

Themer.setTheme(dark);



See also: Theme object


Themer.systemThemeSupport()


Details: Helper function to determine browser support for the system theme.


Returns: boolean


Usage:
// Chrome 76, Firefox 67, Safari 12.1
Themer.systemThemeSupport();
↳ true

// unsupported browsers
Themer.systemThemeSupport();
↳ false



See also: prefers-color-scheme


Config object



Key
Type
Description




debug
boolean
Log debug console statements.


onUpdate
function
A callback function that returns the set theme.


light
object
The dark theme.


dark
object
The light theme.



Example:
{
  debug: true,
  onUpdate: (theme) => console.log(theme),
  ""light"": {
    ""styles"": {
      ""--app-background-color"": ""#f1f1f1"",
      ""--primary-text-color"": ""#555""
    }
  },
  ""dark"": {
    ""styles"": {
      ""--app-background-color"": ""#242835"",
      ""--primary-text-color"": ""#f1f1f1""
    }
  }
}

Theme object



Key
Type
Description




android
string
Sets the meta theme-color.


styles
object
The theme CSS variables.



Example:
{
  ""android"": ""#f1f1f1"",
  ""styles"": {
    ""--app-background-color"": ""#f1f1f1"",
    ""--primary-text-color"": ""#555""
  }
}

Use the CSS variables anywhere in your CSS and it will update in real time to the active theme.
html {
  background-color: var(--app-background-color);
  color: var(--primary-text-color);
}

",16
zhedahht/leetcode,Go,"Leetcode Solutions In Go
I try to solve some LeetCode problems when I'm learning Go. All code here passes LeetCode tests. However, it might not be elegant Go code, because I'm still a Go amateur.
",11
facebook/redex,C++,"ReDex: An Android Bytecode Optimizer
ReDex is an Android bytecode (dex) optimizer originally developed at
Facebook. It provides a framework for reading, writing, and analyzing .dex
files, and a set of optimization passes that use this framework to improve the
bytecode.  An APK optimized by ReDex should be smaller and faster than its
source.
Go to https://fbredex.com for full documentation.
",4434
basinserver/jasm,Java,"JASM
Description
This project is a Java disassembler aimed at automatic analysis of JVM bytecode. It used to have rewriting functionality, and it may again in the future, but it does not at present.
Status
This project can accurately disassemble Minecraft, which is the primary test case.
Goals
The end goal is to write a Java decompiler and possibly deobfuscator.
Running
This project does not have a frontend, it is designed as a library with Classpath as the primary entrypoint. You can hack together a test as I have done, or write a simple CLI frontend.
API
Loading happens through the Classpath class. Klass represents a loaded and disassembled .class file, with all pointers to other classes, methods, constants, etc, resolved.
",3
zewenlee/RandomPicApi,PHP,"OASIS's Free API
My blog https://imoasis.cn ( I'm OASIS)
正在重写
",5
sarojaba/awesome-devblog,None,"Awesome Devblog


개요
국내 개발자 블로그 모음(으로 시작했으나 국내외 개인/단체 사이트로 바뀌어버린, 향후 개발자 사전으로 발전하고 싶은) Awesome Devblog 입니다.

목록은 실명으로 등록을 원칙으로 합니다. (개발자들이여. 음지에서 양지로 나와 유명해집시다.)
목록은 공개된 내용에 기반하여 작성합니다. 이 목록을 다른 프로젝트의 기반 데이터로 사용하셔도 Great! 단, 해당 프로젝트에 이 목록을 사용(또는 참고)했다 정도로만 남겨주세요.
목록을 기반으로 OPML 및 RSS도 함께 제공하고 있습니다. (단, 업데이트는 이 목록보다 늦을 수 있습니다.)
목록을 데이터 분석 또는 렌더링을 용의하게 하기 위해 db.yml 파일을 업데이트 해주세요.

README.md 의 일부분도 db.yml 파일에서 생성하고 있습니다. (평균 월 1회 생성)
OPML 파일은 db.yml 파일에서 생성하고 있습니다. Feed Reader의 국내 개인 목록에서 OPML 포맷을 다운로드 받을 수 있습니다.
각종 차트도 db.yml 파일에서 생성하고 있습니다. (Feed Reader의 통계 페이지에서 확인하실 수 있습니다.)
db.yml 파일을 이용한 2차 저작 환영합니다.


좋아요, 포크, 제보, 풀리퀘 모두 환영입니다.프로젝트에 관심있으시면 gitter 들어오셔서 문의바랍니다.

소개글

awesome-devblog 회고 2017년 2018년
개발자를 위한 (블로그) 글쓰기 intro
기술블로그 구독서비스 개발 후기 - 1부
(tech-trend) 구루들이 찾는다는 테크 트렌드 - 유명 블로그와 소셜 유명인
한국 오픈소스 프로젝트 랭킹 Top 100 - 69위

형제자매 프로젝트

AWESOME DEVBLOG(포탈) Main
어썸블로그(안드로이드)
어썸블로그(iOS)
어썸블로그(페이스북 페이지)
Daily DevBlog(뉴스레터)
DEVBLOG(메타블로그)

목차

국내 개인 사이트

ㄱ
ㄴ
ㄷ
ㄹ
ㅁ
ㅂ
ㅅ
ㅇ
ㅈ
ㅊ
ㅍ
ㅎ


국내 팀 사이트
국외 개인 사이트
국외 팀 사이트

국내 개인 사이트
ᄀ



Name
Blog
Description
Social




강규영
http://www.ecogwiki.com/




강관우
https://brunch.co.kr/@kd4
Java



강대명
https://charsyam.wordpress.com/
서버 사이드



강동혁
https://brunch.co.kr/@dongkang
인디 개발



강동희
https://medium.com/@kanglpmg
iOS, RxSwift



강명훈
http://kangmyounghun.blogspot.kr/
보안



강미경
http://minieetea.com/
기획



강병수
https://01010011.blog/
DevOps



강병욱
https://medium.com/@brillante9111
라이언봇



강성일
https://b.luavis.kr/
Python



강성훈
http://mearie.org/
카일루아



강성훈
https://medium.com/@devholic
RxJava



강성희
https://medium.com/@shaynekang




강준영
https://blog.juneyoung.io/
Web Backend/Server



강진우
https://brunch.co.kr/@alden
Linux



강태욱
http://daddynkidsmakers.blogspot.kr/
IoT



강한별
https://brunch.co.kr/@cloud09
데이터 분석



강형석
https://hskang9.github.io/
머신러닝



강홍구
https://wckhg89.github.io
Web



경준호
http://firejune.com/
Front-end



계주성
http://kyejusung.com/




김철민
http://blog.naver.com/kbs4674/
Ruby on Rails, AWS, Heroku



고득녕
http://blog.naver.com/nackji80/
네트워크



고명진
https://rjs1197.github.io/
C++



고명환
https://brunch.co.kr/@maru7091
스타트업



고유영
https://medium.com/@yooyoungko
Javascript



고종범
https://brunch.co.kr/@jbgo
애자일



고형호
http://hhko.tistory.com/
Functional Programming



곽민수
https://brunch.co.kr/@imagineer
싱가폴 데이터 엔지니어



구교준
http://danielku.com/
AWS



구자철
http://forest71.tistory.com/
Java



구종만
http://theyearlyprophet.com/
알고리즘



권기호
http://kwongyo.tistory.com/




권남
http://kwon37xi.egloos.com/
Java



권동준
https://mayajuni.github.io/
Web



권민재
https://mingrammer.com/
Python



권성준
https://gwonsungjun.github.io/
Back-end



권영재
https://nesoy.github.io/




권용근
https://kingbbode.github.io/
Spring



권용진
https://brunch.co.kr/@nsung
퀀트



권욱제
http://wookje.dance/




권윤학
http://web-front-end.tistory.com/
Web



권재명
http://dataninja.me/
Data Science



권정혁
http://xguru.net/guru
개발 트렌드



권진호
http://jinhokwon.tistory.com/
Full Stack



권창현
http://thoughts.chkwon.net/
산업공학



권태관
https://taetaetae.github.io/
Back-end



권태형
https://taebbong.github.io/
Node.js



권태환
http://thdev.tech/
Android, Kotlin



권혁우
https://medium.com/@khwsc1
React



권희정
https://gmlwjd9405.github.io/




김광현(광파리)
https://kwang82.blogspot.kr/
IT 소식



김국현
http://goodhyun.com/
IT 칼럼



김기훈
http://kihoonkim.github.io/
Agile



김길호
http://kilhokim.github.io/




김나솔
https://brunch.co.kr/@nassol
개발자 영어



김나은
https://brunch.co.kr/@appletreehill
EOS, HCI



김남윤
https://cheese10yun.github.io/
Node.JS, Spring



김남훈
http://namhoon.kim/
Jekyll, Firebase Korea



김놀부
https://nolboo.kim/
번역 모음



김대권
http://nacyot.com/




김대기
http://daegikim.github.io/
Web



김대현
http://hatemogi.com/
백엔드



김덕기
http://martian36.tistory.com/
워드프레스



김덕홍
http://insanehong.kr/




김도곤
http://dokonk.blogspot.kr/
DB



김도균
http://www.dokyun.pe.kr/




김도현
http://loboprix.com/




김도훈
https://dohoons.com/
Web



김동우
https://dongwoo.blog/
Javascript



김만수
https://mansoo-sw.blogspot.kr/




김명신
http://himskim.egloos.com/
Microsoft



김명주
http://blog.naver.com/bansuk78
Lagom



김명준
http://html5lab.kr/
프론트엔드



김민근
http://mingeun.com/
Laravel



김민서
http://blog.naver.com/mseokim011117
Swift



김민석
https://brunch.co.kr/@brunchqvxt
Spring Boot



김민수
http://www.kmshack.kr/
안드로이드



김민수
http://alwayspr.tistory.com/




김민장
http://minjang.github.io/
컴퓨터 아키텍처



김민준
https://velopert.com/
Front-end



김범준
http://bomjun.tistory.com/




김범준
http://nolsigan.github.io/




김범진
https://medium.com/@beejei




김병환
http://kimbyeonghwan.tumblr.com/
UX



김보원
https://boribap.github.io/
Block Chain



김봉현
https://harfangk.github.io/
Elixir



김상훈
https://interpiler.com/
IT 칼럼



김석기
https://brunch.co.kr/@neo3xdh
IT 칼럼



김선영
http://sunyzero.tistory.com/
Linux



김석준
https://seokjun.kim/
React



김선철
http://blog.naver.com/PostList.nhn?blogId=sckim007




김성빈
http://sungbine.github.io/
Web



김성수
http://sungsoo.github.io/




김성식
http://devopser.me/
DevOps



김성준
https://brunch.co.kr/@sungjoonkim
IT 컬럼



김성중
http://sungjk.github.io/




김성현
http://greemate.tistory.com/
윈도우즈 커널



김성호
http://shiren.github.io/
Front-end



김성훈
http://www.se.or.kr/
학교 생활



김성훈
http://elky84.github.io/
C++, C#, Ruby, Python



김수로
http://babysunmoon.tistory.com/




김수민
https://brunch.co.kr/@flatdesign
UX 디자인



김수보
https://subokim.wordpress.com/
IT 칼럼



김순식
http://i-bada.blogspot.kr/




김승호
http://raccoonyy.github.io/
Python



김슬기
http://blog.seulgi.kim/




김시은
https://brunch.co.kr/@springboot
Spring Boot



김영우
http://datajournal.kr/
데이터 분석



김영웅
http://keyassist.tistory.com/
Data Science



김영재
https://youngjaekim.wordpress.com/




김영재
http://haviyj.tistory.com/
Spring Boot



김영전
https://qpfmtlcp.github.io




김영하
https://brunch.co.kr/@fermat39




김영훈
https://blog.martinwork.co.kr/
AI, Javascript



김요섭
https://josephkim75.wordpress.com/




김용균
http://haruair.com/
Web



김용묵
http://moogi.new21.org/
한글 언어, C++



김용일
http://app-developer.tistory.com
iOS



김용준
https://kimkevin.net/
Android



김용현
http://blog.naver.com/drvoss
C++



김용환
http://knight76.tistory.com/




김용휘
https://ggoals.github.io/
Data Infrastructure



김우섭
https://brunch.co.kr/@linterpreteur




김우승
https://kimws.wordpress.com/
빅데이터



김우용
https://brunch.co.kr/@wedump
게임 개발



김원일
http://androidkr.blogspot.kr/
안드로이드



김원호
https://undefine.me/
Javascript



김인권
https://blog.naver.com/gi_balja




김인기
http://ingeec.tistory.com/
Web



김인숙
http://webholic.net/
Web



김재국
https://jaigouk.com/




김재원
http://epiloum.net/
Front-end



김재호
http://www.benjaminlog.com/
C++



김정원
https://swtpumpkin.github.io/
Javascript, Node



김정헌
http://feelteller.com/
Android



김정환
http://blog.jeonghwan.net/
Javascript, Node



김정환
https://blog.naver.com/writer0713/
Javascript, Java, Web



김정훈
https://wonderer80.github.io/
소프트웨어 개발



김종민
http://kimjmin.net/
Elastic Stack



김종민
http://blog.cmiscm.com/
인터렉티브 디벨로퍼



김종민
http://uroa.tistory.com/




김종민
https://jongmin92.github.io/
Spring



김종욱
http://catlog.kr/




김종인
http://zzong.net
Java, Spring



김종철
https://infoscis.github.io/
Front-end



김종하
https://dev.wisedog.net/
Python, Go, Javascript, Cloud



김종하
http://story.wisedog.net/
정적분석



김종헌
http://shovelman.tistory.com/
.NET



김종희
https://kimpaper.github.io/
Back-end



김준성
https://brunch.co.kr/@codertimo




김준영
https://junebuug.github.io/
Java / Python
 


김준철
http://jetalog.net/
Solr



김준형
https://medium.com/@ghilbut




김준환
http://topnanis.tistory.com/




김준희
https://wnsgml972.github.io/




김중근
https://swtools.review/
IT Tour Guider, SW Tools, SW Internals, Android



김지영
https://brunch.co.kr/@pubjinson




김지우
https://brunch.co.kr/@ken1224
스타트업, 음악산업



김지운
https://kishe89.github.io/
Javascript



김지헌
http://java.ihoney.pe.kr/
Java



김지현
https://hyeon.me/




김지홍
https://brunch.co.kr/@jihere1001
프로토타이핑



김진국
http://forensic-proof.com/
포렌식



김진섭
http://vmfhrmfoaj.gitlab.io/
Clojure



김진성
http://jinseong0928.blogspot.kr/




김진수
https://item4.github.io/
Web



김진영
https://brunch.co.kr/@lifidea
데이터 과학



김진욱
http://rein.kr/blog/




김짐
https://medium.com/@jimkimau
Front-end



김찬빈
https://blog.kesuskim.com/
Front-end



김창원
http://www.memoriesreloaded.net/
실리콘밸리



김창준
http://agile.egloos.com/
애자일



김청진
https://jinblog.kr
IT소식/리뷰, 개발(웹, 모바일)
  


김충섭
http://subicura.com/




김태곤
http://taegon.kim/
웹



김태균
http://blog.gaerae.com/




김태기
https://beyondj2ee.wordpress.com/
Java



김태영
https://tykimos.github.io/
머신러닝



김태완
http://taewan.kim/
Oracle



김태현
http://javaexpert.tistory.com/
블록체인, 웹, 모바일



김태호
http://www.androidhuman.com/
안드로이드



김태호
https://medium.com/@xissy
스타트업



김태환
http://thefinestartist.com/
Java, Android



김태헌
https://brunch.co.kr/@myte
해외 취업



김태훈
http://carpedm20.github.io/




김포프
http://kblog.popekim.com/




김한결
http://devkyeol.tistory.com/




김한솔
https://medium.com/@zvuc




김헌진
http://flymogi.tistory.com/
Android



김현남
http://booksummary.tistory.com/




김한웅
https://hanwong.github.io/
Front-end



김현유(미키김)
http://www.mickeykim.com/
구글



김형준
http://www.gisdeveloper.co.kr/
GIS



김형준
http://www.jaso.co.kr/
마이크로 서비스



김형준
http://www.smallake.kr/
트레이딩



김형철
https://sites.google.com/site/hcgoon
Java, 개인위키(주로 일본어)



김형록
http://rokrokss.com
NLP



김호동
http://cogniti-works.blogspot.kr/
nimf



김화수
http://flowerexcel.tistory.com/
C++



김환희
http://greentec.egloos.com/
게임개발



김훈민(김코딩)
http://huns.me/
Web



김희준
https://heejune.me/





ᄂ



Name
Blog
Description
Social




나윤환
https://nayunhwan.github.io/
React



남경진
https://kanteloper.github.io/
Centos



남궁민
https://medium.com/@minnamgoong
스타트업



남상욱
https://nso502354.github.io/
Swift



남세현
https://medium.com/@Nam_se
게임 개발



남영환
http://www.whynam.com/
Java, Javascript, Clojure



남정현
http://www.rkttu.com/
클라우드



남혜연
https://byline.network/author/smilla/
IT 뉴스



노경모
https://brightparagon.wordpress.com/
JavaScript, React, Webpack, 스타트업



노상범
https://medium.com/@sbroh




노찬우
https://rajephon.github.io/blog/




노아론
https://blog.aaronroh.org/
프레임워크, 오픈소스



노용환
http://bugsfixed.blogspot.kr/
윈도우즈 커널



노재민
http://korsnack.kr/




노현석
http://pluu.github.io/
Android




ᄃ



Name
Blog
Description
Social




도경태
http://keen.devpools.kr/




도창욱
https://medium.com/@cwdoh
Android




ᄅ



Name
Blog
Description
Social




류광
http://occamsrazr.net/
번역



류재영
http://longbe00.blogspot.kr/




류종택
http://ryulib.tistory.com/




류현오
https://activity.horyu.me





ᄆ



Name
Blog
Description
Social




마경욱
https://brunch.co.kr/@kyeongwook-ma
스타트업



맹윤호
http://maengdev.tistory.com/
Data Science



모상우
https://brunch.co.kr/@aidenswmo




문동선
http://dsmoon.tistory.com/
머신러닝



문동욱
https://evan-moon.github.io/
Graphics, Web



문준영
https://brunch.co.kr/@moonjoonyoung




문형환
http://blog.lael.be/




민경운
http://min-it.tistory.com/
Docker



민형기
http://pinkwink.kr/
Robot, Data Science




ᄇ



Name
Blog
Description
Social




박경욱
https://kyungw00k.github.io/




박경준
https://ryanpark.me/
Domain Driven Design



박경훈(HOONS)
http://hoonsbara.tistory.com/
닷넷



박광열
http://eastsocial.co.kr/
워드프레스



박기상
https://medium.com/@kpak
실리콘밸리



박미정
https://medium.com/@mjspring
Java



박민
https://isme2n.github.io/
Front-end



박민근(알콜코더)
http://blog.naver.com/agebreak
게임 개발



박민우
http://earlybird.kr/
Realm



박상권
http://gun0912.tistory.com/
Android



박상근
http://parksk.tistory.com/




박상길
http://likejazz.com/




박상길
https://kidmam.github.io/




박상민
https://sangminpark.blog/




박상일
https://tkddlf59.github.io/
AWS



박상훈
https://brunch.co.kr/@sanghoonpak
인도네시아 IT 소식



박성범
https://parksb.github.io
컴퓨터공학, 디자인



박성철
http://blog.fupfin.com/
Java



박석제
https://parkseokje.github.io/




박성현
https://helloworldpark.github.io/
Graphics



박승호
https://devwaf.blogspot.kr/
Front-end



박연오
https://bakyeono.net/
Python



박영록
http://youngrok.com/
위키나무



박용권
https://brunch.co.kr/@arawn




박용서
https://gs.saro.me




박영호
http://mellonia-lab.tistory.com/
Web



박은정
https://www.lucypark.kr/
데이터 과학



박인
https://brunch.co.kr/@lynnata
디지털 노마드



박일
http://parkpd.egloos.com/
게임 개발



박재성(자바지기)
http://javajigi.net/
Java



박재성
https://medium.com/@jspark141515




박재성
https://medium.com/@alberto.park
billboard.js



박재영
https://kujyp.github.io
ML DevOps, Python, Docker



박재현
http://wisefree.tistory.com/
IT 칼럼



박재호
http://jhrogue.blogspot.kr/
빅데이터, 인공지능



박정규
http://bagjunggyu.blogspot.kr/
리눅스



박정운
https://jungwoon.github.io/
Android



박정태
https://pjt3591oo.github.io/, http://blog.naver.com/pjt3591oo
Back-end, 서버



박종명
http://m.mkexdev.net/
기술사



박종영
https://medium.com/@mocona
iOS



박준규
https://brunch.co.kr/@kospoll-lab
인공지능



박준규
https://brunch.co.kr/@nattybear




박준영
https://swalloow.github.io/
BigData, AI



박준호
http://junojunho.com/
Front-end



박준호
https://blessingdev.wordpress.com/
Python



박지수
https://medium.com/@JisuPark




박지홍
https://medium.com/@ggikko
RxJava



박진서
http://jasonpark.me/
Algorithm



박진우
https://www.jinpark.net/




박찬민
https://walkinpcm.blogspot.kr/
AWS



박찬성
https://medium.com/@parkchansung
TensorFlow



박찬준
http://blog.naver.com/bcj1210
NLP



박찬엽
https://mrchypark.github.io/
R



박찬욱
http://chanwookpark.github.io/
Back-end



박천
https://say8425.github.io/
Ruby



박해선
https://tensorflow.blog/
TensorFlow



박훈
http://1ambda.github.io/
함수형 언어



박희근
http://sirini.net/grboard2/blog
GR BOARD



박희찬
https://blog.chann.kr/




반진우
https://mathbarn.wordpress.com/
Ubuntu



방준영
http://bangjunyoung.blogspot.kr/
Java



배기홍
http://www.thestartupbible.com/
IT 칼럼



배수한
https://blog.naver.com/soohan530/
고등학생 개발



배성혁
http://debop.tumblr.com/
스칼라



배진호
https://medium.com/@baejinho/
스타트업, 치킨모임



백재연
http://jybaek.tistory.com/
Cloud



백종찬
https://brunch.co.kr/@jeffpaik
블록체인



백기선
http://whiteship.me/
Java, Spring



백명석
https://brunch.co.kr/@cleancode
Java, OOP



백영진
http://koreaceladon.tistory.com/
Storage



범이
http://blog.daum.net/funfunction
FP



변규현
https://novemberde.github.io/
AWS



변성윤
https://zzsza.github.io/
Machine Learning, Deep Learning, Data Engineering, BigQuery



변정훈(Outsider)
http://blog.outsider.ne.kr/
Web



변수민
http://blog.suminb.com/




부종민
https://durtchrt.github.io/blog/
Java, Spring




ᄉ



Name
Blog
Description
Social




서강혁
https://medium.com/@darkrasid
Docker



서광열
http://gamecodingschool.org/
게임 개발



서민상
http://blog.naver.com/seo0511
Arduino



서보룡
http://inter6.tistory.com/
OpenStack



서시원
https://blog.seotory.com/
Full-stack



서오석
https://brunch.co.kr/@elijah17
Java



서용마
https://brunch.co.kr/@bonfire
Workflowy



서인석
http://isseo90.tistory.com/
디자인



서주영(천재태지)
http://seoz.com/
EFL, 타이젠



서창욱
http://scw0531.blog.me/
Web, Mobile, IoT



서충원
http://snowdeer.github.io/
C++, Linux



서한교
https://brunch.co.kr/@zalhanilll
디자인



성대경
http://healthydeveloper.tistory.com/
Algorithm



성동찬
http://gywn.net/
Database



소용환
http://www.sauru.so/
Docker, Elastic



손승하
http://sonseungha.tistory.com/
Linux



손영수(arload)
https://arload.wordpress.com/
개발방법론, Android



손찬욱
https://sculove.github.io/blog/
Web



송기원
http://blog.naver.com/agilesoft
Front-end



송민승
https://brunch.co.kr/@minseungsong
실리콘밸리



송성광
http://blog.saltfactory.net/
Ghost, AWS



송영길
https://youngsong.com/
창업



송영환
https://purluno.wordpress.com/
Akka



송원준
http://wonjun.kr/




송윤섭
https://songyunseop.github.io/
Python, Node.js



송은우
https://rampart81.github.io/
Python



송재윤
https://blog.jaeyoon.io/
Front-end



송주성(빈꿈)
http://emptydream.tistory.com/
IT 칼럼



송준현
https://medium.com/@it_sjh9973




송호연
https://brunch.co.kr/@chris-song/




송효진
https://lovetoken.github.io/
R



신기용
https://goodgid.github.io/
BE



송효진
https://lovetoken.github.io/
R



신경식
http://multeng.tistory.com/
AI



신관영
http://springsource.tistory.com/
Spring



신동성
https://brunch.co.kr/@adrenalinee31




신민욱
https://minwook-shin.github.io/
Python



신승환
http://www.talk-with-hani.com/




신영진
http://www.jiniya.net/
보안



신예지
http://dev.bloodevil.com/
Python



신용윤
http://uni2u.tistory.com/
네트워크



신은광
https://gracefullight.github.io/




신재명
https://medium.com/@Jaemyung




신재인
http://jaynewho.com/




신철민
https://kato75.blog.me/
엔트리



신현묵
https://brunch.co.kr/@supims
IT 칼럼



신현석
http://hyeonseok.com/
Web



신현식
https://medium.com/@Hyeon_SS
Android



심혜진
https://simhyejin.github.io/




신호철
http://hochulshin.com/
Java



신황규
https://medium.com/@hubert.shin
개발방법론



심재석
https://byline.network/author/jsshim0622/
IT 뉴스




K



Name
Blog
Description
Social




Kevin Lee
https://blog.kevinlee.io/
Java




ᄋ



Name
Blog
Description
Social




안도형
https://adhrinae.github.io/
Javascript



안동혁
https://brunch.co.kr/@donghyeokahn
부산모아



안상욱
https://www.bloter.net/archives/author/nuribit
금융



안성현
http://ash84.net/
Python



안수빈
http://nyeong.github.io/
Python



안승규
http://ahnseungkyu.com/
OpenStack



안영선
https://lovemewithoutall.github.io/
Android, Web



안영회 (Tony Lee)
https://www.popit.kr/author/tony
아키텍처



안오균
http://ohgyun.com/
iOS, Python



안윤호
http://toyfab.tistory.com/
마이크로 프로세서



안재열
http://doublem.org/
Java



안재우
http://blog.naver.com/saltynut
해외취업



안재하
http://programmingsummaries.tistory.com/
Front-end



안정우
https://medium.com/@jeongwooahn
Vue.js



안정호
http://ahnjungho.org/
Web



안정훈
http://www.andrewahn.co/
실리콘밸리



안종태
http://qnibus.com/
iOS



안중원
http://postgame.tistory.com/
게임개발, 웹툰



안형우
https://mytory.net/
PHP



안효근
http://hyogeun.tistory.com/
Android



안희종
http://ahnheejong.name/
Front-end



양권성
https://blog.perfectacle.com/
Front-end



양병규
http://blog.naver.com/delmadang
칼럼



양봉수
https://yangbongsoo.gitbooks.io/study/content/
Java, Spring



양성익
http://unikys.tistory.com
자바스크립트



양욱진
http://blog.hazard.kr/




양용성
https://brunch.co.kr/@ysyang
SQLGate



양준철
https://brunch.co.kr/@promise4u
스타트업



양지수
https://brunch.co.kr/@jisuyang
IT 칼럼



양현석
https://medium.com/@FourwingsY
Front-end



양희찬
https://heechan.me/
Docker



엄승현
http://blog.eomdev.com




엄태규
https://lazygyu.net/
HTML5 게임 개발



엄태웅
http://t-robotics.blogspot.kr/
AI



엄태형
https://brunch.co.kr/@taebari




여준호
http://nogadaworks.tistory.com/
Hacking, Algorithm



염재현
https://only2sea.wordpress.com/




염지원
https://medium.com/@jwyeom63




오광신
http://kwangshin.pe.kr/
함수형 언어



오경식
https://sikeeoh.github.io/
Android



오길호
http://kilho.net/
워드프레스



오명운
http://homoefficio.github.io/
Spring



오민호
http://wergia.tistory.com/
게임 개발



오상준
http://www.namooz.com/
Spring Boot, Angular2



오세빈
http://osebin.tistory.com/




오일석
http://ilseokoh.com/
Azure



오정규
http://ohjeonggyu.github.io/
Kotlin



오종빈
http://ohyecloudy.com/




오준석
https://brunch.co.kr/@hopeless
Android



오현석
http://www.enshahar.me/
스칼라, 스위프트



용영환
https://xenonix.com/
PHP



용찬호
http://blog.naver.com/alice_k106
Docker, RTOS



우웅몬
https://brunch.co.kr/@wej6688
UX



우영준
https://blog.hax0r.info
Serverside, security



우준혁
http://sarojaba.github.io/
Awesome-devblog



원강민
https://blog.wonhada.com/
코로나



원종석
https://tedwon.com/




유경상
http://www.simpleisbest.net/
.NET



유동곤
http://blog.naver.com/ehdrhs1004
C++



유동환
https://brunch.co.kr/@yudong




유병후
https://libsora.so/
Unity



유상엽
https://medium.com/@Dev_Bono
Front-end



유성규
https://skyoo2003.github.io/
Ansible



유영재
https://blog.asamaru.net/
Android



유용우
http://luckyyowu.tistory.com/




유용호
http://blog.eedler.com/
Swift



유재석
https://brunch.co.kr/@yoojs8512
IT 컬럼



유재준
http://jaejunyoo.blogspot.com
머신러닝



유주원
http://hipercube.tistory.com/




유준상
https://wnstkdyu.github.io/
iOS



유진호
https://brunch.co.kr/@jinhoyooephf




유차영
https://yous.be/
CTF



유현석
http://duriepark.tistory.com/




유형준
http://programmeringermany.blogspot.kr/
해외 취업



유희철
https://medium.com/@ryuheechul
DevOps



육승찬
http://loup1788.blogspot.kr/




윤상배
http://www.joinc.co.kr/
위키



윤석찬
http://channy.creation.net/
오픈웹, Mozilla



윤영식
http://mobicon.tistory.com/
Angular



윤우식
http://suitee.me/
Node.js, Python, Backend



윤진
http://storycompiler.tistory.com/
Tizen



윤청하
https://brunch.co.kr/@brunch4nrs
구글러



윤현철
http://metashower.egloos.com/
Data Analytics



이경원
http://blog.woniper.net/
Java



이경일
http://blog.leekyoungil.com/
Java, Spring



이경찬
http://leekchan.com/
Go



이광식
http://www.kwangsiklee.com/




이광헌
http://honeyperl.tistory.com/
Perl



이규원
https://justhackem.wordpress.com/
OOP



이규혁
http://kyuhyuk.kr/
Security



이기영
https://brunch.co.kr/@kiyoungleefige




이기창
https://ratsgo.github.io/
NLP, ML



이다윗
https://brunch.co.kr/@designforhuman
디자인



이도현
http://genesis8.tistory.com/




이동건
http://baked-corn.tistory.com
iOS



이동규
http://chandong83.blog.me/




이동규
https://medium.com/@guleum
Web



이동련
http://start.goodtime.co.kr/




이동욱
https://blog.naver.com/edy5016
Spring Boot



이동욱
https://jojoldu.tistory.com
Java, Spring, IntelliJ, Backend



이동인
https://brunch.co.kr/@leedongins
서버 모니터링



이동준
http://javalab.org/
Java, Math



이동준
https://dongjunlee.github.io/
ML



이동화
https://medium.com/@moralmk
Front-end



이명종
https://jijong.github.io/
Front-end



이명현
https://brunch.co.kr/@brightlee
스타트업



이무열
https://mooyoul.github.io/




이민구
https://medium.com/@premist
Docker



이민석
http://hl1itj.tistory.com/
소프트웨어 문화



이민우
https://brunch.co.kr/@minwoo
기획



이민철
http://bab2min.tistory.com/
Python



이민호
http://lumiamitie.github.io/
Data Science



이범재
https://medium.com/@beomjae




이병준
http://www.buggymind.com/
Agile



이봉균
https://medium.com/@deptno
Front-end



이상균
https://blog.naver.com/iyooha
Game



이상복
https://medium.com/@sangboaklee
Front-end



이상영
http://blog.sangyoung.me/




이상우
http://prostars.net/
C++



이상욱
https://sangwook.github.io/
아키텍처 분석



이상운
https://highluck.github.io/
Back-end



이상주
http://surpreem.com/




이상학
http://sanghaklee.tistory.com/
Back-end



이선협
https://medium.com/@kciter
Back-end



이성규
http://www.shalomeir.com/




이성규
http://blog.ohmynews.com/dangun76/
저널리즘



이성근
http://crazrain.tistory.com/
Spring Boot



이성몽
http://blog.naver.com/santalsm
기술사



이성원
http://blog.sungwonandseohyun.us/




이성호
http://blog.scaloid.org/
스칼라, 안드로이드



이세우
http://blog.xcoda.net/
Raspberry Pi



이소은
https://medium.com/@soeunlee
Front-end



이소현
https://brunch.co.kr/@sohyeonlee




이수진
http://sujinlee.me/




이수홍
https://brunch.co.kr/@sbcoba
Spring Boot, Security



이슬
https://lee-seul.github.io/
Python, Django



이승우
https://blog.2dal.com/
Docker, Kubernetes, AWS



이승재
https://saystone.github.io/
Hexo



이승현
http://hamait.tistory.com/
Python



이승현
https://brunch.co.kr/@oemilk
Android



이승훈
https://brunch.co.kr/@seunghoon82
스타트업



이영경
http://sori-nori.gitlab.io/
AWS



이영민
https://brunch.co.kr/@mapthecity
인공지능



이영훈
http://resoliwan.blogspot.kr/




이완근
http://icednut.github.io/
Java, Spark



이웅희
http://woongheelee.com/




이윤창
http://daddycat.blogspot.kr/
Back-end



이응준
https://blog.npcode.com/
Web



이임복
https://brunch.co.kr/@eundang
IT 트렌드



이재현
http://www.unity3dstudy.com/
Unity



이재홍
http://pyrasis.com/
Go



이정근
http://blog.cjred.net/
OpenShift



이정영
https://blog.jyslash.com/@jungyounglee
UI 디자인



이정운
https://medium.com/@jwlee98
GCP



이정원
https://brunch.co.kr/@madlymissyou
IT 칼럼



이제민
http://goodtogreate.tistory.com/
Data Science



이종립
https://johngrib.github.io/
Python



이종은
https://medium.com/@yomybaby




이종호
http://jhleed.tistory.com/




이주영
https://dev-juyoung.github.io/
Android



이준범
https://beomi.github.io/
Python, Django



이준용
https://brunch.co.kr/@junyong
해외 취업



이준호
http://www.vonzone.kr/
기획



이지영
https://www.bloter.net/archives/author/izziene
통신, 금융



이지현
https://www.bloter.net/archives/author/jihyun
오픈소스



이지훈
http://jihoonlee.io/
로봇공학



이진석
https://medium.com/@allieuslee
Python, Django



이찬진
https://medium.com/@chanjin
IT 칼럼



이찬행
http://blog.naver.com/dlcksgod1
3D Graphics



이찬희
https://iamchanii.github.io/
Go



이태화
http://platformengineer.tistory.com/
Linux



이태희
https://brunch.co.kr/@bradlee
스타트업



이하제
http://realignist.me/
Java, Rust



이한
https://blog.hanlee.io/
Python, Front-end



이한별
http://lhb0517.tistory.com/
Ubuntu, Java



이해영
http://www.haeyounglee.com/
개발자 영어



이현
https://hy00un.github.io/
정보보호



이현규
https://codefict.com/
Python, Go



이현섭
http://hyunseob.github.io/
Web



이현우
https://blog.naver.com/PostList.nhn?blogId=hidejj79




이현주
https://wayhome25.github.io/
Algorithm, Django



이호성
https://brunch.co.kr/@leehosung
8퍼센트 CTO. 개발 문화



이호철
http://hochul.net/blog/
데이터 분석



이홍규
https://medium.com/@mldevhong
머신러닝



이흥섭
http://subl.ee/
게임



이홍식
http://alex.devpools.kr/




이흥현
https://medium.com/@maxzidell
tyle.io CTO. Web



이희승
http://t.motd.kr/ko/
armedia



이희욱
https://www.bloter.net/archives/author/asadal
포털



임동문
http://dmlim.egloos.com/
Java



임성진
http://blog.bbom.org/
Linux



임성현
https://brunch.co.kr/@sunghyunlim




임정택
http://medium.com/@heartsavior
Apache Storm



임종대
https://medium.com/@jongdae.lim
Java, AI



임지훈
http://www.jimmyrim.com/
스타트업 투자



임한솔
http://hsol.tistory.com/
Web



임희진
http://epicdevs.com/
Back-end




ᄌ



Name
Blog
Description
Social




장기효(기효 조슈아 장)
https://joshua1988.github.io/




장동민
https://medium.com/@Dongmin_Jang
Front-end



장동수
http://blog.iolo.kr/
Web



장메튜
https://matthew.kr/
Spring



장문익
http://mooneegee.blogspot.kr/
3D



장성진
https://brunch.co.kr/@99-life
웹디자인, CSS Animation



장성만
http://frontjang.info/




장성민
http://www.jangkunblog.com/
웹접근성



전미정
https://mijeongjeon.github.io/
iOS, Keras



전승현
https://fuzer.github.io/




장영철
http://dkdlel072.tistory.com/
Unity



장영학
https://brunch.co.kr/@younghakjang
조직문화



장요셉
http://blog.lastmind.io/




장용석
http://devyongsik.tistory.com/
Java, Lucene



장재원
http://superjang.com/
Front-end



장재휴
https://jaehue.github.io/
Go



장준혁
https://medium.com/@hyuk
Interaction Design



장한빈
http://dork94.tistory.com/
Linux



장현석
https://devjang.github.io/




장현승
http://www.xeronichs.com/
리버싱



장현정
http://naleejang.tistory.com/
OpenStack



장혜식
http://openlook.org/wp/
빅데이터



전규현
http://www.allofsoftware.net/
기업문화



전동규
http://www.php5.me/blog/
PHP



전무익
https://medium.com/@muik




전민수
https://brunch.co.kr/@ebprux
UX



전상혁
http://sanghyukchun.github.io/
Machine Learning



전종홍
http://mobile2.tistory.com/
기술 동향



전창완
http://wani.kr/
PHP, Backend



전현준
http://guswnsxodlf.github.io/
Web



전형탁
http://hjun.me/
UI



전호상
http://guruble.com/
서버사이드



전희원(고감자)
http://freesearch.pe.kr/
빅데이터



정겨울
https://winterj.me/
Python



정광섭
https://www.lesstif.com/
Linux, 보안, Laravel, JIRA



정덕수
http://blog.iamartin.com/
Cloud



정도현
http://www.moreagile.net/
개발방법론



정동민
http://jdm.kr/blog/
Java



정민혁
https://www.holaxprogramming.com/
JavaScript, DevOps



정범희
http://blog.sonim1.com/
Front-end



정병기
https://byeongkijeong.github.io/
인공지능



정상혁
http://blog.benelog.net/
Java



정순형
https://medium.com/@soonhyungjung
블록체인



정승욱
https://medium.com/@jsuch2362
Android



정양욱
https://yangeok.github.io
Back-end



정원희
https://brunch.co.kr/@hee072794
React, Next.js



정유진
http://dudmy.net/
Android



정유택
https://takeuu.tistory.com
Front-end



정윤성
http://yoonsung.github.io/
Rust



정윤원
http://youknowone.github.io/
IOS



정윤진
http://kerberosj.tistory.com/
Cloud



정재광
https://jae-kwang.github.io/blog/
Front-end



정재남
http://gomugom.github.io/
Front-end



정재훈
http://uzys.net/
Rankedin



정종윤
https://wormwlrm.github.io/
Front-end



정주홍
https://brunch.co.kr/@toughrogrammer




정찬명
http://naradesign.net/
Front-end



정찬웅
https://korchris.github.io/




정창수
http://downman.tistory.com/
C



정창훈
https://code.iamseapy.com/
iOS



정철
https://wedul.site
Full-Stack



정태현
http://chomman.github.io/blog/
Back-end



정현일
http://blog.nuti.pe.kr/
Java, Spring



정희연
https://yeun.github.io/
웹디자인



제갈민
http://jekalmin.tistory.com/
Spring



조동현
https://hudi.kr/
Front-end



조만석
http://manseok.blogspot.kr




조만영
https://medium.com/@manyoung
Web



조병욱(조대협)
http://bcho.tistory.com/
Java



조상현
https://brunch.co.kr/@aaa




조성문
http://sungmooncho.com/
실리콘밸리



조성수
https://printf.kr/
Linux, Openstack



조승연
http://blog.kivol.net/
칼럼



조승진
http://www.tacogrammer.com/
IT 칼럼



조영국
https://brunch.co.kr/@ziyocode
DevOps



조영규
http://dev.youngkyu.kr/
Android



조영인
http://codersbrunch.blogspot.kr/
Algorithm



조영호
http://aeternum.egloos.com/
DDD



조우진
http://www.notforme.kr/
Angular



조은
https://brunch.co.kr/@techhtml
Front-end



조은상
http://itnp.kr/blog/
Spring



조은우
http://jonnung.github.io/




조인석
https://brunch.co.kr/@insuk
SW 칼럼



조재우
http://flowkater.github.io/
Java



조중현
http://blog.naver.com/chowin21
마케팅



조창민
http://incredible.ai/
AI



조현영(ZeroCho)
https://www.zerocho.com/
Javascript



조현석
https://dev.zzoman.com/
Front-end



조현종
http://hangumkj.blogspot.kr/
Eclipse



조현진
http://resistan.com/
웹접근성



조현철
https://cchcc.github.io/
Kotlin



조훈
https://medium.com/@hooncho
UX/UI Design



주길재
http://www.giljae.com/
Cloud



주민하
https://alegruz.imweb.me/
Python, 게임 개발, C



주영익
http://haah.kr/
Web, PHP



주우영
http://blog.coderifleman.com/
React.js



주재범
http://joojaebum.com/
픽셀 아트



지국환
http://wlhermit.blog.me/
게임 개발



지원준
https://brunch.co.kr/@wjchee
IT 칼럼



진민규
https://medium.com/@justin_jin
마케팅



진민완
https://minwan1.github.io/
Spring



진성주
http://softwaregeeks.org/
오픈소스



진수민
http://blog.naver.com/pistolcaffe
Android



진영화
https://medium.com/@timotolkie
Javascript



진용진
https://brunch.co.kr/@yongjinjinipln
IT 칼럼



진유림
https://milooy.wordpress.com/
Front-end



정민석
https://medium.com/@HarrisonJung
꿈많은청년들




ᄎ



Name
Blog
Description
Social




차경묵
http://blog.hannal.com/
Django



차준범
http://khanrc.tistory.com/
Data Science



채반석
https://www.bloter.net/archives/author/chaibs




채수원
http://blog.doortts.com/
Node.js



채영훈
https://proinlab.com/
Web, Data Mining



채윤창
http://mcchae.egloos.com/
Python



최광민
http://dl-ai.blogspot.kr/
AI



최규우
https://medium.com/@kyuwoo.choi
Front-end



최근우
https://keunwoochoi.wordpress.com/
인공지능



최대선
http://decisive.egloos.com/
정보보호



최만
http://manchoikorea.blogspot.kr/
교육



최백준
http://www.baekjoon.com/
알고리즘



최범균
http://javacan.tistory.com/
Java



최석균
http://syaku.tistory.com/
Full-stack



최성재
http://yumere.tistory.com/
Paper Review



최수원
http://cionman.tistory.com/
Linux



최승필
https://brunch.co.kr/@pilsogood
트립그리다



최용석
https://blog.naver.com/cys_star




최유석
http://whitechoi.tistory.com/
Cloud



최윤섭
http://www.yoonsupchoi.com/
헬스케어



최은선
https://im-mota.github.io/
UX



최재길
http://blog.devjoshua.me/




최재훈
http://andromedarabbit.net/
Cloud



최정대
http://blog.woosum.net/
오픈스택



최정렬
http://bestalign.github.io/
Javascript



최정연
http://blog.naver.com/cenodim
Kotlin



최종욱
https://wook.kr/




최종원
https://brunch.co.kr/@voiz
테크 칼럼



최종찬
http://blog.0xabcdef.com/




최준건
http://junegunn.kr/
fzf



최준석
https://jicjjang.github.io/blog
Front-end



최준영
https://rokt33r.github.io/
React, Electron, Boostnote



최준호
http://blog.saturnsoft.net/




최진영
https://wpu.kr/
워드프레스



최진호
http://calmglow.egloos.com/
Web



최창규
https://brunch.co.kr/@cg4jins
카카오헤어샵 서버 개발 후기



최창원
https://qwefgh90.github.io/sphinx
컴퓨터 공학 분야 위키



최창원
https://medium.com/@qwefgh90
웹/번역, 컴퓨터 공학 분야



최창하
https://alklid.github.io/dlog/
Spark



최철호
https://brunch.co.kr/@chulhochoiucj0
UI/UX 디자인



최충엽
https://medium.com/@albertseewhy




최혁재
https://brunch.co.kr/@mirr5510
스타트업



최현민
https://medium.com/@hyunmin.choi
Scala



최현식
https://diveintodata.org/
Apache Tajo



최호섭
https://byline.network/author/hs-choi/
IT 뉴스



최흥배
https://jacking75.github.io/
C++




ᄑ



Name
Blog
Description
Social




편해걸
https://medium.com/@la.place
Front-end




ᄒ



Name
Blog
Description
Social




하동우
https://medium.com/@cookatrice
Angular



하원호(Las)
https://medium.com/@haho6629
인공지능, Devops



하호진(Mimul)
http://www.mimul.com/
칼럼



한고 (10년코더)
https://10yearscoder.tistory.com/
프리랜서



한민석
http://creativeprm.tistory.com/
R



한상곤
http://www.sangkon.com/
Django



한상훈
https://brunch.co.kr/@skykamja24




한성민
https://blog.pigno.se/




한수연
https://www.bloter.net/archives/author/again
블록체인, 빅데이터



한승훈(kkamagui)
http://kkamagui.tistory.com/
OS



한영빈
https://blog.youngbin.xyz/
Ubuntu



한웅제
https://nicewoong.github.io/
Linux



한장현
http://han41858.tistory.com/
Angular



한재엽
https://jaeyeophan.github.io/
Front-end



한정일
https://brunch.co.kr/@lonnie
Android



한정현
http://blog.kazikai.net/
Front-end



한주영
https://medium.com/@jooyunghan
Functional Programming



한창석
https://free-strings.blogspot.kr/
Rust



허광남
http://okjsp.tistory.com/
OKKY



허승
https://seanlion.github.io/
스낵뉴스



허원철
http://heowc.tistory.com/
Java



허재영
https://kirade.github.io/
책



허재위
http://blog.import.re/
Kotlin



허준회(주네)
http://joone.net/
만화



허진호
https://medium.com/@hur
스타트업 투자



현수명
http://soomong.net/
책



현준호
https://jhyun.wordpress.com/
웹접근성



홍길한
http://blog.naver.com/hgh73
워드프레스



홍민희(dahlia)
https://blog.hongminhee.org/




홍성철
https://medium.com/@sungcheulhong




홍영택
http://hackerwins.github.io/
summernote



홍용남
https://brunch.co.kr/@doberman
스타트업



홍정모
http://blog.naver.com/atelierjpro
AI



홍철주
https://medium.com/@angdev
Ruby, Javascript



황교빈
http://archive.htrucci.com
Java, Back-End



황규현
http://lespinside.com
Reactive Programming



황성우
https://king10tech.github.io/
Java



황인서
https://medium.com/@chequer
Spring



황장호
http://xrath.com/




황상철
http://pragmaticstory.com/
애자일



황석주
http://dolppi.egloos.com/
금융공학



황성재
https://brunch.co.kr/@uxinventor
창업



황준식
http://jsideas.net/
Data Science



황준원
http://nuxlear.tistory.com/
Keras



황지현
http://jhhwang4195.tistory.com/
NFV, 5G, Devops



황치규
https://brunch.co.kr/@delight412
IT 칼럼



황태식
https://yahwang.github.io
데이터 분석, 엔지니어링



히언
http://recipes.egloos.com/
Embedded Recipes




S



Name
Blog
Description
Social




Steve Park
https://brunch.co.kr/@stevepark
미국 취업 성공기




",1376
susalib/susa,C++,"Susa Open Source Project  
Susa is a mathematics and signal processing C++ framework based on KISS
principle. It is stand-alone with a modern architecture. It is designed not to have any dependencies to none standard third
party libraries. Indeed, a C++11 compiler along with STL is necessary and sufficient in order to compile it. Therefore,
portability is the key feature of Susa. For example it can be exploited in mobile platforms such as Android NDK (Native
Development Toolkit) without any restriction. This brings the power and speed of the C++ native code to the user friendly
Java based mobile applications. Susa is also a simulation framework for the researchers and engineers who design
computational systems. It has linear algebra, signal processing and common communications blocks.
The matrix and array template classes i.e. types are at the heart of Susa. A vector is simply a single column (or a single row) matrix. It is bundled with a constellation of classes and functions.
Highlights

Algebraic types (template classes): matrix and multi-dimensional array.
Linear algebraic operations and analysis (e.g. Determinant and SVD).
Signal processing operations (e.g. FFT, Filter (FIR/IIR), Convolution and Random Number Generators).
Convolutional Forward Error Correction (FEC) blocks: encoder, MLSE (Viterbi) and MAP (BCJR) decoders.
Channel equalisers: MLSE (Viterbi) and MAP (BCJR).
Automatic memory management i.e. allocation, deallocation, move and copy.

Build, Test and Install
Build
To build Susa you need to have a C++ compiler, Make and CMake installed.
mkdir build
cd build
cmake ..
make

Test
It is highly recommended to run the tests after the build.
make test

Should you verify which test(s) has/have been failed, run the following for a more detailed report.
ctest -V

Install
Once it has been built and tested you are ready to code. Assuming your current path is build directory, run
make install

to be able to build against Susa system-wide. However, you may continue using the local build without installation.
Examples
In the examples directory
a number of simulation and tutorial source codes have been provided.
Contribution
This is a non-profit project and it belongs to its users. You can contribute to your project by reporting bugs and extending it by following the provided guidelines. This paves the way for further improvements and protects the authors' rights.
History
Susa was born in April 2008 out of a university project course in digital communications.
At the time the libraries that could be used for digital communications simulation had
many dependencies (e.g. LAPACK, BLAS and ATLAS).
Once it took about six hours on a decent PC to compile one of them. On the other hand those
weighty codes had nested bugs that sometimes stemmed from their third party dependencies.
The answer to these problems was Susa that was released
in November 2008.
Later in early 2009, Susa was used for a bandwidth efficient coding scheme, namely,
Faster Than Nyquist (FTN).
It required preferment equalizers to decode up to some twenty taps (compared to the fading channels with few taps).
The simulation of such systems took a long time between an hour to a few days. This library could simulate
a FTN system with thirteen taps using a modified BCJR algorithm (a sub-optimal variant that could outperform
the original algorithm) in about an hour whereas a similar script in a commercial computing software took
at least twelve hours.
Unearthed tablets from Susa (2000 BC) revealed a rather precise calculation of Pi = 3.125 with the fractional part
whereas the other earlier efforts calculated the integer part.
Since the very first line of code was simply the definition of constant Pi, it has been named Susa.
License
Susa has been released under GNU Lesser General Public License (LGPL).
",19
susalib/susa,C++,"Susa Open Source Project  
Susa is a mathematics and signal processing C++ framework based on KISS
principle. It is stand-alone with a modern architecture. It is designed not to have any dependencies to none standard third
party libraries. Indeed, a C++11 compiler along with STL is necessary and sufficient in order to compile it. Therefore,
portability is the key feature of Susa. For example it can be exploited in mobile platforms such as Android NDK (Native
Development Toolkit) without any restriction. This brings the power and speed of the C++ native code to the user friendly
Java based mobile applications. Susa is also a simulation framework for the researchers and engineers who design
computational systems. It has linear algebra, signal processing and common communications blocks.
The matrix and array template classes i.e. types are at the heart of Susa. A vector is simply a single column (or a single row) matrix. It is bundled with a constellation of classes and functions.
Highlights

Algebraic types (template classes): matrix and multi-dimensional array.
Linear algebraic operations and analysis (e.g. Determinant and SVD).
Signal processing operations (e.g. FFT, Filter (FIR/IIR), Convolution and Random Number Generators).
Convolutional Forward Error Correction (FEC) blocks: encoder, MLSE (Viterbi) and MAP (BCJR) decoders.
Channel equalisers: MLSE (Viterbi) and MAP (BCJR).
Automatic memory management i.e. allocation, deallocation, move and copy.

Build, Test and Install
Build
To build Susa you need to have a C++ compiler, Make and CMake installed.
mkdir build
cd build
cmake ..
make

Test
It is highly recommended to run the tests after the build.
make test

Should you verify which test(s) has/have been failed, run the following for a more detailed report.
ctest -V

Install
Once it has been built and tested you are ready to code. Assuming your current path is build directory, run
make install

to be able to build against Susa system-wide. However, you may continue using the local build without installation.
Examples
In the examples directory
a number of simulation and tutorial source codes have been provided.
Contribution
This is a non-profit project and it belongs to its users. You can contribute to your project by reporting bugs and extending it by following the provided guidelines. This paves the way for further improvements and protects the authors' rights.
History
Susa was born in April 2008 out of a university project course in digital communications.
At the time the libraries that could be used for digital communications simulation had
many dependencies (e.g. LAPACK, BLAS and ATLAS).
Once it took about six hours on a decent PC to compile one of them. On the other hand those
weighty codes had nested bugs that sometimes stemmed from their third party dependencies.
The answer to these problems was Susa that was released
in November 2008.
Later in early 2009, Susa was used for a bandwidth efficient coding scheme, namely,
Faster Than Nyquist (FTN).
It required preferment equalizers to decode up to some twenty taps (compared to the fading channels with few taps).
The simulation of such systems took a long time between an hour to a few days. This library could simulate
a FTN system with thirteen taps using a modified BCJR algorithm (a sub-optimal variant that could outperform
the original algorithm) in about an hour whereas a similar script in a commercial computing software took
at least twelve hours.
Unearthed tablets from Susa (2000 BC) revealed a rather precise calculation of Pi = 3.125 with the fractional part
whereas the other earlier efforts calculated the integer part.
Since the very first line of code was simply the definition of constant Pi, it has been named Susa.
License
Susa has been released under GNU Lesser General Public License (LGPL).
",19
ernestochero/volskaya,Scala,"GraphQL - Volskaya
GraphQL server written with akka-http, circe and sangria.
",2
CHEF-KOCH/nVidia-modded-Inf,JavaScript,"



nVidia modded Inf project was created 2017 by CHEF-KOCH and is under GNU GENERAL PUBLIC LICENSE v3.
The project is unofficial and not in any relationship or supported by nVidia Cooperation.
This project only support x64 Windows 10 versions, if you like to see x86 ask nVidia to extend the support.




What is a modded INF?
All drivers come with an ""Installation INF(ormation) file"". This tells the Windows internal installer how to install the driver. The INF has the instructions to what files to copy and where. It will also setup settings to install with. The INF-file itself contains a list of supported products (hardware/software/OS) that it will check and install for.
nVidia provides UDA (Unified Driver Architecture) where the driver should work for all their released GPUs. So the driver itself will support ALL GPUs, but the INF the driver comes with will only support a selected set of GPU
nVidia by default don't support laptop GPUs with beta and WHQL driver updates on their server. This is left up to the OEM to organize. I assume nVidia must charge to have an OEM driver made or be included, as they are very rear for OEM to actually update a video driver. So an OEM driver has specific instructions for that laptop (ie model specific) and will only work with a select few laptops.
For example a Toshiba OEM driver with a Toshiba INF (nvts.inf) will only work with the models included in that INF.
Modded drivers will NEVER transform your GPU to another one and will NEVER add features that you do not already have.
Reasons for modding

New driver features tests.
You like to test newer driver(s) which are no longer supported.
It could be used to force installation of newer drivers on OEM locked GPUs.
You need an older driver that is not officially supported for your GPU because you have a specific application that is broken in newer ones and you cannot wait for a fix.
Security reasons, you should ALWAYS use the latest driver (if possible) due to possible driver flaws

Keep in mind:

Fermi GPUs (400 and 500 series) as from 396 driver are official no longer supported! Same goes for 700M and 800M GPU's.

DCH or Standard drivers?
Short answer:

""Standard"" packages are those that do not require the DCH driver components. DCH represents UWD which you can install via the Windows Store or manually.
Standard is the ""old"" way which you (for now) should prefer since UWD drivers aren't tweakable (in terms of mods) compared to the standard (legacy) drivers.
""DCH"" (Declarative, Componentized, Hardware Support Apps) refers to new packages pre-installed by OEMs implementing the Microsoft Universal Driver paradigm.
DCH drivers cannot be installed over a standard system, and Standard drivers cannot be installed over a DCH system.
To confirm the type of system you have, locate Driver Type under the System Information menu in the nVidia Control Panel.

Detailed Answer:
DCH is a collaboration platform supporting the process of commercial forecasting Demand Collaboration Hub (DCH) is a collaboration platform that enables all members of your Sales organization, at the various hierarchical levels, to submit, consolidate and validate their periodic commercial forecast. DCH is fully configurable, allowing you to model the workflow and segment the data between users, in relation to their level of responsibility, to configure your editing form, by selecting and publishing the information that are relevant for your sales organization, to enter commercial forecast at various level of aggregation, with automatic splitting of edited quantities to the level of maximum detail. DCH is part of the SO99+ (Service Optimizer 99+) product suite and more specifically it is complementary to its statistical forecasting functionality, since the statistical forecast may be used as a guidance to support the Sales organization to provide more reliable figures. To support mobility, DCH is available on the web or from any mobile device that your Sales organization may adopt.
Remove old nVidia drivers

Extract Display Driver Uninstaller (DDU) and start the program, boot into ""safe mode"" (you can do this manually or within the given DDU option) and let DDU auto-clean and restart the OS automatically for you. You do not need to uninstall the driver or any package via Windows own uninstaller program first (that's the whole point using DDU). Keep in mind that DDU should only be used in case you get troubles while uninstalling/installing the driver with nVidia's own Setup, it's not recommend and needed to use DDU as 'normal' driver removal procedure. nVidia's own setup routine usually does the job just fine, however in some cases in can help to remove leftovers which might cause trouble.
After you rebooted you install the (modded/repack) nVidia driver, if the driver isn't digital signed you need to do it yourself or disable Windows driver signature enforcement.

An official DDU guide can be found here.
Modded Inf Driver installation

Download and extract the Driver (download from official source) - wait until the installer has unzipped the files e.g. to C:\nVidia.
Search for e.g. ""nv_disp.cat"" (or corresponding inf-file) in the 'Display.Driver Folder'.
Follow the Video or the written instruction to install the certificate manually (optional).
Now you can install all my modded drivers, without disabling 'driver signature enforcement'.

How to show current driver branch?!

Download nVidia Inspector
Check the Overview Window under ""Driver version"" you see the installed driver branch.

Troubleshoot - Disable Driver Signatures
Check the current secure boot status (optional), which gives you true or false (enabled/disabled) back:

Powershell confirm-SecureBootUEFI


Before you install unsigned drivers, you have to manually set those parameters via administrative command prompt to turn secure boot off:
bcdedit -set loadoptions DISABLE_INTEGRITY_CHECKS

bcdedit -set testsigning ON

shutdown.exe /r /o

After you're finished installing the unsigned driver:
bcdedit -set loadoptions ENABLE_INTEGRITY_CHECKS

bcdedit -set {globalsettings} advancedoptions true

bcdedit -set TESTSIGNING OFF

After executing the mentioned commands you need to reboot Windows 10 in order to apply the changes. Some (if not all) new laptops have no options to turn secure boot off.
How to sign your modded driver?
Download and install (or use the portable) SelfCert.
Select the following after starting the app:
x.500 distinguished name: cn=name_here,o=org_here,e=email@example.com

Key size: 2048

Valid from: today

Valid to: Your choice like 5 up to 10 years

Now put in a password in and save as PFX
CN = Microsoft Windows Hardware Compatibility PCA
O = Microsoft Corporation
L = Redmond
S = Washington
C = US
E = Your Email

OK, now that you have your PFX, you can generate a CAT for your modded driver and sing it (you will need the latest Windows Driver Kit)
Re-generate a new CAT with Inf2Cat with:
Inf2Cat /driver:<path_to_folder_with_INF_&_Files> /os:Vista_X86,Vista_X64,Server2008_X86,Server2008_X64,7_X86,7_X64,Server8_X64,8_X86,8_X64,Server6_3_X64,6_3_X86,6_3_X64


Sign the new CAT with your PFX
signtool sign /f <filename>.pfx /p <password> ""<path_to_folder>\nv_disp.cat""
Timesamp your CAT file
signtool timestamp /t http://timestamp.verisign.com/scripts/timstamp.dll ""<path_to_folder>\nv_disp.cat""
Now what you need to do, is get the cert from your PFX, install it in the Trusted Root Cert. Auth. and get the reg from this to give to users to apply

What the ""inf mod"" can't provide

 Adding support for legacy GPU's  (see EOL) because nVidia removed (within the source code) support for it and there is no patch which can undo or manipulate it.
 I'm not permitted to upload modified .dll files so please do not ask for ""patch xyz"". I consider to provide bunch of offset patches I use, without any tool or information how you add these patches because it violates nVidia TOS (it's not my fault).

Telemetry
All information regarding driver bundled telemetry can be found under the /Telemetry folder.
It's not necessary to block telemetry with your firewall, since you can manually opt-out or install a 'crap free' version and you also can remove the unneeded folders/services manually.
The Guru3d user uKER programmed a little utility called NVSlimmer which allows you (via GUI) to remove the unneeded folders/features - it's basically the same as doing it via a batch/cmd but with an simple interface to allow you to manually select all folders based on your own 'removal needs', the program includes also an integrated required list in order to warn user what is really necessary to keep in order to use the driver.
Another program (rip-off from NVSlimmer) called ""NVCleanstall"" can be found in the TechPowerUP forums.
Release








Latest nVidia PhysX System Software: 9.19.0218


Latest nVidia GeForce Experience: 3.18.0.102 Stable & Beta 3.18.0.102


Acknowledgement & References

DDU Source Code (github.com/Wagnard)
GeForce Driver Installation Guide A guide to ensure your drivers are installed properly (forums.geforce.com)
Install nVidia drivers on macOS the easy way (github.com)
LaptopVideo2Go (laptopvideo2go.com)
Official nVidia Display Driver Feedback Page (surveys.nvidia.com)
PC Gaming Wiki (pcgamingwiki.com)
PCI ID Project (pci-ids.ucw.cz)
Windows 10 FAQ and Driver installation tips (forums.geforce.com)
nVidia INF driver modding (forums.guru3d.com)

Debugging

How to enable nVidia Graphics Driver and GeForce Experience installer logging (nvidia.custhelp.com)

Unofficial patches

Driver patch for enabling unlimited NVENC sessions (old) (github.com)
NvencSessionLimitBump (github.com
WhateverGreen (github.com)
Wine patches (github.com)
nVidia kvm patcher (github.com)
nVidia patch to remove restriction on maximum number of simultaneous NVENC video encoding session (github.com)
purge-wrangler (github.com)

Unofficial updater

nVidia Update PowerShell Script (github.com)

Bios

Maxwell Bios Tweaker for edit nVidia GTX 9XX bios (github.com)
VGA and BIOS rom font extraction (github.com)
nVidia-PascalBiosEditor (github.com)
nVidia-vBIOS-VFIO-Patcher (github.com)

EOL

End of Driver Support for Quadro Kepler-series Notebook Products (April 30, 2020)
Support Plan for 3DVision Products (nvidia.custhelp.com)
Support Plan for Kepler-series GeForce GPUs for notebooks (nvidia.custhelp.com)

",68
lopsided98/nixos-config,Nix,"NixOS Configuration
Packages, modules and configurations for my NixOS machines
The files in this repository are used to build all my NixOS machines, across
4 different architectures (armv6l, armv7l, aarch64, and x86_64). Everything
is built with my private Hydra instance, and each device has its own channel
(see machines/default.nix).
My fork of nixpkgs
is required to build my machine configurations. Machines can be deployed
manually to test changes using the deploy.sh script.
Some interesting bits:

Machine specific channels
A pragmatic way of handling secrets in the Nix store
Selectively cross compile packages (search for crossPackages)

",2
jjos2372/blocktalk,Java,"BlockTalk: easy to use smart contracts for Burstcoin


Burstcoin was the world's first HDD-mined
cryptocurrency using an energy efficient
and fair Proof-of-Capacity (PoC) consensus algorithm.
It was also the first to implement a turing-complete smart contract
system in the form of Automated Transactions (AT), as specified by CIYAM.
However, before BlockTalk, the creation and deployment of smart contracts required writing
(assembler-like) bytecode and testing on-chain, making the development of contracts cumbersome.
This project allows the user to write, debug, and deploy Burst smart contracts relying only on Java.
You can use a simple text editor or your preferred IDE.
BlockTalk consists of the following key components:

Contract.java: a Java abstract class defining the basic API available for contracts
Emulator: an emulated blockchain and respective UI
Compiler: a system to convert Java bytecode into Burst AT bytecode that can run on the Burst blockchain


This project is in pre-alpha. Most contracts still cannot be compiled into CIYAM bytecode.
Please carefully inspect your compiled AT contracts and
test it exhaustively on the testnet before production.
Sample Contracts
Take a look on the samples source folder.
Using (write your own contract)
Sample application
The easiest way to start with BlockTalk is to clone the blocktalk-sample.
This sample application is actually a VSCode Java Application.
Just clone or download the sample application and open its folder with VSCode.
Manually add BlockTalk to your gradle project
Add the following to your gradle.build file:
repositories {
	maven { url 'https://jitpack.io' }
}
dependencies {
	implementation 'com.github.jjos2372:blocktalk:-SNAPSHOT'
}

Manually add BlockTalk to your maven project
Add the repository to your configuration:
<repositories>
	<repository>
	    <id>jitpack.io</id>
	    <url>https://jitpack.io</url>
	</repository>
</repositories>
<dependency>
	<groupId>com.github.jjos2372</groupId>
	<artifactId>blocktalk</artifactId>
	<version>-SNAPSHOT</version>
</dependency>

License
This code is licensed under GPLv3.
Author
jjos
Donation address: BURST-JJQS-MMA4-GHB4-4ZNZU
",9
cbuijs/ipasn,None,"ipasn
IP to ASN list
Note: Updated at least once every 24 hours.
",2
starsite/SwiftFM,Swift,"Originally posted Sep 18, 2018. I'll update it soon for SDK 18, Swift 5, and Xcode 10.2
SwiftFM
SwiftFM is a service class for working with the FileMaker Data API. Swift 4.2+ and Xcode 9.4+ required.

Overview
This README.md is aimed at FileMaker devs who want to integrate the v17 Data API into their iOS projects. Each function is paired with an example. Everything shown below is part of the DataAPI.swift file, in this repo.

Class Vars and Lets
A let is a constant, in Swift.
During testing it may be easier to hardcode baseURL and auth values, but best practice is to fetch that information from elsewhere and (optionally) park it in UserDefaults. Do not deploy apps with tokens or credentials visible in code.
I like to fetch my environment settings from CloudKit, in didFinishLaunching or didEnterForeground. Doing it this way also provides a remote kill-switch, if necessary.
import UIKit
 
class ViewController: UIViewController {
 
//  let baseURL = ""https://<hostName>/fmi/data/v1/databases/<databaseName>""
//  let auth    = ""xxxxxxxabcdefg1234567""  // base64 ""user:pass""

    let baseURL = UserDefaults.standard.string(forKey: ""fm-db-path"")  // better
    let auth    = UserDefaults.standard.string(forKey: ""fm-auth"")     //
  
    var token   = UserDefaults.standard.string(forKey: ""fm-token"")
    var expiry  = UserDefaults.standard.object(forKey: ""fm-token-expiry"") as? Date ?? Date(timeIntervalSince1970: 0)
    // ...
}

Active Token (function)
A simple bool check to see if there's an existing token and whether or not it's expired. The _ means we aren't using (don't care about) the token value right now, we only care that there /is/ one.
// swift bools return either 'true' or 'false'
func isActiveToken() -> Bool {
        
    if let _ = self.token, self.expiry > Date() {
        return true
    } else {
        return false
    }
}
Example
// active token?
switch isActiveToken() {  

case true:
    print(""token \(self.token) - expiry \(self.expiry)"")  
    // do stuff with self.token
 
case false:
    refreshToken(for: self.auth, completion: { newToken, newExpiry, error in
    
        guard error == ""0"" else {
            print(""refresh token sad."")  // optionally handle non-zero errors
            return
        }
        
        print(""token \(newToken) - expiry \(newExpiry)"")  
        // do stuff with newToken
    })
}    

Refresh Token (function)
Refresh an expired token. The @escaping marker allows the token, expiry, and error code types to be used later (they're permitted to ""escape"" or outlive the function). That's typical for async calls in Swift.
// returns -> (token, expiry, error code)
func refreshToken(for auth: String, completion: @escaping (String, Date, String) -> Void) {
    
    guard   let path = UserDefaults.standard.string(forKey: ""fm-db-path""),
            let baseURL = URL(string: path) else { return }
    
    let url = baseURL.appendingPathComponent(""/sessions"")
    let expiry = Date(timeIntervalSinceNow: 900)   // 15 minutes
    
    var request = URLRequest(url: url)
    request.httpMethod = ""POST""
    request.setValue(""Basic \(auth)"", forHTTPHeaderField: ""Authorization"")
    request.setValue(""application/json"", forHTTPHeaderField: ""Content-Type"")
    
    URLSession.shared.dataTask(with: request) { data, _, error in
        
        guard   let data      = data, error == nil,
                let json      = try? JSONSerialization.jsonObject(with: data) as! [String: Any],
                let response  = json[""response""] as? [String: Any],
                let messages  = json[""messages""] as? [[String: Any]],
                let code      = messages[0][""code""] as? String,
                let message   = messages[0][""message""] as? String else { return }
        
        guard let token = response[""token""] as? String else {
            print(message)  // optionally pass message to UIAlertController
            return
        }
        
        UserDefaults.standard.set(token, forKey: ""fm-token"")
        UserDefaults.standard.set(expiry, forKey: ""fm-token-expiry"")
        
        completion(token, expiry, code)
        
    }.resume()
}
Example
// refresh token
refreshToken(for: self.auth, completion: { newToken, newExpiry, error in

    guard error == ""0"" else { 
        print(""refresh token sad."")  // optionally handle non-zero errors
        return 
    }

    print(""token \(newToken) - expiry \(newExpiry)"")
    // do stuff with newToken
})

Get Records (function)
Returns an array of records with an offset of 1. This could be refactored to include an offset parameter if you'll be doing a lot of recursive calls/paginating records.
// returns -> ([records], error code)
func getRecords(token: String, layout: String, limit: Int, completion: @escaping ([[String: Any]], String) -> Void) {
    
    guard   let path = UserDefaults.standard.string(forKey: ""fm-db-path""),
            let baseURL = URL(string: path) else { return }
    
    let url = baseURL.appendingPathComponent(""/layouts/\(layout)/records?_offset=1&_limit=\(limit)"")
    
    var request = URLRequest(url: url)
    request.httpMethod = ""GET""
    request.addValue(""Bearer \(token)"", forHTTPHeaderField: ""Authorization"")
    request.addValue(""application/json"", forHTTPHeaderField: ""Content-Type"")
    
    URLSession.shared.dataTask(with: request) { data, _, error in
        
        guard   let data      = data, error == nil,
                let json      = try? JSONSerialization.jsonObject(with: data) as! [String: Any],
                let response  = json[""response""] as? [String: Any],
                let messages  = json[""messages""] as? [[String: Any]],
                let code      = messages[0][""code""] as? String,
                let message   = messages[0][""message""] as? String else { return }
        
        guard let records = response[""data""] as? [[String: Any]] else {
            print(message)  // optionally pass message to UIAlertController
            return
        }
        
        completion(records, code)
        
    }.resume()
}
Example
// get first 20 records
getRecords(token: self.token, layout: myLayout, limit: 20, completion: { records, error in

    guard error == ""0"" else { 
        print(""get records sad."")  // optionally handle non-zero errors
        return 
    }
    
    // array!
    for record in records {
        // deserialize with Codable, append object array, refresh UI
    }
}

Find Request (function)
Note the difference in payload between an ""or"" request vs. an ""and"" request. You can set your payload from the UI, or hardcode a query (like this). Then pass your payload as a parameter.
// returns -> ([records], error code)
func findRequest(token: String, layout: String, payload: [String: Any], completion: @escaping ([[String: Any]], String) -> Void) {
    
    //  myPayload = [""query"": [           myPayload = [""query"": [
    //      [""firstName"": ""Brian""],           [""firstName"": ""Brian"",
    //      [""firstName"": ""Geoff""]            ""lastName"": ""Hamm""]
    //  ]]                                ]]
    
    guard   let path = UserDefaults.standard.string(forKey: ""fm-db-path""),
            let baseURL = URL(string: path),
            let body = try? JSONSerialization.data(withJSONObject: myPayload) else { return }
    
    let url = baseURL.appendingPathComponent(""/layouts/\(layout)/_find"")
    
    var request = URLRequest(url: url)
    request.httpMethod = ""POST""
    request.addValue(""Bearer \(token)"", forHTTPHeaderField: ""Authorization"")
    request.addValue(""application/json"", forHTTPHeaderField: ""Content-Type"")
    request.httpBody = body
    
    URLSession.shared.dataTask(with: request) { data, _, error in
        
        guard   let data      = data, error == nil,
                let json      = try? JSONSerialization.jsonObject(with: data) as! [String: Any],
                let response  = json[""response""] as? [String: Any],
                let messages  = json[""messages""] as? [[String: Any]],
                let code      = messages[0][""code""] as? String,
                let message   = messages[0][""message""] as? String else { return }
        
        guard let records = response[""data""] as? [[String: Any]] else {
            print(message)  // optionally pass message to UIAlertController
            return
        }
        
        completion(records, code)
        
    }.resume()
}
Example
// find request
findRequest(token: self.token, layout: myLayout, payload: myPayload, completion: { records, error in

    guard error == ""0"" else { 
        print(""find request sad."")  // optionally handle non-zero errors
        return 
    }
    
    // array!
    for record in records {
        // deserialize with Codable, append object array, refresh UI
    }
}

Get Record (function)
Fetch a record with recID.
// returns -> (record, error code)
func getRecordWith(id: Int, token: String, layout: String, completion: @escaping ([String: Any], String) -> Void) {
    
    guard   let path = UserDefaults.standard.string(forKey: ""fm-db-path""),
            let baseURL = URL(string: path) else { return }
    
    let url = baseURL.appendingPathComponent(""/layouts/\(layout)/records/\(id)"")
    
    var request = URLRequest(url: url)
    request.httpMethod = ""GET""
    request.addValue(""Bearer \(token)"", forHTTPHeaderField: ""Authorization"")
    request.addValue(""application/json"", forHTTPHeaderField: ""Content-Type"")
    
    URLSession.shared.dataTask(with: request) { data, _, error in
        
        guard   let data      = data, error == nil,
                let json      = try? JSONSerialization.jsonObject(with: data) as! [String: Any],
                let response  = json[""response""] as? [String: Any],
                let messages  = json[""messages""] as? [[String: Any]],
                let code      = messages[0][""code""] as? String,
                let message   = messages[0][""message""] as? String else { return }
        
        guard let records = response[""data""] as? [[String: Any]] else {
            print(message)  // optionally pass message to UIAlertController
            return
        }
        
        completion(records[0], code)
        
    }.resume()
}
Example
// get record
getRecordWith(id: recID, token: self.token, layout: myLayout, completion: { record, error in

    guard error == ""0"" else { 
        print(""get record sad."")  // optionally handle non-zero errors
        return 
    }
    
    // record!
    // deserialize with Codable, refresh UI
}

Delete Record (function)
Delete record with recID. Only an error code is returned with this function.
// returns -> (error code)
func deleteRecordWith(id: Int, token: String, layout: String, completion: @escaping (String) -> Void) {
    
    guard   let path = UserDefaults.standard.string(forKey: ""fm-db-path""),
            let baseURL = URL(string: path) else { return }
    
    let url = baseURL.appendingPathComponent(""/layouts/\(layout)/records/\(id)"")
    
    var request = URLRequest(url: url)
    request.httpMethod = ""DELETE""
    request.addValue(""Bearer \(token)"", forHTTPHeaderField: ""Authorization"")
    request.addValue(""application/json"", forHTTPHeaderField: ""Content-Type"")
    
    URLSession.shared.dataTask(with: request) { data, _, error in
        
        guard   let data      = data, error == nil,
                let json      = try? JSONSerialization.jsonObject(with: data) as! [String: Any],
                let messages  = json[""messages""] as? [[String: Any]],
                let code      = messages[0][""code""] as? String else { return }
                        
        completion(code)
        
    }.resume()
}
Example
// delete record
deleteRecordWith(id: recID, token: self.token, layout: myLayout, completion: { error in
    
    guard error == ""0"" else { 
        print(""delete record sad."")  // optionally handle non-zero errors
        return 
    }
    
    // deleted!
    // remove object from local array, refresh UI
}

Edit Record (function)
Edit record with recID. Only pass values for the fields you want to modify. Optionally, you may include the modID from your last fetch, to ensure the server record isn't newer than the one you're editing. Passing an outdated modID will cause an edit request to fail. /Not/ including a modID will post the request.
Only an error code is returned with this function. The v17 Data API does not currently pass back a modified record object for you to use. Because of this, you may wish to refetch the record and update the view.
// returns -> (error code)
func editRecordWith(id: Int, token: String, layout: String, payload: [String: Any], modID: Int?, completion: @escaping (String) -> Void) {
    
    //  myPayload = [""fieldData"": [
    //      ""firstName"": ""newValue"",
    //      ""lastName"": ""newValue""
    //  ]]
    
    guard   let path = UserDefaults.standard.string(forKey: ""fm-db-path""),
            let baseURL = URL(string: path),
            let body = try? JSONSerialization.data(withJSONObject: myPayload) else { return }
    
    let url = baseURL.appendingPathComponent(""/layouts/\(layout)/records/\(id)"")
    
    var request = URLRequest(url: url)
    request.httpMethod = ""PATCH""
    request.addValue(""Bearer \(token)"", forHTTPHeaderField: ""Authorization"")
    request.addValue(""application/json"", forHTTPHeaderField: ""Content-Type"")
    request.httpBody = body
    
    URLSession.shared.dataTask(with: request) { data, _, error in
        
        guard   let data      = data, error == nil,
                let json      = try? JSONSerialization.jsonObject(with: data) as! [String: Any],
                let messages  = json[""messages""] as? [[String: Any]],
                let code      = messages[0][""code""] as? String else { return }
                                
        completion(code)
        
    }.resume()
}
Example
// edit record
editRecordWith(id: recID, token: self.token, layout: myLayout, playload: myPayload, completion: { error in

    guard error == ""0"" else {
        print(""edit record sad."")  // optionally handle non-zero errors
        return
    }
    
    // edited!
    // refetch record using recID, referesh UI
}

",14
Twinklebear/rtobj,C++,"rtobj
An example of OBJ rendering with OSPRay, Embree and OptiX.
Uses tinyobjloader to load OBJ files.
Ray Tracing Backends
The currently implemented backends are: OSPRay, Embree and OptiX.
When running the program, you can pick which backend you want from
those you compiled with by specifying it as the first argument on
the command line:
./rtobj <backend> <mesh.obj>

All three ray tracing backends use SDL2 for window management
and GLM for math.
If CMake doesn't find your SDL2 install you can point it to the root
of your SDL2 directory by passing -DSDL2=<path>.
Similarly for GLM, you can point it to the glmConfig.cmake file
in your GLM distribution by passing -Dglm_DIR=<path>.
OSPRay
Dependencies: OSPRay.
To build the OSPRay backend run CMake with:
cmake .. -DENABLE_OSPRAY=ON -Dospray_DIR=<path to osprayConfig.cmake>

You can then pass -ospray to use the OSPRay backend.
Embree
Dependencies: Embree,
TBB and ISPC.
To build the Embree backend run CMake with:
cmake .. -DENABLE_EMBREE=ON -Dembree_DIR=<path to embree-config.cmake> \
    -DTBB_DIR=<path TBBConfig.cmake>

You can then pass -embree to use the Embree backend. The TBBConfig.cmake will
be under <tbb root>/cmake, while embree-config.cmake is in the root of the
Embree directory.
OptiX
Dependencies: OptiX 6, CUDA 10.
To build the OptiX backend run CMake with:
cmake .. -DENABLE_OPTIX=ON

You can then pass -optix to use the OptiX backend.
If CMake doesn't find your install of OptiX you can tell it where
it's installed with -DOptiX_INSTALL_DIR.
DirectX Ray Tracing
If you're on Windows 10 1809, have the 10.0.17763 SDK and a DXR capable GPU you can also run
the DirectX Ray Tracing backend.
To build the DXR backend run CMake with:
cmake .. -DENABLE_DXR=ON

You can then pass -dxr to use the DXR backend.
",4
mamedev/mame,C++,"MAME

Build status for tiny build only, containing just core parts of project:



OS/Compiler
Status




Linux GCC / OSX Clang



Windows MinGW



Windows MSVC




Static analysis status for entire build (except for third-party parts of project):

What is MAME?
MAME is a multi-purpose emulation framework.
MAME's purpose is to preserve decades of software history. As electronic technology continues to rush forward, MAME prevents this important ""vintage"" software from being lost and forgotten. This is achieved by documenting the hardware and how it functions. The source code to MAME serves as this documentation. The fact that the software is usable serves primarily to validate the accuracy of the documentation (how else can you prove that you have recreated the hardware faithfully?). Over time, MAME (originally stood for Multiple Arcade Machine Emulator) absorbed the sister-project MESS (Multi Emulator Super System), so MAME now documents a wide variety of (mostly vintage) computers, video game consoles and calculators, in addition to the arcade video games that were its initial focus.
How to compile?
If you're on a *NIX or OSX system, it could be as easy as typing
make

for a MAME build,
make SUBTARGET=arcade

for an arcade-only build, or
make SUBTARGET=mess

for MESS build.
See the Compiling MAME page on our documentation site for more information, including prerequisites for Mac OS X and popular Linux distributions.
For recent versions of OSX you need to install Xcode including command-line tools and SDL 2.0.
For Windows users, we provide a ready-made build environment based on MinGW-w64.
Visual Studio builds are also possible, but you still need build environment based on MinGW-w64.
In order to generate solution and project files just run:
make vs2017

or use this command to build it directly using msbuild
make vs2017 MSBUILD=1

Where can I find out more?

Official MAME Development Team Site (includes binary downloads for MAME and MESS, wiki, forums, and more)
Official MESS Wiki
MAME Testers (official bug tracker for MAME and MESS)

Contributing
Coding standard
MAME source code should be viewed and edited with your editor set to use four spaces per tab. Tabs are used for initial indentation of lines, with one tab used per indentation level. Spaces are used for other alignment within a line.
Some parts of the code follow Allman style; some parts of the code follow K&R style -- mostly depending on who wrote the original version. Above all else, be consistent with what you modify, and keep whitespace changes to a minimum when modifying existing source. For new code, the majority tends to prefer Allman style, so if you don't care much, use that.
All contributors need to either add a standard header for license info (on new files) or inform us of their wishes regarding which of the following licenses they would like their code to be made available under: the BSD-3-Clause license, the LGPL-2.1, or the GPL-2.0.
License
The MAME project as a whole is distributed under the terms of the GNU General Public License, version 2 or later (GPL-2.0+), since it contains code made available under multiple GPL-compatible licenses. A great majority of files (over 90% including core files) are under the BSD-3-Clause License and we would encourage new contributors to distribute files under this license.
Please note that MAME is a registered trademark of Gregory Ember, and permission is required to use the ""MAME"" name, logo, or wordmark.



Copyright (C) 1997-2019  MAMEDev and contributors

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License along
with this program; if not, write to the Free Software Foundation, Inc.,
51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

Please see LICENSE.md for further details.
",3724
inductor/hugo-firebase-tools,Dockerfile,"hugo-firebase-tools
",3
stephengold/Minie,Java,"
The Minie Project is about improving the integration of
Bullet Real-Time Physics into the
jMonkeyEngine Game Engine.
It contains 3 sub-projects:

MinieLibrary: the Minie runtime library (in Java)
MinieExamples: demos, examples, and test software (in Java)
DacWizard: a GUI application to configure a ragdoll (in Java)

Java source code is provided under
a FreeBSD license.
Contents of this document

Why use Minie?
Downloads
Conventions
History
How to install the SDK and the Minie Project
How to add Minie to an existing project
Choosing a collision shape
An introduction to DynamicAnimControl
External links
Acknowledgments


Why use Minie?
jMonkeyEngine comes with 2 Bullet integration libraries.
Why use Minie instead of jme3-bullet or jme3-jbullet?

Minie has many more features. (See the feature list below.)
Minie fixes many bugs found in the jMonkeyEngine libraries.
(See the fix list below.)
Due to its shorter release cycle, future features and bug fixes
will probably appear first in Minie.
Minie has automated tests that reduce the risk of regressions and new bugs.
Minie's classes are better encapsulated, with fewer public/protected fields
and less aliasing of small objects like vectors.  This reduces the risk
of accidentally corrupting its internal data structures.
Minie validates method arguments.  This helps detect usage errors that
can lead to subtle bugs.
Minie's source code is more readable and better documented.

Summary of added features:

DynamicAnimControl for ragdoll simulation:

set dynamic/kinematic mode per bone
understands attachments
highly configurable, with many options for bone mass, center, and shape
apply inverse-kinematic controllers and joints


MultiSphere collision shapes based on btMultiSphereShape
EmptyShape collision shapes based on btEmptyShape
debugging aids:

dump the contents of a BulletAppState or PhysicsSpace
customize debug material per collision object
visualize the local axes, bounding box, and/or CCD swept sphere
of collision objects
visualize physics in multiple viewports
optional high-resolution debug meshes for convex shapes
options to generate debug meshes that include normals (for shading)


all joints, shapes, and collision objects implement the JmeCloneable
and Comparable interfaces
enable/disable a joint
single-ended joints
settable global default for collision margin
access more parameters of rigid bodies:

anisotropic friction
contact damping
contact stiffness
contact-processing threshold
deactivation time
linear factor
rolling friction
spinning friction


option to apply scaling with a RigidBodyControl

Some JME bugs that have been fixed in Minie:

772 scale of a physics shape is applied 2x
877 physics joints don't work unless both bodies are dynamic
883 extra physicsTick() callbacks
887 debug mesh ignores scaling of CollisionShape
889 disabled physics control gets added to a physics space
894 setRestitutionOrthoLin() sets wrong joint parameter
901 collision margin initialized to 0
911 sleeping-threshold setters have unexpected side effects
913 missing implementation of PhysicsJoint.finalizeNative()
917 HingeJoint.read() fails
918 getImpulseClamp() returns the wrong value
919 UnsatisfiedLinkError in getLimitSoftness()
928 crash caused by too many parallel threads
969 linear factors not cloned
1029 sphere-sphere collisions not reported
1037 performance issue with HullCollisionShape
1058 crash while removing body from BroadphaseType.SIMPLE PhysicsSpace
1060 doesn't implement bt32BitAxisSweep3

Some jme3-bullet/jme3-jbullet classes that Minie omits:

CharacterControl: use MinieCharacterControl or BetterCharacterControl
instead, or else use PhysicsCharacter directly
KinematicRagdollControl, HumanoidRagdollPreset, and RagdollPreset:
use DynamicAnimControl instead
RagdollUtils: not needed

Other important differences:

The default collision margin increased from 0 to 0.04 .
RagdollCollisionListener interface changed and moved
from the com.jme3.bullet.collision package
to the com.jme3.bullet.animation package.


Downloads
Newer releases (since v0.5.0) can be downloaded from
GitHub.
Older releases (v0.1.1 through v0.4.5) can be downloaded from
the Jme3-utilities Project.
Maven artifacts are available from
JFrog Bintray.

Conventions
Package names begin with
jme3utilities.minie. (if Stephen Gold holds the copyright) or
com.jme3. (if the jMonkeyEngine Project holds the copyright).
The source code is compatible with JDK 7.

History
Most of Minie was originally forked from jme3-bullet,
a library in the jMonkeyEngine Game Engine.
From January 2018 to November 2018, Minie was a sub-project of
the Jme3-utilities Project.
Since November 2018, the Minie Project has been an independent project at
GitHub.
The evolution of Minie is chronicled in
its release notes.

How to install the SDK and the Minie Project
jMonkeyEngine3 (jME3) Software Development Kit (SDK)
Minie currently targets Version 3.2.3 of jMonkeyEngine.
You are welcome to use the Engine without also using the SDK, but I use the SDK,
and the following installation instructions assume you will too.
The hardware and software requirements of the SDK are documented on
the JME wiki.

Download a jMonkeyEngine 3.2 SDK from
GitHub.
Install the SDK, which includes:

the engine itself,
an integrated development environment (IDE) based on NetBeans,
various plugins, and
the Blender 3D application.


To open the Minie project in the IDE (or NetBeans), you will need the
Gradle Support plugin.  Download and install it before proceeding.
If this plugin isn't shown in the IDE's ""Plugins"" tool,
you can download it from
GitHub.
You don't need this plugin if you merely want to use a pre-built Minie
release in an Ant project.

Source files
Clone the Minie repository using Git:

Open the ""Clone Repository"" wizard in the IDE:

Menu bar -> ""Team"" -> ""Git"" -> ""Clone..."" or
Menu bar -> ""Team"" -> ""Remote"" -> ""Clone...""


For ""Repository URL:"" specify
https://github.com/stephengold/Minie.git
Clear the ""User:"" and ""Password:"" text boxes.
For ""Clone into:"" specify a writable folder (on a local filesystem)
that doesn't already contain ""Minie"".
Click on the ""Next >"" button.
Make sure the ""master"" remote branch is checked.
Click on the ""Next >"" button again.
Make sure the Checkout Branch is set to ""master"".
Make sure the ""Scan for NetBeans Projects after Clone"" box is checked.
Click on the ""Finish"" button.
When the ""Clone Completed"" dialog appears, click on the ""Open Project...""
button.
Expand the root project node to reveal the sub-projects.
Select both sub-projects using control-click, then click on the
""Open"" button.

Build the project

In the ""Projects"" window of the IDE,
right-click on the ""MinieExamples"" sub-project to select it.
Select ""Build"".


How to add Minie to an existing project
Adding Minie to an existing JME3 project should be a simple 6-step process:

Remove any existing physics libraries which might interfere with Minie.
Add libraries to the classpath.
Create, configure, and attach a BulletAppState,
if the application doesn't already do so.
Configure the PhysicsSpace,
if the application doesn't already do so.
Create physics controls, collision objects, and joints
and add them to the PhysicsSpace,
if the application doesn't already do so.
Test and tune as necessary.

Remove any existing physics libraries
Minie replaces (and is therefore incompatible with) the following
jMonkeyEngine libraries:

jme3-bullet
jme3-bullet-native
jme3-jbullet

Before adding Minie, you should remove these libraries from your project so
they won't interfere with Minie.
For Gradle projects
Look for artifacts with these names in the dependencies section
of your project's gradle.build file and remove them.
For Ant projects
Open the project's properties in the IDE (JME 3.2 SDK or NetBeans 8.2):

Right-click on the project (not its assets) in the ""Projects"" window.
Select ""Properties to open the ""Project Properties"" dialog.
Under ""Categories:"" select ""Libraries"".
Click on the ""Compile"" tab.
Look for libraries with these names in the ""Compile-time Libraries""
listbox.  Select them and click on the ""Remove"" button.
Click on the ""OK"" button to exit the ""Project Properties"" dialog.

Add libraries to the classpath
Minie comes pre-built as a single library that includes both Java classes
and native libraries.  The Minie library depends on the
jme3-utilities-heart library, which in turn depends on
the standard jme3-core library from jMonkeyEngine.
For Gradle projects
For projects built using Maven or Gradle, it is sufficient to specify the
dependency on the Minie library.  The build tools should automatically
resolve the remaining dependencies automatically.
Because Minie is not on JCenter yet, you must explicitly specify the
repository location:
repositories {
    maven { url 'https://dl.bintray.com/stephengold/jme3utilities' }
    jcenter()
}
dependencies {
    compile 'jme3utilities:Minie:0.8.1'
}

For Ant projects
For projects built using Ant, download the 2 non-standard
libraries from GitHub:

https://github.com/stephengold/Minie/releases/tag/0.8.1
https://github.com/stephengold/jme3-utilities/releases/tag/heart-2.26.0

You'll want both class JARs
and probably the -sources and -javadoc JARs as well.
Open the project's properties in the IDE (JME 3.2 SDK or NetBeans 8.2):

Right-click on the project (not its assets) in the ""Projects"" window.
Select ""Properties to open the ""Project Properties"" dialog.
Under ""Categories:"" select ""Libraries"".
Click on the ""Compile"" tab.
Add the jme3-utilities-heart class JAR:

Click on the ""Add JAR/Folder"" button.
Navigate to the ""jme3-utilities"" project folder.
Open the ""heart"" sub-project folder.
Navigate to the ""build/libs"" folder.
Select the ""jme3-utilities-heart-2.26.0.jar"" file.
Click on the ""Open"" button.


(optional) Add JARs for javadoc and sources:

Click on the ""Edit"" button.
Click on the ""Browse..."" button to the right of ""Javadoc:""
Select the ""jme3-utilities-heart-2.26.0-javadoc.jar"" file.
Click on the ""Open"" button.
Click on the ""Browse..."" button to the right of ""Sources:""
Select the ""jme3-utilities-heart-2.26.0-sources.jar"" file.
Click on the ""Open"" button again.
Click on the ""OK"" button to close the ""Edit Jar Reference"" dialog.


Similarly, add the Minie JAR(s).
Click on the ""OK"" button to exit the ""Project Properties"" dialog.

Create, configure, and attach a BulletAppState
Strictly speaking, a BulletAppState isn't required for Minie, but
it does provide a convenient interface for configuring, accessing, and
debugging a PhysicsSpace.
If your application already has a BulletAppState, the code will probably
work fine with Minie.  If not, here is a snippet to guide you:
    BulletAppState bulletAppState = new BulletAppState();
    bulletAppState.setDebugEnabled(true); // default=false
    stateManager.attach(bulletAppState);

Configure the PhysicsSpace
Section to be written.
Create physics controls, collision objects, and joints
Section to be written.
Test and tune
Section to be written.

Choosing a collision shape
Minie provides more than a dozen CollisionShape subclasses.
Because jMonkeyEngine models are composed of triangular meshes,
beginners are often tempted to use mesh-based shapes
(such as GImpactCollisionShape) for everything.
However, since mesh-based collision detection is CPU-intensive, primitive
convex shapes (such as boxes and spheres) are usually a better choice, even
if they don't match the model's shape exactly.
In particular, CapsuleCollisionShape is often used with humanoid models.
if (the object isn't involved in collisions) {
    use an EmptyShape
} else if (its shape can be approximated by an infinite plane) {
    use a PlaneCollisionShape
} else if (its shape can be approximated by a triangle or a tetrahedron) {
    use a SimplexCollisionShape
} else if (its shape can be approximated by a centered sphere) {
    use a SphereCollisionShape
} else if (its shape can be approximated by a centered rectangular solid) {
    use a BoxCollisionShape
} else if (its shape can be approximated by a centered capsule) {
    use a CapsuleCollisionShape
} else if (its shape can be approximated by a centered cylinder) {
    use a CylinderCollisionShape
} else if (its shape can be approximated by a centered cone) {
    use a ConeCollisionShape
} else if (its shape can be approximated by an ellipsoid
            or an eccentric sphere
            or an eccentric capsule
            or the convex hull of multiple spheres) {
    use a MultiSphere
} else if (its shape can be approximated by an eccentric rectangular solid
            or an eccentric cylinder
            or an eccentric cone
            or a combination of convex primitives) {
        use a CompoundCollisionShape
} else if (the object does not move) {
    if (it is a 2-D heightfield) {
        use a HeightfieldCollisionShape
    } else {
        use a MeshCollisionShape
    }
} else { // if the object moves
    if (its shape can be approximated by the convex hull of a mesh) {
        use a HullCollisionShape
    } else {
        use a GImpactCollisionShape
    }
}

(Pseudocode adapted from the flowchart on page 13 of the Bullet User Manual.)

An Introduction to DynamicAnimControl
The centerpiece of Minie is DynamicAnimControl, a new PhysicsControl.
Adding a DynamicAnimControl to an animated model provides ragdoll physics and
inverse kinematics.
Configuration of DynamicAnimControl mostly takes place before the Control
is added to a model Spatial.  Adding the Control to a Spatial
automatically creates the ragdoll, including rigid bodies and joints.
No ragdoll exists before the Control is added to a Spatial,
and removing a Control from its controlled Spatial destroys the ragdoll.
The controlled Spatial must include the model's SkeletonControl.
Usually this is the model's root Spatial, but not always.
For a very simple example, see
HelloDac.java.
A model's ragdoll is composed of rigid bodies joined by 6-DOF joints.
Within the Control, each PhysicsRigidBody is represented by
a PhysicsLink, and the links are organized into a tree hierarchy.
PhysicsLink has 3 subclasses:

BoneLink: manages one or more bones in the model’s Skeleton.
Each BoneLink has a parent link, to which it is jointed.
Its parent may be another BoneLink or it may be a TorsoLink.
TorsoLink: is always the root of a link hierarchy,
so it has no parent link.
It manages all root bones in the model's Skeleton.  It also manages any
Skeleton bones that aren't managed by a BoneLink.
AttachmentLink: manages a non-animated model that's
attached to the main model by means of an attachment Node.
An AttachmentLink cannot be the parent of a link.

The default constructor for DynamicAnimControl is configured to create a
ragdoll with no bone links, only a TorsoLink.
Before adding the Control to a Spatial, specify which Skeleton bones
should be linked, by invoking the link() method for each of those bones.
I recommend starting with a default LinkConfig and a generous range of motion
for each linked bone:
dynamicAnimControl.link(boneName, new LinkConfig(), new RangeOfMotion(1f, 1f, 1f));

For a simple example, see
HelloBoneLink.java.
Alternatively, you can generate configuration code for a specific model using
the DacWizard application, which uses animation data to estimate
the range of motion for each linked bone.
You probably don't want to link every Bone in the model's Skeleton.
For instance, if the model has articulated fingers, you probably want to link
the hand bones but not the individual finger bones.
Unlinked bones will be managed by the nearest linked ancestor Bone.
The TorsoLink will manage any bones for which no ancestor Bone is linked.
If you link too many bones, the ragdoll may become inflexible or jittery
due to collisions between rigid bodies that don't share a PhysicsJoint.

External links

The Bullet Physics SDK Manual
The Physics section of the JME Wiki

YouTube videos about Minie:

April 2019 walkthru of the DacWizard application
watch (8:12)
source code
March 2019 short demo (IK for head/eye directions)
watch (1:25)
source code
March 2019 teaser (simulating buoyancy)
watch (0:10)
source code
February 2019 demo (simulating rope)
watch (4:47)
source code
December 2018 demo (inverse kinematics)
watch (6:27)
source code
December 2018 teaser (inverse kinematics)
watch (0:51)
November 2018 demo (single-ended joints):
watch (5:50)
source code
November 2018 demo (MultiSphere shape):
watch (0:13)
source code
October 2018 demo (DynamicAnimControl ragdolls):
watch (2:49)
source code


Acknowledgments
Like most projects, the Minie Project builds on the work of many who
have gone before.  I therefore acknowledge the following
artists and software developers:

Normen Hansen (aka ""normen"") for creating most of the jme3-bullet library
(on which Minie is based) and also for helpful insights
Rémy Bouquet (aka ""nehon"") for co-creating
KinematicRagdollControl (on which DynamicAnimControl is based)
and also for many helpful insights
Jules (aka ""dokthar"") for creating the soft-body fork of jMonkeyEngine
from which Minie's soft-body support is derived
Paul Speed, for helpful insights
""oxplay2"", for reporting a PhysicsRigidBody bug and helping me pin it down.
Nathan Vegdahl, for creating the Puppet model (used in
the TestDac walkthru video)
plus the creators of (and contributors to) the following software:

the Blender 3-D animation suite
the Bullet real-time physics library
the FindBugs source-code analyzer
the Git revision-control system and GitK commit viewer
the Google Chrome web browser
the Gradle build tool
the Java compiler, standard doclet, and runtime environment
jMonkeyEngine and the jME3 Software Development Kit
the Linux Mint operating system
LWJGL, the Lightweight Java Game Library
the MakeHuman Community
the Markdown document conversion tool
Microsoft Windows
the NetBeans integrated development environment
the Nifty graphical user interface library
Open Broadcaster Software Studio
the PMD source-code analyzer
the WinMerge differencing and merging tool



I am grateful to JFrog and Github for providing free hosting for the
Minie Project and many other open-source projects.
I'm also grateful to my dear Holly, for keeping me sane.
If I've misattributed anything or left anyone out, please let me know so I can
correct the situation: sgold@sonic.net
",12
fzxiao233/Auto_Record_Matsuri,Python,"Auto_Record_Matsuri 
此程序能够自动监控夏妹直播并下载当前的直播视频（其实你也可以修改其中的频道编号来监控其他VTB[臭DD]
目前我已经在VPS上部署了，你可以通过访问http://matsuri.fzxiao.tw 获取录播
特性

使用多进程模型，同时监控多个Vtuber占用较低（DD）

功能

自动监控Youtube|Openrec|Mirrativ|Twitcasting平台的直播
自动监控Bilibili熟肉更新
自动发送直播通知至QQ群（延迟极低，在检测时间设置为15s时）
自动即时抓流下载直播视频（从此杜绝苦苦哀求录播man）
自动将下载的视频上传并设置分享到百度云
顺带维护了一个网页用来发布直播存档

使用


请务必在启动前编辑config.py文件


首次启动或更新时
  $ pip3 install -r requirements.txt



执行程序
  $ python run.py



使用机器人需要下载酷Q Air 并安装 HTTP API


使用自动上传分享需要手动在BaiduPCS-Go文件夹中放入文件，并登录


支持

有问题或者新功能请求请发Issues

",11
hongjw1991/Java-DataStructure-Algorithm-DesignPattern,Java,"Java Data Structure

해당 Repository는 자료구조 및 알고리즘 Study 및 문제 풀이를 종합하여 정리한 Repository 입니다.
DataStructure

Java 기반 Data Structure 공부 내용을 정리한 Directory 입니다.


Algorithm

기본 Algorithm 공부한 내용을 정리한 Directory 입니다.


DesignPattern

기본 DesignPattern 공부한 내용을 정리한 Directory 입니다.


JDK 1.8 설치

Linux(Ubuntu)

참조


Ubuntu update 및 필요시 필수 패키지 설치

sudo apt-get update && sudo apt-get upgrate
sudo apt-get install software-properties-common


Java repository 추가

sudo add-apt-repository ppa:webupd8team/java
sudo apt-get update


Java 설치

sudo apt-get install oracle-java8-installer




Windows

여기서 설치




IDE 설치

여기서 설치
IntelliJ를 다운로드 받고 설치.
Community version은 무료, Ultimate는 30일 무료 trial 가능
설치 시, Windows라면, 32/64 선택 후, extension은 모두 check하는 것을 권장
이전 version에서 설치한 이력이 있는 경우 import하고 없는 경우 빈 상태로 시작
Template을 고르고, Default setting 및 plugin을 필요 시 설치 후 next
JDK 경로 설정









",3
rrwick/Long-read-assembler-comparison,HTML,"Benchmarking of long-read assembly tools for bacterial whole genomes
Ryan R. Wick1 and Kathryn E. Holt1,2

1. Department of Infectious Diseases, Central Clinical School, Monash University, Melbourne, Victoria 3004, Australia2. London School of Hygiene & Tropical Medicine, London WC1E 7HT, UK

This repo contains early results for our benchmarking of long-read assemblers for bacterial genomes. It is still a work in progress, and we'll be putting together a manuscript in the future. But until then, we decided to show our results here on GitHub for the eager and/or curious!
Still interested? Read on! Or if you're feeling impatient, skip to the results or conclusions.
Table of contents

Introduction
Methods

Assemblers and commands
Simulated read sets
Real read sets
Assessing chromosome contiguity
Assessing plasmid assembly


Results and discussion

Robustness
Reliability
Sequence identity
Speed performance
Plasmids


Conclusions
References
License

Introduction
We had two primary aims for this study:

To explore how different features of a read set affect assemblers. I.e. what does and does not matter about your reads for the purposes of assembly?
To compare the performance of long-read assemblers for bacterial genomes and make recommendations for users.

We investigated aim 1 using simulated read sets from Badread, which gives a lot of control over many read parameters. For aim 2, we used both simulated read sets (again from Badread) and real read sets (both ONT and PacBio).
We approached this study with the following attitude: long read assembly of a bacterial genome should be a solved problem, at least from a large-scale structural perspective. A successful long-read assembly is complete (one contig per replicon) and without any structural errors – anything else counts as a failure. However, small-scale errors (substitutions and indels) are currently inevitable in long-read assemblies, i.e. consensus sequence accuracy is less than 100%. While consensus accuracy is interesting and does briefly feature in our results, it wasn't a major focus for this study. That's because long-read assemblies should probably be polished with a platform-specific tool like Arrow or Nanopolish anyway. Since sequence identity is something that can (and should) be fixed up in a separate post-assembly step, an assembler's consensus error rate doesn't matter too much.
Finally, I should also point out some things that this study does not do:

It doesn't address where Illumina reads fit into the picture. E.g. is it better to do a hybrid assembly with a tool like Unicycler or to do a long-read-only assembly and then polish with a tool like Pilon? This is a great question which deserves attention, but here we studied assembly of long reads only.
It doesn't investigate how assemblers perform on viruses, eukaryotes, metagenomes or anything else that isn't a bacterial genome.

Methods
Assemblers and commands
We tested the current version (as of the time of writing) of five different assemblers: Canu, Flye, Ra, Unicycler and Wtdbg2.
Assemblers that only work on PacBio reads (i.e. not on Oxford Nanopore reads) were excluded, such as HGAP, FALCON and HINGE. We also excluded SMARTdenovo as it seems to have been superseded by Wtdbg2 (same author).
We ran each assembler with its default options or by following the recommended usage in its documentation. I.e. we didn't explore the effect of assembler options/parameters on assemblies (with the exception of a couple options for Flye, see below).
Canu
Canu v1.8 is run with a single command which needs a genome size estimate. We also used some options to set threads/memory and turn off parallel grid computing, and we used stopOnLowCoverage=0 to ensure that Canu continued in cases where read depth is low. It produces an output directory where the final assembly is named *.contigs.fasta.
canu -p canu -d out_dir genomeSize=3.565m stopOnLowCoverage=0 useGrid=false minThreads=4 maxThreads=4 maxMemory=31 -nanopore-raw reads.fastq.gz

For PacBio read sets we used -pacbio-raw instead of -nanopore-raw. While the Canu documentation makes a setting recommendation for plasmid recovery, this is no longer needed since Canu v1.7, so we did not change the command for the plasmid tests.
Flye
Flye v2.4.2 is run with a single command which needs a genome size estimate. It produces an output directory where the final assembly is named scaffolds.fasta.
flye --nano-raw reads.fastq.gz -g 3.565m -o out_dir -t 4

For PacBio read sets we used --pacbio-raw instead of --nano-raw. For the plasmid tests, we additionally tried Flye with its --plasmids and --meta options.
Ra
Ra 07364a1 (it doesn't have releases, so we used the latest commit) is run with a single command that does not require genome size. It delivers its assembly to stdout.
ra -x ont -t 4 reads.fastq.gz > assembly.fasta

For PacBio read sets we used -x pb instead of -x ont.
Unicycler
While Unicycler is most commonly used as a hybrid assembler, it can do long-read-only assemblies as well. It uses miniasm to conduct the assembly and multiple rounds of Racon to polish the result. Its miniasm has been slightly altered to better handle circularisation of bacterial replicons.
Unicycler v0.4.7 is run with a single command that does not require genome size. It produces an output directory where the final assembly is assembly.fasta.
unicycler -l reads.fastq.gz -o out_dir -t 4

Unicycler doesn't make a distinction between ONT and PacBio reads, so we did not change the command for PacBio read sets. And a disclosure: I am the developer of Unicycler, so may hold some biases here. But I did my best to be fair!
Wtdbg2
Wtdbg2 v2.4 separates its functionality into assembly, consensus and polishing steps, so it requires multiple commands to run, the first of which requires genome size.
wtdbg2 -x nanopore -g 3.565m -i reads.fastq.gz -t 4 -fo out
wtpoa-cns -t 4 -i out.ctg.lay.gz -fo out.ctg.fa
minimap2 -t 4 -x map-ont -a out.ctg.fa reads.fastq.gz | samtools sort > out.ctg.bam
samtools view out.ctg.bam | wtpoa-cns -t 4 -d out.ctg.fa -i - -fo assembly.fasta

For PacBio read sets we used -x rsII instead of -x nanopore in the wtdbg2 command and -x map-pb instead of -x map-ont in the minimap2 command.
Simulated read sets
To make all of our simulated read sets we used Badread (which was developed largely for this assembler comparison) and a Comamonas kerstersii genome as the reference. This genome was chosen because it is of medium size (3.56 Mbp), has no plasmids and is of moderate complexity. It is not particularly repeat-rich, but it does have four tandem copies of its rRNA operon, which creates a 20 kbp repeat region in the genome.
Problem sets
For seven different Badread parameters, we conducted a parameter sweep, generating a range of read sets which varied from very good (often better than a real read set) to very bad (much worse than a real read set). For each of the seven parameter sweeps, the other parameters were kept in a good state. E.g. when varying the amount of junk reads in a set, the chimeric reads were kept at 0%. These read sets served to assess the robustness of each assembler: how tolerant it is of read set problems. They are referred to as the 'problem sets'.
Adapter lengths were varied from 0 to 1000 bp. Real read sets probably have adapters lengths measured in 10s of bp, assuming they haven't been trimmed off.
for x in {0..149}; do
    adapter_len=$(printf ""%04d"" $(python3 -c ""print(int(round(""$x"" ** 1.38046)))""))
    badread simulate --reference ref.fasta --quantity 60x --junk_reads 0 --random_reads 0 --chimeras 0 --glitches 0,0,0 --start_adapter_seq ""$adapter_len"" --end_adapter_seq ""$adapter_len"" --start_adapter 100, 50 --end_adapter 100, 50 | gzip > ""$adapter_len"".fastq.gz
done

Chimera reads were varied from 0% to 25%. I expect a real read set to have less than 2% chimeric reads.
for chimeras in $(seq -f ""%06.3f"" 0.0 0.125 25.0); do
    badread simulate --reference ref.fasta --quantity 60x --junk_reads 0 --random_reads 0 --chimeras ""$chimeras"" --glitches 0,0,0 --start_adapter_seq """" --end_adapter_seq """" | gzip > ""$chimeras"".fastq.gz
done

Read glitches are controlled by three parameters. This parameter sweep varies all three together, with size and skip ranging from 0 to 100 and rate ranging from 100000 to 100.
for size_skip in $(seq -f ""%05.1f"" 0.0 0.5 100.0); do
    rate=$(printf ""%06d"" $(python3 -c ""print(int(round(100000 / (1.995276 ** (""$size_skip""/10)))))""))
    glitches=""$rate"",""$size_skip"",""$size_skip""
    badread simulate --reference ref.fasta --quantity 60x --junk_reads 0 --random_reads 0 --chimeras 0 --glitches ""$glitches"" --start_adapter_seq """" --end_adapter_seq """" | gzip > ""$glitches"".fastq.gz
done

Random read rates and junk read rates were varied together from 0% to 25%. E.g. when set to 15%, that means 15% of the reads are random reads and 15% are junk reads, so a total of 30% of reads are either random or junk. A real read set probably has no more than a few percent of random/junk reads, and possibly much less if the reads have been demultiplexed out of a barcoded run which serves to clean them up.
for random_junk in $(seq -f ""%06.3f"" 0.0 0.125 25.0); do
    badread simulate --reference ref.fasta --quantity 60x --junk_reads ""$random_junk"" --random_reads ""$random_junk"" --chimeras 0 --glitches 0,0,0 --start_adapter_seq """" --end_adapter_seq """" | gzip > ""$random_junk"".fastq.gz
done

Read depth was varied from 0.5× to 100×. Canu documentation states '30× and 60× coverage is the recommended minimum.' Flye's documentation states 'Typically, 30× coverage is enough to produce good draft contigs.'
for read_depth in $(seq -f ""%05.1f"" 0.0 0.5 100.0); do
    badread simulate --reference ref.fasta --quantity ""$read_depth""x --junk_reads 0 --random_reads 0 --chimeras 0 --glitches 0,0,0 --start_adapter_seq """" --end_adapter_seq """" | gzip > ""$read_depth"".fastq.gz
done

Read identity was varied from a mean of 60% (so low as to be nearly indistinguishable from junk) to 99.75%, always using a small standard deviation of 0.25%. Real long read sets typically have a mean identity in the mid-80s to low-90s and with a larger standard deviation.
for identity in $(seq -f ""%.2f"" 60 0.25 99.75); do
    badread simulate --reference ref.fasta --quantity 60x --identity ""$identity"",100,1 --junk_reads 0 --random_reads 0 --chimeras 0 --glitches 0,0,0 --start_adapter_seq """" --end_adapter_seq """" | gzip > ""$identity"".fastq.gz
done

Read length was varied from a mean of 250 bp to 50 kbp, always using a standard deviation of 1/10th the mean. The read length in real read sets depends on the DNA extraction and library preparation but would usually have a much larger standard deviation than I used here.
for length in $(seq -f ""%05g"" 250 250 50000); do
    badread simulate --reference ref.fasta --quantity 60x --length $length"",""$(echo $length"" / 10"" | bc) --junk_reads 0 --random_reads 0 --chimeras 0 --glitches 0,0,0 --start_adapter_seq """" --end_adapter_seq """" 2> ""varying_read_length/""$1"".log"" | gzip > ""varying_read_length/""$1"".fastq.gz""
done

Good sets
Unlike for the problem sets, these read sets all stay within the realm of assemble-ability. The Badread parameters were randomly generated for each set, so there should be a decent amount of variation between them.
We refer to these as the 'good sets' to distinguish them from the problem sets. These doesn't mean that they are all exceptionally good, but they are good enough to assemble, i.e. there's nothing terribly wrong with any of them,
I made 500 such sets using these commands:
for x in {000..499}; do

    # Randomly generate parameters:
    quantity=$(python3 -c ""import random; print(random.randint(142608240, 356520600))"")  # 40x to 100x
    length=$(python3 -c ""import random; print(random.randint(10000, 25000))"")"",""$(python3 -c ""import random; print(random.randint(5000, 20000))"")
    identity=$(python3 -c ""import random; print(random.randint(85, 90))"")"",""$(python3 -c ""import random; print(random.randint(95, 100))"")"",""$(python3 -c ""import random; print(random.randint(1, 8))"")
    chimeras=$(python3 -c ""import random; print('{:.3f}'.format(random.uniform(0.0, 2.0)))"")
    junk_reads=$(python3 -c ""import random; print('{:.3f}'.format(random.uniform(0.0, 2.0)))"")
    random_reads=$(python3 -c ""import random; print('{:.3f}'.format(random.uniform(0.0, 2.0)))"")
    glitches=$(python3 -c ""import random; print(random.randint(1000, 10000))"")"",""$(python3 -c ""import random; print(random.randint(0, 20))"")"",""$(python3 -c ""import random; print(random.randint(0, 20))"")
    start_adapter=$(python3 -c ""import random; print(random.randint(0, 30))"")
    end_adapter=$(python3 -c ""import random; print(random.randint(0, 30))"")

    # Create the read set:
    badread simulate --reference ref.fasta --quantity ""$quantity"" --length ""$length"" --identity ""$identity"" --junk_reads ""$junk_reads"" --random_reads ""$random_reads"" --chimeras ""$chimeras"" --glitches ""$glitches"" --start_adapter_seq ""$start_adapter"" --end_adapter_seq ""$end_adapter"" | gzip > ""$x"".fastq.gz
    gzip ""$x"".fastq
done

Plasmid sets
The plasmid sets are also simulated with Badread, but we used reference genomes with plasmids added in addition to the Comamonas kerstersii chromosome used in the previous tests. I took the plasmids from the curated dataset presented in Orlek 2017, which can be downloaded here.
The make_plasmid_test_references.py script created the references, adding between 1 and 9 plasmids per genome, at various depths:
for x in {000..214}; do
    make_plasmid_test_references.py chromosome.fasta nucleotideseq.fa plasmid_references
done

There's nothing special about the number of reference genomes: 215. This was just the amount which gave us 1000 total usable plasmids (a nice round number) for our assembly tests.
We then simulated the read sets with Badread, keeping the problem parameters at zero and the base (chromosomal) depth at 100x:
for x in {000..214}; do
    badread simulate --reference plasmid_references/""$x"".fasta --quantity 100x --junk_reads 0 --random_reads 0 --chimeras 0 --glitches 0,0,0 --start_adapter_seq 0 --end_adapter_seq 0 | gzip > ""$x"".fastq
done

Real read sets
For real read sets, we used data from PRJNA422511 (produced for Nicola De Maio's paper on long-read sequencing platforms) which contains Illumina, ONT and PacBio reads for 20 bacterial samples. In order to test our assemblers, we need a nice completed assembly for each sample to use as a reference, but to avoid biasing the results we didn't want to use any of the tested long-read assemblers to get that assembly.
We therefore conducted Unicycler hybrid assemblies using its --no_miniasm option (this turns off the part of the hybrid Unicycler pipeline which it has in common with long-read-only Unicycler, ensuring an independent assembly) using both Illumina-ONT and Illumina-PacBio combinations. We looked for samples where: a) both hybrid assemblies successfully completed the genome, and b) the two hybrid assemblies were in near-perfect agreement with each other. This was only the case for five of the 20 samples: SAMN10819801, SAMN10819805, SAMN10819807, SAMN10819813 and SAMN10819815. Those five samples were therefore the ones we used for our real read tests (ONT and PacBio read sets for each) using our hybrid assemblies as reference genomes.
The ONT and PacBio read sets were pretty large, so we made 10 random samplings of each with seqtk to produce read sets between 40× and 100× depth:
genome_size=5390818
reads=SAMN10819801/nanopore_SRR8494941.fastq.gz
total_bases=$(fast_count $reads | cut -f3)
for x in {00..09}; do
    target_bases=$(python3 -c ""import random; print(int(random.uniform(40.0, 100.0) * ""$genome_size""))"")
    fraction=$(python3 -c ""print('{:.4f}'.format(""$target_bases"" / ""$total_bases""))"")
    seqtk sample $reads $fraction | gzip > SAMN10819801_nanopore_""$x"".fastq.gz
done

Making 10 such subsampled sets for both Nanopore and PacBio reads of the five samples produced 10 × 2 × 5 = 100 real read sets.
Assessing chromosome contiguity
Both problem set assemblies and good set assemblies were analysed in the same manner. In an attempt to distil assembly quality down to a single metric, we defined a measure that we call assembly contiguity. This is the longest single alignment between the assembly and the reference, in terms of the reference length:

Contiguity is 100% if the assembly went perfectly.
Contiguity slightly less than 100% (e.g. 99.99%) indicates that the assembly was complete, but some bases were lost at the start/end of the circular chromosome.
Contiguity more than 100% (e.g. 102%) indicates that the assembly contains duplicated sequence via a start-end overlap.
Much lower contiguity (e.g. 70%) indicate that the assembly was not complete, either due to fragmentation or misassembly.

The first step in determining contiguity is to align the assembly to the reference chromosome. However, since the chromosome is circular, an assembled version of this chromosome might start at any point (and on either strand). This means that a single assembly-to-reference alignment is unlikely, even if the assembly went perfectly, as the assembled contig probably starts in a different location from the reference. Instead, we would get two alignments: one alignment from the start of the contig to the end of the reference and a second alignment from the start of the reference to the end of the contig:

By doubling the reference (two copies of the chromosome back-to-back), we can avoid this problem and get a single alignment covering the whole assembly:

However, if the assembly contains extra sequence from a start-end overlap then even a doubled reference may not be enough:

We therefore used a tripled reference to ensure that any assembled version of the genome (even with a big start-end overlap) could fully align to the reference:

Having made this tripled reference, we used minimap2 to align the assembly:
minimap2 -c -t 4 -r 10000 -g 10000 -x asm20 --eqx ref_tripled.fasta assembly.fasta > assembly.paf

The -x asm20 preset allows minimap2 to align sequences even when they are somewhat divergent. To encourage minimap2 to align through lower identity regions in the assembly (as opposed to stopping one alignment and starting another), we also used the -r 10000 -g 10000 options.
We then gave the assembly, the read set, the minimap2 alignment and the reference genome length to the assembly_stats.py script:
assembly_stats.py assembly.fasta reads.fastq.gz assembly.paf 3565206 >> results

It produces a single line of output with many columns which describe both the read set and the assembly, including the contiguity value.
Assessing plasmid assembly
Unlike the problem sets and good sets, the plasmid sets have multiple replicons. We therefore devised a different analysis which produces a binary pass/fail metric for each replicon in the reference.
The completed_contigs.py script takes three inputs: the assembly, the read set and the reference genome.
completed_contigs.py assembly.fasta reads.fastq.gz reference.fasta >> results

It carries out the following steps:

Aligns the reads to the reference to find the read depth of each replicon.
Aligns the assembly to a tripled reference.
For each replicon in the reference, look for a single contig in the assembly that is a clean match: the contig covers aligns to at least 95% of the reference and vice versa. If such a contig is found, that replicon's assembly succeeded. If no such contig is found, that replicon's assembly failed.

Results and discussion
You can find the raw results in the results directory, along with the R script used to make the plots.
Robustness
This plot shows the results for each assembler's attempt at assembling our problem sets. The reference genome for these tests only contains one chromosome, so a single contig (solid dot) at 100% indicates success. An open dot indicates more than one contig and falling below 100% indicates incomplete assembly or misassembly.

Adapter length (lower is better)
I was surprised to see that adding adapters to reads made little difference to the assembly, even when the adapters were unrealistically large (up to 1 kbp). The lesson seems to be that adapters really don't matter for any of the tested assemblers!
Chimeras (lower is better)
Canu was most affected by chimeric reads, starting to suffer at rates of ~6% or more. Unicycler was affected by chimeras at higher rates of ~15% or more.
The other assemblers were robust against all chimera rates. Considering that real read sets typically have 2% chimeras or less, they are probably not a serious concern for assembly, regardless of the assembler used.
Glitches (lower is better)
Read glitchiness is defined by three parameters: rate, size and skip (see Badread docs for more explanation). Here we've condensed those down to a single parameter: glitch level. Higher levels mean glitches are larger and occur more frequently. The glitchiness of reads matters for all assemblers, but Wtdbg2 was the most sensitive to glitchy reads and Unicycler the most robust. This suggests that repairing read glitches (e.g. with a tool like DASCRUBBER) is probably not of great concern, though might be of benefit if using Wtdbg2.
Random/junk reads (lower is better)
Canu was most sensitive to the inclusion of random/junk reads in the read set. Rates of over 10% caused Canu to not finish (mainly due to timing out at four days of assembly time). Unicycler handles random/junk reads well up to a rate of ~17%, beyond which its assemblies often fail. Flye and Wtdbg2 successfully assemble the chromosome at all random/junk rates but usually produce spurious contigs in addition to the chromosome. Also, Flye's assembly time suffered greatly with increasing random/junk rates. Ra tolerated random/junk the best, with essentially no effect at any rate.
Read depth (higher is better)
All assemblers did well with ~25× depth or more. Unicycler was worst in this regard and needed greater read depth, while Flye did best and successfully assembled the chromosome at ~13× depth.
Read identity (higher is better)
Ra was able to tolerate the lowest identity reads, down to ~80% identity. Canu required ~86% identity, and the other assemblers fell in between. Assuming your long-reads are relatively modern, read identity probably won't be an issue for completing your assemblies. It is more likely to play a role in consensus sequence accuracy: higher accuracy reads probably give a better consensus sequence. We didn't explore that relationship in this study, but it is touched upon in our paper on basecalling performance.
Read length (higher is better)
Canu was pickiest with read length, not achieving consistent success until the mean length was ~15 kbp. The other assemblers were able to complete the chromosome with somewhat shorter reads. A subtle feature of the Canu plot is also worth pointing out: the contiguity climbs increasingly over 100% with greater read lengths. I.e. the longer the reads, the more start-end overlap in a Canu assembly.
Reliability
By 'reliability' we mean how likely an assembler is to create a complete chromosome assembly (i.e. a contiguity of ~100%) from a decent set of reads. These plots show the results from the simulated good sets and the real read sets:

The simulated and real read sets are in nice agreement here. Canu, Flye, Ra and Unicycler are all decently reliable, usually producing assemblies with a contiguity near 100%. Of these, Ra was the best, only failing to achieve >95% contiguity for 5/500 simulated read sets and 3/100 real read sets.
Wtdbg2 was the only assembler which often failed to produce a complete assembly. Sometimes the low contiguity of Wtdbg2's assemblies could be explained by fragmentation (the chromosome assembled into more than one piece), but there were also plenty of one-contig Wtdbg2 assemblies which got a low contiguity – why? I used the re-DOT-able tool to manually investigate and found that Wtdbg2 assemblies often contain junky regions hundreds of base pairs in size. Our contiguity metric is based on minimap2 alignments, and these junky regions can break the contig-to-reference alignment into pieces.
These plots also reveal how well the assemblers circularise the chromosome. Canu typically produces a large amount of start-end overlap. As the problem set tests showed, the size of this overlap depends on the read length, shown here with a larger contiguity on ONT assemblies than PacBio assemblies (the ONT read sets contained longer reads). Unicycler was the only assembler able to circularise the chromosome exactly, i.e. a contiguity of exactly 100% with no extra or deleted bases. Flye typically deleted a small amount of sequence at the start/end, on the order of tens of bases. Ra was worse in this respect and tended to delete hundreds of bases.
Sequence identity
This plot shows the consensus sequence identity for the real read sets:

For each assembler, the top part of the distribution is for PacBio read sets and the bottom for ONT read sets. This demonstrates a common issue with ONT reads: systematic read errors which make their way into the consensus sequence. I should also point out that neither the PacBio or ONT reads we used are state-of-the-art, so don't take take the particular accuracy values too seriously – modern data may well have fewer errors.
Ra and Unicycler (both of which use Racon to polish) do well on ONT reads but are underwhelming for PacBio reads. Conversely, Flye does best for PacBio reads and worst for ONT reads. I found this lack of correlation between PacBio and ONT accuracy to be surprising.
While this is interesting, as I stated in the introduction, I don't think consensus sequence identity is the most important metric in this comparison. If you're using a platform-specific polishing tool on your assembly (and you probably should be), then the assembly's structural correctness (measured by our contiguity metric) is much more important than its sequence identity.
Speed performance
This plot shows the time taken to assemble (using four CPU threads) for the real read sets:

Wtdbg2 and Ra were the fastest, both performing especially well on PacBio reads. Flye was slightly slower but always completed in less than 30 minutes. Unicycler was slower, sometimes taking up to 90 minutes to complete. Canu was by far the slowest, taking an hour or two for most PacBio read sets and many hours for Nanopore read sets.
Plasmids
These plots show plasmid assembly for both simulated and real reads. Each of the eight plots contains the same points – one for each plasmid. Filled points indicate successful assembly, and the plot titles indicate the percentage of success:

All assemblers did well with large plasmids of moderate depth. While you might expect that very high depth plasmids would also assemble well, this was curiously not the case for Unicycler and Wtdbg2. Small plasmids were often a problem, especially for Wtdbg2, but Canu did well there.
These were the only tests where I strayed into playing with assembler parameters. This is because Flye has an option specifically for plasmid recovery: --plasmids. It also has an option for metagenomes (--meta) which I tried on the basis that a plasmid-rich bacterial genome is a bit like a metagenome. Running Flye with --plasmids made a noticeable improvement for small plasmids and using --meta boosted its performance on low-depth plasmids. Both options can be used together, which gave the best overall performance on plasmid assembly.
This raises the question: are there any downsides to using Flye's --plasmids and/or --meta options? In particular, do these options negatively affect the assembly of the chromosome? Of the 215 simulated plasmid sets and the 100 real read sets, here are the number of successfully assembled chromosomes (>95% contiguity) for each combination of options:



Flye options
Complete chromosome (simulated)
Complete chromosome (real)




defaults
211 / 215
83 / 100


--plasmids
211 / 215
81 / 100


--meta
208 / 215
84 / 100


--plasmids --meta
209 / 215
84 / 100



No strong trend is apparent, so I'll tentatively conclude that the options do not have a downside – when in doubt, use them.
Conclusions
What about a read set does and does not matter for assembly? Read length and read depth seem to be the most important factors – no surprises there. Higher identity reads are obviously better, but they are probably more important for consensus sequence identity than they are for assembly contiguity. Don't worry about adapter sequences, and chimeras aren't a problem either as long as there aren't too many. Finally, it's probably a good idea to filter junky and glitchy reads out of your set before assembly, so some read QC is warranted.
Here are my final thoughts on each of the assemblers tested:

Canu was the slowest assembler and not the most reliable or robust. Its strength is in its configurability (something we didn't explore in this study) – just take a look at the parameter reference page of Canu's documentation to get an idea of how many knobs it has to adjust. I suspect that power users who are willing to learn Canu's nuances may find that they can tune it to fit their needs. But if you have a bacterial read set and just want to get it assembled without knowing how the sausage is made, then Canu is probably not the best choice.
Flye was definitely one of my favourites. It was reliable, robust and reasonably fast. It also did best on plasmids when we used its --plasmid and --meta options which enhanced assembly of small and low-depth plasmids, respectively. I would probably call it a clear winner if it wasn't for one small wart: Flye tends to delete some bases when assembling a circular chromosome.
Ra was another of my favourites. It was the most reliable assembler we tested, completing the chromosome in more cases than any other, and it was very fast. But it suffered from worse circularisation problems than Flye (deleted hundreds of bases) and wasn't good with small plasmids. If those two issues don't bother you, then Ra may be the best choice. I also liked that Ra doesn't require a genome size parameter – this makes it easier to use when you don't know what you are assembling.
Unicycler performed moderately well in most of our tests, not bad but not exceptional either. However, it was the only assembler that usually achieved perfect circularisation of the chromosome, i.e. the last base of a circular contig was followed by the first base – nothing duplicated and nothing missing. If this feature is crucial to you, then Unicycler is the best choice. Also, Unicycler does not require a genome size parameter to run, which is nice.
Wtdbg2 was the least reliable assembler we tested due to glitches in its assembled sequences that break contiguity, making it a hard one to recommend. But it was very fast, even though we ran it with a round of polishing. Excluding that step (i.e. deriving the consensus but not polishing it) would make it even faster. Since its main appeal is speed performance, I feel as though Wtdbg2 is better suited to assembly of larger eukaryote genomes where resources can be a limiting factor.

If I had to condense my recommendations into a tweet-sized snippet, I would say:

The overall best long-read assembler for bacterial genomes is Flye. However, you might prefer Canu for its configurability, Ra for its reliability, Unicycler for its clean circularisation or Wtdbg2 for its speed.

As always, there is still room for improvement in this space. A perfect long-read bacterial genome assembler would complete assemblies whenever possible, circularise contigs cleanly, handle plasmids of any size, tolerate bad reads, not require a genome size parameter and polish consensus sequences to the highest degree allowed by the read errors. The assemblers we tested were not too far from this platonic ideal, so I hope its realisation isn't far away!
References
Kolmogorov M, Yuan J, Lin Y, Pevzner PA. 2019. Assembly of long, error-prone reads using repeat graphs. Nature Biotechnology. doi:10.1038/s41587-019-0072-8.
Koren S, Walenz BP, Berlin K, Miller JR, Phillippy AM. 2017. Canu: scalable and accurate long-read assembly via adaptive k-mer weighting and repeat separation. Genome Research 27:722–736. doi:10.1101/gr.215087.116.
Li H. 2018. Minimap2: Pairwise alignment for nucleotide sequences. Bioinformatics 34:3094–3100. doi:10.1093/bioinformatics/bty191.
Maio N De, Shaw LP, Hubbard A, et al. 2019. Comparison of long-read sequencing technologies in the hybrid assembly of complex bacterial genomes. bioRxiv:530824. doi:10.1101/530824.
Orlek A, Phan H, Sheppard AE, et al. 2017. A curated dataset of complete Enterobacteriaceae plasmids compiled from the NCBI nucleotide database. Data in Brief 12:423–426. doi:10.1016/j.dib.2017.04.024.
Ruan J, Li H. 2019. Fast and accurate long-read assembly with wtdbg2. bioRxiv:530972. doi:10.1101/530972.
Vaser R, Sović I, Nagarajan N, Šikić M. 2017. Fast and accurate de novo genome assembly from long uncorrected reads. Genome Research 27:737–746. doi:10.1101/gr.214270.116.
Wick RR, Judd LM, Gorrie CL, Holt KE. 2017. Unicycler: resolving bacterial genome assemblies from short and long sequencing reads. PLOS Computational Biology 13:e1005595. doi:10.1371/journal.pcbi.1005595.
License
GNU General Public License, version 3
",11
holdenk/clothes-from-code,Python,"clothes-from-code
Auto generate cool code based clothing.
Requires
This depends on jpglitch and pygments.
Samples
So far we've made samples for the gen.py generating its self and kubicorn's reconciler. I may post more in the glitch code cowcow store.
Development
Much of the development was live-streamed because ""why not?"" and you can look watch it at https://www.youtube.com/watch?v=nUbgxMqp27U
How to use
Run gen.py and provide an input file then take the output to cowcow and upload the individual image components.
",2
jensimmons/jensimmons.com,HTML,"Jen Simmons dot com
What is it?
The personal website of Jen Simmons.
Built on top of Dan Urbanowicz’ Eleventy Netlify Boilerplate, for making a static website using the Eleventy static site generator, with Netlify CMS baked-in, deployed to Netlify.
Notes on Making This Thing Go
Navigate to /admin, and start editing content.
Locally, run Eleventy to build the site
npx eleventy

Or build automatically when a template changes:
npx eleventy --watch

Or in debug mode:
DEBUG=* npx eleventy

",3
CoolerVoid/HiddenWall,C,"HiddenWall
HiddenWall is a Linux kernel module generator for custom rules with netfilter. (block ports, Hidden mode, rootkit functions etc).

The motivation: on bad situation, attacker can put your iptables/ufw to fall... but if you have HiddenWall,
the attacker will not find the hidden kernel module that block external access, because have a hook to netfilter on
kernel land(think like a second layer for firewall).
My beginning purpose at this project is protect my personal server, now is protect the machines of my friends.
When i talk ""friends"", i say peoples that don't know how to write low level code. Using the HiddenWall you can
generate your custom kernel module for your firewall configuration.
The low level programmer can write new templates for modules etc...
First step, understand before run
Verify if the kernel version is 3.x, 4.x or 5.x:
uname -r

Clone the repository
git clone https://github.com/CoolerVoid/HiddenWall

Enter the folder
cd HiddenWall/module_generator

Edit your firewall rules in directory  rules/server.yaml, the python scripts use that file to generate a new firewall module.
$ cat rules/server.yaml
module_name: SandWall
public_ports: 80,443,53
unhide_key: AbraKadabra
hide_key: Shazam
fake_device_name: usb14
liberate_in_2_out: True
whitelist: 
- machine: 
   ip: 192.168.100.181
   open_ports: 22,21
- machine:
   ip: 192.168.100.22
   open_ports: 22


If you want study the static code to generate, look the content at directory ""templates"".
Second step, generate your module
If you want generate a kernel module following your YAML file of rules, follow that command:
$ python3 WallGen.py --template template/hiddenwall.c -r rules/server.yaml

This generate a generic module with rules of server.yaml, if you want to use another template you can use ""wall.c"", so template module ""hiddenwall"" have option to run on hidden mode(is not visible to ""# lsmod"" for example).
Third step, install your module
To test module:
# cd output; make clean; make
# insmod SandWall.ko

The rule of YAML to generate module is simple, drop all out to in packets, accept ports 80,443 and 53. The machine 192*.181 can connect at ports 22 and 21...
if you use nmap at localhost/127.0.0.1 you can view the ports open... because rule liberate_in_2_out is true.
Password to turn Firewall visible is ""AbraKadabra"".
Password to turn Firewall invisible is ""Shazam"".
You need to send password for your fake device ""usb14"".
To exit module, you need turn visible at ""lsmod"" command ...
# echo ""AbraKadabra"" > /dev/usb14
# lsmod | grep SandWall
# rmmod SandWall

Random notes
Tested on ubuntu 16 and fedora 29 at kernels ""3.x"",""4.x"" and ""5.x"".
TODO
Suport to IPV6.
Macro to select the interface(to use multiple modes for each interface).
Option to remove last logs when turn hide mode.
Option to search and remove others toolkits...
Code generator to BFP...
References
Wikipedia Netfilter
https://en.wikipedia.org/wiki/Netfilter
Linux Device Drivers
http://lwn.net/Kernel/LDD3/
M0nad's Diamorphine
https://github.com/m0nad/Diamorphine/
",5
anishkny/integrify,JavaScript,"𝚒𝚗𝚝𝚎𝚐𝚛𝚒𝚏𝚢






🤝 Enforce referential and data integrity in Cloud Firestore using triggers
Introductory blog post
Usage
// index.js

const { integrify } = require('integrify');

const functions = require('firebase-functions');
const admin = require('firebase-admin');
admin.initializeApp();
const db = admin.firestore();

integrify({ config: { functions, db } });

// Automatically replicate attributes from source to target
module.exports.replicateMasterToDetail = integrify({
  rule: 'REPLICATE_ATTRIBUTES',
  source: {
    collection: 'master',
  },
  targets: [
    {
      collection: 'detail1',
      foreignKey: 'masterId',
      attributeMapping: {
        masterField1: 'detail1Field1',
        masterField2: 'detail1Field2',
      },
    },
    {
      collection: 'detail2',
      foreignKey: 'masterId',
      attributeMapping: {
        masterField1: 'detail2Field1',
        masterField3: 'detail2Field3',
      },
    },
  ],

  // Optional:
  hooks: {
    pre: (change, context) => {
      // Code to execute before replicating attributes
      // See: https://firebase.google.com/docs/functions/firestore-events
    },
  },
});

// Automatically delete stale references
module.exports.deleteReferencesToMaster = integrify({
  rule: 'DELETE_REFERENCES',
  source: {
    collection: 'master',
  },
  targets: [
    {
      collection: 'detail1',
      foreignKey: 'masterId',
    },
  ],

  // Optional:
  hooks: {
    pre: (snap, context) => {
      // Code to execute before deleting references
      // See: https://firebase.google.com/docs/functions/firestore-events
    },
  },
});

// Automatically maintain count
[
  module.exports.incrementFavoritesCount,
  module.exports.decrementFavoritesCount,
] = integrify({
  rule: 'MAINTAIN_COUNT',
  source: {
    collection: 'favorites',
    foreignKey: 'articleId',
  },
  target: {
    collection: 'articles',
    attribute: 'favoritesCount',
  },
});
Deploy to Firebase by executing:
$ firebase deploy --only functions
Rules File
Alternately, rules can be specified in a file named integrify.rules.js.
// index.js

const { integrify } = require('integrify');

const functions = require('firebase-functions');
const admin = require('firebase-admin');
admin.initializeApp();
const db = admin.firestore();

integrify({ config: { functions, db } });

// Rules will be loaded from ""integrify.rules.js""
module.exports = integrify();
// integrify.rules.js

module.exports = [
  {
    rule: 'REPLICATE_ATTRIBUTES',
    name: 'replicateMasterToDetail',
    // ...
  },
  // ...
];
",17
ledgersmb/LedgerSMB,PLpgSQL,"LedgerSMB
Small and Medium business accounting and ERP






SYNOPSIS
LedgerSMB is a free integrated web application accounting system, featuring
double entry accounting, budgetting, invoicing, quotations, projects, timecards,
inventory management, shipping and more ...
The UI allows world-wide accessibility; with its data stored in the
enterprise-strength PostgreSQL open source database system, the system is known
to operate smoothly for businesses with thousands of transactions per week.
Screens and customer visible output are defined in templates, allowing easy and
fast customization. Supported output formats are PDF, CSV, HTML, ODF and more.
Directly send orders and invoices from the built-in e-mail function to your
customers or RFQs (request for quotation) to your vendors with PDF attachments.
System requirements
Note that these are the system requirements for LedgerSMB 1.7; the planned next
minor release. Please check the system requirements for the 1.5 old stable
version
and current 1.6 version.
Server

Perl 5.18+
PostgreSQL 9.4+
Web server (e.g. nginx, Apache, lighttpd)

The web external server is only required for production installs;
for evaluation purposes a simpler setup can be used, as detailed
below.
Client
A Dojo 1.14 compatible web browser
is all that's required on the client (except IE8 and 9); it includes Chrome as
of version 13, FireFox as of 3.6 and MS Internet Explorer as of version 10 and
a wide range of mobile browsers.
Quick start (Docker compose)
The quickest way to get the Docker image up and running is by using the
docker-compose file available through the GitHub repository at:
https://github.com/ledgersmb/ledgersmb-docker/blob/1.6/docker-compose.yml
which sets up both the LedgerSMB image and a supporting database image for
production purposes (i.e. with persistent (database) data, with the
exception of one thing: setting up an Nginx or Apache reverse proxy
with TLS 1.2 support -- a requirement if you want to access your
installation over any type of network.
See the documentation on Docker Hub.
Quick start (from source)
The instructions below are for getting started quickly; the project's site
provides in-depth installation instructions
for production installs.
System (library) dependencies
The following non-Perl (system) dependencies need to be in place for the
cpanm command mentioned below to work, in addition to what's documented
on the How to install CPAN modules
page on CPAN.

cpanminus  This can be manually installed, or installed as a system package.
It may not be necessary to install cpanminus if you are only going to install from debian packages.
PostgreSQL client libraries
PostgreSQL server
DBD::Pg 3.4.2+ (so cpanm recognises that it won't need to compile it)
This package is called libdbd-pg-perl in Debian and perl-DBD-Pg
in RedHat/Fedora
make       This is used by cpan dependencies during thier build process

Then, some of the features listed below have system requirements as well:

latex-pdf-ps depends on these binaries or libraries:

latex (usually provided through a texlive package)
pdflatex
dvitopdf
dvitops
pdftops



Perl module dependencies
This section depends on a working local::lib installation
as well as an installed cpanm executable. Both should be available from
your distribution's package repository (Debian calls them liblocal-lib-perl
and cpanminus respectively). cpanm depends on the make and gcc commands being available.
NOTE: gcc can be removed after all cpan dependencies are installed.
However, it may be necessary to reinstall it if additional modules are required during an upgrade
To install the Perl module dependencies, run:
 $ cpanm --quiet --notest --with-feature=starman [other features] --installdeps .

NOTE: Don't miss the ""."" at the end of the cpanm command!
Don't forget to make sure the environment variable PERL5LIB=/home/ledgersmb/perl5/lib/perl5 points at the running user's perl5 dir
Also, NEVER run cpanm as root, it's best to run it as the user you intend to run ledgersmb as when possible.
This installs the cpan modules in ~/perl5
If you can't run it as the final user, don't worry, just run it as any user (eg: johnny),
and make sure the environment variable PERL5LIB=/home/johhny/perl5/lib/perl5 points at jonny's perl5 dir
Setting the PERL5 environment variable is normally done by editing the initscript, or systemd service file.
If you are running manually, then you will need to set and export PERL5 before running starman/plack
The following features may be selected by
specifying --with-feature=<feature>:



Feature
Description




latex-pdf-ps
Enable PDF and PostScript output


starman
Starman Perl/PSGI webserver


openoffice
OpenOffice.org document output


edi
(EXPERIMENTAL) X12 EDI support


xls
Excel output filters (xls+xlsx)



Note: The example command contains --with-feature=starman for the
purpose of the quick start.
When not installing as root or through sudo, cpanm will install unfulfilled
library dependencies into a location which can be used with local::lib.
The in-depth installation instructions
contain a list of distribution provided packages to reduce the
number of dependencies installed from CPAN.
NOTES

For the pdf-ps target, LaTeX is required.

PostgreSQL configuration
While it's possible to use LedgerSMB with the standard postgres user,
it's good practice to create a separate 'LedgerSMB database administrator':
$ sudo -u postgres createuser --no-superuser --createdb --login \
          --createrole --pwprompt lsmb_dbadmin
Enter password for new role: ****
Enter it again: ****

The pg_hba.conf file should have at least these lines in it (order of the entries matters):
local   all                            postgres                         peer
local   all                            all                              peer
host    all                            postgres        127.0.0.1/32     reject
host    all                            postgres        ::1/128          reject
host    postgres,template0,template1   lsmb_dbadmin    127.0.0.1/32     md5
host    postgres,template0,template1   lsmb_dbadmin    ::1/128          md5
host    postgres,template0,template1   all             127.0.0.1/32     reject
host    postgres,template0,template1   all             ::1/128          reject
host    all                            all             127.0.0.1/32     md5
host    all                            all             ::1/128          md5


Note: pg_hba.conf can be found in /etc/postgresql/<version>/main/ on Debian
and in /var/lib/pgsql/data/ on RedHat/Fedora

After editing the pg_hba.conf file, reload the PostgreSQL server
(or without 'sudo' by running the commands as root user):
 $ sudo service postgresql reload
 # -or-
 $ sudo /etc/init.d/postgresql reload
Configure LedgerSMB
(Installation from tarball is highly preferred over installation from GitHub for production installs.)
 $ cp doc/conf/ledgersmb.conf.default ledgersmb.conf
Running Starman
With the above steps completed, the system is ready to run the web server:

NOTE: DO NOT run starman (or any web service) as root, this is considered
a serious security issue, and as such LedgerSMB doesn't support it.
Instead, if you need to start LedgerSMB from a root process, drop
privileges to a user that doesn't have write access to the LedgerSMB Directories first.
Most daemonising mechanisms (eg: systemd) provide a mechanism to do this.
Do not use the starman --user= mechanism, it currently drops privileges too late.

 $ starman -I lib -I old/lib --listen localhost:5762 bin/ledgersmb-server.psgi
2016/05/12-02:14:57 Starman::Server (type Net::Server::PreFork) starting! pid(xxxx)
Resolved [*]:5762 to [::]:5762, IPv6
Not including resolved host [0.0.0.0] IPv4 because it will be handled by [::] IPv6
Binding to TCP port 5762 on host :: with IPv6
Setting gid to ""1000 1000 24 25 27 29 30 44 46 108 111 121 1000""
Environment Variables
All regular Perl environment variables can be used. In particular, it's important to make sure
PERL5LIB is set correctly when setting up local::lib for the first time.
We support the following Environment Variables within our code

LSMB_WORKINGDIR : Optional

Causes a chdir to the specified directory as the first thing done in starman.psgi
If not set the current dir is used.
An example would be

LSMB_WORKINGDIR='/usr/local/ledgersmb/'



We support the following Environment Variables for our dependencies

PGHOST : Optional

Specifies the Postgres server Domain Name or IP address


PGPORT : Optional

Sepcifies the Postgres server Port


PGSSLMODE : Optional

Enables SSL for the Postgres connection



All Environment Variables supported by our dependencies should be passed through to them,
that includes the standard Postgres Variables and others
Next steps
The system is installed and should be available for evaluation through

http://localhost:5762/setup.pl    # creation and privileged management of company databases
http://localhost:5762/login.pl    # Normal login for the application

The system is ready for preparation for first
use.
Project information
Web site: http://ledgersmb.org/
Live chat:

IRC: irc://irc.freenode.net/#ledgersmb
Matrix: https://vector.im/#/room/#ledgersmb:matrix.org (bridged IRC channel)

Forums: http://forums.ledgersmb.org/
Mailing list archives: http://archive.ledgersmb.org
Mailing lists:

https://lists.ledgersmb.org/mailman/listinfo/announce
https://lists.ledgersmb.org/mailman/listinfo/users
https://lists.ledgersmb.org/mailman/listinfo/devel

Repository: https://github.com/ledgersmb/LedgerSMB
Project contributors
Source code contributors can be found in the project's Git commit history
as well as in the CONTRIBUTORS file in the repository root.
Translation contributions can be found in the project's Git commit history
as well as in the Transifex project Timeline.
Copyright
Copyright (c) 2006 - 2018 The LedgerSMB Project contributors
Copyright (c) 1999 - 2006 DWS Systems Inc (under the name SQL Ledger)

License
GPLv2
",122
ultrasilicon/parsly,C++,"Parsely

Describe & deploy net protocol with simply a JSON file.


Parsely is a library which helps developer serialize runtime application data into network packets, according to the network protocol defined by the user in a JSON file.

Get Started:


Step 1: Design and define your protocol in a  json file

How proto.json is defined (example):

{
	""version"" : [0, 0, 0],
	""protocol"" : {
		""header"" : 4,
		""flags"" : [""HeartBeat"", ""ConnectionInfo"", ""TextMessage"", ""ImageMessage""]
	},

	""fields"" : {
		""HeartBeat"" : [
			[""string"", ""uuid""],
			[""string"", ""usrName""],
			[""string"", ""publicKey""],
			[""uint32"", ""timestamp""]
		],
	
		""ConnectionInfo"" : [
			[""string"", ""uuid""],
			[""string"", ""peers""]
		],
		
		""TextMessage"" : [
			[""string"", ""uuid""],
			[""string"", ""msgId""],
			[""string"", ""msg""]
		],
		
		""ImageMessage"" : [
			[""string"", ""uuid""],
			[""string"", ""msgId""],
			[""string"", ""msg""]
		]
	}
}

version : POP version.
protocol : An overview of the protocol.
header : The size of header, which packetize the TCP stream. It is the size of the whole packet except the size of it self.
flags : Flags are used for application to determine what type of message the packet is transmitting. Different type of message has different fields, defined in fields.
fields : The message fields of flags. Consisted of an order sensitive array of sub fields. So that the binary will be serialized and deserialized sequencially described here. The first element of a subfield, like in [""string"", ""uuid""], is the type of the subfield data. And the second is the same of the data, which is just a label for your and the pre-processor's reference when debugging.



STEP 2: Compile your protocol to ParseEngine:

It translates the given protocol description JSON file to C++ code, which is a part of what we called the Parse Engine

$ python3 pop.py ./my_proto_v1.json ./my_proj/src/
Pre-processor of Parsely v1.0.1 stable 
pop: Parsing /home/han/my_proto_v1.json ...
pop: Generating decoder in C++ ...
pop: Injecting compiled code to ./my_proj/src/parse_engine_decode_pop.cpp
Sucessful!
$ make -j8


STEP 3: Encapsulate your own network stack:

The user first need to inherit his/her network class from our interface called the NetStack.
Note: you need to implement virtual int NetStack::write(char* data, string &ip) so that ParseEngine can automatically call NetStack to send out  message

#include ""parsely/net_stack.h""
using namespace std;

class ChatServer : public NetStack
{
public:
	ChatServer(string &ip, const int &port);
	
	virtual int NetStack::write(char* data, string &ip);
}


STEP 4: Send message with ParseEngine:

How ParseEngine is used (example):

#include ""chat_server.h""
#include ""parse_engine.h""

enum ProtoMsgType {
    HeartBeat,
    Text,
    Image,
}

void readMessage(Packet* p){
    // Do something...
}

int main(int argc, char **argv  ){
    // User code...
    
    ChatServer  *s = new ChatServer();
    ParseEngine *e = new ParseEngine(s);
    //bind read message callback
    e->onMessage = &readMessage;
    
    // construct packet according to the protocol
    Packet *p = new Packet{ 
        {
            ""{cfceb206-290e-4b60-b596-1a08a2c8d36a}"", // UUID
            ""731948"", 								  // Message ID
            ""Hello?""								  // Message Text
        },
        Text
    };
    e->message(p, Text}, ""155.246.104.55"");
    
    // User code...
}


What's next for Parsely

We will further simplify the usage
Add packet oriented protocol support, like UDP
Merge into opensource project libParsley - Asynchronous cross-platform C++ library with delicate OOP callback system as a module.

",2
discord-csharp/CSDiscord,C#,"
",6
arsns-iscteiul/SID_2018_32,Java,"SID_2018_32
",3
taka-sho/taka-sho.github.io,Vue,"This page's status

Who is taka-sho?

Shotaro Takahara (Nickname: Takasho)
ペンペンな大学に通う普通の大学生

What award did I win?

Robot Presentation Award at First Lego League in 2014
Progate Award at Kaumo Hackathon in 2016

This page were made by ...

Languages : Typescript & Pug(Jade)
Framework : Vue.js
Bundler : Webpack
Lint : Tslint

",2
TheAlgorithms/C,C,"C
Computer Oriented Statistical Methods
- Gauss_Elimination
- Lagrange_Theorem
- Mean
- Median
- Seidal
- Simpson's_1-3rd_rule.c
- Variance
- statistic (C Lib)

Conversions
- binary_to_decimal
- decimal _to_binary
- decimal_to_hexa
- decimal_to_octal
- to_decimal

Data Structures
- stack
- queue
- dictionary
linked_list
	- singly_link_list_deletion
	- stack_using_linkedlists
binary_trees
	- create_node
	- recursive_traversals
trie
	- trie

Searching
- Binary_Search
- Other_Binary_Search
- Jump_Search

Sorting
- BinaryInsertionSort
- BubbleSort
- BogoSort
- InsertionSort
- MergeSort
- OtherBubbleSort
- QuickSort
- SelectionSort
- ShakerSort
- HeapSort

Hashing
- sdbm
- djb2
- xor8 (8 bit)
- adler_32 (32 bit)

Misc
- Binning
- Factorial
- Fibonacci
- isArmstrong
- LongestSubSequence
- palindrome
- QUARTILE
- rselect
- strongNumber
- TowerOfHanoi
- Greatest Common Divisor
- Sudoku Solver
- prime factorization

exercism
In this directory you will find (in the right order):

hello-world
isogram
acronym
word-count
rna-transcription

Simple Client Server Implementation
This directory contains

client.c
server.c

First execute server.c in a terminal and then client.c in a different terminal. Enables communication between two terminals.
",1763
TheAlgorithms/C,C,"C
Computer Oriented Statistical Methods
- Gauss_Elimination
- Lagrange_Theorem
- Mean
- Median
- Seidal
- Simpson's_1-3rd_rule.c
- Variance
- statistic (C Lib)

Conversions
- binary_to_decimal
- decimal _to_binary
- decimal_to_hexa
- decimal_to_octal
- to_decimal

Data Structures
- stack
- queue
- dictionary
linked_list
	- singly_link_list_deletion
	- stack_using_linkedlists
binary_trees
	- create_node
	- recursive_traversals
trie
	- trie

Searching
- Binary_Search
- Other_Binary_Search
- Jump_Search

Sorting
- BinaryInsertionSort
- BubbleSort
- BogoSort
- InsertionSort
- MergeSort
- OtherBubbleSort
- QuickSort
- SelectionSort
- ShakerSort
- HeapSort

Hashing
- sdbm
- djb2
- xor8 (8 bit)
- adler_32 (32 bit)

Misc
- Binning
- Factorial
- Fibonacci
- isArmstrong
- LongestSubSequence
- palindrome
- QUARTILE
- rselect
- strongNumber
- TowerOfHanoi
- Greatest Common Divisor
- Sudoku Solver
- prime factorization

exercism
In this directory you will find (in the right order):

hello-world
isogram
acronym
word-count
rna-transcription

Simple Client Server Implementation
This directory contains

client.c
server.c

First execute server.c in a terminal and then client.c in a different terminal. Enables communication between two terminals.
",1763
docker-library/repo-info,Perl,"Official Images ""Extended Information""
This repository includes a set of scripts for generating reports of extended information about published official image repositories.
It's still a firm Work In Progress, and concrete suggestions for improvement in gathering or presentation are welcome!


Automated update-remote.sh:

Automated scan-local.sh (one job per repo)

",169
liqiongfan/xserver,C,"Tiny & Effective Server: Xserver
Xserver是什么？
Xserver是一个完全采用C语言编写的多线程、并发型、模块化的服务器程序，支持Linux系统环境，优先采用epoll多路复用机制，具体的设计体系架构如下图所示：

Xserver的体系架构简单、性能把控度100%，因为采用C语言开发因此能够将机器的性能压榨到极致，比较适合推送服务、消息IM系统等并发场景比较高的系统使用。
扩展开发手册
扩展开发可以使用C语言或者C++语言开发，生成so文件即可，暂未处理BSD系统的kqueue机制，后续增加后支持dylib扩展，具体的开发方法如下：
Xserver默认情况下自动解析http请求协议，按照标准格式进行解析，解析的结果会通过参数返回给扩展库的回调函数的参数中，扩展方法的原型如下：
void (HTTP_FUNC)(list *request_headers, list *query_string_list);

遍历 Xserver 自带的 list 结构可以使用 Xserver宏 遍历即可：
其中 request_headers 列表中含有几个固定的键值：



字段
含义
取值




request_method
请求方法名称
GET|PUT|POST|DELETE 等等


request_uri
请求URL
除去host之后的路径取值


http_version
HTTP协议版本
HTTP/1.1


http_body
请求的包主体
GET请求无包体



list *_temp = EMPTY_PTR;
LIST_FOREACH_VAL(request_headers, _temp) {
    
    /* list_data结构体包含一个name字段：键名
     * value:键值 */
    list_data *_kv = (list_data *)_temp->node.data_ptr;
    
    printf(""%s:%s\t"", _kv->name, _kv->value);
    
} LIST_FOREACH_END();

不要担心内存泄漏问题，Xserver会自动在调用库方法返回后自动释放内存。
压力测试的目标机器配置：
16GB内存，4核i5-4460 3.2GHz的Intel CPU

压力测试命令与测试结果：
ab -n100000 -c1000 -r -k http://localhost:8181/

josin@josin-PC:~$ ab -n100000 -c1000 -r -k http://localhost:8181/
This is ApacheBench, Version 2.3 <$Revision: 1826891 $>
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/

Benchmarking localhost (be patient)
Completed 10000 requests
Completed 20000 requests
Completed 30000 requests
Completed 40000 requests
Completed 50000 requests
Completed 60000 requests
Completed 70000 requests
Completed 80000 requests
Completed 90000 requests
Completed 100000 requests
Finished 100000 requests


Server Software:
Server Hostname:        localhost
Server Port:            8181

Document Path:          /
Document Length:        26 bytes

Concurrency Level:      1000
Time taken for tests:   4.206 seconds
Complete requests:      100000
Failed requests:        0
Keep-Alive requests:    0
Total transferred:      12800000 bytes
HTML transferred:       2600000 bytes
Requests per second:    23777.23 [#/sec] (mean)
Time per request:       42.057 [ms] (mean)
Time per request:       0.042 [ms] (mean, across all concurrent requests)
Transfer rate:          2972.15 [Kbytes/sec] received

Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:        0   20  92.4     11    1040
Processing:     5   22   8.4     20      68
Waiting:        0   17   7.8     16      67
Total:          7   42  93.3     33    1084

Percentage of the requests served within a certain time (ms)
  50%     33
  66%     39
  75%     42
  80%     44
  90%     47
  95%     50
  98%     55
  99%     73
 100%   1084 (longest request)

",6
RyanLinrm/CSC473Project,JavaScript,"CSC473 Project
Strategy Arena Game


Team members: Nabhan Maswood, Hongjie Huang, Adam McKoy, Michael Richards, Runmin Lin


Scrum master name： Runmin Lin


Product owner name：Hongjie Huang


Documentation

https://ryanlinrm.github.io/CSC473Project/

Copyright Disclaimer

We do NOT own the sprite sheets/assets,sound/music files nor the images we used throughout the development of the project.
This project and the website that we deployed are only for study purposes for CSC473 class in The City College of New York.
All rights belong to it's rightful owner/owners.
No copyright infringement intended. Any files that violated the rights of the owner/owners wll be removed if needed.

",3
spec-journalism/everything,None,"Everything
A list of all our data analysis and interactives.
Table of Contents

Data Stories
Interactive Feautres
Data Analysis
Tool Ideas


Data Stories



Article
Repo
Description
Developer(s)




🔗
graduate-diversity
Code for the Graduate Student Sex Diversity lead.
Jason


🔗
net-price-inequity
Code for the Net Price Inequity article.
Jason



Interactive Features



Article
Repo
Description
Developer(s)




🔗
uptown-arts
Code for the Uptown Arts feature.
Jason and Raeedah


🔗
genealogy
Code for the Eastern Genealogical Records feature.
Jason



Data Analysis



Article
Repo
Description
Contact





visits-who
Data and analysis of Columbia Undergraduate Admissions events scraping project.
Jason



staff-diversity
Measuring diversity within the Columbia Daily Spectator and comparing staff diversity against other sections and top college newspapers.
Raeedah


🔗
gss-diversity
Data and analysis of Columbia graduate student diversity.
Jason


🔗
homeless-center
Data and analysis contextualizing homeless services at  Columbia.
Raeedah


🔗
turkey
Data, analysis, and code for Turkey's panel cancellation article.
Jason


🔗
international-students
Data analysis and graphics supporting the International Students lead.
Jason


🔗
financial-aid
Data analysis underlying the original analysis reported in the net price inequity article.
Jason


🔗
foreign-gifts
Analyzing monetary gifts from foreign entities to Columbia University.
Jason



Tool Ideas

Easy conversion of Google Docs text to code

Retains AML format but converts paragraphs, styles, and links to HTML
CLI to easily convert a Google Doc with an AML section into a scrollytelling piece (with different templates??!!)


Starter CLI to easily access all templaltes (see POLITICO, The Pudding)
Standard style template for interactive projects (like The Pudding starter for web pages)

",3
streamich/md-mdast,TypeScript,"md-mdast


Markdown to MDAST converter.
Small and fast.
No dependencies.

Installation
npm install md-mdast
Usage
const {create} = require('md-mdast');

const parser = create();

console.log(parser.tokenizeBlock('*hello* __world__'));
Result:
{ type: 'root',
  children:
   [ { type: 'paragraph',
       children:
        [ { type: 'emphasis',
            children: [ { type: 'text', value: 'hello' } ] },
          { type: 'text', value: ' ' },
          { type: 'strong',
            children: [ { type: 'text', value: 'world' } ] } ] } ],
}

License
Unlicense — public domain.
",5
gltd/gltd-td,Python,"gltd-td
touchdesigner files
",2
ermiry/Blackrock,C,"
About
Current Features
Upcoming Features
Installing
Contributing
About
Blackrock is currently a dungeon exploration rouguelike heavily inspired by the great Nethack, nut it is intended to become a much complex game featuring multiplayer and some bigger maps to explore with your friends.
Blackrock first started as a hobby program just to test my skills working with C, but as time went one, it became one of my biggest projects and I have to say that I have had a lot of fun working on it and also I have learned a lot.
This game is also intended to serve as a much more complex tutorial for anyone that would like to sharpen his or her skills in C. It includes many topics that I have had a hard time finding out how to implement them in C such as a database using Sqlite or some object pooling and an ECS. In the future it will feature a full wiki for any topic related to the game and its development. For now the code is heavily commented and I will try to keep that way trough its development. If you have any question please contact me.
Current Features

ECS  (Entity Component System)
Object Pooling
Simple dungeon generation
Ascii characters
Simple random enemy spawner
Loot and currency
Weapons and armor
Some items
Shops
Simple inventory system
Simple UI
Only one class -> warrior
Only one race -> human
Leaderboards
Support for Linux

Instalation
Linux
Build it yourself -> You need to install the following dependencies first:

Sdl 2.0 - libsdl2-dev
Sqlite3 - libsqlite3-dev
Pthread - libpthread-stubs0-dev

If you are using Ubuntu, you can run the following commands:
sudo apt-get install libsdl2-dev
sudo apt-get install sqlite3 libsqlite3-dev
sudo apt-get install libpthread-stubs0-dev
Note: You also need to have installed:

Make

Compiling -> Just use 'make' in the project folder.
Running ->  Use 'make run'.
Contributing
Anyone can make a pull request and I will try to check it, and if it seems fine with the flow of the game, I will include it on the project.
",3
nytimes/encoding-wrapper,Go,"encoding-wrapper



Collection of Go wrappers for Video encoding providers.
Supported providers

Elemental Conductor
Encoding.com

",91
NanoAdblocker/NanoCore2,JavaScript,"Nano Core 2
Restarting fresh
A patch driven fork of uBlock Origin.
Upstream commit pin: 07cbae66a475a8b0cf0a93cfbff0ac63c3bfa8b1 or 1.19.2
Please submit issues to the
Nano Core issues tracker.
Localization
Nano Core 2 has some extra locale strings, most of them are for Nano Linter.
You can contribute to localization here:
https://crowdin.com/project/nano-adblocker
Your language is not listed? Submit an issue to let me know.
Setup

Install latest version of Git and Node.js.
In an appropriate directory, run:

git clone --depth 1 https://github.com/NanoAdblocker/NanoCore2.git
git clone --depth 1 https://github.com/NanoAdblocker/NanoFilters.git
git clone --depth 1 https://github.com/jspenguin2017/Edgyfy.git
git clone --depth 1 https://github.com/jspenguin2017/uBlockProtector


Run git clone --depth 1 https://github.com/gorhill/uBlock.git in an
appropriate directory to get upstream. Check out a tag or commit as
appropriate.
In Nano Core 2 repository:

Navigate to /term directory and run npm install.
Update /config.json as appropriate.
Run node ./term to open Nano Core 2 Terminal.



Note: The private repository Prototype is required for publishing.
Development
You should not modify the upstream repository, instead, use Nano Core 2
Terminal to create and manage development environment.
reset    Nuke development environment and create a fresh one without any
         patches applied
sync     Resynchronize all existing patches, in order
apply    Apply all existing patches, in order
mark     Create a patch based on the difference between tip of applied patches
         and current code in development environment

make     Build extension from development environment
pack     Build, test, then package the extension
publish  Build, test, package, then publish the extension
         Optionally pass browser name to publish for only one browser
clean    Remove all build files

lmake    Build English locale file from locale definition
lsync    Synchronize (non-English) locale files with the latest build of the
         Crowdin project
         This will not rebuild the Crowdin project even if there are changes

config   Print active configuration data
reload   Reload configuration data
exit     Exit the terminal

Version Update Checklist

Pull filters updates
If needed, pull upstream updates then update about string and commit pin
If needed, pull upstream updates to build scripts
If needed, pull upstream updates to assets.json and manifest.json
If needed, pull upstream updates to 1p-filters.js and asset-viewer.js
If needed, build and pull locale updates
Bump version number
Test to make sure everything is working
Add tags to repositories, and upload packages
Upload packages to extension stores

Version Update Watchlist
These are the potential problems to look out for:

Font Awesome related CSS changes

Subresource Integrity Incidence Response Protocol
When there is a severe issue with the content of a subresource, an intervention
can be easily placed with the help of
UltimateMirror and
MirrorEngine:

Lock the subresource in UltimateMirror
Remove problematic content from UltimateMirror, if needed
Update assets.json so the sanitized subresource will be loaded from
UltimateMirror

Later, the intervention can be lifted as follows:

Update assets.json as appropriate, the subresource should no longer be
loaded from UltimateMirror
Wait a bit for cache to expire (takes about 10 minutes), then restart
MirrorEngine
Unlock the subresource in UltimateMirror

Tips
Do not forget to copy and commit generated patches from mark to make them
permanent. Also adjust the configuration file as appropriate.
To update a patch, simply paste further changes to the bottom of that patch and
sync them in. If something breaks, try disabling patches after that patch
temporarily.
To fix conflict, either edit the broken patch file manually or disable it
along with patches after it then recreate the changes.
",73
ninthDevilHAUNSTER/ArknightsAutoHelper,Python,"shaobao_adb

明日方舟辅助脚本，当然只是开发阶段，只是刚刚上线我就很气！

ADBShell
基于夜神模拟器集成了多种安卓模拟器操作方法，可以进行安卓辅助开发。当然你要在电脑端跑
运行须知
config.py
ADB_ROOT = r""D:\Program Files\Nox\bin""
ADB_HOST = ""127.0.0.1:62001""
SCREEN_SHOOT_SAVE_PATH = ""D:\\python_box\\shaobao_adb\\screen_shoot\\""
STORAGE_PATH = ""D:\\python_box\\shaobao_adb\\storage\\""

# arknights INFO
ArkNights_PACKAGE_NAME = ""com.hypergryph.arknights""
ArkNights_ACTIVITY_NAME = ""com.u8.sdk.U8UnityContext""
如想要二次开发，请修改config.py下的相关参数。以绝对路径为佳
依赖包
# python 版本 3.6 + 
Package    Version
---------- --------
certifi    2019.3.9
chardet    3.0.4
idna       2.8
Pillow     6.0.0
pip        10.0.1
requests   2.21.0
setuptools 39.1.0
soupsieve  1.9.1
tesserocr  2.4.0
urllib3    1.24.2
目前支持的功能

ADB 指令
点击动作
拖动动作
截图动作
获取子图
子图与目标子图比较

",2
Harlekuin/SimQLe,Python,"SimQLe

The simple way to SQL



Perfect for no fuss SQL in your Python projects. Execute SQL and return simple
Recordsets. Manage several connections, and be certain that your production
databases aren't touched in your Integration Tests. Also, named parameters
across the board.
Installation
Repository
https://github.com/Harlekuin/SimQLe
Or choose your poison:

$ pip install simqle
$ poetry add simqle
$ pipenv install simqle

Once installed, requires a .connections.yaml file in the root of the project
that defines the connection strings the project should use. See the Connection
String section for syntax.
Usage
In Production
Get a result from the name of your connection, the SQL statement, and a dict
of parameters:
from simqle import recordset, load_connections

load_connections()

sql = ""SELECT name, age FROM people WHERE category = :category""
params = {""category"": 5}
result = recordset(con_name=""my-database"", sql=sql, params=params)
recordset() returns a tuple of (Data, Headings). ""Data"" is a list of row tuples.
Headings is a list of field names from the query.
In Integration Tests
Before running integration tests, set the SIMQLE_TEST environment variable
to True. This will cause the load_connections function to load the
test-connections (which should mirror the connections in terms of name and
type of database), and will cause the code in your project to run exactly the
same, but instead connect to your defined test connections instead.
The .connections.yaml File
Define the connection strings for production and test servers. The names of the test-connections should mirror the connections names. The file .connections.yaml should be in the root of your project. Each connection will be referred to by its name.
Example file:
connections:
    # The name of the connection - this is what will be used in your project
    # to reference this connection.
  - name: my-sql-server-database
    driver: mssql+pyodbc:///?odbc_connect=
    connection: DRIVER={SQL Server};UID=<username>;PWD=<password>;SERVER=<my-server>
    # some odbc connections require urls to be escaped, this is managed by
    # setting url_escaped = true:
    url_escape: true

    # File based databases like sqlite are slightly different - the driver
    # is very simple.
  - name: my-sqlite-database
    driver: sqlite:///
    # put a leading '/' before the connection for an absolute path, or omit
    # if it's relative to the project path
    connection: databases/my-database.db


test-connections:
    # the names of the test-connections should mirror the connections above.
  - name: my-sql-server-database
    driver: mssql+pyodbc:///?odbc_connect=
    # connecting to a different server here
    connection: DRIVER={SQL Server};UID=<username>;PWD=<password>;SERVER=<my-test-server>
    url_escape: true    

  - name: my-sqlite-database
    driver: sqlite:///
    connection: /tmp/my-test-database.db  # note the absolute path syntax
Author
Tom Malkin - tommalkin28@gmail.com
Release History

0.1.0

Add the basic skeleton of the project


0.1.1

Unit tests
Integration tests for sqlite added.
100% coverage


0.2.0

Added url_escape option in connections.yaml file
Integration tests added for mysql and postgresql



Road Map

all available relational databases tested.
scripts for easy project setup.
pypi upload.
default location connection file

",22
scottwillson/racing_on_rails,Ruby,"Racing on Rails is a bike racing association schedule, results, competitions, and membership database and website.
Open source (MIT license)
Ruby on Rails
http://rubyonrails.org/
Documentation, quick start
http://racingonrails.rocketsurgeryllc.com/
Google Group
http://groups.google.com/group/racing-on-rails
Source Code
https://github.com/scottwillson/racing_on_rails
RDoc
http://racingonrails.rocketsurgeryllc.com/rdoc
Issues
https://github.com/scottwillson/racing_on_rails/issues
Parallel tests
RECORD_RUNTIME=true DISABLE_SPRING=1 rake parallel:test[^test/{controllers,helpers,integration,lib,mailers,models,views}]
Requirements
Ruby 2.4
MySQL 5.6



",37
FyuriStudios/SFG,JavaScript,"Struggle for Gera
This is an original digital card game, currently in the very early pre-alpha development stages.
The SFG team consists of a group of friends who love to argue about what makes Hearthstone a bad game, or why Gwent sucks. When we realized that we could do something productive with our arguments like make our own game, we blew our own minds. So here we are. Here is the link to the official rules document. If the rules don't make any sense, sorry: We wrote them, so maybe it only makes sense to us.
The code
The frontend code is located here. The backend code is located here. Tests are located here.
The backend code is meant to be run using NodeJS on a server. It serves up a webpage (which is mostly just an embedded JS script, lol.) /static contains everything regarding the webpage.
",2
open-learning-exchange/open-learning-exchange.github.io,HTML,"What is this?
See what this is all about: http://open-learning-exchange.github.io/.
How to Contribute
First things first
Learn Markdown! Then check out MDwiki's quick start.
Whenever you feel stuck, go to MDwiki's own site for further information.
One Wiki Only? Fork It.
First off fork this repo and call it something like <MyProject>-wiki.
Multiple Wikis? Clone It.
In case would you like to create more than one wiki for the same GitHub user or organization, then forking won't cut it. At the moment of this writing GitHub won't allow you to fork a single repository multiple times for the same account.
There's a workaround to this, however on GitHub's web interface you won't see the sign that says ""forked from"" in your cloned repository.
Below instructions how to clone this repository using the CLI:
First off, create a new (empty) repository on GitHub, then;
git clone https://github.com/exalted/mdwiki-seed.git
cd mdwiki-seed
git remote add foobar <HTTPS/SSH Clone URL of the New Repository>
git push foobar gh-pages
Create a New Wiki
It all begins by creating an initial file structure for any language that you would like to support. For example, if you're interested having a wiki in English, then you will duplicate ll_CC folder and rename your copy to en. (For a complete list of languages [ll] and country codes [CC] see here.)
ll_CC is a starter template folder which you shouldn't ever edit directly, since you may loose your changes when MDwiki gets updated later.

If you want to have your wiki in more than one language, then you duplicate ll_CC as many times as necessary and rename each copy with the appropriate language and country code. (Country code is optional and it is only useful if you want to distinguish, for instance, American English from British English, such as: en_US and en_GB.)

Getting Started
You should have a language wiki folder by now, if not, go back and read above to create one.
Suppose your first wiki is going to be in English, hence you must have a folder called en, as previously described.

Open index.html file with your favorite plain text editor (the one that is at the same level where ll_CC and your language folder is located, NOT the one inside your language folder).
Find where it says ""Override ll_CC below with your default language and country code.""
Change refresh meta tag from url=ll_CC/ to url=en/ (trailing / is very important).

Structure
All file references here are relative to their respective language folder.



Name
Type
Description




index.md
File
Starting point (a.k.a. ""home page"") for your wiki. Note this is not the index.html, but index.md!


navigation.md
File
Various settings of your wiki (e.g., name of your wiki, items in the navigation bar at the top, etc.)


config.json
File
If you don't know what this is for, don’t touch it.


pages
Folder
Ideally, inside this folder, you create one *.md file for every page inside your wiki (e.g., foo.md, much-longer-names-are-also-okay.md, etc.) You can also create as many subfolders as you need, just remember to link them accordingly.


uploads
Folder
An example folder structure where you could put other files. Although it is best to host your files somewhere else, like Dropbox, or a CDN, etc.



Best Practices
Relative URLs
Instead of using absolute URLs when linking one wiki page to another, use relative URLs.
For instance if en/pages/foo.md page had to link to en/pages/bar.md, it is enough to just add [Click here](bar.md) in your markdown.
Don't Host Your Uploads in Git(Hub)
Instead of hosting your uploads inside the uploads folder, consider using Dropbox, Google Drive, or a CDN.
Add References to Uploads
Whenever you can, avoid hosting your uploads using Git(Hub).
If you must add references to files hosted inside the uploads folder here's how to do it, for instance: ![Image Title](uploads/images/foo.png). Add that in your markdown and you're good to go.
How to Preview
In order to preview your changes locally, prior to publishing online, you may need to take some actions. Below some starting points for each operating system, also check out MDwiki's frequently asked questions section for some ideas.
Mac OS
The easiest way to serve up static sites on a Mac is to use Anvil. Go ahead and download it from their website, install and add a site using the status bar icon: simply select the folder where your wiki is located on your Mac.
If you don't want to download any apps, you can use the Terminal on your Mac and Python. To do this, go to your site folder in the Terminal app and type python -m SimpleHTTPServer 8000. This will start running a local server which you can navigate to by typing the URL localhost:8000 in any web browser.
Windows
Text editors like Brackets, and VS Code and Atom (with extensions) can be used to preview your files locally. However, it is recommended that you use the online GitHub editor and RawGit to make and view your changes in your own branch of your fork of this repository because this is the only sure way to preview your changes for live accuracy.
Linux
You can use Prax. Prax is a pure ruby alternative to Pow!! that runs on GNU/Linux.
For Developers
You don't need to read below here or do anything at all if you're only interested creating your own wiki. This section is for developers or maintainers of this repository.
Make changes to MDwiki

If you haven't already, install Node.
Open a command prompt/shell/git bash and navigate to your repo's directory.
Run npm install to install mdwiki dependencies.
For development, you can run ./node_modules/.bin/grunt devel or grunt devel if you have grunt installed globally.  This will start grunt watching the index.tmpl and *.js files for changes, which you can view at localhost:35729. The index.html file that it builds is the debug version of the html with the full js files.
As a note, the index.tmpl is where our custom CSS can be found.  Please only add CSS within the comment denoted section.
Once you have completed your changes, run ./node_modules/.bin/grunt release to build an index.html with minified js.

",28
apache/flink,Java,"Apache Flink
Apache Flink is an open source stream processing framework with powerful stream- and batch-processing capabilities.
Learn more about Flink at http://flink.apache.org/
Features


A streaming-first runtime that supports both batch processing and data streaming programs


Elegant and fluent APIs in Java and Scala


A runtime that supports very high throughput and low event latency at the same time


Support for event time and out-of-order processing in the DataStream API, based on the Dataflow Model


Flexible windowing (time, count, sessions, custom triggers) across different time semantics (event time, processing time)


Fault-tolerance with exactly-once processing guarantees


Natural back-pressure in streaming programs


Libraries for Graph processing (batch), Machine Learning (batch), and Complex Event Processing (streaming)


Built-in support for iterative programs (BSP) in the DataSet (batch) API


Custom memory management for efficient and robust switching between in-memory and out-of-core data processing algorithms


Compatibility layers for Apache Hadoop MapReduce


Integration with YARN, HDFS, HBase, and other components of the Apache Hadoop ecosystem


Streaming Example
case class WordWithCount(word: String, count: Long)

val text = env.socketTextStream(host, port, '\n')

val windowCounts = text.flatMap { w => w.split(""\\s"") }
  .map { w => WordWithCount(w, 1) }
  .keyBy(""word"")
  .timeWindow(Time.seconds(5))
  .sum(""count"")

windowCounts.print()
Batch Example
case class WordWithCount(word: String, count: Long)

val text = env.readTextFile(path)

val counts = text.flatMap { w => w.split(""\\s"") }
  .map { w => WordWithCount(w, 1) }
  .groupBy(""word"")
  .sum(""count"")

counts.writeAsCsv(outputPath)
Building Apache Flink from Source
Prerequisites for building Flink:

Unix-like environment (we use Linux, Mac OS X, Cygwin)
git
Maven (we recommend version 3.2.5)
Java 8 (Java 9 and 10 are not yet supported)

git clone https://github.com/apache/flink.git
cd flink
mvn clean package -DskipTests # this will take up to 10 minutes

Flink is now installed in build-target
NOTE: Maven 3.3.x can build Flink, but will not properly shade away certain dependencies. Maven 3.0.3 creates the libraries properly.
To build unit tests with Java 8, use Java 8u51 or above to prevent failures in unit tests that use the PowerMock runner.
Developing Flink
The Flink committers use IntelliJ IDEA to develop the Flink codebase.
We recommend IntelliJ IDEA for developing projects that involve Scala code.
Minimal requirements for an IDE are:

Support for Java and Scala (also mixed projects)
Support for Maven with Java and Scala

IntelliJ IDEA
The IntelliJ IDE supports Maven out of the box and offers a plugin for Scala development.

IntelliJ download: https://www.jetbrains.com/idea/
IntelliJ Scala Plugin: http://plugins.jetbrains.com/plugin/?id=1347

Check out our Setting up IntelliJ guide for details.
Eclipse Scala IDE
NOTE: From our experience, this setup does not work with Flink
due to deficiencies of the old Eclipse version bundled with Scala IDE 3.0.3 or
due to version incompatibilities with the bundled Scala version in Scala IDE 4.4.1.
We recommend to use IntelliJ instead (see above)
Support
Don’t hesitate to ask!
Contact the developers and community on the mailing lists if you need any help.
Open an issue if you found a bug in Flink.
Documentation
The documentation of Apache Flink is located on the website: http://flink.apache.org
or in the docs/ directory of the source code.
Fork and Contribute
This is an active open-source project. We are always open to people who want to use the system or contribute to it.
Contact us if you are looking for implementation tasks that fit your skills.
This article describes how to contribute to Apache Flink.
About
Apache Flink is an open source project of The Apache Software Foundation (ASF).
The Apache Flink project originated from the Stratosphere research project.
",8628
swdotcom/swdc-vscode,TypeScript,"  
Code Time for Visual Studio Code

Programming metrics right in VS Code.




Power up your development
In-editor dashboard
Get daily and weekly reports of your programming activity right in your code editor.
Status bar metrics
After installing our plugin, your status bar will show real-time metrics about time coded per day.
Weekly email reports
Get a weekly report delivered right to your email inbox.
Data visualizations
Go to our web app to get simple data visualizations, such as a rolling heatmap of your best programming times by hour of the day.
Calendar integration
Integrate with Google Calendar to automatically set calendar events to protect your best programming times from meetings and interrupts.
More stats
See your best music for coding and the speed, frequency, and top files across your commits.
Why you should try it out

Automatic time reports by project
See what time you code your best—find your “flow”
Defend your best code times against meetings and interrupts
Find out what you can learn from your data

It’s safe, secure, and free
We never access your code
We do not process, send, or store your proprietary code. We only provide metrics about programming, and we make it easy to see the data we collect.
Your data is private
We will never share your individually identifiable data with your boss. In the future, we will roll up data into groups and teams but we will keep your data anonymized.
Free for you, forever
We provide 90 days of data history for free, forever. In the future, we will provide premium plans for advanced features and historical data access.
Getting started


Install the Code Time plugin from the Visual Studio Code Marketplace.


After installing Code Time, an alert will appear prompting you to login (you can also click on ""Code Time"" in the status bar of Visual Studio Code.


",51
LFDLFoundation/lfdl-landscape,None," 
Linux Foundation Deep Learning (LFDL) Landscape


LFDL Landscape

Current Version
Interactive Version
New Entries
Logos
Proper SVGs
Corrections
External Data
Best Practices Badge
Non-Updated Items
License
Formats
Installation
Vulnerability reporting
Adjusting the Landscape View



This landscape is intended as a map to explore open source artificial intelligence (AI), machine learning (ML), and deep learning (DL) projects, and also shows the member companies of the LF Deep Learning Foundation. It is modelled after the Cloud Native Computing Foundation (CNCF) landscape and based on the same open source code.
Current Version

Interactive Version
Please see landscape.lfdl.io.
New Entries

Projects must be open source and hosted on or mirrored to GitHub.
AI, ML, and DL projects with at least 300 GitHub stars that clearly fit in an existing category are generally included. Put the project in the single category where it best fits.
We are unlikely to create a new category for projects as we'd rather find the best home with the current options.
Your project or company needs a logo and the logo needs to include the name.
Crunchbase organization should be the company or organization that controls the software. That is normally the owner of the trademark, whether or not a trademark has been formally filed.

If you think your project should be included, please open a pull request to add it to landscape.yml. For the logo, you can either upload an SVG to the hosted_logos directory or put a URL as the value, and it will be fetched.
Netlify will generate a staging server for you to preview your updates. Please check that the logo and information appear correctly and then add LGTM to the pull request confirming your review and requesting a merge.
Logos
The following rules will produce the most readable and attractive logos:

We require SVGs, as they are smaller, display correctly at any scale, and work on all modern browsers. If you only have the logo in another vector format (like AI or EPS), please open an issue and we'll convert it to an SVG for you, or you can often do it yourself at https://cloudconvert.com/. Note that you may need to zip your file to attach it to a GitHub issue. Please note that we require pure SVGs and will reject SVGs that contain embedded PNGs since they have the same problems of being bigger and not scaling seamlessly. We also require that SVGs convert fonts to outlines so that they will render correctly whether or not a font is installed. See Proper SVGs below.
When multiple variants exist, use stacked (not horizontal) logos. For example, we use the second column (stacked), not the first (horizontal), of CNCF project logos.
Don't use reversed logos (i.e., with a non-white, non-transparent background color). If you only have a reversed logo, create an issue with it attached and we'll produce a non-reversed version for you.
Logos must include the company, product or project name in English. It's fine to also include words from another language. If you don't have a version of your logo with the name in it, please open an issue and we'll create one for you (and please specify the font).
Match the item name to the English words in the logos. So an Acme Rocket logo that shows ""Rocket"" should have product name ""Rocket"", while if the logo shows ""Acme Rocket"", the product name should be ""Acme Rocket"". Otherwise, logos looks out of place when you sort alphabetically.
Google images is often the best way to find a good version of the logo (but ensure it's the up-to-date version). Search for grpc logo filetype:svg but substitute your project or product name for grpc.
You can either upload an SVG to the hosted_logos directory or put a URL as the value, and it will be fetched.

Proper SVGs
SVGs need to not rely on external fonts so that they will render correctly in any web browser, whether or not the correct fonts are installed. If you have the original AI file, here are the steps in Illustrator to create a proper SVG:

Open file in Illustrator
Select all text
With the text selected, go to Object > Expand in the top menu
Export file by going to File > Export > Export As in top menu
Select SVG from the format drop down and make sure that ""Use Artboards"" is checked
This will open a SVG options box, make sure to set Decimal to 5 (that is the highest possible, so to ensure that sufficient detail is preserved)
Click Okay to export

Corrections
Please open a pull request with edits to landscape.yml. The file processed_landscape.yml is generated and so should never be edited directly.
If the error is with data from Crunchbase you should open an account there and edit the data. If you don't like a project description, edit it in GitHub. If your project isn't showing the license correctly, you may need to paste the unmodified text of the license into a LICENSE file at the root of your project in GitHub, in order for GitHub to serve the license information correctly.
External Data
The canonical source for all data is landscape.yml. Once a day, we download data for projects and companies from the following sources:

Project info from GitHub
Funding info from Crunchbase
Market cap data from Yahoo Finance
CII Best Practices Badge data

The update server enhances the source data with the fetched data and saves the result in processed_landscape.yml. The app loads a JSON representation of processed_landscape.yml to display data.
Best Practices Badge
As explained at https://bestpractices.coreinfrastructure.org/:

The Linux Foundation (LF) Core Infrastructure Initiative (CII) Best Practices badge is a way for Free/Libre and Open Source Software (FLOSS) projects to show that they follow best practices. Projects can voluntarily self-certify, at no cost, by using this web application to explain how they follow each best practice. The CII Best Practices Badge is inspired by the many badges available to projects on GitHub. Consumers of the badge can quickly assess which FLOSS projects are following best practices and as a result are more likely to produce higher-quality secure software.

The interactive landscape displays the status (or non-existence) of a badge for each open-source project. There's also a feature not available through the filter bar to see all items with and without badges. Note that a passing badge is a requirement for projects to graduate in the CNCF.
Non-Updated Items
We generally remove open source projects that have not had a commit in over 3 months. Note that for projects not hosted on GitHub, we need them to mirror to GitHub to fetch updates, and we try to work with projects when their mirrors are broken. Here is view of projects sorted by last update: https://landscape.lfdl.io/grouping=no&license=open-source&sort=latest-commit
We generally remove closed source products when they have not tweeted in over 3 months. This doesn't apply to Chinese companies without Twitter accounts, since Twitter is blocked there. Here is a view of products sorted by last tweet: https://landscape.lfdl.io/grouping=no&license=not-open-source&sort=latest-tweet
Items that have been removed can apply to be re-added using the regular New Entries criteria above.
License
This repository contains data received from Crunchbase. This data is not licensed pursuant to the Apache License. It is subject to Crunchbase’s Data Access Terms, available at https://data.crunchbase.com/v3.1/docs/terms, and is only permitted to be used with this Landscape Project which is hosted by the Linux Foundation.
Everything else is under the Apache License, Version 2.0, except for project and product logos, which are generally copyrighted by the company that created them, and are simply cached here for reliability. The trail map, static landscape, serverless landscape, and landscape.yml file are alternatively available under the Creative Commons Attribution 4.0 license.
Formats
The LFDL Landscape is available in these formats:

PNG
PDF

Installation
You can install and run locally with the install directions. It's not necessary to install locally if you just want to edit landscape.yml. You can do so via the GitHub web interface.
Vulnerability reporting
Please open an issue or, for sensitive information, email info@cncf.io.
Adjusting the Landscape View
The file src/components/MainContent2.js describes the key elements of a
landscape big picture. It specifies where to put these sections: App Definition
and Development, Orchesteration & Management, Runtime,  Provisioning, Cloud,
Platform, Observability and Analyzis, Special. Also it specifies where to
locate the link to the serverless preview and an info with a QR code.
All these elements should have top, left, width and height properties to
position them. rows and cols specify how much columns or rows we expect in a
given horizontal or vertical section.
When we see that those elements can not fit the sections, we need to either increase
the width of all the horizontal sections, or increase height and amount of rows
in a single horitzontal section and adjust the position of sections below.
Beside that, we have to adjust the width of a parent div (1620), the width in a
src/components/BigPicture/FullscreenLandscape.js (1640) and the width in a
tools/renderLandscape.js (6560, because of x4 zoom and margins)
Sometimes the total height is changed too, then we need to adjust the height the
same way as we adjust the width.
We have an experimental fitWidth property, it is good when you want to get rid of
an extra space on the right of a section.
The best way to test that layout is ok, is to visit /landscape, and if it looks ok, run PORT=3000 babel-node tools/renderLandscape and see the rendered png files, they are in src/images folder.
",40
libretro/RetroArch,C,"

RetroArch
RetroArch is the reference frontend for the libretro API.
Popular examples of implementations for this API includes video game system emulators and game engines as well as
more generalized 3D programs.
These programs are instantiated as dynamic libraries. We refer to these as ""libretro cores"".



libretro
libretro is an API that exposes generic audio/video/input callbacks.
A frontend for libretro (such as RetroArch) handles video output, audio output, input and application lifecycle.
A libretro core written in portable C or C++ can run seamlessly on many platforms with very little to no porting effort.
While RetroArch is the reference frontend for libretro, several other projects have used the libretro
interface to include support for emulators and/or game engines. libretro is completely open and free for anyone to use.
libretro API header
Binaries
Latest binaries are currently hosted on the buildbot.
Support
To reach developers, either make an issue here on GitHub, make a thread on the forum, chat on discord, or visit our IRC channel: #retroarch @ irc.freenode.org.
Documentation
See our Documentation Center. On Unix, man-pages are provided.
More developer-centric stuff is found here.
Related projects

Cg/HLSL shaders: common-shaders
slang shaders: slang-shaders
GLSL shaders: glsl-shaders
Helper scripts to build libretro implementations: libretro-super
GitHub mirrors of projects, useful for generating diff files: libretro-mirrors

Philosophy
RetroArch attempts to be small and lean
while still having all the useful core features expected from an emulator.
It is designed to be very portable and features a gamepad-centric and touchscreen UI.
It also has a full-featured command-line interface.
In some areas, RetroArch goes beyond and emphasizes on not-so-common technical features such as multi-pass shader support,
real-time rewind (Braid-style), video recording (using FFmpeg), run-ahead input latency removal, etc.
RetroArch also emphasizes being easy to integrate into various launcher frontends.
Platforms
RetroArch has been ported to the following platforms:

DOS
Windows
Linux
Emscripten (WebAssembly and JavaScript)
FreeBSD
NetBSD
OpenBSD
Haiku
Solaris
macOS (PPC, x86-32 and x86-64)
PlayStation 3
PlayStation Portable
PlayStation Vita
Original Microsoft Xbox
Microsoft Xbox 360 (Libxenon/XeXDK)
Nintendo GameCube
Nintendo Wii
Nintendo Wii U
Nintendo 3DS
Nintendo Switch
Nintendo NES/SNES Classic Edition
Raspberry Pi
Android
iOS
Blackberry

Dependencies (PC)
There are no true hard dependencies per se.
On Windows, RetroArch can run with only Win32 as dependency.
On Linux, there are no true dependencies. For optimal usage, the
following dependencies come as recommended:

GL headers / Vulkan headers
X11 headers and libs, or EGL/KMS/GBM

OSX port of RetroArch requires latest versions of XCode to build.
RetroArch can utilize these libraries if enabled:

nvidia-cg-toolkit
libfreetype2 (TTF font rendering on screen)

RetroArch needs at least one of these audio driver libraries:

ALSA
OSS
RoarAudio
RSound
OpenAL
JACK
SDL
PulseAudio
XAudio2 (Win32, Xbox 360)
DirectSound (Win32, Xbox 1)
CoreAudio (OSX, iOS)

To run properly, RetroArch requires a libretro implementation present; however, as it's typically loaded
dynamically, it's not required at build time.
Dependencies (Console ports, mobile)
Console ports have their own dependencies, but generally do not require
anything other than what the respective SDKs provide.
Configuring
The default configuration is defined in config.def.h.
It is not recommended to change this unless you know what you're doing.
These can later be tweaked by using a config file.
A sample configuration file is installed to /etc/retroarch.cfg. This is the system-wide config file.
RetroArch will on startup create a config file in $XDG\_CONFIG\_HOME/retroarch/retroarch.cfg if it does not exist.
Users only need to configure a certain option if the desired value deviates from the value defined in config.def.h.
To configure joypads, use the built-in menu or the retroarch-joyconfig command-line tool.
Compiling and installing
Instructions for compiling and installing RetroArch can be found in the Libretro/RetroArch Documentation Center.
CRT 15Khz Resolution Switching
CRT SwitchRes will turn on, on the fly. However, you will need to restart RetroArch to disable it. With CRT SwitchRes enable RetroArch will start in 2560 x 480 @ 60.
If you are running Windows, before enabling the CRT SwitchRes options please make sure you have installed CRTEmudriver and installed some modelines. The minimum modelines for all games to switch correctly are:

2560 x 192 @ 60.000000
2560 x 200 @ 60.000000
2560 x 240 @ 60.000000
2560 x 224 @ 60.000000
2560 x 237 @ 60.000000
2560 x 256 @ 50.000000
2560 x 254 @ 55.000000
2560 x 448 @ 60.000000
2560 x 480 @ 60.000000

Install these modelines replacing 2560 with your desired super resolution. The above resolutions are NTSC only so if you would be playing any PAL content please add PAL modelines:

2560 x 192 @ 50.000000
2560 x 200 @ 50.000000
2560 x 240 @ 50.000000
2560 x 224 @ 50.000000
2560 x 288 @ 50.000000
2560 x 237 @ 50.000000
2560 x 254 @ 55.000000
2560 x 448 @ 50.000000
2560 x 480 @ 50.000000

Some games will require higher PAL resolutions which should also be installed:

2560 x 512 @ 50.000000
2560 x 576 @ 50.000000

Ideally install all these modelines and everything will work great.
Super Resolutions
The default super resolution is 2560. It is displayed just under the CRT switch option, which can be found in video settings. This can be changed within the retroarch.cfg. The only compatible resolutions are 1920, 2560 and 3840. Any other resolutions will be ignored and native switching will be activated.
Native Resolutions
If native resolutions are activated you will need a whole new set of modelines:


256 x 240 @ 50.006977 SNESpal


256 x 448 @ 50.006977 SNESpal


512 x 224 @ 50.006977 SNESpal


512 x 240 @ 50.006977 SNESpal


512 x 448 @ 50.006977 SNESpal


256 x 240 @ 60.098812 SNESntsc


256 x 448 @ 60.098812 SNESntsc


512 x 240 @ 60.098812 SNESntsc


512 x 224 @ 60.098812 SNESntsc


512 x 448 @ 60.098812 SNESntsc


256 x 192 @ 59.922745 MDntsc


256 x 224 @ 59.922745 MDntsc


320 x 224 @ 59.922745 MDntsc


320 x 240 @ 59.922745 MDntsc


320 x 448 @ 59.922745 MDntsc


320 x 480 @ 59.922745 MDntsc


256 x 192 @ 49.701458 MDpal


256 x 224 @ 49.701458 MDpal


320 x 224 @ 49.701458 MDpal


320 x 240 @ 49.701458 MDpal


320 x 288 @ 49.701458 MDpal


320 x 448 @ 49.701458 MDpal


320 x 480 @ 49.701458 MDpal


320 x 576 @ 49.701458 MDpal


256 x 288 @ 49.701458 MSYSpal


256 x 240 @ 60.098812 NESntsc


256 x 240 @ 50.006977 NESpal


640 x 237 @ 60.130001 N64ntsc


640 x 240 @ 60.130001 N64ntsc


640 x 480 @ 60.130001 N64ntsc


640 x 288 @ 50.000000 N64pal


640 x 480 @ 50.000000 N64pal


640 x 576 @ 50.000000 N64pal


256 x 252 @ 49.759998 PSXpal


320 x 252 @ 49.759998 PSXpal


384 x 252 @ 49.759998 PSXpal


640 x 252 @ 49.759998 PSXpal


640 x 540 @ 49.759998 PSXpal


384 x 240 @ 59.941002 PSXntsc


256 x 480 @ 59.941002 PSXntsc


352 x 240 @ 59.820000 Saturn/SGFX_NTSCp


704 x 240 @ 59.820000 SaturnNTSCp


352 x 480 @ 59.820000 SaturnNTSCi


704 x 480 @ 59.820000 SaturnNTSCi


352 x 288 @ 49.701458 SaturnPALp


704 x 288 @ 49.701458 SaturnPALp


352 x 576 @ 49.701458 SaturnPALi


704 x 576 @ 49.701458 SaturnPALi


240 x 160 @ 59.730000 GBA


320 x 200 @ 60.000000 Doom


// Arcade

400 x 254 @ 54.706841 MK
384 x 224 @ 59.637405 CPS1

These modelines are more accurate giving exact hz. However, some games may have unwanted results. This is due to mid-scanline resolution changes on the original hardware. For the best results super resolutions are the way to go.
CRT resolution switching & MAME
Some arcade resolutions can be very different from consumer CRTs. There is resolution detection to ensure MAME games will be displayed in the closest available resolution but drawn at their native resolution within this resolution. Meaning that the MAME game will look just like the original hardware.
MAME ROMs that run in a vertical aspect like DoDonPachi need to be rotated within MAME before resolution switching and aspect correction will work. Do this before enabling CRT SwitchRes so that RetroArch will run in your desktop resolution. Once you have rotated any games that may need it turn CRT SwitchRes on.
",3212
ConceptJunkie/rpn,Python,"rpn
rpn is a command-line Reverse-Polish Notation calculator.
rpn supports arithmetic with arbitrary precision, powers and roots, logarithms, algebraic functions (including polynomials arithmetic and solving), trigonometric functions, complex numbers, computer science related functions (bitwise math, base conversion), number theory functions, astronomical functions, prime number calculations and lookup, can operate with single operands or lists of operands and supports a wide variety of flexible unit conversions comparable to the GNU units program.
Updates
Update - April 26, 2019
Version 8 is released.   This version includes a small number of new features, but the primary focus was a significant refactor of the unit conversion code.  The 'convert' operator is more powerful and easier to use.
Update - March 8, 2019
rpnChilada 7.2.3 fixes the unit conversion bug.  Please upgrade.
Update - March 7, 2019
This is embarrassing.  I just discovered a long-standing bug with the unit conversion code where for some reason it thinks there are 59021.97 seconds in a day.  I've narrowed the bug down to between the 7.0.0 and 7.1.0 releases.  This is weird because
every other unit conversion I checked, including other conversions with days and seconds work correctly.  The makeUnits code has been in place for about 3 or 4 years and always seemed to be rock-solid.  I'll try to push a 7.2.2 in the next few days, or
possibly even 7.3.0 depending on what else gets included.
Update - February 26, 2019
OK, stick a fork in 7.2.0.  It's done.  I'd intended to get back to releasing often, but 10 months is not ""often"".
Aside from the usual ton of bug fixes and minor improvements, this version offers several operators having to do with the physics of black holes.  See ""rpn help physics"" for details.
The 7.2.0 release will show up on PyPI in the next day or two as soon as I have some time to test the wheel.
Update - February 22, 2019
Not much has happened with rpn lately, but I do have some good plans.  I haven't released 7.2 mostly due to laziness, but I've also got some solid ideas for improvements with the unit conversion functionality.  For one thing, I want to fold the constants into the units database, and I have an idea for adding some implicit unit conversion.
I've also wanted to migrate from pyephem, which is no longer being developed, to Skyfield, which is recommended by the people who used to make pyephem.  Skyfield is also a pure Python library, which makes my life easier.  However, Skyfield is a more low-level library, so there's not a one-to-one correspondence for most of the pyephem functionality I've been using.
Update - February 21, 2018
rpn is available on pypi.org.  Since there always was a project called ""rpn"", I had to come up with a new name, so I'm happy to introduce ""rpnChilada"".
Windows users will want to use Christophe Gohlke's Windows installers for gmpy2 and pyephem at https://www.lfd.uci.edu/~gohlke/pythonlibs/.
Update - August 15, 2016
I am very excited that people have started noticing rpn!
Please continue with comments, suggestions and bug reports. rpn has lots of little bugs and possibly some big ones, too, and although I have unit tests, most of the time, I find bugs from using it.
I especially want to thank the folks at The Nineteenth Byte on Stack Exchange for their nice comments. I also love solving puzzles with rpn, so if there's something you'd like to see it be able to do, drop me a line at rickg@his.com.
I'll try to focus on improving the help in the near future.
Update - July 19, 2016
I don't know if anyone has ever looked at this project... not even my Mom. But anyhow, I wanted to leave an update anyway. The ""imminent"" release of version 7 is anything but. I have been too lazy to tackle trying to make the wheel work correctly. I've been making small additions here and there, including bug fixes whenever I find problems.
Currently, my short list is topped with switching to SQLite for caching function results to disk. I was experimenting with the sigma function and found that once the cache of values got past a million, loading it had become unreasonably slow. I think it would also be a very good idea to convert the prime number data files to SQLite tables as well.
There are a few Python 2 compatibility problems that remain, and those should be easy to fix, but I've been too busy with a combiniation of real life and general laziness. The conversion to using generators mentioned in the last update is complete and has greatly improved performance for a lot of operators.
RPN continues to proceed slowly and it works just fine right now, so it can be used just fine despite being a ""pre-release"".
Update - November 16, 2015
The scope of changes for version 7 keeps growing. The transition to lazy list evaluation (using generators) is going to be a very big change and currently a lot of operators are broken.
It is recommended that anyone using RPN from Git stick with the 7.0.alpha1 tag for the time being.
Update - October 15, 2015
I've decided the upcoming release will be version 7 since so much has been added, a lot has been reorganized and I've gotten serious about unit tests.
An official release of version 7 probably won't be for a while, because I really want to be able to release it on PyPI. There's a lot of work I want to do before cutting another release, and it's going to take some time, but the current git master contains all the latest features and is working fine.
rpn should still work on Android, but there are problems with the ephem library. I think it has to do with building the AstroLib code, and haven't had a chance to try to diagnose the problem.
Update - August 5, 2015
I am working on creating a wheel for rpn, and I'm hoping I can also make it Python 2 compatible before cutting another release. The biggest roadblock is just getting some round tuits instead of adding in new operators, which is much more fun.
Another cool update: rpn can now be run on Android with the Termux app
(http://termux.com/)! Right now, it fails a unit test having to do with date formatting, which I haven't gotten around to investigating, but otherwise it works great. Where else can you factor a 50-digit number on your Android device?

The current release is 8.0.0.
See ""rpn help settings"" for more information.
Running RPN using the source:
rpn is written in Python 3, and requires several libraries for the hard math stuff (gmpy2 is optional, but recommended for improved performance).
You will need to install the following prerequisites:
arrow>=0.13.1
convertdate>=2.1.3
ephem>=3.7.6.0
geopy>=1.19.0
gmpy2>=2.0.8
importlib_resources>=1.0.2
mpmath>=1.1.0
numpy>=1.16.2
pyreadline>=2.1
pytz>=2018.9
rpnChiladaData>=1.0.0
skyfield>=1.10
timezonefinder>=4.0.2
tzlocal>=1.5.1

Windows users will want to use Christophe Gohlke's Windows installers for gmpy2 and pyephem at https://www.lfd.uci.edu/~gohlke/pythonlibs/.
Using rpnChilada:
rpnChilada is very easy to use.  It's just like any RPN calculator: Operands go first, then the operators. All examples assume rpn is an alias for python /<path-to-rpn>/rpn.py.  In interactive mode, you leave off the rpn.
I always create an alias for ""python rpn.py"" called ""rpn"".  If you are using the package installed with pip, there are commands in the scripts directories called ""rpn"" and ""rpnChilada"" to launch rpnChilada.
Unit tests can be run with the testRPN command (when installed from the wheel) or by running testRPN.py from the rpn/ directory.
For instance:
rpn 2 2 +

will calculate 2 + 2.
rpn supports more than 900 operators. (rpn _dump_operators will list them all.)
The entire operator list is also included at the bottom of this document.
rpn has pretty extensive built-in help, although the help files are not complete. However, all operators have at least a brief description, and most are obvious enough to use easily.
Start with rpn help for an overview. To dive right in, see rpn help examples. In interactive mode, typing help will launch help mode. Then, topics will print out a list of help topics and exit will return to rpn.
The data files are stored in the same location as rpn.py in a subdirectory called rpndata/.
If you really want to generate prime numbers, see my ""primes"" project: https://github.com/ConceptJunkie/primes I've calculated the first 15 billion prime numbers and will someday update the rpn lookup tables.
The project https://github.com/ConceptJunkie/rpnChiladaData provides the compiled prime number data files.  If you installed rpnChilada with pip, then this data will be automatically installed.
rpn also provides a simple interface for accessing The On-Line Encyclopedia of Integer Sequences (http://oeis.org), see rpn help special and rpn help oeis.
rpnChilada used to provide a Windows installer, but I haven't been able to do that since version 6.4.0.  I hope to bring that back some day.
Feedback, Comments, Bug Reports:
Any feedback is welcome at rickg@his.com.  This was originally an exercise to learn Python, but slowly blossomed into something really useful and fun, so I wanted to share it. rpn also exposes just a few of the features of the amazing mpmath library (by Fredrik Johansson, http://mpmath.org/) which is where almost all the hard math stuff is actually done.
Rick Gutleber
rickg@his.com
p.s. rpn is licensed under the GNU GPL version 3.0. See (see (http://www.gnu.org/licenses/gpl.html) for more information).
Release Notes
8.0.0
The unit conversion code has been heavily refactored and works much better now.
Added the 'base_units' and 'dimensions' operators, mostly for testing purposes.
Added '_dump_conversions' and '_dump_cache', also for testing purposes.
rpnChilada is now smart enough to recognize when an OEIS request has failed,
and to ignore the cached result stored as a result.  If it detects that the
cached value is empty, it will perform the request again and recache the
result.
Help now supports units and constant operators after way too long.  Filling in
the help info for the units and constant operators, along with all the existing
help info that's missing, will take a while, and is continuing.
rpnChilada has officially dropped Python 2 support.  I rarely tested it anyway.
Added 'wind_chill' and 'heat_index' operators.
The unit tests now confirm that aliases do not collide with other reserved
words.  The alias creation for generated types has also been cleaned up.
The astronomy functionality has been refactored to support migrating to the
skyfield library from pyephem.
Removed the 'break_on' operator because it no longer works.  It will be
re-implemented in the future.
Added 'to_ethiopian', 'to_ethiopian_name' and 'from_ethiopian' operators for
converting to and from the Ethiopian calendar.
7.2.5
I fat-fingered an addition to the requirements.txt file.  :-/
7.2.4
Just a bunch of fixes.  makeUnits has been improved a bit, and I've validated that all conversions exist, and are consistent.
7.2.3
I messed up the upload for 7.2.2.  No code changes, just fixed packaging.
7.2.2
A big change that doesn't affect functionality is that the prime number data now resides in a separate package called rpnChiladaData.  This data rarely changes so there's no reason to download it.
A major bug was uncovered after almost a year.  rpnChilada thought there were 51920.97 seconds in a day because of a typo.  This has been fixed, and I figured out how to detect other similar problems if they exist.  This change will be implemented in the next few days.
7.2.1
Unit conversion is now a lot smarter because the automatically-generated area and volume units are generated more intelligently.  This means expressions using the ""square"" and ""cubic"" units will convert automatically and you won't end up with something like ""foot^2/square_mile"".
...and yes, a few bug fixes.
7.2.0
Added 'random_element' operator.
The gmpy2 digits( ) function is a much faster way to convert numbers to bases 2 through 62.
Added support for using yafu for factoring.
Added 'aliquot_limit' operator.
Added support for user configuration:  'set_config', 'get_config', 'delete_config' and 'dump_config'.
Added the 'mothers_day', 'fathers_day' and 'advent' operators.
Added the 'molar_gas_constant', 'aliquot_limit' and 'distance' operators (the old 'distance' operator is now called 'geo_distance').
Added unit tests for converting units, and made a few fixes accordingly.
Verbose mode for factoring gets turned on with -D.
Oops, there were two operators named 'distance'.  'distance' now refers to the physics operator and the geography operator is now named 'geo_distance'.
The 'acceleration' operator has been implemented.
The derived Planck units are now calculated, instead of hard-coded.
Block Hole operators:  'black_hole_entropy', 'black_hole_lifetime', 'black_hole_luminosity', 'black_hole_mass', 'black_hole_radius' (was 'schwarzchild_radius'), 'black_hole_surface_area', 'black_hole_surface_gravity', 'black_hole_temperature'
...and the usual bug fixes.
7.0.0
Version 7 represents over 2-1/2 years of work and I neglected to keep track of the changes.
There are probably around 200 more operators since 6.4 was released, and I replaced the factoring code with a much faster verison.
rpn now supports user-defined variables and functions, including persistent variables and functions.
6.4.0 - ""Factoring Fun""
Revamped factorization to be much, much faster, using the Brent-Pollard
algorithm instead of just brute-force dividing.   More to come...
Added the 'magnetic_constant', 'electric_constant', 'rydberg_constant',
'newtons_constant' and 'fine_structure' operators.
Added 'eulerphi' operator.
Added caching for factorizations.  I often factor the same numbers over and over (like when I'm playing with the Fibonaccis) so it made sense to cache the results for non-trivial factoring.
Added the 'sigma, 'aliquot', 'polypower', 'mobius' and 'mertens' operators.  The old 'mertens' operator was renamed to 'mertens_constant'.  The 'aliquot' operator is another use-case for caching factorizations... try it with 276.
rpn can can now factor the first 450 or so in a reasonably short time.
Added the 'frobenius', 'slice', 'sublist', 'left' and 'right' operators.
Added 'crt' operator.
...and the usual slew of bug fixes.
6.3.0
Added the 'geomean' operator.
Added the 'argument' and 'conjugate' operators.
Fixed 'trianglearea'.  It's been wrong for a long time.  Sorry.
Added the 'fibonorial' operator.
Added the 'eulerbrick' operator.
Added the 'unlist' operator.
Added the 'makepyth3' and 'makepyth4' operators.
Added the 'equal', 'greater', 'less', 'not_equal', 'not_greater', and
'not_less' operators.
Added the 'reduce' operator.
Added the 'lcm' operator.
The 'pascal' operator was renamed to 'pascaltri' to avoid a collision with the 'pascal' unit.
Fixed several minor bugs.
6.2.0
Experimental support for mpath plotting functionality using the new
operators, 'plot', 'plot2', 'plotc'.  These operators are not supported
in the Windows installer.
'quit' is now an alias for 'exit' in interactive mode and help mode.
Improvements in function definition.  'y' and 'z' are now operators, allowing for defining functions on 2 or 3 variables.
Operators 'eval2' and 'eval3' allow for evaluation of 2 and 3 variable
operators.
rpn now throws an error if a user-defined function is invalidly specified, instead of going into an infinite loop.
'filter' allows filtering a list based on a user-defined function.
If the units in a measurement cancel out, then the measurement is converted back to a numerical value.
Added 'rand_' and 'randint_' operators.
Added the 'debruijn' operator.
Fixed several minor bugs.
6.1.0
New operators:  'maxdouble', 'maxfloat', 'mindouble', 'minfloat'
Base conversion for output is no longer limited to 1000 digits.  There's no reason to do that.
'rpn 0 cf' now throws an error rather than dividing by 0.
6.0.1
Added code to prevent scientific notation from messing up base conversions for the integral part of the number (up to 1000 digits).
6.0.0
Introduced interactive mode, including variable declaration and referencing previous results by number.  (see 'rpn help interactive_mode')
Added caching for OEIS operators.  However, it turns out some OEIS text is non-ASCII, so I'll have to deal with that.
Operator help now includes examples by default.
The 'time' operator type conflicted with the 'time' unit type, so I changed the operator type to 'date'... because they were all about dates!
Fixed a long-standing precision problem with unit conversion.
Lots more bug fixes.
5.28.5
More bug fixes and code cleanup.  Added the 'unfloat' and 'undouble' operators.
5.28.4
Added the 'diffs2' operator.
More bug fixes thanks to the test script!
5.28.3
The operators 'doublebal', doublebal_', 'triplebal', and 'triplebal_' now work
correctly.  The data files have been significantly expanded as well.
More prime number updates will come in the next few weeks.  My target is to
expand every table up to the first 10 billion primes.
5.28.2
Several bug fixes relating to 'estimate' and unit conversion.   Some unit types
were folded together because they had the same basic units (e.g., frequency and
radioactivity were both time ^ -1, which confused the conversion logic).
5.28.1
Added separate installers for the plain-vanilla rpn (with only the ""small
primes"" data file, i.e., the first million primes), and the installer with all
of the prime data files.
The 'primes' operator has been fixed so it works correctly for small values.
I'm currently testing the prime functions, which I haven't touched in a long
time, so more fixes will definitely be coming.  The balanced prime functions
are currently broken and will be fixed shortly, including updated data files.
5.28.0
Added 'x', 'eval', 'nsum', 'nprod', 'limit', 'limitn', 'infinity', and
'negative_infinity', and 'value' operators.
5.27.2
Help for unit types now prints out all aliases for the unit operators.
5.27.1
Added an error message if the 'name' operand is out of range, and added support
for negative numbers.
5.27.0
Added the 'name' operator.
5.26.0
Added dynamic_visocity and frequency unit types and a few bug fixes.
Added units for the days and years of the other 8 planets in the Solar System.
Added several constant units for quaint or archaic number terms like 'score'
and 'gross'.
Added mass units for common particle masses.
Updated some natural values (electron mass, etc.).
Fixed some problems with generating and interpreting compound units.
Added the 'prevost' operator.
5.25.0
Added Julian date operators, ISO date operators, calendar operators and the
'ash_wednesday' operator.  Added support for the density unit type and several
small bug fixes.
5.24.0
A few more bug fixes, plus new calendar-related operators:  easter.
election_day, labor_day, memorial_day, nthday, presidents_day, thanksgiving
5.23.1
The help improvements actually work now.  So much for testing.
There are now some examples of absolute time handling.
5.23.0
Help will now search topics for partial matches if a complete match isn't found.
5.22.0
Added a bunch of new constants for powers of 10.
5.21.2
Added -l to format help output for different line lengths.  However, it still
doesn't format the blockquoted help text.
5.21.1
Added percent operator, weekday now throws a proper error is the operand isn't
a time value.
5.21.0
The long-awaited absolute time feature:  rpn can now handle absolute time
values.  For input, just use ISO 8601 format, or a reasonable subset thereof.
There is also the 'maketime' operator, which takes a list similar to the old
'tounixtime' operator.
5.20.7
Added help for unit types.   Help for individual units will come eventually,
but they are pretty self-explanatory.
5.20.6
The prime? operator wasn't working correctly for small values.
5.20.5
rpn now throws an error when attempting to get the 0th or less prime number.
5.20.4
rpn now correctly reports the argument in question on any error.
5.20.3
Made a fix to improve rpn's reporting of the argument in question when there is
an error.  It's probably not 100% correct yet.
5.20.2
Fixed the list operator parsing so polyprod and polysum work correctly.
5.20.1
Several calls to polyval( ) had hard-coded fractions in them instead of calls
to fdiv( ), resulting in rounding errors.
5.20.0
rpn finally comes with an installer for Windows, in 32-bit and 64-bit flavors.
5.19.3
The test script has been rewritten in Python.  It's still very basic and only
does a sanity test to show every operator works without crashing.  It doesn't
test for correct answers yet.
5.19.2
rpn now outputs an empty list correctly.  The 'append' operator (to append
lists) has been fixed.
5.19.1
Fixed several problems with 'tounixtime' and 'fromunixtime'.
The first version of a test script is available as a batch file.
5.19.0
Added 'randint' operator.
5.18.7
compoundUnits was still being referred to without the ""g."" global specifier.
5.18.6
rpn now prints out an error message if you try to get help for an unknown
topic.
5.18.5
Fixed a bug concerning adding dissimilar units.
5.18.4
rpn now correctly parses ""-0"" as a value again.
5.18.3
Added 'split' as an alias for 'unpack' because I couldn't remember what it was
called.
Made some minor fixes made based on running pyflakes, pylint pep8, and the
test script.
5.18.2
Made a bunch of bug fixes that showed up as a result of reorganizing the code.
5.18.1
It's clear I haven't done any unit conversions in a while because there were
still issues with declarations of variables.  Now, I've started eliminating the
use of ""global"" in favor of a global module.
Operators supported by rpn:
( ) aa_battery abs abundance abundance_ratio acceleration accuracy acos acosh
acot acoth acsc acsch add add_digits add_polynomials advent agm aliquot
aliquot_limit alpha_particle_mass alternate_signs alternate_signs_2
alternating_factorial alternating_sum alternating_sum_2 and and_all
angular_separation angular_size antiprism_area antiprism_volume
antitransit_time apery_constant append april argument arrangements ascension
asec asech ash_wednesday asin asinh astronomical_dawn astronomical_dusk atan
atanh atomic_number atomic_symbol atomic_weight august autumnal_equinox
avogadro_number balanced_prime balanced_prime_ barnesg base base_units
bell_polynomial beta binomial bitwise_and bitwise_nand bitwise_nor bitwise_not
bitwise_or bitwise_xor black_hole_entropy black_hole_lifetime
black_hole_luminosity black_hole_mass black_hole_radius black_hole_surface_area
black_hole_surface_gravity black_hole_temperature bohr_radius
boltzmann_constant build_numbers build_step_numbers calendar calkin_wilf
catalan_constant ceiling centered_cube centered_decagonal centered_dodecahedral
centered_heptagonal centered_hexagonal centered_icosahedral centered_nonagonal
centered_octagonal centered_octahedral centered_pentagonal centered_polygonal
centered_square centered_tetrahedral centered_triangular cf
champernowne_constant char christmas classical_electron_radius collate collatz
columbus_day combinations combine_digits comma comma_mode compare_lists
compositions cone_area cone_volume conjugate convert copeland_erdos_constant
cos cosh cot coth coulomb_constant count count_bits count_different_digits
count_digits count_divisors cousin_prime cousin_prime_ crt csc csch cube
cube_root cumulative_diffs cumulative_ratios cyclic_permutations cyclotomic
dawn day_time debruijn decagonal decagonal_centered_square decagonal_heptagonal
decagonal_hexagonal decagonal_nonagonal decagonal_octagonal
decagonal_pentagonal decagonal_triangular december decimal_grouping decrement
default delete_config denomination_combinations density_of_water describe
deuteron_mass dhms difference diffs digamma digital_root digits dimensions
discriminant distance distance_from_earth divide divisors dms dodecahedral
dodecahedron_area dodecahedron_volume double double_balanced double_balanced_
double_factorial dst_end dst_start dump_config duplicate_digits
duplicate_number duplicate_operator duplicate_term dusk e earth_density
earth_gravity earth_mass earth_radius earth_volume easter echo eclipse_totality
eddington_number egypt election_day electric_constant electron_charge
electron_mass element element_block element_boiling_point element_density
element_description element_group element_melting_point element_name
element_occurrence element_period element_state energy_equivalence enumerate
enumerate_dice enumerate_dice_ epiphany equals_one_of erdos_persistence ernal
operators: escape_velocity estimate eta euler_brick euler_mascheroni_constant
euler_phi eval eval0 eval2 eval3 eval_list eval_list2 eval_list3
eval_polynomial exp exp10 exponential_range expphi factor factorial false
faraday_constant fathers_day february fibonacci fibonorial filter
filter_by_index filter_lists find find_palindrome find_polynomial
find_sum_of_cubes find_sum_of_squares fine_structure_constant flatten float
floor for_each for_each_list fraction friday frobenius from_bahai
from_ethiopian from_hebrew from_indian_civil from_islamic from_julian
from_mayan from_persian from_unix_time function gallon_of_ethanol
gallon_of_gasoline gamma gcd gcd2 generalized_pentagonal
generate_polydivisibles geometric_mean geometric_range geometric_recurrence
geo_distance get_base_k_digits get_combinations get_config get_day get_digits
get_hour get_left_digits get_left_truncations get_minute get_month
get_nonzero_base_k_digits get_nonzero_digits get_partitions get_permutations
get_repeat_combinations get_repeat_permutations get_right_digits
get_right_truncations get_second get_timezone get_variable get_year
glaisher_constant good_friday group_elements harmonic harmonic_mean
has_any_digits has_digits has_only_digits heat_index helion_mass help
heptagonal heptagonal_hexagonal heptagonal_pentagonal heptagonal_square
heptagonal_triangular heptanacci hexagonal hexagonal_pentagonal
hexagonal_square hexanacci hex_mode hms horizon_distance hurwitz_zeta hyper4_2
hyperfactorial hyperfine_transition_frequency_of_cesium hypotenuse i
icosahedral icosahedron_area icosahedron_volume identify identify_mode if
imaginary increment independence_day infinity input_radix integer
integer_grouping interleave intersection interval_range invert_units
isolated_prime iso_date iso_day is_abundant is_achilles is_automorphic
is_base_k_pandigital is_base_k_smith_number is_bouncy is_carmichael
is_composite is_decreasing is_deficient is_digital_permutation is_divisible
is_equal is_even is_friendly is_generalized_dudeney is_greater is_harshad
is_increasing is_integer is_kaprekar is_kth_power is_k_hyperperfect
is_k_morphic is_k_narcissistic is_k_semiprime is_k_sphenic is_less
is_narcissistic is_not_equal is_not_greater is_not_less is_not_zero is_odd
is_order_k_smith_number is_palindrome is_palindrome_list is_pandigital is_pddi
is_pdi is_perfect is_polydivisible is_powerful is_power_of_k is_prime is_pronic
is_rough is_ruth_aaron is_semiprime is_smith_number is_smooth is_sphenic
is_square is_squarefree is_step_number is_strong_pseudoprime is_sum_product
is_trimorphic is_unusual is_zero itoi january july june jupiter jupiter_mass
jupiter_radius jupiter_revolution jupiter_volume khinchin_constant
kinetic_energy k_fibonacci k_persistence k_sphere_area k_sphere_radius
k_sphere_volume labor_day lah lambda lambertw larger latlong_to_nac lat_long
lcm lcm2 leading_zero leading_zero_mode left leyland li limit limitn
linear_recurrence linear_recurrence_with_modulo list_from_file location
location_info log log10 log2 logxy log_gamma long longlong lucas
magnetic_constant magnetic_flux_quantum make_cf make_datetime make_iso_time
make_julian_time make_pyth_3 make_pyth_4 mantissa march mars mars_mass
mars_radius mars_revolution mars_volume martin_luther_king_day mass_equivalence
maximum max_char max_double max_float max_index max_long max_longlong
max_quadlong max_short max_uchar max_ulong max_ulonglong max_uquadlong
max_ushort may mean memorial_day mercury mercury_mass mercury_radius
mercury_revolution mercury_volume merten merten_constant mills_constant minimum
min_char min_double min_float min_index min_long min_longlong min_quadlong
min_short min_uchar min_ulong min_ulonglong min_uquadlong min_ushort mobius
modulo molar_gas_constant molar_mass monday moon moonrise moonset
moon_antitransit moon_gravity moon_mass moon_phase moon_radius moon_revolution
moon_transit moon_volume mothers_day multifactorial multinomial multiply
multiply_digits multiply_digit_powers multiply_nonzero_digits
multiply_nonzero_digit_powers multiply_polynomials muon_mass name nand nand_all
narayana nautical_dawn nautical_dusk nearest_int negative negative_infinity
neptune neptune_mass neptune_radius neptune_revolution neptune_volume
neutron_mass newton_constant new_years_day next_antitransit
next_first_quarter_moon next_full_moon next_last_quarter_moon next_new_moon
next_prime next_primes next_quadruplet_prime next_quintuplet_prime next_rising
next_setting next_transit night_time nonagonal nonagonal_heptagonal
nonagonal_hexagonal nonagonal_octagonal nonagonal_pentagonal nonagonal_square
nonagonal_triangular nonzero nor nor_all not november now nprod nsum nth_apery
nth_bell nth_bernoulli nth_carol nth_catalan nth_centered_decagonal
nth_centered_heptagonal nth_centered_hexagonal nth_centered_nonagonal
nth_centered_octagonal nth_centered_pentagonal nth_centered_polygonal
nth_centered_square nth_centered_triangular nth_decagonal nth_delannoy
nth_heptagonal nth_hexagonal nth_jacobsthal nth_kynea nth_leonardo
nth_linear_recurrence nth_linear_recurrence_with_modulo nth_menage
nth_mersenne_exponent nth_mersenne_prime nth_motzkin nth_nonagonal
nth_octagonal nth_padovan nth_pell nth_pentagonal nth_perfect_number
nth_polygonal nth_prime nth_quadruplet_prime nth_quintuplet_prime nth_schroeder
nth_schroeder_hipparchus nth_square nth_stern nth_sylvester nth_thue_morse
nth_triangular nth_weekday nth_weekday_of_year nuclear_magneton occurrences
occurrence_cumulative occurrence_ratios octagonal octagonal_heptagonal
octagonal_hexagonal octagonal_pentagonal octagonal_square octagonal_triangular
octahedral octahedron_area octahedron_volume octal_mode octanacci october oeis
oeis_comment oeis_ex oeis_name oeis_offset omega_constant or orbital_mass
orbital_period orbital_radius orbital_velocity ordinal_name or_all output_radix
pack parity partitions pascal_triangle pentagonal pentagonal_square
pentagonal_triangular pentanacci pentatope pentecost permutations permute_dice
permute_digits permute_lists persistence phi pi planck_acceleration
planck_angular_frequency planck_area planck_charge planck_constant
planck_current planck_density planck_electrical_inductance planck_energy
planck_energy_density planck_force planck_impedance planck_intensity
planck_length planck_magnetic_inductance planck_mass planck_momentum
planck_power planck_pressure planck_temperature planck_time planck_viscosity
planck_voltage planck_volume planck_volumetric_flow_rate plastic_constant plot
plot2 plotc pluto pluto_mass pluto_radius pluto_revolution pluto_volume polyexp
polygamma polygonal polygon_area polylog polynomial_power polynomial_product
polynomial_sum polyprime polytope power powerset power_tower power_tower2
powmod precision presidents_day previous previous_antitransit
previous_first_quarter_moon previous_full_moon previous_last_quarter_moon
previous_new_moon previous_prime previous_primes previous_rising
previous_setting previous_transit prevost_constant prime primes prime_pi
prime_range primorial prism_area prism_volume product proton_mass pyramid
quadlong quadruplet_prime quadruplet_prime_ quintuplet_prime quintuplet_prime_
radiation_constant radical random random_ random_element random_integer
random_integer_ range ratios real reciprocal recurrence reduce
reduced_planck_constant repeat replace_digits repunit result reversal_addition
reverse reverse_digits rhombic_dodecahedral riesel right robbins_constant
roll_dice roll_dice_ roll_simple_dice root rotate_digits_left
rotate_digits_right round round_by_digits round_by_value rydberg_constant
safe_prime saturday saturn saturn_mass saturn_radius saturn_revolution
saturn_volume sec sech september set_config set_variable sextuplet_prime
sextuplet_prime_ sexy_prime sexy_prime_ sexy_quadruplet sexy_quadruplet_
sexy_triplet sexy_triplet_ shift_left shift_right short show_erdos_persistence
show_k_persistence show_persistence shuffle sidereal_year sigma sigma_k sign
silver_ratio sin sinh sized_range sky_location slice smaller solar_constant
solar_noon solve solve_cubic solve_quadratic solve_quartic sophie_prime sort
sort_descending speed_of_light sphere_area sphere_radius sphere_volume square
square_digit_chain square_root square_triangular star stddev
stefan_boltzmann_constant stella_octangula subfactorial sublist subtract sum
summer_solstice sums_of_k_nonzero_powers sums_of_k_powers sum_digits sun sunday
sunrise sunset sun_antitransit sun_luminosity sun_mass sun_radius sun_volume
superfactorial superprime surface_gravity tan tanh tau_mass tetrahedral
tetrahedron_area tetrahedron_volume tetranacci tetrate thabit thanksgiving
thue_morse_constant thursday timer timer_mode time_dilation today tomorrow
topic torus_area torus_volume to_bahai to_bahai_name to_ethiopian
to_ethiopian_name to_hebrew to_hebrew_name to_indian_civil to_indian_civil_name
to_islamic to_islamic_name to_iso to_iso_name to_julian to_julian_day
to_lilian_day to_mayan to_ordinal_date to_persian to_persian_name to_unix_time
transit_time triangle_area triangular tribonacci trigamma triplet_prime
triplet_prime_ triple_balanced triple_balanced_ triple_point_of_water
triton_mass tropical_year true truncated_octahedral truncated_tetrahedral
tuesday twin_prime twin_prime_ uchar uinteger ulong ulonglong undouble unfilter
unfilter_by_index unfloat union unique unit_roots unlist unpack uquadlong
uranus uranus_mass uranus_radius uranus_revolution uranus_volume ushort uuid
uuid_random vacuum_impedance value velocity venus venus_mass venus_radius
venus_revolution venus_volume vernal_equinox veterans_day von_klitzing_constant
wednesday weekday weekday_name wind_chill winter_solstice x xnor xor y ydhms
year_calendar yesterday z zero zeta zeta_zero [ ] _dump_aliases _dump_cache
_dump_constants _dump_conversions _dump_operators _dump_units _stats
",13
wmjordan/Codist,C#,"Codist
Codist is a Visual Studio extension which strives to provide better coding experience and productivity for C# programmers.
Features

Advanced Syntax Highlight with Comment Tagger
Super Quick Info with Click and Go to source code
Smart Bar with symbol reference analyzers
Scrollbar Marker
Symbol Marker
Navigation Bar (new in version 4.0)
Display Enhancements
Comprehensive Configurations
License, Bugs and Sugguestions


Advanced C# Syntax Highlight
The advanced syntax highlight function highlights every aspect of C# language elements with diverse styles, including using various font families and text styles, enlarging or shrinking font sizes, changing foreground or background colors and transparency.
The following screenshots of the TestPage.cs file in the source code project demonstrates possible syntax highlight effects in the Light theme.


The font size of type and member declarations can be enlarged, so it is much easier to spot them.
Syntax highlight can be applied to braces and parentheses.
Various syntax identifiers have different styles, temporary elements such as method parameters and local variables are italic, static symbols are underlined.
Comment content can be tagged (e.g. note).
Unnecessary code is marked strike-through.
Keywords are categorized and highlighted with various styles (e.g. abstract and sealed, return and throw, etc.).
Overriding members (such as ToString) can be painted with gradient background color, so at a glance we know that the marked implementations have overrided their base classes.
Imported symbols (from external assemblies, e.g. NotImplementedException, ToString) can be marked with a different style (bold here) from symbols defined in your code.
All the above styles are customizable.

Default Syntax Highlight Themes
To quickly get started with advanced syntax highlight, open a C# project, then open the Options dialog, navigate to the Syntax Highlight section, click the Light theme or Dark theme button in the dialog and see changes in effect. Don't forget to click the OK button to confirm the changes.

With the Save and Load buttons, you can backup and share your own syntax highlight settings.
If you mess up your syntax highlight styles, you can press the Reset button to reset all settings to default, or press the Light theme or Dark theme button to reapply predefined themes.
Note: There is a known issue in Codist that if you change the theme of Visual Studio, you may have to restart it to make syntax highlight settings to work properly.
From version 4.5 on, it is possible to load only part of the syntax preset or backup theme by unchecking check boxes under Load following parts when importing themes.
Customization of Syntax Highlight Styles
To customize and tweak the syntax highlight styles, click the sub sections inside the Syntax Highlight section to change individual styles, accordingly.

From version 4.6 on, it is possible to configure color and opacity individually. If we change the opacity value only, the default syntax color for a syntax definition is used.
Syntax definitions under the All languages section apply to all languages; those under Comment section apply to comment taggers (see below), others apply to corresponding languages accordingly.
TIP: Open a document window before you change the syntax theme or tweak the syntax highlight settings. While you change theme, you can see how the styles change in the code document window simultaneously.
My Symbols and External Symbols
Codist can also identify symbols which are defined in your source code and which are imported from external assemblies. This feature is so unique that you may not find it elsewhere.
You can customize it in the Symbol Marker tab of in the C# section of Syntax Highlight. Style My Type and Member is used for symbols from your code, and Referenced Type and Member is used for symbols imported from external assemblies.

Note: the predefined Light theme and Dark theme have defined external symbols with bold style, as the above screenshot shows.
Comment Tagger and Styles


The comment tagger highlights comments to your specific styles, according to the first token inside the comment.
Here are default effects of the some tagged comments.

To configure the comment tags, click the Tags tab, in the Comment sub-section of the Syntax Highlight section, where you can add, remove or modify comment tags.

To disable comment tagger, uncheck the check box of Comment Tagger on the Syntax Highlight option page.


The syntax style of comments or C# XML Documentations could be changed too. You can make them semitrasparent to stand behind usual code lines by changing the Opacity or the Font size value of the corresponding syntax parts.

Note: the predefined Light theme and Dark theme have defined XML Doc with a smaller font size (-1), as the above screenshot shows.


Super Quick Info
The quick info (the tooltip shown when you hover your mouse pointer on your C# source code) can be enhanced by Codist.
General Quick Info
To customize the Super Quick Info, adjust the settings in the options page.

Options in the General page apply to all code editor windows.


Hide Quick Info until Shift key is pressed
By default, Quick Info appears when you hover your mouse over a symbol or syntax token in code editor. Some programmers think this behavior interferes their workflow. Checking this option will suppress the Quick Info until Shift key is pressed.


Show info about selection length
This option will show how many characters and lines in your selection (if your selection spans over multiple lines). So you don't have to count characters one by one.



Show info about color
This option enables you preview color values. It works for hex color values (such as #00FF00£¬#FF993300), named colors (such as Black, White, etc.).

In C# code editor, it analysis system colors (such as SystemColors.WindowColor, SystemColors.Control, etc.), Color.FromArgb or Color.FromRgb expression with constant values as well.

The color info not only works in code windows, but also in debugger Watch window.



C# Quick Info
Super Quick Info especially enhances programming experience for C# programmers. There are plenty of options available in the options page.



Click and go to source code of symbol definition
If a symbol is defined in your source code, you can click and go to its definition on the Quick Info window. There's no need to hit F12 on your keyboard any more. Even more, Codist also tells you where the symbol is defined if you hover your mouse over it.



Override XML Documentation
The overridden XML Documentation makes the following changes to the documentation.

More syntax colors (adopting syntax highlight colors) for symbols.
Icons for documetation parts.
Selectable content of the documentation.
Symbols inside the documentation work with Click and Go feature too.
Concise form of members (without leading namespace or containing type names, hover your mouse over a symbol to view its full definition).
Extra tags, such as <b> (for bold), <i> (for italic) and <u> (for underline) are supported.
Extra information from documentations (see below).
Copyable quick info content (First select text with your mouse, and press Ctrl + C shortcut key).


When Override XML Documentation checkbox is checked in the options page, it is also possible to activate options under it.


Inherit from base type or interfaces option will show documentation description from base classes or implemeted interfaces if the XML Doc description of the current symbol is absent.



Inherit from <inheritdoc cref=""MemberName""/> target option will borrow description from the referenced MemberName.



Show <returns> XML Doc and Show <remarks> XML Doc will add content of those tags.


Override <exception> XML Doc option adds back documentations for exceptions to the Quick Info.





Quick Info Item Size
Quite often the Quick Info can take up a lot of space, covering almost half of the screen. It is possible to limit its size with Super Quick Info by assigning values to Max width and Max height in the options page.



Additional Quick Info Items
A dozen of additional quick info items could be displayed in the Additional Quick Info Items options page.



Attributes option shows attributes of a symbol.


Base types and interfaces options shows inheritance and implementation info of a type. It is recommended to check All ancestor types and Inherited interfaces to display the complete info of the hierarchy of a type.

Note: the IDisposable interface has special importance in .NET programming, thus it is assigned a special icon and pinned to the top of the interface list.


Declaration modifier option shows modifiers to a symbol when it is not a public instance one.



Interface implementation option shows if a member implements any interface.



Method overload options shows possible overloads of a method (including applicable extension methods).

This option also helps you find out correct overloads when any argument passed to a method is incorrect.



Parameter of method options shows whether a token or an expression is the parameter of a method in the argument list. What is more, the documentation of the parameter is also displayed.



Type parameter option shows information and documentation about type parameters.


Symbol location shows where a symbol is defined.


Numeric forms shows decimal, hexadecimal and binary forms for constant integer and Enum values.

The binary form is useful when working with bit flags.



String length and Hash codes for string constants.
(Hint: We can use Hash codes to quickly compare whether two strings that look alike are identical)


Smart Bar
The Smart Bar is a context-aware tool bar that appears automatically when you select some text, or double tap the Shift key on your keyboard.
There are two toolbars on Smart Bar. The top bar contains general editing commands for all file types. Buttons on the bottom bar changes according to file types.
Buttons on the Smart Bar changes according to your selection, typical buttons are editing operations (e.g. Cut, Copy, Paste,  Delete, Duplicate, Formatting, Find, etc.), code analysis operations (e.g. Go to defintion, Find references), refactoring operations (e.g. Rename, Extract method, etc.)

Each button on Smart Bar usually has multiple functions. Left clicking, right clicking, Ctrl+clicking and Shift+clicking trigger different commands. For details, see the tooltip for the buttons. Right clicking a button usually expands the effective range of a command to the whole line, or brings out a pop-up menu for more commands.

C# Specific Commands
When you select a symbol, you may probably see a Smart Bar like below.

The C# commands are on the second row.
The first one is Go to Definition, that behaves the same as the keyboard F12 command. With this, you no longer need hitting the F12 key to go to definition.
The second one is the Analyze symbol... button, a menu will pop up showing possible symbol analysis commands for the symbol. Since some commands require considerable amount of calculation, items ending with ""..."" will require a mouse click to expand. For instance, clicking the Find Callers command in the following screen shot will search the source code and list at what places are calling the selected method in a sub-menu. After the sub-menu is popped up, you can click items on the sub-menu and jump to the corresponding location.

Symbol Marker
Symbol marker draws markers for C# symbols.
Typically, you can double click a symbol in the C# source code, select the Mark Symbol command on the Smart Bar and choose the desired highlight marker on the drop-down menu.

After applying the command, all occurrences of the marked symbol will be marked with a different style.

To remove symbol marker, click the Remove symbol mark command in the drop-down menu of the Mark symbol command.
Symbol markers will be cleared when the solution is unloaded.
Note: The style of symbol markers can be customized in options page of the Syntax highlight feature. The default colors are listed below. You also need to turn on the Syntax Highlight feature in order to make this feature work.

Behavior of Smart Bar
By default, Smart Bar appears after selection changes, you can alter the behavior in the options page by unchecking the Show Smart Bar when selection is changed checkbox.

Smart Bar automatically disappears when you move your mouse cursor away from it, or execute a certain commands on the Smart Bar, or click somewhere else in the code editor window, emptying the selection.
To make the Smart Bar reappear, you can tap the Shift key on your keyboard twice within a second. This behavior can also be suppressed by unchecking the Show/hide Smart Bar with Shift key checkbox.
Smart Bar in Other Windows
Smart Bar also works on Output, C# Interactive, Immediate (Debug), Find Results and some other text selectable window panes. If you select a path within those windows, extra commands will pop up allowing you to open it directly or locate it in Windows Explorer.

From version 4.4 on, some extra buttons will show up on Smart Bar in C/C++ code windows.
Scrollbar Marker
Scollbar Marker draws extra glyphs and shapes on the vertical scrollbar for the following syntax elements:


C# class/struct/interface/enum declarations (marked with a square and their names)


C# symbol match marker (matches symbol under the caret, marked with an aqua square)


C# instructions (#if, #else, #region, #pragma) (marked with a gray spot)


Line numbers (marked with gray dashed lines and numbers)


Special comments tagged by comment tagger (marked with small squares)
Please see the first screenshot of this article.


Navigation Bar
Navigation bar locates at the top of the code editor window. It overrides the original navigation bar. When the Navigation Bar is loadeded, it hides two drop-down lists on the original Navigation Bar, but preserves the project drop-down list.
Basically, the Navigation Bar serves the same purpose of the original one, displaying symbol information where the caret is placed.

Note: currently Navigation Bar only works with C# code documents.
Typically you can see three or four items on the bar.

Namespace node: the innermost namespace which contains the caret. On the above screen shot, it is the ""TestProject"" node.
Type node: the type which contains the caret. On the above screen shot, it is the ""MyStruct"" node.
Region node: when the caret is between #region and #endregion, this node appears. On the above screen shot, it is the ""Private fields"" node.
Member node: the member where the caret is in. This node is drawn highlighted. On the above screen shot, it is the ""Constant"" node.

Nodes on the Navigation Bar are clickable.


Clicking on a Namespace node will popup a menu, displaying namespaces and types defined in the active document. You can click on those items and jump to the beginning of corresponding definitions.

On top of the menu there is a Search Declaration box, within which you can type and search declarations.
Besides the Search Declaration box, there are three buttons. The first one is pressed by default, which restricts the search scope to active document. If the second one is pressed, it pops up the first button and expands the search scope to current project (see screen shot below). The third button clears the search box and reverts the items back to unfiltered namespaces and types.



Clicking on a Type node will popup a menu, displaying members and regions defined within the type. You can click on those items and jump to the definition of the corresponding member.

The current symbol where the caret is on is highlighted.
Field values and auto-property expressions are also displayed on this menu. So, you can instantly know the initial value of fields.
There is also a search box in this menu, which filters content of the menu. There are six buttons beside the search box. The first five of them narrow down the displayed items of the menu and the last one clears the filter.
To navigate to the beginning of the type, click the first item on the menu.


Clicking on a Member node will select the whole member. If you have the Smart Bar feature on and let it appear when selection is changed, Smart Bar will be displayed and let you perform actions onto the member.



Customization
The Navigation Bar can be configure via the options page.



If Show syntax detail option is set, the Navigation Bar not only shows available types and declarations in the code window like the original navigation bar, but also syntax nodes such as statements and expressions containing the caret.



If Show symbol info tip option is set, you can read information about a symbol when you hover your mouse onto a node.



If Highlight node range in editor option is set, when you hover the mouse over the node on the bar, corresponding span of the node will be highlighted in the editor.



If Show #region name option is set, #region names will be displayed on the Navigation Bar. If you pad region names with some non-alphabetic characters like ""#region [====== private methods ======]"", you can check the Trim non-letter characters checkbox so only alphabetic part like ""private methods"" will be displayed on the Navigation Bar.


To customize drop-down menus of the Navigation Bar, change options in the Drop-down Menu tab.

Display Enhancements
In the Display tab of the General options page, several display enhancement options are offered.

Within the Extra line margins group box, you can adjust margins between lines to make code lines more readable.
Programmers who do not like ClearType rendering, which made text blurry and colorful, may want to try Force Grayscale Text Rendering options.
Feature Control
Open the Codist section in the Tools->Options dialog. In the General section you can toggle features of Codist.



Feature controllers contains check boxes which can be used to enable/disable features of Codist.
When you are running on a laptop with battery. Disabling Codist may help it sustain a little bit longer.
Someone who does not like the syntax highlight or use another syntax highlighter can also turn off the Syntax Highlight feature individually here.
These options will take effect on new document windows. Existing document windows won't be affected.


To share or backup your settings of Codist, you can use the Save and Load buttons.


Acknowledgements
I have learned a lot from the following extension projects (sorted by the dates when I learned from them).

CommentsPlus: https://github.com/mhoumann/CommentsPlus
Better comments: https://github.com/omsharp/BetterComments
Remarker: https://github.com/jgyo/remarker
Font Sizer: https://github.com/Oceanware/FontSizer
Visual Studio Productivity Power Tools: https://github.com/Microsoft/VS-PPT
Inheritance Margin: https://github.com/tunnelvisionlabs/InheritanceMargin
CoCo: https://github.com/GeorgeAlexandria/CoCo
CodeBlockEndTag: https://github.com/KhaosCoders/VSCodeBlockEndTag
UntabifyReplacement: https://github.com/cpmcgrath/UntabifyReplacement
Extensiblity Tools: https://github.com/madskristensen/ExtensibilityTools
CodeMaid: https://github.com/codecadwallader/codemaid
Select Next Occurence: https://github.com/2mas/SelectNextOccurrence

License
Codist comes from the open source community and it goes back to the community.
Codist is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
You should have received a copy of the GNU General Public License along with this program. If not, see ""https://www.gnu.org/licenses"".
Bugs and Suggestions
Please post New Issue in the GitHub project if you find any bug or have any suggestion.
Your vote and feedback on the Visual Studio Extension Marketplace are also welcomed.
Support Codist by Donation
If you like Codist and want to support the future development of it, you can donate to the author.
You can donate any amount of money as you like. The recommended amount of donation is $19.99.
",30
devsnek/slither,Rust,"slither
A modern scripting runtime
// modules keep your code organized
// and your global clean
import { print } from standard:debug;

function fib(n, a = 0, b = 1) { // argument initializers
  if n == 0 { // no parenthesis around if or try
    return a;
  }

  if n == 1 {
    return // unambiguous grammar means
      b;   // that this returns b, not null
  }

  // tail recursion
  return fib(n - 1, b, a + b);
}

print(fib(10) == 55);
Goals in no particular order

staged JIT for good performance
fast and easy networking
good ffi interface

",28
nytimes/video-transcoding-api,Go,"
Video Transcoding API



The Video Transcoding API provides an agnostic API to transcode media assets
across different cloud services. Currently, it supports the following
providers:

Amazon Elastic Transcoder
Bitmovin
Elemental Conductor
Encoding.com
Hybrik
Zencoder

Setting Up
With latest Go installed, make sure to export the follow
environment variables:
Providers configuration
For Amazon Elastic Transcoder
export AWS_ACCESS_KEY_ID=your.access.key.id
export AWS_SECRET_ACCESS_KEY=your.secret.access.key
export AWS_REGION=""us-east-1""
export ELASTICTRANSCODER_PIPELINE_ID=""yourpipeline-id""

Please notice that for Elastic Transcoder you don't specify the destination
bucket, as it is defined in the Elastic Transcoder
Pipeline.
For Bitmovin
export BITMOVIN_API_KEY=your.api.key
export BITMOVIN_AWS_ACCESS_KEY_ID=your.access.key.id
export BITMOVIN_AWS_SECRET_ACCESS_KEY=your.secret.access.key
export BITMOVIN_AWS_STORAGE_REGION=your.s3.region.such.as.US_EAST_1.or.EU_WEST_1
export BITMOVIN_DESTINATION=s3://your-s3-bucket
export BITMOVIN_ENCODING_REGION=your.provider.region.such.as.AWS_US_EAST_1.or.GOOGLE_EUROPE_WEST_1
export BITMOVIN_ENCODING_VERSION=STABLE.or.BETA

For Elemental Conductor
export ELEMENTALCONDUCTOR_HOST=https://conductor-address.cloud.elementaltechnologies.com/
export ELEMENTALCONDUCTOR_USER_LOGIN=your.login
export ELEMENTALCONDUCTOR_API_KEY=your.api.key
export ELEMENTALCONDUCTOR_AUTH_EXPIRES=30
export ELEMENTALCONDUCTOR_AWS_ACCESS_KEY_ID=your.access.key.id
export ELEMENTALCONDUCTOR_AWS_SECRET_ACCESS_KEY=your.secret.access.key
export ELEMENTALCONDUCTOR_DESTINATION=s3://your-s3-bucket/

For Encoding.com
export ENCODINGCOM_USER_ID=your.user.id
export ENCODINGCOM_USER_KEY=your.user.key
export ENCODINGCOM_DESTINATION=http://access.key.id:secret.access.key@your-s3-bucket.s3.amazonaws.com/
export ENCODINGCOM_REGION=""us-east-1""

For Hybrik
export HYBRIK_URL=your.hybrik.api.endpoint.such.as.https://api_demo.hybrik.com/v1
export HYBRIK_COMPLIANCE_DATE=20170601
export HYBRIK_OAPI_KEY=your.hybrik.oapi.key
export HYBRIK_OAPI_SECRET=your.hybrik.oapi.secret
export HYBRIK_AUTH_KEY=your.hybrik.auth.key
export HYBRIK_AUTH_SECRET=your.hybrik.auth.secret
export HYBRIK_DESTINATION=s3://your-s3-bucket
export HYBRIK_PRESET_PATH=video-transcoding-api-presets

HYBRIK_PRESET_PATH is optional and defines the folder presets will be
stored in. If not specified, it will default to
'video-transcoding-api-presets'.
For Zencoder
export ZENCODER_API_KEY=your.api.key
export ZENCODER_DESTINATION=http://access.key.id:secret.access.key@your-s3-bucket.s3.amazonaws.com/

Database configuration
In order to store preset maps and job statuses we need a Redis instance
running. Learn how to setup and run a Redis
here. With the Redis instance running, set
its configuration variables:
export REDIS_ADDR=192.0.2.31
export REDIS_PASSWORD=p4ssw0rd.here

If you are running Redis in the same host of the API and on the default port
(6379) the API will automatically find the instance and connect to it.
With all environment variables set and redis up and running, clone this
repository and run:
$ git clone https://github.com/NYTimes/video-transcoding-api.git
$ make run

Running tests
$ make test

Using the API
Check out on our Wiki how
to
use this API.
Contributing

Fork it
Create your feature branch: git checkout -b my-awesome-new-feature
Commit your changes: git commit -m 'Add some awesome feature'
Push to the branch: git push origin my-awesome-new-feature
Submit a pull request

License

This code is under Apache 2.0
license.
The video-transcoding-api logo is a variation on the Go gopher that was
designed by Renee French and copyrighted under the Creative Commons
Attribution 3.0 license.

",443
saucepleez/taskt,C#,"
taskt (formerly sharpRPA) is the first truly free, easy to use, and open-source process automation client built on the .NET Framework in C#.  taskt allows you to build and design process automation without needing to write application code.


taskt allows you to automate the boring stuff and create efficienies by giving you the power to craft a digital workforce that executes and performs rule-based automation.  No API? No Problem!  Included is a ""what you see is what you get"" bot designer with dozens of automation commands. An element recorder and screen recorder is also included that can record and replay scripted automation.


taskt works by allowing a bot developer to design a bot configuration known as a script.  The bot configuration is then intepreted by a script engine at run-time and executes against the bot developer's selected parameter inputs.  Each command contains the definitions for the required inputs as well as the required logic at run-time.  Please check out the Wiki for basic documenation surrounding the application and the available commands


taskt can perform automation on both web and desktop applications, simulating the actions a person would do.  From swivel-chair data entry to report generation, taskt can handle your automation needs.  Prefer to write and implement code?  taskt can use youe existing .NET DLLs and services additionally with the ability to compile code on the fly using the Custom Code command! taskt can start and stop processes, launch VB and PowerShell scripts, work directly with Excel workbooks, and perform OCR (OneNote installation required) among many other functions.  You can review all the automation commands by clicking here.

CURRENTLY IN ALPHA Manage and orchestrate your digital workforce with taskt's optional server component that allows you to publish and execute tasks remotely as well as monitor the overall health of your bots and discover metrics around your robot workers.


Find and download the latest signed release by clicking HERE. Extract to any folder and double-click 'taskt.exe'.  taskt will ask if you want to create a scripts folder to store your scripts as well as copy and deploy sample files.  You can also build directly from source -- take the latest from the master branch!

taskt is free for both personal and commercial use. taskt is licensed under the Apache 2.0 License -- see LICENSE.md for further details. As a community-driven project, the goal of taskt is to give everyone, big or small, the ability to build and deploy process automation.

Feel free to open up a feature request or report a bug/issue.
Click here to open a new issue!
Need support? Want to say Hi? Come chat with us on Gitter!
",78
eugenekolo/github-scripts,Shell,"Awesome Stars 

A curated list of my GitHub stars!  Generated by starred

Contents

Java
Scala
JavaScript
Makefile
Perl
[Jupyter Notebook](#jupyter notebook)
Matlab
Shell
Assembly
Python
HTML
QML
Others
Ruby
C
LLVM
C++
[Vim script](#vim script)
CSS
Swift
OCaml
C#
Go
PHP

Java

CTF-Android-Writeup - 很久以前参加CTF比赛做出来的部分Android逆向题目wp（瞎写，自用记录）
santa-tracker-android - Ho Ho Ho
kololib - Library of tools, testng things, and bring up material I use.
Apktool - A tool for reverse engineering Android apk files
binnavi - BinNavi is a binary analysis IDE that allows to inspect, navigate, edit and annotate control flow graphs and call graphs of disassembled code.
Silence - A fork of Signal with only SMS/MMS encryption.
clipcaster - A LastPass clipboard password sniffer

Scala

bfg-repo-cleaner - Removes large or troublesome blobs like git-filter-branch does, but faster. And written in Scala

JavaScript

puppeteer-recorder - Puppeteer recorder is a Chrome extension that records your browser interactions and generates a  Puppeteer script.
appmon - Documentation:
house - A runtime mobile application analysis toolkit with a Web GUI, powered by Frida, written in Python.
pwnjs - A Javascript library for browser exploitation
juice-shop - OWASP Juice Shop: Probably the most modern and sophisticated insecure web application
frida-ios-dump - pull decrypted ipa from jailbreak device
dual-captions - 🌐 Subtitles in two languages for YouTube & Netflix
racer - One-click utility to test race conditions
insomnia - Cross-platform HTTP and GraphQL Client
pibakery - The blocks based, easy to use setup tool for Raspberry Pi
You-Dont-Need-jQuery - Examples of how to do query, style, dom, ajax, event etc like jQuery with plain javascript.
You-Dont-Need-Lodash-Underscore - List of JavaScript methods which you can use natively + ESLint Plugin
aggle - Course project for EC500 Agile Software Development for ECE Applications.
Ghost - 👻 The most popular headless Node.js CMS for professional publishing
atom-pair - An Atom package that allows for epic pair programming
UnuglifyJS - A simpler open-source version of JavaScript deobfuscator JSNice
javascript - JavaScript Style Guide
FuckAdBlock - Detects ad blockers (AdBlock, ...)
rodeo - A data science IDE for Python
WhoAmI - A mind-reading website.
shapdar - shape radar w/ html5 + js

Makefile

frida - Clone this repo to build Frida
srclib-c -
memeshop - exploit challenge from csaw ctf qualifier 2015

Perl

diff-so-fancy - Good-lookin' diffs. Actually… nah… The best-lookin' diffs. 🎉
RegRipper2.8 - RegRipper version 2.8
Academic-Writing-Check - check for passive words, weasel words, duplicate words, typographical errors and words strunk & white don't like
perl5 - The Perl 5 language interpreter (MIRROR ONLY)
watson-perl -

Jupyter Notebook

pytudes - Python programs to practice or demonstrate skills.

Matlab

hbridge - Wireless MSP430 microcontroller hooked up to a small RC car with a MATLAB UI for control

Shell

Zines - Mirror of my favourite hacking Zines for the lulz, nostalgy, and reference
security-tools - Collection of small security tools created mostly in Python. CTFs, pentests and so on
makeself - A self-extracting archiving tool for Unix systems, in 100% shell script.
vbox - Easier to use wrapper around vboxmanage
github-scripts - Some GitHub scripts
ctf-tools - Some setup scripts for security research tools.

Assembly

PwnAdventureZ - NES zombie survival game made to be hacked
REpsych - Psychological warfare in reverse engineering

Python

amazon-dash - Hack your Amazon Dash to run what you want.
collisions - Hash collisions
bfac - BFAC (Backup File Artifacts Checker): An automated tool that checks for backup artifacts that may disclose the web-application's source code.
on-pwning - My solutions to some CTF challenges and a list of interesting resources about pwning stuff
awesome-python-applications - 💿 Free software that works great, and also happens to be open-source Python.
pigaios - A tool for matching and diffing source codes directly against binaries.
internalblue - Bluetooth experimentation framework based on Reverse Engineering of Broadcom Bluetooth Controllers.
T-Fuzz -
GitCTF - Git-based CTF
diaphora - Diaphora, the most advanced Free and Open Source program diffing tool.
AndBug - Android Debugging Library
XSStrike - Most advanced XSS scanner.
IDA-Function-Tagger - This IDAPython script tags subroutines according to their use of imported functions
FRAPL - FRAPL Framework
ida_ea - A set of exploitation/reversing aids for IDA
prefix - Function Prefixing for IDA Pro
gdbida - gdbida - a visual bridge between a GDB session and IDA Pro's disassembler
iOS-AppStore-Malware-Automatic-Hunting-System - Blackhat USA 2018 Arsenal
SublimeTodoReview - A SublimeText plugin for reviewing todo (and other) comments within your code.
HexRaysPyTools -
pyxel - A retro game engine for Python
xsssniper - An automatic XSS discovery tool
uEmu - Tiny cute emulator plugin for IDA based on unicorn.
firmware-analysis-toolkit - Toolkit to emulate firmware and analyse it for security vulnerabilities
pwn_repo - To store some CTF_pwn_bins and exps
gef - GEF - GDB Enhanced Features for exploit devs & reversers
zeropress - A dumb script for finding dumb coding errors in WordPress plugins
Zeratool - Automatic Exploit Generation (AEG) and remote flag capture for exploitable CTF problems
theftfuzzer - TheftFuzzer is a tool that fuzzes Cross-Origin Resource Sharing implementations for common misconfigurations.
WPSeku - WPSeku - Wordpress Security Scanner
PinCTF - Using Intel's PIN tool to solve CTF problems
Pwngdb - gdb for pwn
PayloadsAllTheThings - A list of useful payloads and bypass for Web Application Security and Pentest/CTF
focuson - A tool to surface security issues in python code
pre-commit-python-sorter - A pre-commit hook to sort your Python imports.
hqtrivia-automation - Automate finding better answers in HQ Trivia. This is for educational purposes only!
hqtrivia - hack HQ trivia with OCR and google search
Sublist3r - Fast subdomains enumeration tool for penetration testers
ida -
ctf-crypto-writeups -
dash - Analytical Web Apps for Python. No JavaScript Required.
ctfs - ctf exploit codes or writeups
LazyIDA - Make your IDA Lazy!
domato - DOM fuzzer
truffleHog - Searches through git repositories for high entropy strings and secrets, digging deep into commit history
interactive-coding-challenges - 120+ interactive Python coding interview challenges (algorithms and data structures).  Includes Anki flashcards.
Mailpile - A free & open modern, fast email client with user-friendly encryption and privacy features
peda-heap - Some new commands debug heap for peda
stuffz - Basically a script thrift shop
persepolis - Persepolis Download Manager is a GUI for aria2.
QTodoTxt - Cross Platform todo.txt GUI
Openroast - An open source, cross-platform application for home coffee roasting
PPlayer - Music Player code by Python 2.7 and PyQt. MP3 and WMA format are supported.
simple-markpad - a markdown editor made with pyqt
qhangups - Alternative client for Google Hangouts written in PyQt
manticore - Symbolic execution tool
RSAExploits -
ctf-writeups - Selected CTF writeups
imgkit - 🎆 Wkhtmltoimage python wrapper to convert html to image
python-patterns - A collection of design patterns/idioms in Python
pyshell - PyShell makes interacting with web-based command injection less painful, emulating the feel of an interactive shell as much as possible.
metame - metame is a metamorphic code engine for arbitrary executables
PS4-3.55-Code-Execution-PoC -
flare-floss - FireEye Labs Obfuscated String Solver - Automatically extract obfuscated strings from malware.
chosen-plaintext - A small python library for exploiting simple chosen-plaintext attacks.
linux-insides - A little bit about a linux kernel
ec700-charlie-3 -
CANToolz - CANToolz - framework for black-box CAN network analysis
osxcollector - A forensic evidence collection & analysis toolkit for OS X
flask-user-api -
workshops -
crypto-tools - Some crypto tools I've written
maybe - 📂 🐇 🎩 See what a program does before deciding whether you really want it to happen (NO LONGER MAINTAINED)
linux-ransomware-decrypter - Bitdefender's Linux.Encoder.1 Decrypter
ida-patcher - IDA Patcher is a plugin for Hex-Ray's IDA Pro disassembler designed to enhance IDA's ability to patch binary files and memory.
ete - Python package for building, comparing, annotating, manipulating and visualising trees. It provides a comprehensive API and a collection of command line tools, including utilities to work with the NCBI taxonomy tree.
project-euler - Project Euler solutions in diferent languages
viper - Binary analysis and management framework
libc-binary-collection - A collection of more than 1000 binary libc files
grequests - Requests + Gevent = <3
pwntools-write-ups - A colleciton of CTF write-ups all using pwntools
xortool - A tool to analyze multi-byte xor cipher
pwntools - CTF framework and exploit development library
SimplyEmail - Email recon made fast and easy, with a framework to build on
schedule - Python job scheduling for humans.
num2words - Modules to convert numbers to words. 42 --> forty-two
ARDT - Akamai Reflective DDoS Tool - Attack the origin host behind the Akamai Edge hosts and DDoS protection offered by Akamai services.
pupy - Pupy is an opensource, cross-platform (Windows, Linux, OSX, Android) remote administration and post-exploitation tool mainly written in python
saws - A supercharged AWS command line interface (CLI).
robobrowser -
gdb-dashboard - Modular visual interface for GDB in Python
Pylsy - Pylsy is a simple python library draw tables in the Terminal.Just two lines of code .
big-list-of-naughty-strings - The Big List of Naughty Strings is a list of strings which have a high probability of causing issues when used as user-input data.
python-magic - A python wrapper for libmagic
pwndbg - Exploit Development and Reverse Engineering with GDB Made Easy
net-creds - Sniffs sensitive data from interface or pcap

HTML

MyArticles - 蒸米的文章（iOS冰与火之歌系列，一步一步学ROP系列，安卓动态调试七种武器系列等）
ctf-challenges -
research - A repo for various research
Droid-Application-Fuzz-Framework - Android application fuzzing framework with fuzzers and crash monitor.
owasp-mstg - The Mobile Security Testing Guide (MSTG) is a comprehensive manual for mobile app security testing and reverse engineering.
en.javascript.info - Modern JavaScript Tutorial
CTF-Writeups -
sec-tools - A set of security related tools
highlighter.js - Easily navigate the DOM and highlight the elements - http://720kb.github.io/highlighter.js/
vector-exploit - Exploit repository
kitsu-tools - 🔨 The tools we use to build Kitsu, the coolest platform for anime and manga

QML

eugenekolo-ctf-challenges - CTF challenges I've made in the past

Others

fuzzing_paper - puzzing related paper
browser-pwn - An updated collection of resources targeting browser-exploitation.
ios-resources - Useful resources for iOS hacking
awesome-interview-questions -  A curated awesome list of lists of interview questions. Feel free to contribute! 🎓
slides - won't maintain
awesome-browser-exploit - awesome list of browser exploitation tutorials
ToolsOfTheTrade - Tools of The Trade, from Hacker News.
survivingtheappstore - My book on getting to the #1 Spot in the App Store. Buy my games to support me.
Android-Malwares -
Exploit-Writeups - A collection where my current and future writeups for exploits/CTF will go
secbook - 信息安全从业者书单推荐
secure-ios-app-dev - Collection of the most common vulnerabilities found in iOS applications
Code-Execution - Executables that execute other stuff
awesome-mobile-CTF - This is a curated list of mobile based CTFs, write-ups and vulnerable apps. Most of them are android based due to the popularity of the platform.
android-crackme - You have 8 flags to retrieve.  Crack it already!
iOS - Most usable tools for iOS penetration testing
CTF-pwn-tips - Here record some tips about pwn. Something is obsoleted and won't be updated. Sorry about that.
wifi-cracking - Crack WPA/WPA2 Wi-Fi Routers with Airodump-ng and Aircrack-ng/Hashcat
mms - Modern Memory Safety in C/C++
emptyrepo - an empty repository
2015-ctf-game - Repo containing links to all CTF Challenges used in the 2015 MITRE CTF.
2014-ctf-game - Repo containing links to all CTF Challenges used in the 2014 MITRE CTF.
2013-ctf-game - Repo containing links to all CTF Challenges used in the 2013 MITRE CTF.
MobileApp-Pentest-Cheatsheet - The Mobile App Pentest cheat sheet was created to provide concise collection of high value information on specific mobile application penetration testing topics.
Malware - Course materials for Malware Analysis by RPISEC
awesome-malware-analysis - A curated list of awesome malware analysis tools and resources.
awesome-pentest - A collection of awesome penetration testing resources, tools and other shiny things

Ruby

seccomp-tools - Provide powerful tools for seccomp analysis
blue_hydra - Blue Hydra
brakeman - A static analysis security vulnerability scanner for Ruby on Rails applications
watson-ruby -

C

ctf-writeups - Collection of scripts and writeups
rbndr - Simple DNS Rebinding Service
android_vuln_poc-exp - This project contains pocs and exploits for vulneribilities I found (mostly)
HITCON-Training - For Linux binary Exploitation
Rhme-2016 - Rhme2 challenge (2016)
cb-multios - DARPA Challenges Sets for Linux, Windows, and macOS
armadito-av - Armadito antivirus main repository
ctf-training - Repository with the material of the Tower of Hanoi introductory briefings on binary exploitation
chw00t - chw00t - Unices chroot breaking tool
how2heap - A repository for learning various heap exploitation techniques.
libctf - Library for creating CTF services.
shellforge4 - Enhanced version of secdev's shellforge G3. More platforms and architectures supported.
peinjector - peinjector - MITM PE file infector
minhook - The Minimalistic x86/x64 API Hooking Library for Windows
cdefs - Describe C function prototypes in JSON.
preeny - Some helpful preload libraries for pwning stuff.
SwiftFilesZip - A repo for saving, loading, deleting and unzipping in iOS
MBE - Course materials for Modern Binary Exploitation by RPISEC
GOAT-Plugs - GCC Obfuscation Augmentation Tools
Deviare2 - Deviare API Hook
pcompress - A Parallelized Data Deduplication and Compression utility

LLVM

Tigress_protection - Playing with the Tigress binary protection. Break some of its protections and solve some of its challenges. Automatic deobfuscation using symbolic execution, taint analysis and LLVM.

C++

calculator - Windows Calculator: A simple yet powerful calculator that ships with Windows
exploit_me - Very vulnerable ARM application (CTF style exploitation tutorial)
functionsimsearch - Some C++ example code to demonstrate how to perform code similarity searches using SimHashing.
HexType - HexType: Efficient Detection of Type Confusion Errors for C++
node-memwatch - A NodeJS library to keep an eye on your memory usage, and discover and isolate leaks.
Ponce - IDA 2016 plugin contest winner! Symbolic Execution just one-click away!
devilution - Diablo devolved - magic behind the 1996 computer game
QBDI - A Dynamic Binary Instrumentation framework based on LLVM.
SimplifyGraph - IDA Pro plugin to assist with complex graphs
dxxd-decrypter - DXXD Ransomware Decrypter
al-khaser - Public malware techniques used in the wild: Virtual Machine, Emulation, Debuggers, Sandbox detection.
MazeWalker - Toolkit for enriching and speeding up static malware analysis
paybreak -
sqlitebrowser - Official home of the DB Browser for SQLite (DB4S) project. Previously known as ""SQLite Database Browser"" and ""Database Browser for SQLite"". Website at:
iaito - This project has been moved to:
hexrays_tools -
Process-Dump - Windows tool for dumping malware PE files from memory back to disk for analysis.
coho - Base libraries for C++ development
CrowdDetox -
PackerAttacker - C++ application that uses memory and code hooks to detect packers
keygenme - keygenme challenge from csaw ctf 2013
ghostwriter - ghostwriter is a cross-platform, aesthetic, distraction-free Markdown editor.
Blackbone - Windows memory hacking library
pn - Programmer's Notepad
vld - Visual Leak Detector for Visual C++ 2008-2015
cbang - C! (cbang) is a library of cross-platform C++ utilities.
packJPG - A compression program for further compressing JPEG image files

Vim script

dotfiles - home of the award winning tmux configuration file

CSS

CTF-challenges-by-me - Pwnable|Web Security|Cryptography CTF-style challenges
0xbu.github.io - 0xBU.com website source code
csp-testing - For testing browser support for Content Security Policy
cozy-youth-theme - A cozy, friendly, and readable theme for Ghost blog.
kolotheme - Dark Ghost blog theme for a developer

Swift

androidtool-mac - One-click screenshots, video recordings, app installation for iOS and Android

OCaml

infer - A static analyzer for Java, C, C++, and Objective-C

C#

NBug - Automated bug reporting library for .NET

Go

aquatone - A Tool for Domain Flyovers
grv - GRV is a terminal interface for viewing git repositories
gitrob - Reconnaissance tool for GitHub organizations
GoSublime - A Golang plugin collection for SublimeText 3, providing code completion and other IDE-like features.
pyre - tinder cli built at stupid hackathon san francisco 2015

PHP

phpggc - PHPGGC is a library of PHP unserialize() payloads along with a tool to generate them, from command line or programmatically.
vuejs-serverside-template-xss - Demo of a Vue.js app that mixes both clientside templates and serverside templates leading to an XSS vulnerability
iOS-Mail.app-inject-kit - iOS 8.3 Mail.app inject kit

License

To the extent possible under law, eugenekolo has waived all copyright and related or neighboring rights to this work.
",2
animalsheltr/animalsheltr,PHP,"








In development

AnimalSheltr
Animal Shelter Manager.
Contact
Email: jaimesares@gmail.com | Twitter: @animalsheltr
| Facebook: @animalsheltr
",9
CloudPipelines/scripts,Shell,"Cloud Pipelines Scripts

This project tries to solve the following problems:




Creation of a common deployment pipeline


Propagation of good testing & deployment practices


Speed up the time required to deploy a feature to production




A common way of running, configuring and deploying applications lowers support costs
and time needed by new developers to blend in when they change projects.


You can check out the docs folder for the documentation. The deployed docs are available here
",16
shunwuyu/lesson_md,CSS,"lesson_md
美滴班
",6
openwrt/packages,Makefile,"OpenWrt packages feed
Description
This is the OpenWrt ""packages""-feed containing community-maintained build scripts, options and patches for applications, modules and libraries used within OpenWrt.
Installation of pre-built packages is handled directly by the opkg utility within your running OpenWrt system or by using the OpenWrt SDK on a build system.
Usage
This repository is intended to be layered on-top of an OpenWrt buildroot. If you do not have an OpenWrt buildroot installed, see the documentation at: OpenWrt Buildroot – Installation on the OpenWrt support site.
This feed is enabled by default. To install all its package definitions, run:
./scripts/feeds update packages
./scripts/feeds install -a -p packages

License
See LICENSE file.
Package Guidelines
See CONTRIBUTING.md file.
",1814
malramsay64/statdyn-analysis,Python,"Statdyn Analysis




This is a set of scripts that use
Hoomd to perform the Molecular
dynamics simulations of a glass forming molecular liquid. There is a particular
focus on understanding the dynamic properties of these molecules.
Note that this is still very early alpha software and there are likely to be
large breaking changes that occur.
Installation
The simplest method of installation is using conda. To install
conda install -c malramsay statdyn-analysis

It is also possible to set the repository up as a development environment,
in which case cloning the repository and installing is possible by running
git clone https://github.com/malramsay64/statdyn-analysis.git
cd statdyn-analysis
conda env create
source activate sdanalysis-dev
python setup.py develop

Once the environment is setup the tests can be run with
pytest

Running Analysis
Dynamics of a trajectory can be computed using the command
sdanalysis comp-dynamics trajectory-Trimer-13.50-1.20.gsd

which will generate an hdf5 file of the same name containing a single table,
dynamics which has all the dynamic quantities tabulated. This also includes
a start index, over which statistics can be computed.
Finally the command
sdanalysis figure

will open up a bokeh server which will allow for the interactive visualisation
of all dump-*.gsd files in the current directory.
",4
naderghanbari/clrs-scala,Scala,"Introduction to Algorithms in Scala
You will nee Java 8 or later and sbt in
order to compile and run tests.
This project is an attempt to implement algorithms in
Introduction to algorithms, 3rd edition, CLRS, MIT Press in Scala.
In most cases the implementations are identical to the original
algorithms in the book. In some cases a functional counterpart is also
implemented. Check the docs of classes and functions for more
info per case.
Indices
The book has the convention of 1-based indices for arrays, i.e. first
element is located at index 1, except for very rare cases
where 0-based indices are used.
We follow the 0-based convention here though, as in Scala like many
other programming language, arrays are 0-based.
Test
Tests are written using ScalaTest.
Run them with the following command:
> sbt test

",2
hanickadot/compile-time-regular-expressions,C++,"Compile time regular expressions v2

Fast compile-time regular expression with support for matching/searching/capturing in compile-time or runtime.
You can use single header version from directory single-header. This header can be regenerated with make single-header.
More info at compile-time.re
What this library can do?
ctre::match<""REGEX"">(subject); // C++20
""REGEX""_ctre.match(subject); // C++17 + N3599 extension

Matching
Searching
Capturing content (named captures are supported too)

The library is implementing most of the PCRE syntax with a few exceptions:

atomic groups
boundaries other than ^$
callouts
character properties
comments
conditional patterns
control characters (\cX)
horizontal / vertical character classes (\h\H\v\V)
match point reset (\K)
named characters
octal numbers
options / modes
subroutines
unicode grapheme cluster (\X)

More documentation on pcre.org.
What can be subject (input)?

std::string-like object (std::string_view or your own string if it's providing begin/end functions with forward iterators)
pair of forward iterators

Supported compilers

clang 6.0+ (template UDL, C++17 syntax)
xcode clang 10.0+ (template UDL, C++17 syntax)
gcc 7.4+ (template UDL, C++17 syntax)
gcc 9.0+ (C++17 & C++20 cNTTP syntax)
MSVC 15.8.8+ (C++17 syntax only)

Template UDL syntax
Compiler must support N3599 extension, as GNU extension in gcc (not in GCC 9.1+) and clang.
constexpr auto match(std::string_view sv) noexcept {
	using namespace ctre::literals;
	return ""h.*""_ctre.match(sv);
}
If you need N3599 extension in GCC 9.1+ you can't use -pedantic mode and define macro CTRE_ENABLE_LITERALS.
C++17 syntax
You can provide pattern as a constexpr ctll::fixed_string variable.
static constexpr auto pattern = ctll::fixed_string{ ""h.*"" };

constexpr auto match(std::string_view sv) noexcept {
	return ctre::match<pattern>(sv);
}
(this is tested in MSVC 15.8.8)
C++20 syntax
Currently only compiler which supports cNTTP syntax ctre::match<PATTERN>(subject) is GCC 9+.
constexpr auto match(std::string_view sv) noexcept {
	return ctre::match<""h.*"">(sv);
}
Examples
Extracting number from input
std::optional<std::string_view> extract_number(std::string_view s) noexcept {
	if (auto m = ctre::match<""[a-z]+([0-9]+)"">(s)) {
        return m.get<1>().to_view();
    } else {
        return std::nullopt;
    }
}
link to compiler explorer
Extracting values from date
struct date { std::string_view year; std::string_view month; std::string_view day; };

std::optional<date> extract_date(std::string_view s) noexcept {
    using namespace ctre::literals;
    if (auto [whole, year, month, day] = ctre::match<""(\\d{4})/(\\d{1,2})/(\\d{1,2})"">(s); whole) {
        return date{year, month, day};
    } else {
        return std::nullopt;
    }
}

//static_assert(extract_date(""2018/08/27""sv).has_value());
//static_assert((*extract_date(""2018/08/27""sv)).year == ""2018""sv);
//static_assert((*extract_date(""2018/08/27""sv)).month == ""08""sv);
//static_assert((*extract_date(""2018/08/27""sv)).day == ""27""sv);
link to compiler explorer
Lexer
enum class type {
    unknown, identifier, number
};

struct lex_item {
    type t;
    std::string_view c;
};

std::optional<lex_item> lexer(std::string_view v) noexcept {
    if (auto [m,id,num] = ctre::match<""([a-z]+)|([0-9]+)"">(v); m) {
        if (id) {
            return lex_item{type::identifier, id};
        } else if (num) {
            return lex_item{type::number, num};
        }
    }
    return std::nullopt;
}
link to compiler explorer
Range over input
This support is preliminary and probably the API will be changed.
auto input = ""123,456,768""sv;

for (auto match: ctre::range<""([0-9]+),?"">(input)) {
	std::cout << std::string_view{match.get<0>()} << ""\n"";
}
",725
gosuri/uitable,Go,"uitable  
uitable is a go library for representing data as tables for terminal applications. It provides primitives for sizing and wrapping columns to improve readability.
Example Usage
Full source code for the example is available at example/main.go
table := uitable.New()
table.MaxColWidth = 50

table.AddRow(""NAME"", ""BIRTHDAY"", ""BIO"")
for _, hacker := range hackers {
  table.AddRow(hacker.Name, hacker.Birthday, hacker.Bio)
}
fmt.Println(table)
Will render the data as:
NAME          BIRTHDAY          BIO
Ada Lovelace  December 10, 1815 Ada was a British mathematician and writer, chi...
Alan Turing   June 23, 1912     Alan was a British pioneering computer scientis...
For wrapping in two columns:
table = uitable.New()
table.MaxColWidth = 80
table.Wrap = true // wrap columns

for _, hacker := range hackers {
  table.AddRow(""Name:"", hacker.Name)
  table.AddRow(""Birthday:"", hacker.Birthday)
  table.AddRow(""Bio:"", hacker.Bio)
  table.AddRow("""") // blank
}
fmt.Println(table)
Will render the data as:
Name:     Ada Lovelace
Birthday: December 10, 1815
Bio:      Ada was a British mathematician and writer, chiefly known for her work on
          Charles Babbage's early mechanical general-purpose computer, the Analytical
          Engine

Name:     Alan Turing
Birthday: June 23, 1912
Bio:      Alan was a British pioneering computer scientist, mathematician, logician,
          cryptanalyst and theoretical biologist

Installation
$ go get -v github.com/gosuri/uitable


",475
HussainiLab/BatchTINTV3,Python,"BatchTINTV3
BatchTINT is A GUI created by the Taub Institute in order to create an end-user friendly batch processing solution to complement Axona's new command line modification of TINT.
This GUI will allow the user to define a directory. Within this directory it will be continuously (unless closed) searching for new files to analyze via Tint. The user simply drags a folder containing the appropriate '.set', '.eegx', '.pos', and '.x' tetrode files and the GUI will automatically detect these files and take care of the rest.
Requirements
Operating System: Windows
This GUI was created with the Windows operating system in mind. Therefore, it is untested on Mac OSX, as well as Linux. It is possible
(though unlikely) that it would work on other operating systems.
Suggested: Use of Python 3.4.4, PyQt4 is most compatible with this version of Python, however there are Python 3.5 versions that may work
Python 3.4.4 can be downloaded here as well: https://www.python.org/downloads/release/python-344/
Installation


For those that do not have GitHub installed on their desktop, they will need to install the GitHub Desktop Application:
https://desktop.github.com/


The next step would be to open the Command Prompt


In the Command Prompt navigate to the folder where you want to clone the repository (type in: cd folder_path).


If you want it on your Desktop type in the following: cd ""C:\Users\ [user name]\Desktop\""

Then your next step will be to clone the repository by typing in the following to your Command Prompt:

git clone https://github.com/ephyslab/BatchTINTV3.git

Note: This may take a few minutes. If there is an error produced by the Command Prompt saying the following:
'git' is not recognized as an internal or external command, operable program or batch file.
ensure that you have added the appropriate git locations to the path system variable. Follow these steps in order to add system variables
a) First. you are going to need to find the location of git-core and copy the path, it will be similar to the following:
C:\Users\ [Your Username]\AppData\Local\GitHub\ [Something Similar to PortableGit_25d850739bc178b2eb13c3e2a9faafea2f9143c0]\mingw32\libexec\git-core
b) Second, you will need to find the path of the 'bin' folder within the 'mingw32' folder which will look similar to the following:
C:\Users\ [your name]\AppData\Local\GitHub\ [Something Similar to PortableGit_25d850739bc178b2eb13c3e2a9faafea2f9143c0]\mingw32\bin
c) Go to the following window: Control Panel -> System and Security -> System -> Advanced Systems Settings
-> Advanced Tab (if not already on it) and -> Environmental Variables
d) Under 'System variables' edit the 'Path' variable
e) If the variable is a single line of paths then append the two paths to the end of the variable,
you will need to add a semicolon to separate each path (existing path;git-core path;bin path).
If the window you are looking at has a list of the different paths then you will simply add the two paths separately.
f) Click apply/okay to confirm the addition of these paths, close the existing Command Prompt, open a new Command Prompt and the commands will now be available

If you are using Python 3.4.4 I have included two wheel (.whl) files in the GitHub repository for PyQt4 that will need
to be installed via the Command Prompt.

If you have not added the Python 3.4.4 path to the system variable (as done for the GitHub path) then you will have to do the same.
Add the following paths to the 'path' system variable:
C:\Python34\Scripts
C:\Python34
You should also upgrade the 'pip' python script which allows for the downloading of python libraries. To upgrade pip, type the
following into the Command Prompt (remember to close and re-open the Command Prompt after adding the system variable):
python -m pip install --upgrade pip
Now you can type the following into the command prompt to install PyQt4:
python -m pip install [wheel file path] 
If you have spaces in your wheel file path make sure to surround the path by quotes
example:
python -m pip install ""C:\Users\My Name\Desktop\GitHub\file.whl""
The wheel files are the following:
64-bit PC: use the PyQt4-4.11.4-cp34-none-win_amd64.whl files
32-bit PC: use the PyQt4-4.11.4-cp34-none-win32.whl file
Note: if you are using Python 3.5 you can find those .whl files here: http://www.lfd.uci.edu/~gohlke/pythonlibs/#pyqt4

Install the python library responsible for some of the images on the GUI

Type in the following into the Command Prompt:
python -m pip install pillow

You will also have to add Tint to the system variable as we did before with GitHub and Python

Add the following path for 64-bit systems: C:\Program Files (x86)\Axona\Tint
32-bit systems should be the following: C:\Program Files\Axona\Tint
Running GUI
Now in your Command Prompt you can type the following in order to run the GUI:
The easiest way to run it is to create a '.bat' file on the desktop that contains the following information:
cd ""[pathway to your BatchTINT folder]""
python BatchSort.py
exit

When you click this .bat file it will run the program.
Now you can see a main window of the GUI that states the current directory (if it's your first time opening the program, it will say
there is no directory chosen), as a few checkboxes, and a few buttons at the bottom. You are going to want to click the 'choose directory' button and navigate to a directory that this program will analyze.
If you do not click the 'apply' button on the Choose Directory window, the directory will not be applied, so make sure you click 'apply' and not 'back'
Choose the directory based off of the following modes:

Batch: This you need to choose a directory that contains sub-directories with the sessions you want to analyze.

Chosen Directory
    └── Session 1
    |      └── session_file.set
    |      └── session_file.pos
    |      └── ...
    └── Session 2
    |      └── session_file2.set
    |      └── session_file2.pos
    |      └── ...
    |    
    └── session_file3.set
    └── session_file3.pos
    └── ...
    
In the above example, it will convert the sub-directories 'Session 1' and 'Session 2', but not the session files directly within the chosen directory (session_file3)


Non Batch: You will choose the directory that directly contains the session files that you want to convert. Thus using the same example above, it will skip the files within sub-directories 'Session 1' and 'Session 2', and only look for those files directly within the Chosen Directory, thus session_file3 will be chosen. Note: if you want this mode, click the Non-Batch checkbox

Once a directory is chosen, you will see the Queue populate a list of sessions that are able to be converted (if it has already been converted, it won't be on this list). You can re-order this list if you want certain files to be converted first using the Move Up/Down buttons.
You should now look at the Klusta Settings (by clicking the Klusta Settings button) and look through the basic/advanced settings options. The format should look familiar as it is a replica (almost) of that which you've seen while using Tint.
It is important to change the Number of Tetrodes option in the 'basic' tab. This will help the GUI look for the tetrode data. Our lab uses both 4 and 8 tetrode drives therefore the default for this GUI was set to 8. This number does not need to be exact, but it needs to be greater than or equal to the number of tetrodes you used in the files you are analyzing. If you have 8 tetrodes but the field has the number 4 filled in, it will only analyze the first four tetrodes (if they exist in the folder). It will skip any non-existing tetrodes.
Once these settings have been applied, the values will be saved for the next time you open up the GUI.
There is also the option if you want Tint to run in ""silent"" mode or be ""visible"". There is a Run Silently checkbox on main window of the GUI that you will be able to check. If it is checked everything will run in the background.
You can press Run if all the settings are correct. A Processed folder will be created within the directory you chose which will have your newly created .CUT files.
Authors

Geoff Barrett - Geoff’s GitHub

License
This project is licensed under the GNU  General  Public  License - see the LICENSE.md file for details
",3
ArduPilot/ardupilot,C++,"ArduPilot Project

  


The ArduPilot project is made up of:


ArduCopter (or APM:Copter) : code, wiki


ArduPlane (or APM:Plane) : code, wiki


ArduRover (or APMrover2) : code, wiki


ArduSub (or APM:Sub) : code, wiki


Antenna Tracker : code, wiki


User Support & Discussion Forums


Support Forum: http://discuss.ardupilot.org/


Community Site: http://ardupilot.org


Developer Information


Github repository: https://github.com/ArduPilot/ardupilot


Main developer wiki: http://dev.ardupilot.org


Developer discussion: http://discuss.ardupilot.org


Developer chat: https://gitter.im/ArduPilot/ardupilot


Top Contributors

Flight code contributors
Wiki contributors
Most active support forum users
Partners who contribute financially

How To Get Involved


The ArduPilot project is open source and we encourage participation and code contributions: guidelines for contributors to the ardupilot codebase


We have an active group of Beta Testers especially for ArduCopter to help us find bugs: release procedures


Desired Enhancements and Bugs can be posted to the issues list.


Help other users with log analysis in the support forums


Improve the wiki and chat with other wiki editors on Gitter


Contact the developers on one of the communication channels


License
The ArduPilot project is licensed under the GNU General Public
License, version 3.


Overview of license


Full Text


Maintainers
Ardupilot is comprised of several parts, vehicles and boards. The list below
contains the people that regularly contribute to the project and are responsible
for reviewing patches on their specific area.  See also the list of developers with merge rights.

Andrew Tridgell:

Vehicle: Plane, AntennaTracker
Board: APM1, APM2, Pixhawk, Pixhawk2, PixRacer


Francisco Ferreira:

Bug Master


Grant Morphett:

Vehicle: Rover


Jacob Walser:

Vehicle: Sub


Lucas De Marchi:

Subsystem: Linux


Michael du Breuil:

Subsystem: Batteries
Subsystem: GPS
Subsystem: Scripting


Peter Barker:

Subsystem: DataFlash, Tools


Randy Mackay:

Vehicle: Copter, Rover, AntennaTracker


Tom Pittenger:

Vehicle: Plane


Bill Geyer:

Vehicle: TradHeli


Chris Olson:

Vehicle: TradHeli


Emile Castelnuovo:

Board: VRBrain


Eugene Shamaev:

Subsystem: CAN bus
Subsystem: UAVCAN


Georgii Staroselskii:

Board: NavIO


Gustavo José de Sousa:

Subsystem: Build system


Julien Beraud:

Board: Bebop & Bebop 2


Leonard Hall:

Subsystem: Copter attitude control and navigation


Matt Lawrence:

Vehicle: 3DR Solo & Solo based vehicles


Matthias Badaire:

Subsystem: FRSky


Mirko Denecke:

Board: BBBmini, BeagleBone Blue, PocketPilot


Paul Riseborough:

Subsystem: AP_NavEKF2
Subsystem: AP_NavEKF3


Pierre Kancir:

Subsystem: Copter SITL, Rover SITL


Víctor Mayoral Vilches:

Board: PXF, Erle-Brain 2, PXFmini


Amilcar Lucas:

Subsystem: Marvelmind



",3978
php/php-src,C,"




The PHP Interpreter
PHP is a popular general-purpose scripting language that is especially suited to
web development. Fast, flexible and pragmatic, PHP powers everything from your
blog to the most popular websites in the world. PHP is distributed under the PHP
License v3.01.


Documentation
The PHP manual is available at php.net/docs.
Installation
Prebuilt packages and binaries
Prebuilt packages and binaries can be used to get up and running fast with PHP.
For Windows, the PHP binaries can be obtained from
windows.php.net. After extracting the archive the
*.exe files are ready to use.
For other systems, see the installation chapter.
Building PHP source code
For Windows, see Build your own PHP on Windows.
PHP uses autotools on Unix systems to configure the build:
./buildconf
./configure [options]

See ./configure -h for configuration options.
make [options]

See make -h for make options.
The -j option shall set the maximum number of jobs make can use for the
build:
make -j4

Shall run make with a maximum of 4 concurrent jobs: Generally the maximum
number of jobs should not exceed the number of cores available.
Testing PHP source code
PHP ships with an extensive test suite, the command make test is used after
successful compilation of the sources to run this test suite.
It is possible to run tests using multiple cores by setting -jN in
TEST_PHP_ARGS:
make TEST_PHP_ARGS=-j4 test

Shall run make test with a maximum of 4 concurrent jobs: Generally the maximum
number of jobs should not exceed the number of cores available.
The qa.php.net site provides more detailed info about
testing and quality assurance.
Installing PHP built from source
After a successful build (and test), PHP may be installed with:
make install

Depending on your permissions and prefix, make install may need super user
permissions.
PHP extensions
Extensions provide additional functionality on top of PHP. PHP consists of many
essential bundled extensions. Additional extensions can be found in the PHP
Extension Community Library - PECL.
Contributing
The PHP source code is located in the Git repository at
git.php.net. Contributions are most welcome by forking
the GitHub mirror repository and sending a
pull request.
Discussions are done on GitHub, but depending on the topic can also be relayed
to the official PHP developer mailing list internals@lists.php.net.
New features require an RFC and must be accepted by the developers. See
Request for comments - RFC and
Voting on PHP features for more information
on the process.
Bug fixes do not require an RFC but require a bug tracker ticket. Open a
ticket at bugs.php.net and reference the bug id using
#NNNNNN.
Fix #55371: get_magic_quotes_gpc() throws deprecation warning

After removing magic quotes, the get_magic_quotes_gpc function caused a
deprecated warning. get_magic_quotes_gpc can be used to detect the
magic_quotes behavior and therefore should not raise a warning at any time.
The patch removes this warning.

Pull requests are not merged directly on GitHub. All PRs will be pulled and
pushed through git.php.net. See
Git workflow for more details.
Guidelines for contributors
See further documents in the repository for more information on how to
contribute:

Contributing to PHP
PHP coding standards
Mailinglist rules
PHP release process

Credits
For the list of people who've put work into PHP, please see the
PHP credits page.
",23158
codefellows/seattle-dotnet-401d7,C#,"401 .NET Core Curriculum Overview
Resources:

Readme:
Git Ignore:

Week 1 - Basics

Exception Handling

LAB: Numbers Game


Unit Testing

LAB: ATM


System.IO

LAB: Word Guess Game


Classes, Stack/Heap, Garbage Collector

LAB: Tic-Tac-Toe


Linked Lists

LAB: Linked List Implementation
Class with JavaScript



Data Structures:
- How to Approach a DS
- String & Array manipulation
Quiz 1
Week 2 - Advanced

OOP Principles

LAB: Dealership Part 1


Interfaces

LAB: Dealership Part 2


Collections (Generic) & Enums

LAB: Lending Library


LINQ & Lambda Expressions

LAB: LINQ in Manhattan


Stacks & Queues // Recursion

LAB: Stacks and Queues Implementation



Data Structure: Linked Lists
Quiz 2
Week 3 - MVC & Entity Framework Core

MVC Intro

Fullstack MVC App


Relational Databases & schemas

Lab: System Design//Design a ERD // Schema


CRUD Intro to Entity Framework

Lab: AsyncInn Hotel Project Part 1
Create Models from DB schema
Identify Primary Keys/Foreign Keys/Composite Keys


Entity Framework part 2

LAB: EFCore Seeding/View Models/Tag Helpers


Career Coaching

Data Structure: Stacks/Queues
Quiz 3
Week 4 - Repository Design Pattern & APIs

Dependency Injection & Repository Design Pattern

Incorporate the Repository Design Pattern into lab
Introduce Singleton Design Pattern


Azure Deployment & Unit Tests

Deploy app to Azure and write basic tests


API Introduction

TODO List Part 1


API - API part 2

LAB: Making a call out to an API


Midterm Project Kickoff

Data Structures: Trees (Binary, BST, K-ary)
Quiz 4
Week 5
Midterm Project week

Build a Full CRUD web app
Build a custom API
Make the Web app call out to the API

Week 6 - Identity

Intro to E-Com project and PM Tool ""Azure DevOps""

Beginning of Sprint 1: Create a basic MVC app with database


Intro to Identity: Setup & Registration

Implement Identity API and create use registration


Login and Claims

Add Login page to E-Com and capture specific claims


Custom Policies

Create policies to customize access across the site with previous claims
END OF SPRINT 1


Career Coaching

Data Structure: Hash Tables
Quiz 5
Azure Dev Ops

Sprint Backlog
Create User stories with tasks and Acceptance tests
Task Branching
Time estimation
PRs within Azure Dev Ops
Peer Reviews
Weekly Goals and Retrospectives

Week 7

View Components

SPRINT 2 START
Add a ""Basket"" component to E-Com site
System Design exercise - Integrate basket capabilities into the site with DB tables


OAUTH

Add 2 3rd party providers as login to project


AUTH.NET (How to read 3rd party Docs)

Incorporate ""fake"" payment processing into project


Razor Pages

Add in user profile & admin dashboard page as Razor Page
END OF SPRINT 2


Career Coaching

Data Structure: Graphs
Quiz 6
Week 8 - Azure

Sass//SCSS

SPRINT 3 START


Email Services

LAB: Incorporate SendGrid into project


Azure Blob Storage

LAB: Save all product images in Azure Blob Storage


Web Security

OWASP security Requirements
HTTP and SSL
GDPR
LAB: Vulnerability Report
END OF SPRINT 3


Review Day

Data Structure: Sorting Algorithms
Quiz 7
Week 9 - MISC.

.NET 4.7 MVC & Web Forms
Design Patterns

Different Types
Implement Factory


Ethics in Technology
Open Source Contribution

Learn, Investigate, and Contribute to an open source project


Career Coaching 3

Data Structures: Mock Interviews
Quiz 8
Week 10
Final Project Week

Build a full-stack app with ASP.NET Core
Collaborate as a team with git and VSTS
Present on completed project: test coverage, performance, security, and privacy

",4
php/php-src,C,"




The PHP Interpreter
PHP is a popular general-purpose scripting language that is especially suited to
web development. Fast, flexible and pragmatic, PHP powers everything from your
blog to the most popular websites in the world. PHP is distributed under the PHP
License v3.01.


Documentation
The PHP manual is available at php.net/docs.
Installation
Prebuilt packages and binaries
Prebuilt packages and binaries can be used to get up and running fast with PHP.
For Windows, the PHP binaries can be obtained from
windows.php.net. After extracting the archive the
*.exe files are ready to use.
For other systems, see the installation chapter.
Building PHP source code
For Windows, see Build your own PHP on Windows.
PHP uses autotools on Unix systems to configure the build:
./buildconf
./configure [options]

See ./configure -h for configuration options.
make [options]

See make -h for make options.
The -j option shall set the maximum number of jobs make can use for the
build:
make -j4

Shall run make with a maximum of 4 concurrent jobs: Generally the maximum
number of jobs should not exceed the number of cores available.
Testing PHP source code
PHP ships with an extensive test suite, the command make test is used after
successful compilation of the sources to run this test suite.
It is possible to run tests using multiple cores by setting -jN in
TEST_PHP_ARGS:
make TEST_PHP_ARGS=-j4 test

Shall run make test with a maximum of 4 concurrent jobs: Generally the maximum
number of jobs should not exceed the number of cores available.
The qa.php.net site provides more detailed info about
testing and quality assurance.
Installing PHP built from source
After a successful build (and test), PHP may be installed with:
make install

Depending on your permissions and prefix, make install may need super user
permissions.
PHP extensions
Extensions provide additional functionality on top of PHP. PHP consists of many
essential bundled extensions. Additional extensions can be found in the PHP
Extension Community Library - PECL.
Contributing
The PHP source code is located in the Git repository at
git.php.net. Contributions are most welcome by forking
the GitHub mirror repository and sending a
pull request.
Discussions are done on GitHub, but depending on the topic can also be relayed
to the official PHP developer mailing list internals@lists.php.net.
New features require an RFC and must be accepted by the developers. See
Request for comments - RFC and
Voting on PHP features for more information
on the process.
Bug fixes do not require an RFC but require a bug tracker ticket. Open a
ticket at bugs.php.net and reference the bug id using
#NNNNNN.
Fix #55371: get_magic_quotes_gpc() throws deprecation warning

After removing magic quotes, the get_magic_quotes_gpc function caused a
deprecated warning. get_magic_quotes_gpc can be used to detect the
magic_quotes behavior and therefore should not raise a warning at any time.
The patch removes this warning.

Pull requests are not merged directly on GitHub. All PRs will be pulled and
pushed through git.php.net. See
Git workflow for more details.
Guidelines for contributors
See further documents in the repository for more information on how to
contribute:

Contributing to PHP
PHP coding standards
Mailinglist rules
PHP release process

Credits
For the list of people who've put work into PHP, please see the
PHP credits page.
",23158
codefellows/seattle-dotnet-401d7,C#,"401 .NET Core Curriculum Overview
Resources:

Readme:
Git Ignore:

Week 1 - Basics

Exception Handling

LAB: Numbers Game


Unit Testing

LAB: ATM


System.IO

LAB: Word Guess Game


Classes, Stack/Heap, Garbage Collector

LAB: Tic-Tac-Toe


Linked Lists

LAB: Linked List Implementation
Class with JavaScript



Data Structures:
- How to Approach a DS
- String & Array manipulation
Quiz 1
Week 2 - Advanced

OOP Principles

LAB: Dealership Part 1


Interfaces

LAB: Dealership Part 2


Collections (Generic) & Enums

LAB: Lending Library


LINQ & Lambda Expressions

LAB: LINQ in Manhattan


Stacks & Queues // Recursion

LAB: Stacks and Queues Implementation



Data Structure: Linked Lists
Quiz 2
Week 3 - MVC & Entity Framework Core

MVC Intro

Fullstack MVC App


Relational Databases & schemas

Lab: System Design//Design a ERD // Schema


CRUD Intro to Entity Framework

Lab: AsyncInn Hotel Project Part 1
Create Models from DB schema
Identify Primary Keys/Foreign Keys/Composite Keys


Entity Framework part 2

LAB: EFCore Seeding/View Models/Tag Helpers


Career Coaching

Data Structure: Stacks/Queues
Quiz 3
Week 4 - Repository Design Pattern & APIs

Dependency Injection & Repository Design Pattern

Incorporate the Repository Design Pattern into lab
Introduce Singleton Design Pattern


Azure Deployment & Unit Tests

Deploy app to Azure and write basic tests


API Introduction

TODO List Part 1


API - API part 2

LAB: Making a call out to an API


Midterm Project Kickoff

Data Structures: Trees (Binary, BST, K-ary)
Quiz 4
Week 5
Midterm Project week

Build a Full CRUD web app
Build a custom API
Make the Web app call out to the API

Week 6 - Identity

Intro to E-Com project and PM Tool ""Azure DevOps""

Beginning of Sprint 1: Create a basic MVC app with database


Intro to Identity: Setup & Registration

Implement Identity API and create use registration


Login and Claims

Add Login page to E-Com and capture specific claims


Custom Policies

Create policies to customize access across the site with previous claims
END OF SPRINT 1


Career Coaching

Data Structure: Hash Tables
Quiz 5
Azure Dev Ops

Sprint Backlog
Create User stories with tasks and Acceptance tests
Task Branching
Time estimation
PRs within Azure Dev Ops
Peer Reviews
Weekly Goals and Retrospectives

Week 7

View Components

SPRINT 2 START
Add a ""Basket"" component to E-Com site
System Design exercise - Integrate basket capabilities into the site with DB tables


OAUTH

Add 2 3rd party providers as login to project


AUTH.NET (How to read 3rd party Docs)

Incorporate ""fake"" payment processing into project


Razor Pages

Add in user profile & admin dashboard page as Razor Page
END OF SPRINT 2


Career Coaching

Data Structure: Graphs
Quiz 6
Week 8 - Azure

Sass//SCSS

SPRINT 3 START


Email Services

LAB: Incorporate SendGrid into project


Azure Blob Storage

LAB: Save all product images in Azure Blob Storage


Web Security

OWASP security Requirements
HTTP and SSL
GDPR
LAB: Vulnerability Report
END OF SPRINT 3


Review Day

Data Structure: Sorting Algorithms
Quiz 7
Week 9 - MISC.

.NET 4.7 MVC & Web Forms
Design Patterns

Different Types
Implement Factory


Ethics in Technology
Open Source Contribution

Learn, Investigate, and Contribute to an open source project


Career Coaching 3

Data Structures: Mock Interviews
Quiz 8
Week 10
Final Project Week

Build a full-stack app with ASP.NET Core
Collaborate as a team with git and VSTS
Present on completed project: test coverage, performance, security, and privacy

",4
cityofsandiego/seaboard,CSS,"DataSD is the official portal for City of San Diego Open Data. Here, you'll find data about City operations, including public safety, street repairs, public facilities, code enforcement, business licensing, etc. If you can’t find what you’re looking for, we might still be working on publishing it, or it might come from another agency.
",17
TensShinet/toy_os,None,"toy_os
用 c 语言实现一个 toy_os
",11
Mach131/CMS611-S19Final,C#,"CMS611-S19Final
CMS.611 Spring 2019 Final Project
",2
maohhgg/leetcode,TypeScript,"LST (Leetcode Solutions with TypeScript)
我的 LeetCode 补全计划，旨在用 TypeScript 刷完所有的 LeetCode Problems 😄
解题
问题分类

所有解 (All)
数组 (Array)
字符串 (String)
数学 (Math)
排序 (Sort)
链表 (Linked List)
递归 (Recursion)
栈 (Stack)
堆 (Heap)
树 (Tree)
双指针(Two Pointers)
位运算 (Bit Manipulation))
哈希表 (Hash Table)
动态规划 (Dynamic Programming)
深度优先搜索 (Depth-first Search)
广度优先搜索 (Breadth-first Search)
二分查找 (Binary Search)
分治算法 (Divide and Conquer)

模块
在所有问题中，会经常出现需要额外的代码块
// Definition for singly-linked list.
function ListNode(val) {
    this.val = val;
    this.next = null;
}
// Definition for a binary tree node.
function TreeNode(val) {
    this.val = val;
    this.left = this.right = null;
}
在本项目中会实现一些模块，方便我们调试比如 链表 和 树 之类的题目。具体使用方式请点击下面链接：

数组 IntervalUnit
链表 LinkList
树 Tree

使用说明
依赖
TypeScript 需要 typescript@>=2.0 的支持，如果需要直接运行 TypeScript 脚本，还需要 ts-node 的支持
# 推荐全局安装
npm install -g ts-node
npm install -g typescript
运行
运行代码和调试代码，需要手动运行指定文件：
problems $ ts-node ./src/problems/1.two-sum/index.ts    # 运行 TypeScript 脚本
problems $ tsc ./src/problems/1.two-sum/index.ts        # 转译成 JavaScript 脚本

# 转译成 JavaScript 脚本后，可以使用 nodejs 运行脚本
problems $ node ./src/problems/1.two-sum/index.js
其他
当解答完其他题目，需要更新展示已经解答的 markdown 文档时，只需执行：
cd build
python3 update.py
即可更新 doc 目录下所有分类保存信息的 markdown文档，当前默认分为十类，请查阅上边的问题分类了解具体的分类。如果想添加新的分类，只需要在 update.py 文件的 mian函数中追加想要的分类名：
if __name__ == ""__main__"":
    p = Problems()
    p.update('All')
    p.update('Array')
    p.update('Binary Search')
    ...
    p.update('Breadth-first Search')
需要注意名称必须和 LeetCode 的 tag名称一致。
TODO

 有生之年完成所有题目 😂

",2
vseryakov/backendjs,JavaScript,"Backend library for Node.js
General purpose backend library. The primary goal is to have a scalable platform for running and managing Node.js
servers for Web services implementation.
This project only covers the lower portion of the Web services ecosystem:
Node.js processes, HTTP servers, basic API functionality, database access, caching, messaging between processes,
metrics and monitoring, a library of tools for developing Node.js servers.
For the UI and presentation layer there are no restrictions what to use as long as it can run on top of the Express server.
Features:

Exposes a set of Web service APIs over HTTP(S) using Express framework.
Database API supports SQLite, PostgreSQL, MySQL, DynamoDB, Cassandra, MongoDB, Redis with all basic operations behaving the
same way allowing you to switch databases without changing the code.
Database drivers for LevelDB, LMDB, CouchDB, Riak, ElasticSearch support only a subset of all database operations
Database operations (Get, Put, Del, Update, Select) for all supported databases using the same DB API.
DynamoDB Streams processing in background worker processes
Easily extensible to support any kind of database, provides a database driver on top of Redis with all supported methods as an example.
Provides accounts, connections, locations, messaging and icons APIs with basic functionality for a quick start.
Supports crontab and queue job processing by separate worker processes.
Authentication is based on signed requests using API key and secret, similar to Amazon AWS signing requests.
Runs web server as separate processes to utilize multiple CPU cores.
Supports WebSockets connections and process them with the same Express routes as HTTP requests
Supports several cache modes(Redis, Memcache, Hazelcast, LRU) for the database operations, multiple hosts support
in the clients for failover.
Supports several PUB/SUB modes of operations using Redis, RabbitMQ, Hazelcast.
Supports async jobs processing using several work queue implementations on top of SQS, Redis, DB, RabbitMQ, Hazelcast.
ImageMagick as a separate C++ module for in-process image scaling, see bkjs-wand on NPM.
REPL (command line) interface for debugging and looking into server internals.
Supports push notifications for mobile devices, APN and GCM/FCM.
Supports HTTP(S) reverse proxy mode where multiple Web workers are load-balanced by the proxy
server running in the master process instead of relying on the OS scheduling between processes listening on the same port.
Can be used with any MVC, MVVC or other types of frameworks that work on top of, or with, the Express server.
AWS support is very well integrated including EC2, S3, DynamoDB, SQS and more.
Includes simple log watcher to monitor the log files including system errors.
Supports i18n hooks for request/response objects, easily overriden with any real i18n implementation.
Integrated very light unit testing facility which can be used to test modules and API requests
Support runtime metrics about the timing on database, requests, cache, memory and request rate limit control
Hosted on github, BSD licensed.

Check out the Documentation for more details.
Installation
To install the module with all optional dependencies if they are available in the system
npm install backendjs

This may take some time because of downloading and compiling required dependencies like ImageMagick. They are not required in all
applications but still part of the core of the system to be available once needed.
To install from the git
 npm install git+https://github.com/vseryakov/backendjs.git

or simply
 npm install vseryakov/backendjs

Quick start


Simplest way of using the backendjs, it will start the server listening on port 8000
  $ node
  > const bkjs = require('backendjs')
  > bkjs.server.start()



Access is allowed only with valid signature except urls that are explicitly allowed without it (see api-allow config parameter below)


Same but using the helper tool, by default it will use embedded SQLite database and listen on port 8000.
  bkjs web



To start the server and connect to the DynamoDB (command line parameters can be saved in the etc/config file, see below about config files)
  bkjs web -db-pool dynamodb -db-dynamodb-pool default -aws-key XXXX -aws-secret XXXX



If running on EC2 instance with IAM profile then no need to specify AWS credentials:
  bkjs web -db-pool dynamodb -db-dynamodb-pool default



or to the PostgreSQL server, database backend
  bkjs web -db-pool pgsql -db-pgsql-pool postgresql://postgres@127.0.0.1/backend



All commands above will behave exactly the same


Tables are not created by default, in order to initialize the database, run the server or the shell with -db-create-tables flag,
it is called only inside a master process, a worker never creates tables on start


prepare the tables in the shell
bksh -db-pool pgsql -db-pgsql-pool postgresql://postgres@127.0.0.1/backend -db-create-tables



run the server and create tables on start
bkjs web -db-pool pgsql -db-pgsql-pool postgresql://postgres@127.0.0.1/backend -db-create-tables





While the local backendjs is runnning, the documentation is always available at http://localhost:8000/doc.html (or whatever port is the server using)


To add users from the command line
  bksh -account-add login test secret test name TestUser email test@test.com -scramble 1



By default no external modules are loaded so it needs the accounts module with a
parameter -allow-modules PATTERN, this will load all modules that match the pattern, default modules start with bk_:
  bkjs web -allow-modules bk_



To start Node.js shell with backendjs loaded and initialized, all command line parameters apply to the shell as well
  bkjs shell



To access the database while in the shell
  > db.select(""bk_account"", {}, function(err, rows, info) { console.log(err, rows, info) });
  > db.select(""bk_account"", {}, lib.log);
  > db.add(""bk_account"", { login: 'test2', secret: 'test2', name' Test 2 name', gender: 'f' }, lib.log);
  > db.select(""bk_account"", { gender: 'm' }, lib.log);
  > db.select(""bk_account"", { gender: ['m','f'] }, { ops: { gender: ""in"" } }, lib.log);



To run an example


Go to examples/api directory:


Run the application, it will start the Web server on port 8000:
  ./app.sh



Now log in with the new account,


Go to http://localhost:8000/api.html and click on Login at the top-right corner, then enter 'test' as login and 'test' as secret in the login popup dialog.


To see your account details run the command in the console /account/get


To see current metrics run the command in the console /system/stats/get


To see charts about accumulated metrics go to http://localhost:8000/metrics.html


When the web server is started with -watch parameters any change in the source files will make the server restart automatically
letting you focus on the source code and not server management, this mode is only enabled by default in development mode,
check app.sh for parameters before running it in production.


Configuration
Almost everything in the backend is configurable using config files, a config database or DNS.
The whole principle behind it is that once deployed in production, even quick restarts are impossible to do so
there should be a way to push config changes to the processes without restarting.
Every module defines a set of config parameters that defines the behavior of the code, due to the single threaded
nature of the Node.js. It is simple to update any config parameter to a new value so the code can operate differently.
To achieve this the code must be written in a special way, like driven by configuration which can be changed at
any time.
All configuration goes through the configuration process that checks all inputs and produces valid output which
is applied to the module variables. Config file or database table with configuration can be loaded on demand or
periodically, for example all local config files are watched for modification and reloaded automatically, the
config database is loaded periodically which is defined by another config parameter.
Backend runtime
When the backendjs server starts it spawns several processes that perform different tasks.
There are 2 major tasks of the backend that can be run at the same time or in any combination:

a Web server (server) with Web workers (web)
a job scheduler (master)

These features can be run standalone or under the guard of the monitor which tracks all running processes and restarted any failed ones.
This is the typical output from the ps command on Linux server:
ec2-user    891  0.0  0.6 1071632 49504 ?  Ssl  14:33   0:01 bkjs: monitor
ec2-user    899  0.0  0.6 1073844 52892 ?  Sl   14:33   0:01 bkjs: master
ec2-user    908  0.0  0.8 1081020 68780 ?  Sl   14:33   0:02 bkjs: server
ec2-user    917  0.0  0.7 1072820 59008 ?  Sl   14:33   0:01 bkjs: web
ec2-user    919  0.0  0.7 1072820 60792 ?  Sl   14:33   0:02 bkjs: web
ec2-user    921  0.0  0.7 1072120 40721 ?  Sl   14:33   0:02 bkjs: worker

To enable any task a command line parameter must be provided, it cannot be specified in the config file. The bkjs utility supports several
commands that simplify running the backend in different modes.

bkjs start - this command is supposed to be run at the server startup as a service, it runs in the background and the monitors all tasks,
the env variable BKJS_SERVER can be set in the profile to one of the master or monitor to define which run mode to use, default mode is monitor
bkjs monitor - this command is supposed to be run at the server startup, it runs in the background and the monitors all processes,
the command line parameters are: -daemon -monitor -master -syslog
bkjs master - this command is supposed to be run at the server startup, it runs in the background and the monitors all processes,
the command line parameters are: -daemon -monitor -master -syslog
bkjs watch - runs the master and Web server in wather mode checking all source files for changes, this is the common command to be used
in development, it passes the command line switches: -watch -master
bkjs web - this command runs just web server process.
bkjs run - this command runs without other parameters, all additional parameters can be added in the command line, this command
is a barebone helper to be used with any other custom settings.
bkjs shell or bksh - start backendjs shell, no API or Web server is initialized, only the database pools

Application structure
The main purpose of the backendjs is to provide API to access the data, the data can be stored in the database or some other way
but the access to that data will be over HTTP and returned back as JSON. This is default functionality but any custom application
may return data in whatever format is required.
Basically the backendjs is a Web server with ability to perform data processing using local or remote jobs which can be scheduled similar to Unix cron.
The principle behind the system is that nowadays the API services just return data which Web apps or mobiles apps can render to
the user without the backend involved. It does not mean this is simple gateway between the database, in many cases it is but if special
processing of the data is needed before sending it to the user, it is possible to do and backendjs provides many convenient helpers and tools for it.
When the API layer is initialized, the api module contains app object which is an Express server.
Special module/namespace app is designated to be used for application development/extension. This module is available in the same way as api and core
which makes it easy to refer and extend with additional methods and structures.
The typical structure of a backendjs application is the following:
    const bkjs = require('backendjs');
    const api = bkjs.api;
    const app = bkjs.app;
    const db = bkjs.db;

    app.listArg = [];

    // Define the module config parameters
    core.describeArgs('app', [
        { name: ""list-arg"", array: 1, type: ""list"", descr: ""List of words"" },
        { name: ""int-arg"", type: ""int"", descr: ""An integer parameter"" },
     ]);

    // Describe the tables or data models, all DB pools will use it, the master or shell
    // process only creates new tables, workers just use the existing tables
    db.describeTables({
         ...
    });

     // Optionally customize the Express environment, setup MVC routes or else, `api.app` is the Express server
    app.configureMiddleware = function(options, callback)
    {
       ...
       callback()
    }

    // Register API endpoints, i.e. url callbacks
    app.configureWeb = function(options, callback)
    {
        api.app.get('/some/api/endpoint', function(req, res) {
          // to return an error, the message will be translated with internal i18n module if locales
          // are loaded and the request requires it
          api.sendReply(res, err);
          // or with custom status and message, explicitely translated
          api.sendReply(res, 404, res.__({ phrase: ""not found"", locale: ""fr"" }));

          // with config check
          if (app.intArg > 5) ...
          if (app.listArg.indexOf(req.query.name) > -1) ...

          // to send data back with optional postprocessing hooks
          api.sendJSON(req, err, data);
          // or simply
          res.json(data);
        });
        ...
        callback();
    }

    // Optionally register post processing of the returned data from the default calls
    api.registerPostProcess('', /^\/account\/([a-z\/]+)$/, function(req, res, rows) { ... });
     ...

    // Optionally register access permissions callbacks
    api.registerAccessCheck('', /^\/test\/list$/, function(req, status, callback) { ...  });
    api.registerPreProcess('', /^\/test\/list$/, function(req, status, callback) { ...  });
     ...
    bkjs.server.start();
Except the app.configureWeb and server.start() all other functions are optional, they are here for the sake of completeness of the example. Also
because running the backend involves more than just running web server many things can be setup using the configuration options like common access permissions,
configuration of the cron jobs so the amount of code to be written to have fully functioning production API server is not that much, basically only
request endpoint callbacks must be provided in the application.
As with any Node.js application, node modules are the way to build and extend the functionality, backendjs does not restrict how
the application is structured.
Modules
Another way to add functionality to the backend is via external modules specific to the backend, these modules are loaded on startup from the backend
home subdirectory modules/ and from the backendjs package directory for core modules. The format is the same as for regular Node.js modules and
only top level .js files are loaded on the backend startup.
By default no modules are loaded except bk_accounts|bk_icons, it must be configured by the -allow-modules config parameter.
The modules are managed per process role, by default server and master processes do not load any modules at all to keep them
small and because they monitor workers the less code they have the better.
The shell process loads all modules, it is configured with .+.
To enable any module to be loaded in any process it can be configured by using a role in the config parameter:
  // Global modules except server and master
  -allow-modules '.+'

  // Master modules
  -allow-modules-master 'bk_accounts|bk_debug'

Once loaded they have the same access to the backend as the rest of the code, the only difference is that they reside in the backend home and
can be shipped regardless of the npm, node modules and other env setup. These modules are exposed in the core.modules the same way as all other core submodules
methods.
Let's assume the modules/ contains file facebook.js which implements custom FB logic:
    const bkjs = require(""backendjs"");
    const fb = {
        args: [
            { name: ""token"", descr: ""API token"" },
        ]
    }
    module.exports = fb;

    fb.configureWeb = function(options, callback) {
       ...
    }

    fb.makeRequest = function(options, callback) {
         bkjs.core.sendRequest({ url: options.path, query: { access_token: fb.token } }, callback);
    }
This is the main app code:
    const bkjs = require(""backendjs"");
    const core = bkjs.core;

    // Using facebook module in the main app
    api.app.get(""some url"", function(req, res) {

       core.modules.facebook.makeRequest({ path: ""/me"" }, function(err, data) {
          bkjs.api.sendJSON(req, err, data);
       });
    });

    bkj.server.start();
NPM packages as modules
In case different modules is better keep separately for maintenance or development purposes they can be split into
separate NPM packages, the structure is the same, modules must be in the modules/ folder and the package must be loadable
via require as usual. In most cases just empty index.js is enough. Such modules will not be loaded via require though but
by the backendjs core.loadModule machinery, the NPM packages are just keep different module directories separate from each other.
The config parameter allow-packages can be used to specify NPM package names to be loaded separated by comma, as with the default
application structure all subfolders inside each NPM package will be added to the core:

modules will be loaded from the modules/ older
locales from the locales/ folder
files in the web/ folder will be added to the static search path
all templates from views/ folder will be used for rendering

If there is a config file present as etc/config it will be loaded as well, this way each package can maintain its default config parameters if necessary
without touching other or global configuration. Although such config files will not be reloaded on changes, when NPM installs or updates packages it
moves files around so watching the old config is no point because the updated config file will be different.
Database schema definition
The backend support multiple databases and provides the same db layer for access. Common operations are supported and all other specific usage can be achieved by
using SQL directly or other query language supported by any particular database.
The database operations supported in the unified way provide simple actions like db.get, db.put, db.update, db.del, db.select. The db.query method provides generic
access to the database driver and executes given query directly by the db driver, it can be SQL or other driver specific query request.
Before the tables can be queried the schema must be defined and created, the backend db layer provides simple functions to do it:

first the table needs to be described, this is achieved by creating a JavaScript object with properties describing each column, multiple tables can be described
at the same time, for example lets define album table and make sure it exists when we run our application:

        db.describeTables({
           album: {
               id: { primary: 1 },                         // Primary key for an album
               name: { pub: 1 },                           // Album name, public column
               mtime: { type: ""now"" },                     // Modification timestamp
           },
           photo: {
               album_id: { primary: 1 },                   // Combined primary key
               id: { primary: 1 },                         // consisting of album and photo id
               name: { pub: 1, index: 1 },                 // Photo name or description, public column with the index for faster search
               mtime: { type: ""now"" }
           }
        });

the system will automatically create the album and photos tables, this definition must remain in the app source code
and be called on every app startup. This allows 1) to see the db schema while working with the app and 2) easily maintain it by adding new columns if
necessary, all new columns will be detected and the database tables updated accordingly. And it is all JavaScript, no need to learn one more language or syntax
to maintain database tables.

Each database may restrict how the schema is defined and used, the db layer does not provide an artificial layer hiding all specifics, it just provides the same
API and syntax, for example, DynamoDB tables must have only hash primary key or combined hash and range key, so when creating table to be used with DynamoDB, only
one or two columns can be marked with primary property while for SQL databases the composite primary key can consist of more than 2 columns.
The backendjs always creates several tables in the configured database pools by default, these tables are required to support default API functionality and some
are required for backend operations. Refer below for the JavaScript modules documentation that described which tables are created by default. In the custom applications
the db.describeTables method can modify columns in the default table and add more columns if needed.
For example, to make age and some other columns in the accounts table public and visible by other users with additional columns the following can be
done in the api.initApplication method. It will extend the bk_account table and the application can use new columns the same way as the already existing columns.
Using the birthday column we make 'age' property automatically calculated and visible in the result, this is done by the internal method api.processAccountRow which
is registered as post process callback for the bk_account table. The computed property age will be returned because it is not present in the table definition
and all properties not defined and configured are passed as is.
The cleanup of the public columns is done by the api.sendJSON which is used by all API routes when ready to send data back to the client. If any post-process
hooks are registered and return data itself then it is the hook responsibility to cleanup non-public columns.
    db.describeTables({
        bk_account: {
            gender: { pub: 1 },
            birthday: {},
            ssn: {},
            salary: { type: ""int"" },
            occupation: {},
            home_phone: {},
            work_phone: {},
        });

    app.configureWeb = function(options, callback)
    {
       db.setProcessRow(""post"", ""bk_account"", this.processAccountRow);
       ...
       callback();
    }
    app.processAccountRow = function(req, row, options)
    {
       if (row.birthday) row.age = Math.floor((Date.now() - core.toDate(row.birthday))/(86400000*365));
    }
To define tables inside a module just provide a tables property in the module object, it will be picked up by database initialization automatically.
const mod = {
    name: ""billing"",
    tables: {
       invoices: {
          id: { type: ""int"", primary: 1 },
          name: {},
          price: { type: ""real"" },
          mtime: { type: ""now"" }
       }
    }
}
module.exports = mod;

// Run db setup once all the DB pools are configured, for example produce dynamic icon property
// for each record retrieved
mod.configureModule = function(options, callback)
{
    db.setProcessRows(""post"", ""invoices"", function(req, row, opts) {
       if (row.id) row.icon = ""/images/"" + row.id + "".png"";
    });
    callback();
}
API requests handling
All methods will put input parameters in the req.query, GET or POST.
One way to verify input values is to use lib.toParams, only specified parameters will be returned and converted according to
the type or ignored.
Example:
   var params = {
      test1: { id: { type: ""text"" },
               count: { type: ""int"" },
               email: { regexp: /^[^@]+@[^@]+$/ }
      }
   };

   api.app.all(""/endpoint/test1"", function(req, res) {
      var query = lib.toParams(req.query, params.test1);
      ...
   });
Example of TODO application
Here is an example how to create simple TODO application using any database supported by the backend. It supports basic
operations like add/update/delete a record, show all records.
Create a file named app.js with the code below.
    const bkjs = require('backendjs');
    const api = bkjs.api;
    const lib = bkjs.lib;
    const app = bkjs.app;
    const db = bkjs.db;

    // Describe the table to store todo records
    db.describeTables({
       todo: {
           id: { type: ""uuid"", primary: 1 },  // Store unique task id
           due: {},                           // Due date
           name: {},                          // Short task name
           descr: {},                         // Full description
           mtime: { type: ""now"" }             // Last update time in ms
       }
    });

    // API routes
    app.configureWeb = function(options, callback)
    {
        api.app.get(/^\/todo\/([a-z]+)$/, function(req, res) {
           var options = api.getOptions(req);
           switch (req.params[0]) {
             case ""get"":
                if (!req.query.id) return api.sendReply(res, 400, ""id is required"");
                db.get(""todo"", { id: req.query.id }, options, function(err, rows) { api.sendJSON(req, err, rows); });
                break;
             case ""select"":
                options.noscan = 0; // Allow empty scan of the whole table if no query is given, disabled by default
                db.select(""todo"", req.query, options, function(err, rows) { api.sendJSON(req, err, rows); });
                break;
            case ""add"":
                if (!req.query.name) return api.sendReply(res, 400, ""name is required"");
                // By default due date is tomorrow
                if (req.query.due) req.query.due = lib.toDate(req.query.due, Date.now() + 86400000).toISOString();
                db.add(""todo"", req.query, options, function(err, rows) { api.sendJSON(req, err, rows); });
                break;
            case ""update"":
                if (!req.query.id) return api.sendReply(res, 400, ""id is required"");
                db.update(""todo"", req.query, options, function(err, rows) { api.sendJSON(req, err, rows); });
                break;
            case ""del"":
                if (!req.query.id) return api.sendReply(res, 400, ""id is required"");
                db.del(""todo"", { id: req.query.id }, options, function(err, rows) { api.sendJSON(req, err, rows); });
                break;
            }
        });
        callback();
     }
     bkjs.server.start();
Now run it with an option to allow API access without an account:
node app.js -log debug -web -api-allow-path /todo -db-create-tables

To use a different database, for example PostgresSQL(running localy) or DynamoDB(assuming EC2 instance),
all config parametetrs can be stored in the etc/config as well
node app.js -log debug -web -api-allow-path /todo -db-pool dynamodb -db-dynamodb-pool default -db-create-tables
node app.js -log debug -web -api-allow-path /todo -db-pool pgsql -db-pgsql-pool default -db-create-tables

API commands can be executed in the browser or using curl:
curl 'http://localhost:8000/todo?name=TestTask1&descr=Descr1&due=2015-01-01`
curl 'http://localhost:8000/todo/select'

Backend directory structure
When the backend server starts and no -home argument passed in the command line the backend makes its home environment in the ~/.bkjs directory.
It is also possible to set the default home using BKJS_HOME environment variable.
The backend directory structure is the following:


etc - configuration directory, all config files are there


etc/profile - shell script loaded by the bkjs utility to customize env variables


etc/config - config parameters, same as specified in the command line but without leading -, each config parameter per line:
Example:
  debug=1
  db-pool=dynamodb
  db-dynamodb-pool=http://localhost:9000
  db-pgsql-pool=postgresql://postgres@127.0.0.1/backend

  To specify other config file: bkjs shell -config-file file



etc/config.local - same as the config but for the cases when local environment is different than the production or for dev specific parameters


some config parameters can be configured in DNS as TXT records, the backend on startup will try to resolve such records and use the value if not empty.
All params that  marked with DNS TXT can be configured in the DNS server for the domain where the backend is running, the config parameter name is
concatenated with the domain and queried for the TXT record, for example: cache-host parameter will be queried for cache-host.domain.name for TXT record type.


etc/crontab - jobs to be run with intervals, JSON file with a list of cron jobs objects:
Example:


Create file in ~/.backend/etc/crontab with the following contents:
 [ { ""cron"": ""0 1 1 * * 1,3"", ""job"": { ""app.cleanSessions"": { ""interval"": 3600000 } } } ]



Define the function that the cron will call with the options specified, callback must be called at the end, create this app.js file
 var bkjs = require(""backendjs"");
 bkjs.app.cleanSessions = function(options, callback) {
      bkjs.db.delAll(""session"", { mtime: options.interval + Date.now() }, { ops: ""le"" }, callback);
 }
 bkjs.server.start()



Start the jobs queue and the web server at once
 bkjs master -jobs-workers 1 -jobs-cron





etc/crontab.local - additional local crontab that is read after the main one, for local or dev environment




modules - loadable modules with specific functionality


images - all images to be served by the API server, every subfolder represent naming space with lots of subfolders for images


var - database files created by the server


tmp - temporary files


web - Web pages served by the static Express middleware


Cache configurations
Database layer support caching of the responses using db.getCached call, it retrieves exactly one record from the configured cache, if no record exists it
will pull it from the database and on success will store it in the cache before returning to the client. When dealing with cached records, there is a special option
that must be passed to all put/update/del database methods in order to clear local cache, so next time the record will be retrieved with new changes from the database
and refresh the cache, that is { cached: true } can be passed in the options parameter for the db methods that may modify records with cached contents. In any case
it is required to clear cache manually there is db.clearCache method for that.
Also there is a configuration option -db-caching to make any table automatically cached for all requests.
Local
If no cache is configured the local driver is used, it keeps the cache on the master process in the LRU pool and any worker or Web process
communicate with it via internal messaging provided by the cluster module. This works only for a single server.
memcached
Set ipc-cache=memcache://HOST[:PORT] that points to the host running memcached. To support multiple servers add the option
ipc-cache-options-servers=10.1.1.1,10.2.2.1:5000.
Redis
Set ipc-cache=redis://HOST[:PORT] that points to the server running Redis server.
To support more than one master Redis server in the client add additional servers in the servers parameter,
ipc-cache-options-servers=10.1.1.1,10.2.2.1:5000, the client will reconnect automatically on every
disconnect. To support quick failover it needs a parameter for the node-redis module (which is used by the driver) max_attempts to be a
number how many attempts to reconnect before switching to another server like ipc-cache-options-max_attempts=3. If there is only one
server then it will keep reconnecting until total reconnect time exceeds the connect_timeout ms.
Any other node-redis module parameter can be passed as well.
Cache configurations also can be passed in the url, the system supports special parameters that start with bk-, it will extract them into options automatically.
For example:
ipc-cache=redis://host1?bk-servers=host2,host3&bk-max_attempts=3
ipc-cache-backup=redis://host2
ipc-cache-backup-options-max_attempts=3

Redis Sentinel
To enable Redis Sentinel pass in the option -sentinel-servers: ipc-cache=redis://host1?bk-sentinel-servers=host1,host2.
The system will connect to the sentinel, get the master cache server and connect the cache driver to it, also it will listen constantly on
sentinel events and failover to a new master autimatically. Sentinel use the regular redis module and supports all the same
parameters, to pass options to the sentinel driver prefix them with sentinel-:
ipc-cache=redis://host1?bk-servers=host2,host3&bk-max_attempts=3&bk-sentinel-servers=host1,host2,host3
ipc-cache-backup=redis://host2
ipc-cache-backup-options-sentinel-servers=host1,host2
ipc-cache-backup-options-sentinel-max_attempts=5

PUB/SUB or Queue configurations
Publish/subscribe functionality allows clients to receive notifications without constantly polling for new events. A client can be anything but
the backend provides some partially implemented subscription notifications for Web clients using the Long Poll.
The Account API call /account/subscribe can use any pub/sub mode.
The flow of the pub/sub operations is the following:

a HTTP client makes /account/subscribe API request, the connection is made and is kept open indefinitely or as long as configured using api-subscribe-timeout.
the API backend receives this request, and runs the api.subscribe method with the key being the account id, this will subscribe to the events for the current
account and registers a callback to be called if any events occurred. The HTTP connection is kept open.
some other client makes an API call that triggers an event like makes a connection or sends a message, on such event the backend API handler
always runs ipc.publish after the DB operation succeeds. If the messaging is configured, it publishes the message for the account, the
message being a JSON object with the request API path and mtime, other properties depend on the call made.
the connection that initiated /account/subscribe receives an event

Redis
To configure the backend to use Redis for PUB/SUB messaging set ipc-queue=redis://HOST where HOST is IP address or hostname of the single Redis server.
This will use native PUB/SUB Redis feature.
Redis Queue
To configure the backend to use Redis for job processing set ipc-queue=redisq://HOST where HOST is IP address or hostname of the single Redis server.
This driver implements reliable Redis queue, with visibilityTimeout config option works similar to AWS SQS.
Once configured, then all calls to jobs.submitJob will push jobs to be executed to the Redis queue, starting somewhere a backend master
process with -jobs-workers 2 will launch 2 worker processes which will start pulling jobs from the queue and execute.
An example of how to perform jobs in the API routes:
   app.processAccounts = function(options, callback) {
       db.select(""bk_account"", { type: options.type || ""user"" }, function(err, rows) {
          ...
          callback();
       });
   }

   api.all(""/process/accounts"", function(req, res) {
       jobs.submitJob({ job: { ""app.processAccounts"": { type: req.query.type } } }, function(err) {
          api.sendReply(res, err);
       });
   });

RabbitMQ
To configure the backend to use RabbitMQ for messaging set ipc-queue=amqp://HOST and optionally amqp-options=JSON with options to the amqp module.
Additional objects from the config JSON are used for specific AMQP functions: { queueParams: {}, subscribeParams: {}, publishParams: {} }. These
will be passed to the corresponding AMQP methods: amqp.queue, amqp.queue.sibcribe, amqp.publish. See AMQP Node.js module for more info.
DB
This is a simple queue implementation using the atomic UPDATE, it polls for new jobs in the table and updates the status, only who succeeds
with the update takes the job and executes it. It is not effective but can be used for simple and not busy systems for more or less long jobs.
The advantage is that it uses the same database and does not require additional servers.
SQS
To use AWS SQS for job processing set ipc-queue=https://sqs.amazonaws.com...., this queue system will poll SQS for new messages on a worker
and after successful execution will delete the message. For long running jobs it will automatically extend visibility timeout if it is configured.
Local
The local queue is implemented on the master process as a list, communication is done via local sockets between the master and workers.
This is intended for a single server development purposes only.
Security configurations
API only
This is default setup of the backend when all API requests except must provide valid signature and all HTML, JavaScript, CSS and image files
are available to everyone. This mode assumes that Web development will be based on 'single-page' design when only data is requested from the Web server and all
rendering is done using JavaScript. This is how the examples/api/api.html developers console is implemented, using JQuery-UI and Knockout.js.
To see current default config parameters run any of the following commands:
    bkjs bkhelp | grep api-allow

    node -e 'require(""backendjs"").core.showHelp()'

To enable open registration in this mode just add config parameter api-allow-path=^/account/add$.
Secure Web site, client verification
This is a mode when the whole Web site is secure by default, even access to the HTML files must be authenticated. In this mode the pages must defined 'Backend.session = true'
during the initialization on every html page, it will enable Web sessions for the site and then no need to sign every API request.
The typical client JavaScript verification for the html page may look like this, it will redirect to login page if needed,
this assumes the default path '/public' still allowed without the signature:
   <script src=""/js/jquery.js""></script>
   <link href=""/css/bootstrap.css"" rel=""stylesheet"">
   <script src=""/js/bootstrap.js""></script>
   <script src=""/js/knockout.js"" type=""text/javascript""></script>
   <script src=""/js/crypto.js"" type=""text/javascript""></script>
   <script src=""/js/bkjs.js"" type=""text/javascript""></script>
   <script src=""/js/bkjs-bootstrap.js"" type=""text/javascript""></script>
   <script src=""/js/bkjs-ko.js"" type=""text/javascript""></script>
   <script>
    $(function () {
       Bkjs.session = true;
       $(Bkjs).on(""bkjs.nologin"", function() { window.location='/public/index.html'; });
       Bkjs.koInit();
   });
   </script>
Secure Web site, backend verification
On the backend side in your application app.js it needs more secure settings defined i.e. no html except /public will be accessible and
in case of error will be redirected to the login page by the server. Note, in the login page Bkjs.session must be set to true for all
html pages to work after login without singing every API request.

We disable all allowed paths to the html and registration:

   app.configureMiddleware = function(options, callback) {
      this.allow.splice(this.allow.indexOf('^/$'), 1);
      this.allow.splice(this.allow.indexOf('\\.html$'), 1);
      callback();
   }

We define an auth callback in the app and redirect to login if the request has no valid signature, we check all html pages, all allowed html pages from the /public
will never end up in this callback because it is called after the signature check but allowed pages are served before that:

   api.registerPreProcess('', /^\/$|\.html$/, function(req, status, callback) {
      if (status.status != 200) {
          status.status = 302;
          status.url = '/public/index.html';
      }
      callback(status);
   });
WebSockets connections
The simplest way is to configure ws-port to the same value as the HTTP port. This will run WebSockets server along the regular Web server.
All requests must be properly signed with all parameters encoded as for GET requests.
Example:
    wscat --connect ws://localhost:8000
    connected (press CTRL+C to quit)
    > /account/get
    < {
        ""status"": 400,
        ""message"": ""Invalid request: no host provided""
      }
    >

Versioning
There is no ready to use support for different versions of API at the same because there is no just one solution that satisfies all applications. But there are
tools ready to use that will allow to implement such versioning system in the backend. Some examples are provided below:


Fixed versions
This is similar to AWS version system when versions are fixed and changed not very often. For such cases the backend exposes core.bkVersion which is
supposed to be a core backend version. This version is returned with every backend response in the Server: header. A client also can specify the core version
using bk-version header. When a request is parsed and the version is provided it will be set in the request options object as apiVersion.
All API routes are defined using Express middleware and one of the possible ways of dealing with different versions can look like this, by
appending version to the command it is very simple to call only changed API code.


          api.all(/\/domain\/(get|put|del)/, function(req, res) {
              var options = api.getOptions(req);
              var cmd = req.params[0];
              if (options.apiVersion) cmd += ""/"" + options.apiVersion;
              switch (cmd) {
              case ""get"":
                  break;

              case ""get/2015-01-01"":
                  break;

              case ""put"":
                  break;

              case ""put/2015-02-01"":
                  break;

              case ""del""
                  break;
              }
          });


Application semver support
For cases when applications support Semver kind of versioning and it may be too many releases the method above still can be used while the number of versions is
small, once too many different versions with different minor/patch numbers, it is easier to support greater/less comparisons.
The application version bk-app can be supplied in the query or as a header or in the user-agent HTTP header which is the easiest case for mobile apps.
In the middlware, the code can look like this:


        var options = api.getOptions(req);
        var version = lib.toVersion(options.appVersion);
        switch (req.params[0]) {
        case ""get"":
            if (version < lib.toVersion(""1.2.5"")) {
                res.json({ id: 1, name: ""name"", description: ""descr"" });
                break;
            }
            if (version < lib.toVersion(""1.1"")) {
                res.json([id, name]);
                break;
            }
            res.json({ id: 1, name: ""name"", descr: ""descr"" });
            break;
        }
The actual implementation can be modularized, split into functions, controllers.... there are no restrictions how to build the working backend code,
the backend just provides all necessary information for the middleware modules.
The backend provisioning utility: bkjs
The purpose of the bkjs shell script is to act as a helper tool in configuring and managing the backend environment
and as well to be used in operations on production systems. It is not required for the backend operations and provided as a convenience tool
which is used in the backend development and can be useful for others running or testing the backend.
Running without arguments will bring help screen with description of all available commands.
The tool is multi-command utility where the first argument is the command to be executed with optional additional arguments if needed.
On Linux, when started the bkjs tries to load and source the following config files:
    /etc/sysconfig/bkjs
    $BKJS_HOME/etc/profile

Any of the following config files can redefine any environment variable thus pointing to the correct backend environment directory or
customize the running environment, these should be regular shell scripts using bash syntax.
Most common used commands are:


bkjs watch - run the backend or the app for development purposes, uses local app.js if exists otherwise runs generic server


bkjs shell - start REPL shell with the backend module loaded and available for use, all submodules are available in the shell as well like core, db, api


bkjs sync [-path path] [-host host] [-user user] - sync sources of the app with the remote site, this is for development version of the backend only


bkjs init-server [-home path] [-user user] [-host name] [-domain name] - initialize Linux instance(Amazon,CentOS) for backend use, optional -home can be specified where the backend
home will be instead of ~/.bkjs, optional -user tells to use existing user instead of the current user and not root.
This command will create /etc/sysconfig/bkjs file with BKJS_HOME set to the home of the
backendjs app which was passed in the command line. This makes the bkjs or bksh run globally regardless of the current directory.


Web development notes
The server supports simple web bundling using uglify-js utility. To enable it just add to the local config a list of directories to be
watched for changes. For example adding these lines to the local config will enable the watcher and bundle support
 watch-web=web/js,web/css,$HOME/src/js,$HOME/src/css
 watch-ignore=.bundle.(js|css)$
 build-web=bkjs web-bundle -dev

Now instead of incding a bunch of .js or css files in the html pages it only needs /js/bkjs.bundle.js and /css/bkjs.bundle.css. The configuration is in the
package.json file.
The simple script below allows to build the bundle and refresh Chrome tab automatically, saves several clicks:
 #!/bin/bash
 bkjs web-bundle -dev -file $2
 [ ""$?"" != ""0"" ] && exit
 osascript -e ""tell application \""Google Chrome\"" to reload (tabs of window 1 whose URL contains \""$1\"")""
 #osascript -e 'tell application ""Google Chrome"" to tell the active tab of its first window to reload'

To use it call this script instead in the config.local:
 build-web=web-bundle.sh /website

Deployment use cases
AWS instance setup with node and backendjs
Here is the example how to setup new custom AWS server, it is not required and completely optional but bkjs provides some helpful commands that may simplify
new image configuration.


start new AWS instance via AWS console, use Amazon Linux


login as ec2-user


install commands
  yum-config-manager --enable epel
  sudo yum install npm
  npm install backendjs
  sudo bkjs init-service
  bkjs restart



try to access the instance via HTTP port 8000 for the API console or documentation


after reboot the server will be started automatically


AWS instance as an appliance
To make an API appliance by using the backendjs on the AWS instance as user ec2-user with the backend in the user home


start new AWS instance via AWS console, use Amazon Linux or CentOS 6


login as ec2-user


install commands
  git clone https://github.com/vseryakov/backendjs.git
  sudo backendjs/bkjs install-ec2 -tools $(pwd)/backendjs/tools
  bkjs restart



run ps agx, it should show several backend processes running


try to access the instance via HTTP port for the API console or documentation


NOTE: if running behind a Load balancer and actual IP address is needed set Express option in the command line -api-express-options {""trust%20proxy"":1}. In the config file
replacing spaces with %20 is not required.
AWS Beanstalk deployment
As with any Node.js module, the backendjs app can be packaged into zip file according to AWS docs and deployed the same way as any other Node.js app.
Inside the app package etc/config file can be setup for any external connections.
AWS Provisioning examples
Note: on OS X laptop the -aws-sdk-profile uc when AWS credentials are in the ~/.aws/credentials.
Make an AMI
On the running machine which will be used for an image:
bksh -aws-create-image -no-reboot

Use an instance by tag for an image:
bksh -aws-create-image -no-reboot -instance-id `bkjs ec2-show -tag api -fmt id | head -1`

Launch instances when not using AutoScaling Groups
When launching from an EC2 instance no need to specify any AWS credentials.


admin (EC2)
bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type t2.small -subnet-name api -name admin -elb-name Admin -alarm-name alarms -public-ip 1 -dry-run


api (EC2)
bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type m3.large -subnet-name api -name api -elb-name api -alarm-name alarms -public-ip 1 -dry-run


jobs (EC2)
bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type t2.small -subnet-name internal -name sync -alarm-name alarms -dry-run
bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type t2.small -subnet-name internal -name sync -zone 1c -alarm-name alarms -dry-run


Elasticsearch
bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type m3.large -subnet-name internal -name elasticsearch -bkjs-cmd stop-service -bkjs-cmd ""init-elasticsearch-service -memsize 50"" -alarm-name alarms -public-ip 1 -dry-run


Redis
bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type m3.large -subnet-name internal -name redis -bkjs-cmd stop-service -bkjs-cmd ""init-redis-service -memsize 70"" -alarm-name alarms -public-ip 1 -dry-run


Launch Configurations
bksh -aws-create-launch-config -config-name elasticsearch -aws-sdk-profile uc -instance-type m3.large -update-groups -bkjs-cmd stop-service -bkjs-cmd init-logwatcher -bkjs-cmd ""init-elasticsearch-service -memsize 50"" -device /dev/xvda:gp2:16 -dry-run

Copy Autoscaling launch configs after new AMI is created
bksh -aws-create-launch-config -config-name jobs -aws-sdk-profile uc -update-groups -dry-run
bksh -aws-create-launch-config -config-name api -aws-sdk-profile uc -update-groups -dry-run

Update Route53 with all IPs from running instances
bksh -aws-set-route53 -name elasticsearch.ec-internal -filter elasticsearch

Proxy mode
By default the Web proceses spawned by the server are load balanced using default cluster module which relies on the OS to do scheduling. On Linux with node 0.10
this is proven not to work properly due to the kernel keeping the context switches to a minimum thus resulting in one process to be very busy while the others
idle. Node versions 4 and above perform round-robin by default.
For such case the Backendjs implements the proxy mode by setting proxy-port config parameter to any number above 1000, this will be the initial
port for the web processes to listen for incoming requests, for example if use -proxy-port 3000 and launch 2 web processes they will listen on ports
3000 and 3001. The main server process will start internal HTTP proxy and will perform round-robin load balancing the incoming requests between the web processes by forwarding
them to the web processes over TCP and then returning the responses back to the clients.
Configure HTTP port
The first thing when deploying the backend into production is to change API HTTP port, by default is is 8000, but we would want port 80 so regardless
how the environment is setup it is ultimately 2 ways to specify the port for HTTP server to use:


config file
The config file is always located in the etc/ folder in the backend home directory, how the home is specified depends on the system but basically it can be
defined via command line arguments as -home or via environment variables when using bkjs. See bkjs documentation but on AWS instances created with bkjs
init-server command, for non-standard home use /etc/sysconfig/bkjs profile, specify BKJS_HOME=/home/backend there and the rest will be taken care of


command line arguments
When running node scripts which use the backend, just specify -home command line argument with the directory where your backend should be and the backend will use it
Example:
  node app.js -home $HOME -port 80



config database
If -db-config is specified in the command line or db-config= in the local config file, this will trigger loading additional
config parameters from the specified database pool, it will load all records from the bk_config table on that db pool. Using the database to store
configuration make it easier to maintain dynamic environment for example in case of auto scaling or launching on demand, this way
a new instance will query current config from the database and this eliminates supporting text files and distributing them to all instances.
The config database is refreshed from time to time acording to the db-config-interval parameter, also all records with ttl property in the bk_config
will be pulled every ttl interval and updated in place.


DNS records
Some config options may be kept in the DNS TXT records and every time a instance is started it will query the local DNS for such parameters. Only a small subset of
all config parameters support DNS store. To see which parameters can be stored in the DNS run bkjs show-help and look for 'DNS TXT configurable'.


Backend library development (Mac OS X, developers)


for DB drivers and ImageMagick to work propely it needs some dependencies to be installed:
  port install libpng jpeg tiff lcms2 mysql56 postgresql93



make sure there is no openjpeg15 installed, it will conflict with ImageMagick jp2 codec


git clone https://github.com/vseryakov/backendjs.git or git clone git@github.com:vseryakov/backendjs.git


cd backendjs


if Node.js is already installed skip to the next section


to install binary release run the command, it will install it into /opt/local on Darwin
   bkjs install-node

   # To install into different path
   bkjs install-node -prefix /usr/local/node



Important: Add NODE_PATH=$BKJS_PREFIX/lib/node_modules to your environment in .profile or .bash_profile so
node can find global modules, replace $BKJS_PREFIX with the actual path unless this variable is also set in the .profile




to install all dependencies and make backendjs module and bkjs globally available:
      npm link backendjs



to run local server on port 8000 run command:
      bkjs web



to start the backend in command line mode, the backend environment is prepared and initialized including all database pools.
This command line access allows you to test and run all functions from all modules of the backend without running full server
similar to Node.js REPL functionality. All modules are accessible from the command line.
      $ ./bkjs shell
      > core.version
      '0.70.0'
      > logger.setLevel('info')



Design considerations
While creating Backendjs there were many questions and issues to be considered, some I was able to implement, some still not. Below are the thoughts that
might be useful when designing, developing or choosing the API platform:

purpose of the API:

to expose some parts of the existing system to external apps, users...
to make it the only way to access services
to complement another system


scalability considerations:

unlimited/uncontrolled access like mobile, web, more users the better
enterprise level, controlled growth
not to be horizontally scalable, just vertically


security:

support authentication, users, accounts, profiles...
just for robots, limited by api key only
signed requests only
support all access, web, mobile, desktop
user access controls, how to distinguish users, grant access to only parts of the API
ability to run custom/specific filters during processing API requests, independently and ability to extend the app without rewriting/rebuilding the whole system
third party authentication, OAUTH, user mapping


platform/framework:

one for all, same language/SDK/framework to cover all aspects
multiple languages/frameworks for different tasks, then how to integrate, how to communicate, share code
availability of the third party modules, libraries
support, forums, docs, how easy to learn for new developers
modularity, ability to develop by multiple developers, teams
flexibility in extending, how simple/easy to add custom stuff
maintenance, support,how easy to scale, change, replace parts


database layer:

one central database for everything
multiple database for different parts of the system according to scalability/other requirements
switch databases behind the scene in order to scale, adding to features, easier to maintain
caching, needs to be independent from other parts and easily enabled/disabled for different components preferably via config
to have or not ORM


process management, easy to deploy, monitor
logging, metrics, profiling
agnostic to the frontends or to be included with some kind of MVC/server based tools
ability to support simple Web development for simple web pages without installing/supporting general purpose tools like Apache/PHP/nginx

API endpoints provided by the backend
All API endpoints are optional and can be disabled or replaced easily. By default the naming convention is:
 /namespace/command[/subname[/subcommand]]

Any HTTP methods can be used because its the command in the URL that defines the operation. The payload can be url-encoded query
parameters or JSON or any other format supported by any particular endpoint. This makes the backend universal and usable with any
environment, not just a Web browser. Request signature can be passed in the query so it does not require HTTP headers at all.
Authentication and sessions
Signature
All requests to the API server must be signed with account login/secret pair.

The algorithm how to sign HTTP requests (Version 1, 2):

Split url to path and query parameters with ""?""
Split query parameters with ""&""
'''ignore parameters with empty names'''
'''Sort''' list of parameters alphabetically
Join sorted list of parameters with ""&""

Make sure all + are encoded as %2B


Form canonical string to be signed as the following:

Line1: The signature version
Line2: The application tag or other opaque data
Line3: The login name
Line4: The HTTP method(GET), followed by a newline.
Line5: The host name, lowercase, followed by a newline.
Line6: The request URI (/), followed by a newline.
Line7: The sorted and joined query parameters as one string, followed by a newline.
Line8: The expiration value in milliseconds, required, followed by a newline
Line9: The Content-Type HTTP header, lowercase, optional, followed by a newline
Line10: The SHA1 checksum of the body content, optional, for JSON and other forms of requests not supported by query parameters


Computed HMAC-SHA1 digest from the canonical string and encode it as BASE64 string, preserve trailing = if any
Form the signature HTTP header as the following:

The header string consist of multiple fields separated by pipe |

Field1: Signature version:

version 1, obsolete, do not use first 3 lines in the canonical string
version 2,3 to be used in session cookies only
version 4


Field2: Application tag or other app specific data
Field3: account login or whatever it might be in the login column
Field4: HMAC-SHA digest from the canonical string, version 1 uses SHA1, other SHA256
Field5: expiration value in milliseconds, same as in the canonical string
Field6: SHA1 checksum of the body content, optional, for JSON and other forms of requests not supported by query parameters
Field7: empty, reserved for future use







The resulting signature is sent as HTTP header bk-signature or in the header specified by the api-signature-name config parameter.
For JSON content type, the method must be POST and no query parameters specified, instead everything should be inside the JSON object
which is placed in the body of the request. For additional safety, SHA1 checksum of the JSON payload can be calculated and passed in the signature,
this is the only way to ensure the body is not modified when not using query parameters.
See web/js/bkjs.js function Bkjs.createSignature or
api.js function api.createSignature for the JavaScript implementations.
There is also native iOS implementation Bkjs.m.
Authentication API


/auth
This API request returns the current user record from the bk_auth table if the request is verified and the signature provided
is valid. If no signature or it is invalid the result will be an error with the corresponding error code and message.
By default this endpoint is secured, i.e. requires a valid signature.
Parameters:


_session=1 - if the call is authenticated a cookie with the session signature is returned, from now on
all requests with such cookie will be authenticated, the primary use for this is Web apps


_accesstoken=1 - returns new access token to be used for subsequent requests without a signature for the current account,
the token is short lived with expiration date returned as well. This access token can be used instead of a signature and
is passed in the query as bk-access-token=TOKEN.
Example:
     /auth?_accesstoken=1
     > { id: ""XXXX..."", name: ""Test User"", ""bk-access-token"": ""XXXXX...."", ""bk-access-token-age"": 604800000 }

     /account/get?bk-access-token=XXXXXX...
     > { id: ""XXXX..."", name: ""Test User"", ... }





/login
Same as the /auth but it uses secret for user authentication, this request does not need a signature, just simple
login and secret query parameters to be sent to the backend. This must be sent over SSL.
The intended usage is for Web sessions which use sessions cookies when sent with _session=1 or to be used with access tokens when
sent with _accesstoken=1.
Parameters:

login - account login
zecret - account secret
_session=1 - same as in /auth request
_accesstoken=1 - same as in /auth reuest

On successful login, the result contains full account record including the secret, this is the only time when the secret is returned back
Example:
        $.ajax({ url: ""/login?login=test123&secret=test123&_session=1"",
                 success: function(json, status, xhr) { console.log(json) }
        });

        > { id: ""XXXX..."", name: ""Test User"", login: ""test123"", ...}



/logout
Logout the current user, clear session cookies if exist. For pure API access with the signature this will not do anything on the backend side.


Accounts
The accounts API manages accounts and authentication, it provides basic user account features with common fields like email, name, address.
This is implemented by the accounts module from the core. To enable accounts functionality specify -allow-modules=bk_accounts.


/account/get
Returns information about current account or other accounts, all account columns are returned for the current account and only public columns
returned for other accounts. This ensures that no private fields ever be exposed to other API clients. This call also can used to login into the service or
verifying if the given login and secret are valid, there is no special login API call because each call must be signed and all calls are stateless and independent.
Parameters:

no id is given, return only one current account record as JSON
id=id,id,... - return information about given account(s), the id parameter can be a single account id or list of ids separated by comma
_session=1 - after successful login setup a session with cookies so the Web app can perform requests without signing every request anymore
_accesstoken=1 - after successful login, return new access token that ca be used to make requests without signing every request, it can be
passed in the query or headers with the name bk-access-token

Note: When retrieving current account, all properties will be present including the location, for other accounts only the properties marked as pub in the
bk_account table will be returned.
Response:
    { ""id"": ""57d07a4e28fc4f33bdca9f6c8e04d6c3"",
      ""name"": ""Test User"",
      ""name"": ""Real Name"",
      ""mtime"": 1391824028,
      ""latitude"": 34,
      ""longitude"": -118,
      ""geohash"": ""9qh1"",
      ""login"": ""testuser"",
    }



/account/add
Add new account, all parameters are the columns from the bk_account table, required columns are: name, secret, login.
To enable open registration add api-allow-path=^/account/add$ to the config file or specify in the command line.
More complex ways to perform registration will require adding pre and.or post callbacks to handle account registration
for example with invitation codes....
In the table bk_auth, the column type is used to distinguish between account roles, by default only account with type admin can
add other accounts with this type specified, this column can also be used in account permissions implementations. Because it is in the bk_auth table,
all columns of this table are available as req.account object after the successful authentication where req is an Express request object used in the middleware
parameters.
Secret and login can be anything, the backend does not require any specific formats and does not process the contents of the login/secret fields.
There are several ways to create authenticated account:

API only access with signed signature, supply a login and secret which will be stored in the database as is, when making requests use the same login and secret to produce the signature.
In the Web client web/js/bkjs.js, if Bkjs.scramble is set to 1 then the secret is replaced by the BASE64_HMAC_SHA256(secret, login) automatically,
no actual secret is ever saved or sent, only used in the login form. This is intended for Web apps not to store the actual secret anywhere in the memory or localStorage,
for the backend this is still just a secret.

Example:
      /account/add?name=test&login=test@test.com&secret=fc4f33bd07a4e6c8e&gender=f&phone=1234567

How to make an account as admin
      # Run backend shell
      bkjs shell

      # Update record by login
      > db.update(""bk_auth"", { login: 'login@name', type: 'admin' });



/account/select
Return list of accounts by the given condition, calls db.select for bk_account table. Parameters are the column values to be matched and
all parameters starting with underscore are control parameters that goes into options of the db.select call with underscore removed. This will work for SQL
databases only because DynamoDB or Cassandra will not search by non primary keys. In the DynamoDB case this will run ScanTable action which will be very expensive for
large tables. Supports special query parameters _select,_ops, see docs about db.select for more info.
Example:
      /account/search?email=test&_ops=email,begins_with
      /account/search?name=test

Response:
      {  ""data"": [{
                    ""id"": ""57d07a4e28fc4f33bdca9f6c8e04d6c3"",
                    ""name"": ""Test User1"",
                    ""mtime"": 1391824028,
                    ""login"": ""test1"",
                  },
                  {
                    ""id"": ""57d07a4e2824fc43bd669f6c8e04d6c3"",
                    ""name"": ""Test User2"",
                    ""mtime"": 1391824028,
                    ""login"": ""test2"",
                  }],
          ""next_token"": """"
      }



/account/del
Delete current account, after this call no more requests will be authenticated with the current credentials


/account/update
Update current account with new values, the parameters are columns of the table bk_account, only columns with non empty values will be updated.
Example:
      /account/update?name=New%2BName&gender=m



/account/put/secret
Change account secret for the current account, no columns except the secret will be updated and expected.
Parameters:

secret - new secret for the account
token_secret - set to 1 to reset access token secret to a new value thus revoking access from all existing access tokens

Example:
      /account/put/secret?secret=blahblahblah



/account/subcribe
Subscribe to account events delivered via HTTP Long Poll, a client makes the connection and waits for events to come, whenever
somebody updates the account's counter or send a message or creates a connection to this account the event about it will be sent to this HTTP
connection and delivered as JSON object. This is not a persistent queue so if not listening, all events will just be ignored, only events published
since the connect will be delivered. To specify what kind of events needs to be delivered, match query parameter can be specified which is a
RegExp of the whole event body string.
Note: On the server side there is a config parameter api-subscribe-interval which defines how often to deliver notifications, by default it is 5 seconds which means
only every 5 seconds new events will be delivered to the Web client, if more than one event happened, they all accumulate and will be sent as a JSON list.
Example:
  /account/subscribe
  /account/subscribe?match=connection/add.*type:*like

  // To run in the browser:
  (function poll() {
      Bkjs.send({ url: ""/account/subscribe"", complete: poll }, function(data) {
          console.log(""received event:"", data);
       });
   })();

Response:
  [ { ""path"": ""/message/add"", ""mtime:"" 1234566566, ""type"": ""1"" },
    { ""path"": ""/counter/incr"", ""mtime:"" 1234566566, ""type"": ""like,invite"" } },
    { ""path"": ""/connection/add"", ""mtime"": 1223345545, ""type"": ""like"" } ]



/account/select/icon
Return a list of available account icons, icons that have been uploaded previously with /account/put/icon calls. The url property is an URL to retrieve this particular icon.
Parameters:

id - if specified then icons for the given account will be returned

Example:
  /account/select/icon?id=12345

Response:
  [ { id: '12345', type: '1', url: '/account/get/icon?id=12345&type=1' },
    { id: '12345', type: '2', url: '/account/get/icon?id=12345&type=2' } ]



/account/get/icon
Return an account icon, the icon is returned in the body as binary BLOB, if no icon with specified type exists, i.e. never been uploaded then 404 is returned.
Parameters:

type - a number from 0 to 9 or any single letter a..z which defines which icon to return, if not specified 0 is used

Example:
  /account/get/icon?type=2



/account/put/icon
Upload an account icon, once uploaded, the next /account/get call will return properties in the format iconN where N is any of the
type query parameters specified here, for example if we uploaded an icon with type 5, then /account/get will return property icon5 with the URL
to retrieve this icon.
By default all icons uploaded only accessible for the account which uploaded them.
Parameters:

type - icon type, a number between 0 and 9 or any single letter a..z, if not specified 0 is used
icon - can be passed as base64 encoded image in the query,

can be passed as base64 encoded string in the body as JSON, like: { type: 0, icon: 'iVBORw0KGgoA...' },
for JSON the Content-Type HTTP headers must be set to application/json and data should be sent with POST request
can be uploaded from the browser using regular multi-part form


acl_allow - icon access permissions:

"""" (empty) - only own account can access
all - public, everybody can see this icon
auth - only authenticated users can see this icon
id,id.. - list of account ids that can see this account


_width - desired width of the stored icon, if negative this means do not upscale, if the image width is less than given keep it as is
_height - height of the icon, same rules apply as for the width above
_ext - image file format, default is jpg, supports: gif, png, jpg, jp2

Example:
  /account/put/icon?type=1&icon=iVBORw0KGgoAAAANSUhEUgAAAAcAAAAJCAYAAAD+WDajAAAABGdBTUEAALGPC/xhBQAAAAlwSFlzAAAOwgAADs....



/account/del/icon
Delete account icon
Parameters:

type - what icon to delete, if not specified 0 is used

Example:
  /account/icon/del?type=1



Health enquiry
When running with AWS load balancer there should be a url that a load balancer polls all the time and this must be very quick and lightweight request. For this
purpose there is an API endpoint /ping that just responds with status 200. It is not open by default, the allow-path or other way to allow non-authenticated access
needs to be configured. This is to be able to control how pinging can be perform in the apps in case it is not simple open access.
Public Images endpoint
This endpoint can server any icon uploaded to the server for any account, it is supposed to be a non-secure method, i.e. no authentication will be performed and no signature
will be needed once it is configured which prefix can be public using api-allow or api-allow-path config parameters.
The format of the endpoint is:


/image/prefix/id/type[.png|.jpg]
Example:
  # Configure accounts icons to be public in the etc/config
  api-allow-path=/image/account/

  # Or pass in the command line
  ./app.sh -api-allow-path /image/account/

  # Make requests
  /image/account/12345/0
  /image/account/12345/1
  /image/account/12345/1.jpg

  #Return icons for account 12345 for types 0 and 1



Icons
The icons API provides ability for an account to store icons of different types. Each account keeps its own icons separate form other
accounts, within the account icons can be separated by prefix which is just a namespace assigned to the icons set, for example to keep messages
icons separate from albums, or use prefix for each separate album. Within the prefix icons can be assigned with unique type which can be any string.
Prefix and type can consist from alphabetical characters and numbers, dots, underscores and dashes: [a-z0-9._-]. This means, they are identifiers, not real titles or names,
a special mapping between prefix/type and album titles for example needs to be created separately.
The supposed usage for type is to concatenate common identifiers first with more specific to form unique icon type which later can be queried
by prefix or exactly by icon type. For example album id can be prefixed first, then sequential con number like album1:icon1, album1:icon2....
then retrieving all icons for an album would be only query with album1: prefix.
The is implemented by the icons module from the core. To enable this functionality specify -allow-modules=bk_icons.


/icon/get
Return icon for the current account in the given prefix, icons are kept on the local disk in the directory
configured by -api-images-dir parameter(default is images/ in the backend directory). Current account id is used to keep icons
separate from other accounts. Icon presence is checked in the bk_icon table before returning it and if any permissions are set in
the acl_allow column it will be checked if this icon can be returned.
The following parameters can be used:

prefix - must be specified, this defines the icons namespace
type is used to specify unique icon created with such type which can be any string.
_ext - image extension, like png or jpg if it was saved with it previously



/icon/put
Upload new icon for the given account in the folder prefix, if type is specified it creates an icon for this type to separate
multiple icons for the same prefix. type can be any string consisting from alpha and digits characters. It creates a record in the bk_icon
table with all the parameters passed.
The following parameters can be used:

prefix - prefix for the icons, required
descr - optional description of the icon
latitude, longitude - optional coordinates for the icon
acl_allow - allow access permissions, see /account/put/icon for the format and usage
_width - desired width of the stored icon, if negative this means do not upscale, if the image width is less than given then keep it as is
_height - height of the icon, same rules apply as for the width above
_ext - image file format, default is jpg, supports: gif, png, jpg



/icon/upload
Upload a new image and store on the server, no record is created in bk_icon table, just simple image upload,
but all the same query parameters as for /icon/put are accepted. Returns an JSON object with url property being the full path
to the uploaded image.


/icon/del
Delete the default icon for the current account in the folder prefix or by type


/icon/select
Return list of available icons for the given prefix and type, all icons starting with prefix/type will be returned,
the url property will provide full URL to retrieve the icon contents
Example:
  /icon/select?prefix=album&type=me
  /icon/select?prefix=album&type=12345

Responses:
  [ { id: 'b3dcfd1e63394e769658973f0deaa81a', type: 'me-1', icon: '/icon/get?prefix=album&type=me1' },
    { id: 'b3dcfd1e63394e769658973f0deaa81a', type: 'me-2', icon: '/icon/get?prefix=album&type=me2' } ]

  [ { id: 'b3dcfd1e63394e769658973f0deaa81a', type: '12345-f0deaa81a', icon: '/icon/get?prefix=album&type=12345-f0deaa81a' } ]



File API
The file API provides ability to store and retrieve files. The operations are similar to the Icon API.
This is implemented by the files module from the core. To enable this functionality specify -allow-modules=bk_files.


/file/get
Return a file with given prefix and name, the contents are returned in the response body.
The following parameters can be used:

prefix - must be provided, defines the namespace where the file is stored
name - name of the file, required



/file/put
Store a file on the backend, the file can be sent using form multipart upload or as JSON
The following parameters can be used:

prefix - must be provided, defines the namespace where the file is stored
name - name of the file, required
_name - name of the property that contains the file contents, for use with JSON or defines the name of the file attribute for multipart upload
_tm - append the current timestamp to the file name
_ext - extension to be assign to the file, otherwise the actual extension from the file name is used



/file/del
Delete file, prefix and name must be given


Connections
The connections API maintains two tables bk_connection and bk_reference for links between accounts of any type. bk_connection table maintains my
links, i.e. when i make explicit connection to other account, and bk_reference table is automatically updated with reference for that other account that I made
a connection with it. No direct operations on bk_reference is allowed.
This is implemented by the connections module from the core. To enable this functionality specify -allow-modules=bk_connections.


/connection/add


/connection/put
Create or replace a connection between two accounts, required parameters are:

peer - id of account to connect to
type - type of connection, like,dislike,....
_connected - the reply will contain a connection record if the other side of our connection is connected to us as well

This call automatically creates a record in the bk_reference table which is reversed connection for easy access to information like
''who is connected to me''.
Example:
  /connection/add?peer=12345&type=invite&state=sent



/connection/update


/connection/incr
Update other properties of the existing connection, for connections that may take more than i step or if a connection has other data associated with it beside
the type of the connection.
Example:
  /connection/update?peer=12345&type=invite&state=accepted



/connection/del
Delete existing connection(s), id and/or type may be be specified, if not all existing connections will be deleted.
Example:
  /connection/del?type=invite&peer=12345



/connection/get
Return a single connection for given id
Parameters:

peer - account id of the connection, required
type - connection type, required

Example:
  /connection/get?peer=12345&type=like

Response:
  { ""id"": ""1111"",
    ""type: ""like"",
    ""peer"": ""12345"",
    ""mtime"": ""2434343543543"" }



/reference/get
Return a single reference record for given account id, works the same way as /connection/get


/connection/select
Receive all my connections of the given type, i.e. connection(s) i made, if id is given only one record for the specified connection will be returned. Supports special
query parameters _select,_ops,_desc, see docs about db.select for more info.
Example:
  # Return all accounts who i invited
  /connection/select?type=invite
  # Return connection for specific type and account id
  /connection/select?type=invite&peer=12345
  # Return accounts who i invited me after specified mtime
  /connection/select?type=invite&_ops=mtime,gt&mtime=12334312543
  # Return accounts who i invited before specified mtime
  /connection/select?type=invite&_ops=mtime,le&_desc=1&mtime=12334312543

Response:
  { ""data"": [ { ""id"": ""111"",
                ""type"": ""invite"",
                ""peer"": ""12345"",
                ""status"": """",
                ""mtime"": ""12334312543""
            }],
    ""next_token"": """"
  }



/reference/select
Receive all references that connected with my account, i.e. connections made by somebody else with me, works the same way as for connection query call
Example:
  # Return all accounts who invited me
  /reference/select?type=invite
  # Return accounts who invited me after specified mtime
  /reference/select?type=invite&_ops=mtime,gt&mtime=12334312543

Response:
  { ""data"": [ { ""id"": ""111"",
                ""type"": ""invite"",
                ""peer"": ""12345"",
                ""status"": """",
                ""mtime"": ""12334312543""
            }],
    ""next_token"": """"
  }



Locations
The location API maintains a table bk_location with geolocation coordinates for accounts and allows searching it by distance. The configuration parameter
min-distance defines the radius for the smallest bounding box in km containing single location, radius searches will combine neighboring boxes of
this size to cover the whole area with the given distance request, also this affects the length of geohash keys stored in the bk_location table. By default min-distance is 5 km
which means all geohashes in bk_location table will have geohash of size 4. Once min-distance is set it cannot be changed without rebuilding the bk_location table with new geohash size.
The location search is implemented by using geohash as a primary key in the bk_location table with the account id as the second part of the primary key, for DynamoDB this is the range key.
When request comes for all matches for the location for example 37.7, -122.4, the search that is executed looks like this:

geohash for latitude 37.7 and longitude -122.4 and radius 10 km will be 9q8y
all neighboring areas around this point within 10 km radius will be '9q8z', '9q8v', '9q8w', '9q8x', '9q8t', '9q9n', '9q9p', '9q9j'
we start the search on the bk_location table by the primary key geohash with the value 9q8y
filter out all records beyond our radius by calculating the difference between our point and the candidate record
if total number of results expected is still less than required, continue to the next neighbor area
continue until we visit all neighbors or received required number of matched records
on return the next_token opaque value will be provided if we want to continue the search for more matched for the same location

This is implemented by the locations module from the core. To enable this functionality specify allow-modules=bk_locations.


/location/put
Store currenct location for current account, latitude and longitude parameters must be given, this call will update the bk_account table as well with
these coordinates
Example:
  /location/put?latitude=-188.23232&longitude=23.4545454



/location/get
Return matched accounts within the distance(radius) specified by distance= parameter in kilometers and current position specified by latitude/longitude parameters. This
call returns results in chunks and requires navigation through all pages to receive all matched records. Records returned will start with the closest to the current
point. If there are more matched records than specified by the _count, the next_token property is set with the token to be used in the subsequent call,
it must be passed as is as _token= parameter with all original query parameters.
Note: The current account will not be present in the results  even if it is within the range, to know my own location use /account/get call.
Example:
      /location/get?distance=10&latitude=-118.23434&longitude=23.45665656&_count=25
      /location/get?distance=10&latitude=-118.23434&longitude=23.45665656&_count=25&_token=FGTHTRHRTHRTHTTR.....

Response:
     { ""data"": [ { ""id"": ""12345"",
                   ""distance"": 5,
                   ""latitude"": -118.123,
                   ""longitude"": 23.45
                   ""mtime"": ""12334312543""
                 },
                 { ""id"": ""45678"",
                   ""distance"": 5,
                   ""latitude"": -118.133,
                   ""longitude"": 23.5
                   ""mtime"": ""12334312543""
                 }],
       ""next_token"": """"
     }



Messages
The messaging API allows sending and receiving messages between accounts, it supports text and images. All new messages arrive into the bk_messsage table, the inbox. The client
may keep messages there as new, delete or archive them. Archiving means transferring messages into the bk_archive table. All sent messages are kept in the bk_sent table.
This is implemented by the messages module from the core. To enable this functionality specify -allow-modules=bk_messages.


/message/get/unread
Return how many unread messages in the inbox, this is just a flag to signal about new messages, the actual number may not be up to date,
it is cleared on messages read.
Example:
 /message/get/unread

Response:
{ count: 1 }



/message/get
Read all messages from the inbox.
Parameters:

_archive - if set to 1, all returned messages will be archived automatically, so no individual /message/read call needed
_trash - if set to 1, all returned messages will be deleted, not archived
_total - if set to 1 then return how many messages in the inbox
the unread flag with the actual number of unread messages.

Example:
  # Get all new messages
  /message/get

  # Get all new messages and archive them
  /message/get?_archive=1

  # Get all new messages from the specific sender
  /message/get?sender=12345

  # How many new messages
  /message/get?_total=1



/message/get/archive
Receive archived messages. The images are not returned, only link to the image in icon property of reach record,
the actual image data must be retrieved separately.
Parameters:

mtime - if specified then only messages received since that time will be returned, it must be in milliseconds since midnight GMT on January 1, 1970, this is what
Date.now() return in JavaScript.
sender - if specified then all messages from the given sender will be returned.

NOTE: The mtime is when the backend server received the message, if client and the server clocks are off this may return wrong data or not return anything at all,
also because the arrival order of the messages cannot be guaranteed, sending fast multiple messages may be received in different order by the backend and this will
result in mtimes that do not correspond to actual times when the message has been sent.
Example:
  # Get all messages
  /message/get/archive

  # Get all messages received after given mtime
  /message/get/archive?mtime=123475658690

  # Get all messages received before given mtime
  /message/get/archive?mtime=123475658690&_ops=mtime,lt

  # Get all messages with custom filter: if msg text contains Hi
  /message/get/archive?_ops=msg,iregexp&msg=Hi

  # Get all messages from the specific sender
  /message/get/archive?sender=12345

Response:
  { ""data"": [ { ""sender"": ""12345"",
                ""msg"": ""Hi, how r u?"",
                ""mtime"": ""12334312543""
              },
              { ""sender"": ""45678"",
                ""msg"": ""check this out!"",
                ""icon"": ""/message/image?sender=45678&mtime=12334312543"",
                ""mtime"": ""12334312543""
              }],
       ""next_token"": """"
     }



/message/get/sent
Return all messages i sent out. All the same query rules apply as for the archived messages API call.
Parameters:

recipient - id of the recipient where i have sent messages
mtime - time before or after messages sent, defined by _ops parametrs

Example:
  /message/get/sent?recipient=123
  /message/get/sent?recipient=123&mtime=123475658690&_ops=mtime,le



/message/add
Send a message to an account, the following parameters must be specified:

id - recipient account id
msg - text of the message, can be empty if icon property exists
icon - icon of the message, it can be base64 encoded image in the query or JSON string if the whole message is posted as JSON or
can be a multipart file upload if submitted via browser, can be omitted if msg/connection/get?type=invite&id=12345 property exists.
_nosent - do not save this message in my sent messages

Example:
  /message/add?id=12345&msg=Hello
  /message/add?id=12345&msg=this%2Bis%2Bthe%2Bpic&icon=KHFHTDDKH7676758JFGHFDRDEDET....TGJNK%2D



/message/read
Mark a message as read
Example:
  /message/read?sender=12345&mtime=124345656567676



/message/archive
Move a new message to the archive. The required query parameters are sender and mtime.
Example:
  /message/read?sender=12345&mtime=12366676434



/message/update
Update a message, can be used to keep track of read/unread status, etc...
Example:
  /message/update?sender=12345&mtime=124345656567676&read=1



/message/update/archive
Update a message in the archive.


/message/del
Delete new message(s) by sender and/or mtime which must be passed as query parameters. If no mtime is given, all messages from the given sender will be deleted.
Example:
  /message/del?sender=12345&mtime=124345656567676



/message/del/archive
Delete archived message(s) by sender and/or mtime which must be passed as query parameters. If no mtime is given, all messages from the given sender will be deleted.
Example:
  /message/del/archive?sender=12345&mtime=124345656567676



/message/del/sent
Delete the message(s) by recipient and/or mtime which must be passed as query parameters. If no mtime is given, all messages to the given recipient will be deleted.
Example:
  /message/del/sent?recipient=12345&mtime=124345656567676



/message/image
Return the image data for the given message, the required parameters are:

sender - id of the sender returned in the by /message/get reply results for every message
mtime - exact timestamp of the message



Counters
The counters API maintains realtime counters for every account records, the counters record may contain many different counter columns for different purposes and
is always cached with whatever cache service is used, by default it is cached by the Web server process on every machine. Web worker processes ask the master Web server
process for the cached records thus only one copy of the cache per machine even in the case of multiple CPU cores.
This is implemented by the counters module from the core. To enable this functionality specify -allow-modules=bk_counters|bk_accounts.


/counter/get
Return counter record for current account with all available columns of if id is given return public columns for given account, it works with bk_counter table
which by default defines some common columns:

ping - a counter for general use, can be used to send a notification event to any account by increasing this counter for an account
like0 - how many i liked, how many time i liked someone, i.e. made a new record in bk_connection table with type 'like'
like1 - how many liked me, reverse counter, who connected to me with type 'like'
More columns can be added to the bk_counter table.

NOTE: The columns with suffixes 0 and 1 are special columns that support the Connections API, every time a new connection is created, the type of new connection
is checked against any columns in the bk_counter table, if a property type0 exists and marked in the table description as autoincr then the corresponding
counter property is increased, this is how every time new connection like/dislike/invite/follow is added, the counters in the bk_counter table are increased.


/counter/put
Replace my counters record, all values if not specified will be set to 0


/counter/incr
Increase one or more counter fields, each column can provide a numeric value and it will be added to the existing value, negative values will be substracted.
if id parameter is specified, only public columns will be increased for other account.
Example:
  /counter/incr?msg_read=5&
  /counter/incr?id=12345&ping=1



Data
The data API is a generic way to access any table in the database with common operations, as oppose to the any specific APIs above this API only deals with
one table and one record without maintaining any other features like auto counters, cache...
Because it exposes the whole database to anybody who has a login it is a good idea to disable this endpoint in the production or provide access callback that verifies
who can access it.


To disable this endpoint completely in the config: deny-modules=data


To allow admins to access it only in the config: api-allow-admin=^/data


To allow admins to access it only:
api.registerPreProcess('GET', '/data', function(req, status, cb) { if (req.account.type != ""admin"") return cb({ status: 401, message: 'access denied' }; cb(status)); });



This is implemented by the data module from the core.


/data/columns


/data/columns/TABLE
Return columns for all tables or the specific TABLE


/data/keys/TABLE
Return primary keys for the given TABLE


/data/(select|search|list|get|add|put|update|del|incr|replace)/TABLE
Perform database operation on the given TABLE, all options for the db functiobns are passed as query parametrrs prepended with underscore,
regular parameters are the table columns.
By default the API does not allow table scans without a condition to avoid expensive and long queries, to enable a scan pass _noscan=0.
For this to work the Data API must be configured as unsecure in the config file using the parameter api-unsecure=data.
Some tables like messages and connections perform data convertion before returning the results, mostly splitting combined columns like type into
separate fields. To return raw data pass the parameter _noprocessrows=1.
Example:
  /data/get/bk_account?id=12345
  /data/put/bk_counter?id=12345&like0=1
  /data/select/bk_account?name=john&_ops=name,gt&_select=name,email
  /data/select/bk_connection?_noscan=0&_noprocessrows=1



Pages
The pages API provides a simple Wiki like system with Markdown formatting. It keeps all pages in the database table bk_pages and
exposes an API to manage and render pages.
The pages support public mode, all pages with pub set to true will be returning without an account, this must be enabled with api-allow-path=^/pages/(get|select|show)
to work.
All .md files will be rendered into html automatically if there is not _raw=1 query parameter and pages view exists (api-pages-view=pages.html by default).
This is implemented by the pages module from the core. To enable this functionality specify -allow-modules=bk_accounts.


/pages/get/ID
Return a page with given id or the main page if id is empty. If the query parameter _render=1 is given, the content will be rendered into html from markdown, otherwise
returns all data as is.


/pages/select
Return all pages or only ones which match the query criteria. This potentially scans the whole table to return all pages and
is used to show pages index.


/pages/put
Replace or add a new page.


/pages/del
Delete a page from the database


/pages/show/ID
Render a page with given id, markdown is converted into html using marked. A view must be configured in order to render to work, by default pages.html view
is provided to simply wrap the markdown in the page layout.


System API
The system API returns information about the backend statistics, allows provisioning and configuration commands and other internal maintenance functions. By
default is is open for access to all users but same security considerations apply here as for the Data API.
This is implemented by the system module from the core. To enable this functionality specify -allow-modules=accounts.


/system/restart
Perform restart of the Web processes, this will be done gracefully, only one Web worker process will be restarting while the other processes will keep
serving requests. The intention is to allow code updates on live systems without service interruption.


/system/cache/(init|stats|keys|get|set|put|incr|del|clear)
Access to the caching functions


/system/config/(init)
Access to the config functions


/system/msg/(init|send)
Access to the messaging functions


/system/jobs/(send)
Access to the jobs functions


/system/queue/(init|publish)
Access to the queue functions


/system/params/get
Return all config parameters applied from the config file(s) or remote database.


/system/stats/get
Database pool statistics and other diagnostics

latency - how long a pending request waits in queue at this moment
busy - how many busy error responses have been returned so far
pool - database metrics

response - stats about how long it takes between issuing the db request and till the final moment all records are ready to be sent to the client
queue - stats about db requests at any given moment queued for the execution
cache - db cache response time and metrics


api - Web requests metrics, same structure as for the db pool metrics
url - metrics per url endpoints

Individual sub-objects:

meter - Things that are measured as events / interval.

rmean: The average rate since the meter was started.
rcnt: The total of all values added to the meter.
rate: The rate of the meter since the last toJSON() call.
r1m: The rate of the meter biased towards the last 1 minute.
r5m: The rate of the meter biased towards the last 5 minutes.
r15m: The rate of the meter biased towards the last 15 minutes.


queue or histogram - Keeps a reservoir of statistically relevant values biased towards the last 5 minutes to explore their distribution

hmin: The lowest observed value.
mmax: The highest observed value.
hsum: The sum of all observed values.
hvar: The variance of all observed values.
hmean: The average of all observed values.
hdev: The standard deviation of all observed values.
hcnt: The number of observed values.
hmed: median, 50% of all values in the reservoir are at or below this value.
hp75: See median, 75% percentile.
hp95: See median, 95% percentile.
hp99: See median, 99% percentile.
hp999: See median, 99.9% percentile.



Response:
       {
            ""id"": ""172.31.31.85-25170"",
            ""ip"": ""172.31.31.85"",
            ""mtime"": 1417500027321,
            ""ctime"": 1416941754760,
            ""type"": """",
            ""host"": """",
            ""pid"": 25170,
            ""instance"": ""i-d4c89eff"",
            ""worker"": 27,
            ""latency"": 0,
            ""cpus"": 4,
            ""mem"": 15774367744,
            ""rss_hmin"": 66879488,
            ""rss_hmax"": 151891968,
            ""rss_hsum"": 2451506479104,
            ""rss_hvar"": 254812067010902.66,
            ""rss_hmean"": 118895507.98312236,
            ""rss_hdev"": 15962833.92793719,
            ""rss_hcnt"": 20619,
            ""rss_hmed"": 147644416,
            ""rss_h75p"": 149262336,
            ""rss_h95p"": 150834585.6,
            ""rss_h99p"": 151550033.92000002,
            ""rss_h999p"": 151886266.368,
            ""heap_hmin"": 25790920,
            ""heap_hmax"": 72316184,
            ""heap_hsum"": 1029889929504,
            ""heap_hvar"": 54374337037311.65,
            ""heap_hmean"": 49948587.68630874,
            ""heap_hdev"": 7373895.648658967,
            ""heap_hcnt"": 20619,
            ""heap_hmed"": 57480704,
            ""heap_h75p"": 61934254,
            ""heap_h95p"": 67752391.2,
            ""heap_h99p"": 70544797.92,
            ""heap_h999p"": 72315029.104,
            ""avg_hmin"": 0.04541015625,
            ""avg_hmax"": 0.06005859375,
            ""avg_hsum"": 938.234375,
            ""avg_hvar"": 4.491222722966496e-7,
            ""avg_hmean"": 0.04550338886463941,
            ""avg_hdev"": 0.0006701658543201448,
            ""avg_hcnt"": 20619,
            ""avg_hmed"": 0.04541015625,
            ""avg_h75p"": 0.04541015625,
            ""avg_h95p"": 0.04541015625,
            ""avg_h99p"": 0.05078125,
            ""avg_h999p"": 0.05997363281250001,
            ""free_hmin"": 12879872000,
            ""free_hmax"": 13228994560,
            ""free_hsum"": 268429937405952,
            ""free_hvar"": 5839592954606286,
            ""free_hmean"": 13018572064.889277,
            ""free_hdev"": 76417229.43555522,
            ""free_hcnt"": 20619,
            ""free_hmed"": 12908707840,
            ""free_h75p"": 12915716096,
            ""free_h95p"": 12919331430.4,
            ""free_h99p"": 12922073088,
            ""free_h999p"": 12922164563.968,
            ""util_hmin"": 0.05905642141342145,
            ""util_hmax"": 0.0607655708794173,
            ""util_hsum"": 1230.6298386264643,
            ""util_hvar"": 2.1530671850148948e-7,
            ""util_hmean"": 0.059684263961708346,
            ""util_hdev"": 0.0004640115499656118,
            ""util_hcnt"": 20619,
            ""util_hmed"": 0.05920415878947068,
            ""util_h75p"": 0.059217278415661254,
            ""util_h95p"": 0.05934395790869296,
            ""util_h99p"": 0.059361851867105964,
            ""util_h999p"": 0.0593659827984017,
            ""pool_name"": ""dynamodb"",
            ""pool_que_rate"": 0,
            ""pool_que_rcnt"": 1989,
            ""pool_que_rmean"": 0.0035627883554577716,
            ""pool_que_r1m"": 0,
            ""pool_que_r5m"": 0,
            ""pool_que_r15m"": 0,
            ""pool_que_hmin"": 0,
            ""pool_que_hmax"": 230,
            ""pool_que_hsum"": 45843,
            ""pool_que_hvar"": 366.86587852909315,
            ""pool_que_hmean"": 23.048265460030166,
            ""pool_que_hdev"": 19.15374319889178,
            ""pool_que_hcnt"": 1989,
            ""pool_que_hmed"": 21,
            ""pool_que_h75p"": 23,
            ""pool_que_h95p"": 33,
            ""pool_que_h99p"": 126.42000000000007,
            ""pool_que_h999p"": 225.971,
            ""pool_req_hmin"": 1,
            ""pool_req_hmax"": 2,
            ""pool_req_hsum"": 1991,
            ""pool_req_hvar"": 0.001005024617286425,
            ""pool_req_hmean"": 1.0010055304172951,
            ""pool_req_hdev"": 0.03170212322994195,
            ""pool_req_hcnt"": 1989,
            ""pool_req_hmed"": 1,
            ""pool_req_h75p"": 1,
            ""pool_req_h95p"": 1,
            ""pool_req_h99p"": 1,
            ""pool_req_h999p"": 1.9710000000000036,
            ""pool_count"": 0,
            ""pool_req_0"": 2,
            ""pool_cache_rate"": 0.1303780964797914,
            ""pool_cache_rcnt"": 284,
            ""pool_cache_rmean"": 0.0005087436344326025,
            ""pool_cache_r1m"": 0,
            ""pool_cache_r5m"": 0,
            ""pool_cache_r15m"": 0,
            ""pool_cache_hmin"": 0,
            ""pool_cache_hmax"": 2,
            ""pool_cache_hsum"": 70,
            ""pool_cache_hvar"": 0.19345045538247163,
            ""pool_cache_hmean"": 0.24647887323943662,
            ""pool_cache_hdev"": 0.4398300301053483,
            ""pool_cache_hcnt"": 284,
            ""pool_cache_hmed"": 0,
            ""pool_cache_h75p"": 0,
            ""pool_cache_h95p"": 1,
            ""pool_cache_h99p"": 1,
            ""pool_cache_h999p"": 2,
            ""pool_hits"": 239,
            ""pool_misses"": 45,
            ""cache_inserted"": 484,
            ""cache_deleted"": 310,
            ""cache_cleanups"": 0,
            ""cache_hits"": 7642,
            ""cache_misses"": 1411,
            ""cache_max"": 1000000,
            ""cache_size"": 61586,
            ""cache_count"": 174,
            ""api_que_hmin"": 1,
            ""api_que_hmax"": 6,
            ""api_que_hsum"": 13237,
            ""api_que_hvar"": 0.005674280465987009,
            ""api_que_hmean"": 1.0024992426537414,
            ""api_que_hdev"": 0.07532782000022972,
            ""api_que_hcnt"": 13204,
            ""api_que_hmed"": 1,
            ""api_que_h75p"": 1,
            ""api_que_h95p"": 1,
            ""api_que_h99p"": 1,
            ""api_que_h999p"": 2,
            ""api_nreq"": 1,
            ""api_req_rate"": 0,
            ""api_req_rcnt"": 13203,
            ""api_req_rmean"": 0.02365120609256502,
            ""api_req_r1m"": 0,
            ""api_req_r5m"": 0,
            ""api_req_r15m"": 0,
            ""api_req_hmin"": 0,
            ""api_req_hmax"": 536,
            ""api_req_hsum"": 20115,
            ""api_req_hvar"": 89.12554520926801,
            ""api_req_hmean"": 1.5235173824130879,
            ""api_req_hdev"": 9.440632669968046,
            ""api_req_hcnt"": 13203,
            ""api_req_hmed"": 1,
            ""api_req_h75p"": 1,
            ""api_req_h95p"": 1,
            ""api_req_h99p"": 33.13000000000011,
            ""api_req_h999p"": 99.36200000000008,
            ""url_message_get_rate"": 0,
            ""url_message_get_rcnt"": 24,
            ""url_message_get_rmean"": 0.00004299242196761214,
            ""url_message_get_r1m"": 0,
            ""url_message_get_r5m"": 0,
            ""url_message_get_r15m"": 0,
            ""url_message_get_hmin"": 16,
            ""url_message_get_hmax"": 71,
            ""url_message_get_hsum"": 792,
            ""url_message_get_hvar"": 208.34782608695653,
            ""url_message_get_hmean"": 33,
            ""url_message_get_hdev"": 14.434258764722092,
            ""url_message_get_hcnt"": 24,
            ""url_message_get_hmed"": 30.5,
            ""url_message_get_h75p"": 40.75,
            ""url_message_get_h95p"": 68,
            ""url_message_get_h99p"": 71,
            ""url_message_get_h999p"": 71,
            ""url_message_get_0"": 0,
            ""api_req_0"": 20,
            ""url_ping_rate"": 0,
            ""url_ping_rcnt"": 12407,
            ""url_ping_rmean"": 0.022226981327796796,
            ""url_ping_r1m"": 0,
            ""url_ping_r5m"": 0,
            ""url_ping_r15m"": 0,
            ""url_ping_hmin"": 0,
            ""url_ping_hmax"": 4,
            ""url_ping_hsum"": 6915,
            ""url_ping_hvar"": 0.25785489698686204,
            ""url_ping_hmean"": 0.5573466591440316,
            ""url_ping_hdev"": 0.5077941482400737,
            ""url_ping_hcnt"": 12407,
            ""url_ping_hmed"": 1,
            ""url_ping_h75p"": 1,
            ""url_ping_h95p"": 1,
            ""url_ping_h99p"": 1,
            ""url_ping_h999p"": 2,
            ""url_ping_0"": 5,
            ""url_image_account_rate"": 0,
            ""url_image_account_rcnt"": 95,
            ""url_image_account_rmean"": 0.00017084907295404685,
            ""url_image_account_r1m"": 0,
            ""url_image_account_r5m"": 0,
            ""url_image_account_r15m"": 0,
            ""url_image_account_hmin"": 17,
            ""url_image_account_hmax"": 121,
            ""url_image_account_hsum"": 4295,
            ""url_image_account_hvar"": 372.42329227323637,
            ""url_image_account_hmean"": 45.21052631578947,
            ""url_image_account_hdev"": 19.29827174317007,
            ""url_image_account_hcnt"": 95,
            ""url_image_account_hmed"": 42,
            ""url_image_account_h75p"": 51,
            ""url_image_account_h95p"": 89.59999999999991,
            ""url_image_account_h99p"": 121,
            ""url_image_account_h999p"": 121,
            ""url_image_account_0"": 0,
            ""incr_follow_0"": 0,
            ""api_bad_0"": 3,
            ""url_account_update_rate"": 0,
            ""url_account_update_rcnt"": 6,
            ""url_account_update_rmean"": 0.000010813705805470248,
            ""url_account_update_r1m"": 0,
            ""url_account_update_r5m"": 0,
            ""url_account_update_r15m"": 0,
            ""url_account_update_hmin"": 53,
            ""url_account_update_hmax"": 182,
            ""url_account_update_hsum"": 573,
            ""url_account_update_hvar"": 2041.5,
            ""url_account_update_hmean"": 95.5,
            ""url_account_update_hdev"": 45.18296139032943,
            ""url_account_update_hcnt"": 6,
            ""url_account_update_hmed"": 82,
            ""url_account_update_h75p"": 120.5,
            ""url_account_update_h95p"": 182,
            ""url_account_update_h99p"": 182,
            ""url_account_update_h999p"": 182,
            ""url_account_update_0"": 0,
            ""auth_add_0"": 0,
            ""url_account_get_rate"": 0,
            ""url_account_get_rcnt"": 9,
            ""url_account_get_rmean"": 0.0001993511695335063,
            ""url_account_get_r1m"": 0,
            ""url_account_get_r5m"": 0,
            ""url_account_get_r15m"": 0,
            ""url_account_get_hmin"": 2,
            ""url_account_get_hmax"": 100,
            ""url_account_get_hsum"": 435,
            ""url_account_get_hvar"": 844.0000000000001,
            ""url_account_get_hmean"": 48.333333333333336,
            ""url_account_get_hdev"": 29.051678092667903,
            ""url_account_get_hcnt"": 9,
            ""url_account_get_hmed"": 46,
            ""url_account_get_h75p"": 67,
            ""url_account_get_h95p"": 100,
            ""url_account_get_h99p"": 100,
            ""url_account_get_h999p"": 100,
            ""url_account_get_0"": 1,
            ""url_system_stats_rate"": 0,
            ""url_system_stats_rcnt"": 1,
            ""url_system_stats_rmean"": 0.04501665616278023,
            ""url_system_stats_r1m"": 0,
            ""url_system_stats_r5m"": 0,
            ""url_system_stats_r15m"": 0,
            ""url_system_stats_hmin"": 3,
            ""url_system_stats_hmax"": 3,
            ""url_system_stats_hsum"": 3,
            ""url_system_stats_hmean"": 3,
            ""url_system_stats_hdev"": 0,
            ""url_system_stats_hcnt"": 1,
            ""url_system_stats_hmed"": 3,
            ""url_system_stats_h75p"": 3,
            ""url_system_stats_h95p"": 3,
            ""url_system_stats_h99p"": 3,
            ""url_system_stats_h999p"": 3,
            ""url_system_stats_0"": 2
        }



Author
Vlad Seryakov
Check out the Documentation for more details.
",39
r3eckon/Unity-SimpleGrid-Shader,ShaderLab,"Unity-SimpleGrid-Shader
Simple procedural grid shader with GUI customizable parameters
Usage
Drag and drop both Shader and Material files in your project. The Material can then be edited and also dropped on a gameobject mesh inside Unity.
To edit the grid material, the following GUI options are available.

Line Color, Cell Color and Selected Color all represent their respective grid components. Colors use Alpha Cutoff for full transparency by setting the alpha channel to 0.
Grid Size is the amount of cells in the grid. You must make a simple edit to the shader itself to go above 100.
Line Size is the size of lines making up the grid.
Select Cell enables or disables the cell ""Selected"" by the parameters Selected Cell X and Y. It only colors a certain cell using a different color, but the base code to ""Select"" cells in the grid can be used in more complex ways.
The parameters shown above create the following grid.

More information
There is no aspect ratio based scaling by default, to keep cells squared keep the plane mesh square ( or implement your own aspect ratio scaling )
The selected cell can also be chosen by editing the material using the SetFloat value, to potentially make the material interactive. ( See official unity docs to learn more )
The shader, when applied to new materials, can be found under ""PDTShaders/TestGrid"".
",6
jeaye/safepaste,Clojure,"safepaste
safepaste is a security-conscious paste service for sharing private, encrypted data. All encryption is done client-side and it's impossible for the server, admin, or anyone without your 256 bit key to view the paste. All pastes are encrypted using AES-256.
Find it online here.
Features

AES-256 with random 256 bit secret keys
Always over HTTPS
Optional ""burn after reading""
Always free and open source

Learn more about the service
here.
Command-line tool
There is a command line tool for uploading and downloading pastes, written in
bash, included in the repo. It supports all of the same options as the web page
and performs the client-side encryption using openssl. To install it, use:
$ wget https://raw.githubusercontent.com/jeaye/safepaste/master/tool/safepaste
$ chmod +x ./safepaste

# To paste file (assuming safepaste script is within PATH):
$ safepaste < my-file

# To paste command output:
$ some-command | safepaste

# To download and decrypt a paste:
$ safepaste https://safepaste.org/f1a8f535#31bcdb56b77528a3c1b540bc460ed07d5b74fcf65eb91733bc4d10884e764caf

# To see more options:
$ safepaste -h
License
safepaste is under a strict copyleft license; see the
LICENSE file.
",21
jeaye/safepaste,Clojure,"safepaste
safepaste is a security-conscious paste service for sharing private, encrypted data. All encryption is done client-side and it's impossible for the server, admin, or anyone without your 256 bit key to view the paste. All pastes are encrypted using AES-256.
Find it online here.
Features

AES-256 with random 256 bit secret keys
Always over HTTPS
Optional ""burn after reading""
Always free and open source

Learn more about the service
here.
Command-line tool
There is a command line tool for uploading and downloading pastes, written in
bash, included in the repo. It supports all of the same options as the web page
and performs the client-side encryption using openssl. To install it, use:
$ wget https://raw.githubusercontent.com/jeaye/safepaste/master/tool/safepaste
$ chmod +x ./safepaste

# To paste file (assuming safepaste script is within PATH):
$ safepaste < my-file

# To paste command output:
$ some-command | safepaste

# To download and decrypt a paste:
$ safepaste https://safepaste.org/f1a8f535#31bcdb56b77528a3c1b540bc460ed07d5b74fcf65eb91733bc4d10884e764caf

# To see more options:
$ safepaste -h
License
safepaste is under a strict copyleft license; see the
LICENSE file.
",21
narigacdo/automation-all,Shell,"automation-all
Scripts para diversos tipo de função
",2
jaymcole/DungeonCrawler2,Java,"Dungeon Crawler 2
2D Java Rogue-like rpg dungeon crawler game.
(Screenshots below)
Features

Procedurally generated levels
2D fragment based lighting
Simple Game AI

Pathfinding
Targeting/Attacking/Etc.


100% custom GUI/Interface

Custom windows and window management
Custom widgets
Custom event handling


Experience/Leveling system

Gain attribute points to improve your character by defeating dungeon monsters.


Inventory management system
Fast-paced gameplay
Loot!

Items/Loot can be found throughout the dungeon or by defeating dungeon monsters.


Record keeping system

Keeps track of various in-game stats (time played/attribute points spent/damage dealt/damage taken and many more)
Allows for developers to quickly and easily add new stats to record with just two short lines of code.


Custom asset/resource manager

Ensures duplicate resources aren't loaded into memory.
Ensures resources are disposed of correctly.


Custom Animations system

Getting Started
These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.

Clone DungeonCrawler2 repo
Import DungeonCrawler2 into Eclipse as a Gradle project

Note: There are currently incompatibility issues with Gradle and Java 10.


Set DungeonCrawler2 to use the assets folder

Right-click ""DungeonCrawler2-desktop"" (in Eclipse project explorer)
Run As
Run Configurations
Arguments (tab)
Under ""Working directory:""
Change from ""Default"" to ""Other""
""Workspace..."" (button)
Select ""DungeonCrawler2 > core > assets""
""OK"" (button)


Starting the game

Launch DungeonCrawler2-desktop > src > ecu.se.desktop > DesktopLauncher.java as Java Application



Built With

LibGdx - The graphical framework

Authors

Jason Cole

License
This project is licensed under the MIT License - see the LICENSE.md file for details
Screenshots



",2
ppy/osu-wiki,None,"osu! wiki

Home of the osu! wiki.
Contributing
Please see the ""contributing"" file if you are interested in helping out with the project!
File caching
Wiki articles
Articles are cached for up to five hours.
Images
Images are cached for up to two hours.
News posts
News posts are cached for up to sixty days. If there are any issues after merging a news post, merge a pull request to fix it then tell Ephemeral (ephemeralis#0001) or Shiro (Shiro#1584) on the osu!dev Discord (#osu-wiki channel) to force a refresh for the fixed news post.
Licence
The majority of content in this repository is licensed under CC-BY-NC 4.0. Please see the licence file for more information. tl;dr you can use it in a non-commercial manner.
As this is a wiki, there may be content with third party licences. These licences will be cited local to the content, and override the global licence file.
Please note that this does not cover the usage of the ""osu!"" or ""ppy"" branding in any software, resources, advertising or promotion, as this is protected by trademark law. If you require clearance for the use of these terms, please contact us.
",137
greenriver/hmis-warehouse,Ruby,"Boston HMIS Warehouse 
Introduction
The HMIS Warehouse project was initiated by the City of Boston's Department of Neighborhood Development to gather data from across various HMIS installations, produce aggregated reports, and supply de-duplicated client information to the Boston CAS system for Coordinated Access to housing.
The Warehouse is capable if ingesting standard HUD HMIS CSV files as well as data via the Social Solutions ETO API.
Copyright © 2017 Green River Data Analysis, LLC

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

A copy of the license is available in LICENSE.md
Vision
The City of Boston made a conscientious choice to release this project into the open source under a GPL. Our goal is to promote this opportunity, allowing Boston's investment to assist other municipalities and organizations, and realize the vision of a tool under continuous, collaborative improvement helping communities nationwide.
Looking ahead, we see the Warehouse codebase serving as a foundation for all communities that report to the department of Housing and Urban Development, or have a need to aggregate and de-duplicate homeless client data from across various systems.  To our knowledge, this is the only open source, freely available implementation of many HUD reports.
Application Design
The application is designed around the HUD Data Standards and the data structure is based on the HMIS Logical Model
The application is written primarily in Ruby on Rails and we use RVM to select a ruby version. Other ruby version managers should work fine, as would manually installing the ruby version mentioned in the .ruby-version
The application uses postgres for application data storage and Microsoft SQL Server or postgres for the warehouse data.
We've developed locally on OSX using homebrew and deployed to Ubuntu 16.04 using apt for dependencies.
Developer Prequisites
If you are unfamilar with contributing to open source projects on github you may first want to read some of the guides at:  https://guides.github.com/
There is a simple script to setup a development environment in bin/setup. To make it run smoothly you should have:

A running Ruby 2.3+ environment with bundler 1.11+ installed.
A local install of postgresql 9.4+ allowing your user to create new databases.
A local install of redis for caching. redis-server should be running on the default port
libmagic

Once these are in place, bin/setup should:

Install all ruby dependencies.
Create initial copies of configuration files.
Create an initial database and seed it with reference data and a randomly generated admin user.

If all goes well you should then be able to run bin/rails server and open the Warehouse in your system at http://localhost:3000 using the email/password created during bin/setup. If not, read bin/setup to figure out what went wrong and fix it.
Hack on your version as you see fit and if you have questions or want to contibute open an issue on github.
Developer Notes
We use the following common rails gems and conventions:

haml for view templating
bootstrap for base styles and layout
sass for custom-css
simple_form for forms
kaminari for pagination
brakeman for basic security scanning.
rack-mini-profiler to make sure pages are fast. Ideally <200ms
helpers need to be explictly loaded in controllers. i.e. we have config.action_controller.include_all_helpers = false set
bin/rake generate controller ...  doesn't make fixures and they are disabled in test_helper. We don't use them and instead seed data in test or let test create their own data however they need to.
it also doesn't make helper or asset stubs, make them by hand if you need one. See config/application.rb for details.

Multiple databases
The project reads/writes from several different databases. We keep track of these different environments by setting up parallel db configs and structures for each database. Health care data is configured in config/database_health.yml and database resources are in db/health. Warehouse data is configured in config/database_warehouse.yml and resources are in db/warehouse. When running migrations, use the custom generators.
App migrations can be created with:
rails generate migration foo

and run with
rake db:migrate

Warehouse migrations can be created with:
rails generate warehouse_migration foo

and run with
rake warehouse:db:migrate

Health migrations can be created with
rails generate health_migration foo

and run with
rake health:db:migrate

",8
kuazhu/w1802,JavaScript,"仓库说明
本仓库为演示代码库,仅供参考用


配套学习视频:学习视频
配套学习资料:学习资料


",4
edvb/tisp,C,"tisp - tiny lisp

tisp is a tiny lisp library designed to to be lightweight and easy to embedded
in other programs. Simply drop the tisp.c and tisp.h files into your
project and include the header file in order to use the necessary functions for
your program. An example command line interpreter is provided in main.c.
Options
-h
Print help and exit
-v
Print version info and exit
Usage
Run the program from the command line to launch the REPL, type a command and
press enter to see the result.
$ tisp
> (+ (+ 1 2) 3)
6
> (+ 1 2 3)
6

Alternatively you can pass a file name which will be opened and run, outputting
the result before exiting.
$ echo '((lambda (x) (+ x 1)) 10)' > inc.lisp
$ tisp inc.lisp
11

Commands can also be piped directing into tisp through the command line.
$ echo '(= ""foo"" ""foo"")' | tisp
t

Language
tisp is mainly based off of scheme, with minor features borrowed from other
lisps.
General
Comments
Single line comments with a semicolon, eg (cons 1 2) ; ingnored by tisp until new line.
Types
Nil
Nil, null, empty, or false, represented as an empty list, eg ().
Integers
Whole real numbers, positive or negative with optional + or - prefixes
repressively. Also supports scientific notation with a capital or lowercase
e. The exponent also needs to be integer which can be positive or negative.
eg 1, -48, +837e4, 3E-2.
Floating Pointing
Floating point numbers, as know as decimals, are integers followed by a period
and an optional integer with leading integers. Like integers can be positive or
negative with scientific notation, but still need an integer as an exponent. eg
1., +3.14, -43.00, 800.001e-3.
Rationals
Fraction type, a ratio of two integers. Similar rules apply for numerator and
dominator as integers (real positive or negative), expect for scientific
notation. Will try to simplify fraction where possible, and will through error
on division by zero. eg 1/2, 4/3 -1/2, 01/-30, -6/-3.
Strings
String of characters contained inside two double quotes. eg ""foo"", ""foo bar"".
Symbols
Case sensitive symbols which can be used as variable names. Supports lower and
upper case letters, numbers, as well as the characters _+-*/=<>?. First
character can not be a number, if the first character is a + or - then the
second digit cannot be a number either. Unlike all the previously listed types,
symbols are not self evaluating, but instead return the value they are defined
to. Throws an error if a symbol is evaluated without it being previously
assigned a value. eg foo, foo-bar, cat9, +, >=, nil?.
Lists
Lists composed of one or more element of any other types, including lists them
selves. Expressed with surrounding parentheses and each element is separated by
white space. When evaluated runs the first element as function with the rest of
the elements as arguments. Technically list is not a type, but simply a
nil-terminating chain of nested pairs. A pair is a group of two and only two
elements, normally represented as (a . b), with a being the first element
(car) and b being the second element (cdr). for example (a b c) is actually
(a . (b . (c . ()))). But it is often easier just to think of them as lists.
Functions
Lambda functions created within tisp itself. Called using list syntax where the
first element is the function name and any proceeding elements are the
arguments. For example (cadr '(1 2 3)) is a list of elements cadr and '(1 2 3). It calls the function cadr which returns the 2nd element of the first
argument given, here a list of size 3. In this case it return a 2.
Primitives
Functions built in to the language written in C. Called like regular functions,
see primitives section for more details.
Primitives
Built in primitives included by default.
car
Returns first element of given list
cdr
Return rest of the given list, either just the second element if it is of size
2 or a pair, or a new list with the first element removed.
cons
Creates a new pair with the two given arguments, first one as the car, second
as the cdr.
quote
Returns the given argument unevaluated.
void
Returns nothing. Used to insert a void type in a list or force a function not
to return anything.
begin
Executes all of its arguments and returns the value of the last expression.
eval
Evaluates the expression given.
=
Tests if multiple values equal. Returns nil if any are not, and t otherwise.
cond
Evaluates each expression if the given condition corresponding to it is true.
Runs through any arguments, each is a list with the first element as the
condition which needs to be t after evaluated, and the second element is the
expression to be run if and only if the condition is met.
type
Returns a string stating the given argument's type.
lambda
Creates function, first argument is a list of elements representing the symbol
name for any arguments for the new function. Next argument is code to be run
with the supplied arguments.
define
Create variable with the name of the first argument, with the value of the
second.
load
Loads the library, given as a string.
Differences From Other Lisps
In tisp there are no boolean types, much like common lisp, true is represented
by the self evaluating symbol t and false is nil, represented as (), an
empty list.
tisp also only has one equality primitive, =, which tests integers, symbols,
strings, and objects which occupy the same space in memory, such as primitives.
It also accepts any number of arguments to compare.
Symbols are also case sensitive following the Unix way, unlike many other lisps.
Author
Ed van Bruggen edvb@uw.edu
See Also
See project page at https://edryd.org/projects/tisp.html
View source code at https://git.edryd.org/tisp
LICENSE
zlib License
",5
KalilDev/textos,Dart,"textos
Aplicativo para visualizar os textos do kalil e cria-los
Getting Started
This project is a starting point for a Flutter application.
A few resources to get you started if this is your first Flutter project:

Lab: Write your first Flutter app
Cookbook: Useful Flutter samples

For help getting started with Flutter, view our
online documentation, which offers tutorials,
samples, guidance on mobile development, and a full API reference.
",2
PacktPublishing/Python-Image-Processing-Cookbook,Jupyter Notebook,"Python-Image-Processing-Cookbook
",3
leon-ai/leon,JavaScript,"



Leon
Your open-source personal assistant.






Website ::
  Documentation ::
  Roadmap ::
  Contributing ::
  Story


Introduction
Leon is an open-source personal assistant who can live on your server.
He does stuff when you ask him for.
You can talk to him and he can talk to you.
You can also text him and he can also text you.
If you want to, Leon can communicate with you by being offline to protect your privacy.
Why?


If you are a developer (or not), you may want to build many things that could help in your daily life.
Instead of building a dedicated project for each of those ideas, Leon can help you with his
packages/modules (skills) structure.
With this generic structure, everyone can create their own modules and share them with others.
Therefore there is only one core (to rule them all).
Leon uses AI concepts, which is cool.
Privacy matters, you can configure Leon to talk with him offline. You can already text with him without any third party services.
Open source is great.


What is this repository for?

This repository contains the following nodes of Leon:

The server
The packages/modules
The web app
The hotword node


What is Leon able to do?

Today, the most interesting part is about his core and the way he can scale up. He is pretty young but can easily scale to have new features (packages/modules).
You can find what he is able to do by browsing the packages list.

Sounds good for you? Then let's get started!
Getting Started
Prerequisites

Node.js 10 or 11
npm >= 5
Python 3.6.x
Pipenv
Supported OSes: Linux, macOS and Windows

To install these prerequisites, you can follow the How To section of the documentation.
Installation
# Clone the repository (stable branch)
git clone -b master https://github.com/leon-ai/leon.git leon
# OR download the latest release at: https://github.com/leon-ai/leon/releases/latest

# Go to the project root
cd leon

# Install
npm install
Usage
# Check the setup went well
npm run check

# Build
npm run build

# Run
npm start

# Go to http://localhost:1337
# Hooray! Leon is running
Docker Installation
# Build
npm run docker:build

# Run on Linux or macOS
npm run docker:run

# Run on Windows (you can replace ""UTC"" by your time zone)
docker run -e TZ=UTC -p 1337:1337 -it leonai/leon

# Go to http://localhost:1337
# Hooray! Leon is running
Documentation
For full documentation, visit docs.getleon.ai.
Roadmap
To know what is going on, follow roadmap.getleon.ai.
Contributing
If you have an idea for improving Leon, do not hesitate.
Leon needs open source to live, the more modules he has, the more skillful he becomes.
The Story Behind Leon
You'll find a write-up on this blog post.
Stay Tuned

Newsletter
Blog
GitHub issues
Twitter
#LeonAI

Author
Louis Grenard (@louistiti_fr)
Donate
You can also contribute by buying me a fruit juice.
License
MIT License
Copyright (c) 2019-present, Louis Grenard louis.grenard@gmail.com
Cheers!

",5307
NilsTheBest/NilsTheBot,Python,"NilsTheBot
A discord bot created by @NilsTheBest.
The bot is still being programmed and still can be improved. Here you can report bugs if you find any, or improve the code by making a pull request - any help is appreciated!
List of commands
 ntb!info - tells you a bit more about the bot and how to use him.
 ntb!help - displays a list of commands.
 ntb!mod (off by default)
 ---> {on/off} - toggles moderation mode
 ---> {0-2} - changes moderation mode (0 is off)
 ===============================
 !C-to-F and !F-to-C - converts Celsius to Fahrenheit, and vice-versa.
 !huge - displays huge text.
",2
eccentricdevotion/GameModeInventories,Java,"GameModeInventories
A CraftBukkit plugin for Minecraft Server.
Allow players to have separate inventories for each game mode (Creative, Survival and Adventure).
This plugin (and the GMIDatabaseConverter plugin) are available for download as a single ZIP file from the GameModeInventories page on BukkitDev.
Warning
This version of GameModeInventories uses a different storage format when saving inventories. Before installing this version, you should first run the GMIDatabaseConverter plugin on your CraftBukkit 1.6.4 server to update your GameModeInventories database.
How do I update my GMI database?
Before upgrading your server to CraftBukkit 1.7.x and installing GameModeInventories version 2.x, you should run GMIDatabaseConverter on your 1.6.4 server.

Install GMIDatabaseConverter.jar to the server's plugins folder
Start the server

The plugin will attempt to find and backup your old GameModeInventories database file
It will then read the existing inventory data
The existing data will be converted to the new format
The new data will be written back to the database


Once conversion is complete, you can update your GameModeInventories plugin to version 2.x and restart the server
If you are satisfied that GameModeInventories version 2.x is functioning correctly, you can safely remove GMIDatabaseConverter

Why did you change the storage format?
The format change is a result of code changes removing the reliance on using net.minecraft.server and org.craftbukkit code directly within the plugin (instead of using only the Bukkit API). This led to the plugin breaking with every Minecraft/CraftBukkit update. These code changes mean the plugin should no longer break between versions.
",3
r3eckon/Recursive-Tile-Map-Growth,Java,"Recursive Tile Map Growth
A simple recursive algorithm used to generate corridors and rooms inside of a grid based tile map.
The core of the algorithm has been uploaded alongside a built version for demonstration purposes.

To use the built jar, make sure you have the lastest verson of Java installed.
The title of the window will show you the current generation parameters as well as more useful information.
Original Build is the original version of the algorithm, to compare progress.
Model Build is the newest version of the algorithm with animated showoff.
Model Build - Realtime Generation is the newest version of the algorithm without animated showoff.
Controls
G to generate a new map
Arrow Keys to move the camera around
- and = to zoom
Numpad to select an alternate floor
General Explanation
The algorithm begins by checking that it is still within the bounds of the map, then checks neighboring tiles to make sure no tile types that should be avoided are nearby ( such as Rooms for Corridor tiles )
After checks are complete, the Corridor tile type is added to the current position. Tile Types are stored as an Enum in another class and only represent an arbitrary type. Many more types can be used.
The algorithm then proceeds to RNG rolls, which implies generating multiple float type numbers from 0 to 1 and checking if they are lower than the Branch, Turn, End, Room, Stairs, Model, and CSAvoid to induce their respective effects to the current process. Since we are checking if the generated number is lower, a parameter of 1 will ensure the effect happens every time.


Branch causes the corridor to branch into a perpendicular corridor, the direction of which is determined by a boolean random roll.


Turn converts a currently branching corridor into a single ""turn"" by calling return once the first branch has completed.


End suspends the current corridor by immediately calling return.


Room adds an entrance and calls the room generation algorithm to grow a room, the style and type of which depending on the orientation of the entrance.


Stairs adds stairwells towards the top or bottom of the dungeon, also creating new corridors to grow a new floor.


Model adds a pre made model by chosing randomly from the input list of models.


CSAvoid or Corridor Self Avoid allows or disallow the corridor to connect to existing corridors.


More of those random parameters can be added to generate more desirable results.
Models
Models have been recently added and combine the manual level design capabilities of a human to the growth of the level by randomly chosing from a set of premade tile type arrays. Those models, which can be of any size and span multiple floors, are then rotated and placed by the algorithm.
This system has recently been updated to allow Unique models or models that are limited in how often they can appear on generated maps.
Code samples for model placement, rotation and creation are included in the Source folder.
Deadend Removal
Deadends can now be found and removed using a couple algorithms, which have been added to the Source folder.
Essentially, any Corridor type tile that has less than 2 non empty immediate neighbors can be considered to be deadends. They are found and removed after the map is generated to give them a cleaner layout, as seen below.
Example 1 - Before

Example 1 - After

Example 2 - Before

Example 2 - After

Of course, it is also possible to to single passes of the find+remove algorithms to simply shorten the deadends rather than completely removing them.
Deadend removal code has been recently updated to also remove a new type of ""Planar"" style staircases and useless room entrances.
Post Generation Triggers
Post Generation Triggers have recently been added as a way to invoke generation algorithms after the main layout has been generated.
For example, a room may include an alternate exit or entrance which is not initlally connected to the rest of the layout.
PGTriggers allows the storage of which position on your map require post generation actions, which action to execute and in what orientation.
For the case of our alternate room entrance, simply adding a PGTrigger at the position of this entrance with the ""Grow Corridor"" behavior will start a new corridor from this point and hopefully connect that alternate entrance to another part of your layout.
PGTriggers can also be used to generate extra rooms, stairs and models.
The system has recently been updated to allow artifical inflation of the Current Count and Max Length values used to control the length of corridors, as well as placing the Ending type tile. By using an offset on those values on a corridor being generated from a trigger, the generator can be forced to place the Ending tile after a particular room.
Parameter Scaling
A simple way to make more diverse layouts is to apply a scaling factor to each random roll value. The strength of this scaling factor is calculated using the distance the current floor is relative to the spawn floor.
This means that scaling factors can be used to make floors that are further away from the spawn point get increasingly chaotic and messy by scaling up the Branch parameter.
Selective Layout Generation
To create levels that make a bit more sense in the context of a game, some layout requirements must be put in place. Selective Layout Generation makes sure that only levels that meet those requirements are kept by the generator.
The non animated updated build shows this feature in action. In this instance, only maps that feature one Boss Room ( big yellow area ) are kept.
Since many maps must be discarded, this feature should not be abused since too complex requirement paired with a setup that produces bad maps could result in very long gen time. The maximum amount of tries should be capped for this reason.
",29
georgewfraser/java-language-server,Java,"Language Server for Java using the Java compiler API
A Java language server based on v3.0 of the protocol and implemented using the Java compiler API.

Installation (VS Code)
Install from the VS Code marketplace
Installation (other editors)
Vim (with vim-lsc)

Checkout this repository
Run ./scripts/link_mac.sh
Add the vim plugin natebosch/vim-lsc to your vimrc
Add vim-lsc configuration:
let g:lsc_server_commands = {'java': '<path-to-java-language-server>/java-language-server/dist/mac/bin/launcher --quiet'}


See the vim-lsc README for other configuration options.

Note: This tool is not compatible with vim-lsp as it only supports LSPv2.0.
Issues
Features
Javadoc

Signature help

Autocomplete symbols (with auto-import)


Autocomplete members

Go-to-definition


Find symbols


Lint

Type information on hover

Find references


Usage
The language server will provide autocomplete and other features using:

.java files anywhere in your workspace
Java platform classes
External dependencies specified using pom.xml, Bazel, or settings

Settings
If the language server doesn't detect your external dependencies automatically, you can specify them using .vscode/settings.json
{
    ""java.externalDependencies"": [
        ""junit:junit:jar:4.12:test"", // Maven format
        ""junit:junit:4.12"" // Gradle-style format is also allowed
    ]
}
If all else fails, you can specify the java class path manually:
{
    ""java.classPath"": [
        ""lib/some-dependency.jar""
    ]
}
You can generate a list of external dependencies using your build tool:

Maven: mvn dependency:list
Gradle: gradle dependencies

The Java language server will look for the dependencies you specify in java.externalDependencies in your Maven and Gradle caches ~/.m2 and ~/.gradle. You should use your build tool to download the library and source jars of all your dependencies so that the Java language server can find them:

Maven

mvn dependency:resolve for compilation and autocomplete
mvn dependency:resolve -Dclassifier=sources for inline Javadoc help


Gradle

gradle dependencies for compilation and autocomplete
Include classifier: sources in your build.gradle for inline Javadoc help, for example:
dependencies {
    testCompile group: 'junit', name: 'junit', version: '4.+'
    testCompile group: 'junit', name: 'junit', version: '4.+', classifier: 'sources'
}





Design
The Java language server uses the Java compiler API to implement language features like linting, autocomplete, and smart navigation, and the language server protocol to communicate with text editors like VSCode.
Incremental updates
The Java compiler API provides incremental compilation at the level of files: you can create a long-lived instance of the Java compiler, and as the user edits, you only need to recompile files that have changed. The Java language server optimizes this further by focusing compilation on the region of interest by erasing irrelevant code. For example, suppose we want to provide autocomplete after print in the below code:
class Printer {
    void printFoo() {
        System.out.println(""foo"");
    }
    void printBar() {
        System.out.println(""bar"");
    }
    void main() {
        print // Autocomplete here
    }
}
None of the code inside printFoo() and printBar() is relevant to autocompleting print. Before servicing the autocomplete request, the Java language server erases the contents of these methods:
class Printer {
    void printFoo() {
        
    }
    void printBar() {
        
    }
    void main() {
        print // Autocomplete here
    }
}
For most requests, the vast majority of code can be erased, dramatically speeding up compilation.
Logs
The java service process will output a log file to stderr, which is visible in VSCode using View / Output, under ""Java"".
Contributing
If you have npm and maven installed, you should be able to install locally using
npm install -g vsce
npm install
./scripts/build.sh

At the time of this writing, the build only works on Mac, because of the way it uses JLink. However, it would be straightforward to fix this by changing scripts/link_mac.sh to be more like scripts/link_windows.sh.
",196
ChoGGi/SurvivingMars_CheatMods,Lua,"You should buy a copy: GOG, Steam, or Paradox
For packaged versions of these mods see Steam or Nexus Mods.
Install help (Nexus/GoG)
Place mod folder(s) in %AppData%\Surviving Mars\Mods (Create ""Mods"" folder if it doesn't exist)

https://pcgamingwiki.com/wiki/Surviving_Mars#Save_game_data_location (other OS locations)

ï»¿Enable with in-game mod manager

If you don't have a mod manager button see here or Nexus.
",19
georgewfraser/java-language-server,Java,"Language Server for Java using the Java compiler API
A Java language server based on v3.0 of the protocol and implemented using the Java compiler API.

Installation (VS Code)
Install from the VS Code marketplace
Installation (other editors)
Vim (with vim-lsc)

Checkout this repository
Run ./scripts/link_mac.sh
Add the vim plugin natebosch/vim-lsc to your vimrc
Add vim-lsc configuration:
let g:lsc_server_commands = {'java': '<path-to-java-language-server>/java-language-server/dist/mac/bin/launcher --quiet'}


See the vim-lsc README for other configuration options.

Note: This tool is not compatible with vim-lsp as it only supports LSPv2.0.
Issues
Features
Javadoc

Signature help

Autocomplete symbols (with auto-import)


Autocomplete members

Go-to-definition


Find symbols


Lint

Type information on hover

Find references


Usage
The language server will provide autocomplete and other features using:

.java files anywhere in your workspace
Java platform classes
External dependencies specified using pom.xml, Bazel, or settings

Settings
If the language server doesn't detect your external dependencies automatically, you can specify them using .vscode/settings.json
{
    ""java.externalDependencies"": [
        ""junit:junit:jar:4.12:test"", // Maven format
        ""junit:junit:4.12"" // Gradle-style format is also allowed
    ]
}
If all else fails, you can specify the java class path manually:
{
    ""java.classPath"": [
        ""lib/some-dependency.jar""
    ]
}
You can generate a list of external dependencies using your build tool:

Maven: mvn dependency:list
Gradle: gradle dependencies

The Java language server will look for the dependencies you specify in java.externalDependencies in your Maven and Gradle caches ~/.m2 and ~/.gradle. You should use your build tool to download the library and source jars of all your dependencies so that the Java language server can find them:

Maven

mvn dependency:resolve for compilation and autocomplete
mvn dependency:resolve -Dclassifier=sources for inline Javadoc help


Gradle

gradle dependencies for compilation and autocomplete
Include classifier: sources in your build.gradle for inline Javadoc help, for example:
dependencies {
    testCompile group: 'junit', name: 'junit', version: '4.+'
    testCompile group: 'junit', name: 'junit', version: '4.+', classifier: 'sources'
}





Design
The Java language server uses the Java compiler API to implement language features like linting, autocomplete, and smart navigation, and the language server protocol to communicate with text editors like VSCode.
Incremental updates
The Java compiler API provides incremental compilation at the level of files: you can create a long-lived instance of the Java compiler, and as the user edits, you only need to recompile files that have changed. The Java language server optimizes this further by focusing compilation on the region of interest by erasing irrelevant code. For example, suppose we want to provide autocomplete after print in the below code:
class Printer {
    void printFoo() {
        System.out.println(""foo"");
    }
    void printBar() {
        System.out.println(""bar"");
    }
    void main() {
        print // Autocomplete here
    }
}
None of the code inside printFoo() and printBar() is relevant to autocompleting print. Before servicing the autocomplete request, the Java language server erases the contents of these methods:
class Printer {
    void printFoo() {
        
    }
    void printBar() {
        
    }
    void main() {
        print // Autocomplete here
    }
}
For most requests, the vast majority of code can be erased, dramatically speeding up compilation.
Logs
The java service process will output a log file to stderr, which is visible in VSCode using View / Output, under ""Java"".
Contributing
If you have npm and maven installed, you should be able to install locally using
npm install -g vsce
npm install
./scripts/build.sh

At the time of this writing, the build only works on Mac, because of the way it uses JLink. However, it would be straightforward to fix this by changing scripts/link_mac.sh to be more like scripts/link_windows.sh.
",196
ChoGGi/SurvivingMars_CheatMods,Lua,"You should buy a copy: GOG, Steam, or Paradox
For packaged versions of these mods see Steam or Nexus Mods.
Install help (Nexus/GoG)
Place mod folder(s) in %AppData%\Surviving Mars\Mods (Create ""Mods"" folder if it doesn't exist)

https://pcgamingwiki.com/wiki/Surviving_Mars#Save_game_data_location (other OS locations)

ï»¿Enable with in-game mod manager

If you don't have a mod manager button see here or Nexus.
",19
xiaohao890809/xiaohao890809.github.io,HTML,"我的个人博客

地址：http://xiaohao890809.github.io
基于jekyll的blog
使用了bootstrap框架
使用font-awesome进行修饰
原作者是：https://github.com/enml/blog/tree/jekyll-blog ，特此表示感谢！

",2
triton/triton,Nix,"Triton
Triton is a collection of packages for the Nix package
manager.
Triton linux distribution source code is located inside the
nixos/ folder.
Discussion Channels

Matrix Community: +triton:matrix.org

Documentation
Legacy Documentation

NixOS installation instructions
Documentation (Nix Expression Language chapter)
Manual (How to write packages for Nix)
Manual (NixOS)
Nix Wiki

Supported Platforms (not all platforms implemented)

ARM requires: NEON, VFPv3+ (aka. armv7+)

armv7l-linux WIP
armv8l-linux WIP
aarch64-linux WIP


x86 requires: MMX,SSE,SSE2,SSE3,SSSE3,SSE4,SSE4.1,SSE4.2,AES
(aka. at least Intel Westmere, AMD 15h, or VIA Eden x4)

i686-linux (libs only)
x86_64-linux


POWER requires: POWER8+

powerpc64le-linux Incomplete



",45
lcp0578/cheat-sheets,PHP,"🐘 cheat sheets 📝

symfony

Basic 基础和常用

Basic 基础知识点
Symfony Twig Extensions symfony对Twig的扩展
Shortcuts Methods 控制器中的快捷方法
FileControllerHelper 文件处理的helper
Json Response Json响应及参数设置
Streamed Response 流响应


Twig Twig相关

Twig Twig基础
Twig Extension Twig扩展示例
Twig Form Reference Twig From相关的函数与变量
Twig functions Twig函数用法示例
Whitespace Control 空格控制
Twig tags Twig Tags用法示例
Twig macro macro宏的示例


Doctrine Doctrine相关

Doctrine Doctrine基础知识
Doctrine Types Doctrine字段类型
An Entity Demo 一个略复杂的Entity的示例
Validation Constraints 验证约束设置
Custom Constraint.md自定义验证约束
Doctrine Cache 配置Doctrine缓存配置
Doctrine Annotations Reference
Doctrine Schema Manager Doctrine 模式管理器
Doctrine SQL Filter SQL过滤器示例
Doctrine Query Functions DQL使用SQL函数，例如：DATE_FORMAT
Custom DQL Funtions 自定义DQL函数
DQL(Doctrine Query Language) DQL相关
QueryBuilder examples 查询构造器示例
RawSQLQuery examples 原生SQL查询
associations 表之间关联
Table to Entity (reverse engineering) 数据表转Entity（逆向工程）
MultipleDatabase 多数据库配置与使用
ColumnDefaultValue 设置字段默认值的那些坑
Schema Manager Schema管理器的使用
batch processing 批量处理
truncate table 截断表
SQL log 开发模式下配置SQL log


Router 路由相关

routing.yml yml路由配置示例


Form 表单相关

FormBuilder examples 表单构造器示例
Validation Note 表单验证相关
Validation Groups 验证组
argument value resolver
create custom field type
create form type extension
Custom Form Theme
DataTransformers
Pass Custom Options Form
dynamic form modification利用事件监听动态修改表单数据


Service 服务相关

Service
service id 服务ID
autowiring 服务的自动装配
alias private service
Custom Service Tags
service decoration
Service Container


Dependency Injection 依赖注入相关

Dependency Injection Tags
Compiler Pass


Security 安全相关

security authentication  安全相关介绍
Authenticator demo 认证器的demo
Login and Register 登录和注册相关
Logout Handler 退出登录（包含失败）处理
Logout Success Handler 成功退出登录处理
multi field login 支持多字段登录系统
SetLoginToken 手动用户登录，设置token


EventListener 事件监听相关

EventListener 事件监听
Login Event Listener 登录事件监听
Guzzle Http Event Listener GuzzleHttp事件监听
Enable SQL Filter Event Listener SQL Filter 事件监听
Api Version API版本控制
kernel view 模板层监听
Api Exception Listener API异常监听
Doctrine Event Listeners SubscribersDoctrine的事件监听


Command

Console Command 常用的console命令
Command call Command command之间调用
command in controller 在控制器调用command
Custom Command 自定义command
Command LifecycleCommand生命周期函数


ReusableBundle 创建可重复使用bundle相关

BundleStruct 可复用bundle结构
Bundle Configuration Bundle配置示例


Components 组件相关

Process 在子进程下执行命令
Asset 管理静态资源。
Serializer 序列化和反序列化
Event Dispatcher 事件调度（事件派遣）
Workflow 工作流
Stopwatch 性能调试（时间和内存，可分组）
Finder 文件和目录查找
Filesystem 对文件系统做了面向对象的封装
Dotenv 设置环境变量
Ldap LDAP server连接相关
Config 配置文件组件，支持YAML, XML, INI格式或数据库。
Debug 方便调试的组件
VarDumper 调试时打印信息的组件


Bundles Note 第三方bundle使用笔记

DoctrineFixturesBundle	初始化数据Bundle笔记


symfony coding standard Symfony编码规范

code conventions 代码约定
code standards 代码标准


Others 其他杂项

Version Symfony版本查看
Upload File 文件上传示例
Cookie cookie相关
Session session相关
parameters.yml.dist 配置parameters.yml不更新
Clear Cache In Controller
Symfony Performance
symfony tips and tricks
Logger配置错误日志
Customize Error Pages 自定义错误页面
symfony 3.3 features
symfony 3.4 features
web server configuration


Webpack Encore Webpack Encore相关

Webpack Encore Webpack Encore介绍
Webpack Encore Example Webpack Encore使用示例


Symfony4 sf4相关

flex
maker-bundle
recipes-contrib
recipes


Symfony 1.x

symfony1.4 symfony1.4笔记


Varnish symfony使用Varnish加速网站
Deployment

proxies 设置代理
symfony deploy symfony项目部署文档




chrome extensions
MySQL

MySQL join
MySQL functions
MySQL explain
MySQL table design
MySQL table index
my confguire 常用的配置项
MySQL Optimize
MySQL where
mysqldump
Innodb
master slave 主从配置
grant
sql_mode SQL MODE设置与介绍
update root password
windows mysql windows下安装mysql
DROP INDEX
Atlas Atlas (MySQL proxy) 使用
MySQL 8 windows install MySQL8在windows下的安装
MySQL8 authentication pluginMySQL8密码验证插件更换后，问题解决办法
show processlist 查看正在运行的线程


composer

composer basic composer基础使用
composer config composer配置相关
composer versions composer包版本约定
recover composer.json 恢复composer.json


zend studio
guzzle http
silex
linux

basic
network configure
nohup
sudo
crontab
package management
rsync
CentOS Local yum repo
log view
netstat
telnet
iptables
tar
df & du
scp
rz & sz
iconv
Aliyun服务器配置IPV6
chinese support 中文支持
fdisk Linux下磁盘挂载


Go

gofmt vs go fmt
Compiler Directives
for select
string 字符串操作相关
string number 数字与字符串之间的转换
number base conversion进制转换
vgo 版本控制
Byte Order 字节序
Standard library 标准库笔记

strconv
binary
hex 16进制操作包


Others Library 其他类库笔记
windows下开发

call cmd.exe 调用cmd.exe并隐藏窗口


Package Management 包管理相关

go modules
athens


Fatal Error 常见的fatal error

fatal error: concurrent map read and map write并发读写map错误




redis

basic redis基础
redis windowsredis在windows上的使用
redis install Redis源码编译安装
master slave redis配置主从
redis.conf redis配置文件介绍
Predis VS phpredis Predis与phpredis对比


git相关

git branch git分支相关
git tag git标签相关
rm commit log
git ssh git ssh配置
fork sync fork仓库与原仓同步
Github Github clone慢配置
git update git升级
rm git index 移除文件或目录的git索引
git recover git还原某个提交ID
Gogs Gogs代码平台
Gitea Gitea(Gogs的一个克隆)


javascript

json convert
flexible
Mobile Image Upload
console
knockoutjs
vuejs
ActiveX Object判断对象是否存在的方法
requirejs requrejs引入js、css、fonts等
art template art-template模板引擎


framework7
markdown
yii2
select2
discuz
destoon
CodeIgniter
cakephp
yaf
yar
PHP

PHP Extension Install PHP扩展编译安装
Memcached PHP Memcached扩展安装
oci8PHP Oracle连接扩展
Socket
control structures alternative syntax 流程控制的替代语法
SOAP 调用SOAP服务
preg_match VS preg_match_all 正则匹配对比
PHP Functions PHP常用函数
PHP Extensions PHP常用扩展


PHP code
Shell

deploy.sh 部署项目shell脚本
network configure
exit code退出码
backup.sh 备份项目shell脚本


Nginx

nginx basic nginx基础
nginx conf nginx.conf注释版
vhost conf vhost配置示例
proxy_pass 代理转发
ssl SSL配置示例
syntax 配置语法
nginx errors 常见错误及修复办法


Code::Blocks

Code::Blocks shortcut


Ubuntu

开启sshd服务
防火墙


svg
wechat
FFmpeg

FFmpeg install  编译安装FFmpeg
PHP-FFmpeg PHP-FFmpeg类库


OAuth 2.0

rfc 6749
Go

github.com/golang/oauth2
github.com/dexidp/dex
github.com/ory/fosite


PHP


Modbus

SSCOM串口调试软件


CSS3

rem


webpack
vuejs
HAProxy
phpStudy  phpStudy升级php&MySQL
windows

taskkill 杀进程


assembly汇编语言
CEF
VisualStudio

InstallShield


java

eclipseeclipse相关
jar jar反编译
jdk jdk安装
tomcatTomcat安装


xunsearch
mac mac相关

keyboard系统快捷键
chrome Chrome快捷键


MSSQL

install php sqlsrv extension 安装sqlsrv扩展


Oracle Oracle数据库相关

mac docker oracle mac下通过docker安装Oracle
SQL errors SQL错误笔记


DB 新型数据相关

TiDB开源分布式 NewSQL 关系型数据库
RadonDB 云原生的MySQL数据库,可以无限扩展
influxdb 开源时序型数据库
vitess 数据库中间件，用于部署、扩展和管理大型MySQL实例集群。


hadoop hadoop分布式计算平台
TCP/IP TCP/IP协议相关

MQTT 消息队列遥测传输协议


Docker Docker相关
OA

file2pdf 文件转PDF
install fonts安装中文字体


JavaBridge
ios

xcode


Security代码安全

APP接口安全设计要点
源代码安全审计

cobra




源代码安全审计

cobra


用户认证与授权

单点登录SSO
CAS
OAuth2


消息队列

nsq
RabbitMQ
Kafka
ZeroMQ
ActiveMQ
RocketMQ


ZooKeeper

基本概念
典型应用场景


Erlang
Scala
Kotlin
flutter
Go & PHP

goridge
roadrunner
Spiral Framework
github.com/VKCOM/noverify  Pretty fast linter (code static analysis utility) for PHP


crawler 爬虫相关

selenium

github.com/SeleniumHQ/selenium


PHP

github.com/FriendsOfPHP/Goutte Goutte, a simple PHP Web Scraper
github.com/symfony/dom-crawler The DomCrawler component eases DOM navigation for HTML and XML documents.
github.com/symfony/browser-kit The BrowserKit component simulates the behavior of a web browser, allowing you to make requests, click on links and submit forms programmatically.
github.com/symfony/css-selector The CssSelector component converts CSS selectors to XPath expressions.
github.com/owner888/phpspider


Go

github.com/gocolly/colly Elegant Scraper and Crawler Framework for Golang
github.com/henrylee2cn/pholcus Pholcus is a distributed, high concurrency and powerful web crawler software.
github.com/PuerkitoBio/gocrawl Polite, slim and concurrent web crawler.
github.com/MontFerret/ferret Declarative web scraping





",28
VagnerBomJesus/MyFinances,Java,"My Finances
Meu Projeto Android
My Finaces->é uma aplicação móvel desenvolvida para os dispositivos Android ""até 4.0.3 versão Android"" que permite gerenciar as finanças pessoais ""despesas e receitas"".
My Objectivo do app
                              ->gerenciar as despesas e receitas por meio de banco de dados SQLite.
                              -> interface amigável. 
                              -> Adicionar registros (despesa ou receita) extremamente rápido. 
                              -> Categorizar o tipo de despesa e receita. 
                              -> Adicionar novas categorias. 
                              -> Visualizar um relatório de despesas e receitas diárias, mensais e anuais. 
",3
b3log/symphony,Java,"


下一代的社区系统，为未来而构建
















  
  
  


简介
Symphony（[ˈsɪmfəni]，n.交响乐）是一个现代化的社区平台，因为它：

实现了面向内容讨论的论坛
实现了面向知识问答的社区
包含了面向用户分享、交友、游戏的社交网络
100% 开源

欢迎到 Sym 官方讨论区了解更多。
动机
很多社区论坛系统：

界面风格老式，没有跟上时代发展的步伐
缺少创新、好玩的特性，缺少现代化的交互元素和用户体验
缺乏考虑实际运营需求，管理功能过于单一
细节不够精致、缺乏长期维护

客户案例
社区版：

宽客网
AIQ-机器学习
许昌IT圈
凤凰匯
猪玩派 | 喜欢游玩  热爱生活  乐于分享！
俩猴网
听雨轩
艺赛旗 RPA（商用授权）
北极社区（商用授权）
神州邦邦（商用授权）
......

商业版：

黑客派
IT遇岛
汇桔网
乾学院
GeeCall极客社区
金蝶精斗云社区
......

功能

Sym 简介幻灯片
Sym 功能点脑图

界面
以下截图来自 Sym 商业版。
首页

列表

帖子

发帖

用户 - PC 端

用户 - 移动端



安装
先在 MySQL 中手动建库（库名 symphony，字符集使用 utf8mb4，排序规则 utf8mb4_general_ci），然后按照如下方式之一启动服务。
war 包启动
下载最新的 Sym 包解压，进入解压目录执行：

Windows: java -cp ""WEB-INF/lib/*;WEB-INF/classes"" org.b3log.symphony.Starter
Unix-like: java -cp ""WEB-INF/lib/*:WEB-INF/classes"" org.b3log.symphony.Starter

如果要将 war 包部署到 Servlet 容器中启动请参考安装指南。
Docker 部署
获取最新镜像：
docker pull b3log/symphony
启动容器：
docker run --detach --name sym --network=host \
    --env RUNTIME_DB=""MYSQL"" \
    --env JDBC_USERNAME=""root"" \
    --env JDBC_PASSWORD=""123456"" \
    --env JDBC_DRIVER=""com.mysql.cj.jdbc.Driver"" \
    --env JDBC_URL=""jdbc:mysql://127.0.0.1:3306/symphony?useUnicode=yes&characterEncoding=UTF-8&useSSL=false&serverTimezone=UTC"" \
    b3log/symphony --listen_port=8080 --server_scheme=http --server_host=localhost 
为了简单，使用了主机网络模式来连接主机上的 MySQL。
启动参数说明：

--listen_port：进程监听端口
--server_scheme：最终访问协议，如果反代服务启用了 HTTPS 这里也需要改为 https
--server_host：最终访问域名或公网 IP，不要带端口号

完整启动参数的说明可以使用 -h 来查看。
文档

《提问的智慧》精读注解版
Sym 安装指南
Sym 配置项说明
Sym 贡献指南

授权

社区版：使用 AGPLv3 开源，如果你选择使用社区版，则必须完全遵守 AGPLv3 的相关条款
商业版：提供完整源码以便二开，报价 ¥20000
云服务：提供开箱即用的云端服务，每年 ¥5000

关于商业版和社区版的对比请看这里，企业网站、经营性网站、以营利为目的或实现盈利的网站请购买商业版。
欢迎联系 QQ 845765 或邮箱 d@b3log.org 进行细节咨询。
社区

讨论区
报告问题

鸣谢

jQuery：前端 JavaScript 工具库
Vditor： 浏览器端的 Markdown 编辑器
Highlight.js：前端代码高亮库
pjax：pushState + ajax = pjax
MathJax：前端数学公式渲染引擎
Sass：前端 CSS 处理工具
jsoup：Java HTML 解析器
flexmark：Java Markdown 处理库
Apache Commons：Java 工具库集
Jodd：Java 工具库集
Latke：以 JSON 为主的 Java Web 框架

安全方面特别感谢：

米斯特安全团队
@gh0stkey
@SeagullGR
长亭科技


开源项目推荐

如果你需要搭建一个个人博客系统，可以考虑使用 Solo
如果你需要搭建一个多用户博客平台，可以考虑使用 Pipe
欢迎加入我们的小众开源社区，详情请看这里

",11470
