title,language,original,stars
PineconePi/Pinecone_Pi_Nano,C,"PineconePi Nano（Click to enter the website，Support：support@pineconepi.cn)
PineconePi Nano is a development board that meets all the good ideas of 8051 MCU enthusiasts and creators: low cost, small size, size only 52 mm x 18 mm (DIP40); faster core: ultra-high speed 8051 core (1T), 12 times faster than traditional 8051; wide range of use: working voltage: 2.0V ~ 5.5V (built-in LDO), working temperature: - 40 ~85 (℃); rich peripherals and hardware resources. Source: One-button cold start, 8-channel LED, two SMT digital tubes, onboard Ch330N; 64K Flash, 5-way TIM, 8-way Pwm, 15-way high-speed ADC; easy to use, support C language, assembly, direct insertion breadboard; multi-expansion, a variety of peripheral modules.
List

Chipbook:Chip Datesheet And Driver program And .H file
Document:Teaching documents
Example project:Standard KEIL Project
Library:Rich sharing of peripheral driver Libraries
PinCard:Nano Pin Card
Project:Open Source Projects Used by User with Nano
RTOS:RTX 51 FULL(/tiny)
Schematic diagram and PCB doc：Schematic diagram and PCB doc

Pinecone_Pi_Nano（点我进入官网，官方交流企鹅群：481227232)
Pinecone Pi nano|松果派Nano是一块满足8051单片机爱好者和创客一切美好设想的开发板：低成本，小体积，尺寸仅52 mm x18 mm（DIP40）；更快的内核:超高速8051内核(1T),比传统8051快12倍以上;广泛的使用范围:工作电压:2.0V ~ 5.5V(内置LDO),工作温度:-40℃ ~ 85℃;丰富的外设与硬件资源：一键冷启动,8路LED,两位smt数码管,板载Ch330N;64K Flash,5路TIM,8路Pwm,15路高速ADC；易使用，支持C语言,汇编,直插面包板；多扩展，多种外设模块。
目录结构

Chipbook:Nano使用说明书 And 相关芯片手册 And 驱动程序 And 核心头文件 （建议Clone)
Demo:nano十万种玩法推荐（实例项目，来源于松果社区分享）
Example project:PineconePi提供的标准工程（新手福音）
Library:丰富的外设驱动库分享
Schematic diagram and PCB doc：NANO原理图与封装库
Shell（3D_STL)：nano外壳3D打印文件（STL）
RTOS:RTX 51 FULL(/tiny)安装包及使用指南
PinCard:松果派NANO引脚示意卡（打开QQ扫一扫有彩蛋哦！）

",20
sgo230/Sisense,Python,"Sisense
Code Snippets for Sisense
",4
Lombiq/Tidy-Orchard-Development-Toolkit,C#,"Tidy Orchard Development Toolkit Readme
The Tidy Orchard Development Toolkit allows you to develop Orchard-based applications in a way that you own code (e.g. extensions, configuration) is completely separated from the core Orchard source.
This makes Orchard development not only tidier but it also allows you to:

Manage your extensions better: e.g. now you can keep all your modules under a single repository (with subrepositories for other modules) instead of having all your modules in separate repositories.
Updating or upgrading the Orchard source is a matter of pulling in the latest changes from the Orchard repository.
You can even keep a single (or just a few) folders on your computer with the Orchard source that you link to from each of your solutions, thus minimizing storage space usage and build time.

Keep in mind that this toolkit is purely experimental! It can in no way support a production scenario. Also the aim was to get Orchard working in its basics: it fully runs. Other areas like deplyoment wasn't explored yet.
Creating a Tidy Orchard solution
There is a sample solution with all of the below tasks already done: see the Tidy Orchard Development Quick Start. This solution has all the details just referenced here.

Create a folder in the root for your web project (e.g. “Orchard.Web” but the name is not mandatory) and copy the contents of Orchard.Web there (the Web csproj can also have an arbitrary name). Modify the Web.config as in the sample.
Add the Toolkit to the Lombiq.TidyOrchardDevelopmentToolkit under your web project's folder.
Add the full Orchard source to the Web project's folder under a folder called ""Orchard"". This should be the full Orchard source (e.g. with the lib and src folders in the root). Please note that you have to remove the Web.config from Orchard.Web.
Copy the Orchard solution file to the root (and optionally rename it).
Change all project references of the solution to point to the new web project's content (assuming your web project's folder is called Orchard.Web):

Replace ""Orchard\ with ""Orchard.Web\Orchard\src\Orchard\ (including the quotes).
Replace Orchard.Tests\ with Orchard.Web\Orchard\src\Orchard.Tests.
Replace Orchard.Web.Tests\ with Orchard.Web\Orchard\src\Orchard.Web.Tests.
Replace ""Orchard.Web\ with ""Orchard.Web\Orchard\src\Orchard.Web\ (including the quotes).
Replace Orchard.Tests.Modules\ with Orchard.Web\Orchard\src\Orchard.Tests.Modules.
Replace Orchard.Core.Tests\ with Orchard.Web\Orchard\src\Orchard.Core.Tests.
Replace Orchard.WarmupStarter\ with Orchard.Web\Orchard\src\Orchard.WarmupStarter.
Replace ""Tools\ with ""Orchard.Web\Orchard\src\Tools\ (including the quotes).
Replace Orchard.Specs\ with Orchard.Web\Orchard\src\Orchard.Specs.
Replace Orchard.Profile\ with Orchard.Web\Orchard\src\Orchard.Profile.
Replace Orchard.Web\Orchard\src\Orchard.Web\Orchard.Web.csproj back to Orchard.Web\Orchard.Web.csproj.


Copy over the contents of the original Orchard.Web folder to your own web folder except the Core, Modules, Media and Themes folders.
Adjust Orchard.Web.csproj:

Replace ....\lib\ with Orchard\lib.
Replace ProjectReference Include=""..\ with ProjectReference Include=""Orchard\src.
Replace ProjectReference Include=""Core\ with ProjectReference Include=""Orchard\src\Orchard.Web\Core.


Add the Toolkit's project to the solution and reference it from the web project.
Register the Toolkit's Autofac module in the HostComponents.config file.
Register the TidyDevelopmentHttpModule in the Web.config and change the handlers declaration to use the appropriate accessPolicy.
Add your own themes and modules under the Web project's folder under ""Modules"" and ""Themes"" folders, respectively.
Modify module project files according to the Orchard App Host documentation so they support the new solution structure.

Instead of copying you can always create symlinks with mklink instead.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/tidy-orchard-development-toolkit (Mercurial repository)
https://github.com/Lombiq/Tidy-Orchard-Development-Toolkit (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",3
textlint-ja/textlint-rule-preset-japanese,JavaScript,"textlint-rule-preset-japanese 
textlint rule preset for Japanese.
日本語向けのtextlint ruleのpresetです。
目的

一般的な文書で利用するためのルール集
入れても誤検知が少ないルールに限定する

明らかな誤爆がある場合は、Issueを立ててください


スタイル(スペースの有無など)に関連するルールは含めない

技術的な文章を書くためにより厳しいルールを求める方は次のプリセットを参照してください。

textlint-ja/textlint-rule-preset-ja-technical-writing: 技術文書向けのtextlintルールプリセット

Installation
npm install textlint-rule-preset-japanese

このpresetは以下のルールを含んでいます。

https://github.com/textlint-ja/textlint-rule-max-ten

一文で使える""、""の数


https://github.com/takahashim/textlint-rule-no-doubled-conjunctive-particle-ga

逆接の接続助詞「が」が、同一文中に複数回出現していないかどうか


https://github.com/takahashim/textlint-rule-no-doubled-conjunction

同じ接続詞で開始されていることを検出


https://github.com/textlint-ja/textlint-rule-no-double-negative-ja

二重否定の検出


https://github.com/textlint-ja/textlint-rule-no-doubled-joshi

二重助詞の検出


https://github.com/azu/textlint-rule-sentence-length

一文の最大の長さ


https://github.com/azu/textlint-rule-no-dropping-the-ra

ら抜き言葉を使用し


https://github.com/textlint-ja/textlint-rule-no-mix-dearu-desumasu

文の敬体(ですます調)、常体(である調)の混合をチェック


https://github.com/azu/textlint-rule-no-nfd

ホ゜ケット エンシ゛ン のような、Mac OS XでPDFやFinderからのコピペで発生する濁点のチェック


https://github.com/textlint-rule/textlint-rule-no-invalid-control-character

制御文字の検出



Usage
Via .textlintrc
{
    ""rules"": {
        ""preset-japanese"": true
    }
}
Options
{
    ""rules"": {
        // それぞれのルールのデフォルト値
        ""preset-japanese"": {
             // https://github.com/textlint-ja/textlint-rule-max-ten
             // 一文で使える""、""の数
             ""max-ten"": {
                 ""max"": 3
             },
             // https://github.com/takahashim/textlint-rule-no-doubled-conjunctive-particle-ga
             // 逆接の接続助詞「が」が、同一文中に複数回出現していないかどうか
             // e.g.) 今日は早朝から出発したが、定刻には間に合わなかったが、無事会場に到着した。
             ""no-doubled-conjunctive-particle-ga"": true,
             // https://github.com/takahashim/textlint-rule-no-doubled-conjunction
             // 同じ接続詞が連続して出現していないかどうか
             ""no-doubled-conjunction"": true,
             // https://github.com/textlint-ja/textlint-rule-no-double-negative-ja
             // 二重否定の検出
             ""no-double-negative-ja"": true,
             // https://github.com/textlint-ja/textlint-rule-no-doubled-joshi
             // 二重助詞の検出
             // 連続して同じ助詞が出た場合のみを検出
             ""no-doubled-joshi"": {
                 ""min_interval"": 1
             },
             // https://github.com/azu/textlint-rule-sentence-length
             // 一文の最大の長さ
             ""sentence-length"": {
                 ""max"": 100
             },
             // https://github.com/textlint-ja/textlint-rule-no-dropping-the-ra
             // ら抜き言葉を使用しない
             ""no-dropping-the-ra"": true,
             // https://github.com/azu/textlint-rule-no-mix-dearu-desumasu
             // 文の敬体(ですます調)、常体(である調)のチェック
             ""no-mix-dearu-desumasu"": true,
             // https://github.com/azu/textlint-rule-no-nfd
             // ホ゜ケット エンシ゛ン
             // のような、Mac OS XでPDFやFinderからのコピペで発生する濁点のチェック
             ""no-nfd"": true,
             // https://github.com/textlint-rule/textlint-rule-no-invalid-control-character
             // 制御文字の検出
             ""no-invalid-control-character"": true
        }
    }
}
Semantic Versioning Policy
次のルールでバージョンが更新されます。

Patch リリース

各ルールのバグ修正 (警告を減らす方向への修正)
ドキュメントの改善
内部的な変更 (リファクタリングやテストの改善など)
リリース失敗時の再リリース


Minor リリース

各ルールのバグ修正 (警告を増やす方向への修正)
新オプションの追加
既存ルールの非推奨化


Major リリース

プリセットへのルールの追加
プリセットからルールの削除
既存のオプション値の変更



更新内容はReleases pageを参照してください。
Community
質問は以下のGitterでお願いします。

Contributing

Fork it!
Create your feature branch: git checkout -b my-new-feature
Commit your changes: git commit -am 'Add some feature'
Push to the branch: git push origin my-new-feature
Submit a pull request :D

License
MIT
",26
chipsec/chipsec,C,"CHIPSEC: Platform Security Assessment Framework

CHIPSEC is a framework for analyzing the security of PC platforms including hardware, system firmware (BIOS/UEFI), and platform components. It includes a security test suite, tools for accessing various low level interfaces, and forensic capabilities. It can be run on Windows, Linux, Mac OS X and UEFI shell. Instructions for installing and using CHIPSEC can be found in the manual.
NOTE: This software is for security testing purposes. Use at your own risk. Read WARNING.txt before using.
First version of CHIPSEC was released in March 2014:
Announcement at CanSecWest 2014
Recent presentation on how to use CHIPSEC to find vulnerabilities in firmware, hypervisors and hardware configuration, explore low level system assets and even detect firmware implants:
Exploring Your System Deeper
Projects That Include CHIPSEC

Linux UEFI Validation (LUV)

Contact Us
Mailing lists:

CHIPSEC users: chipsec-users
CHIPSEC discussion list on 01.org

Follow us on twitter
",1790
LiveChief/wireguard-install,Shell,"Wireguard Install
Wireguard Server
wget https://raw.githubusercontent.com/LiveChief/wireguard-install/master/wireguard-server.sh
bash wireguard-server.sh

Wireguard Client
wget https://raw.githubusercontent.com/LiveChief/wireguard-install/master/wireguard-client.sh
bash wireguard-client.sh

Wireguard Multi-hop
wget https://raw.githubusercontent.com/LiveChief/wireguard-install/master/wireguard-multihop.sh
bash wireguard-multihop.sh

Copy $HOME/$CLIENT_NAME-wg0.conf to /etc/wireguard/wg0.conf
Setup Wireguard service on client
systemctl enable wg-quick@wg0.service

Start Wireguard service on client.
systemctl start wg-quick@wg0.service

Stop Wireguard Service
systemctl stop wg-quick@wg0.service

Restart Wireguard Service
systemctl restart wg-quick@wg0.service

Show Wireguard status
wg show

",9
liesauer/Free-SS-SSR,None,"Free-SS-SSR
免费的SS账号、SSR账号，定期更新
更新时间：2019-05-17 08:47 +0800
说明
状态     ：🙂 - 良好 🙁 - 多次连接失败或连接超时
PING值   ：不代表真实使用中的速度，仅供参考。
SS账号：只能在SS里面使用。
兼容账号：能向上向下兼容SS、SSR，你可以在两个软件中使用。
SSR账号：只能在SSR里面使用。
👉扫码页面👈
🎉🎉🎉好消息！订阅地址已全面开放！麻麻再也不用担心我要人工维护酸酸乳了！
🎉🎉🎉好消息！Issues页面 已开放，大家有问题、意见、建议可到 Issues页面 提出。
👉订阅链接👈
兼容账号



-
PING
服务器
端口
密码
加密方式
区域




🙂
42
47.75.116.69
12345
4332erw
rc4-md5
UN


🙂
43
47.91.229.182
12334
rewr
rc4-md5
UN


🙂
44
47.75.125.77
12344
432erwr
rc4-md5
UN


🙂
45
47.52.16.47
12333
43ewrqre
rc4-md5
UN


🙂
69
36.228.214.160
443
nexitally
aes-128-ctr
TW


🙂
77
5.181.5.105
443
nHHpgf
aes-256-cfb
UN


🙂
80
45.67.53.93
369
lncn.org sa6
rc4
UN


🙂
140
172.105.213.201
10456
fafajofdsgc
aes-256-cfb
UN


🙂
145
172.104.123.158
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
JP


🙂
148
172.104.123.158
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
UN


🙂
154
172.105.217.127
14541
TPOYVGxKglpi
aes-256-cfb
UN


🙂
155
91.188.223.72
543
http://t.cn/RD0D7sx
rc4-md5
UN


🙂
156
185.173.92.181
443
sssru.icu
rc4-md5
UN


🙂
156
185.135.82.80
2333
doub.io
aes-128-ctr
RU


🙂
162
46.29.162.46
1026
91vpn.cf
rc4-md5
RU


🙂
168
46.29.162.104
21560
5500
chacha20-ietf
RU


🙂
169
46.17.44.9
369
lncn.org 6a
rc4
UN


🙂
176
91.188.223.72
8080
http://t.cn/EGJIyrl
rc4-md5
RU


🙂
179
45.79.82.49
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
182
185.133.193.83
369
lncn.org 6a
rc4
UN


🙂
183
103.135.102.119
1257
5785455
chacha20
US


🙂
185
192.241.221.124
11987
ss8.pm-74764838
aes-256-cfb
US


🙂
186
67.21.81.240
8388
password
aes-256-cfb
UN


🙂
186
185.243.57.221
80
t.me/SSRSUB
rc4-md5
US


🙂
187
23.95.70.189
59608
supermyssr
chacha20-ietf
UN


🙂
188
185.243.57.221
80
t.me/SSRSUB
rc4-md5
UN


🙂
188
185.133.193.85
369
lncn.org 6a
rc4
UN


🙂
188
185.243.57.221
543
http://t.cn/RD0D7sx
rc4-md5
UN


🙂
189
103.124.107.7
9052
jt2ekBNc9HuVtm2a
aes-256-cfb
US


🙂
190
45.79.93.178
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
190
45.79.93.164
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
190
198.199.109.79
19687
ssx.re-94520056
aes-256-cfb
US


🙂
191
139.162.115.215
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
JP


🙂
192
45.79.95.18
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
192
45.79.87.208
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
193
103.124.107.7
9052
jt2ekBNc9HuVtm2a
aes-256-cfb
UN


🙂
193
45.79.83.180
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
194
45.79.94.57
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
196
45.79.95.58
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
197
207.246.107.206
1996
wujie1314
chacha20
US


🙂
201
45.79.96.77
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
205
157.230.154.68
11012
ssx.re-75871166
aes-256-cfb
US


🙂
207
139.162.115.215
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
UN


🙂
207
45.79.97.186
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
208
141.98.213.163
988
5M57kg11c214qDmK
chacha20
KR


🙂
210
45.77.175.103
2019
doub.io
aes-128-ctr
UN


🙂
211
194.124.35.169
369
lncn.org sa6
rc4
UN


🙂
214
141.98.213.163
988
5M57kg11c214qDmK
chacha20
UN


🙂
239
45.33.32.152
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
US


🙂
240
45.33.48.155
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
US


🙂
241
176.126.78.212
443
jfopwejfasohgadf
chacha20-ietf
UN


🙂
241
168.62.163.117
993
2019.03.07
rc4-md5
UN


🙂
244
217.182.242.93
80
hfdeyibcfgh
chacha20-ietf
UN


🙂
244
66.175.223.22
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
US


🙂
245
173.255.230.159
11255
f55.fun-82730394
aes-256-cfb
US


🙂
245
172.104.179.17
19077
f55.fun-50558565
aes-256-cfb
SG


🙂
247
168.62.163.117
993
2019.03.07
rc4-md5
US


🙂
253
45.33.80.198
13327
f55.fun-46814510
aes-256-cfb
US


🙂
256
192.210.190.101
25581
superssrnet
aes-256-cfb
US


🙂
257
128.199.187.62
14811
ssx.re-92498252
aes-256-cfb
SG


🙂
258
107.172.156.130
80
superssr.net
chacha20-ietf
UN


🙂
260
159.89.114.104
45235
Iwweruyui
chacha20
UN


🙂
262
45.12.205.202
12502
12345678
aes-256-cfb
AU


🙂
266
104.167.97.164
543
http://t.cn/RD0D7sx
rc4-md5
CA


🙂
266
45.33.69.91
11040
f55.fun-40097695
aes-256-cfb
US


🙂
266
178.128.94.215
10613
ss8.pm-02275771
aes-256-cfb
SG


🙂
268
104.167.97.164
80
t.me/SSRSUB
rc4-md5
UN


🙂
268
185.224.249.34
9001
UkXRsXvR6buDMG2Y
aes-256-cfb
RU


🙂
282
91.219.237.119
9001
getvpn20190501
aes-256-cfb
HU


🙂
283
213.183.48.10
17228
ss8.pm-19235472
rc4-md5
RU


🙂
298
213.226.68.94
9030
GeregetR8cvQHzYr
aes-256-cfb
DE


🙂
306
139.162.37.161
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
SG


🙂
313
172.104.39.134
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
SG


🙂
315
139.162.37.161
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
UN


🙂
344
91.201.65.148
9069
tHKW7Ww2mck9CHQG
aes-256-cfb
UN


🙂
356
212.60.5.208
369
lncn.org 6a
rc4
UN


🙂
375
134.209.150.54
45235
Iwweruyui
chacha20
UN


🙂
226
172.104.188.241
11963
f55.fun-96885875
aes-256-cfb
SG


🙂
257
172.104.171.145
19846
f55.fun-58018742
aes-256-cfb
SG


🙂
266
192.154.197.89
40899
Y3oEquMWO2DL
aes-256-cfb
US


🙁
125
172.105.213.201
10456
fafajofdsgc
aes-256-cfb
JP


🙁
0
23.94.5.222
80
199844
aes-256-cfb
UN


🙁
0
stable1.local
443
https://t.me/autossr
aes-256-cfb
UN


🙁
0
47.52.103.137
12378
342eqwrqew
rc4-md5
UN


🙁
0
47.75.197.167
12378
43214er
rc4-md5
UN


🙁
0
47.91.249.54
12334
erw34r
rc4-md5
UN


🙁
0
173.82.232.48
22731
WeCrBq
rc4-md5
UN



",141
ryankurte/rust-radio-at86rf212,Rust,"rust-radio-at86rf212
A rust driver for the Atmel AT86RF212 Sub 1GHz ISM band radio IC, based on ryankurte/libat86rf212.
Status




Open Issues
Work in Progress

 Register operations
 Initialisation
 Polling
 Simple Send
 Simple Receive
 Packet building & parsing
 Auto ACK
 Auto Retransmit
 Interrupt Mode
 DMA support
 Unit testing
 Integration Testing

Testing
Unit testing should be implemented using dbrgn/embedded-hal-mock.
Integration testing is run using a Raspberry Pi model 3 with an IO shield and a pair of XPlained PRO Zigbit + ATRF212B-0-U modules.
The RPi pins are configured at startup as in rpi_setup.sh and the environment is configured in rpi_env.sh (though these are separately injected into the test system). Note that CS0 and CS1 functions are not currently used.
buildkite with a custom worker is used to run integration tests against a physical hardware.

Licensing
This project is licensed as GPLv3 for all purposes. For alternative licensing options / proprietary use, please contact the author (and we'll be happy to help ^_^).
",4
microsoft/terminal,C++,"Welcome!
This repository contains the source code for:

Windows Terminal
The Windows console host (conhost.exe)
Components shared between the two projects
ColorTool
Sample projects that show how to consume the Windows Console APIs

Other related repositories include:

Console API Documentation

Build Status



Project
Build Status




Terminal



ColorTool




Terminal & Console Overview
Please take a few minutes to review the overview below before diving into the code:
Windows Terminal
Windows Terminal is a new, modern, feature-rich, productive terminal application for command-line users. It includes many of the features most frequently requested by the Windows command-line community including support for tabs, rich text, globalization, configurability, theming & styling, and more.
The Terminal will also need to meet our goals and measures to ensure it remains fast, and efficient, and doesn't consume vast amounts of memory or power.
The Windows console host
The Windows console host, conhost.exe, is Windows' original command-line user experience. It implements Windows' command-line infrastructure, and is responsible for hosting the Windows Console API, input engine, rendering engine, and user preferences. The console host code in this repository is the actual source from which the conhost.exe in Windows itself is built.
Console's primary goal is to remain backwards-compatible with existing console subsystem applications.
Since assuming ownership of the Windows command-line in 2014, the team has added several new features to the Console, including window transparency, line-based selection, support for ANSI / Virtual Terminal sequences, 24-bit color, a Pseudoconsole (""ConPTY""), and more.
However, because the Console's primary goal is to maintain backward compatibility, we've been unable to add many of the features the community has been asking for, and which we've been wanting to add for the last several years--like tabs!
These limitations led us to create the new Windows Terminal.
Shared Components
While overhauling the Console, we've modernized its codebase considerably. We've cleanly separated logical entities into modules and classes, introduced some key extensibility points, replaced several old, home-grown collections and containers with safer, more efficient STL containers, and made the code simpler and safer by using Microsoft's WIL header library.
This overhaul work resulted in the creation of several key components that would be useful for any terminal implementation on Windows, including a new DirectWrite-based text layout and rendering engine, a text buffer capable of storing both UTF-16 and UTF-8, and a VT parser/emitter.
Building a new terminal
When we started building the new terminal application, we explored and evaluated several approaches and technology stacks. We ultimately decided that our goals would be best met by sticking with C++ and sharing the aforementioned modernized components, placing them atop the modern Windows application platform and UI framework.
Further, we realized that this would allow us to build the terminal's renderer and input stack as a reusable Windows UI control that others can incorporate into their applications.
FAQ
Where can I download Windows Terminal?
There are no binaries to download quite yet.
The Windows Terminal is in the very early alpha stage, and not ready for the general public quite yet. If you want to jump in early, you can try building it yourself from source.
Otherwise, you'll need to wait until Mid-June for an official preview build to drop.
I built and ran the new Terminal, but I just get a blank window app!
Make sure your are building for your computer's architecture. If your box has a 64-bit Windows change your Solution Platform to x64.
To check your OS architecture go to Settings -> System -> About (or Win+X -> System) and under Device specifications check for the  System type
I built and ran the new Terminal, but it looks just like the old console! What gives?
Firstly, make sure you're building & deploying CascadiaPackage in Visual Studio, NOT Host.EXE. OpenConsole.exe is just conhost.exe, the same old console you know and love. opencon.cmd will launch openconsole.exe, and unfortunately, openterm.cmd is currently broken.
Secondly, try pressing Ctrl + T. The tabs are hidden when you only have one tab by default. In the future, the UI will be dramatically different, but for now, the defaults are supposed to look like the console defaults.
I tried running WindowsTerminal.exe and it crashes!

Don't try to run it unpackaged. Make sure to build & deploy CascadiaPackage from Visual Studio, and run the Windows Terminal (Dev Build) app.
Make sure you're on the right version of Windows. You'll need to be on Insider's builds, or wait for the 1903 release, as the Windows Terminal REQUIRES features from the latest Windows release.

Getting Started
Prerequisites

You must be running Windows 1903 (build >= 10.0.18362.0) or above in order to run Windows Terminal


As of May 2019 this build is only available through Windows Insider Program. You may register and configure Insider Program through your device's system settings.



You must have the 1903 SDK (build 10.0.18362.0) installed


You must have at least VS 2017 installed


You must install the following Workloads via the VS Installer:

Desktop Development with C++

If you're running VS2019, you'll also need to install the following Individual Components:

MSVC v141 - VS 2017 C++ (x86 and x64) build tools
C++ ATL for v141 build tools (x86 and x64)




Universal Windows Platform Development

Also install the following Individual Component:

C++ (v141) Universal Windows Platform Tools







You must also enable Developer Mode in the Windows Settings app to locally install and run the Terminal app.


Debugging

To debug in VS, right click on CascadiaPackage (from VS Solution Explorer) and go to properties, in the Debug menu, change ""Application process"" and ""Background task process"" to ""Native Only""

Contributing
We are excited to work alongside you, our amazing community, to build and enhance Windows Terminal!
We ask that before you start work on a feature that you would like to contribute, please file an issue describing your proposed change: We will be happy to work with you to figure out the best approach, provide guidance and mentorship throughout feature development, and help avoid any wasted or duplicate effort.

👉 Remember! Your contributions may be incorporated into future versions of Windows! Because of this, all pull requests will be subject to the same level of scrutiny for quality, coding standards, performance, globalization, accessibility, and compatibility as those of our internal contributors.


⚠ Note: The Command-Line Team is actively working out of this repository and will be periodically re-structuring the code to make it easier to comprehend, navigate, build, test, and contribute to, so DO expect significant changes to code layout on a regular basis.

Documentation
All documentation is located in the ./doc folder. If you would like to contribute to the documentation, please submit a pull request.
Communicating with the Team
The easiest way to communicate with the team is via GitHub issues. Please file new issues, feature requests and suggestions, but DO search for similar open/closed pre-existing issues before you do.
Please help us keep this repository clean, inclusive, and fun! We will not tolerate any abusive, rude, disrespectful or inappropriate behavior. Read our Code of Conduct for more details.
If you would like to ask a question that you feel doesn't warrant an issue (yet), please reach out to us via Twitter:


Rich Turner, Program Manager: @richturn_ms


Dustin Howett, Engineering Lead: @dhowett


Michael Niksa, Senior Developer: @michaelniksa


Kayla Cinnamon, Program Manager (especially for UX issues): @cinnamon_msft


Developer Guidance
Building the Code
This repository uses git submodules for some of its dependencies. To make sure submodules are restored or updated, be sure to run the following prior to building:
git submodule update --init --recursive
OpenConsole.sln may be built from within Visual Studio or from the command-line using MSBuild. To build from the command line:
.\tools\razzle.cmd
bcz
We've provided a set of convenience scripts as well as README in the /tools directory to help automate the process of building and running tests.
Coding Guidance
Please review these brief docs below relating to our coding standards etc.

👉 If you find something missing from these docs, feel free to contribute to any of our documentation files anywhere in the repository (or make some new ones!)

This is a work in progress as we learn what we'll need to provide people in order to be effective contributors to our project.

Coding Style
Code Organization
Exceptions in our legacy codebase
Helpful smart pointers and macros for interfacing with Windows in WIL

Code of Conduct
This project has adopted the Microsoft Open Source Code of Conduct.
For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.
",37773
kubernetes-client/ruby,Ruby,"Kubernetes Ruby Client
Ruby client for the kubernetes API.
Usage
require 'kubernetes'
require 'pp'

kube_config = Kubernetes::KubeConfig.new(""#{ENV['HOME']}/.kube/config"")
config = Kubernetes::Configuration.new()

kube_config.configure(config)

client = Kubernetes::CoreV1Api.new(Kubernetes::ApiClient.new(config))

pp client.list_namespaced_pod('default')
Contribute
Please see CONTRIBUTING.md for instructions on how to contribute.
Code of conduct
Participation in the Kubernetes community is governed by the Kubernetes Code of Conduct.
Development
Update client
to update the client clone the gen repo and run this command at the root of the client repo:
${GEN_REPO_BASE}/openapi/ruby.sh kubernetes settings
License
This program follows the Apache License version 2.0 (http://www.apache.org/licenses/ ).  See LICENSE file included with the distribution for details.
",27
kk7nc/Text_Classification,Python,"
Text Classification Algorithms: A Survey
   
 
 


Referenced paper : Text Classification Algorithms: A Survey


Table of Contents


Introduction
Text and Document Feature Extraction
Text Cleaning and Pre-processing
Tokenization
Stop words
Capitalization
Slang and Abbreviation
Noise Removal
Spelling Correction
Stemming
Lemmatization


Word Embedding
Word2Vec
Global Vectors for Word Representation (GloVe)
Contextualized Word Representations
FastText


Weighted Words
Term frequency
Term Frequency-Inverse Document Frequency


Comparison of Feature Extraction Techniques


Dimensionality Reduction
Principal Component Analysis (PCA)
Linear Discriminant Analysis (LDA)
Non-negative Matrix Factorization (NMF)
Random Projection
Autoencoder
T-distributed Stochastic Neighbor Embedding (T-SNE)


Text Classification Techniques
Rocchio classification
Boosting and Bagging
Boosting
Bagging


Naive Bayes Classifier
K-nearest Neighbor
Support Vector Machine (SVM)
Decision Tree
Random Forest
Conditional Random Field (CRF)
Deep Learning
Deep Neural Networks
Recurrent Neural Networks (RNN)
Gated Recurrent Unit (GRU)
Long Short-Term Memory (LSTM)


Convolutional Neural Networks (CNN)
Hierarchical Attention Networks
Recurrent Convolutional Neural Networks (RCNN)
Random Multimodel Deep Learning (RMDL)
Hierarchical Deep Learning for Text (HDLTex)




Evaluation
F1 Score
Matthew correlation coefficient (MCC)
Receiver operating characteristics (ROC)
Area Under Curve (AUC)


Text and Document Datasets
IMDB
Reuters-21578
20Newsgroups
Web of Science Dataset


Citations:



Introduction




Text and Document Feature Extraction

Text feature extraction and pre-processing for classification algorithm is very significant. In this section, we start to talk about text cleaning which most of documents have a lot of noise. In this part we discuss about two main methods of text feature extractions which are word embedding and weighted word.

Text Cleaning and Pre-processing
In Natural Language Processing (NLP), most of the text and document datasets contains many unnecessary words such as Stopwords, miss-spelling, slang, and etc. In this section, we briefly explain some techniques and method for text cleaning and pre-processing text datasets. In many algorithm, especially statistical and probabilistic learning algorithm, noise and unnecessary features could have bad effect on performance of the system, so one of the solution could be illumination and remove these features as pre-processing step.

Tokenization
Tokenization is a part of pre-process to break a stream of text up into words, phrases, symbols, or other meaningful elements called tokens.  The main goal of this step is the exploration of the words in a sentence. In text mining beside of text classification, it;'s necessitate a parser which processes the tokenization of the documents; for example:
sentence:
After sleeping for four hours, he decided to sleep for another four

In this case, the tokens are as follows:
{'After', 'sleeping', 'for', 'four', 'hours', 'he', 'decided', 'to', 'sleep', 'for', 'another', 'four'}

Here is python code for Tokenization:
from nltk.tokenize import word_tokenize
text = ""After sleeping for four hours, he decided to sleep for another four""
tokens = word_tokenize(text)
print(tokens)

Stop words
Text and document classification over social media such as Twitter, Facebook, and so on is usually affected by the noisy nature (abbreviations, irregular forms) of these data points.
Here is an exmple from  geeksforgeeks
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

example_sent = ""This is a sample sentence, showing off the stop words filtration.""

stop_words = set(stopwords.words('english'))

word_tokens = word_tokenize(example_sent)

filtered_sentence = [w for w in word_tokens if not w in stop_words]

filtered_sentence = []

for w in word_tokens:
    if w not in stop_words:
        filtered_sentence.append(w)

print(word_tokens)
print(filtered_sentence)
Output:
['This', 'is', 'a', 'sample', 'sentence', ',', 'showing',
'off', 'the', 'stop', 'words', 'filtration', '.']
['This', 'sample', 'sentence', ',', 'showing', 'stop',
'words', 'filtration', '.']

Capitalization
Text and document data points have a diversity of capitalization to became a sentence; substantially, several sentences together create a document. The most common approach of capitalization method could be to reduce everything to lower case. This technique makes all words in text and document in same space, but it is caused to a significant problem for meaning of some words such as ""US"" to ""us"" which first one represent the country of United States of America and second one is pronouns word; thus, for solving this problem, we could use slang and abbreviation converters.
text = ""The United States of America (USA) or America, is a federal republic composed of 50 states""
print(text)
print(text.lower())
Output:
""The United States of America (USA) or America, is a federal republic composed of 50 states""
""the united states of america (usa) or america, is a federal republic composed of 50 states""

Slang and Abbreviation
Slang and Abbreviation is another problem as pre-processing step for cleaning text datasets. An abbreviation  is a shortened form of a word or phrase which contain mostly first letters form the words such as SVM stand for  Support Vector Machine. Slang is a version of language of an informal talk or text that has different meaning such as ""lost the plot"", it essentially means that they've gone mad. The common method for dealing with these words is convert them to formal language.

Noise Removal
The other issue of text cleaning as pre-processing step is noise removal which most of text and document datasets contains many unnecessary characters such as punctuation, special character. It's important to know the punctuation is critical for us to understand the meaning of the sentence, but it could have effect for classification algorithms.
Here is simple code to remove standard noise from text:
def text_cleaner(text):
    rules = [
        {r'>\s+': u'>'},  # remove spaces after a tag opens or closes
        {r'\s+': u' '},  # replace consecutive spaces
        {r'\s*<br\s*/?>\s*': u'\n'},  # newline after a <br>
        {r'</(div)\s*>\s*': u'\n'},  # newline after </p> and </div> and <h1/>...
        {r'</(p|h\d)\s*>\s*': u'\n\n'},  # newline after </p> and </div> and <h1/>...
        {r'<head>.*<\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>
        {r'<a\s+href=""([^""]+)""[^>]*>.*</a>': r'\1'},  # show links instead of texts
        {r'[ \t]*<[^<]*?/?>': u''},  # remove remaining tags
        {r'^\s+': u''}  # remove spaces at the beginning
    ]
    for rule in rules:
    for (k, v) in rule.items():
        regex = re.compile(k)
        text = regex.sub(v, text)
    text = text.rstrip()
    return text.lower()

Spelling Correction
One of the optional part of the pre-processing step is spelling correction which is happened in texts and documents. Many algorithm, techniques, and methods have been addressed this problem in NLP. Many techniques and methods are available for researchers such as hashing-based and context-sensitive spelling correction techniques, or  spelling correction using trie and damerau-levenshtein distance bigram.
from autocorrect import spell

print spell('caaaar')
print spell(u'mussage')
print spell(u'survice')
print spell(u'hte')
Result:
caesar
message
service
the


Stemming
Text Stemming is modifying to obtain variant word forms using different linguistic processes such as affixation (addition of affixes). For example, the stem of the word ""studying"" is ""study"", to which -ing.
Here is an example of Stemming from NLTK
from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize

ps = PorterStemmer()

example_words = [""python"",""pythoner"",""pythoning"",""pythoned"",""pythonly""]

for w in example_words:
print(ps.stem(w))
Result:
python
python
python
python
pythonli


Lemmatization
Text lemmatization is process in NLP to replaces the suffix of a word with a different one or removes the suffix of a word completely to get the basic word form (lemma).
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

print(lemmatizer.lemmatize(""cats""))

Word Embedding
Different word embedding has been proposed to translate these unigrams into understandable input for machine learning algorithms. Most basic methods to perform such embedding is term-frequency~(TF) where each word will be mapped to a number corresponding to the number of occurrence of that word in the whole corpora. The other term frequency functions have been also used that present words frequency as Boolean or logarithmically scaled number. As regarding to results, each document will be translated to a vector with the length of document, containing the frequency of the words in that document. Although such approach is very intuitive but it suffers from the fact that particular words that are used commonly in language literature would dominate such word representation.


Word2Vec
Original from https://code.google.com/p/word2vec/
I’ve copied it to a github project so I can apply and track community
patches for my needs (starting with capability for Mac OS X
compilation).

makefile and some source has been modified for Mac OS X
compilation See
https://code.google.com/p/word2vec/issues/detail?id=1#c5
memory patch for word2vec has been applied See
https://code.google.com/p/word2vec/issues/detail?id=2
Project file layout altered

There seems to be a segfault in the compute-accuracy utility.
To get started:
cd scripts && ./demo-word.sh

Original README text follows:
This tool provides an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words. These representations can be subsequently used in many natural language processing applications and for further research.
this code provides an implementation of the Continuous Bag-of-Words (CBOW) and
the Skip-gram model (SG), as well as several demo scripts.
Given a text corpus, the word2vec tool learns a vector for every word in
the vocabulary using the Continuous Bag-of-Words or the Skip-Gram neural
network architectures. The user should to specify the following: -
desired vector dimensionality - the size of the context window for
either the Skip-Gram or the Continuous Bag-of-Words model - training
algorithm: hierarchical softmax and / or negative sampling - threshold
for downsampling the frequent words - number of threads to use - the
format of the output word vector file (text or binary)
Usually, the other hyper-parameters such as the learning rate do not
need to be tuned for different training sets.
The script demo-word.sh downloads a small (100MB) text corpus from the
web, and trains a small word vector model. After the training is
finished, the user can interactively explore the similarity of the
words.
More information about the scripts is provided at
https://code.google.com/p/word2vec/

Global Vectors for Word Representation (GloVe)

An implementation of the GloVe model for learning word representations is provided, and describe how to download web-dataset vectors or train your own. See the  project page  or the   paper  for more information on glove vectors.

Contextualized Word Representations
ELMo is a deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). These word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. They can be easily added to existing models and significantly improve the state of the art across a broad range of challenging NLP problems, including question answering, textual entailment and sentiment analysis.
ELMo representations are:

Contextual: The representation for each word depends on the entire context in which it is used.
Deep: The word representations combine all layers of a deep pre-trained neural network.
Character based: ELMo representations are purely character based, allowing the network to use morphological clues to form robust representations for out-of-vocabulary tokens unseen in training.

Tensorflow implementation
Tensorflow implementation of the pretrained biLM used to compute ELMo representations from ""Deep contextualized word representations"".
This repository supports both training biLMs and using pre-trained models for prediction.
We also have a pytorch implementation available in AllenNLP.
You may also find it easier to use the version provided in Tensorflow Hub if you just like to make predictions.
pre-trained models:
We have several different English language pre-trained biLMs available for use. Each model is specified with two separate files, a JSON formatted ""options"" file with hyperparameters and a hdf5 formatted file with the model weights. Links to the pre-trained models are available here.
There are three ways to integrate ELMo representations into a downstream task, depending on your use case.

Compute representations on the fly from raw text using character input. This is the most general method and will handle any input text. It is also the most computationally expensive.
Precompute and cache the context independent token representations, then compute context dependent representations using the biLSTMs for input data. This method is less computationally expensive then #1, but is only applicable with a fixed, prescribed vocabulary.
Precompute the representations for your entire dataset and save to a file.

We have used all of these methods in the past for various use cases. #1 is necessary for evaluating at test time on unseen data (e.g. public SQuAD leaderboard). #2 is a good compromise for large datasets where the size of the file in #3 is unfeasible (SNLI, SQuAD). #3 is a good choice for smaller datasets or in cases where you'd like to use ELMo in other frameworks.
In all cases, the process roughly follows the same steps. First, create a Batcher (or TokenBatcher for #2) to translate tokenized strings to numpy arrays of character (or token) ids. Then, load the pretrained ELMo model (class BidirectionalLanguageModel). Finally, for steps #1 and #2 use weight_layers to compute the final ELMo representations. For #3, use BidirectionalLanguageModel to write all the intermediate layers to a file.



Architecture of the language model applied to an example sentence [Reference:  arXiv paper].




FastText



fastText is a library for efficient learning of word representations and sentence classification.
Github: facebookresearch/fastText
Models

Recent state-of-the-art English word vectors.
Word vectors for 157 languages trained on Wikipedia and Crawl.
Models for language identification and various supervised tasks.

Supplementary data :

The preprocessed YFCC100M data .

FAQ
You can find answers to frequently asked questions on Their project website.
Cheatsheet
Also a cheatsheet is provided full of useful one-liners.

Weighted Words

Term frequency
Term frequency is Bag of words that is simplest technique of text feature extraction. This method is based on counting number of the words in each document and assign it to feature space.

Term Frequency-Inverse Document Frequency
The mathematical representation of weight of a term in a document by Tf-idf is given:

Where N is number of documents and df(t) is the number of documents containing the term t in the corpus. The first part would improve recall and the later would improve the precision of the word embedding. Although tf-idf tries to overcome the problem of common terms in document, it still suffers from some other descriptive limitations. Namely, tf-idf cannot account for the similarity between words in the document since each word is presented as an index. In the recent years, with development of more complex models such as neural nets, new methods has been presented that can incorporate concepts such as similarity of words and part of speech tagging. This work uses, word2vec and Glove, two of the most common methods that have been successfully used for deep learning techniques.
from sklearn.feature_extraction.text import TfidfTransformer
def loadData(X_train, X_test,MAX_NB_WORDS=75000):
    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)
    X_train = vectorizer_x.fit_transform(X_train).toarray()
    X_test = vectorizer_x.transform(X_test).toarray()
    print(""tf-idf with"",str(np.array(X_train).shape[1]),""features"")
    return (X_train,X_test)

Comparison of Feature Extraction Techniques


Model
Advantages
Limitation

Weighted Words

Easy to compute
Easy to compute the similarity between 2 documents using it
Basic metric to extract the most descriptive terms in a document
Works with an unknown word (e.g., New words in languages)



It does not capture the position in the text (syntactic)
It does not capture meaning in the text (semantics)
Common words effect on the results (e.g., “am”, “is”, etc.)



TF-IDF

Easy to compute
Easy to compute the similarity between 2 documents using it
Basic metric to extract the most descriptive terms in a document
Common words do not affect the results due to IDF (e.g., “am”, “is”, etc.)



It does not capture the position in the text (syntactic)
It does not capture meaning in the text (semantics)



Word2Vec

It captures the position of the words in the text (syntactic)
It captures meaning in the words (semantics)



It cannot capture the meaning of the word from the text (fails to capture polysemy)
It cannot capture out-of-vocabulary words from corpus



GloVe (Pre-Trained)

It captures the position of the words in the text (syntactic)
It captures meaning in the words (semantics)
Trained on huge corpus



It cannot capture the meaning of the word from  the text (fails to capture polysemy)
Memory consumption for storage
It cannot capture out-of-vocabulary words from corpus



GloVe (Trained)

It is very straightforward, e.g., to enforce the word vectors to capture sub-linear relationships in the vector space (performs better than Word2vec)
Lower weight for highly frequent word pairs such as stop words like “am”, “is”, etc. Will not dominate training progress



Memory consumption for storage
Needs huge corpus to learn
It cannot capture out-of-vocabulary words from the corpus
It cannot capture the meaning of the word from  the text (fails to capture polysemy)



FastText

Works for rare words (rare in their character n-grams which are still shared with other words
Solves out of vocabulary words with n-gram in character level



It cannot capture the meaning of the word from the text (fails to capture polysemy)
Memory consumption for storage
Computationally is more expensive in comparing with GloVe and Word2Vec



Contextualized Word Representations

It captures the meaning of the word from the text (incorporates context, handling polysemy)



Memory consumption for storage
Improves performance notably on downstream tasks. Computationally is more expensive in comparison to others
Needs another word embedding for all LSTM and feedforward layers
It cannot capture out-of-vocabulary words from a corpus
Works only sentence and document level (it cannot work for individual word level)






Dimensionality Reduction


Principal Component Analysis (PCA)
Principle component analysis~(PCA) is the most popular technique in multivariate analysis and dimensionality reduction. PCA is a method to identify a subspace in which the data approximately lies. This means finding new variables that are uncorrelated and maximizing the variance to preserve as much variability as possible.
Example of PCA on text dataset (20newsgroups) from  tf-idf with 75000 features to 2000 components:
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

def TFIDF(X_train, X_test, MAX_NB_WORDS=75000):
    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)
    X_train = vectorizer_x.fit_transform(X_train).toarray()
    X_test = vectorizer_x.transform(X_test).toarray()
    print(""tf-idf with"", str(np.array(X_train).shape[1]), ""features"")
    return (X_train, X_test)


from sklearn.datasets import fetch_20newsgroups

newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

X_train,X_test = TFIDF(X_train,X_test)

from sklearn.decomposition import PCA
pca = PCA(n_components=2000)
X_train_new = pca.fit_transform(X_train)
X_test_new = pca.transform(X_test)

print(""train with old features: "",np.array(X_train).shape)
print(""train with new features:"" ,np.array(X_train_new).shape)

print(""test with old features: "",np.array(X_test).shape)
print(""test with new features:"" ,np.array(X_test_new).shape)
output:
tf-idf with 75000 features
train with old features:  (11314, 75000)
train with new features: (11314, 2000)
test with old features:  (7532, 75000)
test with new features: (7532, 2000)

Linear Discriminant Analysis (LDA)
Linear Discriminant Analysis (LDA) is a commonly used technique for data classification and dimensionality reduction. LDA is particularly helpful where the within-class frequencies are unequal and their performances have been evaluated on randomly generated test data. Class-dependent and class-independent transformation are two approaches to LDA in which the ratio of between class variance to within class variance and the ratio of the overall variance to within class variance are used respectively.
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis


def TFIDF(X_train, X_test, MAX_NB_WORDS=75000):
    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)
    X_train = vectorizer_x.fit_transform(X_train).toarray()
    X_test = vectorizer_x.transform(X_test).toarray()
    print(""tf-idf with"", str(np.array(X_train).shape[1]), ""features"")
    return (X_train, X_test)


from sklearn.datasets import fetch_20newsgroups

newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

X_train,X_test = TFIDF(X_train,X_test)



LDA = LinearDiscriminantAnalysis(n_components=15)
X_train_new = LDA.fit(X_train,y_train)
X_train_new =  LDA.transform(X_train)
X_test_new = LDA.transform(X_test)

print(""train with old features: "",np.array(X_train).shape)
print(""train with new features:"" ,np.array(X_train_new).shape)

print(""test with old features: "",np.array(X_test).shape)
print(""test with new features:"" ,np.array(X_test_new).shape)
output:
tf-idf with 75000 features
train with old features:  (11314, 75000)
train with new features: (11314, 15)
test with old features:  (7532, 75000)
test with new features: (7532, 15)


Non-negative Matrix Factorization (NMF)
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from sklearn.decomposition import NMF


def TFIDF(X_train, X_test, MAX_NB_WORDS=75000):
    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)
    X_train = vectorizer_x.fit_transform(X_train).toarray()
    X_test = vectorizer_x.transform(X_test).toarray()
    print(""tf-idf with"", str(np.array(X_train).shape[1]), ""features"")
    return (X_train, X_test)


from sklearn.datasets import fetch_20newsgroups

newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

X_train,X_test = TFIDF(X_train,X_test)



NMF_ = NMF(n_components=2000)
X_train_new = NMF_.fit(X_train)
X_train_new =  NMF_.transform(X_train)
X_test_new = NMF_.transform(X_test)

print(""train with old features: "",np.array(X_train).shape)
print(""train with new features:"" ,np.array(X_train_new).shape)

print(""test with old features: "",np.array(X_test).shape)
print(""test with new features:"" ,np.array(X_test_new))
output:
tf-idf with 75000 features
train with old features:  (11314, 75000)
train with new features: (11314, 2000)
test with old features:  (7532, 75000)
test with new features: (7532, 2000)


Random Projection
Random projection or random feature is technique for dimensionality reduction which is mostly used for very large volume dataset or very high dimensional feature space. Text and document, especially with weighted feature extraction, generate huge number of features.
Many researchers addressed Random Projection for text data for text mining, text classification and/or dimensionality reduction.
we start to review some random projection techniques.

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

def TFIDF(X_train, X_test, MAX_NB_WORDS=75000):
    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)
    X_train = vectorizer_x.fit_transform(X_train).toarray()
    X_test = vectorizer_x.transform(X_test).toarray()
    print(""tf-idf with"", str(np.array(X_train).shape[1]), ""features"")
    return (X_train, X_test)


from sklearn.datasets import fetch_20newsgroups

newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

X_train,X_test = TFIDF(X_train,X_test)

from sklearn import random_projection

RandomProjection = random_projection.GaussianRandomProjection(n_components=2000)
X_train_new = RandomProjection.fit_transform(X_train)
X_test_new = RandomProjection.transform(X_test)

print(""train with old features: "",np.array(X_train).shape)
print(""train with new features:"" ,np.array(X_train_new).shape)

print(""test with old features: "",np.array(X_test).shape)
print(""test with new features:"" ,np.array(X_test_new).shape)
output:
tf-idf with 75000 features
train with old features:  (11314, 75000)
train with new features: (11314, 2000)
test with old features:  (7532, 75000)
test with new features: (7532, 2000)

Autoencoder
Autoencoder is a neural network technique that is trained to attempt to copy its input to its output. The autoencoder as dimensional reduction methods have achieved great success via the powerful reprehensibility of neural networks. The main idea is one hidden layer between input and output layers has fewer units which could be used as reduced dimension of feature space. Specially for texts, documents, and sequences that contains many features, autoencoder could help to process of data faster and more efficient.

from keras.layers import Input, Dense
from keras.models import Model

# this is the size of our encoded representations
encoding_dim = 1500

# this is our input placeholder
input = Input(shape=(n,))
# ""encoded"" is the encoded representation of the input
encoded = Dense(encoding_dim, activation='relu')(input)
# ""decoded"" is the lossy reconstruction of the input
decoded = Dense(n, activation='sigmoid')(encoded)

# this model maps an input to its reconstruction
autoencoder = Model(input, decoded)

# this model maps an input to its encoded representation
encoder = Model(input, encoded)


encoded_input = Input(shape=(encoding_dim,))
# retrieve the last layer of the autoencoder model
decoder_layer = autoencoder.layers[-1]
# create the decoder model
decoder = Model(encoded_input, decoder_layer(encoded_input))

autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')
Load data:
autoencoder.fit(x_train, x_train,
                epochs=50,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test))

T-distributed Stochastic Neighbor Embedding (T-SNE)
T-distributed Stochastic Neighbor Embedding (T-SNE) is a nonlinear dimensionality reduction method for embedding high-dimensional data for which is mostly used for visualization in a low-dimensional space. This approach is based on G. Hinton and ST. Roweis . SNE works by converting the high dimensional Euclidean distances into conditional probabilities which represent similarities.

Example:
import numpy as np
from sklearn.manifold import TSNE
X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
X_embedded = TSNE(n_components=2).fit_transform(X)
X_embedded.shape
Example of Glove and T-SNE for text:


Text Classification Techniques


Rocchio classification
The first version of Rocchio algorithm is introduced by rocchio in 1971 to use relevance feedback in querying full-text databases. Since then many researchers addressed and developed this technique for text and document classification. This method uses TF-IDF weights for each informative word instead of a set of Boolean features. Using a training set of documents, Rocchio's algorithm builds a prototype vector for each class which is an average vector over all training document vectors that belongs to a certain class. Then, it will assign each test document to a class with maximum similarity that between test document and each of prototype vectors.
When in nearest centroid classifier, we used for text as input data for classification with tf-idf vectors, this classifier is known as the Rocchio classifier.
from sklearn.neighbors.nearest_centroid import NearestCentroid
from sklearn.pipeline import Pipeline
from sklearn import metrics
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.datasets import fetch_20newsgroups

newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', NearestCentroid()),
                     ])

text_clf.fit(X_train, y_train)


predicted = text_clf.predict(X_test)

print(metrics.classification_report(y_test, predicted))
Output:
              precision    recall  f1-score   support

          0       0.75      0.49      0.60       319
          1       0.44      0.76      0.56       389
          2       0.75      0.68      0.71       394
          3       0.71      0.59      0.65       392
          4       0.81      0.71      0.76       385
          5       0.83      0.66      0.74       395
          6       0.49      0.88      0.63       390
          7       0.86      0.76      0.80       396
          8       0.91      0.86      0.89       398
          9       0.85      0.79      0.82       397
         10       0.95      0.80      0.87       399
         11       0.94      0.66      0.78       396
         12       0.40      0.70      0.51       393
         13       0.84      0.49      0.62       396
         14       0.89      0.72      0.80       394
         15       0.55      0.73      0.63       398
         16       0.68      0.76      0.71       364
         17       0.97      0.70      0.81       376
         18       0.54      0.53      0.53       310
         19       0.58      0.39      0.47       251

avg / total       0.74      0.69      0.70      7532

Boosting and Bagging

Boosting

Boosting is a Ensemble learning meta-algorithm for primarily reducing Supervised learning, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by Michael Kearns  and Leslie Valiant (1988, 1989) Can a set of weak learners create a single strong learner. A weak learner is defined to be a Classification that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.pipeline import Pipeline
from sklearn import metrics
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.datasets import fetch_20newsgroups

newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', GradientBoostingClassifier(n_estimators=100)),
                     ])

text_clf.fit(X_train, y_train)


predicted = text_clf.predict(X_test)

print(metrics.classification_report(y_test, predicted))
Output:
             precision    recall  f1-score   support
          0       0.81      0.66      0.73       319
          1       0.69      0.70      0.69       389
          2       0.70      0.68      0.69       394
          3       0.64      0.72      0.68       392
          4       0.79      0.79      0.79       385
          5       0.83      0.64      0.72       395
          6       0.81      0.84      0.82       390
          7       0.84      0.75      0.79       396
          8       0.90      0.86      0.88       398
          9       0.90      0.85      0.88       397
         10       0.93      0.86      0.90       399
         11       0.90      0.81      0.85       396
         12       0.33      0.69      0.45       393
         13       0.87      0.72      0.79       396
         14       0.87      0.84      0.85       394
         15       0.85      0.87      0.86       398
         16       0.65      0.78      0.71       364
         17       0.96      0.74      0.84       376
         18       0.70      0.55      0.62       310
         19       0.62      0.56      0.59       251

avg / total       0.78      0.75      0.76      7532

Bagging

from sklearn.ensemble import BaggingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn import metrics
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.datasets import fetch_20newsgroups

newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', BaggingClassifier(KNeighborsClassifier())),
                     ])

text_clf.fit(X_train, y_train)


predicted = text_clf.predict(X_test)

print(metrics.classification_report(y_test, predicted))
Output:
             precision    recall  f1-score   support
          0       0.57      0.74      0.65       319
          1       0.60      0.56      0.58       389
          2       0.62      0.54      0.58       394
          3       0.54      0.57      0.55       392
          4       0.63      0.54      0.58       385
          5       0.68      0.62      0.65       395
          6       0.55      0.46      0.50       390
          7       0.77      0.67      0.72       396
          8       0.79      0.82      0.80       398
          9       0.74      0.77      0.76       397
         10       0.81      0.86      0.83       399
         11       0.74      0.85      0.79       396
         12       0.67      0.49      0.57       393
         13       0.78      0.51      0.62       396
         14       0.76      0.78      0.77       394
         15       0.71      0.81      0.76       398
         16       0.73      0.73      0.73       364
         17       0.64      0.79      0.71       376
         18       0.45      0.69      0.54       310
         19       0.61      0.54      0.57       251

avg / total       0.67      0.67      0.67      7532

Naive Bayes Classifier
Naïve Bayes text classification has been used in industry
and academia for a long time (introduced by Thomas Bayes
between 1701-1761) ; however, this technique
is studied since 1950s for text and document categorization. Naive Bayes Classifier (NBC) is generative
model which is the most traditional method of text categorization
which is widely used in Information Retrieval. Many researchers addressed and developed this technique
for their applications. We start the most basic version
of NBC which developed by using term-frequency (Bag of
Word) fetaure extraction technique by counting number of
words in documents
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn import metrics
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.datasets import fetch_20newsgroups

newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', MultinomialNB()),
                     ])

text_clf.fit(X_train, y_train)


predicted = text_clf.predict(X_test)

print(metrics.classification_report(y_test, predicted))
Output:
               precision    recall  f1-score   support

          0       0.80      0.52      0.63       319
          1       0.81      0.65      0.72       389
          2       0.82      0.65      0.73       394
          3       0.67      0.78      0.72       392
          4       0.86      0.77      0.81       385
          5       0.89      0.75      0.82       395
          6       0.93      0.69      0.80       390
          7       0.85      0.92      0.88       396
          8       0.94      0.93      0.93       398
          9       0.92      0.90      0.91       397
         10       0.89      0.97      0.93       399
         11       0.59      0.97      0.74       396
         12       0.84      0.60      0.70       393
         13       0.92      0.74      0.82       396
         14       0.84      0.89      0.87       394
         15       0.44      0.98      0.61       398
         16       0.64      0.94      0.76       364
         17       0.93      0.91      0.92       376
         18       0.96      0.42      0.58       310
         19       0.97      0.14      0.24       251

avg / total       0.82      0.77      0.77      7532

K-nearest Neighbor
R
In machine learning, the k-nearest neighbors algorithm (kNN)
is a non-parametric technique used for classification.
This method is used in Natural-language processing (NLP)
as text classification in many researches in past
decad

from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn import metrics
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.datasets import fetch_20newsgroups

newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', KNeighborsClassifier()),
                     ])

text_clf.fit(X_train, y_train)

predicted = text_clf.predict(X_test)

print(metrics.classification_report(y_test, predicted))
Output:
               precision    recall  f1-score   support

          0       0.43      0.76      0.55       319
          1       0.50      0.61      0.55       389
          2       0.56      0.57      0.57       394
          3       0.53      0.58      0.56       392
          4       0.59      0.56      0.57       385
          5       0.69      0.60      0.64       395
          6       0.58      0.45      0.51       390
          7       0.75      0.69      0.72       396
          8       0.84      0.81      0.82       398
          9       0.77      0.72      0.74       397
         10       0.85      0.84      0.84       399
         11       0.76      0.84      0.80       396
         12       0.70      0.50      0.58       393
         13       0.82      0.49      0.62       396
         14       0.79      0.76      0.78       394
         15       0.75      0.76      0.76       398
         16       0.70      0.73      0.72       364
         17       0.62      0.76      0.69       376
         18       0.55      0.61      0.58       310
         19       0.56      0.49      0.52       251

avg / total       0.67      0.66      0.66      7532

Support Vector Machine (SVM)
The original version of SVM was introduced by Vapnik and  Chervonenkis in 1963. The early 1990s, nonlinear version was addressed by BE. Boser et al.. Original version of SVM was designed for binary classification problem, but Many researchers work on multi-class problem using this authoritative technique.
The advantages of support vector machines are based on scikit-learn page:

Effective in high dimensional spaces.
Still effective in cases where number of dimensions is greater than the number of samples.
Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.
Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.

The disadvantages of support vector machines include:

If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.
SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).


from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from sklearn import metrics
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.datasets import fetch_20newsgroups

newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', LinearSVC()),
                     ])

text_clf.fit(X_train, y_train)


predicted = text_clf.predict(X_test)

print(metrics.classification_report(y_test, predicted))
output:
               precision    recall  f1-score   support

          0       0.82      0.80      0.81       319
          1       0.76      0.80      0.78       389
          2       0.77      0.73      0.75       394
          3       0.71      0.76      0.74       392
          4       0.84      0.86      0.85       385
          5       0.87      0.76      0.81       395
          6       0.83      0.91      0.87       390
          7       0.92      0.91      0.91       396
          8       0.95      0.95      0.95       398
          9       0.92      0.95      0.93       397
         10       0.96      0.98      0.97       399
         11       0.93      0.94      0.93       396
         12       0.81      0.79      0.80       393
         13       0.90      0.87      0.88       396
         14       0.90      0.93      0.92       394
         15       0.84      0.93      0.88       398
         16       0.75      0.92      0.82       364
         17       0.97      0.89      0.93       376
         18       0.82      0.62      0.71       310
         19       0.75      0.61      0.68       251

avg / total       0.85      0.85      0.85      7532

Decision Tree
One of earlier classification algorithm for text and data mining is decision tree. Decision tree classifiers (DTC's) are used successfully in many diverse areas for classification. The structure of this technique is  a hierarchical decomposition of the data space (only train dataset). Decision tree as classification task is introduced by D. Morgan and developed by JR. Quinlan. The main idea is creating tree based on attribute for categorized data points, but main challenge of decision tree is which attribute or feature could be in parents' level and which one should be in child level. for solving this problem, De Mantaras introduced statistical modeling for feature selection in tree.
from sklearn import tree
from sklearn.pipeline import Pipeline
from sklearn import metrics
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.datasets import fetch_20newsgroups

newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', tree.DecisionTreeClassifier()),
                     ])

text_clf.fit(X_train, y_train)


predicted = text_clf.predict(X_test)

print(metrics.classification_report(y_test, predicted))
output:
               precision    recall  f1-score   support

          0       0.51      0.48      0.49       319
          1       0.42      0.42      0.42       389
          2       0.51      0.56      0.53       394
          3       0.46      0.42      0.44       392
          4       0.50      0.56      0.53       385
          5       0.50      0.47      0.48       395
          6       0.66      0.73      0.69       390
          7       0.60      0.59      0.59       396
          8       0.66      0.72      0.69       398
          9       0.53      0.55      0.54       397
         10       0.68      0.66      0.67       399
         11       0.73      0.69      0.71       396
         12       0.34      0.33      0.33       393
         13       0.52      0.42      0.46       396
         14       0.65      0.62      0.63       394
         15       0.68      0.72      0.70       398
         16       0.49      0.62      0.55       364
         17       0.78      0.60      0.68       376
         18       0.38      0.38      0.38       310
         19       0.32      0.32      0.32       251

avg / total       0.55      0.55      0.55      7532

Random Forest
Random forests or random decision forests technique is an ensemble learning method for text classification. This method is introduced by T. Kam Ho in 1995 for first time which used t tree as parallel. This technique is developed by L. Breiman in 1999 that they find converge for RF as margin measure.

from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn import metrics
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.datasets import fetch_20newsgroups

newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', RandomForestClassifier(n_estimators=100)),
                     ])

text_clf.fit(X_train, y_train)


predicted = text_clf.predict(X_test)

print(metrics.classification_report(y_test, predicted))
output:
                precision    recall  f1-score   support

          0       0.69      0.63      0.66       319
          1       0.56      0.69      0.62       389
          2       0.67      0.78      0.72       394
          3       0.67      0.67      0.67       392
          4       0.71      0.78      0.74       385
          5       0.78      0.68      0.73       395
          6       0.74      0.92      0.82       390
          7       0.81      0.79      0.80       396
          8       0.90      0.89      0.90       398
          9       0.80      0.89      0.84       397
         10       0.90      0.93      0.91       399
         11       0.89      0.91      0.90       396
         12       0.68      0.49      0.57       393
         13       0.83      0.65      0.73       396
         14       0.81      0.88      0.84       394
         15       0.68      0.91      0.78       398
         16       0.67      0.86      0.75       364
         17       0.93      0.78      0.85       376
         18       0.86      0.48      0.61       310
         19       0.79      0.31      0.45       251

avg / total       0.77      0.76      0.75      7532

Conditional Random Field (CRF)
Conditional Random Field (CRF) is an undirected graphical model as shown in figure. CRFs state the conditional probability of a label sequence Y give a sequence of observation X i.e. P(Y|X). CRFs can incorporate complex features of observation sequence without violating the independence assumption by modeling the conditional probability of the label sequence rather than the joint probability P(X,Y). The concept of clique which is a fully connected subgraph and clique potential are used for computing P(X|Y). Considering one potential function for each clique of the graph, the probability of a variable configuration is corresponding to the product of a series of non-negative potential function. The value computed by each potential function is equivalent to the probability of the variables in its corresponding clique taken on a particular configuration.

Example from Here
Let’s use CoNLL 2002 data to build a NER system
CoNLL2002 corpus is available in NLTK. We use Spanish data.
import nltk
import sklearn_crfsuite
from sklearn_crfsuite import metrics
nltk.corpus.conll2002.fileids()
train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))
test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))
sklearn-crfsuite (and python-crfsuite) supports several feature formats; here we use feature dicts.
def word2features(sent, i):
    word = sent[i][0]
    postag = sent[i][1]

    features = {
        'bias': 1.0,
        'word.lower()': word.lower(),
        'word[-3:]': word[-3:],
        'word[-2:]': word[-2:],
        'word.isupper()': word.isupper(),
        'word.istitle()': word.istitle(),
        'word.isdigit()': word.isdigit(),
        'postag': postag,
        'postag[:2]': postag[:2],
    }
    if i > 0:
        word1 = sent[i-1][0]
        postag1 = sent[i-1][1]
        features.update({
            '-1:word.lower()': word1.lower(),
            '-1:word.istitle()': word1.istitle(),
            '-1:word.isupper()': word1.isupper(),
            '-1:postag': postag1,
            '-1:postag[:2]': postag1[:2],
        })
    else:
        features['BOS'] = True

    if i < len(sent)-1:
        word1 = sent[i+1][0]
        postag1 = sent[i+1][1]
        features.update({
            '+1:word.lower()': word1.lower(),
            '+1:word.istitle()': word1.istitle(),
            '+1:word.isupper()': word1.isupper(),
            '+1:postag': postag1,
            '+1:postag[:2]': postag1[:2],
        })
    else:
        features['EOS'] = True

    return features


def sent2features(sent):
    return [word2features(sent, i) for i in range(len(sent))]

def sent2labels(sent):
    return [label for token, postag, label in sent]

def sent2tokens(sent):
    return [token for token, postag, label in sent]

X_train = [sent2features(s) for s in train_sents]
y_train = [sent2labels(s) for s in train_sents]

X_test = [sent2features(s) for s in test_sents]
y_test = [sent2labels(s) for s in test_sents]
To see all possible CRF parameters check its docstring. Here we are useing L-BFGS training algorithm (it is default) with Elastic Net (L1 + L2) regularization.
crf = sklearn_crfsuite.CRF(
    algorithm='lbfgs',
    c1=0.1,
    c2=0.1,
    max_iterations=100,
    all_possible_transitions=True
)
crf.fit(X_train, y_train)
Evaluation
y_pred = crf.predict(X_test)
print(metrics.flat_classification_report(
    y_test, y_pred,  digits=3
))
Output:
               precision    recall  f1-score   support

      B-LOC      0.810     0.784     0.797      1084
     B-MISC      0.731     0.569     0.640       339
      B-ORG      0.807     0.832     0.820      1400
      B-PER      0.850     0.884     0.867       735
      I-LOC      0.690     0.637     0.662       325
     I-MISC      0.699     0.589     0.639       557
      I-ORG      0.852     0.786     0.818      1104
      I-PER      0.893     0.943     0.917       634
          O      0.992     0.997     0.994     45355

avg / total      0.970     0.971     0.971     51533

Deep Learning

Deep Neural Networks
Deep Neural Networks' architecture is designed to learn by multi connection of layers that each single layer only receives connection from previous and provides connections only to the next layer in hidden part. The input is a connection of feature space (As discussed in Section Feature_extraction with first hidden layer. For Deep Neural Networks (DNN), input layer could be tf-ifd, word embedding, or etc. as shown in standard DNN in Figure. The output layer is number of classes for multi-class classification and only one output for binary classification. But our main contribution of this paper is that we have many training DNN for different purposes. In our techniques, we have multi-classes DNNs which each learning models is generated randomly (number of nodes in each layer and also number of layers are completely random assigned). Our implementation of Deep Neural Networks (DNN) is discriminative trained model that uses standard back-propagation algorithm using sigmoid or ReLU as activation function. The output layer for multi-class classification, should use Softmax.

import packages:
from sklearn.datasets import fetch_20newsgroups
from keras.layers import  Dropout, Dense
from keras.models import Sequential
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from sklearn import metrics
convert text to TF-IDF:
def TFIDF(X_train, X_test,MAX_NB_WORDS=75000):
    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)
    X_train = vectorizer_x.fit_transform(X_train).toarray()
    X_test = vectorizer_x.transform(X_test).toarray()
    print(""tf-idf with"",str(np.array(X_train).shape[1]),""features"")
    return (X_train,X_test)
Build a DNN Model for Text:
def Build_Model_DNN_Text(shape, nClasses, dropout=0.5):
    """"""
    buildModel_DNN_Tex(shape, nClasses,dropout)
    Build Deep neural networks Model for text classification
    Shape is input feature space
    nClasses is number of classes
    """"""
    model = Sequential()
    node = 512 # number of nodes
    nLayers = 4 # number of  hidden layer

    model.add(Dense(node,input_dim=shape,activation='relu'))
    model.add(Dropout(dropout))
    for i in range(0,nLayers):
        model.add(Dense(node,input_dim=node,activation='relu'))
        model.add(Dropout(dropout))
    model.add(Dense(nClasses, activation='softmax'))

    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])

    return model
Load text dataset (20newsgroups):
newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target
run DNN and see our result:
X_train_tfidf,X_test_tfidf = TFIDF(X_train,X_test)
model_DNN = Build_Model_DNN_Text(X_train_tfidf.shape[1], 20)
model_DNN.fit(X_train_tfidf, y_train,
                              validation_data=(X_test_tfidf, y_test),
                              epochs=10,
                              batch_size=128,
                              verbose=2)

predicted = model_DNN.predict(X_test_tfidf)

print(metrics.classification_report(y_test, predicted))
Model summary:
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              (None, 512)               38400512
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656
_________________________________________________________________
dropout_3 (Dropout)          (None, 512)               0
_________________________________________________________________
dense_4 (Dense)              (None, 512)               262656
_________________________________________________________________
dropout_4 (Dropout)          (None, 512)               0
_________________________________________________________________
dense_5 (Dense)              (None, 512)               262656
_________________________________________________________________
dropout_5 (Dropout)          (None, 512)               0
_________________________________________________________________
dense_6 (Dense)              (None, 20)                10260
=================================================================
Total params: 39,461,396
Trainable params: 39,461,396
Non-trainable params: 0
_________________________________________________________________
Output:
Train on 11314 samples, validate on 7532 samples
Epoch 1/10
 - 16s - loss: 2.7553 - acc: 0.1090 - val_loss: 1.9330 - val_acc: 0.3184
Epoch 2/10
 - 15s - loss: 1.5330 - acc: 0.4222 - val_loss: 1.1546 - val_acc: 0.6204
Epoch 3/10
 - 15s - loss: 0.7438 - acc: 0.7257 - val_loss: 0.8405 - val_acc: 0.7499
Epoch 4/10
 - 15s - loss: 0.2967 - acc: 0.9020 - val_loss: 0.9214 - val_acc: 0.7767
Epoch 5/10
 - 15s - loss: 0.1557 - acc: 0.9543 - val_loss: 0.8965 - val_acc: 0.7917
Epoch 6/10
 - 15s - loss: 0.1015 - acc: 0.9705 - val_loss: 0.9427 - val_acc: 0.7949
Epoch 7/10
 - 15s - loss: 0.0595 - acc: 0.9835 - val_loss: 0.9893 - val_acc: 0.7995
Epoch 8/10
 - 15s - loss: 0.0495 - acc: 0.9866 - val_loss: 0.9512 - val_acc: 0.8079
Epoch 9/10
 - 15s - loss: 0.0437 - acc: 0.9867 - val_loss: 0.9690 - val_acc: 0.8117
Epoch 10/10
 - 15s - loss: 0.0443 - acc: 0.9880 - val_loss: 1.0004 - val_acc: 0.8070


               precision    recall  f1-score   support

          0       0.76      0.78      0.77       319
          1       0.67      0.80      0.73       389
          2       0.82      0.63      0.71       394
          3       0.76      0.69      0.72       392
          4       0.65      0.86      0.74       385
          5       0.84      0.75      0.79       395
          6       0.82      0.87      0.84       390
          7       0.86      0.90      0.88       396
          8       0.95      0.91      0.93       398
          9       0.91      0.92      0.92       397
         10       0.98      0.92      0.95       399
         11       0.96      0.85      0.90       396
         12       0.71      0.69      0.70       393
         13       0.95      0.70      0.81       396
         14       0.86      0.91      0.88       394
         15       0.85      0.90      0.87       398
         16       0.79      0.84      0.81       364
         17       0.99      0.77      0.87       376
         18       0.58      0.75      0.65       310
         19       0.52      0.60      0.55       251

avg / total       0.82      0.81      0.81      7532

Recurrent Neural Networks (RNN)

Another neural network architecture that addressed with researchers for text miming and classification is Recurrent Neural Networks (RNN). RNN assigns more weights to the previous data points of sequence. Therefore, this technique is a powerful method for text, string and sequential data classification. Moreover, this technique could be used for image classification as we did in this work. In RNN the neural net considers the information of previous nodes in a very sophisticated method which allows for better semantic analysis of structures of dataset.

Gated Recurrent Unit (GRU)
Gated Recurrent Unit (GRU) is a gating mechanism for RNN which was introduced by  J. Chung et al. and K.Cho et al.. GRU is a simplified variant of the LSTM architecture, but there are differences as follows: GRU contains two gates, a GRU does not possess internal memory (as shown in Figure; and finally, a second non-linearity is not applied (tanh in Figure).


Long Short-Term Memory (LSTM)
Long Short-Term Memory~(LSTM) was introduced by S. Hochreiter and J. Schmidhuber  and developed by many research scientists.
To deal with these problems Long Short-Term Memory (LSTM) is a special type of RNN that preserve long term dependency in a more effective way in comparison to the basic RNN. This is particularly useful to overcome vanishing gradient problem. Although LSTM has a chain-like structure similar to RNN, LSTM uses multiple gates to carefully regulate the amount of information that will be allowed into each node state. Figure shows the basic cell of a LSTM model.
import packages:
from keras.layers import Dropout, Dense, GRU, Embedding
from keras.models import Sequential
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from sklearn import metrics
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.datasets import fetch_20newsgroups
convert text to word embedding (Using GloVe):
def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):
    np.random.seed(7)
    text = np.concatenate((X_train, X_test), axis=0)
    text = np.array(text)
    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
    tokenizer.fit_on_texts(text)
    sequences = tokenizer.texts_to_sequences(text)
    word_index = tokenizer.word_index
    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
    print('Found %s unique tokens.' % len(word_index))
    indices = np.arange(text.shape[0])
    # np.random.shuffle(indices)
    text = text[indices]
    print(text.shape)
    X_train = text[0:len(X_train), ]
    X_test = text[len(X_train):, ]
    embeddings_index = {}
    f = open(""C:\\Users\\kamran\\Documents\\GitHub\\RMDL\\Examples\\Glove\\glove.6B.50d.txt"", encoding=""utf8"")
    for line in f:

        values = line.split()
        word = values[0]
        try:
            coefs = np.asarray(values[1:], dtype='float32')
        except:
            pass
        embeddings_index[word] = coefs
    f.close()
    print('Total %s word vectors.' % len(embeddings_index))
    return (X_train, X_test, word_index,embeddings_index)
Build a RNN Model for Text:
def Build_Model_RNN_Text(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):
    """"""
    def buildModel_RNN(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):
    word_index in word index ,
    embeddings_index is embeddings index, look at data_helper.py
    nClasses is number of classes,
    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences
    """"""

    model = Sequential()
    hidden_layer = 3
    gru_node = 32

    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))
    for word, i in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            # words not found in embedding index will be all-zeros.
            if len(embedding_matrix[i]) != len(embedding_vector):
                print(""could not broadcast input array from shape"", str(len(embedding_matrix[i])),
                      ""into shape"", str(len(embedding_vector)), "" Please make sure your""
                                                                "" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,"")
                exit(1)
            embedding_matrix[i] = embedding_vector
    model.add(Embedding(len(word_index) + 1,
                                EMBEDDING_DIM,
                                weights=[embedding_matrix],
                                input_length=MAX_SEQUENCE_LENGTH,
                                trainable=True))


    print(gru_node)
    for i in range(0,hidden_layer):
        model.add(GRU(gru_node,return_sequences=True, recurrent_dropout=0.2))
        model.add(Dropout(dropout))
    model.add(GRU(gru_node, recurrent_dropout=0.2))
    model.add(Dropout(dropout))
    model.add(Dense(256, activation='relu'))
    model.add(Dense(nclasses, activation='softmax'))


    model.compile(loss='sparse_categorical_crossentropy',
                      optimizer='adam',
                      metrics=['accuracy'])
    return model
run RNN and see our result:
newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)


model_RNN = Build_Model_RNN_Text(word_index,embeddings_index, 20)

model_RNN.fit(X_train_Glove, y_train,
                              validation_data=(X_test_Glove, y_test),
                              epochs=10,
                              batch_size=128,
                              verbose=2)

predicted = Build_Model_RNN_Text.predict_classes(X_test_Glove)

print(metrics.classification_report(y_test, predicted))
Model summary:
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding_1 (Embedding)      (None, 500, 50)           8960500
_________________________________________________________________
gru_1 (GRU)                  (None, 500, 256)          235776
_________________________________________________________________
dropout_1 (Dropout)          (None, 500, 256)          0
_________________________________________________________________
gru_2 (GRU)                  (None, 500, 256)          393984
_________________________________________________________________
dropout_2 (Dropout)          (None, 500, 256)          0
_________________________________________________________________
gru_3 (GRU)                  (None, 500, 256)          393984
_________________________________________________________________
dropout_3 (Dropout)          (None, 500, 256)          0
_________________________________________________________________
gru_4 (GRU)                  (None, 256)               393984
_________________________________________________________________
dense_1 (Dense)              (None, 20)                5140
=================================================================
Total params: 10,383,368
Trainable params: 10,383,368
Non-trainable params: 0
_________________________________________________________________
Output:
Train on 11314 samples, validate on 7532 samples
Epoch 1/20
 - 268s - loss: 2.5347 - acc: 0.1792 - val_loss: 2.2857 - val_acc: 0.2460
Epoch 2/20
 - 271s - loss: 1.6751 - acc: 0.3999 - val_loss: 1.4972 - val_acc: 0.4660
Epoch 3/20
 - 270s - loss: 1.0945 - acc: 0.6072 - val_loss: 1.3232 - val_acc: 0.5483
Epoch 4/20
 - 269s - loss: 0.7761 - acc: 0.7312 - val_loss: 1.1009 - val_acc: 0.6452
Epoch 5/20
 - 269s - loss: 0.5513 - acc: 0.8112 - val_loss: 1.0395 - val_acc: 0.6832
Epoch 6/20
 - 269s - loss: 0.3765 - acc: 0.8754 - val_loss: 0.9977 - val_acc: 0.7086
Epoch 7/20
 - 270s - loss: 0.2481 - acc: 0.9202 - val_loss: 1.0485 - val_acc: 0.7270
Epoch 8/20
 - 269s - loss: 0.1717 - acc: 0.9463 - val_loss: 1.0269 - val_acc: 0.7394
Epoch 9/20
 - 269s - loss: 0.1130 - acc: 0.9644 - val_loss: 1.1498 - val_acc: 0.7369
Epoch 10/20
 - 269s - loss: 0.0640 - acc: 0.9808 - val_loss: 1.1442 - val_acc: 0.7508
Epoch 11/20
 - 269s - loss: 0.0567 - acc: 0.9828 - val_loss: 1.2318 - val_acc: 0.7414
Epoch 12/20
 - 268s - loss: 0.0472 - acc: 0.9858 - val_loss: 1.2204 - val_acc: 0.7496
Epoch 13/20
 - 269s - loss: 0.0319 - acc: 0.9910 - val_loss: 1.1895 - val_acc: 0.7657
Epoch 14/20
 - 268s - loss: 0.0466 - acc: 0.9853 - val_loss: 1.2821 - val_acc: 0.7517
Epoch 15/20
 - 271s - loss: 0.0269 - acc: 0.9917 - val_loss: 1.2869 - val_acc: 0.7557
Epoch 16/20
 - 271s - loss: 0.0187 - acc: 0.9950 - val_loss: 1.3037 - val_acc: 0.7598
Epoch 17/20
 - 268s - loss: 0.0157 - acc: 0.9959 - val_loss: 1.2974 - val_acc: 0.7638
Epoch 18/20
 - 270s - loss: 0.0121 - acc: 0.9966 - val_loss: 1.3526 - val_acc: 0.7602
Epoch 19/20
 - 269s - loss: 0.0262 - acc: 0.9926 - val_loss: 1.4182 - val_acc: 0.7517
Epoch 20/20
 - 269s - loss: 0.0249 - acc: 0.9918 - val_loss: 1.3453 - val_acc: 0.7638


               precision    recall  f1-score   support

          0       0.71      0.71      0.71       319
          1       0.72      0.68      0.70       389
          2       0.76      0.62      0.69       394
          3       0.67      0.58      0.62       392
          4       0.68      0.67      0.68       385
          5       0.75      0.73      0.74       395
          6       0.82      0.74      0.78       390
          7       0.83      0.83      0.83       396
          8       0.81      0.90      0.86       398
          9       0.92      0.90      0.91       397
         10       0.91      0.94      0.93       399
         11       0.87      0.76      0.81       396
         12       0.57      0.70      0.63       393
         13       0.81      0.85      0.83       396
         14       0.74      0.93      0.82       394
         15       0.82      0.83      0.83       398
         16       0.74      0.78      0.76       364
         17       0.96      0.83      0.89       376
         18       0.64      0.60      0.62       310
         19       0.48      0.56      0.52       251

avg / total       0.77      0.76      0.76      7532

Convolutional Neural Networks (CNN)
One of the deep learning architectures is  Convolutional Neural Networks (CNN) that is employed for hierarchical document classification. Although originally built for image processing  with architecture similar to the visual cortex, CNN have also been effectively used for  text classification. In the basic CNN for image processing an image tensor is convolved with a set of kernels of size d by d. These convolution layers are called feature maps and these can be stacked to provide multiple filters on the input. To reduce the computational complexity CNN use pooling which reduces the size of the output from one layer to the next in the network. Different pooling techniques are used to reduce outputs while preserving important features.
The most common pooling method is max pooling where the maximum element is selected in the pooling window. In order to feed the pooled output from stacked featured maps to the next layer, the maps are flattened into one column. The final layers in a CNN are typically fully connected.
In general, during the back-propagation step of a convolutional neural network not only the weights are adjusted but also the feature detector filters. A potential problem of CNN used for text is the number of 'channels', Sigma (size of the feature space). This might be very large (e.g. 50K), for text but for images this is less of a problem (e.g. only 3 channels of RGB). This means the dimensionality of the CNN for text is very high.

import packages:
from keras.layers import Dropout, Dense,Input,Embedding,Flatten, MaxPooling1D, Conv1D
from keras.models import Sequential,Model
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from sklearn import metrics
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.datasets import fetch_20newsgroups
from keras.layers.merge import Concatenate
convert text to word embedding (Using GloVe):
def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):
    np.random.seed(7)
    text = np.concatenate((X_train, X_test), axis=0)
    text = np.array(text)
    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
    tokenizer.fit_on_texts(text)
    sequences = tokenizer.texts_to_sequences(text)
    word_index = tokenizer.word_index
    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
    print('Found %s unique tokens.' % len(word_index))
    indices = np.arange(text.shape[0])
    # np.random.shuffle(indices)
    text = text[indices]
    print(text.shape)
    X_train = text[0:len(X_train), ]
    X_test = text[len(X_train):, ]
    embeddings_index = {}
    f = open(""C:\\Users\\kamran\\Documents\\GitHub\\RMDL\\Examples\\Glove\\glove.6B.50d.txt"", encoding=""utf8"")
    for line in f:
        values = line.split()
        word = values[0]
        try:
            coefs = np.asarray(values[1:], dtype='float32')
        except:
            pass
        embeddings_index[word] = coefs
    f.close()
    print('Total %s word vectors.' % len(embeddings_index))
    return (X_train, X_test, word_index,embeddings_index)
Build a RNN Model for Text:
def Build_Model_CNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):

    """"""
        def buildModel_CNN(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):
        word_index in word index ,
        embeddings_index is embeddings index, look at data_helper.py
        nClasses is number of classes,
        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,
        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py
    """"""

    model = Sequential()
    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))
    for word, i in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            # words not found in embedding index will be all-zeros.
            if len(embedding_matrix[i]) !=len(embedding_vector):
                print(""could not broadcast input array from shape"",str(len(embedding_matrix[i])),
                                 ""into shape"",str(len(embedding_vector)),"" Please make sure your""
                                 "" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,"")
                exit(1)

            embedding_matrix[i] = embedding_vector

    embedding_layer = Embedding(len(word_index) + 1,
                                EMBEDDING_DIM,
                                weights=[embedding_matrix],
                                input_length=MAX_SEQUENCE_LENGTH,
                                trainable=True)

    # applying a more complex convolutional approach
    convs = []
    filter_sizes = []
    layer = 5
    print(""Filter  "",layer)
    for fl in range(0,layer):
        filter_sizes.append((fl+2))

    node = 128
    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
    embedded_sequences = embedding_layer(sequence_input)

    for fsz in filter_sizes:
        l_conv = Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)
        l_pool = MaxPooling1D(5)(l_conv)
        #l_pool = Dropout(0.25)(l_pool)
        convs.append(l_pool)

    l_merge = Concatenate(axis=1)(convs)
    l_cov1 = Conv1D(node, 5, activation='relu')(l_merge)
    l_cov1 = Dropout(dropout)(l_cov1)
    l_pool1 = MaxPooling1D(5)(l_cov1)
    l_cov2 = Conv1D(node, 5, activation='relu')(l_pool1)
    l_cov2 = Dropout(dropout)(l_cov2)
    l_pool2 = MaxPooling1D(30)(l_cov2)
    l_flat = Flatten()(l_pool2)
    l_dense = Dense(1024, activation='relu')(l_flat)
    l_dense = Dropout(dropout)(l_dense)
    l_dense = Dense(512, activation='relu')(l_dense)
    l_dense = Dropout(dropout)(l_dense)
    preds = Dense(nclasses, activation='softmax')(l_dense)
    model = Model(sequence_input, preds)

    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])



    return model
run RNN and see our result:
newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)


model_CNN = Build_Model_CNN_Text(word_index,embeddings_index, 20)


model_CNN.summary()

model_CNN.fit(X_train_Glove, y_train,
                              validation_data=(X_test_Glove, y_test),
                              epochs=15,
                              batch_size=128,
                              verbose=2)

predicted = model_CNN.predict(X_test_Glove)

predicted = np.argmax(predicted, axis=1)


print(metrics.classification_report(y_test, predicted))
Model:
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 500)          0
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 500, 50)      8960500     input_1[0][0]
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 499, 128)     12928       embedding_1[0][0]
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 498, 128)     19328       embedding_1[0][0]
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 497, 128)     25728       embedding_1[0][0]
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 496, 128)     32128       embedding_1[0][0]
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 495, 128)     38528       embedding_1[0][0]
__________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)  (None, 99, 128)      0           conv1d_1[0][0]
__________________________________________________________________________________________________
max_pooling1d_2 (MaxPooling1D)  (None, 99, 128)      0           conv1d_2[0][0]
__________________________________________________________________________________________________
max_pooling1d_3 (MaxPooling1D)  (None, 99, 128)      0           conv1d_3[0][0]
__________________________________________________________________________________________________
max_pooling1d_4 (MaxPooling1D)  (None, 99, 128)      0           conv1d_4[0][0]
__________________________________________________________________________________________________
max_pooling1d_5 (MaxPooling1D)  (None, 99, 128)      0           conv1d_5[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 495, 128)     0           max_pooling1d_1[0][0]
                                                                 max_pooling1d_2[0][0]
                                                                 max_pooling1d_3[0][0]
                                                                 max_pooling1d_4[0][0]
                                                                 max_pooling1d_5[0][0]
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 491, 128)     82048       concatenate_1[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 491, 128)     0           conv1d_6[0][0]
__________________________________________________________________________________________________
max_pooling1d_6 (MaxPooling1D)  (None, 98, 128)      0           dropout_1[0][0]
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 94, 128)      82048       max_pooling1d_6[0][0]
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 94, 128)      0           conv1d_7[0][0]
__________________________________________________________________________________________________
max_pooling1d_7 (MaxPooling1D)  (None, 3, 128)       0           dropout_2[0][0]
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 384)          0           max_pooling1d_7[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1024)         394240      flatten_1[0][0]
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 1024)         0           dense_1[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          524800      dropout_3[0][0]
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 512)          0           dense_2[0][0]
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 20)           10260       dropout_4[0][0]
==================================================================================================
Total params: 10,182,536
Trainable params: 10,182,536
Non-trainable params: 0
__________________________________________________________________________________________________
Output:
Train on 11314 samples, validate on 7532 samples
Epoch 1/15
 - 6s - loss: 2.9329 - acc: 0.0783 - val_loss: 2.7628 - val_acc: 0.1403
Epoch 2/15
 - 4s - loss: 2.2534 - acc: 0.2249 - val_loss: 2.1715 - val_acc: 0.4007
Epoch 3/15
 - 4s - loss: 1.5643 - acc: 0.4326 - val_loss: 1.7846 - val_acc: 0.5052
Epoch 4/15
 - 4s - loss: 1.1771 - acc: 0.5662 - val_loss: 1.4949 - val_acc: 0.6131
Epoch 5/15
 - 4s - loss: 0.8880 - acc: 0.6797 - val_loss: 1.3629 - val_acc: 0.6256
Epoch 6/15
 - 4s - loss: 0.6990 - acc: 0.7569 - val_loss: 1.2013 - val_acc: 0.6624
Epoch 7/15
 - 4s - loss: 0.5037 - acc: 0.8200 - val_loss: 1.0674 - val_acc: 0.6807
Epoch 8/15
 - 4s - loss: 0.4050 - acc: 0.8626 - val_loss: 1.0223 - val_acc: 0.6863
Epoch 9/15
 - 4s - loss: 0.2952 - acc: 0.8968 - val_loss: 0.9045 - val_acc: 0.7120
Epoch 10/15
 - 4s - loss: 0.2314 - acc: 0.9217 - val_loss: 0.8574 - val_acc: 0.7326
Epoch 11/15
 - 4s - loss: 0.1778 - acc: 0.9436 - val_loss: 0.8752 - val_acc: 0.7270
Epoch 12/15
 - 4s - loss: 0.1475 - acc: 0.9524 - val_loss: 0.8299 - val_acc: 0.7355
Epoch 13/15
 - 4s - loss: 0.1089 - acc: 0.9657 - val_loss: 0.8034 - val_acc: 0.7491
Epoch 14/15
 - 4s - loss: 0.1047 - acc: 0.9666 - val_loss: 0.8172 - val_acc: 0.7463
Epoch 15/15
 - 4s - loss: 0.0749 - acc: 0.9774 - val_loss: 0.8511 - val_acc: 0.7313


               precision    recall  f1-score   support

          0       0.75      0.61      0.67       319
          1       0.63      0.74      0.68       389
          2       0.74      0.54      0.62       394
          3       0.49      0.76      0.60       392
          4       0.60      0.70      0.64       385
          5       0.79      0.57      0.66       395
          6       0.73      0.76      0.74       390
          7       0.83      0.74      0.78       396
          8       0.86      0.88      0.87       398
          9       0.95      0.78      0.86       397
         10       0.93      0.93      0.93       399
         11       0.92      0.77      0.84       396
         12       0.55      0.72      0.62       393
         13       0.76      0.85      0.80       396
         14       0.86      0.83      0.84       394
         15       0.91      0.73      0.81       398
         16       0.75      0.65      0.70       364
         17       0.95      0.86      0.90       376
         18       0.60      0.49      0.54       310
         19       0.37      0.60      0.46       251

avg / total       0.76      0.73      0.74      7532

Hierarchical Attention Networks


Recurrent Convolutional Neural Networks (RCNN)
ecurrent Convolutional Neural Networks (RCNN) is used for text classification. The main idea of this technique is capturing contextual information with the recurrent structure and constructs the representation of text using a convolutional neural network. This architecture is a combination of RNN and CNN to use advantages of both technique in a model.
import packages:
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding
from keras.layers import GRU
from keras.layers import Conv1D, MaxPooling1D
from keras.datasets import imdb
from sklearn.datasets import fetch_20newsgroups
import numpy as np
from sklearn import metrics
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
Convert text to word embedding (Using GloVe):
def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):
    np.random.seed(7)
    text = np.concatenate((X_train, X_test), axis=0)
    text = np.array(text)
    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
    tokenizer.fit_on_texts(text)
    sequences = tokenizer.texts_to_sequences(text)
    word_index = tokenizer.word_index
    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
    print('Found %s unique tokens.' % len(word_index))
    indices = np.arange(text.shape[0])
    # np.random.shuffle(indices)
    text = text[indices]
    print(text.shape)
    X_train = text[0:len(X_train), ]
    X_test = text[len(X_train):, ]
    embeddings_index = {}
    f = open(""C:\\Users\\kamran\\Documents\\GitHub\\RMDL\\Examples\\Glove\\glove.6B.50d.txt"", encoding=""utf8"")
    for line in f:
        values = line.split()
        word = values[0]
        try:
            coefs = np.asarray(values[1:], dtype='float32')
        except:
            pass
        embeddings_index[word] = coefs
    f.close()
    print('Total %s word vectors.' % len(embeddings_index))
    return (X_train, X_test, word_index,embeddings_index)
def Build_Model_RCNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50):

    kernel_size = 2
    filters = 256
    pool_size = 2
    gru_node = 256

    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))
    for word, i in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            # words not found in embedding index will be all-zeros.
            if len(embedding_matrix[i]) !=len(embedding_vector):
                print(""could not broadcast input array from shape"",str(len(embedding_matrix[i])),
                                 ""into shape"",str(len(embedding_vector)),"" Please make sure your""
                                 "" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,"")
                exit(1)

            embedding_matrix[i] = embedding_vector



    model = Sequential()
    model.add(Embedding(len(word_index) + 1,
                                EMBEDDING_DIM,
                                weights=[embedding_matrix],
                                input_length=MAX_SEQUENCE_LENGTH,
                                trainable=True))
    model.add(Dropout(0.25))
    model.add(Conv1D(filters, kernel_size, activation='relu'))
    model.add(MaxPooling1D(pool_size=pool_size))
    model.add(Conv1D(filters, kernel_size, activation='relu'))
    model.add(MaxPooling1D(pool_size=pool_size))
    model.add(Conv1D(filters, kernel_size, activation='relu'))
    model.add(MaxPooling1D(pool_size=pool_size))
    model.add(Conv1D(filters, kernel_size, activation='relu'))
    model.add(MaxPooling1D(pool_size=pool_size))
    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))
    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))
    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))
    model.add(LSTM(gru_node, recurrent_dropout=0.2))
    model.add(Dense(1024,activation='relu'))
    model.add(Dense(nclasses))
    model.add(Activation('softmax'))

    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])

    return model
newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)
Run RCNN :
model_RCNN = Build_Model_CNN_Text(word_index,embeddings_index, 20)


model_RCNN.summary()

model_RCNN.fit(X_train_Glove, y_train,
                              validation_data=(X_test_Glove, y_test),
                              epochs=15,
                              batch_size=128,
                              verbose=2)

predicted = model_RCNN.predict(X_test_Glove)

predicted = np.argmax(predicted, axis=1)
print(metrics.classification_report(y_test, predicted))
summary of the model:
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding_1 (Embedding)      (None, 500, 50)           8960500
_________________________________________________________________
dropout_1 (Dropout)          (None, 500, 50)           0
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 499, 256)          25856
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 249, 256)          0
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 248, 256)          131328
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 124, 256)          0
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 123, 256)          131328
_________________________________________________________________
max_pooling1d_3 (MaxPooling1 (None, 61, 256)           0
_________________________________________________________________
conv1d_4 (Conv1D)            (None, 60, 256)           131328
_________________________________________________________________
max_pooling1d_4 (MaxPooling1 (None, 30, 256)           0
_________________________________________________________________
lstm_1 (LSTM)                (None, 30, 256)           525312
_________________________________________________________________
lstm_2 (LSTM)                (None, 30, 256)           525312
_________________________________________________________________
lstm_3 (LSTM)                (None, 30, 256)           525312
_________________________________________________________________
lstm_4 (LSTM)                (None, 256)               525312
_________________________________________________________________
dense_1 (Dense)              (None, 1024)              263168
_________________________________________________________________
dense_2 (Dense)              (None, 20)                20500
_________________________________________________________________
activation_1 (Activation)    (None, 20)                0
=================================================================
Total params: 11,765,256
Trainable params: 11,765,256
Non-trainable params: 0
_________________________________________________________________
Output:
Train on 11314 samples, validate on 7532 samples
Epoch 1/15
 - 28s - loss: 2.6624 - acc: 0.1081 - val_loss: 2.3012 - val_acc: 0.1753
Epoch 2/15
 - 22s - loss: 2.1142 - acc: 0.2224 - val_loss: 1.9168 - val_acc: 0.2669
Epoch 3/15
 - 22s - loss: 1.7465 - acc: 0.3290 - val_loss: 1.8257 - val_acc: 0.3412
Epoch 4/15
 - 22s - loss: 1.4730 - acc: 0.4356 - val_loss: 1.5433 - val_acc: 0.4436
Epoch 5/15
 - 22s - loss: 1.1800 - acc: 0.5556 - val_loss: 1.2973 - val_acc: 0.5467
Epoch 6/15
 - 22s - loss: 0.9910 - acc: 0.6281 - val_loss: 1.2530 - val_acc: 0.5797
Epoch 7/15
 - 22s - loss: 0.8581 - acc: 0.6854 - val_loss: 1.1522 - val_acc: 0.6281
Epoch 8/15
 - 22s - loss: 0.7058 - acc: 0.7428 - val_loss: 1.2385 - val_acc: 0.6033
Epoch 9/15
 - 22s - loss: 0.6792 - acc: 0.7515 - val_loss: 1.0200 - val_acc: 0.6775
Epoch 10/15
 - 22s - loss: 0.5782 - acc: 0.7948 - val_loss: 1.0961 - val_acc: 0.6577
Epoch 11/15
 - 23s - loss: 0.4674 - acc: 0.8341 - val_loss: 1.0866 - val_acc: 0.6924
Epoch 12/15
 - 23s - loss: 0.4284 - acc: 0.8512 - val_loss: 0.9880 - val_acc: 0.7096
Epoch 13/15
 - 22s - loss: 0.3883 - acc: 0.8670 - val_loss: 1.0190 - val_acc: 0.7151
Epoch 14/15
 - 22s - loss: 0.3334 - acc: 0.8874 - val_loss: 1.0025 - val_acc: 0.7232
Epoch 15/15
 - 22s - loss: 0.2857 - acc: 0.9038 - val_loss: 1.0123 - val_acc: 0.7331


             precision    recall  f1-score   support

          0       0.64      0.73      0.68       319
          1       0.45      0.83      0.58       389
          2       0.81      0.64      0.71       394
          3       0.64      0.57      0.61       392
          4       0.55      0.78      0.64       385
          5       0.77      0.52      0.62       395
          6       0.84      0.77      0.80       390
          7       0.87      0.79      0.83       396
          8       0.85      0.90      0.87       398
          9       0.98      0.84      0.90       397
         10       0.93      0.96      0.95       399
         11       0.92      0.79      0.85       396
         12       0.59      0.53      0.56       393
         13       0.82      0.82      0.82       396
         14       0.84      0.84      0.84       394
         15       0.83      0.89      0.86       398
         16       0.68      0.86      0.76       364
         17       0.97      0.86      0.91       376
         18       0.66      0.50      0.57       310
         19       0.53      0.31      0.40       251

avg / total       0.77      0.75      0.75      7532

Random Multimodel Deep Learning (RMDL)
Referenced paper : RMDL: Random Multimodel Deep Learning for
Classification
A new ensemble, deep learning approach for classification. Deep
learning models have achieved state-of-the-art results across many domains.
RMDL solves the problem of finding the best deep learning structure
and architecture while simultaneously improving robustness and accuracy
through ensembles of deep learning architectures. RDML can accept
asinput a variety data to include text, video, images, and symbolic.

Random Multimodel Deep Learning (RDML) architecture for classification.
RMDL includes 3 Random models, oneDNN classifier at left, one Deep CNN
classifier at middle, and one Deep RNN classifier at right (each unit could be LSTMor GRU).
Installation
There are pip and git for RMDL installation:
Using pip
pip install RMDL
Using git
git clone --recursive https://github.com/kk7nc/RMDL.git
The primary requirements for this package are Python 3 with Tensorflow. The requirements.txt file
contains a listing of the required Python packages; to install all requirements, run the following:
pip -r install requirements.txt
Or
pip3  install -r requirements.txt
Or:
conda install --file requirements.txt
Documentation:
The exponential growth in the number of complex datasets every year requires  more enhancement in
machine learning methods to provide  robust and accurate data classification. Lately, deep learning
approaches have been achieved surpassing results in comparison to previous machine learning algorithms
on tasks such as image classification, natural language processing, face recognition, and etc. The
success of these deep learning algorithms relys on their capacity to model complex and non-linear
relationships within data. However, finding the suitable structure for these models has been a challenge
for researchers. This paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble, deep learning
approach for classification.  RMDL solves the problem of finding the best deep learning structure and
architecture while simultaneously improving robustness and accuracy through ensembles of deep
learning architectures. In short, RMDL trains multiple models of Deep Neural Network (DNN),
Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) in parallel and combines
their results to produce better result of any of those models individually. To create these models,
each deep learning model has been constructed in a random fashion regarding the number of layers and
nodes in their neural network structure. The resulting RDML model can be used for various domains such
as text, video, images, and symbolic. In this Project, we describe RMDL model in depth and show the results
for image and text classification as well as face recognition. For image classification, we compared our
model with some of the available baselines using MNIST and CIFAR-10 datasets. Similarly, we used four
datasets namely, WOS, Reuters, IMDB, and 20newsgroup and compared our results with available baselines.
Web of Science (WOS) has been collected  by authors and consists of three sets~(small, medium and large set).
Lastly, we used ORL dataset to compare the performance of our approach with other face recognition methods.
These test results show that RDML model consistently outperform standard methods over a broad range of
data types and classification problems.

Hierarchical Deep Learning for Text (HDLTex)
Refrenced paper : HDLTex: Hierarchical Deep Learning for Text
Classification

Documentation:
Increasingly large document collections require improved information processing methods for searching, retrieving, and organizing  text. Central to these information processing methods is document classification, which has become an important application for supervised learning. Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased. This is because along with growth in the number of documents has come an increase in the number of categories. This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification. Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification (HDLTex). HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy.

Evaluation


F1 Score


Matthew correlation coefficient (MCC)
Compute the Matthews correlation coefficient (MCC)
The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient.
from sklearn.metrics import matthews_corrcoef
y_true = [+1, +1, +1, -1]
y_pred = [+1, -1, +1, +1]
matthews_corrcoef(y_true, y_pred)

Receiver operating characteristics (ROC)
ROC curves are typically used in binary classification to study the output of a classifier. In order to extend ROC curve and ROC area to multi-class or multi-label classification, it is necessary to binarize the output. One ROC curve can be drawn per label, but one can also draw a ROC curve by considering each element of the label indicator matrix as a binary prediction (micro-averaging).
Another evaluation measure for multi-class classification is macro-averaging, which gives equal weight to the classification of each label. [sources]
import numpy as np
import matplotlib.pyplot as plt
from itertools import cycle

from sklearn import svm, datasets
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from scipy import interp

# Import some data to play with
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Binarize the output
y = label_binarize(y, classes=[0, 1, 2])
n_classes = y.shape[1]

# Add noisy features to make the problem harder
random_state = np.random.RandomState(0)
n_samples, n_features = X.shape
X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]

# shuffle and split training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,
                                                    random_state=0)

# Learn to predict each class against the other
classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,
                                 random_state=random_state))
y_score = classifier.fit(X_train, y_train).decision_function(X_test)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr[""micro""], tpr[""micro""], _ = roc_curve(y_test.ravel(), y_score.ravel())
roc_auc[""micro""] = auc(fpr[""micro""], tpr[""micro""])
Plot of a ROC curve for a specific class
plt.figure()
lw = 2
plt.plot(fpr[2], tpr[2], color='darkorange',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc=""lower right"")
plt.show()


Area Under Curve (AUC)
Area  under  ROC  curve  (AUC)  as  a  summarymetric measures the entire area underneath the ROC curve. AUC  holds  helpful  properties  such  as  increased  sensitivityin analysis of variance (ANOVA) tests, being independent ofdecision threshold, being invariant toa prioriclass probabili-ties and indicating how well negative and positive classes areregarding decision index.
import numpy as np
from sklearn import metrics
fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)
metrics.auc(fpr, tpr)

Text and Document Datasets


IMDB

IMDB Dataset

Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer ""3"" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: ""only consider the top 10,000 most common words, but eliminate the top 20 most common words"".
As a convention, ""0"" does not stand for a specific word, but instead is used to encode any unknown word.
from keras.datasets import imdb

(x_train, y_train), (x_test, y_test) = imdb.load_data(path=""imdb.npz"",
                                                      num_words=None,
                                                      skip_top=0,
                                                      maxlen=None,
                                                      seed=113,
                                                      start_char=1,
                                                      oov_char=2,
                                                      index_from=3)

Reuters-21578

Reters-21578 Dataset

Dataset of 11,228 newswires from Reuters, labeled over 46 topics. As with the IMDB dataset, each wire is encoded as a sequence of word indexes (same conventions).
from keras.datasets import reuters

(x_train, y_train), (x_test, y_test) = reuters.load_data(path=""reuters.npz"",
                                                         num_words=None,
                                                         skip_top=0,
                                                         maxlen=None,
                                                         test_split=0.2,
                                                         seed=113,
                                                         start_char=1,
                                                         oov_char=2,
                                                         index_from=3)

20Newsgroups

20Newsgroups Dataset

The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.
This module contains two loaders. The first one, sklearn.datasets.fetch_20newsgroups, returns a list of the raw texts that can be fed to text feature extractors such as sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract feature vectors. The second one, sklearn.datasets.fetch_20newsgroups_vectorized, returns ready-to-use features, i.e., it is not necessary to use a feature extractor.
from sklearn.datasets import fetch_20newsgroups
newsgroups_train = fetch_20newsgroups(subset='train')

from pprint import pprint
pprint(list(newsgroups_train.target_names))

['alt.atheism',
 'comp.graphics',
 'comp.os.ms-windows.misc',
 'comp.sys.ibm.pc.hardware',
 'comp.sys.mac.hardware',
 'comp.windows.x',
 'misc.forsale',
 'rec.autos',
 'rec.motorcycles',
 'rec.sport.baseball',
 'rec.sport.hockey',
 'sci.crypt',
 'sci.electronics',
 'sci.med',
 'sci.space',
 'soc.religion.christian',
 'talk.politics.guns',
 'talk.politics.mideast',
 'talk.politics.misc',
 'talk.religion.misc']

Web of Science Dataset
Description of Dataset:
Here is three datasets which include WOS-11967 , WOS-46985, and WOS-5736
Each folder contains:

X.txt
Y.txt
YL1.txt
YL2.txt

X is input data that include text sequences
Y is target value
YL1 is target value of level one (parent label)
YL2 is target value of level one (child label)
Meta-data:
This folder contain on data file as following attribute:
Y1 Y2 Y Domain area keywords Abstract
Abstract is input data that include text sequences of 46,985 published paper
Y is target value
YL1 is target value of level one (parent label)
YL2 is target value of level one (child label)
Domain is majaor domain which include 7 labales: {Computer Science,Electrical Engineering, Psychology, Mechanical Engineering,Civil Engineering, Medical Science, biochemistry}
area is subdomain or area of the paper such as CS-> computer graphics which contain 134 labels.
keywords : is authors keyword of the papers

Web of Science Dataset WOS-11967


This dataset contains 11,967 documents with 35 categories which include 7 parents categories.

Web of Science Dataset WOS-46985


This dataset contains 46,985 documents with 134 categories which include 7 parents categories.

Web of Science Dataset WOS-5736


This dataset contains 5,736 documents with 11 categories which include 3 parents categories.
Referenced paper: HDLTex: Hierarchical Deep Learning for Text Classification

Citations:

@ARTICLE{Kowsari2018Text_Classification,
    title={Text Classification Algorithms: A Survey},
    author={Kowsari, Kamran and Jafari Meimandi, Kiana and Heidarysafa, Mojtaba and Mendu, Sanjana and Barnes, Laura E. and Brown, Donald E.},
    journal={Information},
    VOLUME = {10},
    YEAR = {2019},
    NUMBER = {4},
    ARTICLE-NUMBER = {150},
    URL = {http://www.mdpi.com/2078-2489/10/4/150},
    ISSN = {2078-2489},
    publisher={Multidisciplinary Digital Publishing Institute}
}

",361
jdc20181/BeffsBrowser,Visual Basic,"Announcments
Krypto Browser
Introducing Krypto, a new version of BeffsBrowser, coming very soon!
https://github.com/jdc20181/Krypto
Notice:
Development is suspended for approximately 5-12 weeks, as the visual studio file became corrupt and requires a rebuild. 2.4.3.1 has been post poned until further notice.
StartPage is discontinued
StartPage is being removed. It will be relaunched soon, and locally stored.
Changes to Compatible OS is coming soon
Read some of the changes: wiki

About
A Simple but powerful Browser using the Gecko Webbrowser engine!
License
Licensed under The MIT License!
System Requirements!
See System Requirements
Archive Policy
A new Archive Policy has been published in the wiki, you can read it here
Simply Said Summary
The new policy will reflect on effeciency in the open source community, providing a constructive policy pointing out what happens when you don't update. Support for older versions are removed every so often (everything is said in the wiki) - making outdated versions discontinued.
In short, you should keep your application up to date with the latest version to avoid missing bug fixes and resolving issues which you may post about well after they are resolved with the next release.
",3
snowleopard/selective,TeX,"Selective applicative functors
  
This is a library for selective applicative functors, or just selective functors
for short, an abstraction between applicative functors and monads, introduced in
this paper.
Abstract of the paper:
Applicative functors and monads have conquered the world of functional programming by
providing general and powerful ways of describing effectful computations using pure
functions. Applicative functors provide a way to compose independent effects that
cannot depend on values produced by earlier computations, and all of which are declared
statically. Monads extend the applicative interface by making it possible to compose
dependent effects, where the value computed by one effect determines all subsequent
effects, dynamically.
This paper introduces an intermediate abstraction called selective applicative functors
that requires all effects to be declared statically, but provides a way to select which
of the effects to execute dynamically. We demonstrate applications of the new
abstraction on several examples, including two real-life case studies.
What are selective functors?
While you're encouraged to read the paper, here is a brief description of
the main idea. Consider the following new type class introduced between
Applicative and Monad:
class Applicative f => Selective f where
    select :: f (Either a b) -> f (a -> b) -> f b

-- | An operator alias for 'select'.
(<*?) :: Selective f => f (Either a b) -> f (a -> b) -> f b
(<*?) = select

infixl 4 <*?
Think of select as a selective function application: you must apply the function
of type a -> b when given a value of type Left a, but you may skip the
function and associated effects, and simply return b when given Right b.
Note that you can write a function with this type signature using
Applicative functors, but it will always execute the effects associated
with the second argument, hence being potentially less efficient:
selectA :: Applicative f => f (Either a b) -> f (a -> b) -> f b
selectA x f = (\e f -> either f id e) <$> x <*> f
Any Applicative instance can thus be given a corresponding Selective
instance simply by defining select = selectA. The opposite is also true
in the sense that one can recover the operator <*> from select as
follows (I'll use the suffix S to denote Selective equivalents of
commonly known functions).
apS :: Selective f => f (a -> b) -> f a -> f b
apS f x = select (Left <$> f) (flip ($) <$> x)
Here we wrap a given function a -> b into Left and turn the value a
into a function ($a), which simply feeds itself to the function a -> b
yielding b as desired. Note: apS is a perfectly legal
application operator <*>, i.e. it satisfies the laws dictated by the
Applicative type class as long as the laws of the Selective
type class hold.
The branch function is a natural generalisation of select: instead of
skipping an unnecessary effect, it chooses which of the two given effectful
functions to apply to a given argument; the other effect is unnecessary. It
is possible to implement branch in terms of select, which is a good
puzzle (give it a try!).
branch :: Selective f => f (Either a b) -> f (a -> c) -> f (b -> c) -> f c
branch = ... -- Try to figure out the implementation!
Finally, any Monad is Selective:
selectM :: Monad f => f (Either a b) -> f (a -> b) -> f b
selectM mx mf = do
    x <- mx
    case x of
        Left  a -> fmap ($a) mf
        Right b -> pure b
Selective functors are sufficient for implementing many conditional constructs,
which traditionally require the (more powerful) Monad type class. For example:
-- | Branch on a Boolean value, skipping unnecessary effects.
ifS :: Selective f => f Bool -> f a -> f a -> f a
ifS i t e = branch (bool (Right ()) (Left ()) <$> i) (const <$> t) (const <$> e)

-- | Conditionally perform an effect.
whenS :: Selective f => f Bool -> f () -> f ()
whenS x act = ifS x act (pure ())

-- | Keep checking an effectful condition while it holds.
whileS :: Selective f => f Bool -> f ()
whileS act = whenS act (whileS act)

-- | A lifted version of lazy Boolean OR.
(<||>) :: Selective f => f Bool -> f Bool -> f Bool
(<||>) a b = ifS a (pure True) b

-- | A lifted version of 'any'. Retains the short-circuiting behaviour.
anyS :: Selective f => (a -> f Bool) -> [a] -> f Bool
anyS p = foldr ((<||>) . p) (pure False)

-- | Return the first @Right@ value. If both are @Left@'s, accumulate errors.
orElse :: (Selective f, Semigroup e) => f (Either e a) -> f (Either e a) -> f (Either e a)
orElse x = select (Right <$> x) . fmap (\y e -> first (e <>) y)
See more examples in src/Control/Selective.hs.
Code written using selective combinators can be both statically analysed
(by reporting all possible effects of a computation) and efficiently
executed (by skipping unnecessary effects).
Laws
Instances of the Selective type class must satisfy a few laws to make
it possible to refactor selective computations. These laws also allow us
to establish a formal relation with the Applicative and Monad type
classes.


Identity:
x <*? pure id = either id id <$> x


Distributivity (note that y and z have the same type f (a -> b)):
pure x <*? (y *> z) = (pure x <*? y) *> (pure x <*? z)


Associativity:
x <*? (y <*? z) = (f <$> x) <*? (g <$> y) <*? (h <$> z)
  where
    f x = Right <$> x
    g y = \a -> bimap (,a) ($a) y
    h z = uncurry z


Monadic select (for selective functors that are also monads):
select = selectM


There are also a few useful theorems:


Apply a pure function to the result:
f <$> select x y = select (fmap f <$> x) (fmap f <$> y)


Apply a pure function to the Left case of the first argument:
select (first f <$> x) y = select x ((. f) <$> y)


Apply a pure function to the second argument:
select x (f <$> y) = select (first (flip f) <$> x) (flip ($) <$> y)


Generalised identity:
x <*? pure y = either y id <$> x


A selective functor is rigid if it satisfies <*> = apS. The following
interchange law holds for rigid selective functors:
x *> (y <*? z) = (x *> y) <*? z


Note that there are no laws for selective application of a function to a pure
Left or Right value, i.e. we do not require that the following laws hold:
select (pure (Left  x)) y = ($x) <$> y -- Pure-Left
select (pure (Right x)) y = pure x     -- Pure-Right
In particular, the following is allowed too:
select (pure (Left  x)) y = pure ()       -- when y :: f (a -> ())
select (pure (Right x)) y = const x <$> y
We therefore allow select to be selective about effects in these cases, which
in practice allows to under- or over-approximate possible effects in static
analysis using instances like Under and Over.
If f is also a Monad, we require that select = selectM, from which one
can prove apS = <*>, and furthermore the above Pure-Left and Pure-Right
properties now hold.
Static analysis of selective functors
Like applicative functors, selective functors can be analysed statically.
We can make the Const functor an instance of Selective as follows.
instance Monoid m => Selective (Const m) where
    select = selectA
Although we don't need the function Const m (a -> b) (note that
Const m (Either a b) holds no values of type a), we choose to
accumulate the effects associated with it. This allows us to extract
the static structure of any selective computation very similarly
to how this is done with applicative computations.
The Validation instance is perhaps a bit more interesting.
data Validation e a = Failure e | Success a deriving (Functor, Show)

instance Semigroup e => Applicative (Validation e) where
    pure = Success
    Failure e1 <*> Failure e2 = Failure (e1 <> e2)
    Failure e1 <*> Success _  = Failure e1
    Success _  <*> Failure e2 = Failure e2
    Success f  <*> Success a  = Success (f a)

instance Semigroup e => Selective (Validation e) where
    select (Success (Right b)) _ = Success b
    select (Success (Left  a)) f = Success ($a) <*> f
    select (Failure e        ) _ = Failure e
Here, the last line is particularly interesting: unlike the Const
instance, we choose to actually skip the function effect in case of
Failure. This allows us not to report any validation errors which
are hidden behind a failed conditional.
Let's clarify this with an example. Here we define a function to
construct a Shape (a circle or a rectangle) given a choice of the
shape s and the shape's parameters (r, w, h) in a selective
context f.
type Radius = Int
type Width  = Int
type Height = Int

data Shape = Circle Radius | Rectangle Width Height deriving Show

shape :: Selective f => f Bool -> f Radius -> f Width -> f Height -> f Shape
shape s r w h = ifS s (Circle <$> r) (Rectangle <$> w <*> h)
We choose f = Validation [String] to report the errors that occurred
when parsing a value. Let's see how it works.
> shape (Success True) (Success 10) (Failure [""no width""]) (Failure [""no height""])
Success (Circle 10)

> shape (Success False) (Failure [""no radius""]) (Success 20) (Success 30)
Success (Rectangle 20 30)

> shape (Success False) (Failure [""no radius""]) (Success 20) (Failure [""no height""])
Failure [""no height""]

> shape (Success False) (Failure [""no radius""]) (Failure [""no width""]) (Failure [""no height""])
Failure [""no width"",""no height""]

> shape (Failure [""no choice""]) (Failure [""no radius""]) (Success 20) (Failure [""no height""])
Failure [""no choice""]
In the last example, since we failed to parse which shape has been chosen,
we do not report any subsequent errors. But it doesn't mean we are short-circuiting
the validation. We will continue accumulating errors as soon as we get out of the
opaque conditional, as demonstrated below.
twoShapes :: Selective f => f Shape -> f Shape -> f (Shape, Shape)
twoShapes s1 s2 = (,) <$> s1 <*> s2

> s1 = shape (Failure [""no choice 1""]) (Failure [""no radius 1""]) (Success 20) (Failure [""no height 1""])
> s2 = shape (Success False) (Failure [""no radius 2""]) (Success 20) (Failure [""no height 2""])
> twoShapes s1 s2
Failure [""no choice 1"",""no height 2""]
Do we still need monads?
Yes! Here is what selective functors cannot do: join :: Selective f => f (f a) -> f a.
Further reading

A paper introducing selective functors: https://www.staff.ncl.ac.uk/andrey.mokhov/selective-functors.pdf.
An older blog post introducing selective functors: https://blogs.ncl.ac.uk/andreymokhov/selective.

",127
RT-Thread/rt-thread,C,"RT-Thread
中文页 |






RT-Thread is an open source IoT operating system from China, which has strong scalability: from a tiny kernel running on a tiny core, for example ARM Cortex-M0, or Cortex-M3/4/7, to a rich feature system running on MIPS32, ARM Cortex-A8, ARM Cortex-A9 DualCore etc.
Overview
RT-Thread RTOS like a traditional real-time operating system. The kernel has real-time multi-task scheduling, semaphore, mutex, mail box, message queue, signal etc. However, it has three different things:

Device Driver;
Component;
Dynamic Module

The device driver is more like a driver framework, UART, IIC, SPI, SDIO, USB device/host, EMAC, MTD NAND etc. The developer can easily add low level driver and board configuration, then combined with the upper framework, he/she can use lots of features.
The Component is a software concept upon RT-Thread kernel, for example a shell (finsh/msh shell), virtual file system (FAT, YAFFS, UFFS, ROM/RAM file system etc), TCP/IP protocol stack (lwIP), POSIX (thread) interface etc. One component must be a directory under RT-Thread/Components and one component can be descripted by a SConscript file (then be compiled and linked into the system).
The Dynamic Module, formerly named as User Applicaion (UA) is a dynamic loaded module or library, it can be compiled standalone without Kernel. Each Dynamic Module has its own object list to manage thread/semaphore/kernel object which was created or initialized inside this UA. More information about UA, please visit another git repo.
Board Support Package
RT-Thread RTOS can support many architectures:

ARM Cortex-M0
ARM Cortex-M3/M4/7
ARM Cortex-R4
ARM Cortex-A8/A9
ARM920T/ARM926 etc
MIPS32
x86
Andes
C-Sky
RISC-V
PowerPC

License
RT-Thread is Open Source software under the Apache License 2.0 since RT-Thread v3.1.1. License and copyright information can be found within the code.
/*
 * Copyright (c) 2006-2018, RT-Thread Development Team
 *
 * SPDX-License-Identifier: Apache-2.0
 */

Since 9th of September 2018, PRs submitted by the community may be merged into the main line only after signing the Contributor License Agreement(CLA).
Usage
RT-Thread RTOS uses scons as building system. Therefore, please install scons and Python 2.7 firstly.
So far, the RT-Thread scons building system support the command line compile or generate some IDE's project. There are some option varaibles in the scons building script (rtconfig.py):

CROSS_TOOL the compiler which you want to use, gcc/keil/iar.
EXEC_PATH the path of compiler.

In SConstruct file:
RTT_ROOT This variable is the root directory of RT-Thread RTOS. If you build the porting in the bsp directory, you can use the default setting. Also, you can set the root directory in RTT_ROOT environment variable and not modify SConstruct files.
When you set these variables correctly, you can use command:
scons

under BSP directory to simplely compile RT-Thread RTOS.
If you want to generate the IDE's project file, you can use command:
scons --target=mdk/mdk4/mdk5/iar/cb -s

to generate the project file.
NOTE: RT-Thread scons building system will tailor the system according to your rtconfig.h configuration header file. For example, if you disable the lwIP in the rtconfig.h by commenting the #define RT_USING_LWIP, the generated project file should have no lwIP related files.
Contribution
Please refer the contributors in the github. Thank all of RT-Thread Developers.
",2566
UdjinM6/dash-bootstrap,Python,"bootstrap.dat files for Dash Core
Usage

Download and extract one of the recent files linked below.
Place bootstrap.dat file inside of your Dash Core folder:


Windows: %APPDATA%\DashCore\
Mac OS: ~/Library/Application Support/DashCore/
Unix/Linux: ~/.dashcore/


Run your wallet and let it sync using bootstrap.dat
Once sync is done bootstrap.dat file will be renamed to bootstrap.dat.old automagically and can be safely removed.

NOTE: bootstrap.dat for testnet must be placed in testnet3 subfolder of corresponding Dash Core folder
Recent files
For mainnet:
Block 1071359: Fri May 17 00:00:01 UTC 2019 zip (6.8G) SHA256
Block 1070822: Thu May 16 00:00:01 UTC 2019 zip (6.7G) SHA256
Block 1070270: Wed May 15 00:00:01 UTC 2019 zip (6.7G) SHA256
Block 1069718: Tue May 14 00:00:01 UTC 2019 zip (6.7G) SHA256
Block 1069172: Mon May 13 00:00:01 UTC 2019 zip (6.7G) SHA256
Block 1068624: Sun May 12 00:00:01 UTC 2019 zip (6.7G) SHA256
For testnet:
Block 100005: Fri May 17 00:32:44 UTC 2019 zip (317M) SHA256
Block 99405: Thu May 16 00:32:54 UTC 2019 zip (316M) SHA256
Block 98680: Wed May 15 00:33:18 UTC 2019 zip (310M) SHA256
Block 98082: Tue May 14 00:34:34 UTC 2019 zip (309M) SHA256
Block 97453: Mon May 13 00:30:13 UTC 2019 zip (307M) SHA256
Block 96845: Sun May 12 00:32:47 UTC 2019 zip (304M) SHA256
Donations are welcome:
DASH: XsV4GHVKGTjQFvwB7c6mYsGV3Mxf7iser6
",31
Kraks/verifyo,Scheme,"Experiments with writing relational verifier/synthesizer in miniKanren.
",3
facebook/folly,C++,"Folly: Facebook Open-source Library

What is folly?
Folly (acronymed loosely after Facebook Open Source Library) is a
library of C++14 components designed with practicality and efficiency
in mind. Folly contains a variety of core library components used extensively
at Facebook. In particular, it's often a dependency of Facebook's other
open source C++ efforts and place where those projects can share code.
It complements (as opposed to competing against) offerings
such as Boost and of course std. In fact, we embark on defining our
own component only when something we need is either not available, or
does not meet the needed performance profile. We endeavor to remove
things from folly if or when std or Boost obsoletes them.
Performance concerns permeate much of Folly, sometimes leading to
designs that are more idiosyncratic than they would otherwise be (see
e.g. PackedSyncPtr.h, SmallLocks.h). Good performance at large
scale is a unifying theme in all of Folly.
Logical Design
Folly is a collection of relatively independent components, some as
simple as a few symbols. There is no restriction on internal
dependencies, meaning that a given folly module may use any other
folly components.
All symbols are defined in the top-level namespace folly, except of
course macros. Macro names are ALL_UPPERCASE and should be prefixed
with FOLLY_. Namespace folly defines other internal namespaces
such as internal or detail. User code should not depend on symbols
in those namespaces.
Folly has an experimental directory as well. This designation connotes
primarily that we feel the API may change heavily over time. This code,
typically, is still in heavy use and is well tested.
Physical Design
At the top level Folly uses the classic ""stuttering"" scheme
folly/folly used by Boost and others. The first directory serves as
an installation root of the library (with possible versioning a la
folly-1.0/), and the second is to distinguish the library when
including files, e.g. #include <folly/FBString.h>.
The directory structure is flat (mimicking the namespace structure),
i.e. we don't have an elaborate directory hierarchy (it is possible
this will change in future versions). The subdirectory experimental
contains files that are used inside folly and possibly at Facebook but
not considered stable enough for client use. Your code should not use
files in folly/experimental lest it may break when you update Folly.
The folly/folly/test subdirectory includes the unittests for all
components, usually named ComponentXyzTest.cpp for each
ComponentXyz.*. The folly/folly/docs directory contains
documentation.
What's in it?
Because of folly's fairly flat structure, the best way to see what's in it
is to look at the headers in top level folly/ directory. You can also
check the docs folder for documentation, starting with the
overview.
Folly is published on GitHub at https://github.com/facebook/folly
Build Notes
Dependencies
folly requires gcc 5.1+ and a version of boost compiled with C++14 support.
googletest is required to build and run folly's tests.  You can download
it from https://github.com/google/googletest/archive/release-1.8.0.tar.gz
The following commands can be used to download and install it:
wget https://github.com/google/googletest/archive/release-1.8.0.tar.gz && \
tar zxf release-1.8.0.tar.gz && \
rm -f release-1.8.0.tar.gz && \
cd googletest-release-1.8.0 && \
cmake . && \
make && \
make install

Finding dependencies in non-default locations
If you have boost, gtest, or other dependencies installed in a non-default
location, you can use the CMAKE_INCLUDE_PATH and CMAKE_LIBRARY_PATH
variables to make CMAKE look also look for header files and libraries in
non-standard locations.  For example, to also search the directories
/alt/include/path1 and /alt/include/path2 for header files and the
directories /alt/lib/path1 and /alt/lib/path2 for libraries, you can invoke
cmake as follows:
cmake \
  -DCMAKE_INCLUDE_PATH=/alt/include/path1:/alt/include/path2 \
  -DCMAKE_LIBRARY_PATH=/alt/lib/path1:/alt/lib/path2 ...

Ubuntu 16.04 LTS
The following packages are required (feel free to cut and paste the apt-get
command below):
sudo apt-get install \
    g++ \
    cmake \
    libboost-all-dev \
    libevent-dev \
    libdouble-conversion-dev \
    libgoogle-glog-dev \
    libgflags-dev \
    libiberty-dev \
    liblz4-dev \
    liblzma-dev \
    libsnappy-dev \
    make \
    zlib1g-dev \
    binutils-dev \
    libjemalloc-dev \
    libssl-dev \
    pkg-config

If advanced debugging functionality is required, use:
sudo apt-get install \
    libunwind8-dev \
    libelf-dev \
    libdwarf-dev

In the folly directory (e.g. the checkout root or the archive unpack root), run:
  mkdir _build && cd _build
  cmake ..
  make -j $(nproc)
  make install # with either sudo or DESTDIR as necessary

OS X (Homebrew)
folly is available as a Formula and releases may be built via brew install folly.
You may also use folly/build/bootstrap-osx-homebrew.sh to build against master:
  ./folly/build/bootstrap-osx-homebrew.sh

This will create a build directory _build in the top-level.
OS X (MacPorts)
Install the required packages from MacPorts:
  sudo port install \
    boost \
    cmake \
    gflags \
    git \
    google-glog \
    libevent \
    libtool \
    lz4 \
    lzma \
    openssl \
    snappy \
    xz \
    zlib

Download and install double-conversion:
  git clone https://github.com/google/double-conversion.git
  cd double-conversion
  cmake -DBUILD_SHARED_LIBS=ON .
  make
  sudo make install

Download and install folly with the parameters listed below:
  git clone https://github.com/facebook/folly.git
  cd folly
  mkdir _build
  cd _build
  cmake ..
  make
  sudo make install

Windows (Vcpkg)
folly is available in Vcpkg and releases may be built via vcpkg install folly:x64-windows.
You may also use vcpkg install folly:x64-windows --head to build against master.
Other Linux distributions


double-conversion (https://github.com/google/double-conversion)
Download and build double-conversion.
You may need to tell cmake where to find it.
[double-conversion/] ln -s src double-conversion
[folly/] mkdir build && cd build
[folly/build/] cmake ""-DCMAKE_INCLUDE_PATH=$DOUBLE_CONVERSION_HOME/include"" ""-DCMAKE_LIBRARY_PATH=$DOUBLE_CONVERSION_HOME/lib"" ..
[folly/build/] make


additional platform specific dependencies:
Fedora >= 21 64-bit (last tested on Fedora 28 64-bit)

gcc
gcc-c++
cmake
automake
boost-devel
libtool
lz4-devel
lzma-devel
snappy-devel
zlib-devel
glog-devel
gflags-devel
scons
double-conversion-devel
openssl-devel
libevent-devel

Optional

libdwarf-dev
libelf-dev
libunwind8-dev



",12947
fsharp/FsAutoComplete,F#,"

FsAutoComplete
The FsAutoComplete project (FSAC) provides a backend service for rich editing or 'intellisense' features for editors.
It can be hosted as command-line interface (stdio mode) or as http server (http mode), both using the same json protocol.
Currently it is used by:

Emacs
Vim
Visual Studio Code

It's based on:

FSharp.Compiler.Service for F# language info.
Dotnet.ProjInfo for project/sln management.
FSharpLint for the linter feature.

Required software
FsAutoComplete can run on .NET/mono or .NET Core.
FSAC .NET

on windows: Microsoft Build Tools 2015
on unix/mac: Required: Mono >= 5.12, Recommended: Mono >= 5.18

FSAC .NET Core

.NET Core Sdk
on unix/mac: Required: Mono >= 5.12, Recommended: Mono >= 5.18

Building and testing
Requirements:

.NET Core Sdk, see global.json for the exact version.
Mono 5.18 on unix/osx
Microsoft Build Tools 2013

There is a FAKE script who can be invoked with build.cmd/build.sh.

To build fsautocomplete binaries in ~/bin directory, do run build LocalRelease
To build, run all tests and create packages, do run build All

The integration tests use a simple strategy of running a scripted session with fsautocomplete and then comparing the output with that saved in the repository. This requires careful checking when the test is first constructed. On later runs, absolute paths are removed using regular expressions to ensure that the tests are machine-independent.
see test/FsAutoComplete.IntegrationTests/README.md for more info.
Troubleshooting
FileWatcher exceptions
You may see a stack trace finishing with System.IO.IOException: kqueue() error at init, error code = ’0’. This is due to a limitation in the number of filehandles that the Mono file watchers can keep open. Restarting FsAutoComplete or the hosting editor should help. If not, try setting export MONO_MANAGED_WATCHER=disabled in your ~/.bash_profile. Note that on OSX, this setting will only take effect if you launch emacs from the terminal.
Communication protocol
It is expected that the editor will launch this program in the background and communicate over a pipe. It is possible to use interactively, although due to the lack of any readline support it isn't pleasant, and text pasted into the buffer may not be echoed. As a result, use this only for very simple debugging. For more complex scenarios it is better to write another integration test by copying an existing one.
The available commands can be listed by running fsautocomplete --commands. Commands are all on a single line, with the exception of the parse command, which should be followed by the current text of the file to parse (which may differ from the contents on disk), and terminated with a line containing only <<EOF>>.
Data is returned as JSON. An example of a simple session is:
project ""Test1.fsproj""

{""Kind"":""project"",""Data"":{""Files"":[""<absolute path removed>/Program.fs""],""Output"":""<absolute path removed>/bin/Debug/Test1.exe""}}
parse ""Program.fs""
module X =
    let func x = x + 1

    let val2 = X.func 2
    <<EOF>>

{""Kind"":""INFO"",""Data"":""Background parsing started""}
completion ""Program.fs"" 4 13
{""Kind"":""completion"",""Data"":[""func""]}
Each response is exactly one line, which simplifies the application of a JSON parser. For further insight into the communication protocol, have a look over the integration tests, which have examples of all the features. Each folder contains one or more *Runner.fsx files which specify a sequence of commands to send, and *.json files, which contain the output.
Maintainers
The maintainers of this repository are:

Steffen Forkmann
Karl Nilsson
Enrico Sada
Krzysztof Cieślak

The primary maintainer for this repository is Enrico Sada
Previous maintainers:

Robin Neatherway

",109
LogtalkDotOrg/logtalk3,Prolog,"
This file is part of Logtalk https://logtalk.org/
Copyright 1998-2019 Paulo Moura pmoura@logtalk.org
Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

README



About
Logtalk is a declarative object-oriented logic programming language that
extends and leverages the Prolog language with a feature set suitable for
programming in the large. Logtalk supports modern code encapsulation and
code reuse mechanisms while preserving the declarative programming features
of Prolog.
Logtalk is implemented as a trans-compiler in highly portable code and can
use most modern and standards compliant Prolog implementations as a backend
compiler.
As a multi-paradigm language, Logtalk includes support for both prototypes
and classes, protocols (interfaces), categories (components and
hot-patching), event-driven programming, coinduction, lambda expressions,
and high-level multi-threading programming. Logtalk uses standard Prolog
syntax with the addition of some operators and directives for a smooth learning
path.
Logtalk is distributed under a commercial friendly license and includes full
documentation, tutorials, portable libraries, a comprehensive set of portable
developer tools, and numerous programming examples to help get you started.
Logtalk development adheres to the Contributor Covenant
code of conduct. By participating,
you are expected to uphold this code. Please report
unacceptable behavior to contact@logtalk.org.
Logtalk website
The latest stable release of the Logtalk is always available at:
https://logtalk.org/
At this address you can download installers for your operating-system and
find full documentation on Logtalk.
Installation
Logtalk can be installed either from sources by running a couple of shell
scripts or by using an installer for your
operating system. For manual installation, see the INSTALL.md
file for detailed instructions.
See the user manual for a description of the source
files organization and for usage instructions.
If you are upgrading from a previous Logtalk version, please check the
UPGRADING.md file for instructions on how to upgrade your
programs or your custom adapter files to run under this new version.
Customization
The CUSTOMIZE.md file provides detailed instructions for
customizing the Logtalk installation and working environment.
Running
The QUICK_START.md file provides quick instructions for
those of you in a hurry to run Logtalk, provided that your favorite Prolog
compiler is supported and installed.
Documentation
A quick and highly recommended introduction for users comfortable with Prolog
and with general knowledge about object-oriented programming is available at
the Learn X in Y minutes website.
The reference and user manuals are provided in HTML format and can be found in
the manuals directory. They are also available online at:
https://logtalk.org/manuals/index.html
The RELEASE_NOTES.md file contains descriptions of all
Logtalk updates since the first public version. Please check it if you are
upgrading from a previous Logtalk version.
The API documentation for the core, library, tools, and contributions is
provided in HTML format and can be found in the docs directory and
also available online at:
https://logtalk.org/docs/index.html
Most directories include NOTES.md or NOTES.txt documentation files.
On POSIX systems, there's also a man page for most shell scripts.
A list of these scripts can be generated using the apropos logtalk command.
HTML versions of the man pages are also available
at the Logtalk website.
Registration
To register as a Logtalk user please use the registration form at:
https://logtalk.org/regform.html
Registration is optional. But it's also a way of showing your support and
an opportunity for us to learn about the cool projects where you will be
using Logtalk.
Support
Support channels include:

Professional services
Community discussion forums
Community live chat room

For more information on support options, please consult:
https://logtalk.org/support.html
Citations
If you find Logtalk useful, please include a citation on your publications
(also cite the used backend Prolog compilers). The BIBLIOGRAPHY.bib
file includes bibliographic references in BibTeX format (including the 2003
PhD thesis on Logtalk design and implementation).
Contributions
Contributions are most welcome! See the CONTRIBUTING.md file
for details.
Legal
Logtalk is copyrighted by Paulo Moura and made available under the Apache
License 2.0. See the LICENSE.txt file for the license terms.
The copyright notice and license applies to all files in this release unless
otherwise explicitly stated. See the NOTICE.txt for additional
copyright information.
Some files that are part of the Logtalk distribution are distributed using
a different license and/or are copyrighted by a Logtalk contributor.
Some examples are adaptations to Logtalk of Prolog examples found elsewhere
(e.g. in manuals, tutorials, books, and public discussion forums). See those
examples documentation for information on the sources of the original code.
Logtalk is a registered trademark of Paulo Moura.
",124
fsharp/FsAutoComplete,F#,"

FsAutoComplete
The FsAutoComplete project (FSAC) provides a backend service for rich editing or 'intellisense' features for editors.
It can be hosted as command-line interface (stdio mode) or as http server (http mode), both using the same json protocol.
Currently it is used by:

Emacs
Vim
Visual Studio Code

It's based on:

FSharp.Compiler.Service for F# language info.
Dotnet.ProjInfo for project/sln management.
FSharpLint for the linter feature.

Required software
FsAutoComplete can run on .NET/mono or .NET Core.
FSAC .NET

on windows: Microsoft Build Tools 2015
on unix/mac: Required: Mono >= 5.12, Recommended: Mono >= 5.18

FSAC .NET Core

.NET Core Sdk
on unix/mac: Required: Mono >= 5.12, Recommended: Mono >= 5.18

Building and testing
Requirements:

.NET Core Sdk, see global.json for the exact version.
Mono 5.18 on unix/osx
Microsoft Build Tools 2013

There is a FAKE script who can be invoked with build.cmd/build.sh.

To build fsautocomplete binaries in ~/bin directory, do run build LocalRelease
To build, run all tests and create packages, do run build All

The integration tests use a simple strategy of running a scripted session with fsautocomplete and then comparing the output with that saved in the repository. This requires careful checking when the test is first constructed. On later runs, absolute paths are removed using regular expressions to ensure that the tests are machine-independent.
see test/FsAutoComplete.IntegrationTests/README.md for more info.
Troubleshooting
FileWatcher exceptions
You may see a stack trace finishing with System.IO.IOException: kqueue() error at init, error code = ’0’. This is due to a limitation in the number of filehandles that the Mono file watchers can keep open. Restarting FsAutoComplete or the hosting editor should help. If not, try setting export MONO_MANAGED_WATCHER=disabled in your ~/.bash_profile. Note that on OSX, this setting will only take effect if you launch emacs from the terminal.
Communication protocol
It is expected that the editor will launch this program in the background and communicate over a pipe. It is possible to use interactively, although due to the lack of any readline support it isn't pleasant, and text pasted into the buffer may not be echoed. As a result, use this only for very simple debugging. For more complex scenarios it is better to write another integration test by copying an existing one.
The available commands can be listed by running fsautocomplete --commands. Commands are all on a single line, with the exception of the parse command, which should be followed by the current text of the file to parse (which may differ from the contents on disk), and terminated with a line containing only <<EOF>>.
Data is returned as JSON. An example of a simple session is:
project ""Test1.fsproj""

{""Kind"":""project"",""Data"":{""Files"":[""<absolute path removed>/Program.fs""],""Output"":""<absolute path removed>/bin/Debug/Test1.exe""}}
parse ""Program.fs""
module X =
    let func x = x + 1

    let val2 = X.func 2
    <<EOF>>

{""Kind"":""INFO"",""Data"":""Background parsing started""}
completion ""Program.fs"" 4 13
{""Kind"":""completion"",""Data"":[""func""]}
Each response is exactly one line, which simplifies the application of a JSON parser. For further insight into the communication protocol, have a look over the integration tests, which have examples of all the features. Each folder contains one or more *Runner.fsx files which specify a sequence of commands to send, and *.json files, which contain the output.
Maintainers
The maintainers of this repository are:

Steffen Forkmann
Karl Nilsson
Enrico Sada
Krzysztof Cieślak

The primary maintainer for this repository is Enrico Sada
Previous maintainers:

Robin Neatherway

",109
LogtalkDotOrg/logtalk3,Prolog,"
This file is part of Logtalk https://logtalk.org/
Copyright 1998-2019 Paulo Moura pmoura@logtalk.org
Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

README



About
Logtalk is a declarative object-oriented logic programming language that
extends and leverages the Prolog language with a feature set suitable for
programming in the large. Logtalk supports modern code encapsulation and
code reuse mechanisms while preserving the declarative programming features
of Prolog.
Logtalk is implemented as a trans-compiler in highly portable code and can
use most modern and standards compliant Prolog implementations as a backend
compiler.
As a multi-paradigm language, Logtalk includes support for both prototypes
and classes, protocols (interfaces), categories (components and
hot-patching), event-driven programming, coinduction, lambda expressions,
and high-level multi-threading programming. Logtalk uses standard Prolog
syntax with the addition of some operators and directives for a smooth learning
path.
Logtalk is distributed under a commercial friendly license and includes full
documentation, tutorials, portable libraries, a comprehensive set of portable
developer tools, and numerous programming examples to help get you started.
Logtalk development adheres to the Contributor Covenant
code of conduct. By participating,
you are expected to uphold this code. Please report
unacceptable behavior to contact@logtalk.org.
Logtalk website
The latest stable release of the Logtalk is always available at:
https://logtalk.org/
At this address you can download installers for your operating-system and
find full documentation on Logtalk.
Installation
Logtalk can be installed either from sources by running a couple of shell
scripts or by using an installer for your
operating system. For manual installation, see the INSTALL.md
file for detailed instructions.
See the user manual for a description of the source
files organization and for usage instructions.
If you are upgrading from a previous Logtalk version, please check the
UPGRADING.md file for instructions on how to upgrade your
programs or your custom adapter files to run under this new version.
Customization
The CUSTOMIZE.md file provides detailed instructions for
customizing the Logtalk installation and working environment.
Running
The QUICK_START.md file provides quick instructions for
those of you in a hurry to run Logtalk, provided that your favorite Prolog
compiler is supported and installed.
Documentation
A quick and highly recommended introduction for users comfortable with Prolog
and with general knowledge about object-oriented programming is available at
the Learn X in Y minutes website.
The reference and user manuals are provided in HTML format and can be found in
the manuals directory. They are also available online at:
https://logtalk.org/manuals/index.html
The RELEASE_NOTES.md file contains descriptions of all
Logtalk updates since the first public version. Please check it if you are
upgrading from a previous Logtalk version.
The API documentation for the core, library, tools, and contributions is
provided in HTML format and can be found in the docs directory and
also available online at:
https://logtalk.org/docs/index.html
Most directories include NOTES.md or NOTES.txt documentation files.
On POSIX systems, there's also a man page for most shell scripts.
A list of these scripts can be generated using the apropos logtalk command.
HTML versions of the man pages are also available
at the Logtalk website.
Registration
To register as a Logtalk user please use the registration form at:
https://logtalk.org/regform.html
Registration is optional. But it's also a way of showing your support and
an opportunity for us to learn about the cool projects where you will be
using Logtalk.
Support
Support channels include:

Professional services
Community discussion forums
Community live chat room

For more information on support options, please consult:
https://logtalk.org/support.html
Citations
If you find Logtalk useful, please include a citation on your publications
(also cite the used backend Prolog compilers). The BIBLIOGRAPHY.bib
file includes bibliographic references in BibTeX format (including the 2003
PhD thesis on Logtalk design and implementation).
Contributions
Contributions are most welcome! See the CONTRIBUTING.md file
for details.
Legal
Logtalk is copyrighted by Paulo Moura and made available under the Apache
License 2.0. See the LICENSE.txt file for the license terms.
The copyright notice and license applies to all files in this release unless
otherwise explicitly stated. See the NOTICE.txt for additional
copyright information.
Some files that are part of the Logtalk distribution are distributed using
a different license and/or are copyrighted by a Logtalk contributor.
Some examples are adaptations to Logtalk of Prolog examples found elsewhere
(e.g. in manuals, tutorials, books, and public discussion forums). See those
examples documentation for information on the sources of the original code.
Logtalk is a registered trademark of Paulo Moura.
",124
xndcn/smzdm.com,None,"smzdm.com
",10
pivotal-cf/docs-riverbed-appinternals,HTML,"Pivotal Cloud Foundry Partners Template
This template helps partners prepare documentation for Pivotal Cloud Foundry (PCF) partner services that appear on Pivotal Network.
Overview
Every partner service in PCF is documented on our PCF documentation site. The links to these partner service docs appear on the front page under Partner Services for Pivotal Cloud Foundry.
For a good example of a partner service doc, see ISS Knowtify Search Analytics.
How To Use This Template
Partners use this template to develop the documentation for their PCF service. This repo currently includes templates for the following topics:

index.html.md.erb: The index of your docs.
installing.html.md.erb: How to install and configure your product tile.
using.html.md.erb: How to use your product.
release-notes.html.md.erb: Release notes for your product.

To begin using this repo to develop your documentation, perform the following steps:

Make a fork of this repo.
Clone your fork onto your local machine.
Work your way through each topic, replacing the placeholders in ALL-CAPS and following the instructions in bold.

When writing your documentation, follow the guidelines in Style Notes for Tile Authors.


Complete the subnav by replacing the placeholders in ALL-CAPS in the subnav file at docs-book/master_middleman/source/subnavs/myservice_subnav.erb in this repo.
View your documentation as a live local site in a browser, by following the steps below in the How To Use Bookbinder To View Your Docs section.
When you've finished your documentation, make a pull request to merge your fork into this repo and email the PCF Docs Team at cf-docs@pivotal.io.

How To Use Bookbinder To View Your Docs
Bookbinder is a command-line utility for stitching Markdown docs into a hostable web app. The PCF Docs Team uses Bookbinder to publish our docs site, but you can also use Bookbinder to view a live version of your documentation on your local machine.
Bookbinder draws the content for the site from docs-content, the subnav from docs-book, and various layout configuration and assets from docs-layout.
To use Bookbinder to view your documentation, perform the following steps:

Install Bookbinder by running gem install bookbindery. If you have trouble, consult the Zero to Bookbinder section to make sure you have the correct dependencies installed.
On your local machine, cd into docs-book in the cloned repo.
Run bundle install to make sure you have all the necessary gems installed.
Build your documentation site with bookbinder in one of the two following ways:

Run bundle exec bookbinder watch to build an interactive version of the docs and navigate to localhost:4567/myservice/ in a browser. (It may take a moment for the site to load at first.) This builds a site from your content repo at docs-content, and then watches that repo to update the site if you make any changes to the repo.
Run bundle exec bookbinder bind local to build a Rack web-app of the book. After the bind has completed, cd into the final_app directory and run rackup. Then navigate to localhost:9292/myservice/ in a browser.



Zero to Bookbinder: How to Install Bookbinder and Build, View, and Edit Your Docs from Nothing
If you are reading this, Pivotal has invited you to a git repo where you can build and edit documentation in the Ruby / Markdown / HTML format that the online publishing tool Bookbinder uses to build Pivotal's documentation.
Here's how to install Bookbinder and build your docs from the repo, starting from scratch, on a Mac OS X machine.
Note: All steps below are implicitly preceded with, ""If you haven't already..."" You should skip any installation steps that have already contributed to your environment.
Install Ruby
In Terminal window:


Make and cd into a workspace directory.
$ mkdir workspace
$ cd workspace


Follow the instructions at http://brew.sh to install brew / homebrew
$ /usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)""


Install your own (non-system) ruby.
$ brew install ruby


Set up Git


Download and Install git by following the instructions at git-scm.com.


Install your own (non-system) bash-completion (optional).
$ brew install git bash-completion


If you don't already have one, generate a public/private RSA key pair, and save the key to your ~/.ssh directory.
$ ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/Users/pspinrad/.ssh/id_rsa): 



Get a Github account.


Add your RSA public key to your Github account / profile page.
$ cat ~/.ssh/id_rsa.pub # copy and paste this into Github profile page as new key


Get the Correct Ruby Version for Bookbinder: Ruby 2.3.0


Install a Ruby manager such as chruby.
$ brew install chruby


Add your Ruby manager to your ~/.bashrc by appending the following line:
source /usr/local/opt/chruby/share/chruby/chruby.sh


Install the ruby-install installer.
$ brew install ruby-install


Run ruby-install to install Ruby 2.3.0.
$ ruby-install ruby 2.3.0


Select the following Ruby version.
chruby ruby-2.3.0


Install Bookbinder


Install bundler.
$ gem install bundler


Install bookbinder (the bookbindery gem).
$ gem install bookbindery


Build the Docs Locally


Clone the docs template repo you will be building from.
$ git clone git@github.com:pivotal-cf/docs-partners-template


cd into the book subdirectory of the repo.
$ cd docs-partners-template/docs-book


Run bundle install to install all book dependencies.
$ bundle install


Run bundle exec bookbinder watch to build the book on your machine.
$ bundle exec bookbinder watch


Browse to localhost:4567 to view the book locally and ""watch"" any changes that you make to source html.md.erb files. As you make and save changes to the local source files for your site, you will see them in your browser after a slight delay.


After each session of writing or revising your docs source files, commit and push them to your github repo.


About Subnavs of Published Tile Documentation
After your tile documentation has been published, the subnav used for the live documentation is contained in this directory: https://github.com/pivotal-cf/docs-book-partners/tree/master/master_middleman/source/subnavs
However, you should also continue to maintain the local subnav file so that the subnav looks correct when you or a Pivotal writer builds your documentation locally with bookbinder for review or editing.
To edit a subnav for your tile documentation, follow these steps:


Make a pull request against the subnav file in https://github.com/pivotal-cf/docs-book-partners/tree/master/master_middleman/source/subnavs


Make the same changes in the subnav file (in /docs-book/master_middleman/source/subnavs/ of your tile repo) and make a pull request for that change too.


Happy documenting!


",2
wararyo/M5Chorder,C++,"M5Stack BLEChorder
BLE Chordpad App
This is a M5Stack app which provides functions to play chords via Bluetooth Low Energy MIDI.

🎥Walkthrough Video
Note
Now works with iOS and macOS and Windows!! 😆
It uses 1.3M byte of program memory.
If you use M5Stack 4MB-flash model, change the partition to ""Huge App (3MB No OTA)"".
Contributors welcome!
How to Install
Using SDUpdater(or LovyanLauncher) (recommended)
This way cannot be used for 4MB-flash model

Install SDUpdater or LovyanLauncher into your M5Stack.
Download M5Chorder_1.0.0_bin.zip from Releases.
Prepare an SD Card and copy files like the following.
Insert the SD into the M5Stack and execute.

(SD Root)
/
├BLEChorder.bin
├json
│└BLEChorder.json
└jpg
  ├BLEChorder.jpg
  └BLEChorder_gh.jpg

Using PlatformIO

Install Visual Studio Code.
Install PlatformIO.
Clone the repository.
Open repository folder in VSCode and run ""PlatformIO: Upload"".

Using esptool (untested)

Install esptool.
Download M5Chorder_1.0.0_bin.zip from Releases.
Run the following.

# Case of 16MB-flash model and of default 16MB partition
esptool --chip esp32 --port /dev/tty.SLAB_USBtoUART --baud 921600 --before default_reset --after hard_reset write_flash -z --flash_freq 40m 0x10000 BLEChorder.bin
Credits

M5TreeView by lovyan03

",2
SplendidStrontium/splendidstrontium.github.io,CSS,"splendidstrontium.github.io
@TODO

sitemap.html

",2
Barbmatt/Proyecto2,JavaScript,"...
",3
packetflinger/openra2,C,"OpenRA2
Joe Reid <claire@packetflinger.com>
An open source remake of the Rocket Arena mod for Quake 2
Rocket Arena?
Rocket Arena 2 is a team-based mod for Quake 2. Games are arranged into rounds
where each player spawns with all allowed items/weapons and they battle until
everyone from one team is dead.
Why?
Yes, there is an existing RA2 mod that is available, but it dates back to the
20th century and the source code is not available (or at least I'm unable to
locate it). There also don't seem to be any 64 bit binaries.
Client commands
accuracy
See your current accuracy statistics
admin
Enter admin mode, elevating your privileges
arena 
Join a specific arena. If the argument is missing a list of all arenas will be displayed
commands
Show client commands list
highscores
Show highscores for your arena
id
Toggle player IDs
join 
Join a team
matchinfo
Show arena settings
menu
Show the GUI game menu
obs
Alias for observer
observe
Part your current team and enter spectator mode
oldscore
Show the scores from the last match
players
List all players connected to the server
ready
Toggles your ready status. Once all team players are ready the match will start.
settings
Alias for matchinfo
stats
Alias for accuracy
team
Alias for join
vote <xxx|yes|no>
Call a vote or cast your vote
Admin commands
Administrators are granted access to a number of privileged client
commands.
acommands
Show administrator commands list.
admin [password]
Toggle administrator status.
ban  [duration] [action]
Add IP address specified by ip-mask into the ban list.  Optional
duration parameter specifies how long this address should stay in the
list. Default duration is 1 hour.  Maximum duration is 12 hours. Default
units for specifying duration are minutes. Add ‘h’ or ‘H’ suffix to
specify hours. Add ‘d’ or ‘D’ suffix to specify days. Optional action
parameter specifies ban type. It can be ban (prevent player from
connecting) or mute (allow player to connect and enter the game, but
disallow chat during the match). Default action is ban.
bans
Show the current ban list.
kick 
Kick player from the server.
kickban
Kick player from the server and ban his IP address for 1 hour.
mute 
Disallow player to talk during the match.
muteall
Globally disable chat during the match.
unban 
Remove IP address specified by ip-mask from the ban list. Permanent bans
added by server operator can't be removed.
unmute 
Allow player to talk during the match.
unmuteall
Globally enable chat during the match.
Server configuration
Under Construction
",3
AnimeNL/anime-2017,PHP,"AnimeCon 2019 Volunteer Portal
This repository contains a Volunteer Portal backend implementation for the
AnimeCon 2019 festival. The front-end may be found in the
portal project, also published on GitHub.
API Documentation
The /api/ directory implements the full set of
documented interfaces. Because the
implementation is provided in PHP, the following mappings must be set up with the webserver.



Request
Handler




/api/environment
/api/environment.php


/api/login
/api/login.php



TODO: Clean up the following
Development setup (Docker)
You can easily get a development setup on Linux by using Docker.
To build the container, run docker build -t anime2017 docker.
After that, run one of the following commands:

To run the services: ./bin/services
To run a test webserver: ./bin/serve
To run the linter: ./bin/lint
To run the testsuite: ./bin/test

Accessing development host
By default, there is a base configuration for the ""anime.test"" environment.
To access this after starting the development setup via Docker, add a line 127.0.0.1  anime.test to the /etc/hosts file.
After having done this, you can access the running development webserver at http://anime.test:8080/ .
Backend code (PHP)
The backend is located in the /anime/ directory and has been written in PHP.
A large portion of the backend code exists to import data from third-party sources in a rather
pendantic way, given that this has caused issues in the past, for which a Service Manager has been
created in the /anime/Services/ directory.
A number of external dependencies will be pulled in using Composer. Be
sure to run composer install when starting to work with this repository, and composer update
every time you pull new changes.
Installation
Create the following files, and make sure that they're readable by the current user, as well as the
user that will be used for serving the application (e.g. apache).

anime/Services/error.log
anime/Services/state.json
configuration/teams/

If you are running on a system that has SELinux set to enforcing, make sure you change the context
of anime/Services/error.log to httpd_sys_rw_content_t
(run chcon -t httpd_sys_rw_content_t anime/Services/error.log).
TODO: Document both backend and frontend deployment in this section.
Configuration
Look in anime/Services/Import{Program,Schedule,Team}Service.php to see how the various services import
the data formats. Then move configuration/configuration.json-example to configuration/configuration.json
and edit the email address and configure the Services you want to use for importing your schedule and
other configuration.
TODO: Write some more explanation about importing the configuration.
",2
AnimeNL/anime-2017,PHP,"AnimeCon 2019 Volunteer Portal
This repository contains a Volunteer Portal backend implementation for the
AnimeCon 2019 festival. The front-end may be found in the
portal project, also published on GitHub.
API Documentation
The /api/ directory implements the full set of
documented interfaces. Because the
implementation is provided in PHP, the following mappings must be set up with the webserver.



Request
Handler




/api/environment
/api/environment.php


/api/login
/api/login.php



TODO: Clean up the following
Development setup (Docker)
You can easily get a development setup on Linux by using Docker.
To build the container, run docker build -t anime2017 docker.
After that, run one of the following commands:

To run the services: ./bin/services
To run a test webserver: ./bin/serve
To run the linter: ./bin/lint
To run the testsuite: ./bin/test

Accessing development host
By default, there is a base configuration for the ""anime.test"" environment.
To access this after starting the development setup via Docker, add a line 127.0.0.1  anime.test to the /etc/hosts file.
After having done this, you can access the running development webserver at http://anime.test:8080/ .
Backend code (PHP)
The backend is located in the /anime/ directory and has been written in PHP.
A large portion of the backend code exists to import data from third-party sources in a rather
pendantic way, given that this has caused issues in the past, for which a Service Manager has been
created in the /anime/Services/ directory.
A number of external dependencies will be pulled in using Composer. Be
sure to run composer install when starting to work with this repository, and composer update
every time you pull new changes.
Installation
Create the following files, and make sure that they're readable by the current user, as well as the
user that will be used for serving the application (e.g. apache).

anime/Services/error.log
anime/Services/state.json
configuration/teams/

If you are running on a system that has SELinux set to enforcing, make sure you change the context
of anime/Services/error.log to httpd_sys_rw_content_t
(run chcon -t httpd_sys_rw_content_t anime/Services/error.log).
TODO: Document both backend and frontend deployment in this section.
Configuration
Look in anime/Services/Import{Program,Schedule,Team}Service.php to see how the various services import
the data formats. Then move configuration/configuration.json-example to configuration/configuration.json
and edit the email address and configure the Services you want to use for importing your schedule and
other configuration.
TODO: Write some more explanation about importing the configuration.
",2
jamesjun/ironclust,Jupyter Notebook,"ironclust
",2
stoplightio/spectral,TypeScript,"


A flexible JSON object linter with out of the box support for OpenAPI v2 and v3
Features

Create custom rules to lint any JSON object
Use JSON paths to apply rules / functions to specific parts of your JSON objects
Built-in set of functions to help build custom rules. Functions include pattern checks, parameter checks, alphabetical ordering, a specified number of characters, provided keys are present in an object, etc
Create custom functions for advanced use cases
Optional ready to use rules and functions to validate and lint OpenAPI v2 and v3 documents
Validate JSON with Ajv

Installation
Local Installation
npm install @stoplight/spectral
Global Installation
npm install -g @stoplight/spectral
Supports Node v8.3+.
Executable binaries
For users without Node and/or NPM/Yarn, we provide standalone packages for all major platforms:

x64 Windows
x64 MacOS
x64 Linux

You can find them here.
Once downloaded, you can proceed with the standard procedure for running any CLI tool.
./spectral-macos lint petstore.yaml
Note, the binaries are not auto-updatable, therefore you will need to download a new version on your own.
Installing binaries system-wide
Linux
sudo mv ./spectral-linux /usr/local/bin/spectral
You may need to restart your terminal.
Now, spectral command will be accessible in your terminal.
Head over to releases for the latest binaries.
Docker
docker run --rm -it stoplight/spectral lint ""${URL}""`
Usage
CLI
Spectral can be run via the command-line:
spectral lint petstore.yaml
Other options include:
  -c, --config=config          path to a config file
  -e, --encoding=encoding      text encoding to use
  -f, --format=json|stylish    formatter to use for outputting results
  -h, --help                   show CLI help
  -m, --maxResults=maxResults  deprecated: use --max-results instead
  -o, --output=output          output to a file instead of stdout
  -r, --ruleset=ruleset        path to a ruleset file (supports remote files)
  -s, --skip-rule=skip-rule    ignore certain rules if they are causing trouble
  -v, --verbose                increase verbosity
  --max-results=max-results    [default: all] maximum results to show


Note: The Spectral CLI supports both YAML and JSON.

Currently, the CLI supports validation of OpenAPI documents and lints them based on our default ruleset. It does not support custom rulesets at this time. Although if you want to build and run custom rulesets outside of the CLI, see Customization.
Example: Linting an OpenAPI document
Spectral includes a number of ready made rules and functions for OpenAPI v2 and v3 documents.
This example uses the OpenAPI v3 rules to lint a document.
const { Spectral } = require('@stoplight/spectral');
const { oas3Functions, oas3Rules } = require('@stoplight/spectral/rulesets/oas3');
// for YAML
const { parseWithPointers } = require(""@stoplight/yaml"");
const myOAS = parseWithPointers(`
responses:
  '200':
    description: ''
    schema:
      $ref: '#/definitions/error-response'
`)

// an OASv3 document
const myOAS = {
  // ... properties in your document
  responses: {
    '200': {
      description: '',
      schema: {
        $ref: '#/definitions/error-response',
      },
    },
  },
  // ... properties in your document
};

// create a new instance of spectral with all of the baked in rulesets
const spectral = new Spectral();

spectral.addFunctions(oas3Functions());
spectral.addRules(oas3Rules());

spectral.addRules({
  // .. extend with your own custom rules
});

// run!
spectral.run(myOAS).then(results => {
  console.log(JSON.stringify(results, null, 4));
});
You can also add to these rules to create a customized linting style guide for your OpenAPI documents.
The existing OAS rules are opinionated. There might be some rules that you prefer to change. We encourage you to create your rules to fit your use case. We welcome additions to the existing rulesets as well!
Rulesets
You can find all about rulesets here.
Advanced
Customization
There are three key concepts in Spectral: Rulesets, Rules and Functions.

Ruleset is a container for a collection of rules and functions.
Rule filters your object down to a set of target values, and specify the function that should evaluate those values.
Function accept a value and return issue(s) if the value is incorrect.

Think of a set of rules and functions as a flexible and customizable style guide for your JSON objects.
Creating a custom rule
Spectral has a built-in set of functions which you can reference in your rules. This example uses the RuleFunction.PATTERN to create a rule that checks that all property values are in snake case.
const { RuleFunction, Spectral } = require('@stoplight/spectral');

const spectral = new Spectral();

spectral.addRules({
  snake_case: {
    summary: 'Checks for snake case pattern',

    // evaluate every property
    given: '$..*',

    then: {
      function: RuleFunction.PATTERN,
      functionOptions: {
        match: '^[a-z]+[a-z0-9_]*[a-z0-9]+$',
      },
    },
  },
});

// run!
spectral.run({name: 'helloWorld',}).then(results => {
  console.log(JSON.stringify(results, null, 4));
});

// => outputs a single result since `helloWorld` is not snake_case
// [
//   {
//     ""code"": ""snake_case"",
//     ""message"": ""must match the pattern '^[a-z]+[a-z0-9_]*[a-z0-9]+$'"",
//     ""severity"": 1,
//     ""path"": [
//       ""name""
//     ]
//   }
// ]
Creating a custom function
Sometimes the built-in functions don't cover your use case. This example creates a custom function, customNotThatFunction, and then uses it within a rule, openapi_not_swagger. The custom function checks that you are not using a specific string (e.g., ""Swagger"") and suggests what to use instead (e.g., ""OpenAPI"").
const { Spectral } = require('@stoplight/spectral');

// custom function
const customNotThatFunction = (targetValue, options) => {
  const { match, suggestion } = options;

  if (targetValue && targetValue.match(new RegExp(match))) {
    // return the single error
    return [
      {
        message: `Use ${suggestion} instead of ${match}!`,
      },
    ];
  }
};

const spectral = new Spectral();

spectral.addFunctions({
  notThat: customNotThatFunction,
});

spectral.addRules({
  openapi_not_swagger: {
    summary: 'Checks for use of Swagger, and suggests OpenAPI.',

    // check every property
    given: '$..*',

    then: {
      // reference the function we added!
      function: 'notThat',

      // pass it the options it needs
      functionOptions: {
        match: 'Swagger',
        suggestion: 'OpenAPI',
      },
    },
  },
});

// run!
spectral.run({description: 'Swagger is pretty cool!',}).then(results => {
  console.log(JSON.stringify(results, null, 4));
});

// => outputs a single result since we are using the term `Swagger` in our object
// [
//   {
//     ""code"": ""openapi_not_swagger"",
//     ""message"": ""Use OpenAPI instead of Swagger!"",
//     ""severity"": 1,
//     ""path"": [
//       ""description""
//     ]
//   }
// ]
FAQs
How is this different than Ajv?
Ajv is a JSON Schema validator, not a linter. Spectral does expose a schema function that you can use in your rules to validate all or part of the target object with JSON Schema (Ajv is used under the hood). However, Spectral also provides a number of other functions and utilities that you can use to build up a linting ruleset to validates things that JSON Schema is not well suited for.
I want to lint my OpenAPI documents but don't want to implement Spectral right now.
No problem! A hosted version of Spectral comes free with the Stoplight platform. Sign up for a free account here.
What is the difference between Spectral and Speccy?
With Spectral, lint rules can be applied to any JSON object. Speccy is designed to work with OpenAPI v3 only. The rule structure is different between the two. Spectral uses JSONPath path parameters instead of the object parameters (which are OpenAPI specific). Rules are also more clearly defined (thanks to TypeScript typings) and now require specifying a type parameter. Some rule types have been enhanced to be a little more flexible along with being able to create your own rules based on the built-in and custom functions.
Contributing
If you are interested in contributing to Spectral itself, check out our contributing docs to get started.
Also, most of the interesting projects are built with Spectral. Please consider using Spectral in a project or contribute to an existing one.
If you are using Spectral in your project and want to be listed in the examples section, we encourage you to open an issue.
Example Implementations

Stoplight's Custom Style and Validation Rules uses Spectral to validate and lint OpenAPI documents on the Stoplight platform
Spectral GitHub Bot, a GitHub pull request bot that lints your repo's OpenAPI document that uses the Probot framework, built by Taylor Barnett
Spectral GitHub Action, a GitHub Action that lints your repo's OpenAPI document, built by Vincenzo Chianese

Helpful Links

JSONPath Online Evaluator, a helpful tool to determine what path you want
stoplightio/json, a library of useful functions for when working with JSON
stoplightio/yaml, a library of useful functions for when working with YAML, including parsing YAML into JSON, and a few helper functions such as getJsonPathForPosition or getLocationForJsonPath

Thanks :)

Phil Sturgeon for collaboration and creating Speccy
Mike Ralphson for kicking off the Spectral CLI

Support
If you have a bug or feature request, please open an issue here.
If you need help using Spectral or have a support question, please use the Stoplight Community forum. We've created an open source category for these questions. It's also a great place to share your implementations.
If you want to discuss something in private, you can reach out to Stoplight support at support@stoplight.io.
",100
tegcommerce/tegcommerce,JavaScript,"




tegcommerce
open source e-commerce platform javascript full stack with reactjs and nodejs


























requirement

git
npm (> = 5.x)
nodejs (> = 8.x)
reactjs
redux
postgresql

installation

make sure you have installed nodejs
git clone git@github.com:tegcommerce/tegcommerce
cd tegcommerce
sudo npm install
sudo npm start
open http://localhost:3000/

application guide
simple free open everyone transparent decentralized collaborative inclusive minimalistic lean clear mean responsive progressive lifetime adaptable customizable light installable secure friendly intuitive integrated direct objective specialized focused basic clean automated cheap economic easy practical convenient innovate useful updated agile reasonable proportional meritocracy
roadmap
in progress

single-page app
progressive web app
byos (bring your own server)
shopping cart
product registration
admin dashboard
marketplace with themes and plugins
chat
integration with erp platforms
integration with crm platforms
integration with marketplace platforms
integration with classified platforms
integration with delivery service companies
inntegration with payment intermediaries
integration with hosting companies
integration with affiliate platforms
integration with social networks
integration with google services
integration with facebook services
api for plugins
spam blocker
search filter products
product recommendation
content block editor

planned

affiliate
dropshipping
transparent checkout
one page checkout
power checkout
freight calculator
order tracking
email marketing
qr code
crm System
erp System
conversational commerce with chat and chatbot
vr commerce with virtual reality and augmented virtual reality
network of stores on a single tegcommerce installation
integrated backup system with cloud storage services
prevent brute force password discovery
app with fingerprint biometrics
app with voice biometrics
app with image biometrics
voice search
image search
hosting
domain
ssl certificate
cdn
blockchain

community
any collaboration will be welcome!
are part of the community:

developers at all levels and in all areas
devops
designers
translators
event organizers
communicators and digital influencers
buyers and sellers
lawyers
others

ideal for those who want:

collaborate when and where you want
collaborate on open source project that positively impact the world
meet different people and participate in a plural and tolerant world-class team
learn, develop and excel personally and professionally

documentation
check out tegcommerce documentation
contributing
check out how to contribute
license
tegcommerce is released under the terms of the mit license
credit
we thank our core team davimonteiro, devfelipemonteiro, felipenoka, adamwairdev, bradlincoln, christie201, jameselbourn, loubrazier, michaelminarwa, richardlond, rudysing, setkinanne and other incredible collaborators.
",39
KwatME/muscle_type,Python,"Muscle Type
Omics App to characterize muscle type based on DNA 💪




This Omics App looks for a specific variant in the ACTN3 gene on chromosome 11 that influences muscle performance. If you have two copies of this variant, you're likely to be an endurance athlete. If you don't have this variant, you're likely a sprinter. And if you have one copy of this variant, your muscles are somewhere in the middle.
Biology
The alpha-actinin-3 (ACTN3) gene codes for a F-actin cross-linking protein, which is involved in actin-binding in skeletal muscle. ACTN3 is expressed only in fast-twitch muscle fibers, which fire more rapidly and generate more force. Fast-twitch muscle fibers are used during short intense physical movements such as sprinting.
The rs1815739 variant encodes a premature stop codon in ACTN3. Having this variant results in less functional F-actin cross-linking protein and therefore less functional fast-twitch muscle fibers.
Interesting Facts

It takes 17 muscles to smile, but 43 to frown!
Muscle makes up about 40% of your body weight.
Muscles can't push, they can only pull. When your arm pushes, your muscles are actually pulling your elbow!

Omics App powered by Guardiome



",2
Nukesor/pueue,Python,"Pueue







Pueue is a command line queue management tool for sequential and parallel execution of long running tasks. Not being bound to any terminal it is possible to check on your processes from any terminal or using the API. And the best part is that the queue will be processed by the daemon, even if you exit your ssh session.
It doesn't work for remote machines though, as the communication happens via unix sockets. You need to ssh into your machine to communicate with the daemon.
It provides functionality for:

Easy output inspection.
Interaction with running processes
Manipulation of the scheduled task order
Running multiple tasks at once (You can decide how many concurrent tasks you want to run)

Why should I use it?
Almost everybody who lives on the commandline knows the situation, when one needs to unzip or transfer huge amounts of data in different directories.
This normally ends with about 10 open terminals/tmux sessions and an overchallenged hard drive.
Pueue is specifically designed for these situations. It executes long running tasks in their respective directories, without being bound to any terminal.
Here a few possible applications:

Compression tasks
Movie encoding
Copying huge amounts of stuff
rsync tasks

If I got your attention, feel free to give it a try!
If you think this is awesome, help me, join the development and create some PRs or suggest some improvements.
I'm always open to suggestions and already implemented a few user requested features.
Installation:
There are three different ways to install pueue.

Use an Arch Linux AUR package manager i.e Yaourt: yaourt -S pueue-git . This will deploy the service file automatically.
Install by using pip: pip install pueue.
Clone the repository and execute python setup.py install.

How to use it:
There is a help option (-h) for all commands, but I'll list them here anyway.
pueue --daemon Starts the daemon. The daemon will try to load any queue from a previous session.
pueue --no-daemon Start the daemon in the current terminal.
pueue --stop-daemon Daemon will shut down after killing all processes.
pueue status Show the current state of the daemon and the processing state of the queue.
pueue reset Remove all commands from the queue, kill the current process and reset the queue index to 0.
pueue clear Remove all done or failed commands from the queue. This will rotate logs as well.
pueue config This command allows to set different config values without editing the config file and restarting the daemon. Look at pueue config -h for more information.
pueue add 'command' Add a command to the queue. It can be used without quotation marks, but a -- may be necessary if you want to pass parameters (pueue add -- ls -al). Also note that bash specific syntax like |, && or ; might cause unwanted behavior without quotation marks.
pueue edit [key] Edit the command of a specific queued or stashed entry in you $EDITOR.
pueue remove [keys...] Remove the specified entries. Running processes can't be removed.
pueue stash [keys...] Stash queued entries for later processing. They won't be processed by the daemon, but can be manually enqueued again.
pueue enqueue [keys...] Enqueue stashed entries. The entries will be normally processed.
pueue switch [key_1] [key_2] Switch the entries at position key_1 and key_2.
pueue start [keys...] This command has three different behaviors, depending on if and what kind of keys are given:
1. If keys of paused processes are given, the processes will be started (SIGCONT), this happens even if the daemon is paused or if the max number of concurrent processes is reached.
2. If keys of queued or stashed processes are given, new processes will be spawned for those entries. This happens even if the daemon is paused or the max amount of processes is exceeded.
3. Otherwise the daemon will start to process the queue. This will start all paused processes (SIGCONT).
pueue pause [keys...] --wait This command has two different behaviors, depending on if keys are given:
1. If keys are given, pause the specified processes by sending a SIGSTOP.
2. Otherwise stop processing the queue and pause all running processes. If the --wait flag is set, the daemon will pause, but all running processes will finish on their own.
pueue restart [keys...] Enqueue the specified done or failed processes again.
pueue kill [keys...] -s [signal] -a This command tries to copy the behaviour of the Linux kill command. It will send a signal (default is sigterm) to the specified processes.

Available signals can be viewed with pueue kill -h under the -s flag.
Either an int 15, the full name sigterm/SIGTERM or the abbreviation term/TERM can be used.
Be aware that by default the signal will only be sent to the children of the shell process , i.e. if you send a sigint right after starting sleep 5 ; sleep 10 the sleep 5 process will be stopped and the sleep 10 will spawn afterwards.
To send the signal to the parent process as well you need to add the -a flag.
If keys are given, the signal will be send to the specified processes.
If no keys are given, send the signal to all running processes. If the signal is sigint, sigterm or sigkill the daemon will be paused.

pueue show --watch -k [key] Show the output of key or the oldest running process.
show --watch will continually show the stdout output of the subprocess in a curses session.
show without --watch will print the stderr as well. This can be useful if the subprocess prompts for user input (This is often piped to stderr).
pueue log [keys...] Print the output and status of all finished processes or of the specified finished processes.
pueue send [input] Send a string to the subprocess's stdin. In case a process prompts for user input, you can use this to interact with the subprocess.
The stdin pipe is flushed after every send command. To simulate a \n you need to add a newline in your string:
    pueue send 'y
    '

Configs
The configuration file of pueue is located in ~/.config/pueue/pueue.ini.
    [default]
    resumeAfterStart = False
    maxProcesses = 1
    customShell = default

    [log]
    logTime = 1209600



resumeAfterStart = False If you want pueue to instantly resume a queue from the last session, set this value to True.


maxProcesses = 1 Determines how many tasks should be processed concurrently.


customShell = default The path to the custom shell that should be used. Enter default for /bin/sh.
In case you use any other shell, every command will be launched from an interactive session.
I.e. [customShell, '-i', '-c', command].
This is useful if you want to use aliases or environment variables.
Warning!!!: This is kind of experimental and really hard to test! Be ready to encounter bugs!!


logTime = 1209600  Old logs will be deleted after the time specified in your config.


Logs
All logs can be found in ~/.shared/pueue/*.log. Logs of previous pueue sessions will be rotated and contain a timestamp in the name.
In case the daemon fails or something goes wrong, there is a separate log for the daemon at ~/.shared/pueue/daemon.log.
If the daemon crashes, please send the stack trace from this log!
Utils
Systemd
If you use systemd and don't install pueue with yaourt, place pueue.service in /etc/systemd/user/.
Afterwards every user can start/enable their own session with:
    systemctl --user start pueue.service
    systemctl --user enable pueue.service

ZSH Completion
Run make completion, which copies the completion file to /usr/share/zsh/site-functions/ or place _pueue in a folder, that is contained in your FPATH environment variable.
This script will be probably added to zsh-users/zsh-completions, when it is finished.
Pueue status querying script for i3pystatus
If you want to see the status of the last 4 entries in your status bar, just use the utils/pueuestatus.py script.
Libraries used
Regards to Robpol86 for providing the awesome terminaltables and colorclass libraries.
And thanks to thesharp for the extremely useful daemonize library.
Progress:
Pueue already works and is used frequently. There might be some small bugs, but I didn't encounter something serious in quite a while.
Copyright © 2016 Arne Beer (@Nukesor)
",126
rjust/defects4j,Perl,"Defects4J -- version 2.0.0 
Defects4J is a collection of reproducible bugs and a supporting infrastructure
with the goal of advancing software engineering research.
Contents of Defects4J
The projects
Defects4J contains 438 bugs from the following open-source projects:



Identifier
Project name
Number of bugs




Chart
JFreeChart
26


Closure
Closure compiler
176


Lang
Apache commons-lang
65


Math
Apache commons-math
106


Mockito
Mockito
38


Time
Joda-Time
27



The bugs
Each bug has the following properties:

Issue filed in the corresponding issue tracker, and issue tracker identifier
mentioned in the fixing commit message.
Fixed in a single commit
Minimized: the Defects4J maintainers manually pruned out
irrelevant changes in the commit (e.g., refactorings or feature additions).
Fixed by modifying the source code (as opposed to configuration files,
documentation, or test files).
A triggering test exists that failed before the fix and passes after the fix
-- the test failure is not random or dependent on test execution order.

The (b)uggy and (f)ixed program revisions are labelled with <id>b and
<id>f, respectively (<id> is an integer).
Setting up Defects4J
Requirements

Java 1.7
Git >= 1.9
SVN >= 1.8
Perl >= 5.0.10

Java version
All bugs have been reproduced and triggering tests verified, using the latest
version of Java 1.7.
Note that using Java 1.8+ might result in unexpected failing tests on a fixed
program version. The next major release of Defects4J will be compatible with
Java 8.
Perl dependencies
All required Perl modules are listed in cpanfile. On many Unix platforms,
these required Perl modules are installed by default. If this is not the case,
you can use cpan (or a cpan wrapper) to install them. For example, if you have
cpanm installed, you can automatically install all modules by running:
cpanm --installdeps .
Timezone
Defects4J generates and executes tests in the timezone America/Los_Angeles.
If you are using the bugs outside of the Defects4J framework, set the TZ
environment variable to America/Los_Angeles and export it.
Steps to set up Defects4J


Clone Defects4J:

git clone https://github.com/rjust/defects4j



Initialize Defects4J (download the project repositories and external libraries, which are not included in the git repository for size purposes and to avoid redundancies):

cd defects4j
./init.sh



Add Defects4J's executables to your PATH:

export PATH=$PATH:""path2defects4j""/framework/bin



Check installation:

defects4j info -p Lang



Using Defects4J
Example commands


Get information for a specific project (commons lang):

defects4j info -p Lang



Get information for a specific bug (commons lang, bug 1):

defects4j info -p Lang -b 1



Checkout a buggy source code version (commons lang, bug 1, buggy version):

defects4j checkout -p Lang -v 1b -w /tmp/lang_1_buggy



Change to the working directory, compile sources and tests, and run tests:

cd /tmp/lang_1_buggy
defects4j compile
defects4j test



The scripts in framework/test/
are examples of how to use Defects4J, which you might find useful
as inspiration when you are writing your own scripts that use Defects4J.


Command-line interface: defects4j command
Use framework/bin/defects4j to execute any of the following commands:



Command
Description




info
View configuration of a specific project or summary of a specific bug


checkout
Checkout a buggy or a fixed project version


compile
Compile sources and developer-written tests of a buggy or a fixed project version


test
Run a single test method or a test suite on a buggy or a fixed project version


mutation
Run mutation analysis on a buggy or a fixed project version


coverage
Run code coverage analysis on a buggy or a fixed project version


monitor.test
Monitor the class loader during the execution of a single test or a test suite


export
Export version-specific properties such as classpaths, directories, or lists of tests



Export version-specific properties
Use defects4j export -p <property_name> [-o output_file] in the working
directory to export a version-specific property:



Property
Description




classes.modified
Classes (source files) modified by the bug fix


cp.compile
Classpath to compile and run the project


cp.test
Classpath to compile and run the developer-written tests


dir.src.classes
Source directory of classes (relative to working directory)


dir.bin.classes
Target directory of classes (relative to working directory)


dir.src.tests
Source directory of tests (relative to working directory)


dir.bin.tests
Target directory of test classes (relative to working directory)


tests.all
List of all developer-written test classes


tests.relevant
List of relevant tests classes (a test class is relevant if, when executed, the JVM loads at least one of the modified classes)


tests.trigger
List of test methods that trigger (expose) the bug



Test execution framework
The test execution framework for generated test suites (framework/bin)
provides the following scripts:



Script
Description




defects4j
Main script, described above


run_bug_detection
Determine the real fault detection rate


run_mutation
Determine the mutation score


run_coverage
Determine code coverage ratios (statement and branch coverage)


run_evosuite
Generate test suites using EvoSuite


run_randoop
Generate test suites using Randoop



Mining and contributing additional bugs to Defects4J
The bug-mining README details the bug-mining process.
Additional resources
Scripts built on Defects4J
Fault localization (FL)

Scripts and annotations for evaluating FL techniques

Automated program repair (APR)

Scripts and annotations for evaluating APR techniques
Patches generated with the Nopol, jGenProg, and jKali APR systems
Repair actions and patterns for Defects4J v1.2.0

Publications


""Defects4J: A Database of Existing Faults to Enable Controlled Testing Studies for Java Programs""
René Just, Darioush Jalali, and Michael D. Ernst,
ISSTA 2014 [download].


""Are Mutants a Valid Substitute for Real Faults in Software Testing?""
René Just, Darioush Jalali, Laura Inozemtseva, Michael D. Ernst, Reid Holmes, and Gordon Fraser,
FSE 2014 [download].


More publications
Implementation details
Documentation for any script or module is available as
html documentation.
The directory structure of Defects4J is as follows:
defects4j
   |
   |--- project_repos:     The version control repositories of the provided projects.
   |
   |--- major:             The Major mutation framework.
   |
   |--- framework:         Libraries and executables of the database abstraction and
       |                   test execution framework.
       |
       |--- bin:           Command line interface to Defects4J.
       |
       |--- core:          The modules of the core framework.
       |
       |--- lib:           Libraries used in the core framework.
       |
       |--- util:          Util scripts used by Defects4J.
       |
       |--- projects:      Project-specific resource files.
       |
       |--- test:          Scripts to test the framework.

Versioning information
Defects4J uses a semantic versioning scheme (major.minor.patch):



Change
major
minor
patch




Addition/Deletion of bugs
X




New/upgraded internal or external tools

X



Fixes and documentation changes


X



License
MIT License, see license.txt for more information.
",190
spaceshipearth/pyspaceship,Python,"PySpaceship
A python implementation of Spaceship Earth.
Installation
We are expecting that you already have pyenv and pyenv-virtualenvwrapper installed.
If you haven't done that already, see how Igor does it in his dotfiles.
If you are on a Mac
Install Homebrew
brew install pyenv
brew install pyenv-virtualenvwrapper

Add the following to your shell initialization config (e.g. ~/.bash_profile, ~/.zshrc, etc.)
export PYENV_VIRTUALENVWRAPPER_PREFER_PYVENV=""true""
if command -v pyenv 1>/dev/null 2>&1
then
  eval ""$(pyenv init -)""
  pyenv virtualenvwrapper
fi

Run the following to install a couple Python versions:
pyenv install 2.7.14
pyenv install 3.7.2

(Note: on macOS X Mojave you may have to run these commands as follows: CFLAGS=""-I$(xcrun --show-sdk-path)/usr/include"" pyenv install .... See https://github.com/pyenv/pyenv/issues/1219 for details)
From the project directory, run the following command:
cp .python-version ~

Follow these instructions to get Docker up and running: https://stackoverflow.com/a/43365425
Set up virtualenv
Create the virtualenv:
$ mkvirtualenv pyspaceship

Then, install some basic tooling:
$ pip install -r requirements.txt

Now, you can see all the tasks we have defined:
$ inv --list

Running Locally
Run the website like so:
$ inv run.flask

To run via the gunicorn server, you can do:
$ inv run.gunicorn

The site will be accessible on localhost port 9876 (it's a countdown!).
Sending email locally
Create a file called sendgrid.key and place the sendgrid secret inside on a single line by itself
MySQL
This app expects a MySQL database on port 9877.
If you have docker-compose set up, you can bring one up like so:
$ inv run.mysql

Migrations
First, you need to create the migration.
You can do this through migration-prep:
$ inv run.migration-prep --name ""my migration description""

This will generate your migration in migrations/versions.
You should edit this migration and commit it to the git repo.
Next, you can run your migration:
$ inv run.upgrade

If you need to back down again, run inv run.downgrade.
In production
You need kubectl set up to talk to our K8S cluster.
You might need the following:
$ gcloud components install kubectl
$ gcloud container clusters get-credentials default --region us-central1-a
$ gcloud auth configure-docker
Run this just to make sure you're set up to talk to the cluster:
$ kubectl get nodes
Building an image
Run inv image.build
Manual steps in production
You have to do these by hand:

set up a mysql database

give it a spaceship-app account
create a spaceship database


create an ssl cert

",2
spaceshipearth/pyspaceship,Python,"PySpaceship
A python implementation of Spaceship Earth.
Installation
We are expecting that you already have pyenv and pyenv-virtualenvwrapper installed.
If you haven't done that already, see how Igor does it in his dotfiles.
If you are on a Mac
Install Homebrew
brew install pyenv
brew install pyenv-virtualenvwrapper

Add the following to your shell initialization config (e.g. ~/.bash_profile, ~/.zshrc, etc.)
export PYENV_VIRTUALENVWRAPPER_PREFER_PYVENV=""true""
if command -v pyenv 1>/dev/null 2>&1
then
  eval ""$(pyenv init -)""
  pyenv virtualenvwrapper
fi

Run the following to install a couple Python versions:
pyenv install 2.7.14
pyenv install 3.7.2

(Note: on macOS X Mojave you may have to run these commands as follows: CFLAGS=""-I$(xcrun --show-sdk-path)/usr/include"" pyenv install .... See https://github.com/pyenv/pyenv/issues/1219 for details)
From the project directory, run the following command:
cp .python-version ~

Follow these instructions to get Docker up and running: https://stackoverflow.com/a/43365425
Set up virtualenv
Create the virtualenv:
$ mkvirtualenv pyspaceship

Then, install some basic tooling:
$ pip install -r requirements.txt

Now, you can see all the tasks we have defined:
$ inv --list

Running Locally
Run the website like so:
$ inv run.flask

To run via the gunicorn server, you can do:
$ inv run.gunicorn

The site will be accessible on localhost port 9876 (it's a countdown!).
Sending email locally
Create a file called sendgrid.key and place the sendgrid secret inside on a single line by itself
MySQL
This app expects a MySQL database on port 9877.
If you have docker-compose set up, you can bring one up like so:
$ inv run.mysql

Migrations
First, you need to create the migration.
You can do this through migration-prep:
$ inv run.migration-prep --name ""my migration description""

This will generate your migration in migrations/versions.
You should edit this migration and commit it to the git repo.
Next, you can run your migration:
$ inv run.upgrade

If you need to back down again, run inv run.downgrade.
In production
You need kubectl set up to talk to our K8S cluster.
You might need the following:
$ gcloud components install kubectl
$ gcloud container clusters get-credentials default --region us-central1-a
$ gcloud auth configure-docker
Run this just to make sure you're set up to talk to the cluster:
$ kubectl get nodes
Building an image
Run inv image.build
Manual steps in production
You have to do these by hand:

set up a mysql database

give it a spaceship-app account
create a spaceship database


create an ssl cert

",2
textlint-ja/textlint-rule-preset-japanese,JavaScript,"textlint-rule-preset-japanese 
textlint rule preset for Japanese.
日本語向けのtextlint ruleのpresetです。
目的

一般的な文書で利用するためのルール集
入れても誤検知が少ないルールに限定する

明らかな誤爆がある場合は、Issueを立ててください


スタイル(スペースの有無など)に関連するルールは含めない

技術的な文章を書くためにより厳しいルールを求める方は次のプリセットを参照してください。

textlint-ja/textlint-rule-preset-ja-technical-writing: 技術文書向けのtextlintルールプリセット

Installation
npm install textlint-rule-preset-japanese

このpresetは以下のルールを含んでいます。

https://github.com/textlint-ja/textlint-rule-max-ten

一文で使える""、""の数


https://github.com/takahashim/textlint-rule-no-doubled-conjunctive-particle-ga

逆接の接続助詞「が」が、同一文中に複数回出現していないかどうか


https://github.com/takahashim/textlint-rule-no-doubled-conjunction

同じ接続詞で開始されていることを検出


https://github.com/textlint-ja/textlint-rule-no-double-negative-ja

二重否定の検出


https://github.com/textlint-ja/textlint-rule-no-doubled-joshi

二重助詞の検出


https://github.com/azu/textlint-rule-sentence-length

一文の最大の長さ


https://github.com/azu/textlint-rule-no-dropping-the-ra

ら抜き言葉を使用し


https://github.com/textlint-ja/textlint-rule-no-mix-dearu-desumasu

文の敬体(ですます調)、常体(である調)の混合をチェック


https://github.com/azu/textlint-rule-no-nfd

ホ゜ケット エンシ゛ン のような、Mac OS XでPDFやFinderからのコピペで発生する濁点のチェック


https://github.com/textlint-rule/textlint-rule-no-invalid-control-character

制御文字の検出



Usage
Via .textlintrc
{
    ""rules"": {
        ""preset-japanese"": true
    }
}
Options
{
    ""rules"": {
        // それぞれのルールのデフォルト値
        ""preset-japanese"": {
             // https://github.com/textlint-ja/textlint-rule-max-ten
             // 一文で使える""、""の数
             ""max-ten"": {
                 ""max"": 3
             },
             // https://github.com/takahashim/textlint-rule-no-doubled-conjunctive-particle-ga
             // 逆接の接続助詞「が」が、同一文中に複数回出現していないかどうか
             // e.g.) 今日は早朝から出発したが、定刻には間に合わなかったが、無事会場に到着した。
             ""no-doubled-conjunctive-particle-ga"": true,
             // https://github.com/takahashim/textlint-rule-no-doubled-conjunction
             // 同じ接続詞が連続して出現していないかどうか
             ""no-doubled-conjunction"": true,
             // https://github.com/textlint-ja/textlint-rule-no-double-negative-ja
             // 二重否定の検出
             ""no-double-negative-ja"": true,
             // https://github.com/textlint-ja/textlint-rule-no-doubled-joshi
             // 二重助詞の検出
             // 連続して同じ助詞が出た場合のみを検出
             ""no-doubled-joshi"": {
                 ""min_interval"": 1
             },
             // https://github.com/azu/textlint-rule-sentence-length
             // 一文の最大の長さ
             ""sentence-length"": {
                 ""max"": 100
             },
             // https://github.com/textlint-ja/textlint-rule-no-dropping-the-ra
             // ら抜き言葉を使用しない
             ""no-dropping-the-ra"": true,
             // https://github.com/azu/textlint-rule-no-mix-dearu-desumasu
             // 文の敬体(ですます調)、常体(である調)のチェック
             ""no-mix-dearu-desumasu"": true,
             // https://github.com/azu/textlint-rule-no-nfd
             // ホ゜ケット エンシ゛ン
             // のような、Mac OS XでPDFやFinderからのコピペで発生する濁点のチェック
             ""no-nfd"": true,
             // https://github.com/textlint-rule/textlint-rule-no-invalid-control-character
             // 制御文字の検出
             ""no-invalid-control-character"": true
        }
    }
}
Semantic Versioning Policy
次のルールでバージョンが更新されます。

Patch リリース

各ルールのバグ修正 (警告を減らす方向への修正)
ドキュメントの改善
内部的な変更 (リファクタリングやテストの改善など)
リリース失敗時の再リリース


Minor リリース

各ルールのバグ修正 (警告を増やす方向への修正)
新オプションの追加
既存ルールの非推奨化


Major リリース

プリセットへのルールの追加
プリセットからルールの削除
既存のオプション値の変更



更新内容はReleases pageを参照してください。
Community
質問は以下のGitterでお願いします。

Contributing

Fork it!
Create your feature branch: git checkout -b my-new-feature
Commit your changes: git commit -am 'Add some feature'
Push to the branch: git push origin my-new-feature
Submit a pull request :D

License
MIT
",26
sangggho/openwi,Python,"OpenWIPy
Open Welding Inspector for Python
",4
mist64/cbmbus_doc,HTML,"""Commodore Peripheral Bus"" Article Series
This repository maintains the history of the ""Commodore Peripheral Bus"" article series at pagetable.com.
I am releasing one part every week, at which time links will be added to the bullet points below. The articles will also be announced on my Twitter account @pagetable and my Mastodon account @pagetable@mastodon.social.

Part 0: Overview and Introduction
Part 1: IEEE-488 [PET/CBM Series; 1977]
Part 2: The TALK/LISTEN Layer
Part 3: The Commodore DOS Layer
Part 4: Standard Serial (IEC) [VIC-20, C64; 1981]
Part 5: TCBM [C16, C116, Plus/4; 1984] (coming soon)
Part 6: JiffyDOS [1985] (coming soon)
Part 7: Fast Serial [C128; 1986] (coming soon)
Part 8: CBDOS [C65; 1991] (coming soon)

Formats

Texts are written in Markdown syntax and converted to HTML using Discount GFM (through Marked 2).
Images are done in Keynote. The script ""convert.sh"" uses ImageMagick to convert/crop the Keynote-exported PNG files into the final PNG/GIF files.

Contributing
Corrections, clarifications and additions are highly appreciated. I will regularly update the articles on pagetable.com.
Credits
The author is Michael Steil, mist64@mac.com
",8
duxingzhe/The-Economist-Audio-Downloader,C,"The Economist Audio Downloader
Provide an app for advanced english learners.
You can download the Audio Edition of The Economists.
The target users are CATTI, Unified National Graduate Entrance Examination in China, MTI, IETLS and TOEFL/GRE etc.
核心库来自WlMedia，由ywl5320提供。在此表示衷心的感谢。
Splash

Input

Download

Add Files

Listening

",10
Unponderable/NGU-Rebirth-Script,AutoHotkey,"NGU Rebirth Script
Rebirth and Challenge AutoHotKey script for NGU Idle
Written by Unponderable and Tatsumasa
Also thanks to EvoGeek, the genesis of much of this script.
Come visit us in Discord: https://discord.gg/5revMxD
Requirements to fully use this script
The Script uses AutoHotKey. Must be downloaded and installed!
Must have 1-energy-capped all Basic Training skills
Must have unlocked: Wandoos, NGUs, Yggdrasil, ITOPOD
Must have purchased with XP: Training Auto Advance, 2+ Loadout Slots, Custom Energy Button 1
Must have puchased with AP: Insta Training Cap (10,000 AP)
Must use the Normal (default) UI theme
I strongly suggest having Auto Boost, Auto Merge, and Advanced Loot Filter, as the script does not (currently) do inventory management.
Setup

If you're doing regular runs, have stats/number sufficient to sustainably kill up through Boss 37 within the first few seconds of your rebirth. For regular runs, the script assumes you have blood magic unlocked from the get-go.
1a. If you're doing challenge runs, have stats sufficient to kill Boss 37 within a couple short rebirths.
Setup beards as desired. Beards will not be touched in this script. Magic beards are more useful for this scripts' purposes.
Set Loadout 1 to be your general run gear. This will be what you wear for most of your run(s).
Set Loadout 2 to be heavy in gold find (without sacrificing too much power/toughness). Used only for the first few kills for time machine purposes.
Ensure all Augmentation/Time Machine targets are set to zero.
5a. If you're doing a long run, ensure advanced training limits are set to appropriate limits. Long runs will try to boost adv training wandoos speed.
Change in-game setting ""Unassign E/M on Loadout Swap?"" to YES and ""Check for Updates"" to OFF.
Make a manual save, just in case stuff doesn't work.
Load script and select your run choice with the menu, changing settings beforehand if desired.
Quit at anytime with Escape.

FAQs
Q: I can't sustain 3-minute rebirths above boss 37 and therefore challenge runs don't work! What do I do?
A: Come back when you have more stats or change the challenge run sequence to use longer duration runs.
Q: HALP. It's giving me an error about the game not being visible/not finding TopLeft.png!
A: The script attempts to match the image TopLeft.png with the top left corner of the game screen. Make sure NGU idle is fully visible on the screen. Make sure the script is being run in the same file directory as the images that it came with. If you're still having issues, you may have an incompatible aspect ratio or scaling factor. For reference, the script was tested in Chrome and Firefox at 100% zoom on a 1920x1080 resolution monitor.  Check to make sure your DPI is set to 100%. Try matching that, if possible. If that doesn't work, I don't really know how to fix it, sorry :-/
Q: Why does the script come to a halt when Iron Pill is active?
A: The script waits 5 minutes for blood to accumulate before casting Iron Pill.
",7
signalapp/Signal-Desktop,JavaScript,"

Signal Desktop
Signal Desktop is an Electron application that links with Signal
on Android
or iOS.
Install the production version: https://signal.org/download/
Install the beta
You can install the beta version of Signal Desktop alongside the production version. The beta uses different data and install locations.

Windows: First, download this file and look for the url property that specifies the location for the latest beta installer. Download the installer by constructing a final URL that looks like this: https://updates.signal.org/desktop/<installer location>. Then run the installer.
macOS: First, download this file and look for the url property that specifies the location for the latest beta installer. Download the installer by constructing a final URL that looks like this: https://updates.signal.org/desktop/<package location>. Then unzip that package and copy the .app file into the /Applications folder using Finder.
Linux: Follow the production instructions to set up the APT repository and run apt install signal-desktop-beta.

Got a question?
You can find answers to a number of frequently asked questions on our support site.
The community forum is another good place for questions.
Found a Bug? Have a feature request?
Please search for any existing issues that describe your bug in order to avoid duplicate submissions.
Contributing Translations
Interested in helping to translate Signal? Contribute here:
https://www.transifex.com/projects/p/signal-desktop
Contributing Code
Please see CONTRIBUTING.md
for setup instructions and guidelines for new contributors. Don't forget to sign the CLA.
Contributing Funds
You can donate to Signal development through the Freedom of the Press Foundation.
Cryptography Notice
This distribution includes cryptographic software. The country in which you currently reside may have restrictions on the import, possession, use, and/or re-export to another country, of encryption software.
BEFORE using any encryption software, please check your country's laws, regulations and policies concerning the import, possession, or use, and re-export of encryption software, to see if this is permitted.
See http://www.wassenaar.org/ for more information.
The U.S. Government Department of Commerce, Bureau of Industry and Security (BIS), has classified this software as Export Commodity Control Number (ECCN) 5D002.C.1, which includes information security software using or performing cryptographic functions with asymmetric algorithms.
The form and manner of this distribution makes it eligible for export under the License Exception ENC Technology Software Unrestricted (TSU) exception (see the BIS Export Administration Regulations, Section 740.13) for both object code and source code.
License
Copyright 2014-2019 Open Whisper Systems
Licensed under the GPLv3: http://www.gnu.org/licenses/gpl-3.0.html
",4509
pvtl/voyager-page-blocks,PHP,"Voyager Page Blocks

This Laravel/Voyager Frontend module is designed to give developers the ability to easily design page blocks, for Voyager admin users to build stunning frontend pages.
Built by Pivotal Agency.

Prerequisites

Composer Installed
Install Laravel
Install Voyager
Install Voyager Frontend


Installation
# 1. Require this Package in your fresh Laravel/Voyager project
composer require pvtl/voyager-page-blocks

# 2. Run the Installer
php artisan voyager-page-blocks:install

# 3. (Optional) Seed the database with example page blocks.
php artisan voyager-page-blocks:seed

Creating & Modifying Blocks
Page blocks are created & configured in 2 steps:

Define the block - in /config/page-blocks.php
Build the block's HTML layout - create the template in /resources/views/vendor/voyager-page-blocks/blocks

1. Define a Block
Familiarize yourself with /config/page-blocks.php. This is where you'll define each block - you'll tell it which fields the block should have (for the admin to manage) and which Blade template it should use on the frontend.

Each array inside this configuration file is a page block
Each block contains fields
Each field contains a unique field key
Each field is based on a Voyager Data Type

The below table explains what each property does and how it is relevant to the block itself:



Key
Purpose




Root key
This is the name of your page block, used to load the configuration


name
This is the display name of your page block, used in the block 'adder'


fields
This is where your page block fields live (text areas, images etc)


fields => field
The content name of your field, used to store/load its content


fields => display_name
The display name of this field in the back-end


fields => partial
The partial that this field will use (check TCG\Voyager\FormFields)


fields => required
Self-explanatory, marks this field as required or not (not available for all partials)


fields => placeholder
Self-explanatory, adds a placeholder to the field (not available for all partials)


fields => options
Used for selects/checkboxes/radios to supply options


template
This points to your blade file for your block template


compatible
TBA



2. Build the HTML
When you're ready to start structuring the display of your block, you'll need to create (or override our defaults) your blade template (located at /resources/views/vendor/voyager-page-blocks/blocks/your_block.blade.php) and use the accessors you defined in your module's configuration file to fetch each fields data ({!! $blockData->image_content !!}).

Example. Putting it all together
Let's say we want to create a new block with 1 WYSIWYG editor, called 'Company Overview'.
Step 1. Define the new block
In /config/page-blocks.php, we'll add:
$blocks['company_overview'] = [
    'name' => 'Company Overview',
    'template' => 'voyager-page-blocks::blocks.company_overview',
    'fields' => [
        'content' => [
            'field' => ""content"",
            'display_name' => ""Company Overview Content"",
            'partial' => 'voyager::formfields.rich_text_box',
            'required' => 1,
            'placeholder' => '<p>Lorem ipsum dolor sit amet. Nullam in dui mauris.</p>',
        ],
    ],
];
Step 2. Build the HTML
In /resources/views/vendor/voyager-page-blocks/blocks, we'll create a new file called company_overview.blade.php with:
<div class=""page-block"">
    <div class=""grid-container column text-center"">
        {!! $blockData->content !!}
    </div>
</div>
Step 3. Add the block to a page
Next, jump into the Voyager Admin > Pages and click 'Content' next to a page. You'll now be able to select Company Overview from the 'Add Block' section. Add the block to the page, drag/drop it into place, edit the text etc.

Developer Controller Blocks
You may also wish to include custom logic and functionality into your page blocks. This can be done with a Developer Controller Block - simply specify your controller namespace'd path and the method you wish to call, which should return a view and you'll be on your way.
For example, the Voyager Frontend package comes with a Recent Posts method/view that you can play with and review.
From the Add Block section of the page in the admin, add the block type of Developer Controller, then input the following into the path field:
Pvtl\VoyagerFrontend\Http\Controllers\PostController::recentBlogPosts(2)

This will output 2 blog posts on the frontend. You could change the first paramter of the method to 6, to output 6 blog posts. Simples.

Troubleshooting
It is important to sanitise your field output, null values will cause errors.
It is very important that you follow the naming scheme that is setup in the example page blocks as the keys reference other cogs in the system to stitch the blocks together. There are example blocks already set up in the resources/views directory and configuration file for you to get started.
",40
Berserk-Games/Tabletop-Simulator-Knowledge-Base,HTML,"Tabletop Simulator Knowledge Base
This is the source of the knowledge base in Tabletop Simulator. It uses a modified version of Material-Design for MKDocs.
How it Works
The .md files in the /docs folder are written in Markdown, which is an easy-to-use markup language to add formatting to text. Additionally, some custom CSS is used, as well as a handful of custom images. When making changes, it is possible to live-preview them as you go if you have set up the local files for mkdocs + material design.
Alternatively, you can make modifications to individual pages then submit them for review. The developers will always be the ones to build and publish the site anyway, all you will do is modify the contents of this Git.
Installing
If you choose to install MKDocs so you can live-preview your changes, you may do so by following these instructions.
Otherwise, you will not need to install anything to edit the text files.
",4
signalapp/Signal-Desktop,JavaScript,"

Signal Desktop
Signal Desktop is an Electron application that links with Signal
on Android
or iOS.
Install the production version: https://signal.org/download/
Install the beta
You can install the beta version of Signal Desktop alongside the production version. The beta uses different data and install locations.

Windows: First, download this file and look for the url property that specifies the location for the latest beta installer. Download the installer by constructing a final URL that looks like this: https://updates.signal.org/desktop/<installer location>. Then run the installer.
macOS: First, download this file and look for the url property that specifies the location for the latest beta installer. Download the installer by constructing a final URL that looks like this: https://updates.signal.org/desktop/<package location>. Then unzip that package and copy the .app file into the /Applications folder using Finder.
Linux: Follow the production instructions to set up the APT repository and run apt install signal-desktop-beta.

Got a question?
You can find answers to a number of frequently asked questions on our support site.
The community forum is another good place for questions.
Found a Bug? Have a feature request?
Please search for any existing issues that describe your bug in order to avoid duplicate submissions.
Contributing Translations
Interested in helping to translate Signal? Contribute here:
https://www.transifex.com/projects/p/signal-desktop
Contributing Code
Please see CONTRIBUTING.md
for setup instructions and guidelines for new contributors. Don't forget to sign the CLA.
Contributing Funds
You can donate to Signal development through the Freedom of the Press Foundation.
Cryptography Notice
This distribution includes cryptographic software. The country in which you currently reside may have restrictions on the import, possession, use, and/or re-export to another country, of encryption software.
BEFORE using any encryption software, please check your country's laws, regulations and policies concerning the import, possession, or use, and re-export of encryption software, to see if this is permitted.
See http://www.wassenaar.org/ for more information.
The U.S. Government Department of Commerce, Bureau of Industry and Security (BIS), has classified this software as Export Commodity Control Number (ECCN) 5D002.C.1, which includes information security software using or performing cryptographic functions with asymmetric algorithms.
The form and manner of this distribution makes it eligible for export under the License Exception ENC Technology Software Unrestricted (TSU) exception (see the BIS Export Administration Regulations, Section 740.13) for both object code and source code.
License
Copyright 2014-2019 Open Whisper Systems
Licensed under the GPLv3: http://www.gnu.org/licenses/gpl-3.0.html
",4509
pvtl/voyager-page-blocks,PHP,"Voyager Page Blocks

This Laravel/Voyager Frontend module is designed to give developers the ability to easily design page blocks, for Voyager admin users to build stunning frontend pages.
Built by Pivotal Agency.

Prerequisites

Composer Installed
Install Laravel
Install Voyager
Install Voyager Frontend


Installation
# 1. Require this Package in your fresh Laravel/Voyager project
composer require pvtl/voyager-page-blocks

# 2. Run the Installer
php artisan voyager-page-blocks:install

# 3. (Optional) Seed the database with example page blocks.
php artisan voyager-page-blocks:seed

Creating & Modifying Blocks
Page blocks are created & configured in 2 steps:

Define the block - in /config/page-blocks.php
Build the block's HTML layout - create the template in /resources/views/vendor/voyager-page-blocks/blocks

1. Define a Block
Familiarize yourself with /config/page-blocks.php. This is where you'll define each block - you'll tell it which fields the block should have (for the admin to manage) and which Blade template it should use on the frontend.

Each array inside this configuration file is a page block
Each block contains fields
Each field contains a unique field key
Each field is based on a Voyager Data Type

The below table explains what each property does and how it is relevant to the block itself:



Key
Purpose




Root key
This is the name of your page block, used to load the configuration


name
This is the display name of your page block, used in the block 'adder'


fields
This is where your page block fields live (text areas, images etc)


fields => field
The content name of your field, used to store/load its content


fields => display_name
The display name of this field in the back-end


fields => partial
The partial that this field will use (check TCG\Voyager\FormFields)


fields => required
Self-explanatory, marks this field as required or not (not available for all partials)


fields => placeholder
Self-explanatory, adds a placeholder to the field (not available for all partials)


fields => options
Used for selects/checkboxes/radios to supply options


template
This points to your blade file for your block template


compatible
TBA



2. Build the HTML
When you're ready to start structuring the display of your block, you'll need to create (or override our defaults) your blade template (located at /resources/views/vendor/voyager-page-blocks/blocks/your_block.blade.php) and use the accessors you defined in your module's configuration file to fetch each fields data ({!! $blockData->image_content !!}).

Example. Putting it all together
Let's say we want to create a new block with 1 WYSIWYG editor, called 'Company Overview'.
Step 1. Define the new block
In /config/page-blocks.php, we'll add:
$blocks['company_overview'] = [
    'name' => 'Company Overview',
    'template' => 'voyager-page-blocks::blocks.company_overview',
    'fields' => [
        'content' => [
            'field' => ""content"",
            'display_name' => ""Company Overview Content"",
            'partial' => 'voyager::formfields.rich_text_box',
            'required' => 1,
            'placeholder' => '<p>Lorem ipsum dolor sit amet. Nullam in dui mauris.</p>',
        ],
    ],
];
Step 2. Build the HTML
In /resources/views/vendor/voyager-page-blocks/blocks, we'll create a new file called company_overview.blade.php with:
<div class=""page-block"">
    <div class=""grid-container column text-center"">
        {!! $blockData->content !!}
    </div>
</div>
Step 3. Add the block to a page
Next, jump into the Voyager Admin > Pages and click 'Content' next to a page. You'll now be able to select Company Overview from the 'Add Block' section. Add the block to the page, drag/drop it into place, edit the text etc.

Developer Controller Blocks
You may also wish to include custom logic and functionality into your page blocks. This can be done with a Developer Controller Block - simply specify your controller namespace'd path and the method you wish to call, which should return a view and you'll be on your way.
For example, the Voyager Frontend package comes with a Recent Posts method/view that you can play with and review.
From the Add Block section of the page in the admin, add the block type of Developer Controller, then input the following into the path field:
Pvtl\VoyagerFrontend\Http\Controllers\PostController::recentBlogPosts(2)

This will output 2 blog posts on the frontend. You could change the first paramter of the method to 6, to output 6 blog posts. Simples.

Troubleshooting
It is important to sanitise your field output, null values will cause errors.
It is very important that you follow the naming scheme that is setup in the example page blocks as the keys reference other cogs in the system to stitch the blocks together. There are example blocks already set up in the resources/views directory and configuration file for you to get started.
",40
Berserk-Games/Tabletop-Simulator-Knowledge-Base,HTML,"Tabletop Simulator Knowledge Base
This is the source of the knowledge base in Tabletop Simulator. It uses a modified version of Material-Design for MKDocs.
How it Works
The .md files in the /docs folder are written in Markdown, which is an easy-to-use markup language to add formatting to text. Additionally, some custom CSS is used, as well as a handful of custom images. When making changes, it is possible to live-preview them as you go if you have set up the local files for mkdocs + material design.
Alternatively, you can make modifications to individual pages then submit them for review. The developers will always be the ones to build and publish the site anyway, all you will do is modify the contents of this Git.
Installing
If you choose to install MKDocs so you can live-preview your changes, you may do so by following these instructions.
Otherwise, you will not need to install anything to edit the text files.
",4
superdesk/superdesk-planning,JavaScript,"Superdesk Planning
Sept 2016, Berlin


Overview
This is a plugin for superdesk.
It allows to ingest and manage events, to create planning within agenda, and to link coverages to them.
Configure Superdesk
In order for Superdesk to expose the Planning module, you must configure it in both the client and server config files.
Client
Add the dependency to your instance of superdesk.
In superdesk/client/package.json add superdesk-planning to the dependencies
(replacing #a79d428 with the specific commit you require):
""dependencies"": {
    ....,
    ""superdesk-planning"": ""superdesk/superdesk-planning#a79d428""
}
Don't forget to add planning to your superdesk config in superdesk/client/superdesk.config.js, and
to enable the planning feature:
apps: [
    ....,
    'superdesk-planning'
],
features: {
    ....,
    planning: true
},
workspace: {
    ....,
    planning: true,
    assignments: true
}
If you have importApps in your superdesk config, you should also add planning to this list:
importApps: [
    ....,
    'superdesk-planning'
]
This will import the superdesk-planning node module and load the superdesk.planning angular module in the main angular application.
Server
Add the dependency to your instance of superdesk.
In superdesk/server/requirements.txt add superdesk-planning to the dependencies
(replacing @a5b14c23e with the specific commit you require):
git+git://github.com/superdesk/superdesk-planning.git@a5b14c23e#egg=superdesk-planning

Last thing you need to configure is to add planning to the list of installed apps.
In superdesk/server/settings.py add the following:
INSTALLED_APPS.extend([
    ....,
    'planning'
])
Celery Task: Expire Items
There is a Celery Task to expire items after a configured amount of time.
In your settings.py, configure CELERY_TASK_ROUTES, CELERY_BEAT_SCHEDULE
and PLANNING_EXPIRY_MINUTES:
CELERY_TASK_ROUTES['planning.flag_expired'] = {
    'queue': celery_queue('expiry'),
    'routing_key': 'expiry.planning'
}

CELERY_BEAT_SCHEDULE['planning:expiry'] = {
    'task': 'planning.flag_expired',
    'schedule': crontab(minute='0')
}

PLANNING_EXPIRY_MINUTES = 4320 # default is 0

The above example config will run the Celery Task once every hour,
flagging items as expired after 3 days from the scheduled date.
If PLANNING_EXPIRY_MINUTES = 0, then no item will be flagged as expired (effectively disabling this feature)
There is also a manage.py command so that you can manually run this task.
python manage.py planning:flag_expired

Celery Task: Expire Items
This is a Celery Task to delete spiked planning items, associated assignments and events after a configured amount of time.
Settings are very similar to ""planning:flag_expired"" task
In your settings.py, configure this task as follows using the variable PLANNING_DELETE_SPIKED_MINUTES:
CELERY_TASK_ROUTES['planning.delete_spiked'] = {
    'queue': celery_queue(''),
    'routing_key': 'delete.planning'
}

CELERY_BEAT_SCHEDULE['planning:delete'] = {
    'task': 'planning.delete_spiked',
    'schedule': crontab(minute='0')
}

PLANNING_DELETE_SPIKED_MINUTES = 4320 # default is 0

The above example config will run the Celery Task once every hour,
deleting spiked items after 3 days from the scheduled date.
If PLANNING_EXPIRY_MINUTES = 0, then no item will be deleted
There is also a manage.py command so that you can manually run this task.
python manage.py planning:delete_spiked

Install for Production/Testing
Installing Superdesk-Planning for production or test environments is as easy as running the following:
cd superdesk/client
npm install
cd ../server
pip install -r requirements.txt
cd ../..

Install for Development
First you will need to clone the repo from GitHub.
In the root folder where your current superdesk folder is, run the following:
git clone git@github.com:superdesk/superdesk-planning.git

Client
Running the following will link the superdesk-planning module in development mode:
cd superdesk/client
npm install
npm link ../../superdesk-planning
cd ../..

Server
Run the following to install the python module in development mode:
cd superdesk/server
pip install -r requirements.txt
cd ../../superdesk-planning
pip install -e .
cd ..

Running Tests
To run the same tests that is used in Travis, run the following:
cd superdesk-planning
make test
cd ..

Or you can run them individually as below.
Client
Code Style
cd superdesk-planning
npm run hint
cd ..

Unit Tests
cd superdesk-planning
npm run unit_test
cd ..

Coverage Report
cd superdesk-planning
npm run coveralls
cd ..

Server
Code Style
cd superdesk-planning/server
flake8
cd ../..

Unit Tests
cd superdesk-planning/server
nosetests -v
cd ../..

Behaviour Tests
cd superdesk-planning/server
behave
cd ../..

",8
Nukesor/sticker-finder,Python,"Sticker Finder (@stfi_bot)





This telegram bot allows you to search your favorite stickers and discover new ones via inline query (just like @gif).
The inline query filters by looking at custom tags, detected text, emojis, sticker set name and title.
There already are about 4100 searchable sticker sets with over 230k stickers. And in case the bot is missing some, you can easily add your own precious sets.
Sticker finder is quite fast (about 0.1 sec for each search), supports custom tagging and features fuzzy searching!
There are several ways to conveniently tag stickers: You can tag either in a direct telegram conversation or on the fly in any other chat.
I even perform basic text recognition on all stickers to allow a nice sticker search by text (even though this doesn't work perfectly all the time).
And in case you get bored, you can go ahead and do some good by tagging a few random stickers with the /tag_random functionality.




Feel free to host your own or to use mine on telegram: @stfi_bot
Features:

Inline query search by tags, text, emoji, sticker set name and title.
A dedicated search for sticker packs. Just add pack or set to your search e.g. @stfi_bot kermit set.
Fuzzy searching to match similar words or typos.
Individual search results (If you use a sticker often you will see it on top.)
Tagging of single stickers or whole sets.
NSFW and Furry filter. Use nsfw or fur tag to explicitly search for this stuff.
Sticker set addition in direct conversations or when the bot is added to groups.
Random tagging of stickers if you're bored.
/report [reason] Report a sticker set for some reason.
A minimalistic mode for groups that don't want user interaction (I already host one with the name @stfil_bot).

Available commands:
/help       Display help
/tag [tags] Tag the last sticker posted in this chat or the sticker you replied to
/replace [tags] Replace the tags of the last sticker posted in this chat or the sticker you replied to
/tag_random Get a random sticker for tagging. Crowdsourcing :). Thanks for everybody doing this :)!
/random_set Get a random sticker set.
/skip       Skip the current sticker
/cancel     Cancel all current tag actions
/english    Only english sticker sets and tags
/international Get sticker sets from all all languages.
/toggle_deluxe Only get the very best sticker packs. (Restricts the search drastically)
/forget_set Forget all usages of all stickers of a specific sticker pack. The sticker pack is determined by the latest sticker in this chat.

Installation and starting:


You will need to install poetry to install all dependencies.


Clone the repository:
 % git clone git@github.com:nukesor/stickerfinder && cd stickerfinder



Now copy the stickerfinder/config.example.py to stickerfinder/config.py and adjust all necessary values.


Finally execute following commands to install all dependencies and to start the bot:
% poetry install
% poetry run initdb.py
% poetry run main.py


When you are updating from a previous version just execute poetry run alembic upgrade head.


Botfather commands
help - Display help
tag - Tag the last sticker posted in this chat or the sticker you replied to
replace - Replace the tags of the last sticker posted in this chat or the sticker you replied to
tag_random - Get a random sticker for tagging. Thanks for doing this :)!
random_set - The bot sends you a random sticker set.
skip - Skip the current sticker
report - Report something regarding the sticker set of the last sticker in the chat.
cancel - Cancel all current actions.
english - Only english sticker sets and tags
international - Get sticker sets from all all languages.
toggle_deluxe - Only get the very best sticker packs. (Restricts the search drastically)
forget_set - Forget all usages of a sticker pack.
donations - Show information on donations

Dev commands
This is just for people hosting a bot and those with admin permissions:
/ban Ban the last sticker posted in this chat.
/unban Unban the last sticker posted in this chat.
/ban_user [name|id] Ban a user
/unban_user [name|id] Unban a user
/tasks Start to process tasks in a maintenance chat
/make_admin [name|id] Make another user admin

/delete_set [name] Completely delete a set
/show [file_id]` Command sends the corresponding sticker for the given Id
/add_set [names...] Add multiple sets at once by `set_name` separated by newline

/toggle_flag [maintenance|newsfeed] Flag a chat as a maintenance or newsfeed chat. Newsfeed chats get the first sticker of every new set that is added, while all tasks are send to maintenance chats.

/stats Get some statistics
/refresh Refresh all stickerpacks.
/refresh_ocr Refresh all stickerpacks including ocr.
/broadcast [message] Send the message after this command to all users.

",40
KenanSulayman/heartbeat,None,"GCM R 20/0c/400 L 20/0c/400
fp7W1feinJ0e6wrs2uQD1c2y1RdSy86dyZ60Rr4twHeyd3Cv6LEXQfFdTeA=Gn1aRS9DdNzFJS3chcKtWjiZui/ugHq2Djcu9KBopmCijfaUJkpqiWDZutMcgoSB+/nyKRrTnJ8Ms3rR...
PwtOGk5kKqt/Olrbmv8KleIv4Xys+IXWG8akYqBH/WuRSdmdbh1Q7wCqnxk=N9ycWjSPMyYVXDBbV1wagsc6aGwmFHFe4mzQ+Ms5j/UrsMACVIsio4+ZyVN/fcn55/A6+yUv/+HlL9WH...
",13
SixArm/sixarm_sql_schema_examples,None,"SixArm → SQL → Schema examples
Contents:

Schema examples
Schema conventions

General standardizations
Date and time standardization
Our data naming conventions
Types
Optimizations


Bonus fields for growth
Liquibase annotation

Why we use Liquibase SQL vs. XML vs. YAML


Conventions
Languages
Tracking

Schema examples
Most popular:

person
place
thing
org
event
action
tag
color

Access control:

access_user
access_assigment
access_attribute
access_permission
access_operation

Geography:

geolocation
country
country_subdivision
region
locality
street_address
neighborhood
postal_code
postal_address

Business:

brand
currency
currency_conversion_service
currency_pair
exchange_rate_specification
market_area
market_sector
money_range
money_transfer
money_value
offer
payment_service
product
trade_action

Accounts:

bank_account
ftp_account
pop_account

Misc:

rag
image_filter
lifespan

Schema conventions
General standardizations
Use industry standard schemas such as the Schema.org project.

For example, a postal address has a field for ""locality"", not ""city"".

Use International System of Units (SI), such as the metric system.

For example, the field name ""height"" is intended to use a unit of a meter, not a unit of a foot as in the Imperial system and United States customary system.

Date and time standardization
Use date formats and time formats that are consistent with ISO standard formats.

For example, the timestamp display format is Year-Month-Day and Hour:Minute:Second and ""Z"" i.e. Zulu time zone, such as ""YYYY-MM-DDTHH:MM:SSZ""

Any date or time field must be in UTC, unless the filed is deliberately representing local time, in which case the field name must use the suffix ""_local"" because this helps with disambiguation.

For example, the ""person"" table has the field name ""birth_date_local"" because we care about the person's local date birthday, not Zulu date birthday.

If you are using PostgreSQL, then you may want to change the ""timestamp"" data type to the PostgreSQL extension ""timestamptz"" data type.

For example, the ""event"" table has the entry ""start_when timestamp""; if you are using PostgreSQL, you probably want to change this field to ""start_when timestamptz"" for Zulu time, or ""start_when_local timestamp"" for local time.

Our data naming conventions
Use a table name that is singular, not plural.

For example, the examples have a ""person"" table, not ""persons"" nor ""people"".

Use lowercase SQL, rather than uppercase SQL.

For example, the examples use ""create table"", not ""CREATE TABLE"".

Use a language code suffix when a text field could be different in different languages.

For example, a person's name in English is ""Amy"" and in French is ""Aimée"", so use fields ""name_as_en"" and ""name_as_fr"".

If a field can be a relation and/or freeform text, use two fields, one with field name suffix ""_id"" and one with field name suffix ""_ie"" meaning ""I.e., in other words"".

For example, a field name ""status_id"" is intended to be a relation to the ""status"" table, and a field name ""status_ie"" is intended to be freeform text that user can enter.

Use some of our notable exceptions because they are better at scale.

For example, for range naming we use ""start"" and ""stop"", not ""begin"" and ""end"", nor ""from"" and ""to"".

Types
Use typical data type default sizes.

For example, we use the data type ""varchar"" as a default, when we don't know a text field will be somewhat short.

Use numeric data type with a large range, rather than float data type, because we prefer consistency over fluidity.

For example, we prefer numeric(20,12) as a general-purpose number type; you can change these as you like.

Use the word ""numeric"" instead of ""decimal"" because its clearer, such as for integrations.

For example, we prefer numeric(x,y) over decimal(x,y).

Optimizations
Fast speed is more important than space, so we prefer some denormalization.

For example, some tables duplicate the field name ""postal_code"", because many of our apps use it to speed up local search.

Handling corner cases well is more important than space.

For example, the concepts of a ""region"" and ""country_subdivision"" are nearly identical, but not quite, so we store both.

Providing usable represenations is more important than space.

For example, the table ""person"" and table ""organization"" both have a field name ""vcard"" that stores the VCard VCF text, and a field name ""hcard"" that stoare the VCard as an HCard HTML microformat.

Bonus fields for growth
In practice we often add some bonus fields to each table; these fields help us with the growth of the app, and also the administration of the app.
Examples:
-- We prefer using secure random 128-bit numbers as primary keys.
-- These numbers are storable in a typical PostgreSQL UUID field.
id uuid not null primary key,

-- An incrementing number that an application can
-- use for optimistic locking for read/write speed
lock_version int,

-- Track who touches the record and when,
-- because this information helps in practice
-- for diagnosing the application as it runs.
created_at timestamp, created_by uuid references user,
updated_at timestamp, updated_by uuid references user,
proofed_at timestamp, proofed_by uuid references user,
retired_at timestamp, retired_by uuid references user,

-- The field name ""type"" is a reserved word in some frameworks,
-- which uses the field for single-table inheritance.
type varchar,

-- The field name ""position"" is a reserved word in some frameworks,
-- which uses the field for ordering by position index number.
position integer,

-- For a record that has a direct parent record
parent_id uuid references self,

-- Status table suitable for the app
status_id uuid references status,
status_ie varchar,

-- Ways to see more about the record, such as a URL to more information, and a note of text.
url varchar,
note longtext,
Liquibase annotation
Liquibase is an open source tool for managing database schema changes.
See https://en.wikipedia.org/wiki/Liquibase
Our projects use database schema changes within source code, and within large projects, so we have schema examples here to help new projects.
Why we use Liquibase SQL vs. XML vs. YAML
Liquibase files can be written in SQL, or XML, or YAML. We prefer SQL because more database administrators know it.
If you prefer XML or YAML and would like to translate our examples, then we welcome the help and also welcome pull requests.
Languages
We prefer to work with multiple languages. For example, we often use English, Spanish, Chinese, and many other languages.
We use a naming convention of ""{field}as{language}"" such as ""name_as_en"" that means ""the name as English"", ""name_as_es""
For the example files here, we list English and Spanish, so you can see how multiple languages can work.
You can add more languages if you want.
Some developers prefer different ways of handling languages, naming, internationalization, and localization. You can customize the files as you like for your goals.
Tracking

Package: sixarm_sql_schema_examples
Version: 8.1.2
Created: 1996-01-01
Updated: 2019-05-16
License: BSD, MIT, GPL
Contact: Joel Parker Henderson (http://joelparkerhenderson.com)

",2
sunnysideup/silverstripe-templateoverview,PHP,"Silverstripe templateoverview module






Documentation

Developer Docs
User Guide
API Docs

Requirements
See composer.json for details
Suggested Modules
See composer.json for details
Installation
composer require sunnysideup/templateoverview

Configuration
In the _config folder you will find the templateoverview.yml.example
file that shows options for the configuration of this module.
We recommend that you:

copy these templateoverview.yml.example files into your
app/_config folder (where available - otherwise search for private static $ in the module to see what can be configured)
remove the .example extension,
delete the lines you do not care about, and
adjust the configurations that you would like to use.

Contributing
We welcome any contributions. See CONTRIBUTING.md for more details.
Paid assistance
You can pay us to create an improved / adapted version of this module for your own projects.  Please contact us if you like to find out more: www.sunnysideup.co.nz.  For exmaple, we can write tests for this module.
Author
Sunny Side Up Ltd.
Care to see more modules?
To find other modules, please visit ssmods.com.
",6
Lombiq/Orchard-Recipe-Remote-Executor,C#,"Orchard Recipe Remote Executor Readme
Project Description
Orchard module for executing recipes on a remote site through API calls.
Documentation
This Orchard module lets you execute recipes on a remote Orchard site by providing WebAPI endpoints to send recipes to. Recipes can be run for a the tenant the API is exposed from or from the Default tenant recipes can be run for arbitrary tenants. Multiple recipes can be run in a batch.
See the API endpoints among the module's controllers.
The API needs authentication: use HTTP Basic authorization to authenticate with an Orchard user account (this account needs to have the ""Access the Recipe Remote Executor API endpoints"" permission). Since there is no encryption only use the API through HTTPS.
Recipe Remote Executor is part of the Lombiq Hosting Suite, a suite of modules making Orchard able to scale better, more fault-tolerant, and have improved maintainability.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/orchard-recipe-remote-executor (Mercurial repository)
https://github.com/Lombiq/Orchard-Recipe-Remote-Executor (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",5
sunnysideup/silverstripe-templateoverview,PHP,"Silverstripe templateoverview module






Documentation

Developer Docs
User Guide
API Docs

Requirements
See composer.json for details
Suggested Modules
See composer.json for details
Installation
composer require sunnysideup/templateoverview

Configuration
In the _config folder you will find the templateoverview.yml.example
file that shows options for the configuration of this module.
We recommend that you:

copy these templateoverview.yml.example files into your
app/_config folder (where available - otherwise search for private static $ in the module to see what can be configured)
remove the .example extension,
delete the lines you do not care about, and
adjust the configurations that you would like to use.

Contributing
We welcome any contributions. See CONTRIBUTING.md for more details.
Paid assistance
You can pay us to create an improved / adapted version of this module for your own projects.  Please contact us if you like to find out more: www.sunnysideup.co.nz.  For exmaple, we can write tests for this module.
Author
Sunny Side Up Ltd.
Care to see more modules?
To find other modules, please visit ssmods.com.
",6
Lombiq/Orchard-Recipe-Remote-Executor,C#,"Orchard Recipe Remote Executor Readme
Project Description
Orchard module for executing recipes on a remote site through API calls.
Documentation
This Orchard module lets you execute recipes on a remote Orchard site by providing WebAPI endpoints to send recipes to. Recipes can be run for a the tenant the API is exposed from or from the Default tenant recipes can be run for arbitrary tenants. Multiple recipes can be run in a batch.
See the API endpoints among the module's controllers.
The API needs authentication: use HTTP Basic authorization to authenticate with an Orchard user account (this account needs to have the ""Access the Recipe Remote Executor API endpoints"" permission). Since there is no encryption only use the API through HTTPS.
Recipe Remote Executor is part of the Lombiq Hosting Suite, a suite of modules making Orchard able to scale better, more fault-tolerant, and have improved maintainability.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/orchard-recipe-remote-executor (Mercurial repository)
https://github.com/Lombiq/Orchard-Recipe-Remote-Executor (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",5
Lombiq/Orchard-Feed-Aggregator,C#,"Feed Aggregator readme
Project description
Orchard module for aggregating feeds by creating content items from them. Supported feed types are: RSS and Atom feeds. This module was created when developing the new Orchard app driving www.dotnetfoundation.org, the website of .NET Foundation. It's also available for all sites on DotNest, the Orchard SaaS.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/orchard-feed-aggregator (Mercurial repository)
https://github.com/Lombiq/Orchard-Feed-Aggregator (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
Feed requirements
Rss

Feed items must contain a ""pubDate"" node.
Feed items must contain a ""guid"", ""title"" or ""description"" node.
Feeds must have an ""rss"" root node.

Atom

Feed entries must contain an ""updated"" node.
Feed entries must contain an ""id"" node.
Feeds must use the http://www.w3.org/2005/Atom namespace in the ""feed"" root node.

",5
csiro-easi/easi-training-pc,Jupyter Notebook,"CSIRO EASI training environment for PC (Win,Mac,Linux)
Table of Contents

Quickly get up and running
Using Jupyter Notebooks
Debugging my own python with Visual Studio Code

Quick-help pages

Using docker
Using git

Quickly get up and running
Prerequisites

Install git - https://git-scm.com/downloads
Install Docker - https://docs.docker.com/install/
(Optional) - a python code editor. This is only needed if you intend on writing python code directly, not just use the jupyter notebooks. Several options exist though many folks use Visual Studio Code https://code.visualstudio.com/download

All commands listed here should be entered into a terminal. On Windows Powershell works just fine.
Windows host notes:

CRLF handling in GIT and Editor As described below many of the text files used by the Docker container are mounted from the Docker host. Since Windows uses CRLF line endings by default and Linux (running in the Containers) uses only LF it is important to ensure that GIT and your chosen editors use LF by default. You will need to determine how to do this in your editor (eg. Visual Studio Code shows the current LF ending in the lower right of the window and has a default you can change in its settings). In Git auto conversion can be disabled prior to cloning by adjusting the global config:
$ git config --global core.autocrlf false



Clone the repository (or Update your copy), noting the submodule(s)
To clone the repository and submodules:
git clone --recursive https://github.com/csiro-easi/easi-training-pc.git

If you have cloned it previously and want to update your copy, or if the submodules didn't load
$ cd easi-training-pc/
$ git pull origin master
$ git submodule update --init --recursive

Download and run the docker images for the first time
$ cd easi-training-pc/easi-pc-py36/
$ docker-compose up -d

Files: Notebooks, data and your own python files
Changes that are user would make, be it python library installs, database index, etc can occur in two places:

On the host system
Inside the container(s)

All notebooks, data and python code are stored on the host system as paths relative to the git root (""$ cd easi-training-pc""). The list below shows the relative path on the host and the path that is mounted in the opendatacube container:
      - ../datacube-core:/home/jovyan/odc
      - ../../sample_data:/data
      - ../output:/home/jovyan/output
      - ../work:/home/jovyan/work
      - ./config/datacube.conf:/home/jovyan/.datacube.conf
      - ./config/datacube_integration.conf:/home/jovyan/.datacube_integration.conf

Placing a user related data, notebooks and files into any of the directories, or modifying files will immediately be reflected in the container.
To shutdown with and without removing the ODC and database containers
Docker has two ways to shutdown containers:

down - will stop and REMOVE the container. This will delete any changes made to the ODC container (e.g. if you added python libraries in the container these will be removed).
stop - will stop the containers but their state will remain. The ODC database and all changes will persist when next started.

The normal workflow is to use up to create the containers, start/stop to do your work, keeping any changes you make and down to remove the containers and start over.
Here's all the commands:
$ cd easi-training-pc/easi-pc-py36/
$ docker-compose up -d # creative
$ docker-compose stop 
$ docker-compose start
$ docker-compose down  # destructive

The database that contains your index of data has a independent lifecycle. It is stored in a separate persistent volume:
$ docker volume ls
DRIVER              VOLUME NAME
local               easi-training-pc-postgres-data

This means you can change the postgres and odc containers as much as you like and delete them, and the database content will still persist. If you want to start again you can remove it using:
$ docker volume rm easi-training-pc-postgres-data

Using Jupyter Notebooks
Connect via your web browser to the jupyter notebooks

Go to a browser and enter ""localhost:8888"". This should connect to the docker's jupyter notebooks. Jupyter password is ""secretpassword"".
Navigate to the ""easi-pc-notebooks"" and open ""01 - PC Getting Started"" and work your way through initialising the database and indexing some sample data.
Other notebooks show how to ingest data and the ODC API

Debugging my own python with Visual Studio Code

Put your python source in the hosts ""work"" directory
Configure Visual Studio Code by adding a Configuration with the following:
        {
            ""name"": ""Attach ODC Container (Remote Debug)"",
            ""type"": ""python"",
            ""request"": ""attach"",
            ""localRoot"": ""${workspaceRoot}/work"",
            ""remoteRoot"": ""/home/jovyan/work"",
            ""port"": 5678,
            ""host"": ""localhost""
        },


Connect to a bash shell inside the container (you can use the Visual Studio Code build in terminal to do this, or the Docker extension you can just right click Attach Shell)
$ docker exec -t -i easi-pc-py36_opendatacube_1 /bin/bash

Note: The container above will change if you have a container of the same name still around. You can list current containers and get the names using:
docker container ls


In the container bash shall use pip3 to install any additional packages you require (you will need to do this whenever your create a new container (using up/down). Using start/stop will preserve container between uses)
In the bash execute change to the ""~/work"" directory and execute ""yourCode.py"" with using ptvsd
$ cd ~/work
$ python3 -m ptvsd --host 0.0.0.0 --port 5678 --wait yourCode.py

The code will block and wait for the debugger to attach.
In Visual Studio Code debugger set breakpoints in your code and then start debugging using the Attach ODC Container (Remote Debug) configuration you setup earlier

",4
KunyiLockeLin/AngryEngine,C++,"Angry Engine

Introduction video:




Even though, I call it a game engine, it is not just for game development. It is for my interest. I might do something that does not relate game development, also might not want to do something, even though it is really important for game development.


Scene: default 0

How to add and delete a scene, object, component,
and load eid for scene, object, component,
and modify paraments and update the modification.
and set model, outline, position, material, show normal and mesh.





Scene: scene1 1

How to set camera control and focus, use post-processing, and modify gamma and exposure.
Main featurn in this scene, cubemap, animation, attach skeletion, particle,
line, reflection & refraction, mulit-render, 2D render, bloom, grayscale.





Scene: pbr1 2

The scene is for PBR and multi-lights demo.





Scene: raytracing1 3

When raytracingDepth's value of camera component is larger than 0, it runs ray tracing.
The system only supports spheres. In transform component, it could edit the location and radius of the sphere. localScaleX is radius.
Material component is neccseary. It only supports baseColor and metallic. If metallic's valule is larger than 0, it will reflect the environment.





In qeheader.h, you can find out the whole flag and type.

Feature

mutli-viewports, post-processing.
light and camera control.
vertex, tessellation control & evaluation(triangle-faces become opposite(cw)), geometry, fragment shader, compute shader(particles).
Phong shading and PBR(Physically-Based Rendering), gamma, exposure, grayscale, bloom.
diffuse map, baseColor map, cubemap & reflection & refraction, normal map.
skeletal animation, attach obj, draw lines, billboards, particle system.
draw mulit-render, 2D render, outline(stencil buffer) and multisample anti-aliasing(MSAA)
Log output, call stack trace and command.
For model  - obj, mtl, gltf (https://github.com/KhronosGroup/glTF-Blender-Exporter).
For image  - png, jpeg, bmp.
For string - xml, json.
Decode deflate, Huffman code, png, jpeg.
Math library.
Setting: data/config.xml
Input:

""esc"" : Close the program.
""+"" : Add a new viewport.
""-"" : Remove the last viewport.
""1""~""9"" : Choose a viewport to control its camera.
""Up"",""Down"",""Left"",""Right"",""W"",""A"",""S"",""D"",""Q"",""E"",""Z"",""C"",""X"",""MouseLeftMove"",""MouseRightMove"" : Move the camera.
""/"" : turn on command mode or input directly in console.

""showmesh"" : switch between mesh mode and model mode.
""shownormal"" : switch if drawing normal or not.
""resetcamera"" : Initialize the camera.
""scene [p1]"" : Restart the programe. p1 is scene name. If p1 is empty, it means to restart the current scene. In config.xml, it has 28000, 28001, 28002, 28003.





ToDoList

Physically based rendering - IBL-image based lighting

http://www.pbr-book.org/
https://learnopengl.com/PBR/IBL/Diffuse-irradiance
https://github.com/SaschaWillems/Vulkan#-physically-based-rendering


Ray tracing - model, texture, lighting, refraction, optimization

https://github.com/SaschaWillems/Vulkan#04---ray-tracing
https://www.scratchapixel.com/
https://github.com/petershirley/raytracinginoneweekend
https://www.youtube.com/watch?v=Q1cuuepVNoY


optimize decode png and jpeg
Wiki
debuggin tool

Bug

release mode crash sometimes.
lines flash sometimes.

",8
flatironinstitute/spikeforest,Python,"SpikeForest

Note: This project is in alpha stage of development
Overview
SpikeForest is a continuously updating platform which benchmarks the performance of spike sorting codes across a large curated database of electrophysiological recordings with ground truth. It consists of an interactive website that presents our up-to-date findings (front-end source code hosted elsewhere), this python package which contains the tools for running the SpikeForest analysis, and an expanding collection of electrophysiology recordings with ground-truth spiking information.
The repository is split into three main directories: spikeforest, mountaintools, and working. MountainTools is not spike-sorting specific and can be used for other applications or downstream analyses. It provides modules for batch processing, automatic caching of results, sharing data between labs (from a python interface), and processing on remote compute resources. The spikeforest directory (SpikeForest package) contains wrappers to the spike sorters, analysis routines, GUI components. The working directory contains the actual scripts used to prepare the data hosted on the website.
We make use of the SpikeInterface project, also in early development stage, that provides tools for extracting, converting between, and curating raw or spike sorted extracellular data from any file format.
Installation from PyPI
To install SpikeForest from PyPI, it is recommended that you start from a fresh conda environment with python (>= 3.6).
pip install --upgrade spikeforest
pip install --upgrade mountaintools

Test the installation by sorting one of the toy examples as described below.
Development installation
To install SpikeForest for development on Linux, it is recommended that you start from a fresh conda environment with python (>= 3.6).
Then clone the repository and install the two packages in editable mode.
git clone https://github.com/flatironinstitute/spikeforest
cd spikeforest
pip install -e ./spikeforest
pip install -e ./mountaintools

This will install all of the python dependencies from PyPI as well as editable (local) versions of the spikeforest and mountaintools packages. Thus if you edit the source code, your changes will take effect on your local system.
To get development updates you can git pull the master branch and then reissue the pip install commands in case any of the python dependencies have changed:
git pull
pip install --upgrade -e ./spikeforest
pip install --upgrade -e ./mountaintools

Test the installation by sorting one of the toy examples as described below.
Spike sorting
To use the containerized versions of the spike sorters (recommended), you should install singularity. This will work for all of the non-Matlab spike sorters (in the future we will also containerize the Matlab packages).
If you choose not to use the containerized sorters, then you will need to install each of the spike sorters individually. Instructions for doing this are found below.
Then the simplest way to perform spike sorting is:
from spikesorters import MountainSort4

MountainSort4.execute(
    recording_dir='<recording directory>',
    firings_out='<output directory>/firings.mda',
    detect_sign=-1,
    adjacency_radius=50,
    _container='default'  # Use _container=None to run without singularity
)

Here '<recording_directory>' must contain your ephys recording in .mda format. Instructions for preparing your data in this format using SpikeExtractors are found below.
The output (in firings.mda format) will be written to the <output directory>/firings.mda file.
To load the results, it is recommended that you use the SpikeExtractors package.
from spikeforest import SFMdaRecordingExtractor, SFMdaSortingExtractor

dsdir = '<recording directory>'
firings_fname = '<output directory>/firings.mda'

recording = SFMdaRecordingExtractor(dataset_directory=dsdir)
sorting = SFMdaSortingExtractor(firings_file=firings_fname)

See SpikeExtractors for more information on using recording and sorting extractors.
To use other spike sorters, simply swap in the appropriate class. For example:
from spikesorters import SpykingCircus

SpykingCircus.execute(
    recording_dir='<recording directory>',
    firings_out='<output directory>/firings.mda',
    detect_sign=-1,
    adjacency_radius=50,
    _container='default'  # Use _container=None to run without singularity
)

Preparing recordings in .mda format
To create a recording in .mda format (suitable for running SpikeForest sorters), use the SpikeExtractors package, which should be installed as part of the SpikeForest installation.
Installing the spike sorters
If you choose not to use the singularity method, you can install the spike sorters individually as follows.
HerdingSpikes2
See the HerdingSpikes2 website and the following Dockerfile.
IronClust
IronClust is a Matlab project. To use it with SpikeForest, first clone the repo and then set an environment variable pointing to the source location:
git clone https://github.com/jamesjun/ironclust
export IRONCLUST_PATH=<source location>

It is recommended that you add the export line to your .bashrc file.
JRClust
Installation is similar to IronClust, except there is an additional environment variable to set (which will be documented soon).
git clone https://github.com/JaneliaSciComp/JRCLUST
export JRCLUST_PATH=<source location>

KiloSort
Installation is similar to IronClust, except you also need to compile the CUDA code.
git clone https://github.com/cortex-lab/KiloSort
export KILOSORT_PATH=<source location>

Compilation instructions may be found on the KiloSort website.
KiloSort2
Installation is similar to IronClust, except you also need to compile the CUDA code.
git clone https://github.com/MouseLand/Kilosort2
export KILOSORT2_PATH=<source location>

Compilation instructions may be found on the KiloSort website.
Note that we encountered some issues with KiloSort2 and are presently using a forked version (more details will be provided later).
MountainSort4
pip install --upgrade ml_ms4alg

or see the following Dockerfile.
SpykingCircus
See the SpykingCircus website and the following Dockerfile.
Tridesclous
See the Tridesclous website.
YASS
See the YASS website and the following Dockerfile.
Running the full analysis (for the website)
Instructions (work-in-progress) for running the full analysis for the website can be found at full_analysis.md.
Sorting batches of multiple recordings
This section needs to be rewritten, and the following notebook may need to be updated: Example notebook: example_multi_recording.ipynb
Sorting using a remote compute resource
This section needs to be written.
Visualization of recordings and sorting results
This section needs to be written.
Data sharing
This section needs to be written.
Authors
This section needs to be written
",2
jantman/home-automation-configs,Python,"home-automation-configs

My home automation and home security configuration, scripts and tooling - mainly for HomeAssistant / AppDaemon, ZoneMinder, and related things.
Note: This repository is really only provided as an example, and is not really ""supported"". See Using It and Important Notes, below, for further information.
Notice/Disclaimer:
The information I provide on home automation/security and surveillance is based on what I've set up for myself based on a balance of cost, ease of use, and security, and should be considered for hobby purposes only. My current system and code has grown organically over time and is not how I'd approach this if I started over from scratch. My code and system has a few obvious vulnerabilities and probably some non-obvious ones as well; I humbly but sincerely ask that you do not attempt to exploit these. I highly recommend that anyone implementing a similar system - especially if you also publish the details of it - have undocumented backup systems/devices. Finally, the systems that I describe are intended to provide some protection against or notification of crimes of opportunity, not targeted attacks. Please keep in mind that none of this is intended to protect against someone who targets me specifically (and takes the time to research me) as opposed to my home at random.
What's Here?

appdaemon apps and configs - also includes my appdaemon logging helper
homeassistant/ - my HomeAssistant configs
testing/ - configurations and scripts for testing HomeAssistant and AppDaemon locally using docker-compose
zoneminder/ - some of my scripts and notes related to my ZoneMinder installation, including event image analysis and notifications
RaspberryPi touchscreen-based alarm control panels - see doorpanels.md

Using It and Important Notes
For anyone other than me, this is mainly intended to be a reference and inspiration. Much of this is quite custom to me and my setup. If you do want to use it, a few notes:

I generally work off of the master branch of this repo, since I assume I'm the only person directly using it. Before you take anything from this repo, it's probably best to check the commit history and assume that anything extremely new (i.e. minutes or hours old, maybe a day or two) might not have all the bugs worked out yet or be complete.
Paths are hard-coded in some places. I've tried to minimize this or pull it out to configuration or at least constants at the top of files.
The actual system that this runs on is managed by Puppet using a private repository. Puppet clones this repo, sets up a bunch of symlinks, installs packages and dependencies, manages systemd services, etc. I'm making every effort to add documentation to this repo describing what's needed to make it work, but some dependencies might be missing. Sorry.

",6
preaction/Yancy,Perl,"

NAME
Yancy - A simple framework and editor for content-driven Mojolicious websites
VERSION
version 1.025
SYNOPSIS
use Mojolicious::Lite;
use Mojo::Pg; # Supported backends: Pg, MySQL, SQLite, DBIx::Class
plugin Yancy => {
    backend => { Pg => Mojo::Pg->new( 'postgres:///myapp' ) },
    read_schema => 1,
};

DESCRIPTION






Yancy is a simple content management system (CMS) for administering
content in a database. Yancy accepts a configuration file that describes
the data in the database and builds a website that lists all of the
available data and allows a user to edit data, delete data, and add new
data.
Yancy uses JSON Schema to define the data in
the database. The schema is added to an OpenAPI
specification which creates a REST
API for
your data.
Yancy can be run in a standalone mode (which can be placed behind
a proxy), or can be embedded as a plugin into any application that uses
the Mojolicious web framework.
Yancy can manage data in multiple databases using different backends
(Yancy::Backend modules). Backends exist for Postgres via
Mojo::Pg, MySQL via
Mojo::mysql, SQLite via
Mojo::SQLite, and DBIx::Class, a Perl
ORM
Mojolicious Plugin
Yancy is primarily a Mojolicious plugin to ease development and
management of Mojolicious applications. Yancy provides:

Controllers which you can use to easily
list data, display
data, modify
data, and delete
data.
Helpers to access data, validate
forms
Templates which you can override
to customize the Yancy editor's appearance

For information on how to use Yancy as a Mojolicious plugin, see
Mojolicious::Plugin::Yancy.
Example Applications
The Yancy Git repository on Github
includes some example applications you can use to help build your own
websites. View the example application directory.
Yancy Plugins
Yancy comes with plugins to enhance your website.

The Auth::Basic plugin provides a simple,
password-based authentication system for the Yancy editor and your
website.
The Form plugin can generate forms for the
configured collections, or for individual fields. There are included
form generators for Bootstrap 4.

More development will be happening here soon!
Standalone App
Yancy can also be run as a standalone app in the event one wants to
develop applications solely using Mojolicious templates. For
information on how to run Yancy as a standalone application, see
Yancy::Help::Standalone.
REST API
This application creates a REST API using the standard
OpenAPI API specification. The API spec document
is located at /yancy/api.
CONFIGURATION
See Yancy::Help::Config for how to configure Yancy in both plugin and
standalone mode.
BUNDLED PROJECTS
This project bundles some other projects with the following licenses:

jQuery (version 3.2.1) Copyright JS Foundation and other contributors (MIT License)
Bootstrap (version 4.0.0) Copyright 2011-2017 the Bootstrap Authors and Twitter, Inc. (MIT License)
Popper.js (version 1.13.0) Copyright 2017 Federico Zivolo (MIT License)
FontAwesome (version 4.7.0) Copyright Dave Gandy (SIL OFL 1.1 and MIT License)
Vue.js (version 2.5.3) Copyright 2013-2018, Yuxi (Evan) You (MIT License)
marked (version 0.3.12) Copyright 2011-2018, Christopher Jeffrey (MIT License)

The bundled versions of these modules may change. If you rely on these in your own app,
be sure to watch the changelog for version updates.
SEE ALSO
JSON schema, Mojolicious
AUTHOR
Doug Bell preaction@cpan.org
CONTRIBUTORS

Ed J mohawk2@users.noreply.github.com
Mohammad S Anwar mohammad.anwar@yahoo.com
Rajesh Mallah mallah.rajesh@gmail.com
William Lindley wlindley@wlindley.com

COPYRIGHT AND LICENSE
This software is copyright (c) 2018 by Doug Bell.
This is free software; you can redistribute it and/or modify it under
the same terms as the Perl 5 programming language system itself.
",25
dotnet/corefx,C#,".NET Core Libraries (CoreFX)

This repo contains the library implementation (called ""CoreFX"") for .NET Core. It includes System.Collections, System.IO, System.Xml, and many other components.
The corresponding .NET Core Runtime repo (called ""CoreCLR"") contains the runtime implementation for .NET Core. It includes RyuJIT, the .NET GC, and many other components.
Runtime-specific library code (System.Private.CoreLib) lives in the CoreCLR repo. It needs to be built and versioned in tandem with the runtime. The rest of CoreFX is agnostic of runtime-implementation and can be run on any compatible .NET runtime (e.g. CoreRT).
.NET Core
Official Starting Page: https://dotnet.microsoft.com/

How to use .NET Core (with VS, VS Code, command-line CLI)

Install official releases
Documentation (Get Started, Tutorials, Porting from .NET Framework, API reference, ...)

Deploying apps


Supported OS versions


Roadmap
Releases
Bringing more APIs to .NET Core (and why some APIs will be left out)

How to Engage, Contribute and Provide Feedback
Some of the best ways to contribute are to try things out, file bugs, join in design conversations, and fix issues.

Dogfooding daily builds
If you have a question or idea, file a new issue.

If you are having issues with the ""full"" .NET Framework (also called ""Desktop""), the best way to file a bug is the Report a Problem tool, which is integrated with the VS Developer Community Portal; or through Product Support if you have a contract.
Issue Guide
This section is in progress here: New contributor Docs - Issues (feel free to make it better - it's easy-to-edit wiki with RW permissions to everyone!)
Each issue area has one or more Microsoft owners, who are listed here.
Contributing Guide
This section is in progress here: New contributor Docs - Contributing (feel free to make it better - it's easy-to-edit wiki with RW permissions to everyone!)
Useful Links

.NET Core source index / .NET Framework source index
API Reference docs
.NET API Catalog (incl. APIs from daily builds and API usage info)
API docs writing guidelines - useful when writing /// comments

Community

General .NET OSS discussions: .NET Foundation forums
Chat with other community members 

This project has adopted the code of conduct defined by the Contributor Covenant
to clarify expected behavior in our community. For more information, see the .NET Foundation Code of Conduct.
Reporting security issues and security bugs
Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) secure@microsoft.com. You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the Security TechCenter.
Also see info about related Microsoft .NET Core and ASP.NET Core Bug Bounty Program.
License
.NET Core (including the corefx repo) is licensed under the MIT license.
.NET Foundation
.NET Core is a .NET Foundation project.
There are many .NET related projects on GitHub.

.NET home repo - links to 100s of .NET projects, from Microsoft and the community.
ASP.NET Core home - the best place to start learning about ASP.NET Core.

CoreFX Project
Daily Builds
Daily builds of .NET Core components are published to dotnet-blob feed (https://dotnetfeed.blob.core.windows.net/dotnet-core/index.json).
The latest version number of each library can be seen in that feed.
Currently, there is no website to visualize the contents of the feed, so in order to do so, you have to use a NuGet feed explorer, like Visual Studio.
Note: See officially supported OS versions.
",16561
PineconePi/Pinecone_Pi_Nano,C,"PineconePi Nano（Click to enter the website，Support：support@pineconepi.cn)
PineconePi Nano is a development board that meets all the good ideas of 8051 MCU enthusiasts and creators: low cost, small size, size only 52 mm x 18 mm (DIP40); faster core: ultra-high speed 8051 core (1T), 12 times faster than traditional 8051; wide range of use: working voltage: 2.0V ~ 5.5V (built-in LDO), working temperature: - 40 ~85 (℃); rich peripherals and hardware resources. Source: One-button cold start, 8-channel LED, two SMT digital tubes, onboard Ch330N; 64K Flash, 5-way TIM, 8-way Pwm, 15-way high-speed ADC; easy to use, support C language, assembly, direct insertion breadboard; multi-expansion, a variety of peripheral modules.
List

Chipbook:Chip Datesheet And Driver program And .H file
Document:Teaching documents
Example project:Standard KEIL Project
Library:Rich sharing of peripheral driver Libraries
PinCard:Nano Pin Card
Project:Open Source Projects Used by User with Nano
RTOS:RTX 51 FULL(/tiny)
Schematic diagram and PCB doc：Schematic diagram and PCB doc

Pinecone_Pi_Nano（点我进入官网，官方交流企鹅群：481227232)
Pinecone Pi nano|松果派Nano是一块满足8051单片机爱好者和创客一切美好设想的开发板：低成本，小体积，尺寸仅52 mm x18 mm（DIP40）；更快的内核:超高速8051内核(1T),比传统8051快12倍以上;广泛的使用范围:工作电压:2.0V ~ 5.5V(内置LDO),工作温度:-40℃ ~ 85℃;丰富的外设与硬件资源：一键冷启动,8路LED,两位smt数码管,板载Ch330N;64K Flash,5路TIM,8路Pwm,15路高速ADC；易使用，支持C语言,汇编,直插面包板；多扩展，多种外设模块。
目录结构

Chipbook:Nano使用说明书 And 相关芯片手册 And 驱动程序 And 核心头文件 （建议Clone)
Demo:nano十万种玩法推荐（实例项目，来源于松果社区分享）
Example project:PineconePi提供的标准工程（新手福音）
Library:丰富的外设驱动库分享
Schematic diagram and PCB doc：NANO原理图与封装库
Shell（3D_STL)：nano外壳3D打印文件（STL）
RTOS:RTX 51 FULL(/tiny)安装包及使用指南
PinCard:松果派NANO引脚示意卡（打开QQ扫一扫有彩蛋哦！）

",20
preaction/Yancy,Perl,"

NAME
Yancy - A simple framework and editor for content-driven Mojolicious websites
VERSION
version 1.025
SYNOPSIS
use Mojolicious::Lite;
use Mojo::Pg; # Supported backends: Pg, MySQL, SQLite, DBIx::Class
plugin Yancy => {
    backend => { Pg => Mojo::Pg->new( 'postgres:///myapp' ) },
    read_schema => 1,
};

DESCRIPTION






Yancy is a simple content management system (CMS) for administering
content in a database. Yancy accepts a configuration file that describes
the data in the database and builds a website that lists all of the
available data and allows a user to edit data, delete data, and add new
data.
Yancy uses JSON Schema to define the data in
the database. The schema is added to an OpenAPI
specification which creates a REST
API for
your data.
Yancy can be run in a standalone mode (which can be placed behind
a proxy), or can be embedded as a plugin into any application that uses
the Mojolicious web framework.
Yancy can manage data in multiple databases using different backends
(Yancy::Backend modules). Backends exist for Postgres via
Mojo::Pg, MySQL via
Mojo::mysql, SQLite via
Mojo::SQLite, and DBIx::Class, a Perl
ORM
Mojolicious Plugin
Yancy is primarily a Mojolicious plugin to ease development and
management of Mojolicious applications. Yancy provides:

Controllers which you can use to easily
list data, display
data, modify
data, and delete
data.
Helpers to access data, validate
forms
Templates which you can override
to customize the Yancy editor's appearance

For information on how to use Yancy as a Mojolicious plugin, see
Mojolicious::Plugin::Yancy.
Example Applications
The Yancy Git repository on Github
includes some example applications you can use to help build your own
websites. View the example application directory.
Yancy Plugins
Yancy comes with plugins to enhance your website.

The Auth::Basic plugin provides a simple,
password-based authentication system for the Yancy editor and your
website.
The Form plugin can generate forms for the
configured collections, or for individual fields. There are included
form generators for Bootstrap 4.

More development will be happening here soon!
Standalone App
Yancy can also be run as a standalone app in the event one wants to
develop applications solely using Mojolicious templates. For
information on how to run Yancy as a standalone application, see
Yancy::Help::Standalone.
REST API
This application creates a REST API using the standard
OpenAPI API specification. The API spec document
is located at /yancy/api.
CONFIGURATION
See Yancy::Help::Config for how to configure Yancy in both plugin and
standalone mode.
BUNDLED PROJECTS
This project bundles some other projects with the following licenses:

jQuery (version 3.2.1) Copyright JS Foundation and other contributors (MIT License)
Bootstrap (version 4.0.0) Copyright 2011-2017 the Bootstrap Authors and Twitter, Inc. (MIT License)
Popper.js (version 1.13.0) Copyright 2017 Federico Zivolo (MIT License)
FontAwesome (version 4.7.0) Copyright Dave Gandy (SIL OFL 1.1 and MIT License)
Vue.js (version 2.5.3) Copyright 2013-2018, Yuxi (Evan) You (MIT License)
marked (version 0.3.12) Copyright 2011-2018, Christopher Jeffrey (MIT License)

The bundled versions of these modules may change. If you rely on these in your own app,
be sure to watch the changelog for version updates.
SEE ALSO
JSON schema, Mojolicious
AUTHOR
Doug Bell preaction@cpan.org
CONTRIBUTORS

Ed J mohawk2@users.noreply.github.com
Mohammad S Anwar mohammad.anwar@yahoo.com
Rajesh Mallah mallah.rajesh@gmail.com
William Lindley wlindley@wlindley.com

COPYRIGHT AND LICENSE
This software is copyright (c) 2018 by Doug Bell.
This is free software; you can redistribute it and/or modify it under
the same terms as the Perl 5 programming language system itself.
",25
dotnet/corefx,C#,".NET Core Libraries (CoreFX)

This repo contains the library implementation (called ""CoreFX"") for .NET Core. It includes System.Collections, System.IO, System.Xml, and many other components.
The corresponding .NET Core Runtime repo (called ""CoreCLR"") contains the runtime implementation for .NET Core. It includes RyuJIT, the .NET GC, and many other components.
Runtime-specific library code (System.Private.CoreLib) lives in the CoreCLR repo. It needs to be built and versioned in tandem with the runtime. The rest of CoreFX is agnostic of runtime-implementation and can be run on any compatible .NET runtime (e.g. CoreRT).
.NET Core
Official Starting Page: https://dotnet.microsoft.com/

How to use .NET Core (with VS, VS Code, command-line CLI)

Install official releases
Documentation (Get Started, Tutorials, Porting from .NET Framework, API reference, ...)

Deploying apps


Supported OS versions


Roadmap
Releases
Bringing more APIs to .NET Core (and why some APIs will be left out)

How to Engage, Contribute and Provide Feedback
Some of the best ways to contribute are to try things out, file bugs, join in design conversations, and fix issues.

Dogfooding daily builds
If you have a question or idea, file a new issue.

If you are having issues with the ""full"" .NET Framework (also called ""Desktop""), the best way to file a bug is the Report a Problem tool, which is integrated with the VS Developer Community Portal; or through Product Support if you have a contract.
Issue Guide
This section is in progress here: New contributor Docs - Issues (feel free to make it better - it's easy-to-edit wiki with RW permissions to everyone!)
Each issue area has one or more Microsoft owners, who are listed here.
Contributing Guide
This section is in progress here: New contributor Docs - Contributing (feel free to make it better - it's easy-to-edit wiki with RW permissions to everyone!)
Useful Links

.NET Core source index / .NET Framework source index
API Reference docs
.NET API Catalog (incl. APIs from daily builds and API usage info)
API docs writing guidelines - useful when writing /// comments

Community

General .NET OSS discussions: .NET Foundation forums
Chat with other community members 

This project has adopted the code of conduct defined by the Contributor Covenant
to clarify expected behavior in our community. For more information, see the .NET Foundation Code of Conduct.
Reporting security issues and security bugs
Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) secure@microsoft.com. You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the Security TechCenter.
Also see info about related Microsoft .NET Core and ASP.NET Core Bug Bounty Program.
License
.NET Core (including the corefx repo) is licensed under the MIT license.
.NET Foundation
.NET Core is a .NET Foundation project.
There are many .NET related projects on GitHub.

.NET home repo - links to 100s of .NET projects, from Microsoft and the community.
ASP.NET Core home - the best place to start learning about ASP.NET Core.

CoreFX Project
Daily Builds
Daily builds of .NET Core components are published to dotnet-blob feed (https://dotnetfeed.blob.core.windows.net/dotnet-core/index.json).
The latest version number of each library can be seen in that feed.
Currently, there is no website to visualize the contents of the feed, so in order to do so, you have to use a NuGet feed explorer, like Visual Studio.
Note: See officially supported OS versions.
",16561
PineconePi/Pinecone_Pi_Nano,C,"PineconePi Nano（Click to enter the website，Support：support@pineconepi.cn)
PineconePi Nano is a development board that meets all the good ideas of 8051 MCU enthusiasts and creators: low cost, small size, size only 52 mm x 18 mm (DIP40); faster core: ultra-high speed 8051 core (1T), 12 times faster than traditional 8051; wide range of use: working voltage: 2.0V ~ 5.5V (built-in LDO), working temperature: - 40 ~85 (℃); rich peripherals and hardware resources. Source: One-button cold start, 8-channel LED, two SMT digital tubes, onboard Ch330N; 64K Flash, 5-way TIM, 8-way Pwm, 15-way high-speed ADC; easy to use, support C language, assembly, direct insertion breadboard; multi-expansion, a variety of peripheral modules.
List

Chipbook:Chip Datesheet And Driver program And .H file
Document:Teaching documents
Example project:Standard KEIL Project
Library:Rich sharing of peripheral driver Libraries
PinCard:Nano Pin Card
Project:Open Source Projects Used by User with Nano
RTOS:RTX 51 FULL(/tiny)
Schematic diagram and PCB doc：Schematic diagram and PCB doc

Pinecone_Pi_Nano（点我进入官网，官方交流企鹅群：481227232)
Pinecone Pi nano|松果派Nano是一块满足8051单片机爱好者和创客一切美好设想的开发板：低成本，小体积，尺寸仅52 mm x18 mm（DIP40）；更快的内核:超高速8051内核(1T),比传统8051快12倍以上;广泛的使用范围:工作电压:2.0V ~ 5.5V(内置LDO),工作温度:-40℃ ~ 85℃;丰富的外设与硬件资源：一键冷启动,8路LED,两位smt数码管,板载Ch330N;64K Flash,5路TIM,8路Pwm,15路高速ADC；易使用，支持C语言,汇编,直插面包板；多扩展，多种外设模块。
目录结构

Chipbook:Nano使用说明书 And 相关芯片手册 And 驱动程序 And 核心头文件 （建议Clone)
Demo:nano十万种玩法推荐（实例项目，来源于松果社区分享）
Example project:PineconePi提供的标准工程（新手福音）
Library:丰富的外设驱动库分享
Schematic diagram and PCB doc：NANO原理图与封装库
Shell（3D_STL)：nano外壳3D打印文件（STL）
RTOS:RTX 51 FULL(/tiny)安装包及使用指南
PinCard:松果派NANO引脚示意卡（打开QQ扫一扫有彩蛋哦！）

",20
sgo230/Sisense,Python,"Sisense
Code Snippets for Sisense
",4
gitlabhq/gitlabhq,Ruby,"GitLab
Test coverage

 Ruby
 JavaScript

Canonical source
The canonical source of GitLab Community Edition is hosted on GitLab.com.
Open source software to collaborate on code
To see how GitLab looks please see the features page on our website.

Manage Git repositories with fine grained access controls that keep your code secure
Perform code reviews and enhance collaboration with merge requests
Complete continuous integration (CI) and CD pipelines to builds, test, and deploy your applications
Each project can also have an issue tracker, issue board, and a wiki
Used by more than 100,000 organizations, GitLab is the most popular solution to manage Git repositories on-premises
Completely free and open source (MIT Expat license)

Hiring
We're hiring developers, support people, and production engineers all the time, please see our jobs page.
Editions
There are two editions of GitLab:

GitLab Community Edition (CE) is available freely under the MIT Expat license.
GitLab Enterprise Edition (EE) includes extra features that are more useful for organizations with more than 100 users. To use EE and get official support please become a subscriber.

Website
On about.gitlab.com you can find more information about:

Subscriptions
Consultancy
Community
Hosted GitLab.com use GitLab as a free service
GitLab Enterprise Edition with additional features aimed at larger organizations.
GitLab CI a continuous integration (CI) server that is easy to integrate with GitLab.

Requirements
Please see the requirements documentation for system requirements and more information about the supported operating systems.
Installation
The recommended way to install GitLab is with the Omnibus packages on our package server.
Compared to an installation from source, this is faster and less error prone.
Just select your operating system, download the respective package (Debian or RPM) and install it using the system's package manager.
There are various other options to install GitLab, please refer to the installation page on the GitLab website for more information.
You can access a new installation with the login root and password 5iveL!fe, after login you are required to set a unique password.
Contributing
GitLab is an open source project and we are very happy to accept community contributions. Please refer to Contributing to GitLab page for more details.
Licensing
GitLab Community Edition (CE) is available freely under the MIT Expat license.
All third party components incorporated into the GitLab Software are licensed under the original license provided by the owner of the applicable component.
All Documentation content that resides under the doc/ directory of this repository is licensed under Creative Commons: CC BY-SA 4.0.
Install a development environment
To work on GitLab itself, we recommend setting up your development environment with the GitLab Development Kit.
If you do not use the GitLab Development Kit you need to install and setup all the dependencies yourself, this is a lot of work and error prone.
One small thing you also have to do when installing it yourself is to copy the example development unicorn configuration file:
cp config/unicorn.rb.example.development config/unicorn.rb

Instructions on how to start GitLab and how to run the tests can be found in the getting started section of the GitLab Development Kit.
Software stack
GitLab is a Ruby on Rails application that runs on the following software:

Ubuntu/Debian/CentOS/RHEL/OpenSUSE
Ruby (MRI) 2.4
Git 2.8.4+
Redis 2.8+
PostgreSQL (preferred) or MySQL

For more information please see the architecture documentation.
UX design
Please adhere to the UX Guide when creating designs and implementing code.
Third-party applications
There are a lot of third-party applications integrating with GitLab. These include GUI Git clients, mobile applications and API wrappers for various languages.
GitLab release cycle
For more information about the release process see the release documentation.
Upgrading
For upgrading information please see our update page.
Documentation
All documentation can be found on docs.gitlab.com/ce/.
Getting help
Please see Getting help for GitLab on our website for the many options to get help.
Why?
Read here
Is it any good?
Yes
Is it awesome?
These people seem to like it.
",21801
Lombiq/Tidy-Orchard-Development-Toolkit,C#,"Tidy Orchard Development Toolkit Readme
The Tidy Orchard Development Toolkit allows you to develop Orchard-based applications in a way that you own code (e.g. extensions, configuration) is completely separated from the core Orchard source.
This makes Orchard development not only tidier but it also allows you to:

Manage your extensions better: e.g. now you can keep all your modules under a single repository (with subrepositories for other modules) instead of having all your modules in separate repositories.
Updating or upgrading the Orchard source is a matter of pulling in the latest changes from the Orchard repository.
You can even keep a single (or just a few) folders on your computer with the Orchard source that you link to from each of your solutions, thus minimizing storage space usage and build time.

Keep in mind that this toolkit is purely experimental! It can in no way support a production scenario. Also the aim was to get Orchard working in its basics: it fully runs. Other areas like deplyoment wasn't explored yet.
Creating a Tidy Orchard solution
There is a sample solution with all of the below tasks already done: see the Tidy Orchard Development Quick Start. This solution has all the details just referenced here.

Create a folder in the root for your web project (e.g. “Orchard.Web” but the name is not mandatory) and copy the contents of Orchard.Web there (the Web csproj can also have an arbitrary name). Modify the Web.config as in the sample.
Add the Toolkit to the Lombiq.TidyOrchardDevelopmentToolkit under your web project's folder.
Add the full Orchard source to the Web project's folder under a folder called ""Orchard"". This should be the full Orchard source (e.g. with the lib and src folders in the root). Please note that you have to remove the Web.config from Orchard.Web.
Copy the Orchard solution file to the root (and optionally rename it).
Change all project references of the solution to point to the new web project's content (assuming your web project's folder is called Orchard.Web):

Replace ""Orchard\ with ""Orchard.Web\Orchard\src\Orchard\ (including the quotes).
Replace Orchard.Tests\ with Orchard.Web\Orchard\src\Orchard.Tests.
Replace Orchard.Web.Tests\ with Orchard.Web\Orchard\src\Orchard.Web.Tests.
Replace ""Orchard.Web\ with ""Orchard.Web\Orchard\src\Orchard.Web\ (including the quotes).
Replace Orchard.Tests.Modules\ with Orchard.Web\Orchard\src\Orchard.Tests.Modules.
Replace Orchard.Core.Tests\ with Orchard.Web\Orchard\src\Orchard.Core.Tests.
Replace Orchard.WarmupStarter\ with Orchard.Web\Orchard\src\Orchard.WarmupStarter.
Replace ""Tools\ with ""Orchard.Web\Orchard\src\Tools\ (including the quotes).
Replace Orchard.Specs\ with Orchard.Web\Orchard\src\Orchard.Specs.
Replace Orchard.Profile\ with Orchard.Web\Orchard\src\Orchard.Profile.
Replace Orchard.Web\Orchard\src\Orchard.Web\Orchard.Web.csproj back to Orchard.Web\Orchard.Web.csproj.


Copy over the contents of the original Orchard.Web folder to your own web folder except the Core, Modules, Media and Themes folders.
Adjust Orchard.Web.csproj:

Replace ....\lib\ with Orchard\lib.
Replace ProjectReference Include=""..\ with ProjectReference Include=""Orchard\src.
Replace ProjectReference Include=""Core\ with ProjectReference Include=""Orchard\src\Orchard.Web\Core.


Add the Toolkit's project to the solution and reference it from the web project.
Register the Toolkit's Autofac module in the HostComponents.config file.
Register the TidyDevelopmentHttpModule in the Web.config and change the handlers declaration to use the appropriate accessPolicy.
Add your own themes and modules under the Web project's folder under ""Modules"" and ""Themes"" folders, respectively.
Modify module project files according to the Orchard App Host documentation so they support the new solution structure.

Instead of copying you can always create symlinks with mklink instead.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/tidy-orchard-development-toolkit (Mercurial repository)
https://github.com/Lombiq/Tidy-Orchard-Development-Toolkit (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",3
chipsec/chipsec,C,"CHIPSEC: Platform Security Assessment Framework

CHIPSEC is a framework for analyzing the security of PC platforms including hardware, system firmware (BIOS/UEFI), and platform components. It includes a security test suite, tools for accessing various low level interfaces, and forensic capabilities. It can be run on Windows, Linux, Mac OS X and UEFI shell. Instructions for installing and using CHIPSEC can be found in the manual.
NOTE: This software is for security testing purposes. Use at your own risk. Read WARNING.txt before using.
First version of CHIPSEC was released in March 2014:
Announcement at CanSecWest 2014
Recent presentation on how to use CHIPSEC to find vulnerabilities in firmware, hypervisors and hardware configuration, explore low level system assets and even detect firmware implants:
Exploring Your System Deeper
Projects That Include CHIPSEC

Linux UEFI Validation (LUV)

Contact Us
Mailing lists:

CHIPSEC users: chipsec-users
CHIPSEC discussion list on 01.org

Follow us on twitter
",1790
LiveChief/wireguard-install,Shell,"Wireguard Install
Wireguard Server
wget https://raw.githubusercontent.com/LiveChief/wireguard-install/master/wireguard-server.sh
bash wireguard-server.sh

Wireguard Client
wget https://raw.githubusercontent.com/LiveChief/wireguard-install/master/wireguard-client.sh
bash wireguard-client.sh

Wireguard Multi-hop
wget https://raw.githubusercontent.com/LiveChief/wireguard-install/master/wireguard-multihop.sh
bash wireguard-multihop.sh

Copy $HOME/$CLIENT_NAME-wg0.conf to /etc/wireguard/wg0.conf
Setup Wireguard service on client
systemctl enable wg-quick@wg0.service

Start Wireguard service on client.
systemctl start wg-quick@wg0.service

Stop Wireguard Service
systemctl stop wg-quick@wg0.service

Restart Wireguard Service
systemctl restart wg-quick@wg0.service

Show Wireguard status
wg show

",9
liesauer/Free-SS-SSR,None,"Free-SS-SSR
免费的SS账号、SSR账号，定期更新
更新时间：2019-05-17 08:52 +0800
说明
状态     ：🙂 - 良好 🙁 - 多次连接失败或连接超时
PING值   ：不代表真实使用中的速度，仅供参考。
SS账号：只能在SS里面使用。
兼容账号：能向上向下兼容SS、SSR，你可以在两个软件中使用。
SSR账号：只能在SSR里面使用。
👉扫码页面👈
🎉🎉🎉好消息！订阅地址已全面开放！麻麻再也不用担心我要人工维护酸酸乳了！
🎉🎉🎉好消息！Issues页面 已开放，大家有问题、意见、建议可到 Issues页面 提出。
👉订阅链接👈
兼容账号



-
PING
服务器
端口
密码
加密方式
区域




🙂
42
47.75.116.69
12345
4332erw
rc4-md5
UN


🙂
43
47.91.229.182
12334
rewr
rc4-md5
UN


🙂
44
47.75.125.77
12344
432erwr
rc4-md5
UN


🙂
45
47.52.16.47
12333
43ewrqre
rc4-md5
UN


🙂
62
36.228.214.160
443
nexitally
aes-128-ctr
TW


🙂
77
5.181.5.105
443
nHHpgf
aes-256-cfb
UN


🙂
80
45.67.53.93
369
lncn.org sa6
rc4
UN


🙂
97
192.154.197.89
40899
Y3oEquMWO2DL
aes-256-cfb
US


🙂
135
172.105.213.201
10456
fafajofdsgc
aes-256-cfb
JP


🙂
140
172.105.213.201
10456
fafajofdsgc
aes-256-cfb
UN


🙂
148
172.104.123.158
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
UN


🙂
150
46.29.162.46
1026
91vpn.cf
rc4-md5
RU


🙂
151
185.135.82.80
2333
doub.io
aes-128-ctr
RU


🙂
154
172.105.217.127
14541
TPOYVGxKglpi
aes-256-cfb
UN


🙂
155
91.188.223.72
543
http://t.cn/RD0D7sx
rc4-md5
UN


🙂
156
185.173.92.181
443
sssru.icu
rc4-md5
UN


🙂
167
91.188.223.72
8080
http://t.cn/EGJIyrl
rc4-md5
RU


🙂
169
46.17.44.9
369
lncn.org 6a
rc4
UN


🙂
182
185.133.193.83
369
lncn.org 6a
rc4
UN


🙂
183
103.135.102.119
1257
5785455
chacha20
US


🙂
184
45.79.82.49
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
185
185.243.57.221
80
t.me/SSRSUB
rc4-md5
US


🙂
186
67.21.81.240
8388
password
aes-256-cfb
UN


🙂
186
45.79.93.178
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
187
192.241.221.124
11987
ss8.pm-74764838
aes-256-cfb
US


🙂
187
23.95.70.189
59608
supermyssr
chacha20-ietf
UN


🙂
188
185.243.57.221
543
http://t.cn/RD0D7sx
rc4-md5
UN


🙂
188
185.243.57.221
80
t.me/SSRSUB
rc4-md5
UN


🙂
188
185.133.193.85
369
lncn.org 6a
rc4
UN


🙂
190
157.230.154.68
11012
ssx.re-75871166
aes-256-cfb
US


🙂
190
45.79.94.57
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
190
45.79.93.164
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
190
45.79.96.77
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
193
103.124.107.7
9052
jt2ekBNc9HuVtm2a
aes-256-cfb
UN


🙂
193
45.79.83.180
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
194
198.199.109.79
19687
ssx.re-94520056
aes-256-cfb
US


🙂
195
207.246.107.206
1996
wujie1314
chacha20
US


🙂
196
45.79.87.208
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
199
45.79.95.18
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
200
103.124.107.7
9052
jt2ekBNc9HuVtm2a
aes-256-cfb
US


🙂
202
45.79.95.58
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
207
139.162.115.215
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
UN


🙂
208
141.98.213.163
988
5M57kg11c214qDmK
chacha20
KR


🙂
210
45.77.175.103
2019
doub.io
aes-128-ctr
UN


🙂
211
194.124.35.169
369
lncn.org sa6
rc4
UN


🙂
214
141.98.213.163
988
5M57kg11c214qDmK
chacha20
UN


🙂
217
45.79.97.186
443
9d6cceaa373bf2c8acb22e60b6a58be6
aes-256-cfb
US


🙂
241
176.126.78.212
443
jfopwejfasohgadf
chacha20-ietf
UN


🙂
241
168.62.163.117
993
2019.03.07
rc4-md5
UN


🙂
241
172.104.188.241
11963
f55.fun-96885875
aes-256-cfb
SG


🙂
241
46.29.162.104
21560
5500
chacha20-ietf
RU


🙂
244
217.182.242.93
80
hfdeyibcfgh
chacha20-ietf
UN


🙂
245
168.62.163.117
993
2019.03.07
rc4-md5
US


🙂
247
45.33.32.152
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
US


🙂
252
192.210.190.101
25581
superssrnet
aes-256-cfb
US


🙂
253
45.33.80.198
13327
f55.fun-46814510
aes-256-cfb
US


🙂
254
45.33.69.91
11040
f55.fun-40097695
aes-256-cfb
US


🙂
257
128.199.187.62
14811
ssx.re-92498252
aes-256-cfb
SG


🙂
257
173.255.230.159
11255
f55.fun-82730394
aes-256-cfb
US


🙂
258
107.172.156.130
80
superssr.net
chacha20-ietf
UN


🙂
260
66.175.223.22
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
US


🙂
260
159.89.114.104
45235
Iwweruyui
chacha20
UN


🙂
261
178.128.94.215
10613
ss8.pm-02275771
aes-256-cfb
SG


🙂
262
45.33.48.155
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
US


🙂
267
45.12.205.202
12502
12345678
aes-256-cfb
AU


🙂
268
104.167.97.164
80
t.me/SSRSUB
rc4-md5
UN


🙂
270
104.167.97.164
543
http://t.cn/RD0D7sx
rc4-md5
CA


🙂
270
185.224.249.34
9001
UkXRsXvR6buDMG2Y
aes-256-cfb
RU


🙂
278
213.183.48.10
17228
ss8.pm-19235472
rc4-md5
RU


🙂
286
172.104.39.134
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
SG


🙂
300
213.226.68.94
9030
GeregetR8cvQHzYr
aes-256-cfb
DE


🙂
307
91.219.237.119
9001
getvpn20190501
aes-256-cfb
HU


🙂
315
139.162.37.161
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
UN


🙂
344
91.201.65.148
9069
tHKW7Ww2mck9CHQG
aes-256-cfb
UN


🙂
356
212.60.5.208
369
lncn.org 6a
rc4
UN


🙂
375
134.209.150.54
45235
Iwweruyui
chacha20
UN


🙂
201
139.162.115.215
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
JP


🙂
233
172.104.179.17
19077
f55.fun-50558565
aes-256-cfb
SG


🙂
294
139.162.37.161
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
SG


🙁
249
172.104.171.145
19846
f55.fun-58018742
aes-256-cfb
SG


🙁
267
172.104.123.158
8097
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
aes-256-cfb
JP


🙁
0
23.94.5.222
80
199844
aes-256-cfb
UN


🙁
0
stable1.local
443
https://t.me/autossr
aes-256-cfb
UN


🙁
0
173.82.232.48
22731
WeCrBq
rc4-md5
UN


🙁
0
47.91.249.54
12334
erw34r
rc4-md5
UN


🙁
0
47.52.103.137
12378
342eqwrqew
rc4-md5
UN


🙁
0
47.75.197.167
12378
43214er
rc4-md5
UN



",141
ryankurte/rust-radio-at86rf212,Rust,"rust-radio-at86rf212
A rust driver for the Atmel AT86RF212 Sub 1GHz ISM band radio IC, based on ryankurte/libat86rf212.
Status




Open Issues
Work in Progress

 Register operations
 Initialisation
 Polling
 Simple Send
 Simple Receive
 Packet building & parsing
 Auto ACK
 Auto Retransmit
 Interrupt Mode
 DMA support
 Unit testing
 Integration Testing

Testing
Unit testing should be implemented using dbrgn/embedded-hal-mock.
Integration testing is run using a Raspberry Pi model 3 with an IO shield and a pair of XPlained PRO Zigbit + ATRF212B-0-U modules.
The RPi pins are configured at startup as in rpi_setup.sh and the environment is configured in rpi_env.sh (though these are separately injected into the test system). Note that CS0 and CS1 functions are not currently used.
buildkite with a custom worker is used to run integration tests against a physical hardware.

Licensing
This project is licensed as GPLv3 for all purposes. For alternative licensing options / proprietary use, please contact the author (and we'll be happy to help ^_^).
",4
jmquigley/gadgets,TypeScript,"gadgets

Reusable React UI widgets - This is my widget library. There are many like it, but this one is mine...







This library provides widgets and composite controls for building desktop apps using Electron.  The widgets are listed below.
Requirements

Electron v4.x+
Node v10.x+
React v16.3+

Usage
The CSS styles must be included within a project using CSS modules or via webpack configuration:
code snippet
const styles = require('gadgets/dist/styles.css');

import {
	Button,
	ButtonToggle
} from 'gadgets';

...

<Button />
<ButtonToggle />
webpack snippet
module.exports = {
	entry: [
		path.resolve(__dirname, 'node_modules', 'gadgets', 'dist', 'styles.css'),
	],
	target: 'node',
	output: {
		path: path.resolve(__dirname, 'dist'),
		filename: 'bundle.js',
		publicPath: ""dist/"",
		libraryTarget: ""commonjs""
	},
    ...
This will give a webpack module an entry point to copy the gadgets CSS file into that build's distribution.  Without this the built in styles for each control will be missing.
Note that React is NOT packaged with the app.  An app that uses this library must supply the React library.  The demo application shows an example of this.
Installation
This module uses yarn to manage dependencies and run scripts for development.
To install as an application dependency:
$ yarn add gadgets

To build the app and run all tests:
$ yarn run all

To build and run the demo application in electron use:
$ yarn run demo

This will check out all dependencies, build the code, test, and then try to run it within electron.  This will take some time to complete.
To just attempt to run the application without building use (assuming the app was recently built):
$ yarn start

To change the code while electron is running and use CMD + R to refresh electron (in two terminals):
$ yarn watch:types.    # starts the typescript compiler in terminal 1
$ yarn watch:webpack   # starts the webpack watcher in terminal 2

This starts two separate watchers.  It will watch for changes in the typescript files first.  When detected the typescript compiler will build from .tsx to .jsx.  The webpack watcher will then see this and rebuild the bundle with the second watcher.
This library was created for use in the Electron UI and has been tested to work with the most recent version of Chromium in Electron.  It contains a custom set of React widgets and composite components used to create a desktop application.  The build uses Typescript and Wepback to create the package.  Once built it contains a distribution bundle (bundle.js) and a CSS file (styles.css) that can be included within another project.  The library also makes use of styled components.
Click on the each documented element below to see a picture of the component (where applicable).  To see a live demo, build the application (listed above).  It will run a demo electron application demonstrating each component.  The demo application also contains samples that demonstrate how the components are used.
Widgets
The module contains the following widgets (click on each header to see attribute/event details for each):
Accordion/AccordionItem
An accordion control contains N number of AccordionItems.  These items will display/hide the contents of that item when the header of that item is clicked.  The content of an AccordionItem can be any set of HTML tags.  The demo application shows a List embedded within the accordion.
Badge
A counter widget that annotates (overlays) another widget.
BaseProps
This module represents the properties that are shared by every class in the project.  This includes properties such as color, id, disabled, etc.
Breadcrumbs
A navigation control used to keep track of previous locations visited.  The rightmost item is the current location.  This provides a ""path"" back to the start of some navigation.
Break
A wrapper for the <br> tag.  This respects the Sizing option for controls so that the height of the break matches the current line height.
Browser
Creates a web browser instance using a webview tag
Button
A typical button control widget.  This control only uses an icon and no text to represent the button.  The icons are Font Awesome strings.  That library is built into this module, so any font available in the current release of that library is available.
ButtonCircle
A circular button control.  Works like a typical button... except it's a circle.
ButtonDialog
A button control that when pushed displays a local dialog box.  The contents of the control make up the dialog window.
ButtonText
A button control that contains an icon and a text string.  The text string can be to the left or right of the icon.
ButtonToggle
A button control that switches between the given icons when clicked.  The state of the button is maintained until it is clicked again.
Container
A generic control used to group other controls.  It creates a section tag around the given child component.
DataGrid
A component that represents data in a 2D table format like excel.  The underlying code is a wrapper around the react-data-grid component.
DateChooser
TODO: create the DateChooser control
DialogBox
A modal, dialog box popup window for yes/no user decisions.
DialogWindow
A modal dialog window.
Dropdown
A dropdown list using the HTML select/option elements.
DynamicList
A specialized List control that can be manipulated by the user.  They can add/remove/select items from it.
Editor
A multi-line text editor control.  It uses a custom markup module under the Quill editor/api.
Gauge
TODO: create a circular gauge control
Icon
Displays a graphical icon within the current container.
Label
A text label string control that can be edited.
List/ListDivider/ListFooter/ListHeader/ListItem
A container element that holds a list of other elements.  The List resolves to a <ul> and the items within it resolve to <li>.
Option
A checkbox/radio button control.  This is just a specialized form of ButtonToggle.
OptionGroup
A grouping of Option conmponents.
Pager
A pagination control.  This takes an input size I and a page size P and breaks it up into N = I/P entries.  The entries are displayed as a list of pages that can be chosen by the user.
Preview
Takes a string of a markup language (asciidoc, markdown, or restructuredtext), converts it to HTML, and presents it in a webview control.
ProgressBar
TODO: create the ProgressBar control
Rating
TODO: create the Rating control
Slider
The Slider control creates a horizontal line overlapped by a chevron that can be moved along this horizontal line.
Switch
A button control that works like a toggle.  Pressing the button will turn it on or off.  The color of the slider shows the state.
TagList (Tag)
The tag list control is a basic list of strings that act as metadata for another control.  They are used to categorize information.
TabContainer (Tab)
A typical tab control container.  This manages Tab elements within it.
TextArea
A multiline text editing component.  It is a contenteditable div.
TextField (Validator)
The TextField is a wrapper component for the built in <input> tag.  This control allows the user to add validation routines to the input control beyond the builtin routines.
TimeChooser
TODO: create TimeChooser control
Title
A reusable title block used to format two items: a title and a widget.  The title is generally a text string and the widget can be a text string or another control.
Toast
A popup that contains a message on the top or bottom of that container.  The message will disapper after N seconds.  Contains an X button to remove sooner.  It contains four basic modes: info, warning, error, custom.
Toolbar/Divider
A grouping of buttons and/or controls in a horizontal bar.
Tooltip
A text popup window on a control used to give help or feedback to the user of that control.
Treeview
This component represents data in a hierarchical parent/child view.  The underlying code is a wrapper around the react-sortable-tree component written by Chris Fritz.
Triangle
Uses SVG to draw a triangle within the container.
Voting
TODO: Create a Voting control (up/down arrows to affect vote count like Reddit)
Wrapper
Each component uses the Wrapper component to catch potential errors within the control.  These are error boundaries that are new to React 16.  It also wraps the ThemeProvider used by the styled-components.
Styles
The library contains an external style sheet that must be included.  It is located in node_modules/gadgets/dist/styles.css.  This library also uses highlight.js.  If the Editor control is used, then the highlight CSS files need to be included as well.  They are located in node_modules/gadgets/dist/highlights/*.css.  These should be copied to the root of the site/app (with something like the Copy Webpack Plugin).
",2
regardscitoyens/AFP-AN-RSS,Shell,"AFP AN RSS
RSS pour la page des dépèches AFP Assemblée nationale.
Mise-à-jour automatique réalisée via un commit git toutes les 10 minutes. Pour souscrire au flux, le lien est : https://raw.github.com/RegardsCitoyens/AFN-AN-RSS/master/afp-an.rss
Regards Citoyens
",5
bcgov/dbcrss,Python,"
dbcrss
DataBC Application Feeds Service
Visualizations
DataBC Web Services
https://uptime.apps.gov.bc.ca 
Purpose
Service Status Page
License
Copyright 2016 Province of British Columbia

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

",2
mkfryer/AutonomousAgentSurvival,Python,"AutonomousAgentSurvival
A replication of the Cape Town water crisis to explore information cascades and bayesian inference for survival.
",3
ermir18/verklegt_namskeid_2,JavaScript,"verklegt_namskeid_2
Verklegt Namskeid 2,
Castle Apartments, Real Estate App
Ermir Pellumbi
Hannes Kristjánsson
Valgeir Árnason
written in django.
To run this application, you must first install
required packages.
You do that by running this command in the terminal
(command line):



pip install -r requirements.txt
When the above install is complete, run the program
by entering the following in terminal (cmd line):
2)
python manage.py runserver
When manage.py is up and running, go to your web
browser and visit
the site that shows the application:
3) http://localhost:8000
© Castle Apartments 2019
",2
yulei88/autohosts,Shell,"autohosts
data/hosts是每天最新取得的hosts文件，任何操作系统皆可使用。
以下相关的工具程序，则只用在Linux下
这个工具的原理非常简单：
1）在墙外的服务器上部署一个定时任务，每天取得data/hosts.lst文件中的那些域名的IP地址，并且自动生成data/hosts 文件
2）取下data/hosts文件，提供给墙内的服务器使用
墙外的服务器可以定时执行以下的任务以生成hosts文件：
tools/hostsupdate.sh data/hosts.lst > data/hosts
Android下，推荐此程序（需要root）
https://github.com/ChinaHuibinWang/autohosts
",14
home-assistant/home-assistant,Python,"Home Assistant 

Home Assistant is a home automation platform running on Python 3. It is able to track and control all devices at home and offer a platform for automating control.
To get started:
python3 -m pip install homeassistant
hass --open-ui
Check out home-assistant.io for a
demo, installation instructions,
tutorials and documentation.


Featured integrations

The system is built using a modular approach so support for other devices or actions can be implemented easily. See also the section on architecture and the section on creating your own
components.
If you run into issues while using Home Assistant or during development
of a component, check the Home Assistant help section of our website for further help and information.
",23586
home-assistant/home-assistant,Python,"Home Assistant 

Home Assistant is a home automation platform running on Python 3. It is able to track and control all devices at home and offer a platform for automating control.
To get started:
python3 -m pip install homeassistant
hass --open-ui
Check out home-assistant.io for a
demo, installation instructions,
tutorials and documentation.


Featured integrations

The system is built using a modular approach so support for other devices or actions can be implemented easily. See also the section on architecture and the section on creating your own
components.
If you run into issues while using Home Assistant or during development
of a component, check the Home Assistant help section of our website for further help and information.
",23586
silvernoo/ac-rss,Python,"AC-RSS
订阅以下地址
https://raw.githubusercontent.com/silvernoo/ac-rss/master/rrs/feed_110.xml
https://raw.githubusercontent.com/silvernoo/ac-rss/master/rrs/feed_164.xml
https://raw.githubusercontent.com/silvernoo/ac-rss/master/rrs/feed_184.xml
https://raw.githubusercontent.com/silvernoo/ac-rss/master/rrs/feed_73.xml
https://raw.githubusercontent.com/silvernoo/ac-rss/master/rrs/feed_74.xml
https://raw.githubusercontent.com/silvernoo/ac-rss/master/rrs/feed_75.xml
",17
unimelb/pattern-lib,Vue,"Pattern Library
Copyright © 2017 - The University of Melbourne
The contents of this repository have been produced by The University of Melbourne for internal use and must not be distributed without the express permission of The University of Melbourne.


Development Preview (Netlify deploy from dev branch)
Production (Netlify deploy from master branch)

Getting started
The design system requires:

Node (~6.11.3)
Yarn (latest version)

# 1. Clone the repository.
git clone https://github.com/unimelb/pattern-lib.git

# 2. Enter your newly-cloned folder.
cd pattern-lib

# 3. Copy the env file.
cp .env.example .env

# 4. Copy the git pre-push hook
cp pre-push.sh .git/hooks/pre-push

# 4. Install dependencies. Make sure yarn is installed: https://yarnpkg.com/lang/en/docs/install
yarn

# 5. Read the documentation linked below.
Development

Contributing Guidelines

Storybook is the main development environment.

yarn dev - http://localhost:7002/
yarn build to build the documentation site to /.out/docs. Environment variable LOAD_EXTERNAL_ASSETS controls whether the documentation site is to load the library assets locally (false) or from the CDN (true).

Generator
This project includes generators to speed up common development tasks. Commands include:
# Generate a new component
yarn generate component
You will then be asked for the name of the component, this will be used to create a new folder with a minimal component layout and story

Note: Always remember to add the new component to the file index.js inside of the folders target/lib and target/vue, that way the component will be exportable to Matrix CMS via CDN and Vue via NPM.

# Generate a new view component
yarn generate story
You will need to select the component from the list of folders, then confirm the selection by selecting choose this directory. You will then be asked to give the story a name.

Note: This requires some special comments are added in the stories/index.js file. If it doesn't work make sure the comments are the same as in the template directory.

Using a component in CMS
In Storybook: When adding the ""how to use a component"" documentation, add a description that clearly shows that ( in the CMS environment) a component must be used in the way of closing the tag explicitly, as shown in the folllowing example:
Do not:
(In the CMS, self closing tags won't load the component:)
# self close the components
<my-new-component/>
Do
(To be compatible with CMS, call the component this way:)
# explicity closing
<my-new-component></my-new-component>

Note: Matrix CMS can only use the components in that way and must be in the target/lib folder as well. You can self-close a component when importing it in a parent component in the pattern-lib context. The rule described above applies just when the component is called in the CMS context. ie. footer component, which is used like this in CMS: <page-footer></page-footer> instead of <page-footer />.

Targets
UI library - targets/lib
The main UI library for use in the CMS. The target provides a local development environment for testing purposes.

yarn start:lib - http://localhost:7003/.
yarn build:lib to compile the library to .out/lib/<version>, including ui.css, ui.js, sprite.svg, and SVG assets in components/shared. You can then use http-server or another static file server to serve the output directory.

The following environment variables are available to configure the behaviour of yarn build:lib:

LOAD_EXTERNAL_ASSETS controls whether the library is to load its assets locally (false) or from the CDN (true).
LIB_EMIT_HTML controls whether to emit the demo HTML file - set it to true to emit the file.
LIB_LOAD_VERSION controls which version of the library to load in the demo:

leave it blank to load the local bundles (e.g. to test a new feature),
set it to auto to load the latest version from the CDN (i.e. the version specified in package.json),
set it to a specific version number to load that version from the CDN - e.g. 0.0.12 (no v prefix).



Vue library - targets/vue
The library with all the Vue components for use in single-page apps and other Vue-based projects.

yarn build:vue to compile the library to .out/vue.js.

Linting
CSS files are linted on the fly with stylelint. The configuration file, .stylelintrc, extends two shared configuration: stylelint-config-standard and stylelint-config-property-sort-order-smacss.
JS files and single-file Vue components are linted on the fly with ESLint. The configuration file, .eslintrc, extends two shared configurations: eslint-config-airbnb and plugin:vue/recommended
For your own sanity, make sure to install your code editor's ESLint and stylelint extensions. The following commands are available for on-demand linting and fixing:

yarn lint
yarn lint:fix
yarn lint:css
yarn lint:css --fix
yarn lint:js
yarn lint:js --fix

Release process
At the start of a new release sprint:

Create a milestone called next-release.

Throughout the release sprint:

Assign the appropriate pr- label to every new PR: pr-major if it contains a breaking change, pr-minor if it adds a new feature, pr-patch in all other cases.
Assign issues and PRs to next-release as they are resolved/merged.
Assign additional labels to issues when relevant (e.g. bug, chore, feature, etc.)

At the end of the release sprint:

Look at all the PRs that were assigned to next-release throughout the sprint and identify the highest-level of change (major, minor or patch). Deduce the next release's version number and rename the milestone accordingly.
Create a new release notes draft based on the following template: .github/RELEASE_NOTES_TEMPLATE.md.
Write the release notes by going through all the issues and PRs assigned to the milestone.
Deploy to production (cf. next section).
Once the library and documentation sites are deployed, publish the release notes and close the milestone.
Share the ZenHub milestone report with stakeholders.

See more Notes about the release process in the release.md section in docs:
Docs section
Deployment
To deploy to production:

Bump the version number in package.json (cf. note below).
Commit the version change to the dev branch.
Create a pull request to merge the dev branch into master - e.g. ""Deploy v1.0.1"".
Wait for the mandatory checks to pass then select ""Rebase and merge"" (cf. note below).

Semaphore then automatically builds the library and syncs the output files to S3. If the version you're deploying had been previously deployed, you'll need to invalidate the files on the CDN (AWS Cloudfront) or wait a day or so for this to happen automatically. Once the library is deployed, follow the release process below.

Note on versioning: the version number follows the semver convention MAJOR.MINOR.PATCH, where: MAJOR corresponds to a breaking change (e.g. a change in a component's markup), MINOR to a new feature (e.g. a new component, a new feature for an existing component, etc.), and PATCH to a bug fix or under-the-hood change (e.g. code clean-up, performance improvement, etc.)


Note on rebase: rebasing dev onto master avoids creating a merge commit that would require merging master back into dev.

Semi-automatic deployment to dev
Pre-release builds can be created like this (using the git pre-push hook behind the scenes):

Check out a clean dev branch
in bash git push


This will increment the pre-release version number and make a commit to your local repository


in bash git push again


You will be prompted that this will trigger a build. Answer 'y'
This will push (only) your version number change commit to the remote dev repo


After the normal checks a build with the new version will be triggered by Semaphore


Note on pre-release versions: These are legitimate semver versions. They have the format MAJOR.MINOR.PATCH-beta.NUMBER. Only these pre-release versions will be published on dev.

Testing
How could you test before going live?

The dev branch is set up into Semaphore CI to deploy into the AWS S3 Bucket in a folder called latest where the CMS team can appoint to latest and test it out before go to production.


Each pull request that is opened, also is automatically generated a comment with a preview link to test it.


Note: Always check the Semaphore CI check into your pull request and make sure is building properly, before merge into dev.

Supported browsers:

last two versions of Chrome, Firefox and Edge
IE 11
Safari 8+
iOS 8.4+
Android 4.4+
Firefox ESR (v52.x)

Recommended mobile devices for testing:

iPhone 4S
iPhone 6
iPad 2
Galaxy s5

Developer documentation

Documenting stories - how to customise the content of the README panel for each story
Icons - how to add new icons, and how to use icons in CSS and Vue components
Contributing - how to get involved and contribute code

Example websites


The following UoM websites are using these components on the new Squiz CMS (Gen 2). Documentation on how to use these components in Edit+ Gen 2 documentation


University Home Page


About Us


Brand Hub


MSpace


",2
juankipedia/CompetitiveProgramming,C++,"CompetitiveProgramming
Contributors:

Juan Diego
Marco Sandoval

Emails:

juandiegp17@gmail.com
kada9001@gmail.com

Description:
Exercises that has been solve on some competitive programming platforms and
theory for those problems, this proyect will work for saving those corret
solutions in order to train for ACM-ICPC competition, and also to give people a
guide to train for competitive contests.
Folders
codeforces -> Solutions for codeforces
coj -> Solutions for caribbean online judge
spoj -> Solutions for sphere online judge
topcoder -> Solutions for topcoder
UVa -> Solutions for UVa
CodeGym/01 -> Data structures supported by the C++ STL.
CodeGym/02 -> Data structures not supported by the C++ STL
(graphs, Union-Find Disjoint Sets, segmented tree)
CodeGym/03 -> Union-Find Disjoint Sets.
Platforms:

COJ
UVa
TopCoder
Codeforces
Spoj

Reference

Competitive Programming, Steven & Felix.
cplusplus.com

Join us at Telegram!
https://t.me/joinchat/Bb3ChxLtmR5sDOeI4OygOQ
",5
indieweb/indieweb-chat-archive,PHP,"IndieWeb Chat Archive
This repo contains the full archive of IndieWeb chat log data files visible at https://chat.indieweb.org
Chat logs are added to this repo every 15 minutes.
File Format
Each channel's files can be read using QuartzDB. The files follow a simple format:
2017-12-01 23:15:06.218000 {""type"":""message"",""timestamp"":1512170106.218,""network"":""irc"",""server"":""freenode"",""channel"":{""uid"":""#indieweb"",""name"":""#indieweb""},""author"":{""uid"":""Loqi"",""nickname"":""Loqi"",""username"":""Loqi"",""name"":""Loqi"",""photo"":null,""url"":null,""tz"":""US\/Pacific""},""content"":""[@indiewebcamp] This week in the #indieweb https://indieweb.org/this-week/2017-12-01.html https://pbs.twimg.com/media/DP_z5rCVwAAGdTk.jpg (http://twtr.io/1Yx4r5CHSBC)"",""modes"":[]}


Each line begins with the timestamp.
There will always be 26 characters followed by a space.
The timestamp is UTC and has 6 digits of precision for the seconds.
The rest of the line is a JSON-encoded string representing the IRC message and who sent it.

Spam removal
For a guide on how we deal with spam in these logs, see IRC#Spam on the wiki.
",3
C3DSU/edefpr-backend,PHP,"e-DefPR
Software para Controle de Processos - Defensoria Pública do Paraná
Patrocinador



Sumário

Guia Geral
Workflow
Instalação da Aplicação
Execução e Gerenciamento de Tarefas
Estrutura
Sobre

Guia Geral
Esclarecimentos gerais relacionados a documentação:



1.1 Nomenclaturas:

Issues: tarefas






1.2 Siglas:

PR: Pull Request






1.3 Notas Gerais:

Em comandos, os colchetes [] delimitam que alguns conteúdos devem ser preenchidos em seu lugar;
A distro Ubuntu 16.04 foi utilizada como base de referência para a elaboração desta documentação, em outras distribuições podem ocorrer pequenas variações.



Workflow



2.1 Ferramentas:

Waffle: Gerenciamento de tarefas (issues)
Github: Versionamento
Slack: Chat e bots






2.2 Levantamento e distribuição de tarefas:


2.2.1. Draft (Github):
Consiste no levantamento de demanda semanal em reunião de equipe técnica com equipe de produto, onde são debatidas e anotadas todas as solicitações para serem convertidas em tasks posteriormente.


2.2.2. Tasks (Waffle):


São as menores fragmentações do processo, são as tarefas técnicas executadas para que uma determinada funcionalidade seja implementada, sendo que essas nem sempre são independentes, e são necessárias diversas tarefas técnicas para completar um item de checklist do roadmap.





2.3 Ciclo de vida de Tarefas:


Para cada tarefa há um prazo máximo de execução de 5 dias;


Caso a execução de uma tarefa ultrapasse 5 dias a mesma deve ser reavaliada;


Tarefas devem ser quebradas em caso de:

Tarefas muito grandes;
Tarefas que modifiquem diversas áreas distintas do projeto;
Tarefas em que a execução ultrapasse os 5 dias.

Execução de tarefas em fluxo normal:
1. Iniciada em colunas To do: (Random, Backend, Frontend)
2. Executada pelo Desenvolvedor (in progress)
3. Enviada para Revisão de código pela equipe (review)
4. Marcada como concluída (done)








2.4 Revisão de Pull Request:

As revisões de Pull Request devem ser feitas exclusivamente através do Github;
Comentários devem ser feitos na Pull Request e avisados via Slack;
É proibido realizar merge de Pull Request sem responder aos comentários;






2.5 Solicitações no Slack: utilizamos por padrão flags de classificações no início de cada solicitação.

REVIEW: a notificação de REVIEW, é direcionada para o channel correto, de acordo com a categoria.

Ex.: @here: gianlucabine needs a *REVIEW*: https://github.com/sices/C3DSU/e-DefPR/pull/1/files
Para responder uma solicitação utilizamos por padrão o nome de usuário junto a resposta.
Ex.: @gianlucabine [MESSAGE]

Nota: Para respostas curtas de confirmação pode ser utilizado apenas :+1:



Instalação da Aplicação



3.1 Git e Github:


3.1.1. Instalando o Git


$ sudo apt install git



3.1.2. Configurando informações do Git


$ git config --global user.email ""mail@mail.com""
$ git config --global user.name ""Full Name""



3.1.3. Criando chave para acesso SSH


$ ssh-keygen -t rsa -b 4096 -C ""mail@mail.com""
$ cat ~/.ssh/id_rsa.pub



3.1.4. Inserindo chave SSH no Github
→ Tutorial Github


3.1.5. Clonando o repositório do Github


$ git clone git@github.com:C3DSU/e-DefPR.git






3.2 Bash Profile:


3.2.1. Abra o Arquivo .profile com seu editor (Vim, Nano ou outro)


$ sudo vim ~/.profile



3.2.2. Adicione ao final do arquivo as linhas


export EDEF_PATH=[PROJECT_PATH]
source $EDEF_PATH/devops/config/variables             
export PATH=$PATH:$EDEF_PATH/devops


Nota: lembre-se de substitir [PROJECT_PATH] pelo caminho do projeto.



3.2.3. Carregue as alterações do arquivo bash


$ source ~/.profile



3.2.4. Na variável PATH agora devem aparecer alguns caminhos relacionados a pasta do projeto


$ echo $PATH | grep edef


Nota: caso o comando acima não possua retorno revise os passos de instalação e reinicie o sistema operacional.



Execução e Gerenciamento de Tarefas
Processo automatizado:
1. Executar: edef-issue-start xxx, onde xxx se refere ao número da tarefa no Waffle.
2. Efetuar as modificações no código-fonte.
3. Executar: git add [ARQUIVO INDIVIDUAL ou LISTA DE ARQUIVOS].
    - IMPORTANTE: Não recomendo o uso de: 'git add .'
4. Executar: git commit -m ""MENSAGEM EXPLICATIVA"" após cada 'git add [ARQUIVO INDIVIDUAL ou LISTA DE ARQUIVOS]' do passo 6.
    - IMPORTANTE: Na ""MENSAGEM EXPLICATIVA"" explicar de forma resumida o que foi modificado nos arquivos que que foram adicionados no 'git add'.
5. Executar: git push origin issue#xxx, onde xxx se refere ao número da tarefa no Waffle.
6. Executar: edef-issue-request-review auto ou edef-issue-request-review manual (auto cria a PR automaticamente e o manual redirecionada para a página do GitHub)
7. Comunicar no channel o link da PR pedindo review de código.
8. Esperar pelo menos 1 ou 2 Approves e após isso realizar o merge no site do GitHub.
    - IMPORTANTE: Caso as modificações foram complexas e/ou muito imporantes requisitar mais Approves do que somente 2.
9. Ir na página da Pull Request no GitHub e realizar o merge.

Processo manual:
1. Comunicar no channel apropriado (backend, frontend, random) o início da tarefa
2. Executar: git pull origin master
3. Executar: git checkout -b issue#xxx, onde XXX é o numero da issue no Waffle.
4. Executar: git push --set-upstream origin issue#xxx, onde XXX é o numero da issue no Waffle.
5. Mover a tarefa para a coluna In-progress no Waffle caso não for movida automaticamente.
6. Efetuar as modificações do código fonte.
7. Executar: git add [ARQUIVO INDIVIDUAL ou LISTA DE ARQUIVOS].
    - IMPORTANTE: Não recomendo o uso de: 'git add .'
8. Executar: git commit -m ""MENSAGEM EXPLICATIVA"" após cada 'git add [ARQUIVO INDIVIDUAL ou LISTA DE ARQUIVOS]' do passo 6.
    - IMPORTANTE: Na ""MENSAGEM EXPLICATIVA"" explicar de forma resumida o que foi modificado nos arquivos que que foram adicionados no 'git add'.
9. Executar: git push origin issue#xxx, onde xxx se refere ao número da tarefa no Waffle..
10. Ir na página do repositório no GitHub na parte de branches e criar a PR (Pull Request).
    - IMPORTANTE: Na descrição da PR colocar: fixed #XXX, onde XXX é o numero da issue no Waffle.
12. Comunicar no channel o link da PR pedindo review de código.
13. Esperar pelo menos 1 ou 2 Approves e após isso realizar o merge no site do GitHub.
    - IMPORTANTE: Caso as modificações foram complexas e/ou muito imporantes requisitar mais Approves do que somente 2.
14. Ir na página da Pull Request no GitHub e realizar o merge.
15. Executar: git checkout master
16. Executar: git pull origin master
17. Executar: git branch -d issue#xxx.

Inicialização
Frontend:
Dentro da pasta Frontend
1. yarn install
2. yarn start

Backend:
Dentro da pasta Backend
1. composer install
2. php artisan migrate:refresh
3. php artisan migrate
4. php artisan passport:install
5. php artisan db:seed
6. php artisan serve

Estrutura



6.1 Ambientes:

local: servidor local de desenvolvimento, configurado na máquina de cada desenvolvedor
production: servidor remoto de produção, base final de uso de deploy manual






6.2 Pastas raiz:

backend: pasta backend do projeto, contendo arquivos de models e controllers
frontend: pasta frontend do projeto, contendo arquivos de views
devops: pasta de uso geral de devops, como operações de ecossistema, processos, etc
docs: além do README.md, utilizamos essa pasta para documentações de arquivos e UML



Sobre



7.1 A equipe:


Gianluca Bine


Backend developer
Slack: @gianlucabine
Github: @Pr3d4dor
E-mail: gian_bine@hotmail.com



Jean Pierri


Backend developer
Slack: @envikeyy
Github: @EnViKeyy
E-mail: pierre.jp@outlook.com



Higor Gardin


Frontend developer
Slack: @HGardin
Github: @HigorG
E-mail: higorgardin@hotmail.com



Prof. Mauro Miazaki


Professor
Slack: @mmiazaki
Github: @hmmiazaki
E-mail: maurom@unicentro.br



Prof. Marcos Antônio Quináia


Professor
Slack: @marcosquinaia
E-mail: quiana@unicentro.br



Davi Bastos


Backend developer
Slack: @Davi Bastos
Github: @dav3sk
E-mail: davibastos.v@gmail.com



Enrique Augusto da Roza


Backend developer
Slack: @Enrique Augusto da Roza
Github: @hdoidao
E-mail: enriqueaugroza@gmail.com



Luiz Eduardo Chicouski da Cruz


Slack: @Luiz Eduardo Chicouski da Cruz
Github: @SUdoWinchester
E-mail: luizeduardo2607@gmail.com



Paulo Henrique Pieczarka da Silva


Fullstack developer
Slack: @paulopieczarka
Github: @paulopieczarka
E-mail: paulopieczarka@gmail.com



Alexandre Gueths


Frontend developer
Slack: @agueths
Github: @agueths
E-mail: agueths@gmail.com



Gabriel Pimenta


Backend developer
Slack: @Pimenta
Github: @Pimenta1
E-mail: pimenta@programmer.net



",3
Egregius/PHP-Floorplan-and-advanced-automation-for-Domoticz,PHP,"PHP-Floorplan-and-advanced-automation-for-Domoticz
Advanced automation and floorplans for Domoticz system written in PHP.
A new combined repository of the previous deprecated repo's:
https://github.com/Egregius/PHP-Custom-Floorplan-for-Domoticz
https://github.com/Egregius/LUA-Pass2PHP-for-Domoticz

The repository is automatically updated so all the changes I made are reflected instantly here.


Follow the Domoticz forum topic for information at https://www.domoticz.com/forum/viewtopic.php?f=64&t=12343&start=680#p211279

@gimic wrote his installation steps in https://www.domoticz.com/forum/viewtopic.php?f=64&t=12343&start=740#p211699

Also @sincze wrote a nice post with information at https://www.domoticz.com/forum/viewtopic.php?p=212446#p212453

Screenshots of the floorplans are available at https://egregius.be/2019/pass2php-and-floorplan-v3-for-domoticz/

This repository is not intended to be used with git pull. There are to many personal changes that has to be done.
You can however do a git pull to a different folder outside your www root and just include some files in your own local copy of the repository so that for example the functions.php file stays up to date.

This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
You should have received a copy of the GNU General Public License along with this program. If not, see http://www.gnu.org/licenses/.
",2
parkr/status,JavaScript,"status
This is a repository hosting a status site for my various web properties.
This repository uses sourcegraph/checkup to write to the updates/ directory.
Web
Normal usage of this repository is just visiting https://www.parkermoore.de/status/. It shows a lovely series of graphs for my web properties. It tracks up, down, and degraded states.
Generating
This repo uses Jess Frazelle's Docker image, r.j3ss.co/checkup to run checkup.
It is passed a configuration file like this:
{
  ""storage"": {
    ""provider"": ""github"",
    ""access_token"": ""some_api_access_token_with_repo_scope"",
    ""repository_owner"": ""owner"",
    ""repository_name"": ""repo"",
    ""committer_name"": ""Commiter Name"",
    ""committer_email"": ""you@yours.com"",
    ""branch"": ""gh-pages"",
    ""dir"": ""updates""
  },
  ""checkers"": [
    {
      ""type"": ""http"",
      ""endpoint_name"": ""Example HTTP"",
      ""endpoint_url"": ""http://www.example.com""
    }
  ]
}
Then, I run checkup on a cron. It will automatically write to GitHub.
",5
C3DSU/edefpr-backend,PHP,"e-DefPR
Software para Controle de Processos - Defensoria Pública do Paraná
Patrocinador



Sumário

Guia Geral
Workflow
Instalação da Aplicação
Execução e Gerenciamento de Tarefas
Estrutura
Sobre

Guia Geral
Esclarecimentos gerais relacionados a documentação:



1.1 Nomenclaturas:

Issues: tarefas






1.2 Siglas:

PR: Pull Request






1.3 Notas Gerais:

Em comandos, os colchetes [] delimitam que alguns conteúdos devem ser preenchidos em seu lugar;
A distro Ubuntu 16.04 foi utilizada como base de referência para a elaboração desta documentação, em outras distribuições podem ocorrer pequenas variações.



Workflow



2.1 Ferramentas:

Waffle: Gerenciamento de tarefas (issues)
Github: Versionamento
Slack: Chat e bots






2.2 Levantamento e distribuição de tarefas:


2.2.1. Draft (Github):
Consiste no levantamento de demanda semanal em reunião de equipe técnica com equipe de produto, onde são debatidas e anotadas todas as solicitações para serem convertidas em tasks posteriormente.


2.2.2. Tasks (Waffle):


São as menores fragmentações do processo, são as tarefas técnicas executadas para que uma determinada funcionalidade seja implementada, sendo que essas nem sempre são independentes, e são necessárias diversas tarefas técnicas para completar um item de checklist do roadmap.





2.3 Ciclo de vida de Tarefas:


Para cada tarefa há um prazo máximo de execução de 5 dias;


Caso a execução de uma tarefa ultrapasse 5 dias a mesma deve ser reavaliada;


Tarefas devem ser quebradas em caso de:

Tarefas muito grandes;
Tarefas que modifiquem diversas áreas distintas do projeto;
Tarefas em que a execução ultrapasse os 5 dias.

Execução de tarefas em fluxo normal:
1. Iniciada em colunas To do: (Random, Backend, Frontend)
2. Executada pelo Desenvolvedor (in progress)
3. Enviada para Revisão de código pela equipe (review)
4. Marcada como concluída (done)








2.4 Revisão de Pull Request:

As revisões de Pull Request devem ser feitas exclusivamente através do Github;
Comentários devem ser feitos na Pull Request e avisados via Slack;
É proibido realizar merge de Pull Request sem responder aos comentários;






2.5 Solicitações no Slack: utilizamos por padrão flags de classificações no início de cada solicitação.

REVIEW: a notificação de REVIEW, é direcionada para o channel correto, de acordo com a categoria.

Ex.: @here: gianlucabine needs a *REVIEW*: https://github.com/sices/C3DSU/e-DefPR/pull/1/files
Para responder uma solicitação utilizamos por padrão o nome de usuário junto a resposta.
Ex.: @gianlucabine [MESSAGE]

Nota: Para respostas curtas de confirmação pode ser utilizado apenas :+1:



Instalação da Aplicação



3.1 Git e Github:


3.1.1. Instalando o Git


$ sudo apt install git



3.1.2. Configurando informações do Git


$ git config --global user.email ""mail@mail.com""
$ git config --global user.name ""Full Name""



3.1.3. Criando chave para acesso SSH


$ ssh-keygen -t rsa -b 4096 -C ""mail@mail.com""
$ cat ~/.ssh/id_rsa.pub



3.1.4. Inserindo chave SSH no Github
→ Tutorial Github


3.1.5. Clonando o repositório do Github


$ git clone git@github.com:C3DSU/e-DefPR.git






3.2 Bash Profile:


3.2.1. Abra o Arquivo .profile com seu editor (Vim, Nano ou outro)


$ sudo vim ~/.profile



3.2.2. Adicione ao final do arquivo as linhas


export EDEF_PATH=[PROJECT_PATH]
source $EDEF_PATH/devops/config/variables             
export PATH=$PATH:$EDEF_PATH/devops


Nota: lembre-se de substitir [PROJECT_PATH] pelo caminho do projeto.



3.2.3. Carregue as alterações do arquivo bash


$ source ~/.profile



3.2.4. Na variável PATH agora devem aparecer alguns caminhos relacionados a pasta do projeto


$ echo $PATH | grep edef


Nota: caso o comando acima não possua retorno revise os passos de instalação e reinicie o sistema operacional.



Execução e Gerenciamento de Tarefas
Processo automatizado:
1. Executar: edef-issue-start xxx, onde xxx se refere ao número da tarefa no Waffle.
2. Efetuar as modificações no código-fonte.
3. Executar: git add [ARQUIVO INDIVIDUAL ou LISTA DE ARQUIVOS].
    - IMPORTANTE: Não recomendo o uso de: 'git add .'
4. Executar: git commit -m ""MENSAGEM EXPLICATIVA"" após cada 'git add [ARQUIVO INDIVIDUAL ou LISTA DE ARQUIVOS]' do passo 6.
    - IMPORTANTE: Na ""MENSAGEM EXPLICATIVA"" explicar de forma resumida o que foi modificado nos arquivos que que foram adicionados no 'git add'.
5. Executar: git push origin issue#xxx, onde xxx se refere ao número da tarefa no Waffle.
6. Executar: edef-issue-request-review auto ou edef-issue-request-review manual (auto cria a PR automaticamente e o manual redirecionada para a página do GitHub)
7. Comunicar no channel o link da PR pedindo review de código.
8. Esperar pelo menos 1 ou 2 Approves e após isso realizar o merge no site do GitHub.
    - IMPORTANTE: Caso as modificações foram complexas e/ou muito imporantes requisitar mais Approves do que somente 2.
9. Ir na página da Pull Request no GitHub e realizar o merge.

Processo manual:
1. Comunicar no channel apropriado (backend, frontend, random) o início da tarefa
2. Executar: git pull origin master
3. Executar: git checkout -b issue#xxx, onde XXX é o numero da issue no Waffle.
4. Executar: git push --set-upstream origin issue#xxx, onde XXX é o numero da issue no Waffle.
5. Mover a tarefa para a coluna In-progress no Waffle caso não for movida automaticamente.
6. Efetuar as modificações do código fonte.
7. Executar: git add [ARQUIVO INDIVIDUAL ou LISTA DE ARQUIVOS].
    - IMPORTANTE: Não recomendo o uso de: 'git add .'
8. Executar: git commit -m ""MENSAGEM EXPLICATIVA"" após cada 'git add [ARQUIVO INDIVIDUAL ou LISTA DE ARQUIVOS]' do passo 6.
    - IMPORTANTE: Na ""MENSAGEM EXPLICATIVA"" explicar de forma resumida o que foi modificado nos arquivos que que foram adicionados no 'git add'.
9. Executar: git push origin issue#xxx, onde xxx se refere ao número da tarefa no Waffle..
10. Ir na página do repositório no GitHub na parte de branches e criar a PR (Pull Request).
    - IMPORTANTE: Na descrição da PR colocar: fixed #XXX, onde XXX é o numero da issue no Waffle.
12. Comunicar no channel o link da PR pedindo review de código.
13. Esperar pelo menos 1 ou 2 Approves e após isso realizar o merge no site do GitHub.
    - IMPORTANTE: Caso as modificações foram complexas e/ou muito imporantes requisitar mais Approves do que somente 2.
14. Ir na página da Pull Request no GitHub e realizar o merge.
15. Executar: git checkout master
16. Executar: git pull origin master
17. Executar: git branch -d issue#xxx.

Inicialização
Frontend:
Dentro da pasta Frontend
1. yarn install
2. yarn start

Backend:
Dentro da pasta Backend
1. composer install
2. php artisan migrate:refresh
3. php artisan migrate
4. php artisan passport:install
5. php artisan db:seed
6. php artisan serve

Estrutura



6.1 Ambientes:

local: servidor local de desenvolvimento, configurado na máquina de cada desenvolvedor
production: servidor remoto de produção, base final de uso de deploy manual






6.2 Pastas raiz:

backend: pasta backend do projeto, contendo arquivos de models e controllers
frontend: pasta frontend do projeto, contendo arquivos de views
devops: pasta de uso geral de devops, como operações de ecossistema, processos, etc
docs: além do README.md, utilizamos essa pasta para documentações de arquivos e UML



Sobre



7.1 A equipe:


Gianluca Bine


Backend developer
Slack: @gianlucabine
Github: @Pr3d4dor
E-mail: gian_bine@hotmail.com



Jean Pierri


Backend developer
Slack: @envikeyy
Github: @EnViKeyy
E-mail: pierre.jp@outlook.com



Higor Gardin


Frontend developer
Slack: @HGardin
Github: @HigorG
E-mail: higorgardin@hotmail.com



Prof. Mauro Miazaki


Professor
Slack: @mmiazaki
Github: @hmmiazaki
E-mail: maurom@unicentro.br



Prof. Marcos Antônio Quináia


Professor
Slack: @marcosquinaia
E-mail: quiana@unicentro.br



Davi Bastos


Backend developer
Slack: @Davi Bastos
Github: @dav3sk
E-mail: davibastos.v@gmail.com



Enrique Augusto da Roza


Backend developer
Slack: @Enrique Augusto da Roza
Github: @hdoidao
E-mail: enriqueaugroza@gmail.com



Luiz Eduardo Chicouski da Cruz


Slack: @Luiz Eduardo Chicouski da Cruz
Github: @SUdoWinchester
E-mail: luizeduardo2607@gmail.com



Paulo Henrique Pieczarka da Silva


Fullstack developer
Slack: @paulopieczarka
Github: @paulopieczarka
E-mail: paulopieczarka@gmail.com



Alexandre Gueths


Frontend developer
Slack: @agueths
Github: @agueths
E-mail: agueths@gmail.com



Gabriel Pimenta


Backend developer
Slack: @Pimenta
Github: @Pimenta1
E-mail: pimenta@programmer.net



",3
Egregius/PHP-Floorplan-and-advanced-automation-for-Domoticz,PHP,"PHP-Floorplan-and-advanced-automation-for-Domoticz
Advanced automation and floorplans for Domoticz system written in PHP.
A new combined repository of the previous deprecated repo's:
https://github.com/Egregius/PHP-Custom-Floorplan-for-Domoticz
https://github.com/Egregius/LUA-Pass2PHP-for-Domoticz

The repository is automatically updated so all the changes I made are reflected instantly here.


Follow the Domoticz forum topic for information at https://www.domoticz.com/forum/viewtopic.php?f=64&t=12343&start=680#p211279

@gimic wrote his installation steps in https://www.domoticz.com/forum/viewtopic.php?f=64&t=12343&start=740#p211699

Also @sincze wrote a nice post with information at https://www.domoticz.com/forum/viewtopic.php?p=212446#p212453

Screenshots of the floorplans are available at https://egregius.be/2019/pass2php-and-floorplan-v3-for-domoticz/

This repository is not intended to be used with git pull. There are to many personal changes that has to be done.
You can however do a git pull to a different folder outside your www root and just include some files in your own local copy of the repository so that for example the functions.php file stays up to date.

This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
You should have received a copy of the GNU General Public License along with this program. If not, see http://www.gnu.org/licenses/.
",2
parkr/status,JavaScript,"status
This is a repository hosting a status site for my various web properties.
This repository uses sourcegraph/checkup to write to the updates/ directory.
Web
Normal usage of this repository is just visiting https://www.parkermoore.de/status/. It shows a lovely series of graphs for my web properties. It tracks up, down, and degraded states.
Generating
This repo uses Jess Frazelle's Docker image, r.j3ss.co/checkup to run checkup.
It is passed a configuration file like this:
{
  ""storage"": {
    ""provider"": ""github"",
    ""access_token"": ""some_api_access_token_with_repo_scope"",
    ""repository_owner"": ""owner"",
    ""repository_name"": ""repo"",
    ""committer_name"": ""Commiter Name"",
    ""committer_email"": ""you@yours.com"",
    ""branch"": ""gh-pages"",
    ""dir"": ""updates""
  },
  ""checkers"": [
    {
      ""type"": ""http"",
      ""endpoint_name"": ""Example HTTP"",
      ""endpoint_url"": ""http://www.example.com""
    }
  ]
}
Then, I run checkup on a cron. It will automatically write to GitHub.
",5
factly/dega-web,Vue,"dega-web

Website template for Dega

Build Setup
# install dependencies
$ npm install

# serve with hot reload at localhost:3000
$ npm run dev

# build for production and launch server
$ npm run build
$ npm start

# generate static project
$ npm run generate
Using Docker to run the production build of Dega Web
Run the following command from the root folder to build for production and start Dega Web in a docker container.
docker-compose build && docker-compose up -d

You should be able to access the website on: http://localhost:8000
Run the following command from the root folder to stop and remove the container:
docker-compose down

",2
DataSketches/DataSketches.github.io,HTML,"Repository for DataSketches website
Documentation
Comments
",30
osmlab/name-suggestion-index,JavaScript,"

name-suggestion-index
Canonical common brand names for OpenStreetMap
What is it?
The goal of this project is to maintain a canonical
list of commonly used names for suggesting consistent spelling and tagging of features
in OpenStreetMap.
How it's used
When mappers create features in OpenStreetMap, they are not always consistent about how they
name and tag things. For example, we may prefer McDonald's tagged as amenity=fast_food
but we see many examples of other spellings (Mc Donald's, McDonalds, McDonald’s) and
taggings (amenity=restaurant).
Building a canonical name index allows two very useful things:

We can suggest the most ""correct"" way to tag things as users create them while editing.
We can scan the OSM data for ""incorrect"" features and produce lists for review and cleanup.


The name-suggestion-index is in use in iD when adding a new item
Currently used in:

iD (see above)
Vespucci
JOSM presets available

Browse the index
You can browse the index at
http://osmlab.github.io/name-suggestion-index/brands/index.html
to see which brands are missing Wikidata links, or have incomplete Wikipedia pages.
Participate!

Read the project Code of Conduct and remember to be nice to one another.
See CONTRIBUTING.md for info about how to contribute to this index.

We're always looking for help!  If you have any questions or want to reach out to a maintainer, ping bhousel on:

OpenStreetMap US Slack
(#poi or #general channels)

Prerequisites

Node.js version 6 or newer
git for your platform

Installing

Clone this project, for example:
git clone git@github.com:osmlab/name-suggestion-index.git
cd into the project folder,
Run npm install to install libraries

About the index
Generated files (do not edit):
Preset files (used by OSM editors):

dist/name-suggestions.json - Name suggestion presets
dist/name-suggestions.min.json - Name suggestion presets, minified
dist/name-suggestions.presets.xml - Name suggestion presets, as JOSM-style preset XML

Name lists:

dist/names_all.json - all the frequent names and tags collected from OpenStreetMap
dist/names_discard.json - subset of names_all we are discarding
dist/names_keep.json - subset of names_all we are keeping
dist/wikidata.json - cached brand data retrieved from Wikidata

Configuration files (edit these):

config/*

config/filters.json- Regular expressions used to filter names_all into names_keep / discardNames


brands/* - Config files for each kind of branded business, organized by OpenStreetMap tag

brands/amenity/*.json
brands/leisure/*.json
brands/shop/*.json
brands/tourism/*.json



👉 See CONTRIBUTING.md for info about how to contribute to this index.
Building the index

npm run build

Regenerates dist/names_keep.json and dist/names_discard.json
Any new entries from names_keep not already present in the index will be added to it
Outputs many warnings to suggest updates to brands/**/*.json



Other commands

npm run wikidata - Fetch useful data from Wikidata - labels, descriptions, logos, etc.
npm run docs - Updates the index summary pages
npm run - Lists other available tools

Updating dist/names_all.json from planet
This takes a long time and a lot of disk space. It can be done occasionally by project maintainers.
You do not need to do these steps in order to contribute to the index.

Install osmium commandline tool and node package globally (may only work on some environments)

apt-get install osmium-tool or brew install osmium-tool or similar
npm install -g osmium


Download the planet

curl -o planet-latest.osm.pbf https://planet.openstreetmap.org/pbf/planet-latest.osm.pbf


Prefilter the planet file to only include named items with keys we are looking for:

osmium tags-filter planet-latest.osm.pbf -R name -o named.osm.pbf
osmium tags-filter named.osm.pbf -R amenity,shop,leisure,tourism -o wanted.osm.pbf


Run node build_all_names wanted.osm.pbf

results will go in dist/names_all.json
git add dist/names_all.json && git commit -m 'Updated dist/names_all.json'



License
name-suggestion-index is available under the 3-Clause BSD License.
See the LICENSE.md file for more details.
",139
theRemix/Multi-Stage-Node-Image,JavaScript,"Multi-Stage Node Image
Demo repo for building a minimal node app image
See Dockerfile
Building
Building the multi-stage docker image will run the tests, and only complete the build if the tests pass.
docker build -t theremix/multi-stage-node-image:latest .
this will run the tests in test/, then copy over just the artifacts needed to run the project in production, resulting in a minimal docker image
Successfully tagged theremix/multi-stage-node-image:latest

Test Failures
Watch the build fail by changing the test/api-spec.js, then attempt to build again.
results:
> multi-stage-node-image@1.0.0 test /build
> mocha



  Basic demo http json api
    1) should respond to GET /


  0 passing (28ms)
  1 failing

  1) Basic demo http json api
       should respond to GET /:

      AssertionError: expected 'ok' to equal 'not ok'
      + expected - actual

      -ok
      +not ok

      at request.get.expect.expect.expect.then.res (test/api-spec.js:14:31)
      at <anonymous>
      at process._tickCallback (internal/process/next_tick.js:188:7)



npm ERR! Test failed.  See above for more details.
The command '/bin/sh -c npm test' returned a non-zero code: 1

Running
Run a container with
docker run --rm -it -p 3000:3000 theremix/multi-stage-node-image:latest
or daemonized with
docker run -d --name=multi-stage-node-demo -p 3000:3000 theremix/multi-stage-node-image:latest
use curl
curl localhost:3000
{""data"":""ok""}
",3
1Computer1/comp_iler,JavaScript,"Comp_iler
Sandboxed code execution discord bot.
Invite the bot!
Usage
Code Blocks
>```lang
code
```

>options```lang
code
```

Inline Code
>`lang code`

>options`lang code`

Examples
>```cpp
#include <iostream>

int main()
{
    std::cout << ""Hello World!"" << std::endl;
}
```

>harmony```js
class Foo {
    bar = 1;
}

console.log(new Foo().bar);
```

>`py print('hello world')`

>e`hs (+) <$> Just 1 <*> Just 2`

Supported Languages and Options
One of the following language codes is set in lang.
Options are optionally set in options, which is a semicolon-delimited list of flag or flag=value.

bash Bash
c C (GCC)
clj Clojure
cpp C++ (G++)
cs C# (Mono)

e evaluates a single expression instead of a module


elixir Elixir
fs F# (Mono)
go Go
hs Haskell (GHC)

e evaluates a single expression instead of a module


java Java (OpenJDK)
js JavaScript (Node)

harmony enables harmony features (--harmony on node)
e prints the result of evaluating the code


julia Julia

e prints the result of evaluating the code


lisp Racket
lua Lua
ocaml OCaml
pas Pascal (FPC)
php PHP
pl Perl5
prolog Prolog (SWI-Prolog)
py Python (CPython)

2 runs Python 2 instead of Python 3


rb Ruby
rs Rust

How it Works
Read the source code, specifically src/struct/LanguageHandler.js.
In summary, for every language there is a docker image which spins up a docker container.
The container is used for all evaluations of code, restarting if something goes wrong.
The container is locked down, so there is no networking, limited memory and CPU usage, and a time limit.
Setup

Install Docker 18+
Install Node 10+
Run npm i
Fill out config.json

owner - The owner(s) of the bot.
Use an array for multiple owners.
token - The bot token.
languages Languages to use.
The language names here are different from the user-facing ones.
Check the folders in docker/ for the language names.
Change to null to enable all languages.
memory Max memory usage of a container.
cpu Max CPU usage of a container.
timeout Time limit for code in milliseconds.
prepare Whether to run containers on setup.
Setting to true will speed up the first eval, but that language might not be used.


Run node .

",16
c-keys/c-keys.github.io,CSS,"Future home of the ckeys website. Mechanical keyboards and workshops.
",3
WeihanLi/ActivityReservation,JavaScript,"ActivityReservation 
Intro
活动室预约系统，起初的设计和开发是因为学校活动室预约流程希望从之前繁琐低效的完全线下预约
修改为线上预约+线下盖章审批的方式来预约学校的活动室。
原本是用 ASP.NET WebForm 写的，后来用 ASP.NET MVC 重写一下并增加一些功能，最近迁移到 asp.net core 重新上线
演示地址：https://reservation.weihanli.xyz
后台登录地址： https://reservation.weihanli.xyz/Admin/Account/Login
后台登录账号：
管理员用户名: admin 密码: Admin888
普通用户： Alice 密码：Test1234
管理员有更多的权限，可以设置更多系统相关的配置，也可以增加系统普通管理员
演示地址的部署方式是通过 Docker 部署在我的虚拟上的，网站前面有个 Nginx 作为反向代理，现在已经配置了 Azure pipeline 可以自动化的 build docker 镜像并 push 镜像到 docker hub，之后 SSH 到虚拟机上，重新部署到 docker。
Roadmap
Check it here
",20
houqp/vimrc-houqp,Vim script,"This is houqp's vim configuration, use at your own risk. ;-P
Installation
Move the whole repository into your home directory and rename to .vim.
Then issue following command in .vim directory:
$ make install
NOTE: you will need libclang to install the YouComplateMe plugin.
Usage
Mappings
<leader><TAB>: switch to previous window.
<leader>tl: toggle TagBar window.
<leader>f: find files.
<leader>bf: find buffers.
<leader>r: show MRU list.
<leader>be: show buffer lists.
<leader>w: quick save in normal mode.
<leader>h: toggle hex mode.
<leader>t: show tasklist (todo, fix, etc).
<leader>q: toggle quickfix window.
[q & ]q: previous & next quickfix item.
<Ctrl>s: quick save in insert mode.
<Ctrl>j: jump to place holder.
<Ctrl>l: OmniComplete.
<Alt><F5>: issue make command in current directory.
Commands
Ag KEYWORD: search keyword using ag.
Agc: search current word under cursor using ag.
Use sessions for project management
Because I use different color scheme for .gvimrc and .vimrc, so I only
enabled auto session management in GUI. Every time you fire up GVim with no arguments,
the session mode will be set and you can use <Leader><Leader>ms to save a
session on current directory.
Then you can easily recover your previous workspace by fire up GVim in the
same directory, where Session.vim is saved. When you quit GVim, it will automatically
save the session for you.
Tips for installed plugins
Colorizer
Colorizer is turned off by default, use command ColorToggle to turn it on.
BufExplorer
:help bufexplorer
VOoM
:Voom format
VOoM supports lots of format, check out this page for the list.
easymotion
The default leader for easymotion has been changed to  to avoid
conflcts. So use <Leader><Leader>w to trigger the word motion.
CtrlP
inside the prompt:

toggle search by filename between by full path: <Ctrl>d
selection up: <Ctrl>k
selection down: <Ctrl>j
exit: <Ctrl>c, <Ctrl>g, <esc>
toggle regexp search: <Ctrl>r
purge cache: <F5>
forward: <Ctrl>f
backward: <Ctrl>b
wipe MRU list: <F7>
help page: ?+<CR>

Tagbar
in Tagbar buffer:

display help page: <F1>
change tags sort order: s
folding: same as Vim's built-in mappings:

open: zo
close: zc
toggle: za
open all: zR
close all: zM


goto next top level: <Ctrl>N
goto previous top level: <Ctrl>P
zoom tagbar window: x
display tag protogype: <Space>

echofunc
next proto: <Alt>=
previous proto: <Alt>-
Fugitive
find out the author of every line: Gblame
add current editing file: Gwrite
commit staged changes: Gcommit
view revisions for current file: Glog
show complete commit logs: Glog --
GitGutter
Toggle GitGutter: <leader>gd or :GitGutterToggle
jump bwteen hunks: ]h, [h
vim-trailing-whitespace
use FixWhitespace command to automatically remove all trailing whitespaces.
tcomment_vim
toggle comment: gc
",6
jonathanpoelen/katepart-script,HTML,"katepart-script
These scripts adds more than one hundred new commands to the Kate Editor and derivatives (KWrite, KDevelop, Kile, etc).
By default, the shortcut to command line is f7.
Documentation
Open ./doc-script/functions-descriptions.xml or ./doc-script/doc.html with a browser (fr with examples) or used help a_command in the command line.
Install
KDE 5:
scriptdir=""${XDG_DATA_HOME=~/.local/share}/katepart5/script/""
tmpdir=""${TMPDIR=/tmp}/katepart-script-$$""
mkdir -p ""$tmpdir/commands"" ""$scriptdir/commands"" &&
cp -ri libraries/ ""$scriptdir"" &&
for f in commands/* ; do ./4to5.sh ""$f"" > ""$tmpdir/$f"" ; done &&
mv -i ""$tmpdir/commands""/* ""$scriptdir/commands""
Or merge commands into one file:
cd commands
jsfile=""${TMPDIR=/tmp}""/katepartscript.kfs4.js
../build.sh *.js > ""$jsfile"" && \
../4to5.sh ""$jsfile"" > ""${XDG_DATA_HOME=~/.local/share}""/katepart5/script/commands/katepartscript.kfs5.js
KDE 4:
cp -ri libraries/ commands/ ""${KDEHOME=~}""/.kde/share/apps/katepart/script/
Or merge commands into one file with build.sh:
cd commands
../build.sh *.js > ""${KDEHOME=~}""/.kde/share/app/katepart/script/commands/katepartscript.kfs4.js
",3
castle-games/share.lua,Lua,"share.lua
share.lua is a set of modules that aims to make it easier to make multiplayer games with Lua, especially with LÖVE and Castle. LÖVE provides the lua-enet library that is used for network communication, and Castle provides the lua-marshal library that is used for serialization. Castle can also be used to automatically run a game server in the cloud.
Files in this repository:

state.lua -- A 'state' data structure that keeps track of changes to itself so that they can be sent over the network. Can store any serializable data. Every table in a state can only appear once (and thus cycles are not allowed either).
tests.lua -- Tests for the state data structure.
cs.lua -- 'client' and 'server' modules that allow setting up events to listen for on clients and servers and hook up state data structures for transferring data. The 'share' state can be written to by the server and read by all clients, and each client gets a 'home' state that it can write to and the server can read.
example_server.lua, example_client.lua -- Example usage of the 'cs' module. Start here to learn how to use these modules.
example_local.lua -- Run both the example client and the server locally in the same process for easy testing.
example_castle.lua -- Run the example client locally and the server in the cloud using Castle.

",15
henrysher/fedora-infra-ansible,JavaScript,"Fedora Infrastructure
Welcome! This is the Fedora Infrastructure Pagure project.
issues against this project are for issues in Fedora Infrastructure.
git repo of this project is misc scripts and tools for Fedora
If you are looking for the Fedora Infrastructure ansible repo,
that is not here, look at:
https://infrastructure.fedoraproject.org/cgit/ansible.git/
If you would like to help out with Fedora Infrastructure,
see:
https://fedoraproject.org/wiki/Infrastructure/GettingStarted
and
https://fedoraproject.org/wiki/Infrastructure_Apprentice
",23
wobblui/wobblui,Python,"wobblui
(experimental project)
Wobblui is a versatile & easy-to-use UI framework for Python 3!
Why wobblui is awesome:

Cross-platform: Works on Windows, Linux, and
Android
Easy: simple API & versatile auto-scaling box layouts
Efficient: 3d accelerated, on-demand redraw/relayout ('lazy') and more!

It also has a consistent look on all platforms and supports styling,
including freeform UI scaling!
This is how easy wobblui is to use:
from wobblui import event_loop
from wobblui.label import Label
from wobblui.window import Window

w = Window()
w.add(Label(""Hello World! This is a wobblui example!""))

event_loop()

See the Quickstart Guide for more!
Installation
You'll need SDL2 and some SDL2-related libraries as prerequisite,
see the Installation Guide.
Afterwards, just install from pip:
pip install --user -U wobblui

(make sure to use a Python 3.X pip! Python 2 is NOT supported)
Documentation
Jump into the documentation here!
License
Wobblui is open-source under various licenses (due to some included
3rd-party components), but most of it is zlib-licensed.
See the LICENSE.md document for full details!
",3
GorillaStack/acl,JavaScript,"
ACL
ACL is a JavaScript library based on the Zend Permissions ACL library that works equally well on the server as in the browser.
Introduction
ACL is a role/resource based ACL that allows for easy definition of permissions by combining rules for specific roles, resources and privileges. Roles can inherit from earlier defined roles and resources can inherit from earlier defined resources. After the ACL is loaded with a permissions set, easy testing through the isAllowed method returns either a true or false value.
Installation
npm install --save acljs

Tests
To run the tests, after cloning this repository first install the required dependencies:
npm install

You can now run the tests by issuing the following command:
./node_modules/.bin/jasmine

Usage
To use ACL start by defining a permissions list. We can start with an empty list:
var permissions = {
  roles:     [],
  resources: [],
  rules:     []
};
Our permissions list contains three top-level requirements, roles, resources and rules. The idea behind this role based ACL is that a specific role has access to resources through specified rules. Don't confuse the elements you define in this list with 'real' objects in your application. The ACL is simply be a structure (or model) we can test against, it can be static and therefore it's not required to be stored in a database. You can define the ACL as a business object in your application or as part of your business rules. However, if you prefer, or if your ACL is dynamic, you can store the permissions set in database if you wish to do so.
OK. Let's add some permissions..
For the purpose of this demonstration we define four roles; guest, member, author and admin. For the sake of argument, we define the resources for a simple blog so we have post and comment as resources:
var permissions = {
  roles: [
    {name: ""guest""},
    {name: ""member"", parent: ""guest""},
    {name: ""author"", parent: ""member""},
    {name: ""admin""}
  ],
  resources: [
    {name: ""post""},
    {name: ""comment""}
  ],
  rules: []
};
Easy as. Now lets define a rule that allows guests to view both posts and comments:
var permissions = {
  roles: [
    {name: ""guest""},
    {name: ""member"", parent: ""guest""},
    {name: ""author"", parent: ""member""},
    {name: ""admin""}
  ],
  resources: [
    {name: ""post""},
    {name: ""comment""}
  ],
  rules: [
    {
      access:     ""allow"",
      role:       ""guest"",
      privileges: [""view""],
      resources:  [""post"", ""comment""]
    }
  ]
};
As you can see, the rule is pretty straight forward. both privileges and resources can either be set as single values or as an array. Notice how the values on the right hand side can be read in a meaningful way; ""allow guest to view post & comment"".
Now, let's create a rule that allows members to create comments:
var permissions = {
  roles: [
    {name: ""guest""},
    {name: ""member"", parent: ""guest""},
    {name: ""author"", parent: ""member""},
    {name: ""admin""}
  ],
  resources: [
    {name: ""post""},
    {name: ""comment""}
  ],
  rules: [
    {
      access:     ""allow"",
      role:       ""guest"",
      privileges: [""view""],
      resources:  [""post"", ""comment""]
    }, {
      access:     ""allow"",
      role:       ""member"",
      privileges: [""create""],
      resources:  [""comment""]
    }
  ]
};
Great. Now let's fill in the rest of the permissions:
var permissions = {
  roles: [
    {name: ""guest""},
    {name: ""member"", parent: ""guest""},
    {name: ""author"", parent: ""member""},
    {name: ""admin""}
  ],
  resources: [
    {name: ""post""},
    {name: ""comment""}
  ],
  rules: [
    {
      access:     ""allow"",
      role:       ""guest"",
      privileges: [""view""],
      resources:  [""post"", ""comment""]
    }, {
      access:     ""allow"",
      role:       ""member"",
      privileges: [""create""],
      resources:  [""comment""]
    }, {
      access:     ""allow"",
      role:       ""author"",
      privileges: [""create"", ""edit"", ""delete""],
      resources:  [""post""]
    }, {
      access:     ""allow"",
      role:       ""admin"",
      privileges: null,
      resources:  null
    }
  ]
};
We added the author permissions to allow authors to create, edit and delete posts and we've allowed the admin to perform all privileges (null) or all resources (null).
To use the permissions we need to load the permissions into the ACL, like this:
var acl = new Acl(permissions);
We can now test if a specified role can perform a requested privilege on a specified resource. E.g:
acl.isAllowed('guest', 'post', 'view');
// true

acl.isAllowed('member', 'post', 'delete');
// false

acl.isAllowed('admin', 'post', 'delete');
// true
That's easy as!
",13
hunzhiwange/framework,PHP,"











English | 中文

The QueryPHP Framework

This is the core framework code of QueryPHP application, starts from this moment with QueryPHP.

QueryPHP is a modern, high performance PHP 7 resident framework, with engineer user experience as its historical mission, let every PHP application have a good framework.
A hundred percent coverage of the unit tests to facing the bug,based on Zephir implemented framework resident,with Swoole ecology to achieve business resident,
now or in the future step by step. Our vision is USE LEEVEL WITH SWOOLE DO BETTER, let your business to support more user services.
The PHP Framework For Code Poem As Free As Wind, Starts From This Moment With QueryPHP.

Site: https://www.queryphp.com/
API: https://api.queryphp.com
Document: https://www.queryphp.com/docs/


QueryPHP was based on the DoYouHaoBaby framework which released 0.0.1 version at 2010.10.03.
The core packages

QueryPHP On Github: https://github.com/hunzhiwange/queryphp/
QueryPHP On Gitee: https://gitee.com/dyhb/queryphp/
Framework On Github: https://github.com/hunzhiwange/framework/
Framework On Gitee: https://gitee.com/dyhb/framework/
Leevel On Github: https://github.com/hunzhiwange/leevel/
Leevel On Gitee: https://gitee.com/dyhb/leevel
Tests: https://github.com/leevels/tests/
Packages: https://github.com/leevels/
Packages From Hunzhiwange: https://packagist.org/packages/hunzhiwange/
Packages From Leevel: https://packagist.org/packages/leevel/

Optional Extension








We think the performance of PHP applications is very important and the development of pleasure also needs to be considered, and that's why we have developed the QueryPHP framework to achieve great ideals.

PHP 7 - We choose the lowest version of 7.3.2, because php7 has a unique advantage over the earlier version.
Leevel - We provides an optional c extension to takeover core components of the framework,such as ioc, log, cache.
Swoole - Enable PHP developers to write high-performance, scalable, concurrent TCP, UDP, Unix socket, HTTP, Websocket services in PHP programming language.
Redis - QueryPHP encapsulation a cache component, including files, redis and so on, so as to reduce the pressure of database.
Roadrunner - RoadRunner is an open source high-performance PHP application server, load balancer and process manager. It supports running as a service with the ability to extend its functionality on a per-project basis.

Wow! Cool! Query Yet Simple!

How to install
composer require hunzhiwange/framework

The components that make up the QueryPHP framework
Components for the framework can be found on the Github website and Packagist website.
composer require leevel/di
composer require leevel/cache

... and more

Run Tests
_____________                           _______________
 ______/     \__  _____  ____  ______  / /_  _________
  ____/ __   / / / / _ \/ __`\/ / __ \/ __ \/ __ \___
   __/ / /  / /_/ /  __/ /  \  / /_/ / / / / /_/ /__
     \_\ \_/\____/\___/_/   / / .___/_/ /_/ .___/
        \_\                /_/_/         /_/

$cd /data/codes/queryphp/vendor/hunzhiwange/framework      
$composer install
$cp ./tests/config.php ./tests/config.local.php // Modify the config
$php vendor/bin/phinx migrate
$php vendor/bin/phpunit tests

Make Doc For Framework
$cd /data/codes/queryphp
$php leevel make:docwithin tests

Unified Code Style
Install PHP Coding Standards Fixer
https://github.com/friendsofphp/php-cs-fixer
Base use
$cd /data/codes/queryphp/vendor/hunzhiwange/framework
$php-cs-fixer fix --config=.php_cs.dist

With Git hooks
Add a pre-commit for it.
cp ./build/pre-commit.sh ./.git/hooks/pre-commit
chmod 777 ./.git/hooks/pre-commit

Pass hook
# git commit -h
# git commit -n -m 'pass hook' #bypass pre-commit and commit-msg hooks

Travis CI Supported
Let code poem.
Official Documentation
Documentation for the framework can be found on the QueryPHP website.
Thanks
Thanks my colleague John.mao for your selfless help in the development of this project and and let me have a new understanding, it makes QueryPHP more beautiful.
Thanks for these excellent projects, we have absorbed a lot of excellent design and ideas, standing on the shoulders of giants for innovation.

QeePHP: https://github.com/dualface/qeephp2_x/
Swoole: https://github.com/swoole/
JeCat: https://github.com/JeCat/
ThinkPHP: https://github.com/top-think/
Laravel: https://github.com/laravel/
Symfony: https://github.com/symfony/
Doctrine: https://github.com/doctrine/
Phalcon: https://github.com/phalcon/

License
The QueryPHP framework is open-sourced software licensed under the MIT license.
",25
collin80/OBD2Scan,C,"OBD2Scan
Sketch to scan OBDII with an M2
A sketch that will automatically attempt to determine speed and connection status for the two CAN buses on the M2.
It then queries for ECUs and investigates what sort of things those ECUs support.
",4
GorillaStack/acl,JavaScript,"
ACL
ACL is a JavaScript library based on the Zend Permissions ACL library that works equally well on the server as in the browser.
Introduction
ACL is a role/resource based ACL that allows for easy definition of permissions by combining rules for specific roles, resources and privileges. Roles can inherit from earlier defined roles and resources can inherit from earlier defined resources. After the ACL is loaded with a permissions set, easy testing through the isAllowed method returns either a true or false value.
Installation
npm install --save acljs

Tests
To run the tests, after cloning this repository first install the required dependencies:
npm install

You can now run the tests by issuing the following command:
./node_modules/.bin/jasmine

Usage
To use ACL start by defining a permissions list. We can start with an empty list:
var permissions = {
  roles:     [],
  resources: [],
  rules:     []
};
Our permissions list contains three top-level requirements, roles, resources and rules. The idea behind this role based ACL is that a specific role has access to resources through specified rules. Don't confuse the elements you define in this list with 'real' objects in your application. The ACL is simply be a structure (or model) we can test against, it can be static and therefore it's not required to be stored in a database. You can define the ACL as a business object in your application or as part of your business rules. However, if you prefer, or if your ACL is dynamic, you can store the permissions set in database if you wish to do so.
OK. Let's add some permissions..
For the purpose of this demonstration we define four roles; guest, member, author and admin. For the sake of argument, we define the resources for a simple blog so we have post and comment as resources:
var permissions = {
  roles: [
    {name: ""guest""},
    {name: ""member"", parent: ""guest""},
    {name: ""author"", parent: ""member""},
    {name: ""admin""}
  ],
  resources: [
    {name: ""post""},
    {name: ""comment""}
  ],
  rules: []
};
Easy as. Now lets define a rule that allows guests to view both posts and comments:
var permissions = {
  roles: [
    {name: ""guest""},
    {name: ""member"", parent: ""guest""},
    {name: ""author"", parent: ""member""},
    {name: ""admin""}
  ],
  resources: [
    {name: ""post""},
    {name: ""comment""}
  ],
  rules: [
    {
      access:     ""allow"",
      role:       ""guest"",
      privileges: [""view""],
      resources:  [""post"", ""comment""]
    }
  ]
};
As you can see, the rule is pretty straight forward. both privileges and resources can either be set as single values or as an array. Notice how the values on the right hand side can be read in a meaningful way; ""allow guest to view post & comment"".
Now, let's create a rule that allows members to create comments:
var permissions = {
  roles: [
    {name: ""guest""},
    {name: ""member"", parent: ""guest""},
    {name: ""author"", parent: ""member""},
    {name: ""admin""}
  ],
  resources: [
    {name: ""post""},
    {name: ""comment""}
  ],
  rules: [
    {
      access:     ""allow"",
      role:       ""guest"",
      privileges: [""view""],
      resources:  [""post"", ""comment""]
    }, {
      access:     ""allow"",
      role:       ""member"",
      privileges: [""create""],
      resources:  [""comment""]
    }
  ]
};
Great. Now let's fill in the rest of the permissions:
var permissions = {
  roles: [
    {name: ""guest""},
    {name: ""member"", parent: ""guest""},
    {name: ""author"", parent: ""member""},
    {name: ""admin""}
  ],
  resources: [
    {name: ""post""},
    {name: ""comment""}
  ],
  rules: [
    {
      access:     ""allow"",
      role:       ""guest"",
      privileges: [""view""],
      resources:  [""post"", ""comment""]
    }, {
      access:     ""allow"",
      role:       ""member"",
      privileges: [""create""],
      resources:  [""comment""]
    }, {
      access:     ""allow"",
      role:       ""author"",
      privileges: [""create"", ""edit"", ""delete""],
      resources:  [""post""]
    }, {
      access:     ""allow"",
      role:       ""admin"",
      privileges: null,
      resources:  null
    }
  ]
};
We added the author permissions to allow authors to create, edit and delete posts and we've allowed the admin to perform all privileges (null) or all resources (null).
To use the permissions we need to load the permissions into the ACL, like this:
var acl = new Acl(permissions);
We can now test if a specified role can perform a requested privilege on a specified resource. E.g:
acl.isAllowed('guest', 'post', 'view');
// true

acl.isAllowed('member', 'post', 'delete');
// false

acl.isAllowed('admin', 'post', 'delete');
// true
That's easy as!
",13
hunzhiwange/framework,PHP,"











English | 中文

The QueryPHP Framework

This is the core framework code of QueryPHP application, starts from this moment with QueryPHP.

QueryPHP is a modern, high performance PHP 7 resident framework, with engineer user experience as its historical mission, let every PHP application have a good framework.
A hundred percent coverage of the unit tests to facing the bug,based on Zephir implemented framework resident,with Swoole ecology to achieve business resident,
now or in the future step by step. Our vision is USE LEEVEL WITH SWOOLE DO BETTER, let your business to support more user services.
The PHP Framework For Code Poem As Free As Wind, Starts From This Moment With QueryPHP.

Site: https://www.queryphp.com/
API: https://api.queryphp.com
Document: https://www.queryphp.com/docs/


QueryPHP was based on the DoYouHaoBaby framework which released 0.0.1 version at 2010.10.03.
The core packages

QueryPHP On Github: https://github.com/hunzhiwange/queryphp/
QueryPHP On Gitee: https://gitee.com/dyhb/queryphp/
Framework On Github: https://github.com/hunzhiwange/framework/
Framework On Gitee: https://gitee.com/dyhb/framework/
Leevel On Github: https://github.com/hunzhiwange/leevel/
Leevel On Gitee: https://gitee.com/dyhb/leevel
Tests: https://github.com/leevels/tests/
Packages: https://github.com/leevels/
Packages From Hunzhiwange: https://packagist.org/packages/hunzhiwange/
Packages From Leevel: https://packagist.org/packages/leevel/

Optional Extension








We think the performance of PHP applications is very important and the development of pleasure also needs to be considered, and that's why we have developed the QueryPHP framework to achieve great ideals.

PHP 7 - We choose the lowest version of 7.3.2, because php7 has a unique advantage over the earlier version.
Leevel - We provides an optional c extension to takeover core components of the framework,such as ioc, log, cache.
Swoole - Enable PHP developers to write high-performance, scalable, concurrent TCP, UDP, Unix socket, HTTP, Websocket services in PHP programming language.
Redis - QueryPHP encapsulation a cache component, including files, redis and so on, so as to reduce the pressure of database.
Roadrunner - RoadRunner is an open source high-performance PHP application server, load balancer and process manager. It supports running as a service with the ability to extend its functionality on a per-project basis.

Wow! Cool! Query Yet Simple!

How to install
composer require hunzhiwange/framework

The components that make up the QueryPHP framework
Components for the framework can be found on the Github website and Packagist website.
composer require leevel/di
composer require leevel/cache

... and more

Run Tests
_____________                           _______________
 ______/     \__  _____  ____  ______  / /_  _________
  ____/ __   / / / / _ \/ __`\/ / __ \/ __ \/ __ \___
   __/ / /  / /_/ /  __/ /  \  / /_/ / / / / /_/ /__
     \_\ \_/\____/\___/_/   / / .___/_/ /_/ .___/
        \_\                /_/_/         /_/

$cd /data/codes/queryphp/vendor/hunzhiwange/framework      
$composer install
$cp ./tests/config.php ./tests/config.local.php // Modify the config
$php vendor/bin/phinx migrate
$php vendor/bin/phpunit tests

Make Doc For Framework
$cd /data/codes/queryphp
$php leevel make:docwithin tests

Unified Code Style
Install PHP Coding Standards Fixer
https://github.com/friendsofphp/php-cs-fixer
Base use
$cd /data/codes/queryphp/vendor/hunzhiwange/framework
$php-cs-fixer fix --config=.php_cs.dist

With Git hooks
Add a pre-commit for it.
cp ./build/pre-commit.sh ./.git/hooks/pre-commit
chmod 777 ./.git/hooks/pre-commit

Pass hook
# git commit -h
# git commit -n -m 'pass hook' #bypass pre-commit and commit-msg hooks

Travis CI Supported
Let code poem.
Official Documentation
Documentation for the framework can be found on the QueryPHP website.
Thanks
Thanks my colleague John.mao for your selfless help in the development of this project and and let me have a new understanding, it makes QueryPHP more beautiful.
Thanks for these excellent projects, we have absorbed a lot of excellent design and ideas, standing on the shoulders of giants for innovation.

QeePHP: https://github.com/dualface/qeephp2_x/
Swoole: https://github.com/swoole/
JeCat: https://github.com/JeCat/
ThinkPHP: https://github.com/top-think/
Laravel: https://github.com/laravel/
Symfony: https://github.com/symfony/
Doctrine: https://github.com/doctrine/
Phalcon: https://github.com/phalcon/

License
The QueryPHP framework is open-sourced software licensed under the MIT license.
",25
collin80/OBD2Scan,C,"OBD2Scan
Sketch to scan OBDII with an M2
A sketch that will automatically attempt to determine speed and connection status for the two CAN buses on the M2.
It then queries for ECUs and investigates what sort of things those ECUs support.
",4
PhasecoreX/docker-red-discordbot,Shell,"Red-Discordbot V3
The newest Red-Discordbot in a convenient multi-arch container


First Time Setup
Simply run it like so:
docker run -it --rm -v /local/folder/for/persistence:/data -e TZ=America/Detroit -e PUID=1000 phasecorex/red-discordbot


-v /local/folder/for/persistence:/data: Folder to persist data.
-e TZ=America/Detroit: Specify a timezone.
-e PUID=1000: Specify the user Red-Discordbot will run as. All files it creates will be owned by this user on the host.
-e PGID=1000: Can also be specified if you want a specific group. If not specified, the PUID will be used as the group.

After this initial setup, add the bot to your server with the displayed URL. Once the bot joins, you are free to ctrl+c to kill Red-Discordbot.
Subsequent Runs
Once the initial setup is completed, you can run Red-Discordbot without -it or --rm. Just make sure you mount the same /data directory as before!
docker run --name red-discordbot --restart always -d -v /local/folder/for/persistence:/data -e TZ=America/Detroit -e PUID=1000 phasecorex/red-discordbot

You should see Red-Discordbot connect to the server that you set in the setup.
Enjoy!
Updates
If you hear that Red-Discordbot was updated, simply issue the [p]restart command. Red-Discordbot will gracefully shut down, and if you have set up your container to always restart (--restart always), it will come back up after updating to the latest version. If you do not have automatic restart enabled for the docker container, just rerun the above comand and Red-Discordbod will update itself and start.
Alternatively, consider using the UpdateNotify cog I created to get notifications when Red-Discordbot updates!
Notes
This image will run Red-Discordbot as a non-root user. This is great, until you want to install any cogs that depend on external libraries or pip packages. To get around this, the image will run Red-Discordbot in a python virtual environment. You can see this in the directory /data/venv. This allows for Red-Discordbot to install any package it wants as the non-root user. This also allows for Red-Discordbot to always be up-to-date when it first launches.
Some pip packages will require external libraries, so some of the popular ones (the ones I need for my bot) are included. If you find that Red-Discordbot cannot install a popular cog, you can either let me know for including the package in this image, or you can extend this image, running apk add --no-cache to install your dependencies:
FROM phasecorex/red-discordbot

RUN apt-get update; \
    apt-get install -y --no-install-recommends \
        your \
        packages \
        here \
    ; \
    rm -rf /var/lib/apt/lists/*;

No need to define anything else, as the VOLUME and CMD will be the defaults.
Versions
latest/audio
The default version. It contains Java so that you can use the Audio cog. You can extend this one (or any of the other versions) to add your own packages for your own 3rd party cogs.
noaudio
This version only contains the bare minimum to run Red-Discordbot (no Java, so no Audio cog support).
full
This is the version that I use. It is the same as the latest version, but with added packages. It will be occasionally updated with more dependencies that popular cogs need. If you need another dependency for your cog, let me know, and I'll consider adding it.
",6
box/boxcli,JavaScript,"Box CLI

A command line interface to the Box Content API.

Getting Started
Usage
Command Topics

Getting Started
To get started with the Box CLI, first set up a Box application using Server Authentication with JWT and
download the JSON configuration file from the Configuration page of your app in the
Box Developer Console.  Then, set up the CLI by pointing it to your configuration file:
$ box configure:environments:add PATH_TO_CONFIG_FILE
Successfully added CLI environment ""default""
If you manually generated your own private key to use with JWT authentication, you will need to point the CLI to the
location of your private key file:
$ box configure:environments:add PATH_TO_CONFIG_FILE --private-key-path PATH_TO_PRIVATE_KEY --name ManualKey
Successfully added CLI environment ""ManualKey""
Usage
$ box --version
box-cli/0.0.0 darwin-x64 node-v10.10.0
$ box users:get --help
Get information about a Box user

USAGE
  $ box users:get [ID]

ARGUMENTS
  ID  [default: me] ID of the user to get; defaults to the current user

OPTIONS
  -h, --help                             Show CLI help
  -s, --save                             Save report to default reports folder on disk
  -t, --token=token                      Provide a token to perform this call
  -v, --verbose                          Show verbose output, which can be helpful for debugging
  -y, --yes                              Automatically respond yes to all confirmation prompts
  --as-user=as-user                      Provide an ID for a user
  --bulk-file-path=bulk-file-path        File path to bulk .csv or .json objects
  --csv                                  Output formatted CSV
  --fields=fields                        Comma separated list of fields to show
  --json                                 Output formatted JSON
  --no-color                             Turn off colors for logging
  --save-to-file-path=save-to-file-path  Override default file path to save report
$ box users:get
Type: user
ID: '77777'
Name: Example User
Login: user@example.com
Created At: '2016-12-07T17:30:40-08:00'
Modified At: '2018-11-15T17:33:06-08:00'
Language: en
Timezone: America/Los_Angeles
Space Amount: 10737418240
Space Used: 53569393
Max Upload Size: 5368709120
Status: active
Job Title: ''
Phone: ''
Address: ''
Avatar URL: 'https://app.box.com/api/avatar/large/77777'
Command Topics
Command Topics

box autocomplete - Display autocomplete installation instructions
box collaboration-whitelist - List collaboration whitelist entries
box collaborations - Manage collaborations
box collections - List your collections
box comments - Manage comments on files
box configure - Configure the Box CLI
box device-pins - List all the device pins for your enterprise
box events - Get events
box files - Manage files
box folders - Manage folders
box groups - List all groups
box help - Display help for the Box CLI
box legal-hold-policies - List legal hold policies
box metadata-cascade-policies - List the metadata cascade policies on a folder
box metadata-templates - Get all metadata templates in your Enterprise
box recent-items - List information about files accessed in the past 90 days up to a 1000 items
box request - Manually specify a Box API request
box retention-policies - List all retention policies for your enterprise
box search - Search for files and folders in your Enterprise
box shared-links - Manage shared links
box storage-policies - List storage policies
box tasks - Manage tasks
box terms-of-service - List terms of services for your enterprise
box tokens - Get a token. Returns the service account token by default
box trash - List all items in trash
box users - List all Box users
box watermarking - Apply a watermark on an item
box web-links - Manage web links
box webhooks - List all webhooks

Questions, Bugs, and Feature Requests?
Browse the issues tickets! Or, if that doesn't work, file a new one and someone will get back to you.   If you have general questions about the
Box API, you can post to the Box Developer Forum.
Contributing to the Box CLI

Clone this repo.
Run npm install.
Run npm test to ensure everything is working.
Make the changes you want in the src/ directory.  Be sure to add corresponding tests
in the test/ directory!
Create a pull request with your changes — we'll review it and help you get it merged.

For more information, please see the Contribution guidelines.
Copyright and License
Copyright 2018 Box, Inc. All rights reserved.
Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
This software includes third party libraries, which are distributed under their own licenses' terms;
see LICENSE-THIRD-PARTY.txt for details.
",42
loweas/bidibidi,CSS,"Title : Bidi Bidi Refugee Settlement Camp
Rough Sketched-Out Idea


Group Members:
Paul, Owen and Ashley
The Motivation:
Bidi Bidi is a refugee camp is located in northern Uganda mostly made up of South Sudanese seeking refugee from the South Sudanese civil war which began in 2013. The peak of the war occured in 2016 and this is when the greatest amount of refugees migrated to the camp. For a brief time in 2017 it is the largest refugee settlement in the world. This project will focus on the technique of story-maping to share some of the experience within the camp. Our overall goal for the project is to synthesize changes in the Bidibidi camp (origins, development, current trends, predictions) with what everyday life in the camp actually looks like for those living there.
The Description :
First the project will give some background on the situation and how it arised, placing it into an historical context.
Next we will move into some of the geospatial data available on the settlement to model and convey some of the issues faced by the settlement.
Thirdly, we will have a section dedicated to the current social media indicators people are using to share stories with in the camp. This indicators will included Instagram data such as #bidibidi (this tag will need to be parsed with other tags to correctly identify the camp), #bidibidirefugeesettlement, and the location ID tag : 1281867775213393. From this data we can create wordclouds to express an idea of place and highlight photos with large engagement (likes and Comments).  Also, I would like to explore the API of twitter to see how to incorporate some of other social media platforms.
The Data:
QGIS Screen




Data
Data Source
Website




Uganda
The Humanitarian Data Exchange
https://data.humdata.org/group/uga


Instagram Data
Instagram
https://www.instagram.com/explore/tags/bidibidirefugeesettlement/ https://www.instagram.com/explore/tags/bidibidi/ https://www.instagram.com/explore/location/1281867775213393/


Uganda Population
unhcr
https://data2.unhcr.org/api/population/get/timeseries?widget_id=84726&geo_id=220&sv_id=5&population_group=5071&frequency=day&fromDate=1900-01-01 https://data2.unhcr.org/api/population/get/sublocation?widget_id=84724&geo_id=220&sv_id=5&population_collection=5&forcesublocation=0&fromDate=1900-01-01



Historical Context:
When introducing the project we will use historical data. Currently focusing on UNHCR likes to JSON but will update with more as data is found:
Population Growth of Entire Region 
Current Data:
For our second portion of our story we will use current data from UNHCR and link the JSON urls so the map can be updated in real time.
URL’s such as :
Population Breakdown by District
Social Media:
I already scrapped Instagram JSON  data given in assets folder:
#bidibidirefugeesettlement 
#bidibidi 
1281867775213393
Working on locating surrounding Instagram locations IDs.
Twitter API
Interface Design:
The current direction is a story-map visualization which will follow the structure layer as :
The Setup, - Historical
Confrontation and - Current Data
The Resolution - Current Social Media Involvement. (possibly predictions for the future of the camp, extrapolate on data trends from the previous section)
Examples :
#StandingRock
The Uprooted
On The Front Lines of Famine
",2
mzky/easyNmon,CSS,"因新版改动过大，并且不再使用Nmon作为采集工具，另开一个项目
新项目将支持mysql、pgsql、tomcat、redis、Nginx等监控
https://github.com/mzky/mesro

EasyNmon后期将不再提供新功能，仅优化现有功能和fixbug
EasyNmon
为了方便多场景批量性能测试，用golang写了个监控程序，可以通过get url方式启动和停止nmon服务，
适合配合Loadrunner和jmeter进行性能测试，可以做到批量执行场景并生成监控报告！
easyNmon的目标很明确：简单、轻量、绿色,在不需要安装任何语言环境和插件的情况下进行Linux系统资源监控
如在固定服务器上进行长期监控，建议使用open-falcon、Telegraf+Influxdb+grafana或NetData等优秀的监控工具
使用说明：
https://www.jianshu.com/p/c7c36ba14d3e
执行文件下载：（以下执行文件不包含源码）
https://github.com/mzky/easyNmon/releases
镜像：
https://pan.baidu.com/s/1XCeNQPMtymlI79kgNCg1ZA
为方便沟通，建了一个QQ群：
点击链接加入群聊【EasyNmon交流】：https://jq.qq.com/?_wv=1027&k=5sgrpm9
更新日志：
https://github.com/mzky/easyNmon/wiki/ChangeLog
近期规划：（将在mesro项目中实现）
1.增加系统识别，计划支持ubuntu、suse、centos
2.去掉shell脚本，全部使用go编写
3.通过模版导出word报告（可能会通过多选生成汇总报告）
长期规划：（将在mesro项目中实现）
与LR和jmeter批量测试自动化框架结合 达到自动执行性能，自动监控服务器，自动生成html报告（包括TPS、RT和服务器性能报告）
jmeter4.0/5.0的html报告汉化模版：
https://github.com/mzky/jmeter4.0-cn-report-template
https://github.com/mzky/jmeter5.0-cn-report-template
感谢：
go的web框架gin：https://github.com/gin-gonic/gin
图表插件echarts：http://echarts.baidu.com/
前端amazeui：http://amazeui.org
新版nmon下载地址：http://nmon.sourceforge.net/
#注意： 默认nmon为CentOS版本（CentOS6.5~7.4正常），Ubuntu和SUSE需要下载对应版本的nmon替换（SUSE11.4测试正常）
推荐：
go解析nmon报告生成html：https://github.com/ElectricBubble/lazy-nmon
",45
chris-torrence/vscode-idl,IDL,"VS Code IDL (Interactive Data Language) Extension
This extension adds syntax highlighting and code snippets for the Interactive Data Language (IDL) from Harris Geospatial Solutions (formerly Research Systems, Inc.).
The TextMate files and code snippets are originally from Mike Galloy's idl.tmbundle.
The JSON syntax rules for the .task files are from VSCode and are included so that you don't have to edit file associations for them to look correct.
Features


Support for VSCode's auto-comment (Ctrl+/ or command+/)


Syntax highlighting and coloring


Code snippets for common code blocks


Colorization of ENVI and IDL task files.


Notes


The actual code that gets highlighted is strongly dependent on your theme. Themes such as Abyss, Atom One Dark (extension), Dark+ (default dark), or Monokai show the most color.


The previous issue for line continuations, where arguments were being colored as procedures has been resolved. In order to have the line continuation correctly highlight your code, you will need to have the next line indented such as:


someProcedure,$
  nextLine
This should be something that you are doing in your code anyways, so it shouldn't cause problems for most people. If you do not do this, then procedures will not be colored until your next line with an indentation and no $ character. If this is an issue, let us know on the GitHub page via an issue and we will take a look at improving this functionality.
Known Issues


Properties will not highlight if being accessed directly from square brackets such as trace[-3].LINE.


When you have code blocks from line continuations a property will not be colored correctly if at the start of the line. Here is an example of the syntax that won't highlight correctly:


someProcedure,$
  nextLine.property,
Release Notes
See CHANGELOG.
Contributors
Thanks goes to these wonderful people (emoji key):



Chris Torrence💻 🎨 📦
Zachary Norman💻 🎨 📖
Michael Galloy💻 🔌



This project follows the all-contributors specification. Contributions of any kind welcome!
",2
Blockmodo/coin_registry,None,"





Introduction
Coin Registry is a collection of JSON formatted information files that is primarily used by website developers, exchange operators, terminal developers, and news publications to show accurate information about different coins.
The Payload
A COINREGISTRY JSON information file:
{
	""type"": ""COINREGISTRY"",
	""version"": 1,
	""name"": ""Ethereum"",
	""fromSymbol"": ""ETH"",
	""toSymbol"":""ALL"",
	""website"": ""https://www.ethereum.org/"",
	""images"": {
		""image64"": ""https://image_path/img.jpg"",
		""image128"": ""https://image_path/img.jpg"",
		""image256"": ""https://image_path/img.jpg"",
		""image512"": ""https://image_path/img.jpg"",
		""image1024"": ""https://image_path/img.jpg""
	},
	""network"": {
		""t_total_supply"": 17104062,
		""t_available_supply"": 17104062,
		""t_max_supply"": 21000000,
		""t_block_reward"": 12.5
	},
	""is_crypto"": true,
	""is_minable"": true,
	""proof_type"": ""PoW"",
	""algorithm"": ""Ethash"",
	""description"": {
		""en"": ""Ethereum is an open-source, public, blockchain-based distributed computing platform and operating system featuring smart contract functionality. It supports a modified version of Nakamoto consensus via transaction-based state transitions.""
	},
	""quote"": [
		""https://blockmodo.com/quotes/ETH""
	],
	""explorer"": [
		""https://etherscan.io/""
	],
	""chat"": [
		""https://telegram.me/joinchat/AyAwgj-vtnMdUxRvCgicuQ""
	],
	""social"": [
		""https://twitter.com/ethereum"",
		""https://www.facebook.com/ethereumproject/""
	],
	""community"": [
		""https://www.reddit.com/r/ethereum"",
		""https://www.reddit.com/r/ethtrader""
	],
	""source_code"": [
		""https://github.com/ethereum/go-ethereum"",
		""https://github.com/ethereum/cpp-ethereum""
	]
}
Description of Fields

root → type

The Blockmodo API always assigns type names to Payloads for easy parsing. In this paticular case, the type will always be ""COINREGISTRY"" for this payload.

root → version

The version of the payload. From time to time, Blockmodo, or its contributors, might change the schema of the JSON payload when adding or removing fields. In such a scenario, the version number will be incremented.

root → name

The name of the currency.

root → fromSymbol

Each Blockmodo payload must have a fromSymbol and toSymbol unless otherwise specififed. In this case, the fromSymbol will be the symbol of the currency being described.

root → fromSymbol

Like above, the toSymbol will describe the relation to some other currency. In this case, since we are describing a currency with no relation to another, the toSymbol will always be 'ALL'.

root → website

The website for the currency.

root → network

This block holds network related statistics.

root → images

A hash that holds images of the currency with different sizes. Hosting provided by Blockmodo.

root → network → t_total_supply

The number of coins that are currently available in some form.
NOTE: This is a termporal value and might change frequently. If you wish to update this field, please make sure to update it no more than once a day via a pull request.

root → network → t_available_supply

The number of coins that are available to trade. For example, if a currency has coins locked up in escrow the available supply will be a subset of the total supply.
NOTE: This is a termporal value and might change frequently. If you wish to update this field, please make sure to update it no more than 3 times a day via a pull request.

root → network → t_max_supply

The max number of coins that can be mined or generated.
NOTE: This is a termporal value and might change frequently. If you wish to update this field, please make sure to update it no more than 3 times a day via a pull request.

root → network → t_block_reward

The number of coins that are rewarded per mined block.
NOTE: This is a termporal value and might change frequently. If you wish to update this field, please make sure to update it no more than 3 times a day via a pull request.

root → is_crypto

If the fromSymbol being described is a cryptocurrency. Typically this value will be 'true'.

root → is_minable

Is the coin minable using a proof_type.

root → proof_type

The type of work required to verify blocks of transactions.

root → algorithm

The algorithm used to sign blocks.

root → description

Hash of description types. The key in this block will be the language code and the value will be a string.

root → quote

An array of URLs of where users can get quotes for the given fromSymbol.

root → explorer

An array of URLs of where users can get blockchain realted inforation.

root → chat

An array of URLs where users can find information about chat communities. For example, Telegram or Rocket chat links would be listed here.

root → social

An array of social media URLs. For example, Facebook pages or groups would be listed here.

root → social

An array of community URLs. For example, Sub-Reddi's would be listed here or forums.

root → source_code

An array of repo URLs. For example, GitHub repos would be listed here.
Contributions
We are always looking for help. If you have a relevant edit, please feel free to issue a pull request. Some things to keep in mind:


Please do not change the schema. If you wish to suggest a schema enhancement, please open an issue.


Temporal value changes should be done no more than three times a day. If you would like to update temporal values, please drop open an issue.


Contributors
Icon: cryptocurrency by mikicon
Looking for real-time streaming pricing data?
Blockmodo provides real-time streaming pricing data on over 1500+ coins straight from exchanges. In addition, the API also streams news, code checkins, and social posts. Feel free to check out Blockmodo API docs.
Community
Coin Registry is maintained by Blockmodo and we're on Telegram. Visit us on Telegram
",64
chris-torrence/vscode-idl,IDL,"VS Code IDL (Interactive Data Language) Extension
This extension adds syntax highlighting and code snippets for the Interactive Data Language (IDL) from Harris Geospatial Solutions (formerly Research Systems, Inc.).
The TextMate files and code snippets are originally from Mike Galloy's idl.tmbundle.
The JSON syntax rules for the .task files are from VSCode and are included so that you don't have to edit file associations for them to look correct.
Features


Support for VSCode's auto-comment (Ctrl+/ or command+/)


Syntax highlighting and coloring


Code snippets for common code blocks


Colorization of ENVI and IDL task files.


Notes


The actual code that gets highlighted is strongly dependent on your theme. Themes such as Abyss, Atom One Dark (extension), Dark+ (default dark), or Monokai show the most color.


The previous issue for line continuations, where arguments were being colored as procedures has been resolved. In order to have the line continuation correctly highlight your code, you will need to have the next line indented such as:


someProcedure,$
  nextLine
This should be something that you are doing in your code anyways, so it shouldn't cause problems for most people. If you do not do this, then procedures will not be colored until your next line with an indentation and no $ character. If this is an issue, let us know on the GitHub page via an issue and we will take a look at improving this functionality.
Known Issues


Properties will not highlight if being accessed directly from square brackets such as trace[-3].LINE.


When you have code blocks from line continuations a property will not be colored correctly if at the start of the line. Here is an example of the syntax that won't highlight correctly:


someProcedure,$
  nextLine.property,
Release Notes
See CHANGELOG.
Contributors
Thanks goes to these wonderful people (emoji key):



Chris Torrence💻 🎨 📦
Zachary Norman💻 🎨 📖
Michael Galloy💻 🔌



This project follows the all-contributors specification. Contributions of any kind welcome!
",2
Blockmodo/coin_registry,None,"





Introduction
Coin Registry is a collection of JSON formatted information files that is primarily used by website developers, exchange operators, terminal developers, and news publications to show accurate information about different coins.
The Payload
A COINREGISTRY JSON information file:
{
	""type"": ""COINREGISTRY"",
	""version"": 1,
	""name"": ""Ethereum"",
	""fromSymbol"": ""ETH"",
	""toSymbol"":""ALL"",
	""website"": ""https://www.ethereum.org/"",
	""images"": {
		""image64"": ""https://image_path/img.jpg"",
		""image128"": ""https://image_path/img.jpg"",
		""image256"": ""https://image_path/img.jpg"",
		""image512"": ""https://image_path/img.jpg"",
		""image1024"": ""https://image_path/img.jpg""
	},
	""network"": {
		""t_total_supply"": 17104062,
		""t_available_supply"": 17104062,
		""t_max_supply"": 21000000,
		""t_block_reward"": 12.5
	},
	""is_crypto"": true,
	""is_minable"": true,
	""proof_type"": ""PoW"",
	""algorithm"": ""Ethash"",
	""description"": {
		""en"": ""Ethereum is an open-source, public, blockchain-based distributed computing platform and operating system featuring smart contract functionality. It supports a modified version of Nakamoto consensus via transaction-based state transitions.""
	},
	""quote"": [
		""https://blockmodo.com/quotes/ETH""
	],
	""explorer"": [
		""https://etherscan.io/""
	],
	""chat"": [
		""https://telegram.me/joinchat/AyAwgj-vtnMdUxRvCgicuQ""
	],
	""social"": [
		""https://twitter.com/ethereum"",
		""https://www.facebook.com/ethereumproject/""
	],
	""community"": [
		""https://www.reddit.com/r/ethereum"",
		""https://www.reddit.com/r/ethtrader""
	],
	""source_code"": [
		""https://github.com/ethereum/go-ethereum"",
		""https://github.com/ethereum/cpp-ethereum""
	]
}
Description of Fields

root → type

The Blockmodo API always assigns type names to Payloads for easy parsing. In this paticular case, the type will always be ""COINREGISTRY"" for this payload.

root → version

The version of the payload. From time to time, Blockmodo, or its contributors, might change the schema of the JSON payload when adding or removing fields. In such a scenario, the version number will be incremented.

root → name

The name of the currency.

root → fromSymbol

Each Blockmodo payload must have a fromSymbol and toSymbol unless otherwise specififed. In this case, the fromSymbol will be the symbol of the currency being described.

root → fromSymbol

Like above, the toSymbol will describe the relation to some other currency. In this case, since we are describing a currency with no relation to another, the toSymbol will always be 'ALL'.

root → website

The website for the currency.

root → network

This block holds network related statistics.

root → images

A hash that holds images of the currency with different sizes. Hosting provided by Blockmodo.

root → network → t_total_supply

The number of coins that are currently available in some form.
NOTE: This is a termporal value and might change frequently. If you wish to update this field, please make sure to update it no more than once a day via a pull request.

root → network → t_available_supply

The number of coins that are available to trade. For example, if a currency has coins locked up in escrow the available supply will be a subset of the total supply.
NOTE: This is a termporal value and might change frequently. If you wish to update this field, please make sure to update it no more than 3 times a day via a pull request.

root → network → t_max_supply

The max number of coins that can be mined or generated.
NOTE: This is a termporal value and might change frequently. If you wish to update this field, please make sure to update it no more than 3 times a day via a pull request.

root → network → t_block_reward

The number of coins that are rewarded per mined block.
NOTE: This is a termporal value and might change frequently. If you wish to update this field, please make sure to update it no more than 3 times a day via a pull request.

root → is_crypto

If the fromSymbol being described is a cryptocurrency. Typically this value will be 'true'.

root → is_minable

Is the coin minable using a proof_type.

root → proof_type

The type of work required to verify blocks of transactions.

root → algorithm

The algorithm used to sign blocks.

root → description

Hash of description types. The key in this block will be the language code and the value will be a string.

root → quote

An array of URLs of where users can get quotes for the given fromSymbol.

root → explorer

An array of URLs of where users can get blockchain realted inforation.

root → chat

An array of URLs where users can find information about chat communities. For example, Telegram or Rocket chat links would be listed here.

root → social

An array of social media URLs. For example, Facebook pages or groups would be listed here.

root → social

An array of community URLs. For example, Sub-Reddi's would be listed here or forums.

root → source_code

An array of repo URLs. For example, GitHub repos would be listed here.
Contributions
We are always looking for help. If you have a relevant edit, please feel free to issue a pull request. Some things to keep in mind:


Please do not change the schema. If you wish to suggest a schema enhancement, please open an issue.


Temporal value changes should be done no more than three times a day. If you would like to update temporal values, please drop open an issue.


Contributors
Icon: cryptocurrency by mikicon
Looking for real-time streaming pricing data?
Blockmodo provides real-time streaming pricing data on over 1500+ coins straight from exchanges. In addition, the API also streams news, code checkins, and social posts. Feel free to check out Blockmodo API docs.
Community
Coin Registry is maintained by Blockmodo and we're on Telegram. Visit us on Telegram
",64
tykimos/tykimos.github.io,Jupyter Notebook,"Jekyll-Pithy
Jekyll-Pithy is a theme for Jekyll. Here is my blog which use this theme.

Usage

Clone it.
cd Jekyll-Pithy, and Run ""jekyll serve""(You must install jekyll first)
Open web browser and enter ""http://127.0.0.1:4000/"", you can see the web page like snapshot above.
If you want to host you blog on Github Page, you can follow the steps provide by Github.

License
The code follows MIT License.
",6
BandTec/sylo,CSS,"sylo
",3
apple/swift-syntax,Swift,"SwiftSyntax
SwiftSyntax is a set of Swift bindings for the
libSyntax library. It
allows for Swift tools to parse, inspect, generate, and transform Swift source
code.

Note: SwiftSyntax is still in development, and the API is not guaranteed to
be stable. It's subject to change without warning.

Usage
Declare SwiftPM dependency with release tag
Add this repository to the Package.swift manifest of your project:
// swift-tools-version:4.2
import PackageDescription

let package = Package(
  name: ""MyTool"",
  dependencies: [
    .package(url: ""https://github.com/apple/swift-syntax.git"", .exact(""<#Specify Release tag#>"")),
  ],
  targets: [
    .target(name: ""MyTool"", dependencies: [""SwiftSyntax""]),
  ]
)
Replace <#Specify Release tag#> by the version of SwiftSyntax that you want to use (see the following table for mapping details).



Swift Release Tag
SwiftSyntax Release Tag




swift-5.0-RELEASE
0.50000.0


swift-4.2-RELEASE
0.40200.0



Then, import SwiftSyntax in your Swift code.
Declare SwiftPM dependency with nightly build


Download and install the latest Trunk Development (master) toolchain.


Define the TOOLCHAINS environment variable as below to have the swift command point inside the toolchain:


$ export TOOLCHAINS=swift



To make sure everything is setup correctly, check the result of xcrun --find swift. It should point inside the OSS toolchain.


Add this entry to the Package.swift manifest of your project:


// swift-tools-version:4.2
import PackageDescription

let package = Package(
  name: ""MyTool"",
  dependencies: [
    .package(url: ""https://github.com/apple/swift-syntax.git"", .revision(""swift-DEVELOPMENT-SNAPSHOT-2019-02-26"")),
  ],
  targets: [
    .target(name: ""MyTool"", dependencies: [""SwiftSyntax""]),
  ]
)
Tags will be created for every nightly build in the form of swift-DEVELOPMENT-SNAPSHOT-<DATE>. Revision field
should be specified with the intended tag.
Different from building SwiftSyntax from source, declaring SwiftSyntax as a SwiftPM dependency doesn't require
the Swift compiler source because we always push gyb-generated files to a tag.
Some Example Users
Swift AST Explorer: a Swift AST visualizer.
Swift Stress Tester: a test driver for sourcekitd and Swift evolution.
SwiftRewriter: a Swift code formatter.
SwiftPack: a tool for automatically embedding Swift library source.
Periphery: a tool to detect unused code.
BartyCrouch: a tool to incrementally update strings files to help App localization.
Muter: Automated mutation testing for Swift
Swift Variable Injector: a tool to replace string literals with environment variables values.
Reporting Issues
If you should hit any issues while using SwiftSyntax, we appreciate bug reports on bugs.swift.org in the SwiftSyntax component.
Contributing
Building SwiftSyntax from master
Since SwiftSyntax relies on definitions in the main Swift repository to generate the layout of the syntax tree using gyb, a checkout of apple/swift is still required to build master of SwiftSyntax.
To build the master version of SwiftSyntax, follow the following instructions:

Check swift-syntax and  swift out side by side:

- (enclosing directory)
  - swift
  - swift-syntax


Make sure you have a recent master Swift toolchain installed.
Define the TOOLCHAINS environment variable as below to have the swift command point inside the toolchain:

$ export TOOLCHAINS=swift


To make sure everything is setup correctly, check the return statement of xcrun --find swift. It should point inside the latest installed master toolchain. If it points inside an Xcode toolchain, check that you exported the TOOLCHAINS environment variable correctly. If it points inside a version specific toolchain (like Swift 5.0-dev), you'll need to remove that toolchain.
Run swift-syntax/build-script.py.

If, despite following those instructions, you get compiler errors, the Swift toolchain might be too old to contain recent changes in Swift's SwiftSyntaxParser C library. In that case, you'll have to build the compiler and SwiftSyntax together with the following command:
$ swift/utils/build-script --swiftsyntax --swiftpm --llbuild

Swift-CI will automatically run the code generation step whenever a new toolchain (development snapshot or release) is published. It should thus almost never be necessary to perform the above build yourself.
Afterwards, SwiftPM can also generate an Xcode project to develop SwiftSyntax by running swift package generate-xcodeproj.
If you also want to run tests locally, read the section below as testing has additional requirements.
Local Testing
SwiftSyntax uses some test utilities that need to be built as part of the Swift compiler project. To build the most recent version of SwiftSyntax and test it, follow the steps in swift/README.md and pass --llbuild --swiftpm --swiftsyntax to the build script invocation to build SwiftSyntax and all its dependencies using the current master compiler.
SwiftSyntax can then be tested using the build script in apple/swift by running
swift/utils/build-script --swiftsyntax --swiftpm --llbuild -t --skip-test-cmark --skip-test-swift --skip-test-llbuild --skip-test-swiftpm

This command will build SwiftSyntax and all its dependencies, tell the build script to run tests, but skip all tests but the SwiftSyntax tests.
Note that it is not currently supported to SwiftSyntax while building the Swift compiler using Xcode.
CI Testing
Running @swift-ci Please test on the main Swift repository will also test the most recent version of SwiftSyntax.
Testing SwiftSyntax from its own repository is now available by commenting @swift-ci Please test macOS platform.
Example
This is a program that adds 1 to every integer literal in a Swift file.
import SwiftSyntax
import Foundation

/// AddOneToIntegerLiterals will visit each token in the Syntax tree, and
/// (if it is an integer literal token) add 1 to the integer and return the
/// new integer literal token.
class AddOneToIntegerLiterals: SyntaxRewriter {
  override func visit(_ token: TokenSyntax) -> Syntax {
    // Only transform integer literals.
    guard case .integerLiteral(let text) = token.tokenKind else {
      return token
    }

    // Remove underscores from the original text.
    let integerText = String(text.filter { (""0""...""9"").contains($0) })

    // Parse out the integer.
    let int = Int(integerText)!

    // Return a new integer literal token with `int + 1` as its text.
    return token.withKind(.integerLiteral(""\(int + 1)""))
  }
}

let file = CommandLine.arguments[1]
let url = URL(fileURLWithPath: file)
let sourceFile = try SyntaxParser.parse(url)
let incremented = AddOneToIntegerLiterals().visit(sourceFile)
print(incremented)
This example turns this:
let x = 2
let y = 3_000
into:
let x = 3
let y = 3001
",901
blur-network/blur,C++,"
Blur Network
Copyright (c) 2018-2019, Blur Network
See LICENSE.
See Code of Conduct
About the Project
The Blur Network is an experimental project based upon the premise that privacy and centralization cannot coexist.  We focus on maximizing accessibility to block rewards, while fostering an educational environment. We seek to create an ecosystem where individuals can take a first-hand role in protecting their right to privacy.
The Blur Network employs a custom algorithm for mining, called CryptoNight-Dynamic.  The algorithm adjusts approximately once every five seconds, with a goal of maintaining CPU advantages over specialized mining hardware.  Making use of a Unix timestamp, the current block height, and the previous block’s hash,  CryptoNight-Dynamic varies iterations in the CryptoNight algorithm. Employing a timestamp in the calculation serves the purpose of dynamic iterations on an intra-block basis, while height and the previous block’s hash create variation on an inter-block basis.  The iterations necessary to mine each next block are directly dependent upon the result of the block before it.
Contents:

Compiling from Source 

Dependency Packages & Dynamic Linking 
Dependencies from Source & Static Linking 
Building with packages on Host OS 


Mining on Linux/Mac 
Mining on Windows
Fixing Synchronization Issues 




Currency:
 Blur (Ticker: BLUR) 




Website:
 https://blur.cash 


Block Explorer:
http://explorer.blur.cash/


Block Explorer API Example:
http://explorer.blur.cash/api/transactions


Offline Wallet Generator:
 Offline HTML Page


Discord (Main Point of Contact for Blur):
 Blur Official Discord 


Telegram:
  Telegram  


BitcoinTalk Announcement:
 Official Thread on BitcoinTalk.org 


Reddit:
 Official Blur Network Subreddit


CLI Binary Release(s):
  Download via Github  


GUI Wallet Release(s):
 Download via Github


Algorithm:
 Cryptonight-Dynamic v2


Ports:
 P2P port = 52541 RPC port = 52542


Seed Nodes:
Node 1: 45.33.92.232:52541Node 2: 45.79.85.65:52541Node 3: 212.71.234.44:52541



Donate to help support Blur:
BTC: 13u7qiXLRvLq1sBVgSQRykWYCTeAXmMWEn 
LTC: LM2tBw25UMfdjAJbo52tzh5r3R47a1aMeM 
XMR: 46MT7yy9mF3Bvk61XmRzirB4wdSkPqNRJ3pwUcRYxj3WWCGLMHD9sE5bNtChnYQiPYiCYDPyXyJGKFG3uT2CbeKWCwnvWLv 
BLUR: bL4PdWFk3VVgEGYezGTXigHrsoJ3JGKgxKDi1gHXT7GKTLawFu3WMhu53Gc2KCmxxmCHbR4VEYMQ93PRv8vWgJ8j2mMHVEzLu 
Old Donation address:  19onVUREbP89qu4dYBfVqtGisWaoyWs3BX
Compiling from Source
Blur uses the CMake build system and a top-level Makefile that invokes cmake commands as needed.
Step 1: Clone this repository's stable branch:

git clone --recursive https://github.com/blur-network/blur.git

Step 2a: Install dependencies (If you wish to link the binaries dynamically)
If you wish to instead compile statically, scroll to step 2(b)
Ubuntu/Debian:
Required:  sudo apt-get install -y build-essential cmake pkg-config libboost-all-dev libssl-dev libsodium-dev
Optional:  sudo apt-get install -y libunwind-dev liblzma-dev libreadline-dev libldns-dev libexpat1-dev libgtest-dev
Arch Linux:
Required:  sudo pacman -S base-devel cmake boost openssl libsodium 
Optional:  sudo pacman -S libunwind xz readline ldns expat gtest
Step 2(b): Build dependencies, and link statically, all in one go
To compile for Linux (distro-agnostic): make release-cross-linux-x86_64
To compile for Windows(mingw64): make release-cross-winx64
The Makefile entries run by the above commands will build dependencies for x86_64-gnu-linux and x86_64-w64-mingw32 hosts, respectively. If you would like to compile for a different type of host platform, cd into the contrib/depends directory, and build with make HOST=xxx-yyy-zzz where xxx-yyy-zzz is a standard host triplet.
Step 3: For dynamic linking, or MacOS

cd ~/blur && make release-all

There are multiple platforms and configurations for which you can compile.  A non-exhaustive list can be found below:
For statically linked binaries (defaults to the platform configuration of the host compiler):

make release-static

For Windows portable binaries (Cross-Compiled on Linux using contrib/depends system from Bitcoin):
Follow the link above to setup build environment, then issue the command below

make release-static-win64

For MacOS portable binaries:

make release-static-mac-x86_64

Note that we do not officially support builds for 32-bit architecture, arm architecture, or the freebsd linux distribution currently. However, there are options within the Makefile for these configurations.  These entries may require significant modifications to source files, in order to work properly.
Mining on Linux
Compile from source, or download the  latest binary release from the Releases page.
We also now offer a Snap package on the Ubuntu Snap Store: 
Open a terminal in the directory within which the binaries were downloaded.  Assuming that is your Downloads folder, enter the following command:

cd ~/Downloads && tar xvzf blur-v0.1.9.5-linux-x86_64.tar.xz

Navigate into the directory you just unzipped from the archive, and start the daemon.

cd blur-v0.1.9.5-linux_x86_64 && ./blurd

Wait for sync to complete, open a new tab or terminal window, and then start the wallet:

./blur-wallet-cli

Follow the prompts to setup a new wallet.  When prompted for the password, the CLI will not show a password as you type.  It is recording your keystrokes, however.
Record the information for your wallet.
Once the wallet is open, type into the wallet CLI: start_mining [# of threads] where [# of threads] is the amount of cpu threads you wish to dedicate to mining BLUR.
You should see the message: Mining started in daemon
Switch back to the terminal or tab in which your daemon is running, and type show_hr for real-time  hashrate monitoring.  For further commands in either the wallet or the daemon, type help into either CLI.  Note that the commands for the daemon and wallet are different.
Whenever you find a block, your daemon will show a bold message with the block # found.  There is a slight delay between that message and the balance reflecting in your wallet.
You can also start mining, without opening a wallet, by launching the daemon with: ./blurd --start-mining <address> --mining-threads <num. threads> (omit the brackets)
 Mining on Windows 
Download the latest release from our Releases page.
Open your Downloads Library in your File Explorer.  Extract the executables from the compressed archive, and navigate to the folder that you just extracted.
Start the daemon by double-clicking the blurd.exe file.
You will see a pop-up from your firewall.  Be sure to check the box next to ""Private Networks"" if you are on a private network, or your daemon will not be able to sync with the network. If you daemon stalls while syncing, close and restart the program.  You will not lose any blocks you have already synced with. Once your daemon is synced with the network...
Start the wallet by double-clicking the blur-wallet-cli file.
Follow the prompts to setup a new wallet.  When prompted for the password, the CLI will not show a password as you type.  It is recording your keystrokes, however.
Once the wallet is open, type into the wallet CLI: start_mining [# of threads] where [# of threads] is the amount of cpu threads you wish to dedicate to mining BLUR. .
You should see the message: Mining started in daemon
Switch back to the terminal or tab in which your daemon is running, and type show_hr for real-time  hashrate monitoring.  For further commands in either the wallet or the daemon, type help into either CLI.  Note that the commands for the daemon and wallet are different.
Whenever you find a block, your daemon will show a bold message with the block # found.  There is a slight delay between that message and the balance reflecting in your wallet.
You can also start mining, without opening a wallet, by launching the daemon with: blurd.exe --start-mining <address> --mining-threads <num. threads> (omit the brackets)
How to Fix Synchronizing Issues
If you cannot synchronize with the network, kill your daemon & restart with the following options:
Linux: cd to the directory you downloaded the files into, and type:
./blurd --add-priority-node=45.33.92.232:52541 --add-priority-node=212.71.234.44:52541 --add-priority-node=45.79.85.65:52541 --p2p-bind-port 52541 --rpc-bind-port 52542 --rpc-bind-ip 127.0.0.1
Windows:  Open cmd.exe, cd to the directory you downloaded the files into, and type:
blurd.exe --add-priority-node=45.33.92.232:52541 --add-priority-node=212.71.234.44:52541 --add-priority-node=45.79.85.65:52541 --p2p-bind-port=52541 --rpc-bind-port=52542 --rpc-bind-ip=127.0.0.1
This should fix the synchronizing issue if the daemon does not connect to the seed nodes automatically.
You can also see additional command-line options by running the daemon with the option --help.  The program will return a list of startup flags and their descriptions.
",18
Gymmasssorla/finshir,Rust,"
finshir
















You are seeing a high-performant, coroutines-driven, and fully customisable implementation of Low & Slow load generator designed for real-world pentesting. You can easily torify/proxify it using various platform-dependent utilities.

Pulse ·
    Stargazers ·
    Releases ·
    Contributing



Contents

Features
Installation

Building from crates.io
Building from sources
Pre-compiled binaries


Options
Overview

Minimal command
Test intensity
Connections count
Custom data portions
Logging options
TLS support


Gallery

Initialisation
Errors
Being verbose


Contributing
Target platform
Legal disclaimer
Project references
Contacts


Features


Coroutines-driven. Finshir uses coroutines (also called lightweight threads) instead of ordinary threads, which lets you open many more connections with fewer system resources.


Generic. Unlike other Low & Slow utilities, Finshir lets you transmit arbitrary data sets over the TCP protocol. It may be partial HTTP headers, empty spaces, and so on.


Written in Rust. How you can see, all the logic is written completely in Rust, which means that it leverages bare-metal performance and high-level safety (no SIGSEGV, SIGILL, and other ""funny"" stuff).



Installation
Building from crates.io
$ cargo install finshir
Building from sources
$ git clone https://github.com/Gymmasssorla/finshir.git
$ cd finshir
$ cargo build --release
Pre-compiled binaries
The easiest way to run Finshir on your system is to download the pre-compiled binaries from the existing releases, which doesn't require any external software (unlike the two previous approaches).

Options
finshir 0.2.0
Temirkhan Myrzamadi <gymmasssorla@gmail.com>
A coroutines-driven Low & Slow traffic sender, written in Rust

USAGE:
    finshir [FLAGS] [OPTIONS] --receiver <SOCKET-ADDRESS>

FLAGS:
    -h, --help       Prints help information
        --use-tls    Use a TLS connection instead of the ordinary TCP protocol. It might be used to test HTTPS-based
                     services.
    -V, --version    Prints version information

OPTIONS:
        --connect-periodicity <TIME-SPAN>    This option will be applied if a socket connection error occurs (the next
                                             connection will be performed after this periodicity) [default: 10secs]
        --connect-timeout <TIME-SPAN>        Try connect a socket within a specified timeout. If a timeout is reached
                                             and a socket wasn't connected, the program will retry the operation later
                                             [default: 30secs]
    -c, --connections <POSITIVE-INTEGER>     A number of connections the program will handle simultaneously. This option
                                             also equals to a number of coroutines [default: 1000]
        --date-time-format <STRING>          A format for displaying local date and time in log messages. Type `man
                                             strftime` to see the format specification [default: %X]
        --failed-count <POSITIVE-INTEGER>    A number of failed data transmissions used to reconnect a socket to a
                                             remote web server [default: 5]
        --ip-ttl <UNSIGNED-INTEGER>          Specifies the IP_TTL value for all future sockets. Usually this value
                                             equals a number of routers that a packet can go through
    -f, --portions-file <LOCATION>           A file which consists of a custom JSON array of data portions, specified as
                                             strings.
                                             
                                             When a coroutine finished sending all portions, it reconnects its socket
                                             and starts sending them again.
    -r, --receiver <SOCKET-ADDRESS>          A receiver of generator traffic, specified as an IP address (or a domain
                                             name) and a port number, separated by a colon
    -d, --test-duration <TIME-SPAN>          A whole test duration, after which all spawned coroutines will stop their
                                             work [default: 64years 64hours 64secs]
    -v, --verbosity <LEVEL>                  Enable one of the possible verbosity levels. The zero level doesn't print
                                             anything, and the last level prints everything [default: 3]  [possible
                                             values: 0, 1, 2, 3, 4, 5]
    -w, --wait <TIME-SPAN>                   A waiting time span before test execution used to prevent a launch of an
                                             erroneous (unwanted) test [default: 5secs]
        --write-periodicity <TIME-SPAN>      A time interval between writing data portions. This option can be used to
                                             modify test intensity [default: 30secs]
        --write-timeout <TIME-SPAN>          If a timeout is reached and a data portion wasn't sent, the program will
                                             retry the operation later [default: 30secs]

By default, Finshir generates 100 empty spaces as data portions. If you want to override this behaviour, consider using
the `--portions-file` option.

For more information see <https://github.com/Gymmasssorla/finshir>.


Overview
Minimal command
The following command spawns 1000 coroutines, each trying to establish a new TCP connection. When connections are established, it sends empty spaces every 30 seconds, thereby order a server to wait as long as it can:
# Specify one of the Google's IP addresses as a target web server
$ finshir --receiver=google.com:80
Test intensity
Low & Slow techniques assume to be VERY SLOW, which means that you typically send a couple of bytes every N seconds. For instance, Finshir uses the 30 seconds interval by default, but it's modifiable as well:
# Test the Google's server sending data portions every one minute
$ finshir --receiver=google.com:80 --write-periodicity=1min
Connections count
The default number of parallel connections is 1000. However, you can modify this limit using the --connections option, but be sure that you system is able to handle such amount of file descriptors:
# Modify the default limit of file descriptors to 17015
$ sudo ulimit -n 17015

# Test the target server using 17000 parallel TCP connections
$ finshir --receiver=google.com:80 --connections=17000
Custom data portions
By default, Finshir generates 100 empty spaces as data portions to send. You can override this behaviour by specifying your custom messages as a file, consisting of a single JSON array. This example is focused on Google:
# Send partial HTTP headers to Google using `--portions-file`
$ finshir --receiver=google.com:80 --portions-file files/google.json
Logging options
Consider specifying a custom verbosity level from 0 to 5 (inclusively), which is done by the --verbosity option. There is also the --date-time-format option which tells Finshir to use your custom date-time format.
# Use a custom date-time format and the last verbosity level
$ finshir --receiver=google.com:80 --date-time-format=""%F"" --verbosity=5
TLS support
Most of web servers today use the HTTPS protocol instead of HTTP, which is based on TLS. Since v0.2.0, Finshir has functionality to connect through TLS using the --use-tls flag.
# Connect to the Google's server through TLS on 443 port (HTTPS)
$ finshir --receiver=google.com:443 --use-tls


Gallery

Initialisation

Errors

Being verbose



Contributing
You are always welcome for any contribution to this project! But before you start, you should read the appropriate document to know about the preferred development process and the basic communication rules.

Target platform
Like most of pentesting utilities, this project is developed, tested, and maintained for only UNIX-based systems. If you are a Windows user, you probably need a virtual machine or another computer with UNIX.

Legal disclaimer
Finshir was developed as a means of testing stress resistance of web servers, and not for hacking, that is, the author of the project IS NOT RESPONSIBLE for any damage caused by your use of his program.

Project references

https://www.reddit.com/r/rust/comments/bm6ttn/finshir_a_coroutinesdriven_low_slow_ddos_attack/
https://www.producthunt.com/posts/finshir
https://www.reddit.com/r/hacking/comments/bpg0by/ive_written_a_customizable_optimized_alternative/
https://news.ycombinator.com/item?id=19931443


Contacts
Temirkhan Myrzamadi <gymmasssorla@gmail.com> (the author)
",43
TecnologiaVideojuegos/proyecto-videojuego-darpa-gamers,Java,"PROYECTO VIDEOJUEGO 2018-2019
Componentes

David Carrascal Acebrón
David Gómez Zafra
Rubén adarve pérez
Ledo Galano Antón Emilio
Pablo Cancio Castaño


Código del colaborador
Código del colaborador
",2
meganetaaan/moddable-examples,JavaScript,"ModdableでつくるIoTアプリケーション（仮） サンプルコード集
必要環境（Prerequisities）
M5Stack

ESP32を搭載した開発ボードです。
https://m5stack.com/
日本国内ではスイッチサイエンスやヨドバシ.comから購入できます。

Moddable SDK

JavaScriptでマイコン向けのプログラムを開発できるSDKです。
https://www.moddable.com/
公式の手順を参考にセットアップしてください。

ビルド方法

デバイスをPCにUSB接続します
下記のコマンドを実行します。

$ cd (各サンプルのディレクトリ)
$ mcconfig -d -m -p esp32/m5stack
各サンプルの紹介
bongo
ボンゴキャットがボンゴを叩く！ひたすら叩く！

Aボタン、Cボタン：ボンゴを叩く
Bボタン：ニャーンと鳴く

Bongo Cat originally created by @StrayRogue and @DitzyFlama
Image by obBilder from Pixabay
qr-ble
BLE(Bluetooth Low Energy) を使ったサンプルです。
スマートフォンから文字列を送信し、M5StackにQRコードとして表示できます。
line-things

line/line-things-starterのModdableSDK向け移植です。

",2
meganetaaan/moddable-examples,JavaScript,"ModdableでつくるIoTアプリケーション（仮） サンプルコード集
必要環境（Prerequisities）
M5Stack

ESP32を搭載した開発ボードです。
https://m5stack.com/
日本国内ではスイッチサイエンスやヨドバシ.comから購入できます。

Moddable SDK

JavaScriptでマイコン向けのプログラムを開発できるSDKです。
https://www.moddable.com/
公式の手順を参考にセットアップしてください。

ビルド方法

デバイスをPCにUSB接続します
下記のコマンドを実行します。

$ cd (各サンプルのディレクトリ)
$ mcconfig -d -m -p esp32/m5stack
各サンプルの紹介
bongo
ボンゴキャットがボンゴを叩く！ひたすら叩く！

Aボタン、Cボタン：ボンゴを叩く
Bボタン：ニャーンと鳴く

Bongo Cat originally created by @StrayRogue and @DitzyFlama
Image by obBilder from Pixabay
qr-ble
BLE(Bluetooth Low Energy) を使ったサンプルです。
スマートフォンから文字列を送信し、M5StackにQRコードとして表示できます。
line-things

line/line-things-starterのModdableSDK向け移植です。

",2
syndesisio/syndesis-react,Java,"Syndesis


A flexible and customizable, open source platform that provides core integration capabilities as a service.
All developer related documentation can be found at the Syndesis Developer Handbook.
Quickstart

To get started quickly please install Minishift first.
Clone this repository and enter it:

git clone https://github.com/syndesisio/syndesis.git
cd syndesis


Startup minishift and install:

./tools/bin/syndesis minishift --install --open

This will install the latest bleeding edge version from Syndesis from the master branch.
For a more stable experience, use the option --tag with a stable version.
",3
KwatME/tumor_suppressor,Python,"Tumor Suppressor
Omics App to find dysfunctional tumor suppressor genes ♿️
Tumor suppressors keep tumors away
Sometimes cells go crazy (under high cellular stress for example). These crazy cells then rapidly divide and produce more crazy cells. And this is how a cancer develop.
To prevent this nightmare, tumor suppressors protect a cell from going crazy. Even if a cell has already gone crazy, tumor suppressors can still heal or kill the crazy cell. Tumor suppressors are the guardians of the cells.
Do you know your tumor suppressors?
If you have a dysfunctional tumor suppressor, your cells are protected less and, as a result, you are more likely to develop cancer and other diseases. So, checking the integrity of your tumor suppressor genes can inform you about your cells' capacity to protect themselves.
This Omics App analyzes tumor suppressor genes
This Omics App looks for major tumor suppressor genes with germline loss-of-function variant. (Theoretically) since a germline cell divide and produce all cells in a body, the result applies to all the cells.
What to do next?
If you have a harmful variant in a tumor suppressor gene, it is recommended to speak with a health care professional.
Omics App powered by Guardiome



",2
OriginProtocol/origin,JavaScript,"



Origin is empowering developers to build decentralized marketplaces on the blockchain!
Visit our Developer's page to learn more about what we're building and how to get involved.
You can see the Origin ecosystem in action at https://dapp.originprotocol.com.
Development
Ready to get started? Have a look at our developer quickstart and our contributing guidelines.
This repository
This repository is a monorepo containing many npm packages. It is managed using Lerna.
Core packages
These packages are used to build DApps on Origin.



Package
Description




@origin/contracts
Smart contracts


@origin/eventsource
Derives current state of listings and offers from contract events


@origin/graphql
GraphQL interface to Origin Protocol


@origin/ipfs
Convenience methods for getting and setting data in IPFS


@origin/messaging-client
Client for Origin messaging


@origin/services
Utility package for running Ganache and IPFS


@origin/token
Package for manipulating Origin Tokens (OGN)


@origin/validator
JSON Schema validation



DApp packages
Example DApps that we have built.



Package
Description




@origin/admin
DApp similar to @origin/marketplace but exposes more functionality


@origin/graphql-simple-demo
Example of building a DApp with @origin/graphql


@origin/marketplace
Our marketplace DApp



Infrastructure packages
Servers and packages that provide extra functionality to DApps (e.g. search or attestations).



Package
Description




@origin/bridge
Server providing attestation services


@origin/cron
Runs background tasks


@origin/dapp-creator-client
Client that generates configs for @origin/marketplace


@origin/dapp-creator-server
Server that generates configs for @origin/marketplace


@origin/discovery
Provides search features to @origin/marketplace


@origin/faucet
Token faucet


@origin/growth
Growth engine


@origin/identity
Database models for storing identity


@origin/ipfs-proxy
Layer between IPFS and clients to prevent malicious use


@origin/messaging
Messaging server


@origin/notifications
Delivers in browser notifications


@origin/tests
Runs integration tests in Docker Compose


@origin/token-transfer-client
Client for delivering tokens


@origin/token-transfer-server
Server for delivering tokens



Mobile



Package
Description




@origin/mobile
Mobile application



The @origin/mobile package is not managed by Lerna due to issues with react-native and hoisting.
Contributing
Origin is an 100% open-source and community-driven project and we welcome contributions of all sorts. There are many ways to help, from reporting issues, contributing code, and helping us improve our community.
To get involved, please review our guide to contributing.
",426
pirj/dotfiles,Perl,"Humble dotfiles



Born 2013. Written and cherry-picked with love.




What’s inside


Mostly tmux, vim, git, zsh, urxvt, ruby, js, awesome wm.
Should work in most Linux’es, OSX and BSD. Tested on ArchLinux, OSX 10.11 and OpenBSD.









Installation



git clone --recursive --depth 1 --single-branch https://github.com/pirj/dotfiles
mv dotfiles/.* .
rm -rf dotfiles
mkdir .tmp



OSX specific

Tell iTunes not to grab the keys:



sudo su -
cd /Applications/iTunes.app/Contents/MacOS
mv iTunes iTunes.bak
touch iTunes
chflags uchg iTunes
chflags schg iTunes



Prevent Chrome from emailing the page when mistyping Command+Option+I with Command+Shift+I:



defaults write com.google.Chrome NSUserKeyEquivalents -dict-add 'Email Page Location' '\0'
defaults write com.google.Chrome.canary NSUserKeyEquivalents -dict-add 'Email Page Location' '\0'



No clamshell mode (installed via brew) and no backlight trick:



curl https://raw.githubusercontent.com/pirj/nobacklight/master/nobacklight.plist > ~/Library/LaunchAgents
launchctl load ~/Library/LaunchAgents/noclamshell.plist



Add the following to .ssh/config:



UseKeychain yes






Inspiration sources


I use to browse repositories on GH, and check other people’s dotfiles. dotshare.it is another interesting source.




Packages


On ArchLinux (pacman)

Query (-Q) package list without version numbers (-q), only those explicitly installed (-t) and not required directly by other packages (-tt), native (-n) and foreign (-m):



pacman -Qqettn >! .packages.native
pacman -Qqettm >! .packages.foreign



Install:



sudo pacman -S $(< .packages.native )
aurget -S $(< .packages.foreign )




On OSX (homebrew)

Show installed formulae that are not dependencies of another installed formula:



brew leaves >! .formulae
brew cask list >! .casks



Install:



brew install $(< .formulae )
brew tap caskroom/cask
brew cask
brew cask install $(< .casks)




Keeping passwords locally

Create a certificate first:



gpg --gen-key



To keep GPG encrypted passwords locally:



echo password | gpg --encrypt --recipient $(whoami) > ~/.passwd/server.gpg



And to get the password back:



gpg --decrypt --use-agent --quiet --batch --no-tty < ~/.passwd/server.gpg






TODO




explain what works:



vim


command line, shell


ruby workflow





vim plugin bootstrap? pathogen?


how to install aurget?


how to install all those aur packages?


bsd packages?






About me


Phil Pirozhkov, software engineer


",7
google/boringssl,C,"BoringSSL
BoringSSL is a fork of OpenSSL that is designed to meet Google's needs.
Although BoringSSL is an open source project, it is not intended for general
use, as OpenSSL is. We don't recommend that third parties depend upon it. Doing
so is likely to be frustrating because there are no guarantees of API or ABI
stability.
Programs ship their own copies of BoringSSL when they use it and we update
everything as needed when deciding to make API changes. This allows us to
mostly avoid compromises in the name of compatibility. It works for us, but it
may not work for you.
BoringSSL arose because Google used OpenSSL for many years in various ways and,
over time, built up a large number of patches that were maintained while
tracking upstream OpenSSL. As Google's product portfolio became more complex,
more copies of OpenSSL sprung up and the effort involved in maintaining all
these patches in multiple places was growing steadily.
Currently BoringSSL is the SSL library in Chrome/Chromium, Android (but it's
not part of the NDK) and a number of other apps/programs.
There are other files in this directory which might be helpful:

PORTING.md: how to port OpenSSL-using code to BoringSSL.
BUILDING.md: how to build BoringSSL
INCORPORATING.md: how to incorporate BoringSSL into a project.
API-CONVENTIONS.md: general API conventions for BoringSSL consumers and developers.
STYLE.md: rules and guidelines for coding style.
include/openssl: public headers with API documentation in comments. Also available online.
FUZZING.md: information about fuzzing BoringSSL.
CONTRIBUTING.md: how to contribute to BoringSSL.
BREAKING-CHANGES.md: notes on potentially-breaking changes.

",589
dilson-pieve/Exercises,C++,"Exercises
I am using Online Judges to learn more about algorithms and C++.
I am solving while I'm learning, so they are easy to understand and can be used as a starting point for beginners. Just search by name or use the links.

http://codeforces.com/                            (CF)
https://www.hackerrank.com/                       (HE)
http://olimpiada.ic.unicamp.br/                   (OBI)
http://www.spoj.com/                              (SPOJ)
https://www.urionlinejudge.com.br/judge/en/login  (URI)
https://uva.onlinejudge.org/                      (UVa)
https://www.codechef.com/                         (CodeChef)
zRandom (Contests e outros - Disciplina Maratona de Pragramação)

",3
aroberge/friendly-traceback,Python,"friendly-traceback
Aimed at Python beginners: replacing standard traceback by something easier to understand, translatable into various languages.
See the documentation here
Friendly-traceback uses Black.

State of this project
Friendly-traceback is alpha. The current focus is on quickly increasing
the number of exceptions covered.  We strive to test the code as new additions
are made: everything included should work.  However, the code itself
could probably be refactored and could definitely be better documented.
Code of Conduct
We completely support the
Python Community Code of Conduct.
Contributors to this project, including those filing or commenting on an issue,
are expected to do the same.
Note about the documentation
See Issue #11.
",15
missdeer/blocklist,Go,"blocklist
domain block list

Download



File Name
Short URL
Comment




toblock.lst
https://git.io/dnbl
domains of short url services are blocked


toblock-without-shorturl.lst
https://git.io/dnbls
domains of short url services are not blocked


toblock-optimized.lst
https://git.io/dnblo
domains of short url services are blocked that optimized for bind9/dnsmasq/unbound/dnescrypt etc


toblock-without-shorturl-optimized.lst
https://git.io/dnblso
domains of short url services are not blocked that optimized for bind9/dnsmasq/unbound/dnescrypt etc



",24
nodejs/node,JavaScript,"




Node.js is a JavaScript runtime built on Chrome's V8 JavaScript engine. For
more information on using Node.js, see the Node.js Website.
The Node.js project uses an open governance model. The
Node.js Foundation provides support for the project.
This project is bound by a Code of Conduct.
Table of Contents

Support
Release Types

Download

Current and LTS Releases
Nightly Releases
API Documentation


Verifying Binaries


Building Node.js
Security
Current Project Team Members

TSC (Technical Steering Committee)
Collaborators
Release Keys


Contributing to Node.js

Support
Node.js contributors have limited availability to address general support
questions. Please make sure you are using a currently-supported version of
Node.js.
When looking for support, please first search for your question in these venues:

Node.js Website
Node.js Help
Open or closed issues in the Node.js GitHub organization

If you didn't find an answer in the resources above, try these unofficial
resources:

Questions tagged 'node.js' on StackOverflow
#node.js channel on chat.freenode.net
Node.js Slack Community

To register: nodeslackers.com



GitHub issues are for tracking enhancements and bugs, not general support.
The open source license grants you the freedom to use Node.js. It does not
guarantee commitments of other people's time. Please be respectful and manage
your expectations.
Release Types

Current: Under active development. Code for the Current release is in the
branch for its major version number (for example,
v10.x). Node.js releases a new
major version every 6 months, allowing for breaking changes. This happens in
April and October every year. Releases appearing each October have a support
life of 8 months. Releases appearing each April convert to LTS (see below)
each October.
LTS: Releases that receive Long-term Support, with a focus on stability
and security. Every even-numbered major version will become an LTS release.
LTS releases receive 18 months of Active LTS support and a further 12 months
of Maintenance. LTS release lines have alphabetically-ordered codenames,
beginning with v4 Argon. There are no breaking changes or feature additions,
except in some special circumstances.
Nightly: Code from the Current branch built every 24-hours when there are
changes. Use with caution.

Current and LTS releases follow Semantic Versioning. A
member of the Release Team signs each Current and LTS release.
For more information, see the
Release README.
Download
Binaries, installers, and source tarballs are available at
https://nodejs.org/en/download/.
Current and LTS Releases
https://nodejs.org/download/release/
The latest directory is an
alias for the latest Current release. The latest-codename directory is an
alias for the latest release from an LTS line. For example, the
latest-carbon directory
contains the latest Carbon (Node.js 8) release.
Nightly Releases
https://nodejs.org/download/nightly/
Each directory name and filename contains a date (in UTC time) and the commit
SHA at the HEAD of the release.
API Documentation
Documentation for the latest Current release is at https://nodejs.org/api/.
Version-specific documentation is available in each release directory in the
docs subdirectory. Version-specific documentation is also at
https://nodejs.org/download/docs/.
Verifying Binaries
Download directories contain a SHASUMS256.txt file with SHA checksums for the
files.
To download SHASUMS256.txt using curl:
$ curl -O https://nodejs.org/dist/vx.y.z/SHASUMS256.txt
To check that a downloaded file matches the checksum, run
it through sha256sum with a command such as:
$ grep node-vx.y.z.tar.gz SHASUMS256.txt | sha256sum -c -
For Current and LTS, the GPG detached signature of SHASUMS256.txt is in
SHASUMS256.txt.sig. You can use it with gpg to verify the integrity of
SHASUM256.txt. You will first need to import
the GPG keys of individuals authorized to create releases. To
import the keys:
$ gpg --keyserver pool.sks-keyservers.net --recv-keys DD8F2338BAE7501E3DD5AC78C273792F7D83545D
See the bottom of this README for a full script to import active release keys.
Next, download the SHASUMS256.txt.sig for the release:
$ curl -O https://nodejs.org/dist/vx.y.z/SHASUMS256.txt.sig
Then use gpg --verify SHASUMS256.txt.sig SHASUMS256.txt to verify
the file's signature.
Building Node.js
See BUILDING.md for instructions on how to build Node.js from
source and a list of supported platforms.
Security
For information on reporting security vulnerabilities in Node.js, see
SECURITY.md.
Current Project Team Members
For information about the governance of the Node.js project, see
GOVERNANCE.md.
TSC (Technical Steering Committee)

addaleax -
Anna Henningsen <anna@addaleax.net> (she/her)
apapirovski -
Anatoli Papirovski <apapirovski@mac.com> (he/him)
ChALkeR -
Сковорода Никита Андреевич <chalkerx@gmail.com> (he/him)
cjihrig -
Colin Ihrig <cjihrig@gmail.com> (he/him)
danbev -
Daniel Bevenius <daniel.bevenius@gmail.com> (he/him)
fhinkel -
Franziska Hinkelmann <franziska.hinkelmann@gmail.com> (she/her)
Fishrock123 -
Jeremiah Senkpiel <fishrock123@rocketmail.com>
gabrielschulhof -
Gabriel Schulhof <gabriel.schulhof@intel.com>
gireeshpunathil -
Gireesh Punathil <gpunathi@in.ibm.com> (he/him)
jasnell -
James M Snell <jasnell@gmail.com> (he/him)
joyeecheung -
Joyee Cheung <joyeec9h3@gmail.com> (she/her)
mcollina -
Matteo Collina <matteo.collina@gmail.com> (he/him)
mhdawson -
Michael Dawson <michael_dawson@ca.ibm.com> (he/him)
MylesBorins -
Myles Borins <myles.borins@gmail.com> (he/him)
sam-github -
Sam Roberts <vieuxtech@gmail.com>
targos -
Michaël Zasso <targos@protonmail.com> (he/him)
thefourtheye -
Sakthipriyan Vairamani <thechargingvolcano@gmail.com> (he/him)
Trott -
Rich Trott <rtrott@gmail.com> (he/him)

TSC Emeriti

bnoordhuis -
Ben Noordhuis <info@bnoordhuis.nl>
chrisdickinson -
Chris Dickinson <christopher.s.dickinson@gmail.com>
evanlucas -
Evan Lucas <evanlucas@me.com> (he/him)
gibfahn -
Gibson Fahnestock <gibfahn@gmail.com> (he/him)
indutny -
Fedor Indutny <fedor.indutny@gmail.com>
isaacs -
Isaac Z. Schlueter <i@izs.me>
joshgav -
Josh Gavant <josh.gavant@outlook.com>
mscdex -
Brian White <mscdex@mscdex.net>
nebrius -
Bryan Hughes <bryan@nebri.us>
ofrobots -
Ali Ijaz Sheikh <ofrobots@google.com> (he/him)
orangemocha -
Alexis Campailla <orangemocha@nodejs.org>
piscisaureus -
Bert Belder <bertbelder@gmail.com>
rvagg -
Rod Vagg <r@va.gg>
shigeki -
Shigeki Ohtsu <ohtsu@ohtsu.org> (he/him)
TimothyGu -
Tiancheng ""Timothy"" Gu <timothygu99@gmail.com> (he/him)
trevnorris -
Trevor Norris <trev.norris@gmail.com>

Collaborators

addaleax -
Anna Henningsen <anna@addaleax.net> (she/her)
ak239 -
Aleksei Koziatinskii <ak239spb@gmail.com>
AndreasMadsen -
Andreas Madsen <amwebdk@gmail.com> (he/him)
antsmartian -
Anto Aravinth <anto.aravinth.cse@gmail.com> (he/him)
apapirovski -
Anatoli Papirovski <apapirovski@mac.com> (he/him)
aqrln -
Alexey Orlenko <eaglexrlnk@gmail.com> (he/him)
bcoe -
Ben Coe <bencoe@gmail.com> (he/him)
bengl -
Bryan English <bryan@bryanenglish.com> (he/him)
benjamingr -
Benjamin Gruenbaum <benjamingr@gmail.com>
BethGriggs -
Beth Griggs <Bethany.Griggs@uk.ibm.com> (she/her)
bmeck -
Bradley Farias <bradley.meck@gmail.com>
bmeurer -
Benedikt Meurer <benedikt.meurer@gmail.com>
bnoordhuis -
Ben Noordhuis <info@bnoordhuis.nl>
boneskull -
Christopher Hiller <boneskull@boneskull.com> (he/him)
brendanashworth -
Brendan Ashworth <brendan.ashworth@me.com>
BridgeAR -
Ruben Bridgewater <ruben@bridgewater.de> (he/him)
bzoz -
Bartosz Sosnowski <bartosz@janeasystems.com>
calvinmetcalf -
Calvin Metcalf <calvin.metcalf@gmail.com>
cclauss -
Christian Clauss <cclauss@me.com> (he/him)
ChALkeR -
Сковорода Никита Андреевич <chalkerx@gmail.com> (he/him)
cjihrig -
Colin Ihrig <cjihrig@gmail.com> (he/him)
claudiorodriguez -
Claudio Rodriguez <cjrodr@yahoo.com>
codebytere -
Shelley Vohr <codebytere@gmail.com> (she/her)
danbev -
Daniel Bevenius <daniel.bevenius@gmail.com> (he/him)
DavidCai1993 -
David Cai <davidcai1993@yahoo.com> (he/him)
davisjam -
Jamie Davis <davisjam@vt.edu> (he/him)
devsnek -
Gus Caplan <me@gus.host> (he/him)
digitalinfinity -
Hitesh Kanwathirtha <digitalinfinity@gmail.com> (he/him)
edsadr -
Adrian Estrada <edsadr@gmail.com> (he/him)
eljefedelrodeodeljefe -
Robert Jefe Lindstaedt <robert.lindstaedt@gmail.com>
eugeneo -
Eugene Ostroukhov <eostroukhov@google.com>
evanlucas -
Evan Lucas <evanlucas@me.com> (he/him)
fhinkel -
Franziska Hinkelmann <franziska.hinkelmann@gmail.com> (she/her)
Fishrock123 -
Jeremiah Senkpiel <fishrock123@rocketmail.com>
gabrielschulhof -
Gabriel Schulhof <gabriel.schulhof@intel.com>
gdams -
George Adams <george.adams@uk.ibm.com> (he/him)
geek -
Wyatt Preul <wpreul@gmail.com>
gibfahn -
Gibson Fahnestock <gibfahn@gmail.com> (he/him)
gireeshpunathil -
Gireesh Punathil <gpunathi@in.ibm.com> (he/him)
guybedford -
Guy Bedford <guybedford@gmail.com> (he/him)
hashseed -
Yang Guo <yangguo@chromium.org> (he/him)
hiroppy -
Yuta Hiroto <hello@hiroppy.me> (he/him)
iarna -
Rebecca Turner <me@re-becca.org>
imyller -
Ilkka Myller <ilkka.myller@nodefield.com>
indutny -
Fedor Indutny <fedor.indutny@gmail.com>
italoacasas -
Italo A. Casas <me@italoacasas.com> (he/him)
JacksonTian -
Jackson Tian <shyvo1987@gmail.com>
jasnell -
James M Snell <jasnell@gmail.com> (he/him)
jasongin -
Jason Ginchereau <jasongin@microsoft.com>
jbergstroem -
Johan Bergström <bugs@bergstroem.nu>
jdalton -
John-David Dalton <john.david.dalton@gmail.com>
jhamhader -
Yuval Brik <yuval@brik.org.il>
jkrems -
Jan Krems <jan.krems@gmail.com> (he/him)
joaocgreis -
João Reis <reis@janeasystems.com>
joshgav -
Josh Gavant <josh.gavant@outlook.com>
joyeecheung -
Joyee Cheung <joyeec9h3@gmail.com> (she/her)
julianduque -
Julian Duque <julianduquej@gmail.com> (he/him)
JungMinu -
Minwoo Jung <minwoo@nodesource.com> (he/him)
kfarnung -
Kyle Farnung <kfarnung@microsoft.com> (he/him)
kunalspathak -
Kunal Pathak <kunal.pathak@microsoft.com>
lance -
Lance Ball <lball@redhat.com> (he/him)
Leko -
Shingo Inoue <leko.noor@gmail.com> (he/him)
lpinca -
Luigi Pinca <luigipinca@gmail.com> (he/him)
lucamaraschi -
Luca Maraschi <luca.maraschi@gmail.com> (he/him)
lundibundi -
Denys Otrishko <shishugi@gmail.com> (he/him)
maclover7 -
Jon Moss <me@jonathanmoss.me> (he/him)
mafintosh
Mathias Buus <mathiasbuus@gmail.com> (he/him)
mcollina -
Matteo Collina <matteo.collina@gmail.com> (he/him)
mhdawson -
Michael Dawson <michael_dawson@ca.ibm.com> (he/him)
misterdjules -
Julien Gilli <jgilli@nodejs.org>
mmarchini -
Matheus Marchini <mat@mmarchini.me>
MoonBall -
Chen Gang <gangc.cxy@foxmail.com>
mscdex -
Brian White <mscdex@mscdex.net>
MylesBorins -
Myles Borins <myles.borins@gmail.com> (he/him)
not-an-aardvark -
Teddy Katz <teddy.katz@gmail.com> (he/him)
ofrobots -
Ali Ijaz Sheikh <ofrobots@google.com> (he/him)
oyyd -
Ouyang Yadong <oyydoibh@gmail.com> (he/him)
princejwesley -
Prince John Wesley <princejohnwesley@gmail.com>
psmarshall -
Peter Marshall <petermarshall@chromium.org> (he/him)
Qard -
Stephen Belanger <admin@stephenbelanger.com> (he/him)
refack -
Refael Ackermann <refack@gmail.com> (he/him)
richardlau -
Richard Lau <riclau@uk.ibm.com>
ronkorving -
Ron Korving <ron@ronkorving.nl>
RReverser -
Ingvar Stepanyan <me@rreverser.com>
rubys -
Sam Ruby <rubys@intertwingly.net>
rvagg -
Rod Vagg <rod@vagg.org>
ryzokuken -
Ujjwal Sharma <usharma1998@gmail.com> (he/him)
saghul -
Saúl Ibarra Corretgé <saghul@gmail.com>
sam-github -
Sam Roberts <vieuxtech@gmail.com>
santigimeno -
Santiago Gimeno <santiago.gimeno@gmail.com>
sebdeckers -
Sebastiaan Deckers <sebdeckers83@gmail.com>
seishun -
Nikolai Vavilov <vvnicholas@gmail.com>
shigeki -
Shigeki Ohtsu <ohtsu@ohtsu.org> (he/him)
shisama -
Masashi Hirano <shisama07@gmail.com> (he/him)
silverwind -
Roman Reiss <me@silverwind.io>
srl295 -
Steven R Loomis <srloomis@us.ibm.com>
starkwang -
Weijia Wang <starkwang@126.com>
targos -
Michaël Zasso <targos@protonmail.com> (he/him)
thefourtheye -
Sakthipriyan Vairamani <thechargingvolcano@gmail.com> (he/him)
thekemkid -
Glen Keane <glenkeane.94@gmail.com> (he/him)
thlorenz -
Thorsten Lorenz <thlorenz@gmx.de>
TimothyGu -
Tiancheng ""Timothy"" Gu <timothygu99@gmail.com> (he/him)
tniessen -
Tobias Nießen <tniessen@tnie.de>
trevnorris -
Trevor Norris <trev.norris@gmail.com>
trivikr -
Trivikram Kamat <trivikr.dev@gmail.com>
Trott -
Rich Trott <rtrott@gmail.com> (he/him)
vdeturckheim -
Vladimir de Turckheim <vlad2t@hotmail.com> (he/him)
vkurchatkin -
Vladimir Kurchatkin <vladimir.kurchatkin@gmail.com>
watilde -
Daijiro Wachi <daijiro.wachi@gmail.com> (he/him)
watson -
Thomas Watson <w@tson.dk>
XadillaX -
Khaidi Chu <i@2333.moe> (he/him)
yhwang -
Yihong Wang <yh.wang@ibm.com>
yorkie -
Yorkie Liu <yorkiefixer@gmail.com>
yosuke-furukawa -
Yosuke Furukawa <yosuke.furukawa@gmail.com>
ZYSzys -
Yongsheng Zhang <zyszys98@gmail.com> (he/him)

Collaborator Emeriti

andrasq -
Andras <andras@kinvey.com>
AnnaMag -
Anna M. Kedzierska <anna.m.kedzierska@gmail.com>
estliberitas -
Alexander Makarenko <estliberitas@gmail.com>
chrisdickinson -
Chris Dickinson <christopher.s.dickinson@gmail.com>
firedfox -
Daniel Wang <wangyang0123@gmail.com>
imran-iq -
Imran Iqbal <imran@imraniqbal.org>
isaacs -
Isaac Z. Schlueter <i@izs.me>
lxe -
Aleksey Smolenchuk <lxe@lxe.co>
matthewloring -
Matthew Loring <mattloring@google.com>
micnic -
Nicu Micleușanu <micnic90@gmail.com> (he/him)
mikeal -
Mikeal Rogers <mikeal.rogers@gmail.com>
monsanto -
Christopher Monsanto <chris@monsan.to>
Olegas -
Oleg Elifantiev <oleg@elifantiev.ru>
orangemocha -
Alexis Campailla <orangemocha@nodejs.org>
othiym23 -
Forrest L Norvell <ogd@aoaioxxysz.net> (he/him)
petkaantonov -
Petka Antonov <petka_antonov@hotmail.com>
phillipj -
Phillip Johnsen <johphi@gmail.com>
piscisaureus -
Bert Belder <bertbelder@gmail.com>
pmq20 -
Minqi Pan <pmq2001@gmail.com>
rlidwka -
Alex Kocharin <alex@kocharin.ru>
rmg -
Ryan Graham <r.m.graham@gmail.com>
robertkowalski -
Robert Kowalski <rok@kowalski.gd>
romankl -
Roman Klauke <romaaan.git@gmail.com>
stefanmb -
Stefan Budeanu <stefan@budeanu.com>
tellnes -
Christian Tellnes <christian@tellnes.no>
tunniclm -
Mike Tunnicliffe <m.j.tunnicliffe@gmail.com>
vsemozhetbyt -
Vse Mozhet Byt <vsemozhetbyt@gmail.com> (he/him)
whitlockjc -
Jeremy Whitlock <jwhitlock@apache.org>

Collaborators follow the COLLABORATOR_GUIDE.md in
maintaining the Node.js project.
Release Keys
GPG keys used to sign Node.js releases:

Beth Griggs <bethany.griggs@uk.ibm.com>
4ED778F539E3634C779C87C6D7062848A1AB005C
Colin Ihrig <cjihrig@gmail.com>
94AE36675C464D64BAFA68DD7434390BDBE9B9C5
Evan Lucas <evanlucas@me.com>
B9AE9905FFD7803F25714661B63B535A4C206CA9
Gibson Fahnestock <gibfahn@gmail.com>
77984A986EBC2AA786BC0F66B01FBB92821C587A
James M Snell <jasnell@keybase.io>
71DCFD284A79C3B38668286BC97EC7A07EDE3FC1
Jeremiah Senkpiel <fishrock@keybase.io>
FD3A5288F042B6850C66B31F09FE44734EB7990E
Michaël Zasso <targos@protonmail.com>
8FCCA13FEF1D0C2E91008E09770F7A9A5AE15600
Myles Borins <myles.borins@gmail.com>
C4F0DFFF4E8C1A8236409D08E73BC641CC11F4C8
Rod Vagg <rod@vagg.org>
DD8F2338BAE7501E3DD5AC78C273792F7D83545D
Ruben Bridgewater <ruben@bridgewater.de>
A48C2BEE680E841632CD4E44F07496B3EB3C1762
Shelley Vohr <shelley.vohr@gmail.com>
B9E2F5981AA6E0CD28160D9FF13993A75599653C

To import the full set of trusted release keys:
gpg --keyserver pool.sks-keyservers.net --recv-keys 4ED778F539E3634C779C87C6D7062848A1AB005C
gpg --keyserver pool.sks-keyservers.net --recv-keys B9E2F5981AA6E0CD28160D9FF13993A75599653C
gpg --keyserver pool.sks-keyservers.net --recv-keys 94AE36675C464D64BAFA68DD7434390BDBE9B9C5
gpg --keyserver pool.sks-keyservers.net --recv-keys B9AE9905FFD7803F25714661B63B535A4C206CA9
gpg --keyserver pool.sks-keyservers.net --recv-keys 77984A986EBC2AA786BC0F66B01FBB92821C587A
gpg --keyserver pool.sks-keyservers.net --recv-keys 71DCFD284A79C3B38668286BC97EC7A07EDE3FC1
gpg --keyserver pool.sks-keyservers.net --recv-keys FD3A5288F042B6850C66B31F09FE44734EB7990E
gpg --keyserver pool.sks-keyservers.net --recv-keys 8FCCA13FEF1D0C2E91008E09770F7A9A5AE15600
gpg --keyserver pool.sks-keyservers.net --recv-keys C4F0DFFF4E8C1A8236409D08E73BC641CC11F4C8
gpg --keyserver pool.sks-keyservers.net --recv-keys DD8F2338BAE7501E3DD5AC78C273792F7D83545D
gpg --keyserver pool.sks-keyservers.net --recv-keys A48C2BEE680E841632CD4E44F07496B3EB3C1762
See the section above on Verifying Binaries for how to
use these keys to verify a downloaded file.
Other keys used to sign some previous releases:

Chris Dickinson <christopher.s.dickinson@gmail.com>
9554F04D7259F04124DE6B476D5A82AC7E37093B
Isaac Z. Schlueter <i@izs.me>
93C7E9E91B49E432C2F75674B0A78B0A6C481CF6
Italo A. Casas <me@italoacasas.com>
56730D5401028683275BD23C23EFEFE93C4CFFFE
Julien Gilli <jgilli@fastmail.fm>
114F43EE0176B71C7BC219DD50A3051F888C628D
Timothy J Fontaine <tjfontaine@gmail.com>
7937DFD2AB06298B2293C3187D33FF9D0246406D

Contributing to Node.js

Contributing to the project
Working Groups
Strategic Initiatives

",61216
ProcessMaker/spark,PHP,"ProcessMaker 4.1 Community Edition Documentation


ProcessMaker 4.1 Community Edition Documentation
Overview
ProcessMaker is an open source, workflow management software suite, which includes tools to automate your workflow, design forms, create documents, assign roles and users, create routing rules, and map an individual process quickly and easily. It's relatively lightweight and doesn't require any kind of installation on the client computer. This file describes the requirements and installation steps for the server.
Development
System Requirements
You can develop ProcessMaker as well as ProcessMaker packages locally. In order to do so, you must have the following:

Virtualbox  5.2 or above
Vagrant 2.2.0 or above
PHP 7.2.0 or above

Windows users can install XAMPP


Composer
Node.js 10.13.0 or above

Steps for Development Installation

Clone the repository into a directory
Perform composer install to install required libraries. If you are on windows, you may need to run composer install --ignore-platform-reqs due to Horizon requiring the pcntl extension. You can safely ignore this as the application runs in the virtual machine which has the appropriate extensions installed.
Perform npm install in the project directory
Perform npm run dev to build the front-end assets
Modify your local /etc/hosts file to point bpm4.local.processmaker.com to 192.168.10.10. On Windows, this file is located at C:\Windows\System32\Drivers\etc\hosts.

If you need to change the ip address to something else to avoid conflicts on your network, modify the Homestead.yaml file accordingly. Do not commit this change to the repository.


Execute vagrant up in the project directory to bring up the laravel homestead virtual machine
Execute vagrant ssh to ssh into the newly created virtual machine
Execute php artisan spark:install in /home/vagrant/processmaker to start the ProcessMaker Installation

Specify localhost as your local database server
Specify 3306 as your local database port
Specify spark as your local database name
Specify homestead as your local database username
Specify secret as your local database password
Specify https://spark.local.processmaker.com as your application url


Visit https://spark.local.processmaker.com in your browser to access the application

Login with the username of admin and password of admin



When developing, make sure to turn on debugging in your .env so you can see the actual error instead of the Whoops page.
APP_DEBUG=TRUE

Optionally, trust the self-signed certificate on your host machine so you don't get the ""Not Secure"" warnings in chrome and postman.
For macOS: 1. In your-repository-root/storage/ssl, double-click on bpm4.local.processmaker.com.crt 2. Click on ""Add"" to add it to your login keychain 3. In the Keychain Access window click on the Certificates category on the bottom left. 4. Double-click on the bpm4 certificate 5. Open the Trust section. For ""When using this certificate"", select ""always trust"" 6. Close the window. You will be asked for your password. Close and reopen the processmaker tab in chrome.
If you choose not to install the certificate, you should access the socket.io js file in your browser to allow unsafe connections from it. Otherwise, real-time notifications may not work in your development environment.

https://bpm4.local.processmaker.com:6001/socket.io/socket.io.js

Customize Logos

Add images to resources/img/
Add The following variables to the .env file

MAIN_LOGO_PATH={{EXPANDED LOGO PATH HERE}}
ICON_PATH_PATH={{ICON LOGO PATH HERE}}
LOGIN_LOGO_PATH={{LOGIN PAGE LOGO PATH HERE}}


Run npm run dev

Scheduled tasks/events
To run time based BPMN events like Timer Start Events or Intermediate Timer Events, the laravel scheduler should be enabled. To do this open a console and: 1. Execute crontab -e 2. Add to the cron tab the following line (replacing the upper cased text with the directory where your proyecto is located ):
* * * * * cd YOUR_BPM_PROJECT && php artisan schedule:run >> /dev/null 2>&1

API
The ProcessMaker API is documented using OpenAPI 3.0 documentation and can be viewed at /api/documentation. The documention is generated by adding annotations to Models and Controllers.
You should add annotations to all models and controllers that you create or modify because it's how we generate the SDKs that are used when running scripts.
When developing, make sure to add this to your .env file so that any changes you make to the annotations are automatically turned into documentation when you reload the /api/documentation page:
L5_SWAGGER_GENERATE_ALWAYS=TRUE

At the comment block at the top of the model, add an @OA annotation to describe the schema. See ProcessMaker/Models/Process.php for an example.
To keep things dry, you can define 2 schemas. One that inherits the other.
/**
 * ...existing comments above...
 * 
 * @OA\Schema(
 *   schema=""ProcessEditable"",
 *   @OA\Property(property=""process_category_uuid"", type=""string"", format=""uuid""),
 *   @OA\Property(property=""name"", type=""string""),
 *   @OA\Property(property=""description"", type=""string""),
 *   @OA\Property(property=""status"", type=""string"", enum={""ACTIVE"", ""INACTIVE""}),
 * ),
 * @OA\Schema(
 *   schema=""Process"",
 *   allOf={
 *       @OA\Schema(ref=""#/components/schemas/ProcessEditable"")
 *       @OA\Schema(
 *           type=""object"",
 *           @OA\Property(property=""user_uuid"", type=""string"", format=""uuid""),
 *           @OA\Property(property=""uuid"", type=""string"", format=""uuid""),
 *           @OA\Property(property=""created_at"", type=""string"", format=""date-time""),
 *           @OA\Property(property=""updated_at"", type=""string"", format=""date-time""),
 *       ),
 *   },
 *   
 * )
 */
class Process extends Model implements HasMedia
{
...
Now you can use the reference to the schema when annotating the controllers. See ProcessMaker/Http/Controllers/Api/ProcessController.php for an example.
    /**
     * @OA\Get(
     *     path=""/processes"",
     *     summary=""Returns all processes that the user has access to"",
     *     operationId=""getProcesses"",
     *     tags={""Process""},
     *     @OA\Parameter(ref=""#/components/parameters/filter""),
     *     @OA\Parameter(ref=""#/components/parameters/order_by""),
     *     @OA\Parameter(ref=""#/components/parameters/order_direction""),
     *     @OA\Parameter(ref=""#/components/parameters/per_page""),
     *     @OA\Parameter(ref=""#/components/parameters/""),
     * 
     *     @OA\Response(
     *         response=200,
     *         description=""list of processes"",
     *         @OA\JsonContent(
     *             type=""object"",
     *             @OA\Property(
     *                 property=""data"",
     *                 type=""array"",
     *                 @OA\Items(ref=""#/components/schemas/Process""),
     *             ),
     *             @OA\Property(
     *                 property=""meta"",
     *                 type=""object"",
     *                 allOf={@OA\Schema(ref=""#/components/schemas/metadata"")},
     *             ),
     *         ),
     *     ),
     * )
     */
    public function index(Request $request)
    {
    ...
And for a show method
    /**
     * @OA\Get(
     *     path=""/processes/{processUuid}"",
     *     summary=""Get single process by ID"",
     *     operationId=""getProcessByUuid"",
     *     tags={""Process""},
     *     @OA\Parameter(
     *         description=""ID of process to return"",
     *         in=""path"",
     *         name=""processUuid"",
     *         required=true,
     *         @OA\Schema(
     *           type=""string"",
     *         )
     *     ),
     *     @OA\Response(
     *         response=200,
     *         description=""Successfully found the process"",
     *         @OA\JsonContent(ref=""#/components/schemas/Process"")
     *     ),
     */
    public function show(Request $request, Process $process)
    {
    ...
Notes
operationId will be the method name of the generated code. It can be anything camel cased but should be named some intuitive.
Testing with Swagger UI
Reload the swagger UI at api/documentation page in your browser to see the results and debug any errors with the annotations.
By default, Swagger UI will use your bpm4 app auth. So as long as you're logged into the app you should be able to run API Commands from Swagger UI as your logged in user.
You can also create a personal access token to see the API results as a specific user would.
$user->createToken('Name it here')->accessToken;

Copy the token. In api/documentation, click on the Authenticate button on the top right and enter it in the pm_api_bearer value field.
More Info
Detailed examples can be found at https://github.com/zircote/swagger-php/tree/master/Examples/petstore.swagger.io
Full OpenAPI 3.0 specification at https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.1.md
Testing with Laravel Dusk
When testing in Laravel Dusk, make sure to turn off debugging mode in your .env so you can use the whole page and screens executing functional tests. Then, change app_env value to develop in the same file:
APP_DEBUG=FALSE
APP_ENV=develop

Execute vagrant ssh to ssh into the newly created virtual machine.
Execute php artisan dusk in /home/vagrant/processmaker to execute Laravel dusk test cases.
Execute php artisan dusk:make newTest to generate a new Dusk test. The generated test will be placed in the tests/Browser directory.
More Info
Detailed installation can be found at https://laravel.com/docs/5.7/dusk#installation
To interact with web elements https://laravel.com/docs/5.7/dusk#interacting-with-elements
List of available assertions https://laravel.com/docs/5.7/dusk#available-assertions
License
Distributed under the AGPL Version 3
ProcessMaker (C) 2002 - 2019 ProcessMaker Inc.
For further information visit: http://www.processmaker.com/
",42
judymou/spacedb,JavaScript,"spacedb
To get started:
virtualenv venv
source venv/bin/activate

First time:
# Python backend
pip install -r requirements.txt
./manage.py migrate
./manage.py loaddata orbit_class

# Webpack frontend
yarn install

Now run it:
# Terminal 1: run the web server
./manage.py runserver

# Terminal 2: build the js assets continuously
yarn build:watch

",2
docker-library/official-images,Shell,"Docker Official Images

Table of Contents

Docker Official Images

Table of Contents
What are ""Official Images""?
What do you mean by ""Official""?
Architectures other than amd64?
More FAQs?
Contributing to the standard library

Review Guidelines

Maintainership
Repeatability
Consistency
Clarity
init
Cacheability
Security

Image Build
Runtime Configuration
Security Releases


Multiple Architectures


Commitment


Library definition files

Filenames
Tags and aliases
Instruction format
Creating a new repository
Adding a new tag in an existing repository (that you're the maintainer of)
Change to a tag in an existing repository (that you're the maintainer of)


Bashbrew



What are ""Official Images""?
See Docker's documentation for a good high-level overview of the program.
What do you mean by ""Official""?
In many cases, the images in this program are not only supported but also maintained directly by the relevant upstream projects.
For some, they're developed in collaboration with the upstream project (or with the explicit blessing of the upstream project).
In all cases, we strive to create images that are true to the upstream project's vision for how their project is intended to be consumed, occasionally adding additional behavior to make them more friendly to use within Docker / containerization in general.
The name of this program was chosen in an attempt to reflect that upstream-first focus (although in hindsight, it's clear that it was a choice with some amount of confusion potential for which we're sorry).
(If you are a representative of an upstream for which there exists an image and you would like to get involved, please see the Maintainership section below!)
Architectures other than amd64?
Some images have been ported for other architectures, and many of these are officially supported (to various degrees).

Architectures officially supported by Docker, Inc. for running Docker: (see download.docker.com)

ARMv7 32-bit (arm32v7): https://hub.docker.com/u/arm32v7/
ARMv8 64-bit (arm64v8): https://hub.docker.com/u/arm64v8/
Linux x86-64 (amd64): https://hub.docker.com/u/amd64/
Windows x86-64 (windows-amd64): https://hub.docker.com/u/winamd64/


Other architectures built by official images: (but not officially supported by Docker, Inc.)

ARMv5 32-bit (arm32v5): https://hub.docker.com/u/arm32v5/
ARMv6 32-bit (arm32v6): https://hub.docker.com/u/arm32v6/
IBM POWER8 (ppc64le): https://hub.docker.com/u/ppc64le/
IBM z Systems (s390x): https://hub.docker.com/u/s390x/
x86/i686 (i386): https://hub.docker.com/u/i386/



As of 2017-09-12, these other architectures are included under the non-prefixed images via ""manifest lists"" (also known as ""indexes"" in the OCI image specification), such that, for example, docker run hello-world should run as-is on all supported platforms.
If you're curious about how these are built, head over to https://doi-janky.infosiftr.net/job/multiarch/ to see the build scaffolding.
See the multi-arch section below for recommendations in adding more architectures to an official image.
More FAQs?
Yes! We have a dedicated FAQ repository where we try to collect other common questions (both about the program and about our practices).
Contributing to the standard library
Thank you for your interest in the Docker official images project! We strive to make these instructions as simple and straightforward as possible, but if you find yourself lost, don't hesitate to seek us out on Freenode IRC in channel #docker-library or by creating a GitHub issue here.
Be sure to familiarize yourself with Official Repositories on Docker Hub and the Best practices for writing Dockerfiles in the Docker documentation. These will be the foundation of the review process performed by the official images maintainers. If you'd like the review process to go more smoothly, please ensure that your Dockerfiles adhere to all the points mentioned there, as well as below, before submitting a pull request.
Also, the Hub descriptions for these images are currently stored separately in the docker-library/docs repository, whose README.md file explains more about how it's structured and how to contribute to it. Please be prepared to submit a PR there as well, pending acceptance of your image here.
Review Guidelines
Because the official images are intended to be learning tools for those new to Docker as well as the base images for advanced users to build their production releases, we review each proposed Dockerfile to ensure that it meets a minimum standard for quality and maintainability. While some of that standard is hard to define (due to subjectivity), as much as possible is defined here, while also adhering to the ""Best Practices"" where appropriate.
A checklist which may be used by the maintainers during review can be found in NEW-IMAGE-CHECKLIST.md.
Maintainership
Version bumps and security fixes should be attended to in a timely manner.
If you do not represent upstream and upstream becomes interested in maintaining the image, steps should be taken to ensure a smooth transition of image maintainership over to upstream.
For upstreams interested in taking over maintainership of an existing repository, the first step is to get involved in the existing repository. Making comments on issues, proposing changes, and making yourself known within the ""image community"" (even if that ""community"" is just the current maintainer) are all important places to start to ensure that the transition is unsurprising to existing contributors and users.
When taking over an existing repository, please ensure that the entire Git history of the original repository is kept in the new upstream-maintained repository to make sure the review process isn't stalled during the transition. This is most easily accomplished by forking the new from the existing repository, but can also be accomplished by fetching the commits directly from the original and pushing them into the new repo (ie, git fetch https://github.com/jsmith/example.git master, git rebase FETCH_HEAD, git push -f). On GitHub, an alternative is to move ownership of the git repository. This can be accomplished without giving either group admin access to the other owner's repository:

create temporary intermediary organization

docker-library-transitioner is available for this purpose if you would like our help


give old and new owners admin access to intermediary organization
old owner transfers repo ownership to intermediary organization
new owner transfers repo ownership to its new home

recommend that old owner does not fork new repo back into the old organization to ensure that GitHub redirects will just work



Repeatability
Rebuilding the same Dockerfile should result in the same version of the image being packaged, even if the second build happens several versions later, or the build should fail outright, such that an inadvertent rebuild of a Dockerfile tagged as 0.1.0 doesn't end up containing 0.2.3. For example, if using apt to install the main program for the image, be sure to pin it to a specific version (ex: ... apt-get install -y my-package=0.1.0 ...). For dependent packages installed by apt there is not usually a need to pin them to a version.
No official images can be derived from, or depend on, non-official images with the following notable exceptions:

FROM scratch
FROM microsoft/windowsservercore
FROM microsoft/nanoserver

Consistency
All official images should provide a consistent interface. A beginning user should be able to docker run official-image bash (or sh) without needing to learn about --entrypoint. It is also nice for advanced users to take advantage of entrypoint, so that they can docker run official-image --arg1 --arg2 without having to specify the binary to execute.


If the startup process does not need arguments, just use CMD:
CMD [""irb""]


If there is initialization that needs to be done on start, like creating the initial database, use an ENTRYPOINT along with CMD:
ENTRYPOINT [""/docker-entrypoint.sh""]
CMD [""postgres""]


Ensure that docker run official-image bash (or sh) works too. The easiest way is to check for the expected command and if it is something else, just exec ""$@"" (run whatever was passed, properly keeping the arguments escaped).
#!/bin/sh
set -e

# this if will check if the first argument is a flag
# but only works if all arguments require a hyphenated flag
# -v; -SL; -f arg; etc will work, but not arg1 arg2
if [ ""$#"" -eq 0 ] || [ ""${1#-}"" != ""$1"" ]; then
    set -- mongod ""$@""
fi

# check for the expected command
if [ ""$1"" = 'mongod' ]; then
    # init db stuff....
    # use gosu (or su-exec) to drop to a non-root user
    exec gosu mongod ""$@""
fi

# else default to run whatever the user wanted like ""bash"" or ""sh""
exec ""$@""




If the image only contains the main executable and its linked libraries (ie no shell) then it is fine to use the executable as the ENTRYPOINT, since that is the only thing that can run:
ENTRYPOINT [""swarm""]
CMD [""--help""]
The most common indicator of whether this is appropriate is that the image Dockerfile starts with scratch (FROM scratch).


Clarity
Try to make the Dockerfile easy to understand/read. It may be tempting, for the sake of brevity, to put complicated initialization details into a standalone script and merely add a RUN command in the Dockerfile. However, this causes the resulting Dockerfile to be overly opaque, and such Dockerfiles are unlikely to pass review. Instead, it is recommended to put all the commands for initialization into the Dockerfile as appropriate RUN or ENV command combinations. To find good examples, look at the current official images.
Some examples at the time of writing:

php
python
ruby:2.2

init
Following the Docker guidelines it is highly recommended that the resulting image be just one concern per container; predominantly this means just one process per container, so there is no need for a full init system. There are two situations where an init-like process would be helpful for the container. The first being signal handling. If the process launched does not handle SIGTERM by exiting, it will not be killed since it is PID 1 in the container (see ""NOTE"" at the end of the Foreground section in the docker docs). The second situation would be zombie reaping. If the process spawns child processes and does not properly reap them it will lead to a full process table, which can prevent the whole system from spawning any new processes. For both of these concerns we recommend tini. It is incredibly small, has minimal external dependencies, fills each of these roles, and does only the necessary parts of reaping and signal forwarding.
Be sure to use tini in CMD or ENTRYPOINT as appropriate.
It is best to install tini from a distribution-provided package (ex. apt-get install tini). If tini is not available in your distribution or is too old, here is a snippet of a Dockerfile to add in tini:
# Install tini for signal processing and zombie killing
ENV TINI_VERSION v0.18.0
ENV TINI_SIGN_KEY 595E85A6B1B4779EA4DAAEC70B588DFF0527A9B7
RUN set -eux; \
  wget -O /usr/local/bin/tini ""https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini""; \
  wget -O /usr/local/bin/tini.asc ""https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini.asc""; \
  export GNUPGHOME=""$(mktemp -d)""; \
  gpg --batch --keyserver ha.pool.sks-keyservers.net --recv-keys ""$TINI_SIGN_KEY""; \
  gpg --batch --verify /usr/local/bin/tini.asc /usr/local/bin/tini; \
  command -v gpgconf && gpgconf --kill all || :; \
  rm -r ""$GNUPGHOME"" /usr/local/bin/tini.asc; \
  chmod +x /usr/local/bin/tini; \
  tini --version
Cacheability
This is one place that experience ends up trumping documentation for the path to enlightenment, but the following tips might help:


Avoid COPY/ADD whenever possible, but when necessary, be as specific as possible (ie, COPY one-file.sh /somewhere/ instead of COPY . /somewhere).
The reason for this is that the cache for COPY instructions considers file mtime changes to be a cache bust, which can make the cache behavior of COPY unpredictable sometimes, especially when .git is part of what needs to be COPYed (for example).


Ensure that lines which are less likely to change come before lines that are more likely to change (with the caveat that each line should generate an image that still runs successfully without assumptions of later lines).
For example, the line that contains the software version number (ENV MYSOFTWARE_VERSION 4.2) should come after a line that sets up the APT repository .list file (RUN echo 'deb http://example.com/mysoftware/debian some-suite main' > /etc/apt/sources.list.d/mysoftware.list).


Security
Image Build
The Dockerfile should be written to help mitigate man-in-the-middle attacks during build: using https where possible; importing PGP keys with the full fingerprint in the Dockerfile to check package signing; embedding checksums directly in the Dockerfile if PGP signing is not provided. When importing PGP keys, we recommend using the high-availability server pool from sks-keyservers (ha.pool.sks-keyservers.net). Here are a few good and bad examples:


Bad: download the file over http with no verification.
RUN curl -fSL ""http://julialang.s3.amazonaws.com/bin/linux/x64/${JULIA_VERSION%[.-]*}/julia-${JULIA_VERSION}-linux-x86_64.tar.gz"" | tar ... \
    # install


Good: download the file over https, but still no verification.
RUN curl -fSL ""https://julialang.s3.amazonaws.com/bin/linux/x64/${JULIA_VERSION%[.-]*}/julia-${JULIA_VERSION}-linux-x86_64.tar.gz"" | tar ... \
    # install


Better: embed the checksum into the Dockerfile. It would be better to use https here too, if it is available.
ENV RUBY_DOWNLOAD_SHA256 5ffc0f317e429e6b29d4a98ac521c3ce65481bfd22a8cf845fa02a7b113d9b44
RUN curl -fSL -o ruby.tar.gz ""http://cache.ruby-lang.org/pub/ruby/$RUBY_MAJOR/ruby-$RUBY_VERSION.tar.gz"" \
    && echo ""$RUBY_DOWNLOAD_SHA256 *ruby.tar.gz"" | sha256sum -c - \
    # install


Best: full key fingerprint imported to apt-key which will check signatures when packages are downloaded and installed.
RUN apt-key adv --keyserver ha.pool.sks-keyservers.net --recv-keys 492EAFE8CD016A07919F1D2B9ECBEC467F0CEB10
RUN echo ""deb http://repo.mongodb.org/apt/debian wheezy/mongodb-org/$MONGO_MAJOR main"" > /etc/apt/sources.list.d/mongodb-org.list
RUN apt-get update \
    && apt-get install -y mongodb-org=$MONGO_VERSION \
    && rm -rf /var/lib/apt/lists/* \
    # ...
(As a side note, rm -rf /var/lib/apt/lists/* is roughly the opposite of apt-get update -- it ensures that the layer doesn't include the extra ~8MB of APT package list data, and enforces appropriate apt-get update usage.)


Alternate Best: full key fingerprint import, download over https, verify PGP signature of download.
# gpg: key F73C700D: public key ""Larry Hastings <larry@hastings.org>"" imported
RUN curl -fSL ""https://www.python.org/ftp/python/$PYTHON_VERSION/Python-$PYTHON_VERSION.tar.xz"" -o python.tar.xz \
    && curl -fSL ""https://www.python.org/ftp/python/$PYTHON_VERSION/Python-$PYTHON_VERSION.tar.xz.asc"" -o python.tar.xz.asc \
    && export GNUPGHOME=""$(mktemp -d)"" \
    && gpg --batch --keyserver ha.pool.sks-keyservers.net --recv-keys 97FC712E4C024BBEA48A61ED3A5CA953F73C700D \
    && gpg --batch --verify python.tar.xz.asc python.tar.xz \
    && rm -r ""$GNUPGHOME"" python.tar.xz.asc \
    # install


Runtime Configuration
By default, Docker containers are executed with reduced privileges: whitelisted Linux capabilities, Control Groups, and a default Seccomp profile (1.10+ w/ host support). Software running in a container may require additional privileges in order to function correctly, and there are a number of command line options to customize container execution. See docker run Reference and Seccomp for Docker for reference.
Official Repositories that require additional privileges should specify the minimal set of command line options for the software to function, and may still be rejected if this introduces significant portability or security issues. In general, --privileged is not allowed, but a combination of --cap-add and --device options may be acceptable. Additionally, --volume can be tricky as there are many host filesystem locations that introduce portability/security issues (e.g. X11 socket).
Security Releases
For image updates which constitute a security fix, there are a few things we recommend to help ensure your update is merged, built, and released as quickly as possible:

Contact us a few days in advance to give us a heads up and a timing estimate (so we can schedule time for the incoming update appropriately).
Include [security] in the title of your pull request (for example, [security] Update FooBar to 1.2.5, 1.3.7, 2.0.1).
Keep the pull request free of changes that are unrelated to the security fix -- we'll still be doing review of the update, but it will be expedited so this will help us help you.
Be active and responsive to comments on the pull request after it's opened (as usual, but even more so if the timing of the release is of importance).

Multiple Architectures
Each repo can specify multiple architectures for any and all tags. If no architecture is specified, images are built in Linux on amd64 (aka x86-64). To specify more or different architectures, use the Architectures field (comma-delimited list, whitespace is trimmed). Valid architectures are found in oci-platform.go:

amd64
arm32v6
arm32v7
arm64v8
i386
ppc64le
s390x
windows-amd64

The Architectures of any given tag must be a strict subset of the Architectures of the tag it is FROM.
We strongly recommend that most images create a single Dockerfile per entry in the library file that can be used for multiple architectures. This means that each supported architecture will have the same FROM line (e.g. FROM debian:jessie). While official images are in the process of completing image indexes to make this work naturally, the servers that build for non-amd64 architectures will pull the correct architecture-specific base and docker tag the base image to make the FROM work correctly. See golang, docker, haproxy, and php for examples of library files using one Dockerfile per entry and see their respective git repos for example Dockerfiles.
For images that are FROM scratch like debian it will be necessary to have a different Dockerfile and build context in order to ADD architecture specific binaries. Since these images use the same Tags, they need to be in the same entry. Use the architecture specific fields for GitRepo, GitFetch, GitCommit, and Directory, which are the architecture concatenated with hyphen (-) and the field (e.g. arm32v7-GitCommit). Any architecture that does not have an architecture-specific field will use the default field (e.g. no arm32v7-Directory means Directory will be used for arm32v7). See the debian or ubuntu files in the library for examples. The following is an example for hello-world:
Maintainers: Tianon Gravi <admwiggin@gmail.com> (@tianon),
             Joseph Ferguson <yosifkit@gmail.com> (@yosifkit)
GitRepo: https://github.com/docker-library/hello-world.git
GitCommit: 7d0ee592e4ed60e2da9d59331e16ecdcadc1ed87

Tags: latest
Architectures: amd64, arm32v5, arm32v7, arm64v8, ppc64le, s390x
# all the same commit; easy for us to generate this way since they could be different
amd64-GitCommit: 7d0ee592e4ed60e2da9d59331e16ecdcadc1ed87
amd64-Directory: amd64/hello-world
arm32v5-GitCommit: 7d0ee592e4ed60e2da9d59331e16ecdcadc1ed87
arm32v5-Directory: arm32v5/hello-world
arm32v7-GitCommit: 7d0ee592e4ed60e2da9d59331e16ecdcadc1ed87
arm32v7-Directory: arm32v7/hello-world
arm64v8-GitCommit: 7d0ee592e4ed60e2da9d59331e16ecdcadc1ed87
arm64v8-Directory: arm64v8/hello-world
ppc64le-GitCommit: 7d0ee592e4ed60e2da9d59331e16ecdcadc1ed87
ppc64le-Directory: ppc64le/hello-world
s390x-GitCommit: 7d0ee592e4ed60e2da9d59331e16ecdcadc1ed87
s390x-Directory: s390x/hello-world

Tags: nanoserver
Architectures: windows-amd64
# if there is only one architecture, you can use the unprefixed fields
Directory: amd64/hello-world/nanoserver
# or use the prefixed versions
windows-amd64-GitCommit: 7d0ee592e4ed60e2da9d59331e16ecdcadc1ed87
Constraints: nanoserver

See the instruction format section for more information on the format of the library file.
Commitment
Proposing a new official image should not be undertaken lightly. We expect and require a commitment to maintain your image (including and especially timely updates as appropriate, as noted above).
Library definition files
The library definition files are plain text files found in the library/ directory of the official-images repository. Each library file controls the current ""supported"" set of image tags that appear on the Docker Hub description. Tags that are removed from a library file do not get removed from the Docker Hub, so that old versions can continue to be available for use, but are not maintained by upstream or the maintainer of the official image. Tags in the library file are only built through an update to that library file or as a result of its base image being updated (ie, an image FROM debian:jessie would be rebuilt when debian:jessie is built). Only what is in the library file will be rebuilt when a base has updates.
Given this policy, it is worth clarifying a few cases: backfilled versions, release candidates, and continuous integration builds. When a new repository is proposed, it is common to include some older unsupported versions in the initial pull request with the agreement to remove them right after acceptance. Don't confuse this with a comprehensive historical archive which is not the intention. Another common case where the term ""supported"" is stretched a bit is with release candidates. A release candidate is really just a naming convention for what are expected to be shorter-lived releases, so they are totally acceptable and encouraged. Unlike a release candidate, continuous integration builds which have a fully automated release cycle based on code commits or a regular schedule are not appropriate.
It is highly recommended that you browse some of the existing library/ file contents (and history to get a feel for how they change over time) before creating a new one to become familiar with the prevailing conventions and further help streamline the review process (so that we can focus on content instead of esoteric formatting or tag usage/naming).
Filenames
The filename of a definition file will determine the name of the image repository it creates on the Docker Hub. For example, the library/ubuntu file will create tags in the ubuntu repository.
Tags and aliases
The tags of a repository should reflect upstream's versions or variations. For example, Ubuntu 14.04 is also known as Ubuntu Trusty Tahr, but often as simply Ubuntu Trusty (especially in usage), so ubuntu:14.04 (version number) and ubuntu:trusty (version name) are appropriate aliases for the same image contents. In Docker, the latest tag is a special case, but it's a bit of a misnomer; latest really is the ""default"" tag. When one does docker run xyz, Docker interprets that to mean docker run xyz:latest. Given that background, no other tag ever contains the string latest, since it's not something users are expected or encouraged to actually type out (ie, xyz:latest should really be used as simply xyz). Put another way, having an alias for the ""highest 2.2-series release of XYZ"" should be xyz:2.2, not xyz:2.2-latest. Similarly, if there is an Alpine variant of xyz:latest, it should be aliased as xyz:alpine, not xyz:alpine-latest or xyz:latest-alpine.
It is strongly encouraged that version number tags be given aliases which make it easy for the user to stay on the ""most recent"" release of a particular series. For example, given currently supported XYZ Software versions of 2.3.7 and 2.2.4, suggested aliases would be Tags: 2.3.7, 2.3, 2, latest and Tags: 2.2.4, 2.2, respectively. In this example, the user can use xyz:2.2 to easily use the most recent patch release of the 2.2 series, or xyz:2 if less granularity is needed (Python is a good example of where that's most obviously useful -- python:2 and python:3 are very different, and can be thought of as the latest tag for each of the major release tracks of Python).
As described above, latest is really ""default"", so the image that it is an alias for should reflect which version or variation of the software users should use if they do not know or do not care which version they use. Using Ubuntu as an example, ubuntu:latest points to the most recent LTS release, given that it is what the majority of users should be using if they know they want Ubuntu but do not know or care which version (especially considering it will be the most ""stable"" and well-supported release at any given time).
Instruction format
The manifest file format is officially based on RFC 2822, and as such should be familiar to folks who are already familiar with the ""headers"" of many popular internet protocols/formats such as HTTP or email.
The primary additions are inspired by the way Debian commonly uses 2822 -- namely, lines starting with # are ignored and ""entries"" are separated by a blank line.
The first entry is the ""global"" metadata for the image. The only required field in the global entry is Maintainers, whose value is comma-separated in the format of Name <email> (@github) or Name (@github). Any field specified in the global entry will be the default for the rest of the entries and can be overridden in an individual entry.
# this is a comment and will be ignored
Maintainers: John Smith <jsmith@example.com> (@example-jsmith),
             Anne Smith <asmith@example.com> (@example-asmith)
GitRepo: https://github.com/docker-library/wordpress.git

# this is also a comment, and will also be ignored

Tags: 4.1.1, 4.1, 4, latest
GitCommit: bbef6075afa043cbfe791b8de185105065c02c01

Tags: 2.6.17, 2.6
GitRepo: https://github.com/docker-library/redis.git
GitCommit: 062335e0a8d20cab2041f25dfff2fbaf58544471
Directory: 2.6

Tags: 13.2, harlequin
GitRepo: https://github.com/openSUSE/docker-containers-build.git
GitFetch: refs/heads/openSUSE-13.1
GitCommit: 0d21bc58cd26da2a0a59588affc506b977d6a846
Directory: docker
Constraints: !aufs
Maintainers: Bob Smith (@example-bsmith)

Bashbrew will fetch code out of the Git repository (GitRepo) at the commit specified (GitCommit). If the commit referenced is not available by fetching master of the associated GitRepo, it becomes necessary to supply a value for GitFetch in order to tell Bashbrew what ref to fetch in order to get the commit necessary.
The built image will be tagged as <manifest-filename>:<tag> (ie, library/golang with a Tags value of 1.6, 1, latest will create tags of golang:1.6, golang:1, and golang:latest).
Optionally, if Directory is present, Bashbrew will look for the Dockerfile inside the specified subdirectory instead of at the root (and Directory will be used as the ""context"" for the build instead of the top-level of the repository).
See the multi-arch section for details on how to specify a different GitRepo, GitFetch, GitCommit, or Directory for a specific architecture.
Creating a new repository

Create a new file in the library/ folder. Its name will be the name of your repository on the Hub.
Add your tag definitions using the appropriate syntax (see above).
Create a pull request adding the file from your forked repository to this one. Please be sure to add details as to what your repository does.

Adding a new tag in an existing repository (that you're the maintainer of)

Add your tag definition using the instruction format documented above.
Create a pull request from your Git repository to this one. Please be sure to add details about what's new, if possible.

Change to a tag in an existing repository (that you're the maintainer of)

Update the relevant tag definition using the instruction format documented above.
Create a pull request from your Git repository to this one. Please be sure to add details about what's changed, if possible.

Bashbrew
Bashbrew (bashbrew) is a tool for cloning, building, tagging, and pushing the Docker official images. See README.md in the bashbrew/ subfolder for more information.
",3557
paragonie/ciphersweet,PHP,"CipherSweet





CipherSweet is a backend library developed by Paragon Initiative Enterprises
for implementing searchable field-level encryption.
Requires PHP 5.5+, although 7.2 is recommended for better performance.
Before adding searchable encryption support to your project, make sure you understand
the appropriate threat model
for your use case. At a minimum, you will want your application and database
server to be running on separate cloud instances / virtual machines.
(Even better: Separate bare-metal hardware.)
CipherSweet is available under the very permissive ISC License
which allows you to use CipherSweet in any of your PHP projects, commercial
or noncommercial, open source or proprietary, at no cost to you.
CipherSweet Features at a Glance

Encryption that targets the 256-bit security level
(using AEAD modes
with extended nonces to minimize users' rekeying burden).
Compliance-Specific Protocol Support. Multiple backends to satisfy a
diverse range of compliance requirements. More can be added as needed:

ModernCrypto uses libsodium, the de
facto standard encryption library for software developers.
Algorithm details.
FIPSCrypto only uses the cryptographic algorithms covered by the
FIPS 140-2 recommendations to avoid auditing complexity.
Algorithm details.


Key separation. Each column is encrypted with a different key, all of which are derived from
your master encryption key using secure key-splitting algorithms.
Key management integration. CipherSweet supports integration with Key
Management solutions for storing and retrieving the master encryption key.
Searchable Encryption. CipherSweet uses
blind indexing
with the fuzzier and Bloom filter strategies to allow fast ciphertext search
with minimal data leakage.

Each blind index on each column uses a distinct key from your encryption key
and each other blind index key.
This doesn't allow for LIKE operators or regular expression searching, but
it does allow you to index transformations (e.g. substrings) of the plaintext,
hashed under a distinct key.


Adaptability. CipherSweet has a database- and product-agnostic design, so
it should be easy to write an adapter to use CipherSweet in any PHP-based
software.
File/stream encryption. CipherSweet has an API for encrypting files (or
other PHP streams) that provides authenticated encryption that defeats TOCTOU
attacks with minimal overhead. Learn more.

Installing CipherSweet
Use Composer.
composer require paragonie/ciphersweet
Using CipherSweet
Please refer to the documentation
to learn how to use CipherSweet.
Security experts may be interested in the security properties of our design.
Integration Support
Please feel free to create an issue
if you'd like to integrate CipherSweet with your software.
CipherSweet in Other Languages

JavaScript (Node.js)

Why ""CipherSweet""?
CipherSweet was originally intended for use in SuiteCRM
(a fork of the SugarCRM Community Edition) and related products, although
there is nothing preventing its use in other products.
Therefore, we opted for a pun on ""ciphersuite"" that pays homage to the
open source heritage of the project we designed this library for.
If the wordplay is too heavy, feel free to just call it ""Sweet"", or juxtapose
the two component nouns and call it ""SweetCipher"" in spoken conversation.
Support Contracts
If your company uses this library in their products or services, you may be
interested in purchasing a support contract from Paragon Initiative Enterprises.
",226
hamzamuric/100DaysOfCode,Kotlin,"100 Days Of Code Challange

Coding every day for at least one hour.
If you want to start the challange, here is official website 100DaysOfCode
Good Luck :)
",3
vovkos/llvm-package-windows,Batchfile,"LLVM packages for Windows


Abstract
Unfortunately, pre-built packages on the official LLVM download page cover but a tiny fraction of the possible build configuration matrix on Microsoft Windows.
llvm-package-windows project builds all major versions of LLVM for the following, much more complete matrix:


Toolchain:

Visual Studio 2010 (LLVM 3.4.2 only)
Visual Studio 2013
Visual Studio 2015





Configuration:

Debug
Release





Target CPU:

IA32 (a.k.a. x86)
AMD64 (a.k.a. x86_64)





C/C++ Runtime:

libcmt (static)
msvcrt (dynamic)





The resulting LLVM binary packages are made publicly available as GitHub release artifacts. Other projects can then download LLVM package archives and unpack LLVM binaries, instead of building LLVM locally.

Releases

LLVM 3.4.2
LLVM 3.5.2
LLVM 3.6.2
LLVM 3.7.1
LLVM 3.8.1
LLVM 3.9.1
LLVM 4.0.0

",3
0x90n/toolset,None,"Toolset Overview
OSINT & Recon
i2 Analyst's Notebook

Paid


OPSEC
http://grugq.github.io/
Internal Pentesting
https://info.varonis.com/hubfs/docs/whitepapers/en/ebook_pen_testing_031317.pdf?submissionGuid=af8d5b95-8b40-4f86-baa0-944bf9cf4900
https://www.trustwave.com/Resources/SpiderLabs-Blog/Top-Five-Ways-SpiderLabs-Got-Domain-Admin-on-Your-Internal-Network/
Mobile Testing
Scanning & Reporting
Mobile Security Framework

Android (Dynamic/Static) & iOS (Static) Analysis with vulnerability reports.
Lacks on the iOS side - doesn't show exact locations of reported issues.
Tool-author udemy course HERE

Bluemix

10 free scans with vulnerability report.
App-specific mobile testing documentation HERE

idbtool

Open source

Codified Security

Fricken awesome tool - however, expensive. Better use corperate $ for it.
Note, false positives have been reported

ostorlab

Not yet tried

Reverse Engineering & Analysis

IDA Pro
ImmunityDebugger - Dynamic
Ollydbg - Dynamic
Online Disassembler
Hopper Disassembler - Static

Demo has time limit



",3
JeffersonLab/clas12-offline-software,Java,"clas12-offline-software  
CLAS12 Offline Software
Quick Start
If you just want to use the software without modifying/building it, you can download the pre-built package from the releases page (download coatjava.tar.gz from the Downloads section).
If you plan to use coatjava as a plugin of clara for running CLAS12 reconstruction, you can skip downloading it; installing clara will automatically download coatjava for you. Coatjava can then be found at $CLARA_HOME/plugins/clas12/. See the clara documentation for more information.
To build coatjava, your system must have Maven and Java JDK 1.8 or greater installed. Depending on your OS and Java installation, you may also have to install JavaFX (on some systems it will already be installed). If those requirements are met, then to build coatjava simply do:
git clone git@github.com:JeffersonLab/clas12-offline-software.git
cd clas12-offline-softwre
./build-coatjava.sh
This will create a new directory called ""coatjava"" which is your complete coatjava build. Point COATJAVA to this directory:
setenv COATJAVA /path/to/clas12-offline-software/coatjava/
See the troubleshooting wiki page if you experience any issues. Javadocs can be found at the repository's gh-page. A build history can be found at Travis CI.
Repository Structure and Dependency Management
Common Tools
The heart and soul of coatjava is the common tools, or coat-libs, the source code for which can be found in the common-tools subdirectory. coat-libs has 6 modules - clas-utils, clas-physics, clas-io, clas-geometry, clas-detector, and clas-reco - each of which is contained in a subdirectory of common-tools and has the following dependencies. The order of the modules matters and a module can depend on previous modules.

clas-utils: Apache Commons  Math (https://mvnrepository.com/artifact/org.apache.commons/commons-math3)
clas-physics: none
clas-io: org.jlab.coda.jevio, org.hep.hipo.hipo, org.jlab.coda.et, org.jlab.coda.xmsg (all from http://clasweb.jlab.org/clas12maven/), and clas-utils
clas-geometry: ccdb (http://clasweb.jlab.org/clas12maven/)
clas-detector: clas-utils, clas-io, clas-geometry, org.jlab.groot (http://clasweb.jlab.org/clas12maven/)
clas-reco: clas-io, clas-physics, clas-utils, clas-detector

(Aside: It would be good to know where the source code is for all of the above dependencies. groot and hipo are currently kept in Gagik's personal github account (gavalian), but he has discussed moving them to the JeffersonLab organization in the future. From within JLab, the clas12maven repo is at /group/clas/www/clasweb/html/clas12maven/org/jlab/coat/)
When build-coatjava.sh runs, it first goes into common-tools and uses Maven to build the coat-libs jar and then creates a new local repository (myLocalMvnRepo) and adds coat-libs to this repository for other parts of the project to use.
CLAS JCSG (Java Constructive Solid Geometry)
A modified version of https://github.com/miho/JCSG. This is the next thing built by build-coatjava.sh (using Gradle). After the build, the jcsg jar is also added to the aforementioned local repository. Andrey is the expert on jcsg.
Reconstruction
The reconstruction subdirectory contains the reconstruction code for each CLAS12 detector subsystem (in progress). Many of these codes depend on coat-libs, jcsg, and/or other reconstruction codes. build-coatjava.sh goes through each subsystem and builds the reconstruction code, adding the resulting jar files to the aforementioned local repository when necessary. Developers of the reconstruction code should also keep their bank definitions up-to-date inside the etc/bankdefs/ subdirectories. Dependencies are as follows:

CVT: coat-libs (local repository), org.jlab.coda.jclara (http://clasweb.jlab.org/clas12maven/)
DC: coat-libs, jcsg (both from local repo), org.jlab.coda.jclara (http://clasweb.jlab.org/clas12maven/)
TOF: coat-libs, jcsg, dc (all from local repo), org.jlab.coda.jclara (http://clasweb.jlab.org/clas12maven/)

Other Stuff
After a successful build, you should have a new coatjava/ directory in your working directory which contains all of the CLAS Offline Analysis Tools. In a few cases, the jar files in coatjava/ are simply hard copied from this repository (e.g. lib/clas/* and lib/utils/*). This is probably not a very good practice and will hopefully be fixed in the future.
Merging of the various reconstruction codes was finished on April 14, 2017. The commit histories were preserved; however, take note of github's method of displaying commit histories: https://help.github.com/articles/differences-between-commit-views/
some useful links:
http://scottwb.com/blog/2012/07/14/merge-git-repositories-and-preseve-commit-history/ 
https://www.smashingmagazine.com/2014/05/moving-git-repository-new-server/ 
http://roufid.com/3-ways-to-add-local-jar-to-maven-project/ 
http://stackoverflow.com/questions/4955635/how-to-add-local-jar-files-in-maven-project 
sparse checkout: http://stackoverflow.com/questions/600079/how-do-i-clone-a-subdirectory-only-of-a-git-repository/28039894#28039894
",5
Lombiq/Hastlayer-Hardware-Framework---Xilinx,VHDL,"Hastlayer Hardware Framework - Xilinx readme
This document is a guideline which provides a brief description of the Hastlayer Hardware Framework for Xilinx FPGAs. The aim of this document is to help the reader to reconstruct and test the Hastlayer FPGA firmware design and to give a hand when you run into a problem.
If you're not familiar with Hastlayer take a look at https://hastlayer.com/.
Table of contents

Prerequisite requirements
Getting started
Running hardware designs
Release notes
Version control
Upgrading the project to the latest Vivado version
Design reproduction steps
Testing custom IP cores
AXI Lite interface slave registers
Adding custom library functions to the design
Debugging with an ILA core

",2
spack/spack,Python," Spack




Spack is a multi-platform package manager that builds and installs
multiple versions and configurations of software. It works on Linux,
macOS, and many supercomputers. Spack is non-destructive: installing a
new version of a package does not break existing installations, so many
configurations of the same package can coexist.
Spack offers a simple ""spec"" syntax that allows users to specify versions
and configuration options. Package files are written in pure Python, and
specs allow package authors to write a single script for many different
builds of the same package.  With Spack, you can build your software
all the ways you want to.
See the
Feature Overview
for examples and highlights.
To install spack and your first package, make sure you have Python.
Then:
$ git clone https://github.com/spack/spack.git
$ cd spack/bin
$ ./spack install libelf

Documentation
Full documentation for Spack is
the first place to look.
Try the
Spack Tutorial,
to learn how to use spack, write packages, or deploy packages for users
at your site.
See also:

Technical paper and
slides on Spack's design and implementation.
Short presentation from the Getting Scientific Software Installed BOF session at Supercomputing 2015.

Get Involved!
Spack is an open source project.  Questions, discussion, and
contributions are welcome. Contributions can be anything from new
packages to bugfixes, or even new core features.
Mailing list
If you are interested in contributing to spack, join the mailing list.
We're using Google Groups for this:

Spack Google Group

Slack channel
Spack has a Slack channel where you can chat about all things Spack:

Spack on Slack

Sign up here to get an invitation mailed
to you.
Twitter
You can follow @spackpm on Twitter for
updates. Also, feel free to @mention us in in questions or comments
about your own experience with Spack.
Contributions
Contributing to Spack is relatively easy.  Just send us a
pull request.
When you send your request, make develop the destination branch on the
Spack repository.
Your PR must pass Spack's unit tests and documentation tests, and must be
PEP 8 compliant.  We enforce
these guidelines with Travis CI.  To
run these tests locally, and for helpful tips on git, see our
Contribution Guide.
Spack uses a rough approximation of the
Git Flow
branching model.  The develop branch contains the latest
contributions, and master is always tagged and points to the latest
stable release.
Authors
Many thanks go to Spack's contributors.
Spack was created by Todd Gamblin, tgamblin@llnl.gov.
Citing Spack
If you are referencing Spack in a publication, please cite the following paper:

Todd Gamblin, Matthew P. LeGendre, Michael R. Collette, Gregory L. Lee,
Adam Moody, Bronis R. de Supinski, and W. Scott Futral.
The Spack Package Manager: Bringing Order to HPC Software Chaos.
In Supercomputing 2015 (SC’15), Austin, Texas, November 15-20 2015. LLNL-CONF-669890.

License
Spack is distributed under the terms of both the MIT license and the
Apache License (Version 2.0). Users may choose either license, at their
option.
All new contributions must be made under both the MIT and Apache-2.0
licenses.
See LICENSE-MIT,
LICENSE-APACHE,
COPYRIGHT, and
NOTICE for details.
SPDX-License-Identifier: (Apache-2.0 OR MIT)
LLNL-CODE-647188
",966
spack/spack,Python," Spack




Spack is a multi-platform package manager that builds and installs
multiple versions and configurations of software. It works on Linux,
macOS, and many supercomputers. Spack is non-destructive: installing a
new version of a package does not break existing installations, so many
configurations of the same package can coexist.
Spack offers a simple ""spec"" syntax that allows users to specify versions
and configuration options. Package files are written in pure Python, and
specs allow package authors to write a single script for many different
builds of the same package.  With Spack, you can build your software
all the ways you want to.
See the
Feature Overview
for examples and highlights.
To install spack and your first package, make sure you have Python.
Then:
$ git clone https://github.com/spack/spack.git
$ cd spack/bin
$ ./spack install libelf

Documentation
Full documentation for Spack is
the first place to look.
Try the
Spack Tutorial,
to learn how to use spack, write packages, or deploy packages for users
at your site.
See also:

Technical paper and
slides on Spack's design and implementation.
Short presentation from the Getting Scientific Software Installed BOF session at Supercomputing 2015.

Get Involved!
Spack is an open source project.  Questions, discussion, and
contributions are welcome. Contributions can be anything from new
packages to bugfixes, or even new core features.
Mailing list
If you are interested in contributing to spack, join the mailing list.
We're using Google Groups for this:

Spack Google Group

Slack channel
Spack has a Slack channel where you can chat about all things Spack:

Spack on Slack

Sign up here to get an invitation mailed
to you.
Twitter
You can follow @spackpm on Twitter for
updates. Also, feel free to @mention us in in questions or comments
about your own experience with Spack.
Contributions
Contributing to Spack is relatively easy.  Just send us a
pull request.
When you send your request, make develop the destination branch on the
Spack repository.
Your PR must pass Spack's unit tests and documentation tests, and must be
PEP 8 compliant.  We enforce
these guidelines with Travis CI.  To
run these tests locally, and for helpful tips on git, see our
Contribution Guide.
Spack uses a rough approximation of the
Git Flow
branching model.  The develop branch contains the latest
contributions, and master is always tagged and points to the latest
stable release.
Authors
Many thanks go to Spack's contributors.
Spack was created by Todd Gamblin, tgamblin@llnl.gov.
Citing Spack
If you are referencing Spack in a publication, please cite the following paper:

Todd Gamblin, Matthew P. LeGendre, Michael R. Collette, Gregory L. Lee,
Adam Moody, Bronis R. de Supinski, and W. Scott Futral.
The Spack Package Manager: Bringing Order to HPC Software Chaos.
In Supercomputing 2015 (SC’15), Austin, Texas, November 15-20 2015. LLNL-CONF-669890.

License
Spack is distributed under the terms of both the MIT license and the
Apache License (Version 2.0). Users may choose either license, at their
option.
All new contributions must be made under both the MIT and Apache-2.0
licenses.
See LICENSE-MIT,
LICENSE-APACHE,
COPYRIGHT, and
NOTICE for details.
SPDX-License-Identifier: (Apache-2.0 OR MIT)
LLNL-CODE-647188
",966
LeifYaoYuXiang/SoftwareEngineeringProject,Java,"Software-Engineering-Project:Make It One

Yao Yuxiang 17205995(GROUP LEADER)
Li Jiadi 17205985
Li Zigen 17205998
Su Zhan 17205994
Zhang Yu 17205936

Project Background
What is it about?
It is a integrated efficiency tool which can help users to concentrate on things which should be focused. Its basic function is a clock which can be used to record the time interval which has been used to focus.
Why we design it?
For the following  two reasons:

Society issue is concerned：
It has to be admitted that in the contemporary society, due to the use of Internet technology, more and more people obtain relevant information through mobile phones. But at the same time, we cannot ignore that, due to the widespread popularity of smart phones, more and more people have become dependent on mobile phones. We can easily see that many people around us are addicted to some applications of mobile devices. Some of them even cannot help chatting, playing games or watching videos on their mobile phones while studying and working. Based on our sense of social responsibility, we feel it is very necessary to develop a product to regulate their behaviors, so that it can reasonably arrange their time to use mobile devices.
Potential market of products:
There are some related apps available in today's market. But it is not difficult to find that the existed apps have a single function. Their target group are mainly students and office workers, which goals are mainly to help users improve their work efficiency. However, these goals are not satisfied people'need yet since our lives need not only these focused goals. From the other hand, the current phenomenon of mobile phone addiction can not only be found in students and office workers. Our app divides functionality into three parts: work, exercise, and meetings. The reason why we do this is that many people do not leave their mobile phones when they exercise regularly, and many people also start to play their mobile phones in meetings because they can't restrain themselves. Of course, that's why we call it make it one: our product is an integrated efficiency tool.

Technologies
Program Language: Java Language & XML Language(Android Studio)
Since we want to develop an Android App , we chose Android Studio as our development IDE. As we all know, there are currently two IDEs available for Android development. One is Android Studio, which we did not initially contact, and the other is Eclipse, which we have studied for nearly a semester. After consideration, we all agreed that Android Studio is more suitable for us, mainly because we hope that our choice will enable us to access more knowledge and skills.
Tool-kits: BaiduMap API
BaiduMap API  is used to ensure the validity of map navigation in the layout of ""Focus on Sports"", since it is easy to get and the price to pay is cheap.
Databases: SQLite
In order to enable users to focus on the situations before querying, we use the SQLite Database, because it is seen as a remote and easy database to be accessed and modified.
Platform: GitHub
In the field of teamwork, because we are a five-person team, we
choose GitHub as the platform for teamwork which will do help to the foundation for the success of our teamwork as it is a mature platform for group development.
",4
dudu-miranda/simulacao,C++,"simulacao
Trabalho do diegao
",2
Xwoder/Blog,None,"Blog
",2
SSWConsulting/ng-firebootcamp-bne-201905,TypeScript,"FirebootcampCrm
This project was generated with Angular CLI version 7.3.9.
Development server
Run ng serve for a dev server. Navigate to http://localhost:4200/. The app will automatically reload if you change any of the source files.
Code scaffolding
Run ng generate component component-name to generate a new component. You can also use ng generate directive|pipe|service|class|guard|interface|enum|module.
Build
Run ng build to build the project. The build artifacts will be stored in the dist/ directory. Use the --prod flag for a production build.
Running unit tests
Run ng test to execute the unit tests via Karma.
Running end-to-end tests
Run ng e2e to execute the end-to-end tests via Protractor.
Further help
To get more help on the Angular CLI use ng help or go check out the Angular CLI README.
",2
ProjectTIER/projecttier.org,Python,"





Project TIER (Teaching Integrity in Empirical Research) promotes the integration of principles and practices related to transparency and replicability in the research training of social scientists.
This repository is Project TIER's website (https://www.projecttier.org/), which is developed in Python using the Django-based Wagtail framework.
Local development

Install Vagrant and VirtualBox, if you haven't already.
Clone the project: git clone https://github.com/ProjectTIER/projecttier.org.git
Enter the project directory: cd projecttier.org
Start the Vagrant VM: vagrant up
Shell into the VM: vagrant ssh
Run the development server: djrun
Visit 0.0.0.0:8000 in your browser.

Pull production data/media
Your local version of the project will have an empty database and no media uploads.
You can copy the production database and files to your local version.

To pull the database: fab pull_production_data
To pull file uploads: fab pull_production_media

See fabfile.py for more information.
You'll need authentication to run these commands.
License and credits
Copyright © 2018 Richard Ball.
Licensed under the GNU AGPL 3.0. See the LICENSE file for the full license.
This project was originally developed by PromptWorks.
In April 2016 the project was inherited by Torchbox who continued its development.
In January 2018 the project was transferred to Candlewaster who continues to develop the project.
",6
JuliaCI/julia-buildbot,Python,"julia-buildbot
Buildbot configuration for build.julialang.org
",11
ryceg/Eigengrau-s-Essential-Establishment-Generator,JavaScript,"Eigengrau's Essential Establishment Generator
A Tabletop Generator Unlike Any Other 🎲
Eigengrau's Essential Establishment Generator, or EEEG for short, is a tabletop procedural generation tool for towns, npcs, and more. It creates paragraphs of text suitable to be read directly to the players during a game. No longer do you have to simply describe a generic, unnamed tavern as ""small""- this is the sort of stuff that you can read out instead:

What It's For

Generate thousands of unique towns for table play
Create new plothooks for games
Generate new NPCs to flesh out existing places
Help new and veteran DMs come up with fresh ideas for their table

Live Build 🚀
Go to https://eigengrausgenerator.com/ to see the most current live build of EEEG in action!
NOTE: The live build is often several weeks or months behind the current build here on GitHub. Compile the generator locally to see all the latest features and updates!
Community 👪
Join our Discord to talk about the project in real time

Learn more about the project
Ask questions and learn from other contributors
Show off your work

DISCORD
Also consider joining the subreddit.
Compiling 💻
To compile EEEG for local testing you will need the latest version of TweeGo and SugarCube.
# Show TweeGo knows where the SugarCube format is
export TWEEGO_PATH=LOCATION_OF_SUGARCUBE

# go to where you installed Tweego. If you installed it globally, feel free to skip this
cd $TWEEGO_PATH

# replace PROJECT_ROOT with wherever you git cloned the repository
tweego -o EEEG.html {PROJECT_ROOT}/EssentialEstablishmentGenerator --head={PROJECT_ROOT}/main.txt
This generates EEEG.html in the project root directory that you can open in a browser.
NOTE: You can save time once you've set your directories by saving that command as a .bat or .sh file.
Contributing ✒️
We love getting pull requests! You can find out more about contributing to the project here.
Once you've cloned the project, remember to yarn or npm install in order to install eslint.
If you don't want to code, that's okay! The Generator is built out of a novel's worth of words, and we're always in need of more descriptions. You can find writing tasks here.
You can also find easy work to do on the generator here.
Built With 🔨

Twine - The front end framework
Sugarcube 2 - A language for Twine
TweeGo - Twine command line compiler


If you can't contribute pull requests consider supporting the Generator through Patreon
We hope that you find it useful!
",328
icebreaker-fpga/icebreaker,None,"ICEBreaker FPGA
 
The iCEBreaker FPGA board is a low cost, open-source educational FPGA
development board.
The main motivating application of this board is for classes and workshops
teaching the use of the open source FPGA design flow using
Yosys,
nextpnr,
icestorm,
iverilog,
symbiflow and others. This means the board has
to be low cost and have a nice set of features to allow for the design of
interesting classes and workshop exercises. At the same time we want to allow
the user to use the proprietary vendor tools if they choose to. Because of that
we need to be compatible with their firmware upload tools.
Hardware Specifications


iCE40UP5K in QFN48 (SG48) package

iCE40 UltraPlus 5K
5280 Logic Cells (4-LUT + Carry + FF)
128 KBit Dual-Port Block RAM
1 MBit (128 KB) Single-Port RAM
PLL, Two SPI and two I2C hard IPs
Two internal oscillators (10 kHz and 48 MHz)
8 DSPs (16x16 multiply + 32 bit accumulate)
3x 24mA drive and 3x hard PWM IP



QSPI-DDR-capable flash 128 MBit (16 MB)

We selected to use the Winbond
W25Q128JVSIM
We want to enable projects that access the flash and we want to provide the
highest flash access speed possible.



FT2232H interface (microUSB plug)

programming compatible with iCEstick and HX8K board

works with Dimond Programmer and
iceprog


serial port compatible with iCEstick and HX8K breakout board
12 MHz XTAL oscillator (shared with FPGA)
Solder jumpers to offer direct SRAM programming (like on HX8K breakout board)



39 I/O capable pins:

4 pins for config (SDI, SDO, SCK, CSB)

Either loading config from on board FLASH chip or provided through the
FTDI chip with direct SRAM config.


2 extra GPIO pins for QSPI

Together with the config pins, allows storage of additional data in the
FLASH chip, and high speed QSPI DDR access. For example
picosoc
firmware.


3 PINs for RGB LED (pin header)
2 LEDs (one on output-only PLL pin)
1 Clock pin (on PLL GBIN)
1 UART Rx Pin via FTDI
1 UART Tx Pin via FTDI
1 Push Button
16 PINs on dual PMOD
8 PINs on single PMOD / snap-off section



Support for FTDI Async FIFO mode

We want to support FTDI Async mode via some (unpopulated by default) zero
ohm resistors
This shares 8 GPIOs with the single PMOD / snap-off section
This would also enable use of full list of RS232 signals
BDATA[0] -- Tx on FTDI / Rx on FPGA (always connected, no zohm resistor
required)
BDATA[1] -- Rx on FTDI / Tx on FPGA (always connected, no zohm resistor
required)
BDATA[7:2] -- Shared with snap-off section (via zohm resistor footprint)
RX Full -- Shared with snap-off section (via zohm resistor footprint)
TX Empty -- Shared with snap-off section (via zohm resistor footprint)
Read -- Shared with LED 1 (via zohm resistor footprint, LED used as RX
indicator)
Write -- Shared with LED 2 (via zohm resistor footprint, LED used as TX
indicator)
WakeUp -- Shared with Push Button (via zohm resistor footprint)
This configuration uses the following pins when the jumpers ore reconfigured:

The two on board red and green LED pins
The on board user button pin
snap off section single PMOD pins





Snap-off section (convertible to PMOD host / PMOD device)

5 LEDs in similar arrangement to iCEstick
3 Push Buttons



Other stuff

Status LEDs for Power and CDONE
Header with supply rails: 5V, 3V3, 1V2, GND
Debug header for all 6 QSPI pins
Test points for UART Rx / Tx signals
Jumpers or zohm resistors on all rails for measuring currents
Four 3mm mounting holes on the main section and two more on the snap-off
section
The two LEDs on the main section should be wired ""active low"" so they work
well as indicator LEDs for FIFO read/write.
The five LEDs on the snap-off section should be wired ""active high""
A zohm resistor for Bank 2 supply so that the IO voltage can be changed. Use
Bank 2 for one of the ports on the double PMOD.
There should be auxilary 5V pin headers available for the PMODs. Some PMOD
need either higher voltage or need to regulate their own voltage from 5V.



Unpopulated parts shipped with the board

3x Host PMOD (2x for dual PMOD port, 1x for snap-off section)
1x Device PMOD (for other side of snap-off section)




iCEBreaker bolck diagram

iCEBreaker V1.0b pinout legend

iCEBreaker V1.0b jumper legend

Branding

iCEbreaker (https://twitter.com/AboveVacant/status/948323920595308545)



",152
forkgood/easyhosts,None,"最新动态
Easy Hosts 项目一直以来都是根据上游 Github Hosts 规则，在服务器上无人工干预自动完成整合更新的，但是大陆网络环境复杂，Google 网域 IP 封锁严重，如果遇到无法访问Google网域的状况，可在 Issues 进行反馈，核实后我们将会尽快手动更新最新可用的 Google 网域 IP，以确保项目可用性，感谢网友反馈与关注，谢谢。
easyhosts
基于 Github 项目整合的远程 Hosts 直链，适配多种规则、终端。
每 30 分钟 自动同步一次 Github 最新可用项目并提供打包下载。
hosts 支持站点
目前支持访问以下网域，具体自行搜索规则文件：
Google 全家桶，Facebook，Instagram，Twitter，Tumblr，Youtube视频，Google Play下载等。
注意：Hosts方式访问以上站点，大部分只允许以HTTPS方式打开。
文件说明
更多说明请访问 easyhosts 项目主页：https://windows.cat

常规增强 Hosts：(racaljk 常用网站 + sy618 Google Play 下载源 + sy618 Youtube 视频源)
hosts.txt：常规增强 Hosts 规则，适用于Android/iOS/Windows/Mac OS/Linux等。
dnsmasq.txt：常规增强 dnsmasq规则，适用于 Linux 及 OpenWrt，路由器可用。
surge.txt：常规增强 Surge 规则，适用于Surge/Shadowrocket等各种代理软件。

去广告增强 Hosts：(racaljk 常用网站 + sy618 Google Play 下载源 + sy618 Youtube 视频源 + vokins 去广告源)
hosts-noad.txt：去广告增强 Hosts 规则，适用于Android/iOS/Windows/Mac
dnsmasq-noad.txt：去广告增强 dnsmasq规则，适用于 Linux 及 OpenWrt，路由器可用。
surge-noad.txt：去广告增强 Surge 规则，适用于Surge/Shadowrocket等各种代理软件。

其他文件：
log.txt：最新检测时间（更新周期以此文件为准）
检测周期为30分钟，本 Github 项目中的所有规则只会在上游Hosts发生变化时自动与Github同步，同步周期往往超过30分钟。
hosts.tar.gz：包含所有规则的压缩包文件
windows_hosts_manager_tools.zip：Windows Hosts 管理工具(批处理)
感谢
racaljk / sy618 / vokins / 景文互联
",237
airlift/airlift,Java,"Airlift


Airlift is a framework for building REST services in Java.
This project is used as the foundation for distributed systems like Presto.
",285
FloodHydrology/Wetland_Hydrologic_Capacitance_Model,R,"Wetland Hydrologic Capacitance Model
The wetland hydrologic capacitance model (WHC) is a process-based wetland hydrology model that simulates the water balance for headwater catchments in wetland rich landscapes. It is composed of a lumped upland component and an individual wetland component. Model inputs include precip, PET, wetland morphology, and soils information. Various versions of the model have been used in McLaughlin et al. 2014, Watts et al., 2015, and Jones et al. 2018. Please contact Nate Jones (njones@sesync.org) or Daniel McLaughlin (mclaugd@vt.edu) for more information about the model.
",2
urbit/urbit.org,CSS,"urbit.org
The website for the Urbit project.
Build

git clone this repository
Install zola
cd into content/docs and run git pull
In the root directory of urbit.org, run zola build or zola serve

Deploy
Commits to master are automatically deployed with Netlify.
",9
aaneto2606/EntregasPGP,None,"EntregasPGP
Repositório para entregas de mindmaps e outras atividades da matéria de Planejamento e Gerenciamento de Projetos
Grupo:
Antônio Almeida da Silva Neto (aasn) 
Pedro Henrique Alves Cruz (phac) 
Matheus Andrade Gomes (mag2) 
Gabriel Estevam Longuinhos (gel) 
Link - Trello: https://trello.com/invite/b/9T2K6kaO/fa4f48880a907896c0fa0c663530262d/sgeequipe-2
",2
overtone/overtone,Clojure,"                                                          888
                                                          888
         _ooooooooo._                                     888
      ,o888PP""""""""PP88   .d88b.  888  888  .d88b.  888d888 888888 .d88b.  88888b.   .d88b.
    d88P''          '  d88""""88b 888  888 d8P  Y8b 888P""   888   d88""""88b 888 ""88b d8P  Y8b
  ,88P                 88    88 Y88  88P 88888888 888     888   88    88 888  888 88888888
 ,88                   Y88..88P  Y8bd8P  Y8b.     888     Y88b. Y88..88P 888  888 Y8b.
,88'                    ""Y88P""    Y88P    ""Y8888  888      ""Y888 ""Y88P""  888  888  ""Y8888
d8P
d8b                        88[
`88                       J88
 Y8b                     ,88'
  Y8b.                  d88'
   `Y8b._            _o88P
     `Y888oo.____ooo888P'
        '""PP888888PP''

Collaborative Programmable Music.
Overtone is an Open Source toolkit for designing synthesizers and
collaborating with music.  It provides:

A Clojure API to the SuperCollider synthesis engine
A growing library of musical functions (scales, chords, rhythms,
arpeggiators, etc.)
Metronome and timing system to support live-programming and sequencing
Plug and play MIDI device I/O
A full Open Sound Control (OSC) client and server implementation.
Pre-cache - a system for locally caching external assets such as .wav
files
An API for querying and fetching sounds from http://freesound.org
A global concurrent event stream

Quick Start
Installation
    # Install lein2
    # https://github.com/technomancy/leiningen

    $ lein new insane-noises

    # add the following dependencies to insane-noises/project.clj
    # [org.clojure/clojure ""1.9.0""]
    # [overtone ""0.10.4""]

    $ cd insane-noises
    $ lein repl
Making sounds
    ;; boot the server
    user=> (use 'overtone.live)

    ;; listen to the joys of a simple sine wave
    user=> (demo (sin-osc))

    ;; or something more interesting...
    user=> (demo 7 (lpf (mix (saw [50 (line 100 1600 5) 101 100.5]))
                   (lin-lin (lf-tri (line 2 20 5)) -1 1 400 4000)))
Detailed Instructions
For a more detailed set of setup instructions (including details
specific to Windows and Linux) head over to the
Overtone wiki installation page
We maintain documentation for all aspects of the system in the
project wiki, you'll
find tutorials and examples on topics such as synthesizing new sounds
from scratch, live-coding and generating musical scores on the fly. If
you see anything missing, please feel free to add it yourself, or hit us
up on the mailing list and
we'll sort something out.
Cheat Sheet
For a quick glance at all the exciting functionality Overtone puts at
your musical fingertips check out the cheat sheet:
https://github.com/overtone/overtone/raw/master/docs/cheatsheet/overtone-cheat-sheet.pdf
Overtone Powered Bands
A list of bands using Overtone to generate sounds:

Meta-eX
Repl Electric

Community
Mailing List
We encourage you to join the
mailing list to see what
other people are getting up to with Overtone. Use it to ask questions,
show off what you've made and even meet fellow Overtoners in your area
so you can meet up for impromptu jam sessions. All we ask is that you be
considerate, courteous and respectful and that you share as much of your
code as possible so we can all learn how to make crazy cool sounds
together.
Twitter
Follow @overtone on Twitter: http://twitter.com/overtone
Web
Our main website is hosted on GitHub: http://overtone.github.io
Videos
Introductions
Head over to Vimeo for a fast-paced 4 minute introduction to live-coding
with Overtone to see what's possible
http://vimeo.com/22798433
For a nice overview of the basics of creating and playing with
synthesized instruments in Overtone checkout Philip Potter's 20 minute
workshop:
http://skillsmatter.com/podcast/scala/clojurex-unpanel-2894
Chris Ford also delivers a beautifully paced introduction to fundamental music
concepts from basic sine waves to Bach's Goldberg Variations with live examples throughout:
http://skillsmatter.com/podcast/home/functional-composition
There are also the following tutorials:

Setting up an Overtone Development Environment - Running on Edge
http://vimeo.com/25102399
How to Hack Overtone with Emacs http://vimeo.com/25190186

Presentations

Rich Hickey - Harmonikit: http://www.youtube.com/watch?v=bhkdyCPYgLs
Sam Aaron - Programming Music With Overtone: http://www.youtube.com/watch?v=imoWGsipe4k
Chris Ford - Functional Composition: http://www.youtube.com/watch?v=Mfsnlbd-4xQ
Meta-eX - Live Coding with Meta-eX: https://www.youtube.com/watch?v=zJqH5bNcIN0

Interviews
Overtone has generated quite a bit of interest. Here's a list of
available interviews which go into further depth on the background and
philosophy of Overtone:

http://twit.tv/show/floss-weekly/197
http://mostlylazy.com/2011/11/18/episode-0-0-2-sam-aaron-and-overtone-at-clojure-conj-2011/
http://codebassradio.net/2011/11/29/runtime-expectations-episode-13-hot-clojure-conj/
(scroll down to the section with Sam Aaron, Ghadi Shayban, and Daniel Spiewak)
http://clojure.com/blog/2012/01/04/take5-sam-aaron.html

Performances

Repl Electric: https://vimeo.com/95988263
Piotr Jagielski‏: https://www.youtube.com/watch?v=r8YKC7Qugm8
Sam Aaron Live @ Arnolfini:  https://vimeo.com/46867490
Meta-eX Live @ Music Tech Fest: http://youtu.be/zJqH5bNcIN0?t=15m25s

Source Repository
Downloads and the source repository can be found on GitHub:
http://github.com/overtone/overtone
Clone the repository on GitHub to get started developing, and if you are
ready to submit a patch then fork your own copy and do a pull request.
Lein Support
Overtone and its dependencies are on http://clojars.org, and the
dependency for your project.clj is:
[overtone ""0.10.4""]

Contributors
See: https://github.com/overtone/overtone/graphs/contributors
",4590
ok1hra/IP_switch,C++,"IP switch

Based on ESP32-GATEWAY
Controled from

Open Interface III
Band decoder MK2
Manual IP switch MK2

",2
DrazorV/Team13-Nutrition,Java,"Nutrition🍑 - Team13
Android application for nutrition program
Προδιαγραφή των απαιτήσεων λογισμικού
",3
ok1hra/IP_switch,C++,"IP switch

Based on ESP32-GATEWAY
Controled from

Open Interface III
Band decoder MK2
Manual IP switch MK2

",2
DrazorV/Team13-Nutrition,Java,"Nutrition🍑 - Team13
Android application for nutrition program
Προδιαγραφή των απαιτήσεων λογισμικού
",3
FrankerFaceZ/FrankerFaceZ,JavaScript,"FrankerFaceZ
Copyright (c) 2018 Dan Salvato LLC
Licensed under the Apache License, Version 2.0. See LICENSE.
Developing
FrankerFaceZ uses node.js to manage development dependencies and to run an HTTP
server for development. To get everything you need:

Install node.js and npm
Run npm install within the FrankerFaceZ directory.

From there, you can use npm to build FrankerFaceZ from source simply by
running npm run build. For development, you can instruct gulp to watch
the source files for changes and re-build automatically with npm start
FrankerFaceZ comes with a local development server that listens on port 8000
and it serves up local development copies of files, falling back to the CDN
when a local copy of a file isn't present.
To make FrankerFaceZ load from your local development server, you must set
the local storage variable ffzDebugMode to true. Just run the following
in your console on Twitch: localStorage.ffzDebugMode = true;
It should be noted that this project is not a browser extension that you
would load in your browser's extensions system. You still need the FrankerFaceZ
extension or user-script for your browser.
Editor Settings
Please make sure that your editor is configured to use tabs rather than spaces
for indentation and that lines are ended with \n. It's recommended that you
configure linting support for your editor as well.
If you're using Visual Studio Code, make sure to install the ESLint extension
and add the following to your workspace settings:
{
	""eslint.validate"": [
		""javascript"",
		""javascriptreact"",
		""vue""
	]
}
",220
QubesOS/qubes-doc,HTML,"Qubes OS Documentation
Canonical URL: https://www.qubes-os.org/doc/
All Qubes OS Project documentation pages are stored as plain text
files in this dedicated repository. By cloning and regularly pulling from
this repo, users can maintain their own up-to-date offline copy of all
Qubes documentation rather than relying solely on the Web.
For more information about the documentation, including how to contribute,
please see the Documentation Guidelines.
",168
sobotka/blender,C,"Blender
Blender is the free and open source 3D creation suite.
It supports the entirety of the 3D pipeline-modeling, rigging, animation, simulation, rendering, compositing,
motion tracking and video editing.




Project Pages

Main Website
Reference Manual
User Community


Development

Build Instructions
Code Review & Bug Tracker
Developer Forum
Developer Documentation


License
Blender as a whole is licensed under the GNU Public License, Version 3.
Individual files may have a different, but compatible license.
See blender.org/about/license for details.
",142
jhy/jsoup,Java,"jsoup: Java HTML Parser
jsoup is a Java library for working with real-world HTML. It provides a very convenient API for extracting and manipulating data, using the best of DOM, CSS, and jquery-like methods.
jsoup implements the WHATWG HTML5 specification, and parses HTML to the same DOM as modern browsers do.

scrape and parse HTML from a URL, file, or string
find and extract data, using DOM traversal or CSS selectors
manipulate the HTML elements, attributes, and text
clean user-submitted content against a safe white-list, to prevent XSS attacks
output tidy HTML

jsoup is designed to deal with all varieties of HTML found in the wild; from pristine and validating, to invalid tag-soup; jsoup will create a sensible parse tree.
See jsoup.org for downloads and the full API documentation.

Example
Fetch the Wikipedia homepage, parse it to a DOM, and select the headlines from the In the News section into a list of Elements:
Document doc = Jsoup.connect(""http://en.wikipedia.org/"").get();
log(doc.title());
Elements newsHeadlines = doc.select(""#mp-itn b a"");
for (Element headline : newsHeadlines) {
  log(""%s\n\t%s"", 
    headline.attr(""title""), headline.absUrl(""href""));
}
Online sample, full source.
Open source
jsoup is an open source project distributed under the liberal MIT license. The source code is available at GitHub.
Getting started

Download the latest jsoup jar (or add it to your Maven/Gradle build)
Read the cookbook
Enjoy!

Development and support
If you have any questions on how to use jsoup, or have ideas for future development, please get in touch via the mailing list.
If you find any issues, please file a bug after checking for duplicates.
The colophon talks about the history of and tools used to build jsoup.
Status
jsoup is in general, stable release.
",7057
yandex/ClickHouse,C++,"
ClickHouse is an open-source column-oriented database management system that allows generating analytical data reports in real time.
Useful Links

Official website has quick high-level overview of ClickHouse on main page.
Tutorial shows how to set up and query small ClickHouse cluster.
Documentation provides more in-depth information.
Blog contains various ClickHouse-related articles, as well as announces and reports about events.
Contacts can help to get your questions answered if there are any.
You can also fill this form to meet Yandex ClickHouse team in person.

Upcoming Events

ClickHouse at Percona Live 2019 in Austin on May 28-30.
ClickHouse Community Meetup in San Francisco on June 4.
ClickHouse Community Meetup in Beijing on June 8.
ClickHouse Community Meetup in Shenzhen on October 20.
ClickHouse Community Meetup in Shanghai on October 27.

",6915
kazzastic/CV,Jupyter Notebook,"maze_solving_image_processing
Libraries used matplot, numpy, openCV, mazelib, pandas, scipy, scikit-learn
The basic idea here is to solve the given maze using the shortest-path algorithm.
the library mazelib has been used so far to solve complex mazes.
The second step is supposed to be image processing, where the given image would be converted
into the numbers(1's and 0's) which the library mazelib can process and solve on its own.
The third step is to integrate the above given processing of image processing and maze solving in
real life maze solving car build upon raspberry pi using python and RPI GPIO library.
",4
dirigeants/klasa,JavaScript,"Klasa









Let's stop reinventing the wheel, and start coding the bots of our dreams!

Klasa is an OOP discord.js bot framework which aims to be the most feature complete, while feeling like a consistent extension of discord.js.
Originally based on Komada, Klasa has become a ship of Theseus, keeping many similarities with the former framework but with many enhancements and extra features.
What's with the name?
Following suit from Komada (the Croatian word for ""pieces""), Klasa is the Croatian word for ""class"". By the same token, Klasa is modular, and each module is a piece of a puzzle you use to build your own bot. And you can replace, enhance, reload or remove these pieces; the difference is that Klasa uses classes.
Features

Abstracted database handler, works with any database, or atomically written JSON (by default).
Easy and powerful command system, featuring usage string, dependent arguments, and custom types.
Easy and powerful to configure the permission levels system.
Easy to create your own pieces and structures!
Editable commands with quoted string support and custom parameter delimiter.
Flag arguments.
Full OOP and hot-reloadable pieces.
Full personalizable configuration system that can serve for much more than just guilds.
Incredibly fast loading (~100ms) with deep loading for commands.
Per-command cooldowns with bucket support and easy to configure.
Many different pieces and standalone utils to help you build the bot of your dreams!

Commands: The most basic piece, they run when somebody types the prefix and the command name or any of its aliases.
Events: Hot-reloadable structures for events, with internal error handling.
Extendables: Easily extend Klasa or discord.js.
Finalizers: Structures that run after successful command run.
Inhibitors: Middleware that can stop a command from running (blacklist, permissions...).
Languages: Easy internationalization support for your bot!
Monitors: Watch every single message your bot receives! They're perfect for no-mention-spam, swear word filter, and so on!
Providers: You can have one, or more, they're interfaces for the settings system and ensures the data is written correctly!
Serializers: These allow you to change how the Settings system reads, writes, and displays information.
Tasks: Pieces that handle scheduled tasks.



Time to get started!
See the following tutorial on how to get started using Klasa.
See also:

Documentation for Klasa
Example premade pieces
VS Code extension for rapid development

",140
kubernetes/test-infra,Go,"Kubernetes Test Infrastructure
 
The test-infra repository contains a collection of tools for testing Kubernetes
and displaying Kubernetes tests results. See also CONTRIBUTING.md.
See the architecture diagram for an overview of how
the different services interact.
Viewing test results

The Kubernetes TestGrid shows historical test results

Configure your own testgrid dashboard at testgrid/config.yaml
Gubernator formats the output of each run


PR Dashboard finds PRs that need your attention
Prow schedules testing and updates issues

Prow responds to GitHub events, timers and manual commands
given in GitHub comments.
The prow dashboard shows what it is currently testing
Configure prow to run new tests at config/jobs


Triage Dashboard aggregates failures

Triage clusters together similar failures
Search for test failures across jobs
Filter down failures in a specific regex of tests and/or jobs


Velodrome metrics track job and test health.

Kettle does collection, metrics does reporting, and velodrome is the frontend.



E2E Testing
Our e2e testing uses kubetest to build/deploy/test kubernetes
clusters on various providers. Please see those documents for additional details
about this tool as well as e2e testing generally.
Anyone can reconfigure our CI system with a test-infra PR that updates the
appropriate files. Detailed instructions follow:
CI Job management
Create a new job
Bootstrap is deprecated, Please utilize the podutils to create new prowjobs.
Create a PR in this repo to add/update/remove a job or suite. Specifically
you'll need to do the following:


Add the job to the appropriate section in config/jobs

Directory Structure:

In general for jobs for github.com/org/repo use config/jobs/org/repo/filename.yaml
For Kubernetes repos we also allow config/jobs/kubernetes/sig-foo/filename.yaml
We use basename of the config name as a key in the prow configmap, so the name of your config file need to be unique across the config subdir


Type of jobs:

Presubmit jobs run on unmerged code in PRs
Postsubmit jobs run after merging code
Periodic job run on a timed basis
You can find more prowjob definitions at how-to-add-new-jobs


A simple sample job uses podutil looks like:
- name: foo-repo-presubmit-test
  decorate: true
  spec:
    containers:
    - image: gcr.io/k8s-testimages/kubekins-e2e:latest-master
      command:
      - /path/to/cmd
      args:
      - positional
      - --and
      - flags




Add the job name to the test_groups list in testgrid/config.yaml

Also the group to at least one dashboard_tab



The configs need to be sorted and kubernetes must be in sync with the security repo, or else presubmit will fail.
You can run the script below to keep them valid:
hack/update-config.sh

Local testing
docker run your image locally, and mount in the repos you depend on.
Release branch jobs & Image validation jobs
Release branch jobs and image validation jobs are defined in test_config.yaml.
We test different master/node image versions against multiple k8s branches on different features.
Those jobs are using channel based versions, current supported testing map is:

k8s-dev : master
k8s-beta : release-1.15
k8s-stable1 : release-1.14
k8s-stable2 : release-1.13
k8s-stable3 : release-1.12

Our build job will generate a ci/(channel-name) file pointer in gcs.
After you update test_config.yaml, please run
bazel run //experiment:generate_tests -- --yaml-config-path=experiment/test_config.yaml

to regenerate the job configs.
We are moving towards making more jobs to fit into the generated config.
Presubmit will tell you if you forget to do any of this correctly.
Merge your PR and @k8s-ci-robot will deploy your change automatically.
Update an existing job
Largely similar to creating a new job, except you can just modify the existing
entries rather than adding new ones.
Update what a job does by editing its definition in config/jobs.
Update where the job appears on testgrid by changing testgrid/config.yaml.
Delete a job
The reverse of creating a new job: delete the appropriate entries in
config/jobs and testgrid/config.yaml.
Merge your PR and @k8s-ci-robot will deploy your change automatically.
Building and testing the test-infra
We use Bazel to build and test the code in this repo.
The commands bazel build //... and bazel test //... should be all you need
for most cases. If you modify Go code, run ./hack/update-bazel.sh to keep
BUILD.bazel files up-to-date.
Contributing Test Results
The Kubernetes project encourages organizations to contribute execution of e2e
test jobs for a variety of platforms (e.g., Azure, rktnetes). For information about
how to contribute test results, see Contributing Test Results.
Other Docs

kubernetes/test-infra dependency management

",1184
yandex/ClickHouse,C++,"
ClickHouse is an open-source column-oriented database management system that allows generating analytical data reports in real time.
Useful Links

Official website has quick high-level overview of ClickHouse on main page.
Tutorial shows how to set up and query small ClickHouse cluster.
Documentation provides more in-depth information.
Blog contains various ClickHouse-related articles, as well as announces and reports about events.
Contacts can help to get your questions answered if there are any.
You can also fill this form to meet Yandex ClickHouse team in person.

Upcoming Events

ClickHouse at Percona Live 2019 in Austin on May 28-30.
ClickHouse Community Meetup in San Francisco on June 4.
ClickHouse Community Meetup in Beijing on June 8.
ClickHouse Community Meetup in Shenzhen on October 20.
ClickHouse Community Meetup in Shanghai on October 27.

",6915
kazzastic/CV,Jupyter Notebook,"maze_solving_image_processing
Libraries used matplot, numpy, openCV, mazelib, pandas, scipy, scikit-learn
The basic idea here is to solve the given maze using the shortest-path algorithm.
the library mazelib has been used so far to solve complex mazes.
The second step is supposed to be image processing, where the given image would be converted
into the numbers(1's and 0's) which the library mazelib can process and solve on its own.
The third step is to integrate the above given processing of image processing and maze solving in
real life maze solving car build upon raspberry pi using python and RPI GPIO library.
",4
dirigeants/klasa,JavaScript,"Klasa









Let's stop reinventing the wheel, and start coding the bots of our dreams!

Klasa is an OOP discord.js bot framework which aims to be the most feature complete, while feeling like a consistent extension of discord.js.
Originally based on Komada, Klasa has become a ship of Theseus, keeping many similarities with the former framework but with many enhancements and extra features.
What's with the name?
Following suit from Komada (the Croatian word for ""pieces""), Klasa is the Croatian word for ""class"". By the same token, Klasa is modular, and each module is a piece of a puzzle you use to build your own bot. And you can replace, enhance, reload or remove these pieces; the difference is that Klasa uses classes.
Features

Abstracted database handler, works with any database, or atomically written JSON (by default).
Easy and powerful command system, featuring usage string, dependent arguments, and custom types.
Easy and powerful to configure the permission levels system.
Easy to create your own pieces and structures!
Editable commands with quoted string support and custom parameter delimiter.
Flag arguments.
Full OOP and hot-reloadable pieces.
Full personalizable configuration system that can serve for much more than just guilds.
Incredibly fast loading (~100ms) with deep loading for commands.
Per-command cooldowns with bucket support and easy to configure.
Many different pieces and standalone utils to help you build the bot of your dreams!

Commands: The most basic piece, they run when somebody types the prefix and the command name or any of its aliases.
Events: Hot-reloadable structures for events, with internal error handling.
Extendables: Easily extend Klasa or discord.js.
Finalizers: Structures that run after successful command run.
Inhibitors: Middleware that can stop a command from running (blacklist, permissions...).
Languages: Easy internationalization support for your bot!
Monitors: Watch every single message your bot receives! They're perfect for no-mention-spam, swear word filter, and so on!
Providers: You can have one, or more, they're interfaces for the settings system and ensures the data is written correctly!
Serializers: These allow you to change how the Settings system reads, writes, and displays information.
Tasks: Pieces that handle scheduled tasks.



Time to get started!
See the following tutorial on how to get started using Klasa.
See also:

Documentation for Klasa
Example premade pieces
VS Code extension for rapid development

",140
kubernetes/test-infra,Go,"Kubernetes Test Infrastructure
 
The test-infra repository contains a collection of tools for testing Kubernetes
and displaying Kubernetes tests results. See also CONTRIBUTING.md.
See the architecture diagram for an overview of how
the different services interact.
Viewing test results

The Kubernetes TestGrid shows historical test results

Configure your own testgrid dashboard at testgrid/config.yaml
Gubernator formats the output of each run


PR Dashboard finds PRs that need your attention
Prow schedules testing and updates issues

Prow responds to GitHub events, timers and manual commands
given in GitHub comments.
The prow dashboard shows what it is currently testing
Configure prow to run new tests at config/jobs


Triage Dashboard aggregates failures

Triage clusters together similar failures
Search for test failures across jobs
Filter down failures in a specific regex of tests and/or jobs


Velodrome metrics track job and test health.

Kettle does collection, metrics does reporting, and velodrome is the frontend.



E2E Testing
Our e2e testing uses kubetest to build/deploy/test kubernetes
clusters on various providers. Please see those documents for additional details
about this tool as well as e2e testing generally.
Anyone can reconfigure our CI system with a test-infra PR that updates the
appropriate files. Detailed instructions follow:
CI Job management
Create a new job
Bootstrap is deprecated, Please utilize the podutils to create new prowjobs.
Create a PR in this repo to add/update/remove a job or suite. Specifically
you'll need to do the following:


Add the job to the appropriate section in config/jobs

Directory Structure:

In general for jobs for github.com/org/repo use config/jobs/org/repo/filename.yaml
For Kubernetes repos we also allow config/jobs/kubernetes/sig-foo/filename.yaml
We use basename of the config name as a key in the prow configmap, so the name of your config file need to be unique across the config subdir


Type of jobs:

Presubmit jobs run on unmerged code in PRs
Postsubmit jobs run after merging code
Periodic job run on a timed basis
You can find more prowjob definitions at how-to-add-new-jobs


A simple sample job uses podutil looks like:
- name: foo-repo-presubmit-test
  decorate: true
  spec:
    containers:
    - image: gcr.io/k8s-testimages/kubekins-e2e:latest-master
      command:
      - /path/to/cmd
      args:
      - positional
      - --and
      - flags




Add the job name to the test_groups list in testgrid/config.yaml

Also the group to at least one dashboard_tab



The configs need to be sorted and kubernetes must be in sync with the security repo, or else presubmit will fail.
You can run the script below to keep them valid:
hack/update-config.sh

Local testing
docker run your image locally, and mount in the repos you depend on.
Release branch jobs & Image validation jobs
Release branch jobs and image validation jobs are defined in test_config.yaml.
We test different master/node image versions against multiple k8s branches on different features.
Those jobs are using channel based versions, current supported testing map is:

k8s-dev : master
k8s-beta : release-1.15
k8s-stable1 : release-1.14
k8s-stable2 : release-1.13
k8s-stable3 : release-1.12

Our build job will generate a ci/(channel-name) file pointer in gcs.
After you update test_config.yaml, please run
bazel run //experiment:generate_tests -- --yaml-config-path=experiment/test_config.yaml

to regenerate the job configs.
We are moving towards making more jobs to fit into the generated config.
Presubmit will tell you if you forget to do any of this correctly.
Merge your PR and @k8s-ci-robot will deploy your change automatically.
Update an existing job
Largely similar to creating a new job, except you can just modify the existing
entries rather than adding new ones.
Update what a job does by editing its definition in config/jobs.
Update where the job appears on testgrid by changing testgrid/config.yaml.
Delete a job
The reverse of creating a new job: delete the appropriate entries in
config/jobs and testgrid/config.yaml.
Merge your PR and @k8s-ci-robot will deploy your change automatically.
Building and testing the test-infra
We use Bazel to build and test the code in this repo.
The commands bazel build //... and bazel test //... should be all you need
for most cases. If you modify Go code, run ./hack/update-bazel.sh to keep
BUILD.bazel files up-to-date.
Contributing Test Results
The Kubernetes project encourages organizations to contribute execution of e2e
test jobs for a variety of platforms (e.g., Azure, rktnetes). For information about
how to contribute test results, see Contributing Test Results.
Other Docs

kubernetes/test-infra dependency management

",1184
vsl-lang/VSL,JavaScript,"











Language Documentation  —
    API Documentation


    Versatile Scripting Language
  

VSL is a modern, powerful, fast, and easy to write programming language designed
for the 21st century.
Download
You can either build from source (see Building) or installed a pre-compiled
binary/executable:



Windows
macOS
Linux




Download
Download
Download



Changes
Git revision history is a mess but checkout CHANGELOG.md for detailed devleopment
information.
Building
Building isn't too diffiult. Usually you'll want to install a pre-built binary
but if you're feeling adventurous or just want to help build VSL (:D) building
from source is simple:
$ git clone --recursive https://github.com/vsl-lang/VSL
$ npm install
$ npm run build
Do note, branch of the develop branch to make changes. All PRs go there. Other
commands:
$ npm run coverage # Generates testing coverage reports
$ npm test         # Runs all tests
$ npm run dev      # Development build
$ npm run docs     # Make docs
$ npm run lint     # Lint code and make sure not crap

Do note you don't need to generate docs unless you want them for yourself because
the CI will automatically generate docs.
Development Info

Docs are located here.
A bunch of READMEs are located in the dirs which do more complex things

Problem
Today they are quite a few languages, some popular ones you may of heard of are:

Python
Java
JavaScript
C/C++

and while these are all great and well (and have worked). Here are the things
one wants from a programming language:

Portability (C/C++ lack here)
Ease of Use (Java & C/C++ lack here, but arguable)
Rapid prototyping (Java, C/C++, and even JS ES2015+)
Saftey: type, memory, etc. (JS & Python lack)
Bare-metal speed (yeah...)
Powerful and close-to-hardware (JS, Python, and Java lack)

So VSL aims to solve all of these problems


Portability: By leveraging the LLVM
bytecode engine VSL can compile to almost all targets and is designed for simple
compatibility with existing C projects.


Fast: Due to careful design and implementation choices, VSL compiles to
very similar ASM to what something written in a low-level language such as C
would produce.


Safe: By using syntax sugar, and powerful type-negotiation, VSL has one
of the best type deduction algorithms. Combined with low compilation overhead,
VSL can generate code with bare minimum boilerplate and guarunteed saftey at
compile-time.


Powerful: VSL uses high-level syntax to be able to write code that works
for all types of programmers, whether you are functional, OO, scripting, or
low-level engineer. Through both high-level interfaces for low-level functions,
you can use VSL for tasks low-level such as read/writing bits from a serial port
to running a server.


Reliability: through bindings of reliable, trusted, and industry-standard
libraries such as libcurl, and libc backends, VSL has powerful low-level
pointer interopability and the power of full OO-classes.


Examples
VSL functions both as a scripting and a full-blown language so two alterntaives
are given for all programs. That said, they are many more ways to write many of
these programs, neither more correct than the other.
Hello, World!
print(""Hello, World"")
func main(args: String[]) {
  print(""Hello, World!"")
}
Fizzbuzz
let fizzbuzz :: (of: Int) -> String
fizzbuzz(i where i % 3, i % 5) -> ""FizzBuzz""
fizzbuzz(i where i % 3) -> ""Fizz""
fizzbuzz(i where i % 5) -> ""Buzz""
fizzbuzz(i) -> String(for: i)
",15
serhatuzunbayir/Crime_File_Management_System_1-Batuhan-Can-Gizem-Dilara,Java,"Crime_File_Management_System_1-Batuhan-Can-Gizem-Dilara
Online crime file management is an application that provides a facility for reporting crimes, complaints, missing
people, and etc. Any number of people can simultaneously access the server by making a login.
In the current process when a person has to make a complaint he or she has to go to a nearby police station.
This process has many drawbacks like it consumes lot of time, man power, consumes large amount of work,
lacks attention etc.
The user can file a complaint, add a crime report, upload the details of missing person, view all further updates
by other users. They also can edit complaints, chat or mail people, get updates on crime reports or the
complaints filed by them.
The administrator can view and reply to a users complaint, View and reply to users crimes, add and delete
latest hot news, view and delete users feedback, Add, delete and view most wanted people, add, delete and view
missing people, add and view criminal reports. It also felicitates e-mail and chat facilities along with other log
in controls
",4
docker-library/docs,Shell,"What is this?
This repository contains the docs for each of the Docker official images. See docker-library/official-images for the configuration how the images are built. To see all of the official images go to the hub.
All Markdown files here are run through tianon's fork of markdownfmt (only forked to add some smaller-diff preference and minor DockerHub-compatibility changes), and verified as formatted correctly via Travis-CI.

Travis CI:

Automated update.sh and push.sh:


Table of Contents

What is this?

Table of Contents


How do I add a new image's docs
How do I update an image's docs
What are all these files?

update.sh
generate-repo-stub-readme.sh
push.pl
.template-helpers/generate-dockerfile-links-partial.sh
.template-helpers/template.md and .template-helpers/user-feedback.md
folder <image name>
<image name>/README.md
<image name>/content.md
<image name>/README-short.txt
<image name>/logo.png
<image name>/license.md
<image name>/maintainer.md
<image name>/github-repo
<image name>/user-feedback.md


Issues and Contributing

How do I add a new image's docs

create a folder for my image: mkdir myimage
create a README-short.txt (required, 100 char max)
create a content.md (required)
create a license.md (required)
create a maintainer.md (required)
create a github-repo (required)
add a logo.png (recommended)

Optionally:

run ./markdownfmt.sh -l myimage to verify whether format of your markdown files is compliant to tianon/markdownfmt. In case you see any file names, markdownfmt detected some issues, which might result in a failed build during continuous integration. run ./markdownfmt.sh -d myimage to see a diff of changes required to pass.
run ./update.sh myimage to generate myimage/README.md for manual review of the generated copy.
Note: do not actually commit the README.md file; it is automatically generated/committed before being uploaded to Docker Hub.

How do I update an image's docs
To update README.md for a specific image do not edit README.md directly. Please edit content.md or another appropriate file within the folder. To see the changes, run ./update.sh myimage from the repo root, but do not add the README.md changes to your pull request. See also markdownfmt.sh point above.
What are all these files?
update.sh
This is the main script used to generate the README.md files for each image. The generated file is committed along with the files used to generate it (see below on what customizations are available). Accepted arguments are which image(s) you want to update or no arguments to update all of them.
This script assumes bashbrew is in your PATH (for scraping relevant tag information from the library manifest file for each repository).
generate-repo-stub-readme.sh
This is used to generate a simple README.md to put in the image's repo. Argument is the name of the image, like golang and it then outputs the readme to standard out.
push.pl
This is used by us to push the actual content of the READMEs to the Docker Hub as special access is required to modify the Hub description contents.
.template-helpers/generate-dockerfile-links-partial.sh
This script is used by update.sh to create the ""Supported tags and respective Dockerfile links"" section of each generated README.md from the information in the official-images library/ manifests.
.template-helpers/template.md and .template-helpers/user-feedback.md
These files are the templates used in building the <image name>/README.md file, in combination with the individual image's files.
folder <image name>
This is where all the partial and generated files for a given image reside, (ex: golang/).
<image name>/README.md
This file is generated using update.sh.
<image name>/content.md
This file contains the main content of your image's long description. The basic parts you should have are a ""What Is"" section and a ""How To"" section. See the doc on Official Repos for more information on long description. The issues and contribution section is generated by the script but can be overridden. The following is a basic layout:
# What is XYZ?

// about what the contained software is

%%LOGO%%

# How to use this image

// descriptions and examples of common use cases for the image
// make use of subsections as necessary
<image name>/README-short.txt
This is the short description for the docker hub, limited to 100 characters in a single line.

Go (golang) is a general purpose, higher-level, imperative programming language.

<image name>/logo.png
Logo for the contained software. While there are not hard rules on formatting, most existing logos are square or landscape and stay within a few hundred pixels of width.
<image name>/license.md
This file should contain a link to the license for the main software in the image. Here is an example for golang:
View [license information](http://golang.org/LICENSE) for the software contained in this image.
<image name>/maintainer.md
This file should contain a link to the maintainers of the Dockerfile.
<image name>/github-repo
This file should contain the URL to the GitHub repository for the Dockerfiles that become the images. The file should be in a single line ending in a newline with no extraneous whitespace. Only one GitHub repo per image repository is supported. It is used in generating links. Here is an example for golang:
https://github.com/docker-library/golang

<image name>/user-feedback.md
This file is an optional override of the default user-feedback.md for those repositories with different issue and contributing policies.
Issues and Contributing
If you would like to make a new Official Image, be sure to follow the guidelines.
Feel free to make a pull request for fixes and improvements to current documentation. For questions or problems on this repo come talk to us via the #docker-library IRC channel on Freenode or open up an issue.
",2522
inorichi/tachiyomi-extensions,Kotlin,"

This repository contains the available extension catalogues for the Tachiyomi app.
Usage
Extension sources can be downloaded, installed, and uninstalled via the main Tachiyomi app. They are installed and uninstalled like regular apps, in .apk format.
Downloads
If you prefer to directly download the APK files, or would like to find an older version, they are available in the repo branch.
Requests
source requests here are meant as up-for-grabs for any developer, thus it's impossible to provide a time estimation for any of them. Furthermore, some sites are impossible to do, usually because of various technical reasons.
Contributing
Contributions are welcome!
Check out the repo's issue backlog for source requests and bug reports.
To get started with development, see CONTRIBUTING.md.
",349
dnc1994/blog,JavaScript,"Linghao's Blog
linghao.io
Development

Install yarn from yarnpkg.org
Dev: yarn dev and open localhost:3000
Build static: yarn build then yarn export to /out

▲ Deployment
Every single push to master gets deployed to the Now Cloud automatically.
Powered by Next.js, MDX and ▲ ZEIT Now.

This is a shameless copy from Shu's website.
Website code open sourced under MIT.
Website content under CC BY-NC-SA 4.0.
",2
jenkins-infra/wechat,Groovy,"Jenkins WeChat
Jenkins WeChat subscription account will deliver the messages or events from the Jenkins Community.
All articles should be open-source, every contributor could create a PR. Once we reviewed it, your articles could be released.
We have a robot who can reply to your messages automatically.
Unfortunately, its ability is very limitation. It just can understand a few words
from here.
TODO List
Pick up a task from here, if you're interesting in contribution. See our contributing guide.
You can find all contributors at here.
Join us
Please scan QRCode below:

",6
jenkins-infra/wechat,Groovy,"Jenkins WeChat
Jenkins WeChat subscription account will deliver the messages or events from the Jenkins Community.
All articles should be open-source, every contributor could create a PR. Once we reviewed it, your articles could be released.
We have a robot who can reply to your messages automatically.
Unfortunately, its ability is very limitation. It just can understand a few words
from here.
TODO List
Pick up a task from here, if you're interesting in contribution. See our contributing guide.
You can find all contributors at here.
Join us
Please scan QRCode below:

",6
apple/foundationdb,C++,"
FoundationDB is a distributed database designed to handle large volumes of structured data across clusters of commodity servers. It organizes data as an ordered key-value store and employs ACID transactions for all operations. It is especially well-suited for read/write workloads but also has excellent performance for write-intensive workloads. Users interact with the database using API language binding.
To learn more about FoundationDB, visit foundationdb.org
Documentation
Documentation can be found online at https://apple.github.io/foundationdb/. The documentation covers details of API usage, background information on design philosophy, and extensive usage examples. Docs are built from the source in this repo.
Forums
The FoundationDB Forums are the home for most of the discussion and communication about the FoundationDB project. We welcome your participation!  We want FoundationDB to be a great project to be a part of and, as part of that, have established a Code of Conduct to establish what constitutes permissible modes of interaction.
Contributing
Contributing to FoundationDB can be in contributions to the code base, sharing your experience and insights in the community on the Forums, or contributing to projects that make use of FoundationDB. Please see the contributing guide for more specifics.
Getting Started
Binary downloads
Developers interested in using the FoundationDB store for an application can get started easily by downloading and installing a binary package. Please see the downloads page for a list of available packages.
Compiling from source
Developers on a OS for which there is no binary package, or who would like to start hacking on the code can get started by compiling from source.
Currently there are two build systems: a collection of Makefiles and a
CMake-based. Both of them should work for most users and CMake will eventually
become the only build system available.
Makefile
MacOS

Check out this repo on your Mac.
Install the Xcode command-line tools.
Download version 1.67.0 of Boost.
Set the BOOSTDIR environment variable to the location containing this boost installation.
Install Mono.
Install a JDK. FoundationDB currently builds with Java 8.
Navigate to the directory where you checked out the foundationdb repo.
Run make.

Linux


Install Docker.


Check out the foundationdb repo.


Run the docker image interactively Docker Run with the directory containing the foundationdb repo mounted Docker Mounts.
docker run -it -v '/local/dir/path/foundationdb:/docker/dir/path/foundationdb' foundationdb/foundationdb-build:latest


Navigate to the container's mounted directory which contains the foundationdb repo.
cd /docker/dir/path/foundationdb


Run make.


This will build the fdbserver binary and the python bindings. If you want to build our other bindings, you will need to install a runtime for the language whose binding you want to build. Each binding has an .mk file which provides specific targets for that binding.
CMake
FoundationDB is currently in the process of migrating the build system to cmake.
The CMake build system is currently used by several developers. However, most of
the testing and packaging infrastructure still uses the old VisualStudio+Make
based build system.
To build with CMake, generally the following is required (works on Linux and
Mac OS - for Windows see below):

Check out this repository.
Install cmake Version 3.12 or higher CMake
Download version 1.67 of Boost.
Unpack boost (you don't need to compile it)
Install Mono.
Install a JDK. FoundationDB currently builds with Java 8.
Create a build directory (you can have the build directory anywhere you
like): mkdir build
cd build
cmake -DBOOST_ROOT=<PATH_TO_BOOST> <PATH_TO_FOUNDATIONDB_DIRECTORY>
make

CMake will try to find its dependencies. However, for LibreSSL this can be often
problematic (especially if OpenSSL is installed as well). For that we recommend
passing the argument -DLibreSSL_ROOT to cmake. So, for example, if you
LibreSSL is installed under /usr/local/libressl-2.8.3, you should call cmake like
this:
cmake -DLibreSSL_ROOT=/usr/local/libressl-2.8.3/ ../foundationdb

FoundationDB will build just fine without LibreSSL, however, the resulting
binaries won't support TLS connections.
Language Bindings
The language bindings that are supported by cmake will have a corresponding
README.md file in the corresponding bindings/lang directory.
Generally, cmake will build all language bindings for which it can find all
necessary dependencies. After each successful cmake run, cmake will tell you
which language bindings it is going to build.
Generating compile_commands.json
CMake can build a compilation database for you. However, the default generated
one is not too useful as it operates on the generated files. When running make,
the build system will create another compile_commands.json file in the source
directory. This can than be used for tools like
CCLS,
CQuery, etc. This way you can get
code-completion and code navigation in flow. It is not yet perfect (it will show
a few errors) but we are constantly working on improving the development experience.
Using IDEs
CMake  has built in support for a number of popular IDEs. However, because flow
files are precompiled with the actor compiler, an IDE will not be very useful as
a user will only be presented with the generated code - which is not what she
wants to edit and get IDE features for.
The good news is, that it is possible to generate project files for editing
flow with a supported IDE. There is a cmake option called OPEN_FOR_IDE which
will generate a project which can be opened in an IDE for editing. You won't be
able to build this project, but you will be able to edit the files and get most
edit and navigation features your IDE supports.
For example, if you want to use XCode to make changes to FoundationDB you can
create a XCode-project with the following command:
cmake -G Xcode -DOPEN_FOR_IDE=ON <FDB_SOURCE_DIRECTORY>
You should create a second build-directory which you will use for building
(probably with make or ninja) and debugging.
Linux
There are no special requirements for Linux. However, we are currently working
on a Docker-based build as well.
If you want to create a package you have to tell cmake what platform it is for.
And then you can build by simply calling cpack. So for debian, call:
cmake -DINSTALL_LAYOUT=DEB  <FDB_SOURCE_DIR>
make
cpack

For RPM simply replace DEB with RPM.
MacOS
The build under MacOS will work the same way as on Linux. To get LibreSSL and boost you
can use Homebrew. LibreSSL will not be installed in
/usr/local instead it will stay in /usr/local/Cellar. So the cmake command
will look something like this:
cmake -DLibreSSL_ROOT=/usr/local/Cellar/libressl/2.8.3 <PATH_TO_FOUNDATIONDB_SOURCE>
To generate a installable package, you have to call CMake with the corresponding
arguments and then use cpack to generate the package:
cmake -DINSTALL_LAYOUT=OSX  <FDB_SOURCE_DIR>
make
cpack
Windows
Under Windows, the build instructions are very similar, with the main difference
that Visual Studio is used to compile.

Install Visual Studio 2017 (Community Edition is tested)
Install cmake Version 3.12 or higher CMake
Download version 1.67 of Boost.
Unpack boost (you don't need to compile it)
Install Mono.
Install a JDK. FoundationDB currently builds with Java 8.
Set JAVA_HOME to the unpacked location and JAVA_COMPILE to
$JAVA_HOME/bin/javac.
Install Python if it is not already installed by Visual Studio.
(Optional) Install WIX. Without it Visual Studio
won't build the Windows installer.
Create a build directory (you can have the build directory anywhere you
like): mkdir build
cd build
cmake -G ""Visual Studio 15 2017 Win64"" -DBOOST_ROOT=<PATH_TO_BOOST> <PATH_TO_FOUNDATIONDB_DIRECTORY>
This should succeed. In which case you can build using msbuild:
msbuild /p:Configuration=Release foundationdb.sln. You can also open the resulting
solution in Visual Studio and compile from there. However, be aware that
using Visual Studio for development is currently not supported as Visual
Studio will only know about the generated files. msbuild is located at
c:\Program Files (x86)\MSBuild\14.0\Bin\MSBuild.exe for Visual Studio 15.

If you want TLS support to be enabled under Windows you currently have to build
and install LibreSSL yourself as the newer LibreSSL versions are not provided
for download from the LibreSSL homepage. To build LibreSSL:

Download and unpack libressl (>= 2.8.2)
cd libressl-2.8.2
mkdir build
cd build
cmake -G ""Visual Studio 15 2017 Win64"" ..
Open the generated LibreSSL.sln in Visual Studio as administrator (this is
necessary for the install)
Build the INSTALL project in Release mode

This will install LibreSSL under C:\Program Files\LibreSSL. After that cmake
will automatically find it and build with TLS support.
If you installed WIX before running cmake you should find the
FDBInstaller.msi in your build directory under packaging/msi.
",9261
int128/kubelogin,Go,"kubelogin  
This is a kubectl plugin for Kubernetes OpenID Connect (OIDC) authentication, also known as kubectl oidc-login.
It gets a token from the OIDC provider and writes it to the kubeconfig.
Getting Started
You need to setup the following components:

OIDC provider
Kubernetes API server
Role for your group or user
kubectl authentication

You can install the latest release from Homebrew, Krew or GitHub Releases as follows:
# Homebrew
brew tap int128/kubelogin
brew install kubelogin

# Krew
kubectl krew install oidc-login

# GitHub Releases
curl -LO https://github.com/int128/kubelogin/releases/download/v1.11.0/kubelogin_linux_amd64.zip
unzip kubelogin_linux_amd64.zip
ln -s kubelogin kubectl-oidc_login
After initial setup or when the token has been expired, just run:
% kubelogin
Open http://localhost:8000 for authentication
You got a valid token until 2019-05-16 22:03:13 +0900 JST
Updated ~/.kubeconfig

or run as a kubectl plugin:
% kubectl oidc-login

It opens the browser and you can log in to the provider.
After authentication, it gets an ID token and refresh token and writes them to the kubeconfig.
For more, see the following documents:

Getting Started with Keycloak
Getting Started with Google Identity Platform
Team Operation

Configuration
This document is for the development version.
If you are looking for a specific version, see the release tags.
Kubelogin supports the following options.
Options:
      --kubeconfig string              Path to the kubeconfig file
      --context string                 The name of the kubeconfig context to use
      --user string                    The name of the kubeconfig user to use. Prior to --context
      --listen-port ints               Port to bind to the local server. If multiple ports are given, it will try the ports in order (default [8000,18000])
      --skip-open-browser              If true, it does not open the browser on authentication
      --certificate-authority string   Path to a cert file for the certificate authority
      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure
  -v, --v int                          If set to 1 or greater, it shows debug log

It supports the following keys of auth-provider in a kubeconfig.
See kubectl authentication for more.



Key
Direction
Value




idp-issuer-url
Read (Mandatory)
Issuer URL of the provider.


client-id
Read (Mandatory)
Client ID of the provider.


client-secret
Read (Mandatory)
Client Secret of the provider.


idp-certificate-authority
Read
CA certificate path of the provider.


idp-certificate-authority-data
Read
Base64 encoded CA certificate of the provider.


extra-scopes
Read
Scopes to request to the provider (comma separated).


id-token
Write
ID token got from the provider.


refresh-token
Write
Refresh token got from the provider.



Kubeconfig
You can set path to the kubeconfig file by the option or the environment variable just like kubectl.
It defaults to ~/.kube/config.
# by the option
kubelogin --kubeconfig /path/to/kubeconfig

# by the environment variable
KUBECONFIG=""/path/to/kubeconfig1:/path/to/kubeconfig2"" kubelogin
If you set multiple files, kubelogin will find the file which has the current authentication (i.e. user and auth-provider) and write a token to it.
Extra scopes
You can set extra scopes to request to the provider by extra-scopes in the kubeconfig.
kubectl config set-credentials keycloak --auth-provider-arg extra-scopes=email
Note that kubectl does not accept multiple scopes and you need to edit the kubeconfig as like:
kubectl config set-credentials keycloak --auth-provider-arg extra-scopes=SCOPES
sed -i '' -e s/SCOPES/email,profile/ $KUBECONFIG
Redirect URIs
By default kubelogin starts the local server at port 8000 or 18000.
You need to register the following redirect URIs to the OIDC provider:

http://localhost:8000
http://localhost:18000 (used if port 8000 is already in use)

You can change the ports by the option:
kubelogin --listen-port 12345 --listen-port 23456
CA Certificates
You can set your self-signed certificates for the OIDC provider (not Kubernetes API server) by kubeconfig or option.
kubectl config set-credentials keycloak \
  --auth-provider-arg idp-certificate-authority=$HOME/.kube/keycloak-ca.pem
HTTP Proxy
You can set the following environment variables if you are behind a proxy: HTTP_PROXY, HTTPS_PROXY and NO_PROXY.
See also net/http#ProxyFromEnvironment.
Contributions
This is an open source software licensed under Apache License 2.0.
Feel free to open issues and pull requests for improving code and documents.
",96
ctjb/ctjb.net,HTML,"CTJB.net
",2
comit-network/comit-rs,Rust,"COMIT-rs

COMIT is an open protocol facilitating trustless cross-blockchain applications.
This is a reference implementation for the COMIT protocol.
WARNING - We do not recommend running COMIT on mainnet for now!!!
Structure
The repository contains two main folders: vendor and application.
Vendor
Contains crates that provide general functionality that is not specific to the domain of atomic swaps.
Crates defined in here MUST NOT depend on crates in application.
They may be separated from the repository at some point (and possibly released on crates.io).
Application
Contains crates specific to our application. Can depend on libraries located in vendor.
Setup build environment

Install rustup: curl https://sh.rustup.rs -sSf | sh
Install libzmq:

Ubuntu/Debian: apt install libzmq3-dev
Mac (Homebrew) brew install zeromq


Install SSL libraries

Ubuntu/Debian: apt install libssl-dev
Mac (Homebrew) brew install openssl


solc is currently needed to build (will be deprecated). 2 choices:

Install docker
OR install solc



Build & Run

cargo build (do export SOLC_BIN=/usr/bin/solc if solc is installed locally)
Put a default.toml config file into ~/.config/comit_node or set COMIT_NODE_CONFIG_PATH as folder path to where the default.toml is located.
Put a default.toml config file into ~/.config/btsieve or set BTSIEVE_CONFIG_PATH as folder path to where the default.toml is located
startup bitcoin node (port to be set according to btsieve configuration)
startup ethereum node (port to be set according to btsieve configuration)
startup btsieve: cargo run --bin btsieve
startup comit_node: cargo run --bin comit_node

If the [web_gui] section is specified in the configuration the current release of the user interface comit-i will be served once the comit node started up (served at localhost:8080 as default).
In order to do a swap you will have to start two comit nodes.
Setup testing/dev environment

Install docker & docker-compose
Install node (check the version required in package.json) & yarn
Install cargo-make: cargo install cargo-make
Run cargo make in the root folder of the repository, this will install various crates & tools such as rustfmt & clippy

Testing

cargo make runs the whole test suite including integration tests but not end-to-end.
cargo make all also runs the whole test suite, including end-to-end tests.
cargo make format to format Rust code
cargo make js-format to format JavaScript code
cargo make btsieve to run btsieve tests
cargo make dry to run COMIT node dry tests
cargo make api to run all API tests
cargo make e2e to run COMIT node end-to-end tests
cargo make e2e *btc* to run COMIT node end-to-end tests with btc in the folder name (supports shell glob)

Contributing
Contributions are welcome, please visit CONTRIBUTING for more details.
License
This project is licensed under the terms of the GNU GENERAL PUBLIC LICENSE v3.
",94
JamesSingleton/React-Dashboard,JavaScript,"React Dashboard

Short blurb about what your product does.

Table of Contents

Sending Feedback
Available Scripts

npm run dev


GraphQL

Sending Feedback
We are always open to your feedback.
Available Scripts
In the project directory, you can run:
npm run dev
Runs the app in the development mode.
Open http://localhost:4000 to view it in the browser.
The page will reload if you make edits.
You will also see any lint errors in the console.
GraphQL
Open http://localhost:4000/graphql to view it in the browser.
",6
aog0036/TFG-SmartBeds,Jupyter Notebook,"TFG-SmartBeds
TFG sobre detección de crisis epilépticas mediante camas inteligentes. Universidad de Burgos.
",3
facebook/openr,C++,"OpenR: Open Routing

Open Routing, OpenR, is Facebook's internally designed and developed routing
protocol/platform. Originally built for performing routing on the Terragraph
network, its awesome design and flexibility have led to its adoption in
other networks at Facebook including our new WAN network, Express Backbone.
Documentation

Please refer to openr/docs/Overview.md to get
started with OpenR.
Library Examples

Please refer to the examples directory to see some useful ways to
leverage the openr and fbzmq libraries to build software to run with OpenR.
Resources


Developer Group: https://www.facebook.com/groups/openr/
Github: https://github.com/facebook/openr/

Contribute

Take a look at openr/docs/DeveloperGuide.md
and CONTRIBUTING.md to get started contributing.
The Developer Guide outlines best practices for code contribution and testing.
Any single change should be well tested for regressions and version
compatibility.
Code of Conduct

The code of conduct is described in CODE_OF_CONDUCT.md
Requirements

We have tried OpenR on Ubuntu-14.04, Ubuntu-16.04 and CentOS-7.
This should work on all Linux based platforms without any issues.

Compiler supporting C++14 or higher
libzmq-4.0.6 or greater

Build

Repo Directory Structure
At the top level of this repo are the build and openr directories. Under the
former is a tool, fbcode_builder, that contains scripts for generating a
docker context to build the project. The openr directory contains the
source for the project.
Dependencies
If docker is not a good option for you, you can install these dependencies for
your system and follow the traditional cmake build steps below.

cmake
gflags
gtest
libsodium
libzmq
zstd
folly
fbthrift
fbzmq
libnl

One Step Build - Ubuntu-16.04
We've provided a script, build/build_openr.sh, well tested on
Ubuntu-16.04, to install all necessary dependencies, compile OpenR and install
C++ binaries as well as python tools. Please modify the script as needed for
your platform. Also, note that some library dependencies require a newer version
than provided by the default package manager on the system and hence we are
compiling them from source instead of installing via the package manager. Please
see the script for those instances and the required versions.

libnl also requires custom patch, included herewith, for correct handling
of add/remove multicast routes.

Build using Docker
Learn more here.
Build Steps
// Step into `build` directory
cd build

// Install dependencies and openr
sudo bash ./build_openr.sh

// Run tests (some tests requires sudo privileges)
sudo make test

If you make any changes you can run cmake ../openr and make from the build
directory to build openr with your changes.
Installing
openr builds both static and dynamic libraries and the install step installs
libraries and all header files to /usr/local/lib/ and /usr/local/include/
(under openr subdirectory) along with python modules in site-packages.
Note: the build_openr.sh script will run this step for you.
sudo make install

Installing Python Libraries
You will need python setup-tools to build and install python modules. All
library dependencies will be automatically installed except the
fbthrift-python module which you will need to install manually using steps
similar to those described below. This will install breeze, a cli tool to
interact with OpenR.
cd openr/openr/py
python setup.py build
sudo python setup.py install

License

OpenR is MIT licensed.
",648
microservices-security-in-action/samples,None,"Microservices Security In Action
By Prabath Siriwardena and Nuwan Dias
Part 1 Overview
Chapter 1: Welcome to microservices security

How security works in a monolithic application
Challenges of securing microservices
Key security funamentals
Edge security
Securing service-to-service communication
Security in DevOps
Security code development lifecycle (SCDL)
Summary

Chapter 2: Hello World microservices security

Your first microservice
Setting up an OAuth 2.0 server
Securing a microservice with OAuth 2.0
Invoking a secured microservice with a client application
Authorization of requests based on OAuth 2.0 scopes
Summary

Part 2 Edge Security
Chapter 3: Deploying a microservice behind an API gateway

The need for an API gateway in a microservices architecture?
Security at the edge
How to pick the correct OAuth 2.0 grant type?
Setting up an API gateway with Zuul
Deploying a microservice behind Zuul
Securing communication between Zuul and the microservice
Summary

Chapter 4: Building a single-page application to talk to microservices

Building a single-page app
Introducing an API gateway, and setting up cross-origin resource sharing (CORS)
Securing a SPA with OpenID Connect
Federated authentication
Summary

Chapter 5: Engaging throttling, monitoring and access control

Engaging throttling at the API gateway with Zuul
Monitoring & analytics with Prometheus and Grafana
Enforce access control policies at the API gateway with Zuul and Open Policy Agent (OPA)
Summary

Part 3 Service to Service Communication
Chapter 6: Securing service-to-service communication with certificates

Why use mTLS?
Creating certificates
Securing microservices with TLS
Engaging mTLS
Challenges in key management
Key rotation
Monitoring key use
SPIFFE
Summary

Chapter 7: Securing service-to-service communication with JWT

What is a JSON Web Token (JWT)?
What does a JWT look like?
JSON Web Signature (JWS)
JSON Web Encryption (JWE)
Use cases for securing microservices with JWT
Setting up an STS to issue JWTs
Securing microservices with JWT
Using JWT as a data source to do access control
Securing service-to-service communication with JWT
Exchanging a JWT for a new one with a new audience
Summary

Chapter 8: Securing service-to-service communication happens over gRPC

Understanding gRPC
Service-to-service communications over gRPC
Securing gRPC service-to-service communications with mTLS
Securing gRPC service-to-service communications with JWT
Summary

Chapter 9: Securing Event-driven microservices

Why event-driven microservices?
Setting up Kafka as a message broker in a microservices deployment
Developing a microservice to push events to a topic in Kafka
Developing a microservice to read events from a Kafka topic
Using Transport Layer Security (TLS) to protect data in transit
Using mutual Transport Layer Security (mTLS) for authentication
Controlling access to Kafka topics with ACLs
Controlling access to Kafka topics with OPA
Summary

Part 4 Secure Deployment
Chapter 10: Conquering container security with Docker

Docker security principles
Deploying a microservice on Docker
Securing the host
Running Docker Bench for security
Running Docker in Swarm mode
Challenges in container security
Summary

Chapter 11: Securing microservices on Kubernetes

Setting up a Docker cluster with Kubernetes
Kubernetes built in security features
Setting up Kubernetes security policies
Using Kubernetes network policies
Securing applications with Calico
Implementing security as a sidecar
Summary

Chapter 12: Securing microservices with Istio service mesh

Setting up Istio on Kubernetes
Istio authentication architecture
Securing service-to-service communication with mTLS
Securing service-to-service communication with JWT
Istio authorization architecture
Enabling authorization
Summary

Part 5 Secure Development
Chapter 13: Secure coding practices and automation

OWASP top 10 most critical web application security risks
Static code analysis vs. dynamic analysis
Running static code analysis
Running dependency checks
Running dynamic analysis with OWASP ZAP
Integrating security testing with Jenkins
Summary

",10
mIcHyAmRaNe/okadminfinder3,Python,"





OKadminFinder: Easy way to find admin panel of site
OKadminFinder is an Apache2 Licensed utility, rewritten in Python 3.x, for admins/pentesters who want to find admin panel of a website. There are many other tools but not as effective and secure. Yeah, Okadminfinder has the the ability to use tor and hide your identity


Requirements







Linux
sudo apt install tor
sudo apt install python3-socks  (optional)
pip3 install --user -r requirements.txt



Windows
download tor expert bundle
pip3 install -r requirements.txt




Usage


Preview



Linux
git clone https://github.com/mIcHyAmRaNe/okadminfinder3.git
cd okadminfinder3
chmod +x okadminfinder.py
python3 okadminfinder.py



Windows
download & extract zip
cd okadminfinder3
py -3 okadminfinder.py



Pentestbox (same procedure as Linux)
you can add an alias by adding this line: okadminfinder=py -3 ""%pentestbox_ROOT%/bin/Path/to/okadminfinder3/okadminfinder.py"" $* to C://Pentestbox/bin/customtools/customaliases file and so you'll be able to launch it using      okadminfinder




Features

 More than 500 potential admin panels
 Tor & Proxy
 Random-Proxy
 Random-Agents
 Console work with params, like: okadminfinder.py -u example.com --proxy 127.0.0.1:8080
 Self-Update
 Classify admin panel links by popularity
 Multithreading, for faster work
 Adding more potential admin panel pages

Youtube videos

okadminfinder : admin page finder
okadminfinder3 : admin page finder (update)
admin panel finder Kali Linux 2018.3

Most Blogs that shared okadminfinder

kitploit.com
securityonline.info
prodefence.org
kalilinuxtutorials.com

",56
mIcHyAmRaNe/okadminfinder3,Python,"





OKadminFinder: Easy way to find admin panel of site
OKadminFinder is an Apache2 Licensed utility, rewritten in Python 3.x, for admins/pentesters who want to find admin panel of a website. There are many other tools but not as effective and secure. Yeah, Okadminfinder has the the ability to use tor and hide your identity


Requirements







Linux
sudo apt install tor
sudo apt install python3-socks  (optional)
pip3 install --user -r requirements.txt



Windows
download tor expert bundle
pip3 install -r requirements.txt




Usage


Preview



Linux
git clone https://github.com/mIcHyAmRaNe/okadminfinder3.git
cd okadminfinder3
chmod +x okadminfinder.py
python3 okadminfinder.py



Windows
download & extract zip
cd okadminfinder3
py -3 okadminfinder.py



Pentestbox (same procedure as Linux)
you can add an alias by adding this line: okadminfinder=py -3 ""%pentestbox_ROOT%/bin/Path/to/okadminfinder3/okadminfinder.py"" $* to C://Pentestbox/bin/customtools/customaliases file and so you'll be able to launch it using      okadminfinder




Features

 More than 500 potential admin panels
 Tor & Proxy
 Random-Proxy
 Random-Agents
 Console work with params, like: okadminfinder.py -u example.com --proxy 127.0.0.1:8080
 Self-Update
 Classify admin panel links by popularity
 Multithreading, for faster work
 Adding more potential admin panel pages

Youtube videos

okadminfinder : admin page finder
okadminfinder3 : admin page finder (update)
admin panel finder Kali Linux 2018.3

Most Blogs that shared okadminfinder

kitploit.com
securityonline.info
prodefence.org
kalilinuxtutorials.com

",56
Lombiq/Hastlayer-SDK,C#,"Hastlayer SDK Readme
Overview
Hastlayer - be the hardware. Automatically transforming .NET assemblies into computer chips to improve performance and lower power consumption.
Hastlayer uses FPGAs (chips that can be ""re-wired"" on the fly): You just need to select the compute-bound part of your .NET program and Hastlayer will seamlessly swap it out with a generated FPGA implementation. Since not C#, VisualBasic or other code but .NET Intermediate Language assemblies are transformed in theory any .NET language can be used, including C#, VB, F#, C++, Python, PHP, JavaScript...
Hastlayer was also featured on .NET Conf 2017; the recorded session covers most of what's interesting about Hastlayer (it's also on YouTube). Check out the FAQ for some more basic info.
This is the PC-side component of Hastlayer, the one that transforms .NET assemblies, programs attached FPGAs and communicates with said FPGAs.
Created by Lombiq Technologies, an open source .NET web development company.
Hastlayer uses ILSpy to process CIL assemblies and Orchard Application Host to utilize Orchard as the application framework.
Notes on Hastlayer's documentation
These text files should only serve as a starting point. On how to use Hastlayer the samples are the best source. The public API of Hastlayer is also documented inline as code comments, so make sure to check those out too if something's not clear. The projects also have further Readme files.
Table of contents

Getting started
Working with Hastlayer
Developing Hastlayer
Release notes
Roadmap
Support

Repositories and contributions
The project's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/hastlayer-sdk (Mercurial repository)
https://github.com/Lombiq/Hastlayer-SDK (Git repository)

(Note that due to a repository purge the repo history doesn't contain anything from before July 2017 though development has been ongoing more or less actively from 2015.)
Bug reports, feature requests and comments are warmly welcome, please do so via GitHub. Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",85
lightning-power-users/node-launcher,Python,"User Guides

Please start here!
Send a payment with the Joule Chrome extension

Requirements

~300 GB of download bandwidth
~10 GB of disk space (~300 GB if you want the Bitcoin transaction index, makes for a faster LND)
Windows 7+ or macOS 10.12.6+

Linux works but it is not packaged, follow the developer steps below to run it from the Linux command line.
Please submit a pull request if you want to add Linux packaging! A .deb and .rpm would help grow the Lightning network...
Install
Download and open the latest release for your operating system:
https://github.com/PierreRochard/node-launcher/releases
Node Launcher

Creates a node launcher data directory

macOS: ~/Library/Application Support/Node Launcher/
Windows: %localappdata%/Node\ Launcher/


Finds available ports for Bitcoin and LND, testnet and mainnet
When launched, Bitcoin nodes use the datadir directory specified in bitcoin.conf (or the default data directory)
If you don't have >300 GB of disk space free, Bitcoin nodes will fall back to pruned
Pruning still requires downloading data, so make sure you can handle downloading ~300 GB of data



Development
Review the contributing.md file https://github.com/PierreRochard/node-launcher/blob/master/contributing.md
Install Python3.7+
macOS: brew install python3
Linux: sudo apt install python3.7-venv
For Windows see https://medium.com/@pierre_rochard/node-launcher-developer-setup-on-windows-5ba6e0fbb38a
For macOS and Linux:

git clone https://github.com/PierreRochard/node-launcher
cd node-launcher
python3.7 -m venv venv
. venv/bin/activate
pip3.7 install -r requirements.txt
python setup.py develop
python run.py

Testing
pytest tests
To include tests with network calls to GitHub:
pytest tests --run_slow
Packaging
macOS: pyinstaller run-mac.spec
Windows: pyinstaller run-windows.spec (pyinstaller packaging only works on Windows 7)
Generate LND Bindings
https://github.com/lightningnetwork/lnd/blob/master/docs/grpc/python.md
",231
facebook/watchman,C++,"Watchman
A file watching service.
Purpose
Watchman exists to watch files and record when they actually change.  It can
also trigger actions (such as rebuilding assets) when matching files change.
Documentation
Head on over to https://facebook.github.io/watchman/
Build Status




License
Watchman is made available under the terms of the Apache License 2.0.  See the
LICENSE file that accompanies this distribution for the full text of the
license.
Contributing
Please see the contributing guide.
",7958
satya-das/cib,C++,"Component Interface Binder (CIB)




CIB
In a nutshell CIB is the answer to the problem for which C is used for exporting APIs of an application/library even when the application/library itself uses C++ for most of its implementation. C++ is great in expressing API but compiler generated ABI makes it difficult to use to publish compiler independent and ABI stable SDK.
Jargon

ABI Compatibility: Compatibility of binary C++ components even when they are built with different compilers.
API Stability: Ability to compile client of a library with newer SDK headers without any change.
ABI Stability: Ability of binary component to work with newer version of another component without recompiling. Example of ABI stability is the ability of a plugin (in binary form) of an application to seemlessly work with newer application.
Forward Compatibility: It is specific ABI stability where older library can work with newer client.
Backward Compatibility: It is specific ABI stability where newer library can work with older client.

Note: In this document when ABI Stability is mentioned we will mean both forward and backward compatibility.
Overview
CIB is an architecture to publish compiler independent and ABI stable C++ library.
This project is also about a tool that implements cib architecture automatically for given library headers.
CIB can also be used as plugin architecture of application.
What is ABI
We programmers do have some understanding about ABI. The wikipedia also has a page for ABI. But I want to add another perspective about what exactly is ABI. An ABI of a component is implementation details of features of the language in which the component is developed. And this implementation detail depends on the compiler. For example, for a C++ component, we see mangled function names at binary level. The reason for that is C++ allows function overloading and compilers use name mangling to implement function overloading language feature of C++. In the same way all the implementation detail of other features of C++, like inheritance, encapsulation, runtime polymorphism, etc. end up being the ABI itself.
CIB Features

ABI Compatibility aka Compiler Independence: Library and clients can use their own choice of compilers.
ABI Stability: Both new library with old client and old library with new client should be binary compatible.
ABI Resilience: Virtual functions can be reordered in SDK without breaking ABI stability. With CIB there are other cases of ABI resilience too.
Perfect Isolation: Clients can use library provided classes without access to original complete definition of library classes.

CIB  allows client of a library to use all exported classes, as if those classes are part of the client code itself, without exposing the internals of classes.
CIB Goals

Platform and compiler agnostic.
Minimum footprint. Should not affect how developers write their program.

Other Solutions
I have come across some solutions that try to solve the same problem but none of them is good enough. Some wants you to write separate layer on top of existing classes so that vtable is exported across component boundary in a portable manner or some exploits how compiler behaves and uses hacks to achieve goals or some is too specific to the project it was developed for.

CppComponent: It basically uses hand written vtable to solve ABI problem. It looks like a clone of COM without idl. More details can be found here: https://github.com/jbandela/cppcomponents.
DynObj: It exploits how compiler implements vtable. For details here: http://dynobj.sourceforge.net.
Libcef's translator: Its a python script that parses C++ headers to produce automatic C layer for client and library. But it is too much specific to libcef and cannot be used in other project.

And none of these solutions I am aware of are for ABI stability, they only target ABI compatibility for different compilers. This is my understanding, of course I can be wrong.
Why C++ has ABI stability issues
Actually even C has this problem, it's just another matter that it is relatively easier to achieve ABI compatibility and stability in C.
Things that can cause ABI compatibility and stability issues in C++ are:

Object layout.
Function calling convention.
Allocators and deallocators.
Underlying integer size of enum.
Size of various integer types.
Mangled function name.
Virtual function table.
RTTI.
Exceptions.
Inheritance.

First 5 can be problems in C as well. But techniques are well known and used to cicumvent them to achieve ABI stability in C.
For C++, problems start with name mangling, that's the first reason of misunderstanding that can happen between 2 components. Layout for C++ objects are far more complex than C. There can be different layout for virtual tables depending upon compiler. Same goes for RTTI and exception. So, ensuring ABI compatibility is hard in C++. Ensuring ABI stability is super hard in general.
One thing to note is that maintaining ABI compatibility and stability in C is largely a responsibility of library developers. In C++, CIB can make ABI compatibility and stability achievable, but developers will have to be as reponsible as they need to be when pure C is used.
ABI Resilience
Some changes are conceptually unimportant for clients of a library but they break binary compatiblity. CIB makes client resilient against such changes and so client and library remain binary compatible even when such changes are made. Below is a list of changes that don't affect compatibility of client and library if SDK is published using CIB:

Any change in internal data member of a class.
Addition of new virtual methods anywhere in the class.
Change in order of virtual functions of a class.
Change in inheritance that doesn't violate previous is-a relationship. For example:

if a class starts deriving from one more base class without removing previous base class.
if a class changes it's base class to another derived class of it's previous base class.
inheritance type is changed to/from virtual inheritance.


Change in inlineness of a function. For CIB generated SDKs all inline functions are basically non-inlined and so it doesn't make any difference if inlineness of a function is changed.

CIB Architecture
Core architecture concept
As we know language feature implementations end up being ABI, see What is ABI, CIB avoids direct sharing of language feature implementation with another component. It provides very basic ABI that is shared between components and gurenteed to be compatible and easy to maintain for stability. CIB still allows working of language features across component but without letting the sharing of implementation details. Graphically it can be depicted as:

The CIB layers of each components act like proxy to another component and so each component ""feels"" like it is interacting directly with another component. The language feature implementation detail is absorved within CIB layers and doesn't cross component boundary. So, each component can be compiled using different compilers and still they will work together.
CIB ABI that is implemented in CIB layer is especially designed to ensure ABI stability.
CIB Architecture Elements
Following are the broad elements of CIB architecture:

CIB architecture needs two sets of files that are created based on public headers that library wants to publish.
One set of files, that is called library side glue code, should be compiled with the library.
The other set should be used by the client of the library. This is client side glue code.
Library side glue code defines free C style functions for all functions including class methods, constructors, and destructors.
Implementation of such C functions are just to delegate the call to original function/method/constructor/destructor/etc.
All C functions are assigned an index value. The index once assigned remains same across releases.
For every class/struct/union/namespace a MethodTable is defined which is basically an array of function pointers.
MethodTable is populated with free functions mentioned above.
Functions in MethodTable are placed at the same position as thier indices.
Library side glue code exports a C function that returns MethodTable for given class/struct/union/namespace ID.
Class definitions for client is generated with same class-name but without any data member other than an opaque pointer to original class defined by library. In CIB terminology classes that are seen by client are called proxy-classes and the opaque pointer held by proxy-class is called handle. This is basically pimpl pattern (aka bridge pattern) with pimpl pointing to object across component boundary. Note: There is another kind of proxy class too, see Layout Sharing Proxy Class.
Function index is used to fetch function pointer from MethodTable.
Implementation of all functions including methods, constructors, and destructors of proxy classes are provided by means of invoking function pointer from MethodTable.

Above is only broad description of CIB architecture. For understanding detail of CIB architecture please have a look at Examples. Each example tries to explain one peice of CIB architecture. Since this project is also about developing a tool that will automatically implement CIB architecture for a library the examples mentioned shows the code generated by CIB. Please be forewarned that little paitence will be required to analyse such code. :)
Library Side Glue Code
The code that forms library side layer is called library side glue code. This layer contains the code to represent class as set of free functions. These free functions for a class are bundled together in an array. This array of pointers-to-free-functions is called MethodTable. To avoid name mangling done by compilers that ruin the ABI compatibility, the library needs to export the MethodTable instead.
Client Side Glue Code
The code that forms client side layer is called client side glue code. This layer contains the code to reconstruct the class back from the MethodTable exported by library.
The rest of the details of CIB architecture can be understood with examples.
Examples aka CIB Architecture Detail
Details of CIB architecture is explained with various examples. Please see Examples to know the details.
Building CIB
Get the source
git clone https://github.com/satya-das/common.git
git clone https://github.com/satya-das/cppparser.git
git clone https://github.com/satya-das/cib.git
Configure and build
cd cib
mkdir builds
cd builds
cmake ..
make && make test
Alternatively, if you prefer Ninja instead of make:
cd cib
mkdir builds
cd builds
cmake -G Ninja ..
ninja && ninja test
Feature Progress of CIB tool



Feature
Description
Status




Basic
CIB should work for a simple library that exports classes with non-virtual functions.
Done


Virtual functions and inheritance
CIB should make virtual function and inheritance available to clients.
Done


Function overloading
Same named functions of a class should be seamlessly exported.
Done


Function overridding
Allow library to use interface implemented by client.
Done


Forward compatibility of client
Older client works with newer library.
Done


Forward compatibility of library
Library should be ABI compatible with newer client. As of now library and client can catch exception when non existing function is called and should take corrective measures to work with newer component.
Done


Backward compatibility of client
When newer client invokes a method present only in new SDK then std::bad_function_call exception will be thrown. Clients that want to be backward compatible should handle this exception when invoking methods present only in newer SDK.
Done


Backward compatibility of library
When library invokes a method of interface implemented by only newer client then std::bad_function_call exception will be thrown. Library developer should be aware about this to remain backward compatible when invoking new methods of it's own public interface.
Done


Create correct proxy class
A base class pointer returned by an API of library may actually be pointing to an object of a derived class. At client side we should create proxy class of exact same type to which the returned pointer is pointing to. It is needed so that dynamic_cast at client side should work as expected.
Done


Operator overloading
It is common for C++ classes to have overloaded operators.
Done


Typecast operator overloading
Typecast operator overloading of user defined types is common in C++.
Done


Simple Template class support
Support for simple template classes when it crosses component boundary. Simple template means without template template argument, or any of other fancy stuff.
Done


Return existing proxy class
If a function returns pointer or reference of object for which proxy class already exists then existing proxy class should be returned.
Done


Support protected methods
Protected methods are accessible to derived class and they should be exported so that client's derived class can call them.
Done


Support private pure virtual
Private pure virtual is used in template method design pattern.
Done


Rvalue reference parameter
RValue references need to cross component boundary.
Done


Support of free functions
Free functions in global and orher namespaces too should be supported.
Done


Goal: Use CIB in real production SDK
To demonstrably prove viability of this architecture and tool it will be better to use it for at least one large production quality software. My plan is to use CIB for ObjectARX SDK of AutoCAD to demonstrate it's viability.
IN PROGRESS


Phase1: For AutoCAD SDK subset
Make CIB work for ObjectARX SDK even when it requires changing of SDK headers to avoid problems of SDK and/or the tool.
DONE


Phase2: For complete AutoCAD SDK
Improve tool so that only minimal change in ObjectARX SDK is required and that too only to avoid problems of SDK and not of tool.
IN PROGRESS


Support deleted members
Constructors and operators marked as delete should be deleted for proxy classes as well.



Enum and enum classes
Enums used as parameter or return type.



STL classes
It is common for a C++ programs to use stl classes. CIB should make it possible to export STL classes in the same way it does for every other classes.



Support shared_ptr and unique_ptr
Modern C++ programing expects these to be used more often.



Exception support
Make exception object travel across component boundary in a compatible way.



Function pointer as parameter
Function pointers can be used as parameter or return type of another function.



Support std::function
std::function can be used as function parameter or return type. They too should be supported.



Support for intrusive pointer
Many libraries use intrusive pointer to manage object life cyle and functions can return smart pointer for intrusively managed reference count of object.



Non-const pointer ref return type
When a reference of pointer of non-POD is returned from a function a change in that should be propagated to the library.



Support public data members
Public data members of a class should be exported in ABI stable way.





",4
vim-jp/syntax-vim-ex,Vim script,"syntax-vim-ex
An excellent Vim's syntax highlighting file for Vim script.
Difference between original

The syntax-vim-ex has been generated by parse the C source code of Vim.  So
keyword extraction is accurate.
Omitted keyword extraction algorithm of Ex command is perfect because it is
same and the C source code of Vim.
It will immediately follow the Vim updates.
Generator source code is all open sourced.  (See generator branch)

Requirement
Vim 7.4.1486 or later
How to Install
$ cd ~/.vim
$ mkdir -p pack/my/opt
$ cd pack/my/opt
$ git clone --single-branch --depth 1 https://github.com/vim-jp/syntax-vim-ex.git
Add following line to your .vimrc.
packadd! syntax-vim-ex
Remarks:

In the case of Windows should read the ~/.vim to
%HOMEDRIVE%%HOMEPATH%\vimfiles.  And mkdir -p option is not required.
Directory name my can be changed to any name.
If you want to configure the package under ~/.vim directory as in this
example the setting of option 'packpath' is not required.
If you are calling :packloadall explicitly in your .vimrc, packadd
should be added before it.

How to Update
$ cd ~/.vim/pack/my/opt/syntax-vim-ex
$ git pull
Licence
Follow the Vim license
Authors

Hirohito Higashi (h_east)
vim-jp
Charles E. Campbell (author of original syntax/vim.vim)

",37
huoji555/huojiDemo,None,"huojiDemo
第一个库
以后会在这个库里更新一些自己的想法
2018.3.13
在搭新的框架
想着要在底层加一个神经网络
感觉心里有点空落落的
明明学很久了
但还感觉自己很菜
晚上有空的话继续刷算法题
毕竟4.1还有场比赛等着我呢
2018.7.1
最近数据库这方面看了很多，调整了许多结构，优化了许多代码，项目现在比较不满意的地方就是那个验证码(很小的东西)
其次想集成Redis,数据库这块，将来可能会换成JPA,或者是MyBatis
底层加一个自动学习算法(首先造别的轮子，不然不会考虑的)
关于工作，也许是web工程师,也许是游戏端，还想要算法工程师，都想试一试
关于考研，在职更适合我一点
八月的时候再来记录
2018.8.1
已经入职了，新的一个月就是把公司框架流程走一次(包括代码风格，规范的书写)
深入了解JPA,之前用的不多，现在有机会好好了解了
前端用的AngularJs,也在学习中，感觉它的数据双向绑定和模块化哪个都很棒
还有框架里的Redis，要把它吃透
关于思维导图，还是在拿他画着自己学习过的知识
这是一点，其二了解NPL等相关路径算法，以及谱聚类，分类等算法(能把知识图谱每个点都整理一下我觉得很有趣)
学会AngularJs后,周末时间试着重构前端代码(感觉不频繁操作DOMd的话，应该会比之前好不少，而且更加清晰)
思维导图Redis配置及应用(要数据对比)
九月再来记录
2018.9.1
以往这个时间，都已经开学了
夏天过去了，秋天还没到
没想到9.1就来公司加班
可能9月要学更多的东西吧
9月会细细的看数据结构与算法
搞清楚之前页面加载为什么这么慢的问题(可能和load有一定的关系)
重装服务器，换系统（linux）
看程序员的数学（我觉得卷积很重要）
JPA虽然写东西没什么问题了，需要更深入的了解下(知识图谱见)
AngularJS也需要深入
想找一个好的demo看下
尽可能的把java23种设计模式都过一遍
2018.10.8
国庆假期已过
毛线头改造再往后推吧
现在要拼一把试试考研
这个事从中秋开始折磨我
到下定决心去做
好像也就那样了
因为不想止步不前
思维导图搜索引擎（CNN,NLP）
高级点的功能
还是离不开算法呀
难点在于语义的识别和挖掘
日志这块，可以发布一个查看日志的功能（后台功能）
此外，登录信息可放到此处做
还可以单独增加一个日志异常记录，以表格的形式显示出来
在@AfterThrowing里记录异常，保存到相应的实体类
2018.11.1
心里来回纠结了一个月，也慢慢习惯了现在的生活，其实也不算很糟糕，毕竟它能学点东西嘛，另一个是抬高天花板
上个月项目上倒没什么事，新框架也搭建完毕，后台管理模块也在继续往上加(是一个通用的以后可以作为)
关于HandLP,刚找到时欣喜的不得了，并且效果也不算很差，但是不适用于现在的场景进行分析
这个月继续准备KY,下周回学校现场确认，平时有时间就整整自己的系统
一步一步踏实走下去吧，想要的都会有的
2018.12.1
再有两天就会把知识点都过完了，思维导图之前也抽空加了很多东西，已经确定毕设要做什么了，KY是一定要考的，当
我为这个发愁时，同学已经直博了，怎么像黄金时代里说的一样，日子是越来越像受锤了,不过我打算努力过下去，挨锤
的同时，再坚持梦想就很不错了，KY之后去大连一段时间，回来和头儿约个饭，的确很久不见了
技术我会先放一放，KY为大，努力读个硕士博士吧，总是感觉本科能接触的东西太片面，太局限了
2019.1.2
是个新开始了，过去一年经历了很多，今年想要做的事是面到大公司，学到更加深，更全面的技术，最近一个月做自己的
毕设，改文档，之后就是学自己想学的东西了，已经放松过了，要朝着想去的地方前进了，现在市场很不景气，可还是很
喜欢Coding,新的一年，加油
2019.2.1
这一个月是沉寂期，在低谷的时候，看了很多基础相关的东西，基础还是薄弱，继续看吧，最近看了，明朝那些事，王垠的博客
阮一峰的weekly，我觉得都是很好的东西，只是应该在更早的时候看到，此时亦不晚，向前走吧，每个决定都是对的
2019.3.1
新公司，最近在写登陆系统，显然是没什么东西，打算最近再重学一次数据结构，LeetCode也要刷题，前段时间写的Tomcat和23种设计模式又
忘记了，下午就复习，平时每天半个小时的读书时间，早上的跑步时间，再学一会儿PostMan,这个月搞定数据结构思维，并做好笔记，希望越
来越好，厚积薄发
2019.4.1
上个月写了几个小工具，Redis消息队列，sql存储过程，大致就是些，数据结构思维看到了LinkedList,还要继续看，已经有了新的打算，在前进的
路上 这个月可能会出差几天 但望每日有所得 这个月搞定毕业论文等相关事宜，然后就是啃数据结构 记录另外一则显而易见却不易执行的道理 你要做
一件事，就一定去集中自己所有的资源去做这件事，这才有可能会成功，否则，生活将是杂乱无章，高不成，低不就，做完一件再去做另一件
2019.5.1
数据结构思维看到了剩最后几章，四月主要就是这些没用的存储过程，加上些不重要的业务，但是这段时间想了很多关于以后的事，前些日子跟女朋友也分开了，
休整几天，总体来说，四月是不怎样的一个月，心里想的杂事太多了，分手之后也能更冷静的分析现在的状况，也该知道这段时间想要做的事事什么了，大概就是
这样了，像人间失格里所讲的那样一切都会过去的，大概失去更能让人明白些什么，喜欢的太沉重了，所以以后继续向前走吧，继续努力，把确定的放在手上，
不确定的交给未来
",3
objeck/objeck-lang,C++,"Objeck
Objeck is an object-oriented computer language with functional features. The language has ties with Java, Scheme and UML. In this language all data types, except for higher-order functions, are treated as objects.

Objeck is a general-purpose programming language with an emphasis placed on simplicity. The programming environment consists of a compiler, virtual machine and command line debugger.
class Hello {
  function : Main(args : String[]) ~ Nil {
    ""Hello World"" → PrintLine();
    ""Καλημέρα κόσμε"" → PrintLine();
    ""こんにちは 世界"" → PrintLine();
  }
}
See more on Rosetta Code and checkout the following programming tasks.
Notable features:

Object-oriented and functional

Classes, interfaces and higher-order functions
Anonymous classes
Reflection
Object serialization
Type inference


Native support for threads, sockets, files, date/time, etc.
Libraries

Collections (vectors, queues, trees, hashes, etc.)
HTTP and HTTPS clients
RegEx
JSON and XML parsers
Encryption
Database access
Data structure querying
2D Gaming


Garbage collection
JIT support (IA-32 and AMD64)

Documentation
Please refer to the programmer's guide documentation and online tutorial. Also checkout Rosetta Code examples.
Deployment
Build and deployment instructions for Windows, Linux and OS X.
Binaries
Get the latest binaries.
",16
BumblebeeBat/dump,Erlang,"Dump
This is a very simple harddrive database. It only stores one size of block. here is an example of using it.
1> ID = database003,
2> dump_sup:start_link(ID, 5, 10000, ram, ""temp"").
{ok,<0.37.0>}
3> V = <<3,2,1,5,6>>.
<<3,2,1,5,6>>
4> Location = dump:put(V, ID).
15992
5> V = dump:get(Location, ID).
<<3,2,1,5,6>>
6> dump:delete(Location, ID).
ok
7> dump:get(Location, ID).
<<3,2,1,5,6>>
8> Location = dump:put(<<0:40>>, ID).
15992
9> dump:get(Location, ID).
<<0,0,0,0,0>>

it uses the atom database003 so that you can start more than one dump, and give them different names.
",2
satya-das/cib,C++,"Component Interface Binder (CIB)




CIB
In a nutshell CIB is the answer to the problem for which C is used for exporting APIs of an application/library even when the application/library itself uses C++ for most of its implementation. C++ is great in expressing API but compiler generated ABI makes it difficult to use to publish compiler independent and ABI stable SDK.
Jargon

ABI Compatibility: Compatibility of binary C++ components even when they are built with different compilers.
API Stability: Ability to compile client of a library with newer SDK headers without any change.
ABI Stability: Ability of binary component to work with newer version of another component without recompiling. Example of ABI stability is the ability of a plugin (in binary form) of an application to seemlessly work with newer application.
Forward Compatibility: It is specific ABI stability where older library can work with newer client.
Backward Compatibility: It is specific ABI stability where newer library can work with older client.

Note: In this document when ABI Stability is mentioned we will mean both forward and backward compatibility.
Overview
CIB is an architecture to publish compiler independent and ABI stable C++ library.
This project is also about a tool that implements cib architecture automatically for given library headers.
CIB can also be used as plugin architecture of application.
What is ABI
We programmers do have some understanding about ABI. The wikipedia also has a page for ABI. But I want to add another perspective about what exactly is ABI. An ABI of a component is implementation details of features of the language in which the component is developed. And this implementation detail depends on the compiler. For example, for a C++ component, we see mangled function names at binary level. The reason for that is C++ allows function overloading and compilers use name mangling to implement function overloading language feature of C++. In the same way all the implementation detail of other features of C++, like inheritance, encapsulation, runtime polymorphism, etc. end up being the ABI itself.
CIB Features

ABI Compatibility aka Compiler Independence: Library and clients can use their own choice of compilers.
ABI Stability: Both new library with old client and old library with new client should be binary compatible.
ABI Resilience: Virtual functions can be reordered in SDK without breaking ABI stability. With CIB there are other cases of ABI resilience too.
Perfect Isolation: Clients can use library provided classes without access to original complete definition of library classes.

CIB  allows client of a library to use all exported classes, as if those classes are part of the client code itself, without exposing the internals of classes.
CIB Goals

Platform and compiler agnostic.
Minimum footprint. Should not affect how developers write their program.

Other Solutions
I have come across some solutions that try to solve the same problem but none of them is good enough. Some wants you to write separate layer on top of existing classes so that vtable is exported across component boundary in a portable manner or some exploits how compiler behaves and uses hacks to achieve goals or some is too specific to the project it was developed for.

CppComponent: It basically uses hand written vtable to solve ABI problem. It looks like a clone of COM without idl. More details can be found here: https://github.com/jbandela/cppcomponents.
DynObj: It exploits how compiler implements vtable. For details here: http://dynobj.sourceforge.net.
Libcef's translator: Its a python script that parses C++ headers to produce automatic C layer for client and library. But it is too much specific to libcef and cannot be used in other project.

And none of these solutions I am aware of are for ABI stability, they only target ABI compatibility for different compilers. This is my understanding, of course I can be wrong.
Why C++ has ABI stability issues
Actually even C has this problem, it's just another matter that it is relatively easier to achieve ABI compatibility and stability in C.
Things that can cause ABI compatibility and stability issues in C++ are:

Object layout.
Function calling convention.
Allocators and deallocators.
Underlying integer size of enum.
Size of various integer types.
Mangled function name.
Virtual function table.
RTTI.
Exceptions.
Inheritance.

First 5 can be problems in C as well. But techniques are well known and used to cicumvent them to achieve ABI stability in C.
For C++, problems start with name mangling, that's the first reason of misunderstanding that can happen between 2 components. Layout for C++ objects are far more complex than C. There can be different layout for virtual tables depending upon compiler. Same goes for RTTI and exception. So, ensuring ABI compatibility is hard in C++. Ensuring ABI stability is super hard in general.
One thing to note is that maintaining ABI compatibility and stability in C is largely a responsibility of library developers. In C++, CIB can make ABI compatibility and stability achievable, but developers will have to be as reponsible as they need to be when pure C is used.
ABI Resilience
Some changes are conceptually unimportant for clients of a library but they break binary compatiblity. CIB makes client resilient against such changes and so client and library remain binary compatible even when such changes are made. Below is a list of changes that don't affect compatibility of client and library if SDK is published using CIB:

Any change in internal data member of a class.
Addition of new virtual methods anywhere in the class.
Change in order of virtual functions of a class.
Change in inheritance that doesn't violate previous is-a relationship. For example:

if a class starts deriving from one more base class without removing previous base class.
if a class changes it's base class to another derived class of it's previous base class.
inheritance type is changed to/from virtual inheritance.


Change in inlineness of a function. For CIB generated SDKs all inline functions are basically non-inlined and so it doesn't make any difference if inlineness of a function is changed.

CIB Architecture
Core architecture concept
As we know language feature implementations end up being ABI, see What is ABI, CIB avoids direct sharing of language feature implementation with another component. It provides very basic ABI that is shared between components and gurenteed to be compatible and easy to maintain for stability. CIB still allows working of language features across component but without letting the sharing of implementation details. Graphically it can be depicted as:

The CIB layers of each components act like proxy to another component and so each component ""feels"" like it is interacting directly with another component. The language feature implementation detail is absorved within CIB layers and doesn't cross component boundary. So, each component can be compiled using different compilers and still they will work together.
CIB ABI that is implemented in CIB layer is especially designed to ensure ABI stability.
CIB Architecture Elements
Following are the broad elements of CIB architecture:

CIB architecture needs two sets of files that are created based on public headers that library wants to publish.
One set of files, that is called library side glue code, should be compiled with the library.
The other set should be used by the client of the library. This is client side glue code.
Library side glue code defines free C style functions for all functions including class methods, constructors, and destructors.
Implementation of such C functions are just to delegate the call to original function/method/constructor/destructor/etc.
All C functions are assigned an index value. The index once assigned remains same across releases.
For every class/struct/union/namespace a MethodTable is defined which is basically an array of function pointers.
MethodTable is populated with free functions mentioned above.
Functions in MethodTable are placed at the same position as thier indices.
Library side glue code exports a C function that returns MethodTable for given class/struct/union/namespace ID.
Class definitions for client is generated with same class-name but without any data member other than an opaque pointer to original class defined by library. In CIB terminology classes that are seen by client are called proxy-classes and the opaque pointer held by proxy-class is called handle. This is basically pimpl pattern (aka bridge pattern) with pimpl pointing to object across component boundary. Note: There is another kind of proxy class too, see Layout Sharing Proxy Class.
Function index is used to fetch function pointer from MethodTable.
Implementation of all functions including methods, constructors, and destructors of proxy classes are provided by means of invoking function pointer from MethodTable.

Above is only broad description of CIB architecture. For understanding detail of CIB architecture please have a look at Examples. Each example tries to explain one peice of CIB architecture. Since this project is also about developing a tool that will automatically implement CIB architecture for a library the examples mentioned shows the code generated by CIB. Please be forewarned that little paitence will be required to analyse such code. :)
Library Side Glue Code
The code that forms library side layer is called library side glue code. This layer contains the code to represent class as set of free functions. These free functions for a class are bundled together in an array. This array of pointers-to-free-functions is called MethodTable. To avoid name mangling done by compilers that ruin the ABI compatibility, the library needs to export the MethodTable instead.
Client Side Glue Code
The code that forms client side layer is called client side glue code. This layer contains the code to reconstruct the class back from the MethodTable exported by library.
The rest of the details of CIB architecture can be understood with examples.
Examples aka CIB Architecture Detail
Details of CIB architecture is explained with various examples. Please see Examples to know the details.
Building CIB
Get the source
git clone https://github.com/satya-das/common.git
git clone https://github.com/satya-das/cppparser.git
git clone https://github.com/satya-das/cib.git
Configure and build
cd cib
mkdir builds
cd builds
cmake ..
make && make test
Alternatively, if you prefer Ninja instead of make:
cd cib
mkdir builds
cd builds
cmake -G Ninja ..
ninja && ninja test
Feature Progress of CIB tool



Feature
Description
Status




Basic
CIB should work for a simple library that exports classes with non-virtual functions.
Done


Virtual functions and inheritance
CIB should make virtual function and inheritance available to clients.
Done


Function overloading
Same named functions of a class should be seamlessly exported.
Done


Function overridding
Allow library to use interface implemented by client.
Done


Forward compatibility of client
Older client works with newer library.
Done


Forward compatibility of library
Library should be ABI compatible with newer client. As of now library and client can catch exception when non existing function is called and should take corrective measures to work with newer component.
Done


Backward compatibility of client
When newer client invokes a method present only in new SDK then std::bad_function_call exception will be thrown. Clients that want to be backward compatible should handle this exception when invoking methods present only in newer SDK.
Done


Backward compatibility of library
When library invokes a method of interface implemented by only newer client then std::bad_function_call exception will be thrown. Library developer should be aware about this to remain backward compatible when invoking new methods of it's own public interface.
Done


Create correct proxy class
A base class pointer returned by an API of library may actually be pointing to an object of a derived class. At client side we should create proxy class of exact same type to which the returned pointer is pointing to. It is needed so that dynamic_cast at client side should work as expected.
Done


Operator overloading
It is common for C++ classes to have overloaded operators.
Done


Typecast operator overloading
Typecast operator overloading of user defined types is common in C++.
Done


Simple Template class support
Support for simple template classes when it crosses component boundary. Simple template means without template template argument, or any of other fancy stuff.
Done


Return existing proxy class
If a function returns pointer or reference of object for which proxy class already exists then existing proxy class should be returned.
Done


Support protected methods
Protected methods are accessible to derived class and they should be exported so that client's derived class can call them.
Done


Support private pure virtual
Private pure virtual is used in template method design pattern.
Done


Rvalue reference parameter
RValue references need to cross component boundary.
Done


Support of free functions
Free functions in global and orher namespaces too should be supported.
Done


Goal: Use CIB in real production SDK
To demonstrably prove viability of this architecture and tool it will be better to use it for at least one large production quality software. My plan is to use CIB for ObjectARX SDK of AutoCAD to demonstrate it's viability.
IN PROGRESS


Phase1: For AutoCAD SDK subset
Make CIB work for ObjectARX SDK even when it requires changing of SDK headers to avoid problems of SDK and/or the tool.
DONE


Phase2: For complete AutoCAD SDK
Improve tool so that only minimal change in ObjectARX SDK is required and that too only to avoid problems of SDK and not of tool.
IN PROGRESS


Support deleted members
Constructors and operators marked as delete should be deleted for proxy classes as well.



Enum and enum classes
Enums used as parameter or return type.



STL classes
It is common for a C++ programs to use stl classes. CIB should make it possible to export STL classes in the same way it does for every other classes.



Support shared_ptr and unique_ptr
Modern C++ programing expects these to be used more often.



Exception support
Make exception object travel across component boundary in a compatible way.



Function pointer as parameter
Function pointers can be used as parameter or return type of another function.



Support std::function
std::function can be used as function parameter or return type. They too should be supported.



Support for intrusive pointer
Many libraries use intrusive pointer to manage object life cyle and functions can return smart pointer for intrusively managed reference count of object.



Non-const pointer ref return type
When a reference of pointer of non-POD is returned from a function a change in that should be propagated to the library.



Support public data members
Public data members of a class should be exported in ABI stable way.





",4
vim-jp/syntax-vim-ex,Vim script,"syntax-vim-ex
An excellent Vim's syntax highlighting file for Vim script.
Difference between original

The syntax-vim-ex has been generated by parse the C source code of Vim.  So
keyword extraction is accurate.
Omitted keyword extraction algorithm of Ex command is perfect because it is
same and the C source code of Vim.
It will immediately follow the Vim updates.
Generator source code is all open sourced.  (See generator branch)

Requirement
Vim 7.4.1486 or later
How to Install
$ cd ~/.vim
$ mkdir -p pack/my/opt
$ cd pack/my/opt
$ git clone --single-branch --depth 1 https://github.com/vim-jp/syntax-vim-ex.git
Add following line to your .vimrc.
packadd! syntax-vim-ex
Remarks:

In the case of Windows should read the ~/.vim to
%HOMEDRIVE%%HOMEPATH%\vimfiles.  And mkdir -p option is not required.
Directory name my can be changed to any name.
If you want to configure the package under ~/.vim directory as in this
example the setting of option 'packpath' is not required.
If you are calling :packloadall explicitly in your .vimrc, packadd
should be added before it.

How to Update
$ cd ~/.vim/pack/my/opt/syntax-vim-ex
$ git pull
Licence
Follow the Vim license
Authors

Hirohito Higashi (h_east)
vim-jp
Charles E. Campbell (author of original syntax/vim.vim)

",37
huoji555/huojiDemo,None,"huojiDemo
第一个库
以后会在这个库里更新一些自己的想法
2018.3.13
在搭新的框架
想着要在底层加一个神经网络
感觉心里有点空落落的
明明学很久了
但还感觉自己很菜
晚上有空的话继续刷算法题
毕竟4.1还有场比赛等着我呢
2018.7.1
最近数据库这方面看了很多，调整了许多结构，优化了许多代码，项目现在比较不满意的地方就是那个验证码(很小的东西)
其次想集成Redis,数据库这块，将来可能会换成JPA,或者是MyBatis
底层加一个自动学习算法(首先造别的轮子，不然不会考虑的)
关于工作，也许是web工程师,也许是游戏端，还想要算法工程师，都想试一试
关于考研，在职更适合我一点
八月的时候再来记录
2018.8.1
已经入职了，新的一个月就是把公司框架流程走一次(包括代码风格，规范的书写)
深入了解JPA,之前用的不多，现在有机会好好了解了
前端用的AngularJs,也在学习中，感觉它的数据双向绑定和模块化哪个都很棒
还有框架里的Redis，要把它吃透
关于思维导图，还是在拿他画着自己学习过的知识
这是一点，其二了解NPL等相关路径算法，以及谱聚类，分类等算法(能把知识图谱每个点都整理一下我觉得很有趣)
学会AngularJs后,周末时间试着重构前端代码(感觉不频繁操作DOMd的话，应该会比之前好不少，而且更加清晰)
思维导图Redis配置及应用(要数据对比)
九月再来记录
2018.9.1
以往这个时间，都已经开学了
夏天过去了，秋天还没到
没想到9.1就来公司加班
可能9月要学更多的东西吧
9月会细细的看数据结构与算法
搞清楚之前页面加载为什么这么慢的问题(可能和load有一定的关系)
重装服务器，换系统（linux）
看程序员的数学（我觉得卷积很重要）
JPA虽然写东西没什么问题了，需要更深入的了解下(知识图谱见)
AngularJS也需要深入
想找一个好的demo看下
尽可能的把java23种设计模式都过一遍
2018.10.8
国庆假期已过
毛线头改造再往后推吧
现在要拼一把试试考研
这个事从中秋开始折磨我
到下定决心去做
好像也就那样了
因为不想止步不前
思维导图搜索引擎（CNN,NLP）
高级点的功能
还是离不开算法呀
难点在于语义的识别和挖掘
日志这块，可以发布一个查看日志的功能（后台功能）
此外，登录信息可放到此处做
还可以单独增加一个日志异常记录，以表格的形式显示出来
在@AfterThrowing里记录异常，保存到相应的实体类
2018.11.1
心里来回纠结了一个月，也慢慢习惯了现在的生活，其实也不算很糟糕，毕竟它能学点东西嘛，另一个是抬高天花板
上个月项目上倒没什么事，新框架也搭建完毕，后台管理模块也在继续往上加(是一个通用的以后可以作为)
关于HandLP,刚找到时欣喜的不得了，并且效果也不算很差，但是不适用于现在的场景进行分析
这个月继续准备KY,下周回学校现场确认，平时有时间就整整自己的系统
一步一步踏实走下去吧，想要的都会有的
2018.12.1
再有两天就会把知识点都过完了，思维导图之前也抽空加了很多东西，已经确定毕设要做什么了，KY是一定要考的，当
我为这个发愁时，同学已经直博了，怎么像黄金时代里说的一样，日子是越来越像受锤了,不过我打算努力过下去，挨锤
的同时，再坚持梦想就很不错了，KY之后去大连一段时间，回来和头儿约个饭，的确很久不见了
技术我会先放一放，KY为大，努力读个硕士博士吧，总是感觉本科能接触的东西太片面，太局限了
2019.1.2
是个新开始了，过去一年经历了很多，今年想要做的事是面到大公司，学到更加深，更全面的技术，最近一个月做自己的
毕设，改文档，之后就是学自己想学的东西了，已经放松过了，要朝着想去的地方前进了，现在市场很不景气，可还是很
喜欢Coding,新的一年，加油
2019.2.1
这一个月是沉寂期，在低谷的时候，看了很多基础相关的东西，基础还是薄弱，继续看吧，最近看了，明朝那些事，王垠的博客
阮一峰的weekly，我觉得都是很好的东西，只是应该在更早的时候看到，此时亦不晚，向前走吧，每个决定都是对的
2019.3.1
新公司，最近在写登陆系统，显然是没什么东西，打算最近再重学一次数据结构，LeetCode也要刷题，前段时间写的Tomcat和23种设计模式又
忘记了，下午就复习，平时每天半个小时的读书时间，早上的跑步时间，再学一会儿PostMan,这个月搞定数据结构思维，并做好笔记，希望越
来越好，厚积薄发
2019.4.1
上个月写了几个小工具，Redis消息队列，sql存储过程，大致就是些，数据结构思维看到了LinkedList,还要继续看，已经有了新的打算，在前进的
路上 这个月可能会出差几天 但望每日有所得 这个月搞定毕业论文等相关事宜，然后就是啃数据结构 记录另外一则显而易见却不易执行的道理 你要做
一件事，就一定去集中自己所有的资源去做这件事，这才有可能会成功，否则，生活将是杂乱无章，高不成，低不就，做完一件再去做另一件
2019.5.1
数据结构思维看到了剩最后几章，四月主要就是这些没用的存储过程，加上些不重要的业务，但是这段时间想了很多关于以后的事，前些日子跟女朋友也分开了，
休整几天，总体来说，四月是不怎样的一个月，心里想的杂事太多了，分手之后也能更冷静的分析现在的状况，也该知道这段时间想要做的事事什么了，大概就是
这样了，像人间失格里所讲的那样一切都会过去的，大概失去更能让人明白些什么，喜欢的太沉重了，所以以后继续向前走吧，继续努力，把确定的放在手上，
不确定的交给未来
",3
objeck/objeck-lang,C++,"Objeck
Objeck is an object-oriented computer language with functional features. The language has ties with Java, Scheme and UML. In this language all data types, except for higher-order functions, are treated as objects.

Objeck is a general-purpose programming language with an emphasis placed on simplicity. The programming environment consists of a compiler, virtual machine and command line debugger.
class Hello {
  function : Main(args : String[]) ~ Nil {
    ""Hello World"" → PrintLine();
    ""Καλημέρα κόσμε"" → PrintLine();
    ""こんにちは 世界"" → PrintLine();
  }
}
See more on Rosetta Code and checkout the following programming tasks.
Notable features:

Object-oriented and functional

Classes, interfaces and higher-order functions
Anonymous classes
Reflection
Object serialization
Type inference


Native support for threads, sockets, files, date/time, etc.
Libraries

Collections (vectors, queues, trees, hashes, etc.)
HTTP and HTTPS clients
RegEx
JSON and XML parsers
Encryption
Database access
Data structure querying
2D Gaming


Garbage collection
JIT support (IA-32 and AMD64)

Documentation
Please refer to the programmer's guide documentation and online tutorial. Also checkout Rosetta Code examples.
Deployment
Build and deployment instructions for Windows, Linux and OS X.
Binaries
Get the latest binaries.
",16
BumblebeeBat/dump,Erlang,"Dump
This is a very simple harddrive database. It only stores one size of block. here is an example of using it.
1> ID = database003,
2> dump_sup:start_link(ID, 5, 10000, ram, ""temp"").
{ok,<0.37.0>}
3> V = <<3,2,1,5,6>>.
<<3,2,1,5,6>>
4> Location = dump:put(V, ID).
15992
5> V = dump:get(Location, ID).
<<3,2,1,5,6>>
6> dump:delete(Location, ID).
ok
7> dump:get(Location, ID).
<<3,2,1,5,6>>
8> Location = dump:put(<<0:40>>, ID).
15992
9> dump:get(Location, ID).
<<0,0,0,0,0>>

it uses the atom database003 so that you can start more than one dump, and give them different names.
",2
facebook/proxygen,C++,"Proxygen: Facebook's C++ HTTP Libraries

This project comprises the core C++ HTTP abstractions used at
Facebook. Internally, it is used as the basis for building many HTTP
servers, proxies, and clients. This release focuses on the common HTTP
abstractions and our simple HTTPServer framework. Future releases will
provide simple client APIs as well. The framework supports HTTP/1.1,
SPDY/3, SPDY/3.1, and HTTP/2. The goal is to provide a simple,
performant, and modern C++ HTTP library.
We have a Google group for general discussions at https://groups.google.com/d/forum/facebook-proxygen.
The original blog post
also has more background on the project.
Installing
Note that currently this project has only been tested on Ubuntu 14.04,
although it likely works on many other platforms. Support for Mac OSX is
incomplete.
You will need at least 3 GiB of memory to compile proxygen and its
dependencies.
Easy Install
Just run ./deps.sh from the proxygen/ directory to get and build all
the dependencies and proxygen. It will also run all the tests. Then run
./reinstall.sh to install it. You can run ./deps.sh && ./reinstall.sh
whenever to rebase the dependencies, and then rebuild and reinstall proxygen.
A note on compatibility: this project relies on system installed
folly. If you rebase proxygen and make starts to fail, you likely
need to update to the latest version of folly. Running
./deps.sh && ./reinstall.sh will do this for you. We are still working
on a solution to manage dependencies more predictably.
Other Platforms
If you are running on another platform, you may need to install several
packages first. Proxygen and folly are all autotools based projects.
Introduction
Directory structure and contents:



Directory
Purpose




proxygen/external/
Contains non-installed 3rd-party code proxygen depends on.


proxygen/lib/
Core networking abstractions.


proxygen/lib/http/
HTTP specific code.


proxygen/lib/services/
Connection management and server code.


proxygen/lib/utils/
Miscellaneous helper code.


proxygen/httpserver/
Contains code wrapping proxygen/lib/ for building simple C++ http servers. We recommend building on top of these APIs.



Architecture
The central abstractions to understand in proxygen/lib are the session, codec,
transaction, and handler. These are the lowest level abstractions, and we
don't generally recommend building off of these directly.
When bytes are read off the wire, the HTTPCodec stored inside
HTTPSession parses these into higher level objects and associates with
it a transaction identifier. The codec then calls into HTTPSession which
is responsible for maintaining the mapping between transaction identifier
and HTTPTransaction objects. Each HTTP request/response pair has a
separate HTTPTransaction object. Finally, HTTPTransaction forwards the
call to a handler object which implements HTTPTransaction::Handler. The
handler is responsible for implementing business logic for the request or
response.
The handler then calls back into the transaction to generate egress
(whether the egress is a request or response). The call flows from the
transaction back to the session, which uses the codec to convert the
higher level semantics of the particular call into the appropriate bytes
to send on the wire.
The same handler and transaction interfaces are used to both create requests
and handle responses. The API is generic enough to allow
both. HTTPSession is specialized slightly differently depending on
whether you are using the connection to issue or respond to HTTP
requests.

Moving into higher levels of abstraction, proxygen/httpserver has a
simpler set of APIs and is the recommended way to interface with proxygen
when acting as a server if you don't need the full control of the lower
level abstractions.
The basic components here are HTTPServer, RequestHandlerFactory, and
RequestHandler. An HTTPServer takes some configuration and is given a
RequestHandlerFactory. Once the server is started, the installed
RequestHandlerFactory spawns a RequestHandler for each HTTP
request. RequestHandler is a simple interface users of the library
implement. Subclasses of RequestHandler should use the inherited
protected member ResponseHandler* downstream_ to send the response.
Using it
Proxygen is a library. After installing it, you can build your own C++
server. Try cding to the directory containing the echo server at
proxygen/httpserver/samples/echo/. You can then build it with this one
liner:

g++ -std=c++14 -o my_echo EchoServer.cpp EchoHandler.cpp -lproxygenhttpserver -lfolly -lglog -lgflags -pthread

After running ./my_echo, we can verify it works using curl in a different terminal:
$ curl -v http://localhost:11000/
*   Trying 127.0.0.1...
* Connected to localhost (127.0.0.1) port 11000 (#0)
> GET / HTTP/1.1
> User-Agent: curl/7.35.0
> Host: localhost:11000
> Accept: */*
>
< HTTP/1.1 200 OK
< Request-Number: 1
< Date: Thu, 30 Oct 2014 17:07:36 GMT
< Connection: keep-alive
< Content-Length: 0
<
* Connection #0 to host localhost left intact
Documentation
We use Doxygen for Proxygen's internal documentation. You can generate a
copy of these docs by running doxygen Doxyfile from the project
root. You'll want to look at html/namespaceproxygen.html to start. This
will also generate folly documentation.
License
See LICENSE.
Contributing
Contributions to Proxygen are more than welcome. Read the guidelines in CONTRIBUTING.md.
Make sure you've signed the CLA before sending in a pull request.
Whitehat
Facebook has a bounty program for
the safe disclosure of security bugs. If you find a vulnerability, please
go through the process outlined on that page and do not file a public issue.
",6154
weimingtom/Kuuko,C#,"Kuuko
Copy of kopilua, for someone in heaven.
https://ja.wikipedia.org/wiki/%E6%9D%BE%E6%9D%A5%E6%9C%AA%E7%A5%90
http://39miyu-chan.official.jp/
Kuuko is the name of クー子 in Haiyore! Nyaruko-san.
https://en.wikipedia.org/wiki/Nyaruko:_Crawling_with_Love
NOTE: this project is moved to https://github.com/weimingtom/kurumi
This project contains some notes for study which are not related to kopilua.
Ref

http://www.ppl-pilot.com/kopilua.aspx

History

2017-03-03 : First running KopiLuaJava successfully (20:22 2017-03-03).
2017-03-03 : Convert to Java successfully.
2016-03-08 : First public modification.
2015-10-27 : First private modification.

Converting Tool

C# to Java Converter 1.9
http://www.tangiblesoftwaresolutions.com/

Script Engine List
README2.md
Database Engine List
README3.md
Visual Novel Engine
README4.md
NOTE: moved to weimingtom/wmt_link_collections_in_Chinese
https://github.com/weimingtom/wmt_link_collections_in_Chinese/blob/master/vn.md
Lua Hack
lua_hack.md
",3
notable/notable,TypeScript,"Notable (DOWNLOAD)



The markdown-based note-taking app that doesn't suck.
I couldn't find a note-taking app that ticked all the boxes I'm interested in: notes are written and rendered in GitHub-flavored Markdown, no WYSIWYG, no proprietary formats, I can run a search & replace across all notes, notes support attachments, the app isn't bloated, the app has a pretty interface, tags are indefinitely nestable and can import Evernote notes (because that's what I was using before).
So I built my own.
Features
/path/to/your/data_directory
├─┬ attachments
│ ├── foo.ext
│ ├── bar.ext
│ └── …
└─┬ notes
  ├── foo.md
  ├── bar.md
  └── …



No proprietary formats: Notable is just a pretty front-end for a folder structured as shown above. Notes are plain Markdown files, their metadata is stored as Markdown front matter. Attachments are also plain files, if you attach a picture.jpg to a note everything about it will be preserved, and it will remain accessible like any other file.


Proper editor: Notable doesn't use any WYSIWYG editor, you just write some Markdown and it gets rendered as GitHub-flavored Markdown. The built-in editor is Monaco Editor, the same one VS Code uses, this means you get things like multi-cursor by default. If you need more advanced editing features with a single shortcut you can open the current note in your default Markdown editor.


Indefinitely nestable tags: Pretty much all the other note-taking apps differentiate between notebooks, tags and templates. IMHO this unnecessarily complicates things. In Notable you can have root tags (foo), indefinitely nestable tags (foo/bar, foo/.../qux) and it still supports notebooks and templates, they are just special tags with a different icon (Notebooks/foo, Templates/foo/bar).


Upon first instantiation, some tutorial notes will be added to the app, check them out for more in-depth details about the app and how to use it. You can also find the raw version here.
Comparison

Part of this comparison is personal opinion: you may disagree on the UI front, things I consider bloat may be considered features by somebody else etc. but hopefully this comparison did a good job at illustrating the main differences.
Demo
Dark Theme

Indefinitely Nestable Tags

Editor

Multi-Note Editor

Split-Editor + Zen Mode + Quick Open

Contributing
There are multiple ways to contribute to this project, read about them here.
Related

enex-dump: Dump the content of Evernote's .enex files, preserving attachments, some metadata and optionally converting notes to Markdown.
Noty: Autosaving sticky note with support for multiple notes without needing multiple windows.
Markdown Todo: Manage todo lists inside markdown files with ease. Have the same todo-related shortcuts that Notable provides, but in Visual Studio Code.
Todo+: Manage todo lists with ease. Powerful, easy to use and customizable.

License
AGPLv3 © Fabio Spampinato
",10162
microsoftgraph/ios-objectivec-connect-rest-sample,Objective-C,"Office 365 Connect Sample for iOS Using Microsoft Graph
Connecting to Office 365 is the first step every iOS app must take to start working with Office 365 services and data. This sample shows how to connect and then call one API through Microsoft Graph (previously called Office 365 unified API).
Prerequisites


Xcode from Apple


An Office 365 account. You can sign up for an Office 365 Developer subscription that includes the resources that you need to start building Office 365 apps.

Note: If you already have a subscription, the previous link sends you to a page with the message Sorry, you can’t add that to your current account. In that case, use an account from your current Office 365 subscription.



A Microsoft Azure tenant to register your application. Azure Active Directory (AD) provides identity services that applications use for authentication and authorization. A trial subscription can be acquired here: Microsoft Azure.

Important: You will also need to ensure your Azure subscription is bound to your Office 365 tenant. To do this, see the Active Directory team's blog post, Creating and Managing Multiple Windows Azure Active Directories. The section Adding a new directory will explain how to do this. You can also see Set up your Office 365 development environment and the section Associate your Office 365 account with Azure AD to create and manage apps for more information.



A client id (application id) and redirect uri values of an application registered in Azure. This sample application must be granted the Send mail as user permission for Microsoft Graph. To create the registration, see Grant permissions to the Connect application in Azure.


Running this sample in Xcode

Clone this repository
Use Carthage to import the Microsoft Authenticaion Library (MSAL) iOS dependency. Download the latest version of Carthage here.
Open O365-iOS-Microsoft-Graph-Connect.xcodeproj
Open info.plist. You'll see that the ClientID (application id you received from the registration process in the prerequisites section) goes here.

  <key>CFBundleURLTypes</key>
    <array>
        <dict>
            <key>CFBundleTypeRole</key>
            <string>Editor</string>
            <key>CFBundleURLName</key>
            <string>$(PRODUCT_BUNDLE_IDENTIFIER)</string>
            <key>CFBundleURLSchemes</key>
            <array>
                <string>msalENTER_YOUR_CLIENT_ID</string>
                <string>auth</string>
            </array>
        </dict>
    </array>

Build the MSAL framework
The preview version of MSAL is distributed as source code using Carthage. To build the source code, do these steps:


Open the Bash terminal and go to the app root folder.


Create a cartfile: Copy echo ""github \""AzureAD/microsoft-authentication-library-for-objc\"" \""master\"""" > Cartfile  into the terminal and run the command.


Build the MSAL library: Copy carthage update into the terminal and run the command.


Run the sample.


To learn more about the sample, see Call Microsoft Graph in an iOS App.
Questions and comments
We'd love to get your feedback about the Office 365 iOS Microsoft Graph Connect project. You can send your questions and suggestions to us in the Issues section of this repository.
Questions about Office 365 development in general should be posted to Stack Overflow. Make sure that your questions or comments are tagged with [Office365] and [MicrosoftGraph].
Contributing
You will need to sign a Contributor License Agreement before submitting your pull request. To complete the Contributor License Agreement (CLA), you will need to submit a request via the form and then electronically sign the CLA when you receive the email containing the link to the document.
This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.
Additional resources

Office Dev Center
Microsoft Graph overview page
Using CocoaPods

Copyright
Copyright (c) 2017 Microsoft. All rights reserved.
",8
rsocket/rsocket-cpp,C++,"rsocket-cpp
C++ implementation of RSocket


Dependencies
Install folly:
brew install --HEAD folly

Building and running tests
After installing dependencies as above, you can build and run tests with:
# inside root ./rsocket-cpp
mkdir -p build
cd build
cmake -DCMAKE_BUILD_TYPE=DEBUG ../
make -j
./tests

License
By contributing to rsocket-cpp, you agree that your contributions will be licensed
under the LICENSE file in the root directory of this source tree.
",122
web-platform-tests/wpt,HTML,"The web-platform-tests Project 
The web-platform-tests Project is a W3C-coordinated attempt to build a
cross-browser testsuite for the Web-platform stack. Writing tests in a way
that allows them to be run in all browsers gives browser projects
confidence that they are shipping software that is compatible with other
implementations, and that later implementations will be compatible with
their implementations. This in turn gives Web authors/developers
confidence that they can actually rely on the Web platform to deliver on
the promise of working across browsers and devices without needing extra
layers of abstraction to paper over the gaps left by specification
editors and implementors.
Setting Up the Repo
Clone or otherwise get https://github.com/web-platform-tests/wpt.
Note: because of the frequent creation and deletion of branches in this
repo, it is recommended to ""prune"" stale branches when fetching updates,
i.e. use git pull --prune (or git fetch -p && git merge).
Running the Tests
The tests are designed to be run from your local computer. The test
environment requires Python 2.7+ (but not Python 3.x).
On Windows, be sure to add the Python directory (c:\python2x, by default) to
your %Path% Environment Variable,
and read the Windows Notes section below.
To get the tests running, you need to set up the test domains in your
hosts file.
The necessary content can be generated with ./wpt make-hosts-file; on
Windows, you will need to precede the prior command with python or
the path to the Python binary (python wpt make-hosts-file).
For example, on most UNIX-like systems, you can setup the hosts file with:
./wpt make-hosts-file | sudo tee -a /etc/hosts
And on Windows (this must be run in a PowerShell session with Administrator privileges):
python wpt make-hosts-file | Out-File $env:systemroot\System32\drivers\etc\hosts -Encoding ascii -Append
If you are behind a proxy, you also need to make sure the domains above are
excluded from your proxy lookups.
Running Tests Manually
The test server can be started using
./wpt serve

On Windows: You will need to precede the prior command with
python or the path to the python binary.
python wpt serve
This will start HTTP servers on two ports and a websockets server on
one port. By default the web servers start on ports 8000 and 8443 and
the other ports are randomly-chosen free ports. Tests must be loaded
from the first HTTP server in the output. To change the ports,
create a config.json file in the wpt root directory, and add
port definitions of your choice e.g.:
{
  ""ports"": {
    ""http"": [1234, ""auto""],
    ""https"":[5678]
  }
}

After your hosts file is configured, the servers will be locally accessible at:
http://web-platform.test:8000/
https://web-platform.test:8443/ *
*See Trusting Root CA
Running Tests Automatically
Tests can be run automatically in a browser using the run command of
the wpt script in the root of the checkout. This requires the hosts
file setup documented above, but you must not have the
test server already running when calling wpt run. The basic command
line syntax is:
./wpt run product [tests]
On Windows: You will need to precede the prior command with
python or the path to the python binary.
python wpt run product [tests]
where product is currently firefox or chrome and [tests] is a
list of paths to tests. This will attempt to automatically locate a
browser instance and install required dependencies. The command is
very configurable; for example to specify a particular binary use
wpt run --binary=path product. The full range of options can be see
with wpt run --help and wpt run --wptrunner-help.
Not all dependencies can be automatically installed; in particular the
certutil tool required to run https tests with Firefox must be
installed using a system package manager or similar.
On Debian/Ubuntu certutil may be installed using:
sudo apt install libnss3-tools

And on macOS with homebrew using:
brew install nss

On other platforms, download the firefox archive and common.tests.tar.gz
archive for your platform from
Mozilla CI.
Then extract certutil[.exe] from the tests.tar.gz package and
libnss3[.so|.dll|.dynlib] and put the former on your path and the latter on
your library path.
Command Line Tools
The wpt command provides a frontend to a variety of tools for
working with and running web-platform-tests. Some of the most useful
commands are:

wpt serve - For starting the wpt http server
wpt run - For running tests in a browser
wpt lint - For running the lint against all tests
wpt manifest - For updating or generating a MANIFEST.json test manifest
wpt install - For installing the latest release of a browser or
webdriver server on the local machine.

Windows Notes
On Windows wpt commands must be prefixed with python or the path
to the python binary (if python is not in your %PATH%).
python wpt [command]
Alternatively, you may also use
Bash on Ubuntu on Windows
in the Windows 10 Anniversary Update build, then access your windows
partition from there to launch wpt commands.
Please make sure git and your text editor do not automatically convert
line endings, as it will cause lint errors. For git, please set
git config core.autocrlf false in your working tree.
Certificates
By default pre-generated certificates for the web-platform.test domain
are provided in tools/certs. If you wish to generate new
certificates for any reason it's possible to use OpenSSL when starting
the server, or starting a test run, by providing the
--ssl-type=openssl argument to the wpt serve or wpt run
commands.
If you installed OpenSSL in such a way that running openssl at a
command line doesn't work, you also need to adjust the path to the
OpenSSL binary. This can be done by adding a section to config.json
like:
""ssl"": {""openssl"": {""binary"": ""/path/to/openssl""}}

On Windows using OpenSSL typically requires installing an OpenSSL distribution.
Shining Light
provide a convenient installer that is known to work, but requires a
little extra setup, i.e.:
Run the installer for Win32_OpenSSL_v1.1.0b (30MB). During installation,
change the default location for where to Copy OpenSSL Dlls from the
System directory to the /bin directory.
After installation, ensure that the path to OpenSSL (typically,
this will be C:\OpenSSL-Win32\bin) is in your %Path%
Environment Variable.
If you forget to do this part, you will most likely see a 'File Not Found'
error when you start wptserve.
Finally, set the path value in the server configuration file to the
default OpenSSL configuration file location. To do this create a file
called config.json.  Then add the OpenSSL configuration below,
ensuring that the key ssl/openssl/base_conf_path has a value that is
the path to the OpenSSL config file (typically this will be
C:\\OpenSSL-Win32\\bin\\openssl.cfg):
{
  ""ssl"": {
    ""type"": ""openssl"",
    ""encrypt_after_connect"": false,
    ""openssl"": {
      ""openssl_binary"": ""openssl"",
      ""base_path: ""_certs"",
      ""force_regenerate"": false,
      ""base_conf_path"": ""C:\\OpenSSL-Win32\\bin\\openssl.cfg""
    },
  },
}

Trusting Root CA
To prevent browser SSL warnings when running HTTPS tests locally, the
web-platform-tests Root CA file cacert.pem in tools/certs
must be added as a trusted certificate in your OS/browser.
NOTE: The CA should not be installed in any browser profile used
outside of tests, since it may be used to generate fake
certificates. For browsers that use the OS certificate store, tests
should therefore not be run manually outside a dedicated OS instance
(e.g. a VM). To avoid this problem when running tests in Chrome or
Firefox use wpt run, which disables certificate checks and therefore
doesn't require the root CA to be trusted.
Publication
The master branch is automatically synced to http://w3c-test.org/.
Pull requests are
automatically mirrored except those
that modify sensitive resources (such as .py). The latter require
someone with merge access to comment with ""LGTM"" or ""w3c-test:mirror"" to
indicate the pull request has been checked.
Finding Things
Each top-level directory matches the shortname used by a standard, with
some exceptions. (Typically the shortname is from the standard's
corresponding GitHub repository.)
For some of the specifications, the tree under the top-level directory
represents the sections of the respective documents, using the section
IDs for directory names, with a maximum of three levels deep.
So if you're looking for tests in HTML for ""The History interface"",
they will be under html/browsers/history/the-history-interface/.
Various resources that tests depend on are in common, images, and
fonts.
Branches
In the vast majority of cases the only upstream branch that you
should need to care about is master. If you see other branches in
the repository, you can generally safely ignore them.
Contributing
Save the Web, Write Some Tests!
Absolutely everyone is welcome (and even encouraged) to contribute to
test development, so long as you fulfill the contribution requirements
detailed in the Contributing Guidelines. No test is
too small or too simple, especially if it corresponds to something for
which you've noted an interoperability bug in a browser.
The way to contribute is just as usual:

Fork this repository (and make sure you're still relatively in sync
with it if you forked a while ago).
Create a branch for your changes:
git checkout -b topic.
Make your changes.
Run the lint script described below.
Commit locally and push that to your repo.
Send in a pull request based on the above.

Issues with web-platform-tests
If you spot an issue with a test and are not comfortable providing a
pull request per above to fix it, please
file a new issue.
Thank you!
Lint tool
We have a lint tool for catching common mistakes in test files. You
can run it manually by starting the lint executable from the root of
your local web-platform-tests working directory like this:
./wpt lint

The lint tool is also run automatically for every submitted pull
request, and reviewers will not merge branches with tests that have
lint errors, so you must fix any errors the lint tool reports.
In the unusual case of error reports for things essential to a
certain test or that for other exceptional reasons shouldn't prevent
a merge of a test, update and commit the lint.whitelist file in the
web-platform-tests root directory to suppress the error reports.
For more details, see the lint-tool documentation.
Adding command-line scripts (""tools"" subdirs)
Sometimes you may want to add a script to the repository that's meant
to be used from the command line, not from a browser (e.g., a script
for generating test files). If you want to ensure (e.g., for security
reasons) that such scripts won't be handled by the HTTP server, but
will instead only be usable from the command line, then place them in
either:


the tools subdir at the root of the repository, or


the tools subdir at the root of any top-level directory in the
repository which contains the tests the script is meant to be used
with


Any files in those tools directories won't be handled by the HTTP
server; instead the server will return a 404 if a user navigates to
the URL for a file within them.
If you want to add a script for use with a particular set of tests but
there isn't yet any tools subdir at the root of a top-level
directory in the repository containing those tests, you can create a
tools subdir at the root of that top-level directory and place your
scripts there.
For example, if you wanted to add a script for use with tests in the
notifications directory, create the notifications/tools subdir and
put your script there.
Test Review
We can sometimes take a little while to go through pull requests
because we have to go through all the tests and ensure that they match
the specification correctly. But we look at all of them, and take
everything that we can.
META.yml files are used only to indicate who should be notified of pull
requests.  If you are interested in receiving notifications of proposed
changes to tests in a given directory, feel free to add yourself to the
META.yml file. Anyone with expertise in the specification under test can
approve a pull request.  In particular, if a test change has already
been adequately reviewed ""upstream"" in another repository, it can be
pushed here without any further review by supplying a link to the
upstream review.
Search filters to find things to review:

Open PRs (excluding vendor exports)
Reviewed but still open PRs (excluding vendor exports) (Merge? Something left to fix? Ping other reviewer?)
Open PRs without reviewers
Open PRs with label infra (excluding vendor exports)
Open PRs with label docs (excluding vendor exports)

Getting Involved
If you wish to contribute actively, you're very welcome to join the
public-test-infra@w3.org mailing list (low traffic) by
signing up to our mailing list.
The mailing list is archived.
Join us on irc #testing (irc.w3.org, port 6665). The channel
is archived.
Documentation

How to write and review tests
Documentation for the wptserve server

",2251
WPO-Foundation/wptagent,Python,"wptagent

Cross-platform WebPageTest agent
Supported Platforms/Browsers
Chrome is the only browser that currently supports manipulating requests (changing headers, blocking requests, etc).  Chrome, IE and Microsoft Edge are the only browsers that currently support capturing response bodies and running optimization checks.  All browsers should support basic page loading, scripts and video capture on all platforms.  Traffic-shaping is supported on all platforms as well.
Linux (with display or headless with Xvfb)

Chrome (Stable, Beta and Unstable)
Firefox (Stable and Nightly)
Opera (Stable, Beta and Developer)

Windows

Chrome (Stable, Beta, Dev and Canary)
Firefox (Stable, ESR, Developer Edition, Beta and Nightly)
Microsoft Edge
Internet Explorer
Opera (Stable, Beta and Developer)

OSX

Chrome (Stable and Canary)
Firefox (Stable and Nightly)

Android (requires a tethered host - Raspberry Pi's preferred)

Chrome (Stable, Beta, Dev and Canary)
Several browsers run as ""black box"" tests (single page load, visual metrics only):

Chrome (Stable, Beta, Dev and Canary)
Samsung Browser
Opera
Opera Mini
UC Browser
UC Mini
Firefox (Stable and Beta)



Known Issues

Internet Explorer does not support manipulating requests (adding headers, blocking requests, etc)

Installation

Install docs are here

Run with docker
Check out the docker instructions for information on how to
run the agent in a docker container.
Command-line options
Basic agent config

-v : Increase verbosity (specify multiple times for more). -vvvv for full debug output.
--name : Agent name (defaults to the machine's hostname).
--exit : Exit after the specified number of minutes.

Useful for running in a shell script that does some maintenance or updates periodically (like hourly).


--dockerized: The agent is running inside a docker container.
--ec2 : Load config settings from EC2 user data.
--gce : Load config settings from GCE user data.
--log : Log critical errors to the given file.

Video capture/display settings

--xvfb : Use an xvfb virtual display for headless testing (Linux only).
--fps : Video capture frame rate (defaults to 10). Valid range is 1-60. (Linux only).

Server/location configuration

--server (required): URL for WebPageTest work (i.e. http://www.webpagetest.org/work/).
--validcertificate: Validate server certificates (HTTPS server, defaults to False).
--location (required): Location ID (as configured in locations.ini on the server).
--key : Location key (if configured in locations.ini).

Traffic-shaping options (defaults to host-based)

--shaper : Override default traffic shaper. Current supported values are:

none - Disable traffic-shaping (i.e. when root is not available).
netem,<interface> - Use NetEm for bridging rndis traffic (specify outbound interface).  i.e. --shaper netem,eth0
remote,<server>,<down pipe>,<up pipe> - Connect to the remote server over ssh and use pre-configured dummynet pipes (ssh keys for root user should be pre-authorized).



CPU Throttling

--throttle: Enable cgroup-based CPU throttling for mobile emulation (Linux only).

Android testing options

--android : Run tests on an attached android device.
--device : Device ID (only needed if more than one android device attached).
--gnirehtet : Use gnirehtet for reverse-tethering. You will need to manually approve the vpn once per mobile device. Valid options are:

,: i.e. --gnirehtet eth0,8.8.8.8


--vpntether : (Android < 7) Use vpn-reverse-tether for reverse-tethering. This is the recommended way to reverse-tether devices. You will need to manually approve the vpn once per mobile device. Valid options are:

,: i.e. --vpntether eth0,8.8.8.8


--simplert : Use SimpleRT for reverse-tethering.  The APK should be installed manually (adb install simple-rt/simple-rt-1.1.apk) and tested once manually (./simple-rt -i eth0 then disconnect and re-connect phone) to dismiss any system dialogs.  The ethernet interface and DNS server should be passed as options:

,: i.e. --simplert eth0,8.8.8.8


--rndis : (deprecated) Enable reverse-tethering over rndis (Android < 6.0).  Valid options are:

/,,,: Static Address.  i.e. --rndis 192.168.0.8/24,192.168.0.1,8.8.8.8,8.8.4.4
dhcp



Options for authenticating the agent with the server:

--username : User name if using HTTP Basic auth with WebPageTest server.
--password : Password if using HTTP Basic auth with WebPageTest server.
--cert : Client certificate if using certificates to authenticate the WebPageTest server connection.
--certkey : Client-side private key (if not embedded in the cert).

Currently supported features

Feature complete except as noted below (for Windows, Linux, Mac and Android devices)
Supported Script Commands:

navigate
exec (execAndWait)
block
sleep
logData
combineSteps
setEventName
setUserAgent
setBrowserSize
setViewportSize
setDeviceScaleFactor
setActivityTimeout
setTimeout
blockDomains
blockDomainsExcept
setDns
setDnsName
setHeader
addHeader (can not add multiple values for the same header, effectively the same as setHeader)
resetHeaders
setCookie
setABM
click (clickAndWait)
selectValue
sendClick
setInnerHTML
setInnerText
setValue
submitForm



Not yet supported (actively being worked on)

Browser installs/updates
Windows general cleanup/health (temp files, downloads, killing processes, etc)
Script Commands:

firefoxPref



Not Supported (no plans to implement)

Script Commands:

sendKeyDown
setDOMElement
waitForComplete
overrideHostUrl
ignoreErrors
logErrors
loadFile
loadVariables
minInterval
endInterval
expireCache
requiredRequest
setDOMRequest
waitForJSDone (change semantics to console log message)
overrideHost (depends on support being added to dev tools)
if/else/endif



",83
facebookincubator/LogDevice,C++,"LogDevice

LogDevice is a scalable and fault tolerant distributed log system. While a
file-system stores and serves data organized as files, a log system stores and
delivers data organized as logs. The log can be viewed as a record-oriented,
append-only, and trimmable file.
LogDevice is designed from the ground up to serve many types of logs with high
reliability and efficiency at scale. It is also highly tunable allowing each use
case to be optimized for the right set of trade-offs in the durability-efficiency
and consistency-availability space. Here are some examples of workloads supported
by LogDevice:

Write-ahead logging for durability
Transaction logging in a distributed database
Event logging
Stream processing
ML training pipelines
Replicated state machines
Journals of deferred work items

for full documentation, please visit the logdevice.io website
Requirements
LogDevice is supported on Ubuntu 18.04 (Bionic Beaver) only. However, it should
be possible to build it on any other Linux distribution without significant
challenges.
Building LogDevice

Building

Full documentation

Introduction
Cluster Configuration Guide
LogDevice API
LogDevice Shell
LDQuery

Join the LogDevice community

Website: logdevice.io
Facebook: LogDevice Users Group

See the CONTRIBUTING file for how to help out.
License
LogDevice is BSD licensed, as found in the LICENSE file.
",1488
facebookincubator/fizz,C++,"



Fizz is a TLS 1.3 implementation.
Fizz currently supports TLS 1.3 drafts 28, 26 (both wire-compatible with the
final specification), and 23. All major handshake modes are supported, including
PSK resumption, early data, client authentication, and HelloRetryRequest.
More background and details are available on the
Facebook Code Blog.
Dependencies
Fizz largely depends on three libraries: folly,
OpenSSL, and libsodium.
Source Layout

fizz/crypto:   Cryptographic primitive implementations (most are wrapping
OpenSSL or libsodium)
fizz/record:   TLS 1.3 record layer parsing
fizz/protocol: Common protocol code shared between client and server
fizz/client:   Client protocol implementation
fizz/server:   Server protocol implementation
fizz/tool:     Example CLI app source

Design
The core protocol implementations are in ClientProtocol and ServerProtocol.
FizzClientContext and FizzServerContext provide configuration options.
FizzClient and FizzServer (which both inherit from FizzBase) provide
applications with an interface to interact with the state machine.
FizzClient/FizzServer receives events from the application layer, invokes the
correct event handler, and invokes the application ActionVisitor to process the
actions.
AsyncFizzClient and AsyncFizzServer provide implementations of the folly
AsyncTransportWrapper interface. They own an underlying transport (for example
AsyncSocket) and perform the TLS handshake and encrypt/decrypt application
data.
Features
Fizz has several important features needed from a modern TLS library.
Performance
Fizz supports scatter/gather IO by default via folly's IOBufs, and will encrypt
data in-place whenever possible, saving memcpys. Due to this and several
other optimizations, we found in our load balancer benchmarks that Fizz has 10%
higher throughput than our prior SSL library which uses folly's
AsyncSSLSocket.
Fizz also consumes less memory per connection than AsyncSSLSocket.
Async by default
Fizz has asynchronous APIs to be able to offload functions like certificate
signing and ticket decryption. The API is based on folly's
Futures for painless
async programming.
TLS features
Fizz supports APIs like exported keying material as well as zero-copy APIs
needed to use TLS in other protocols like QUIC.
Secure design abstractions
Fizz is built on a custom state machine which uses the power of the C++ type system
to treat states and actions as types of their own. As the code changes, this allows us
to catch invalid state transitions as compilation errors instead of runtime errors and
helps us move fast.
Sample Applications
Fizz includes an example program that showcases the basic client/server functionality
supported by Fizz. The binary is called fizz and it has similar usage to the
openssl or bssl commands.
For example, to start a TLS server on port 443 with a specified cert:
fizz server -accept 443 -cert foo.pem -key foo.key

Then, on the same host, you can connect with:
fizz client -connect localhost:443

Both ends will echo whatever data they receive and send any terminal input to the
peer. Hitting CTRL+D on either end will terminate the connection.
The source code for this program can be found under fizz/tool.
Building
Ubuntu 16.04 LTS
To begin, you should install the dependencies we need for build. This largely
consists of folly's dependencies, as well as
libsodium.
sudo apt-get install \
    g++ \
    cmake \
    libboost-all-dev \
    libevent-dev \
    libdouble-conversion-dev \
    libgoogle-glog-dev \
    libgflags-dev \
    libiberty-dev \
    liblz4-dev \
    liblzma-dev \
    libsnappy-dev \
    make \
    zlib1g-dev \
    binutils-dev \
    libjemalloc-dev \
    libssl-dev \
    pkg-config \
    libsodium-dev

Then, build and install folly:
git clone https://github.com/facebook/folly
mkdir folly/build_ && cd folly/build_
cmake ..
make -j $(nproc)
sudo make install

And lastly, build and install fizz.
cd ../..
git clone https://github.com/facebookincubator/fizz
mkdir fizz/build_ && cd fizz/build_
cmake ../fizz
make -j $(nproc)
sudo make install

Building on Mac
The following instructions were tested on MacOS High Sierra
with Xcode 9.4.1. They should work with later Xcode versions as well.
Run the helper script from within the fizz subdirectory. The helper
script assumes that you have homebrew installed and are using homebrew
as your package manager. To install homebrew use the instructions on
the homebrew website.
It will install and link the required dependencies and also build folly.
This may take several minutes the first time.
cd fizz
./mac-build.sh

After building, the directory out/ will contain the libraries as well as
out/bin will contain the ClientSocket and ServerSocket binaries.
Running it again will be faster and only rebuild fizz.
You can also install both fizz as well as folly to a custom directory
using the build script, by supplying an INSTALL_PREFIX env var.
INSTALL_PREFIX=/usr/local ./mac-build.sh

You might need to run the script as root to install to certain directories.
Contributing
We'd love to have your help in making Fizz better. If you're interested, please
read our guide to guide to contributing
License
Fizz is BSD licensed, as found in the LICENSE file.
Reporting and Fixing Security Issues
Please do not open GitHub issues or pull requests - this makes the problem
immediately visible to everyone, including malicious actors. Security issues in
Fizz can be safely reported via Facebook's Whitehat Bug Bounty program:
https://www.facebook.com/whitehat
Facebook's security team will triage your report and determine whether or not is
it eligible for a bounty under our program.
",710
AzureAD/azure-activedirectory-identitymodel-extensions-for-dotnet,C#,"Windows Azure Active Directory IdentityModel Extensions for .Net
IdentityModel Extensions for .Net provide assemblies that are interesting for web developers that wish to use federated identity providers for establishing the callers identity.
Versions
Current version - 5.2.4
Minimum recommended version - 5.2.2
You can find the release notes for each version here.
Security Vulnerability in Microsoft.IdentityModel.Tokens 5.1.0
IdentityModel Extensions library Microsoft.IdentityModel.Tokens has a known security vulnerability affecting version 5.1.0. Please update to >= 5.1.1 immediately. An updated package is available on NuGet. For more details, see the security notice.
Usage
IdentityModel Extensions for .NET 5 has now been released. If you are using IdentityModel Extensions with ASP.NET, the following combinations are supported:

IdentityModel Extensions for .NET 5.x, ASP.NET Core 1.x, ASP.NET Core 2.x, Katana 4.x
IdentityModel Extensions for .NET 4.x, ASP.NET 4, Katana 3.x
All other combinations aren't supported.

For more details see Migration notes here
Samples and Documentation
The scenarios supported by IdentityModel extensions for .NET are described in Scenarios. The libraries are in particular used part of ASP.NET security to validate tokens in ASP.NET Web Apps and Web APIs. To learn more about token validation, and find samples, see:

Azure Active Directory with ASP.NET Core
Developing ASP.NET Apps with Azure Active Directory
Validating tokens
more generally, the librarie's Wiki
the reference documentation

Community Help and Support
We leverage Stack Overflow to work with the community on supporting Azure Active Directory and its SDKs, including this one! We highly recommend you ask your questions on Stack Overflow (we're all on there!) Also browse existing issues to see if someone has had your question before.
We recommend you use the ""adal"" tag so we can see it! Here is the latest Q&A on Stack Overflow for ADAL: http://stackoverflow.com/questions/tagged/adal
Security Reporting
If you find a security issue with our libraries or services please report it to secure@microsoft.com with as much detail as possible. Your submission may be eligible for a bounty through the Microsoft Bounty program. Please do not post security issues to GitHub Issues or any other public site. We will contact you shortly upon receiving the information. We encourage you to get notifications of when security incidents occur by visiting this page and subscribing to Security Advisory Alerts.
Contributing
All code is licensed under the MIT license and we triage actively on GitHub. We enthusiastically welcome contributions and feedback. See Contributing.md for guidelines, branch information, build instructions, and legalese.
License
Copyright (c) Microsoft Corporation.  All rights reserved. Licensed under the MIT License (the ""License"");
We Value and Adhere to the Microsoft Open Source Code of Conduct
This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.
",521
opendistro-for-elasticsearch/opendistro-build,Python,"Open Distro for Elasticsearch build
This repo contains the scripts for building Open Distro for Elasticsearch & Kibana Docker images and packages for Linux distributions (RPM & DEB).
Contributing
Open Distro for Elasticsearch is and will remain 100% open source under the Apache 2.0 license. As the project grows, we invite you to join the project and contribute. We want to make it easy for you to get started and remove friction—no lengthy Contributor License Agreement — so you can focus on writing great code.
Questions
If you have any questions, please join our community forum here
Issues
File any issues here.
",5
fastly/lucet,Rust,"Lucet   
Lucet is a native WebAssembly compiler and runtime. It is designed to safely
execute untrusted WebAssembly programs inside your application.
Check out our announcement post on the Fastly
blog.
Lucet uses, and is developed in collaboration with, Mozilla's
Cranelift code generator.
Lucet powers Fastly's Terrarium platform.

Status
Lucet supports running WebAssembly programs written in C (via clang), Rust,
and AssemblyScript. It does not yet support the entire WebAssembly spec, but
full support is coming in the near future.
Lucet's runtime currently only supports x86-64 based Linux systems, with
experimental support for macOS.
Contents
lucetc
lucetc is the Lucet Compiler.
The Rust crate lucetc provides an executable lucetc. It compiles
WebAssembly modules (.wasm or .wat files) into native code (.o or .so
files).
lucet-runtime
lucet-runtime is the runtime for WebAssembly modules compiled through
lucetc. It is a Rust crate that provides the functionality to load modules
from shared object files, instantiate them, and call exported WebAssembly
functions. lucet-runtime manages the resources used by each WebAssembly
instance (linear memory & globals), and the exception mechanisms that detect
and recover from illegal operations.
The bulk of the library is defined in the child crate
lucet-runtime-internals.  The public API is exposed in lucet-runtime.  Test
suites are defined in the child crate lucet-runtime-tests. Many of these
tests invoke lucetc and the wasi-sdk tools.
lucet-runtime is usable as a Rust crate or as a C library. The C language
interface is found at lucet-runtime/include/lucet.h.
lucet-wasi
lucet-wasi is a crate providing runtime support for the WebAssembly System
Interface (WASI).  It can be used as a library to support
WASI in another application, or as an executable, lucet-wasi, to execute WASI
programs compiled through lucetc.
See ""Your first Lucet application"" for an
example that builds a C program and executes it with lucet-wasi.
For details on WASI's implementation, see
lucet-wasi/README.md.
lucet-wasi-sdk
wasi-sdk is a Cranelift project
that packages a build of the Clang toolchain, the WASI reference sysroot, and a
libc based on WASI syscalls. lucet-wasi-sdk is a Rust crate that provides
wrappers around these tools for building C programs into Lucet modules. We use
this crate to build test cases in lucet-runtime-tests and lucet-wasi.
lucet-module-data
lucet-module-data is a crate with data structure definitions and serialization
functions that we emit into shared objects with lucetc, and read with
lucet-runtime.
lucet-analyze
lucet-analyze is a Rust executable for inspecting the contents of a shared
object generated by lucetc.
lucet-idl
lucet-idl is a Rust executable that implements code generation via an
Interface Description Language (IDL).  The generated code provides zero-copy
accessor and constructor functions for datatypes that have the same
representation in both the WebAssembly guest program and the host program.
Functionality is incomplete at the time of writing, and not yet integrated with
other parts of the project.  Rust code generator, definition of import and
export function interfaces, and opaque type definitions are planned for the
near future.
lucet-spectest
lucet-spectest is a Rust crate that uses lucetc and lucet-runtime, as well
as the (external) wabt crate, to run the official WebAssembly spec test suite,
which is provided as a submodule in this directory. Lucet is not yet fully spec
compliant, and the implementation of lucet-spectest has not been maintained
very well during recent codebase evolutions. We expect to fix this up and reach
spec compliance in the near future.
lucet-builtins
lucet-builtins is a C library that provides optimized native versions of libc
primitives. lucetc can substitute the implementations defined in this library
for the WebAssembly implementations.
lucet-builtins/wasmonkey is the Rust crate that lucetc uses to transform
function definitions in a WebAssembly module into uses of an import function.
Vendor libraries
Lucet is tightly coupled to several upstream dependencies, and Lucet
development often requires making changes to these dependencies which are
submitted upstream once fully baked. To reduce friction in this development
cycle, we use git submodules to vendor these modules into the Lucet source
tree.
Cranelift
We keep the primary Cranelift project repository as a submodule at
/cranelift.
Cranelift provides the native code generator used by lucetc, and a ton of
supporting infrastructure.
Cranelift was previously known as Cretonne.  Project developers hang out in the
#cranelift channel on irc.mozilla.org:6697.
Faerie
faerie is a Rust crate for producing ELF files.  Faerie is used by Cranelift
(through the module system's cranelift-faerie backend) and also directly by
lucetc, for places where the cranelift-module API can't do everything we
need.
Tests
Most of the crates in this repository have some form of unit tests. In addition,
lucet-runtime/lucet-runtime-tests defines a number of integration tests for
the runtime, and lucet-wasi has a number of integration tests using WASI C
programs.
Benchmarks
We created the sightglass benchmarking tool to measure the runtime of C code
compiled through a standard native toolchain against the Lucet toolchain. It
is provided as a submodule at /sightglass.
Sightglass ships with a set of microbenchmarks called shootout. The scripts
to build the shootout tests with native and various versions of the Lucet
toolchain are in /benchmarks/shootout.
Furthermore, there is a suite of benchmarks of various Lucet runtime functions,
such as instance creation and teardown, in /benchmarks/lucet-benchmarks.
Development Environment
Operating System
Lucet is developed and tested on Linux and macOS. We expect it to work on any
POSIX system which supports shared libraries.
Dependencies
Lucet requires:

Stable Rust, and rustfmt. We typically track the latest stable release.
wasi-sdk, providing a Clang
toolchain with wasm-ld, the WASI reference sysroot, and a libc based on WASI
syscalls.
GNU Make, CMake, & various standard Unix utilities for the build system.

Getting started
The easiest way to get started with the Lucet toolchain is by using the provided
Docker-based development environment.
This repository includes a Dockerfile to build a complete environment for
compiling and running WebAssembly code with Lucet, but you shouldn't have to use
Docker commands directly. A set of shell scripts with the devenv_ prefix are
used to manage the container.
Setting up the environment


The Lucet repository uses git submodules. Make sure they are checked out
by running git submodule init && git submodule update.


Install and run the docker service. We do not support podman at this
time. On MacOS, Docker for
Mac is an option.


Once Docker is running, in a terminal, and at the root of the cloned
repository, run: source devenv_setenv.sh. (This command requires the
current shell to be zsh, ksh or bash). After a couple minutes, the
Docker image is built and a new container is run.


Check that new commands are now available:


lucetc --help
You're now all set!
Your first Lucet application
The devenv_setenv.sh shell script ensures the Lucet executables are available
in your shell. Under the hood, these commands are executed in the Docker
container.  The container has limited visibility into the host's filesystem - it
can only see files under the lucet repository.
Create a new work directory in the lucet directory:
mkdir -p src/hello

cd src/hello
Save the following C source code as hello.c:
#include <stdio.h>

int main(void)
{
    puts(""Hello world"");
    return 0;
}
Time to compile to WebAssembly! The development environment includes a version
of the Clang toolchain that is built to generate WebAssembly by default. The
related commands are accessible from your current shell, and are prefixed by
wasm32-unknown-wasi-.
For example, to create a WebAssembly module hello.wasm from hello.c:
wasm32-unknown-wasi-clang -Ofast -o hello.wasm hello.c
The next step is to use Lucet to build native x86_64 code from that
WebAssembly file:
lucetc-wasi -o hello.so hello.wasm
lucetc is the WebAssembly to native code compiler. The lucetc-wasi command
runs the same compiler, but automatically configures it to target WASI.
hello.so is created and ready to be run:
lucet-wasi hello.so
Additional shell commands

./devenv_build_container.sh rebuilds the container image. This is never
required unless you edit the Dockerfile.
./devenv_run.sh [<command>] [<arg>...] runs a command in the container. If
a command is not provided, an interactive shell is spawned. In this
container, Lucet tools are installed in /opt/lucet by default. The command
source /opt/lucet/bin/devenv_setenv.sh can be used to initialize the
environment.
./devenv_start.sh and ./devenv_stop.sh start and stop the container.

Security
The lucet project aims to provide support for secure execution of untrusted code. Security is achieved through a combination of lucet-supplied security controls and user-supplied security controls. See SECURITY.md for more information on the lucet security model.
Reporting Security Issues
The Lucet project team welcomes security reports and is committed to providing
prompt attention to security issues. Security issues should be reported
privately via Fastly’s security issue reporting
process. Remediation of
security vulnerabilities is prioritized. The project teams endeavors to
coordinate remediation with third-party stakeholders, and is committed to
transparency in the disclosure process.
",2087
facebookincubator/mvfst,C++,"
Introduction
mvfst (Pronounced move fast) is a client and server implementation of IETF QUIC protocol in C++ by Facebook. QUIC is a UDP based reliable, multiplexed transport protocol that will become an internet standard. The goal of mvfst is to build a performant implementation of the QUIC transport protocol that applications could adapt for use cases on both the internet and the data-center. mvfst has been tested at scale on android, iOS apps, as well as servers and has several features to support large scale deployments.
Features
Server features:

Multi-threaded UDP socket server with a thread local architecture to be able to scale to multi-core servers
Customizable Connection-Id routing. The default Connection-Id routing implementation integrates seamlessly with katran
APIs to enable zero-downtime restarts of servers, so that applications do not have to drop connections when restarting.
APIs to expose transport and server statistics for debuggability
Zero-Rtt connection establishment and customizable zero rtt path validation
Support for UDP Generic segmentation offloading (GSO) for faster UDP writes.

Client features:

Native happy eyeballs support between ipv4 and ipv6 so that applications do not need to implement it themselves
Pluggable congestion control and support for turning off congestion control to plug in application specific control algorithms
QUIC trace APIs to retrieve deep transport level stats.

Source Layout

quic/api:         Defines API that applications can use to interact with the QUIC transport layer.
quic/client:      Client transport implementation
quic/codec:       Read and write codec implementation for the protocol
quic/common:      Implementation of common utility functions
quic/congestion_control: Implementation of different congestion control algorithms such as Cubic and Copa
quic/flowcontrol: Implementations of flow control functions
quic/handshake:   Implementations cryptographic handshake layer
quic/happyeyeballs: Implementation of mechanism to race IPV4 and IPV6 connection and pick a winner
quic/logging:     Implementation of logging framework
quic/loss:        Implementations of different loss recovery algorithms
quic/samples:     Example client and server
quic/server:      Server transport implementation
quic/state:       Defines and implements both connection and stream level state artifacts and state machines

Dependencies
mvfst largely depends on two libraries: folly and fizz.
Building mvfst
Ubuntu 16+
To begin, you should install the dependencies we need for build. This largely
consists of dependencies from folly as well as
fizz.
sudo apt-get install         \
    g++                      \
    cmake                    \
    libboost-all-dev         \
    libevent-dev             \
    libdouble-conversion-dev \
    libgoogle-glog-dev       \
    libgflags-dev            \
    libiberty-dev            \
    liblz4-dev               \
    liblzma-dev              \
    libsnappy-dev            \
    make                     \
    zlib1g-dev               \
    binutils-dev             \
    libjemalloc-dev          \
    libssl-dev               \
    pkg-config               \
    libsodium-dev

Then, build and install folly and fizz
Alternatively, run the helper script build_helper.sh in this subdirectory.
It will install and link the required dependencies and also build folly and fizz.
This may take several minutes the first time.
./build_helper.sh

After building, the directory _build/ will contain the dependencies
(under _build/deps) whereas _build/build will contain all the
built libraries and binaries for mvfst.
You can also install mvfst as well as its dependencies folly and fizz
to a custom directory using the build script, by supplying an INSTALL_PREFIX
env var.
./build_helper.sh -i /usr/local

See ./build_helper.sh --help for more options
You might need to run the script as root to install to certain directories.
By default the build script build_helper.sh enables the building of test target (i.e. runs with -DBUILD_TEST=ON option). Since some of tests in mvfst require some test artifacts of Fizz, it is necessary to supply the path of the Fizz src directory (via option DFIZZ_PROJECT) to correctly build all test targets in mvfst.
Run a sample client and server
Building the test targets of mvfst (or via build_helper.sh) should automatically build the sample client and server binaries as well. You can then spin a simple echo server by running:
./quic/samples/echo -mode=server -host=<ip> -port=<port>

and to run a client:
./quic/samples/echo -mode=client  -host=<ip> -port=<port>

For more options, see
./quic/samples/echo --help

Contributing
We'd love to have your help in making mvfst better. If you're interested, please
read our guide to guide to contributing
Please also join our
slack to ask questions or start discussions.
License
mvfst is MIT licensed, as found in the LICENSE file.
API
The API should be considered in alpha. We can't predict all the use cases that
people will have, so we're waiting some time before declaring a more stable API.
We are open to have several different APIs for different constraints.
Reporting and Fixing Security Issues
Please do not open GitHub issues or pull requests - this makes the problem
immediately visible to everyone, including malicious actors. Security issues in
mvfst can be safely reported via Facebook's Whitehat Bug Bounty program:
https://www.facebook.com/whitehat
Facebook's security team will triage your report and determine whether or not is
it eligible for a bounty under our program.
",263
nats-io/nats-operator,Go,"NATS Operator



NATS Operator manages NATS clusters atop Kubernetes, automating their creation and administration.
Requirements

Kubernetes v1.10+.

Configuration reloading is only supported in Kubernetes v1.12+.
Authentication using service accounts is only supported in Kubernetes v1.12+ having the TokenRequest API enabled.



Introduction
NATS Operator provides a NatsCluster Custom Resources Definition (CRD) that models a NATS cluster.
This CRD allows for specifying the desired size and version for a NATS cluster, as well as several other advanced options:
apiVersion: nats.io/v1alpha2
kind: NatsCluster
metadata:
  name: example-nats-cluster
spec:
  size: 3
  version: ""1.4.0""
NATS Operator monitors creation/modification/deletion of NatsCluster resources and reacts by attempting to perform the any necessary operations on the associated NATS clusters in order to align their current status with the desired one.
Installing
NATS Operator supports two different operation modes:

Namespace-scoped (classic): NATS Operator manages NatsCluster resources on the Kubernetes namespace where it is deployed.
Cluster-scoped (experimental): NATS Operator manages NatsCluster resources across all namespaces in the Kubernetes cluster.

The operation mode must be chosen when installing NATS Operator and cannot be changed later.
Namespace-scoped installation
To perform a namespace-scoped installation of NATS Operator in the Kubernetes cluster pointed at by the current context, you may run:
$ kubectl apply -f https://github.com/nats-io/nats-operator/releases/download/v0.4.5/00-prereqs.yaml
$ kubectl apply -f https://github.com/nats-io/nats-operator/releases/download/v0.4.5/10-deployment.yaml
This will, by default, install NATS Operator in the default namespace and observe NatsCluster resources created in the default namespace, alone.
In order to install in a different namespace, you must first create said namespace and edit the manifests above in order to specify its name wherever necessary.
WARNING: To perform multiple namespace-scoped installations of NATS Operator, you must manually edit the nats-operator-binding cluster role binding in deploy/00-prereqs.yaml file in order to add all the required service accounts.
Failing to do so may cause all NATS Operator instances to malfunction.
WARNING: When performing a namespace-scoped installation of NATS Operator, you must make sure that all other namespace-scoped installations that may exist in the Kubernetes cluster share the same version.
Installing different versions of NATS Operator in the same Kubernetes cluster may cause unexpected behavior as the schema of the CRDs which NATS Operator registers may change between versions.
Alternatively, you may use Helm to perform a namespace-scoped installation of NATS Operator.
To do so you may go to helm/nats-operator and use the Helm charts found in that repo.
Cluster-scoped installation (experimental)
Cluster-scoped installations of NATS Operator must live in the nats-io namespace.
This namespace must be created beforehand:
$ kubectl create ns nats-io
Then, you must manually edit the manifests in deployment/ in order to reference the nats-io namespace and to enable the ClusterScoped feature gate in the NATS Operator deployment.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nats-operator
  namespace: nats-io
spec:
  (...)
    spec:
      containers:
      - name: nats-operator
        (...)
        args:
        - nats-operator
        - --feature-gates=ClusterScoped=true
        (...)
Once you have done this, you may install NATS Operator by running:
$ kubectl apply -f https://raw.githubusercontent.com/nats-io/nats-operator/master/deploy/00-prereqs.yaml
$ kubectl apply -f https://raw.githubusercontent.com/nats-io/nats-operator/master/deploy/10-deployment.yaml
WARNING: When performing a cluster-scoped installation of NATS Operator, you must make sure that there are no other deployments of NATS Operator in the Kubernetes cluster.
If you have a previous installation of NATS Operator, you must uninstall it before performing a cluster-scoped installation of NATS Operator.
Creating a NATS cluster
Once NATS Operator has been installed, you will be able to confirm that two new CRDs have been registered in the cluster:
$ kubectl get crd
NAME                       CREATED AT
natsclusters.nats.io       2019-01-11T17:16:36Z
natsserviceroles.nats.io   2019-01-11T17:16:40Z
To create a NATS cluster, you must create a NatsCluster resource representing the desired status of the cluster.
For example, to create a 3-node NATS cluster you may run:
$ cat <<EOF | kubectl create -f -
apiVersion: nats.io/v1alpha2
kind: NatsCluster
metadata:
  name: example-nats-cluster
spec:
  size: 3
  version: ""1.3.0""
EOF
NATS Operator will react to the creation of such a resource by creating three NATS pods.
These pods will keep being monitored (and replaced in case of failure) by NATS Operator for as long as this NatsCluster resource exists.
Listing NATS clusters
To list all the NATS clusters:
$ kubectl get nats --all-namespaces
NAMESPACE   NAME                   AGE
default     example-nats-cluster   2m
TLS support
By using a pair of opaque secrets (one for the clients and then another for the routes),
it is possible to set TLS for the communication between the clients and also for the
transport between the routes:
apiVersion: ""nats.io/v1alpha2""
kind: ""NatsCluster""
metadata:
  name: ""nats""
spec:
  # Number of nodes in the cluster
  size: 3
  version: ""1.3.0""

  tls:
    # Certificates to secure the NATS client connections:
    serverSecret: ""nats-clients-tls""

    # Certificates to secure the routes.
    routesSecret: ""nats-routes-tls""
In order for TLS to be properly established between the nodes, it is
necessary to create a wildcard certificate that matches the subdomain
created for the service from the clients and the one for the routes.
By default, the routesSecret has to provide the files: ca.pem, route-key.pem, route.pem,
for the CA, server private and public key respectively.
$ kubectl create secret generic nats-routes-tls --from-file=ca.pem --from-file=route-key.pem --from-file=route.pem

Similarly, by default the serverSecret has to provide the files: ca.pem, server-key.pem, and server.pem
for the CA, server private key and public key used to secure the connection
with the clients.
$ kubectl create secret generic nats-clients-tls --from-file=ca.pem --from-file=server-key.pem --from-file=server.pem

NATS also supports kubernetes.io/tls secrets (like the ones managed by cert-manager) and any secrets containing a CA, private and public keys with arbitrary names.
It is possible to overwrite the default names as follows:
apiVersion: ""nats.io/v1alpha2""
kind: ""NatsCluster""
metadata:
  name: ""nats""
spec:
  # Number of nodes in the cluster
  size: 3
  version: ""1.3.0""

  tls:
    # Certificates to secure the NATS client connections:
    serverSecret: ""nats-clients-tls""
    # Name of the CA in serverSecret
    serverSecretCAFileName: ""ca.crt""
    # Name of the key in serverSecret
    serverSecretKeyFileName: ""tls.key""
    # Name of the certificate in serverSecret
    serverSecretCertFileName: ""tls.crt""

    # Certificates to secure the routes.
    routesSecret: ""nats-routes-tls""
    # Name of the CA in routesSecret
    routesSecretCAFileName: ""ca.crt""
    # Name of the key in routesSecret
    routesSecretKeyFileName: ""tls.key""
    # Name of the certificate in routesSecret
    routesSecretCertFileName: ""tls.crt""
Cert-Manager
If cert-manager is available in your cluster, you can easily generate TLS certificates for NATS as follows:
Create a self-signed cluster issuer (or namespace-bound issuer) to create NATS' CA certificate:
apiVersion: certmanager.k8s.io/v1alpha1
kind: ClusterIssuer
metadata:
  name: selfsigning
spec:
  selfSigned: {}
Create your NATS cluster's CA certificate using the new selfsigning issuer:
apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: nats-ca
spec:
  secretName: nats-ca
  duration: 8736h # 1 year
  renewBefore: 240h # 10 days
  issuerRef:
    name: selfsigning
    kind: ClusterIssuer
  commonName: nats-ca
  organization:
  - Your organization
  isCA: true
Create your NATS cluster issuer based on the new nats-ca CA:
apiVersion: certmanager.k8s.io/v1alpha1
kind: Issuer
metadata:
  name: nats-ca
spec:
  ca:
    secretName: nats-ca
Create your NATS cluster's server certificate (assuming NATS is running in the nats-io namespace, otherwise, set the commonName and dnsNames fields appropriately):
apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: nats-server-tls
spec:
  secretName: nats-server-tls
  duration: 2160h # 90 days
  renewBefore: 240h # 10 days
  issuerRef:
    name: nats-ca
    kind: Issuer
  organization:
  - Your organization
  commonName: nats.nats-io.svc.cluster.local
  dnsNames:
  - nats.nats-io.svc
Create your NATS cluster's routes certificate (assuming NATS is running in the nats-io namespace, otherwise, set the commonName and dnsNames fields appropriately):
apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: nats-routes-tls
spec:
  secretName: nats-routes-tls
  duration: 2160h # 90 days
  renewBefore: 240h # 10 days
  issuerRef:
    name: nats-ca
    kind: Issuer
  organization:
  - Your organization
  commonName: ""*.nats-mgmt.nats-io.svc.cluster.local""
  dnsNames:
  - ""*.nats-mgmt.nats-io.svc""
Authorization

Using ServiceAccounts
The NATS Operator can define permissions based on Roles by using any present ServiceAccount in a namespace.
This feature requires a Kubernetes v1.12+ cluster having the TokenRequest API enabled.
To try this feature using minikube v0.30.0+, you can configure it to start as follows:
$ minikube start \
  --extra-config=apiserver.service-account-signing-key-file=/var/lib/minikube/certs/apiserver.key \
  --extra-config=apiserver.service-account-issuer=api \
  --extra-config=apiserver.service-account-api-audiences=api \
  --kubernetes-version=v1.12.4
Please note that availability of this feature across Kubernetes offerings may vary widely.
ServiceAccounts integration can then be enabled by setting the
enableServiceAccounts flag to true in the NatsCluster configuration.
apiVersion: nats.io/v1alpha2
kind: NatsCluster
metadata:
  name: example-nats
spec:
  size: 3
  version: ""1.3.0""

  pod:
    # NOTE: Only supported in Kubernetes v1.12+.
    enableConfigReload: true
  auth:
    # NOTE: Only supported in Kubernetes v1.12+ clusters having the ""TokenRequest"" API enabled.
    enableServiceAccounts: true
Permissions for a ServiceAccount can be set by creating a
NatsServiceRole for that account.  In the example below, there are
two accounts, one is an admin user that has more permissions.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nats-admin-user
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nats-user
---
apiVersion: nats.io/v1alpha2
kind: NatsServiceRole
metadata:
  name: nats-user
  namespace: nats-io

  # Specifies which NATS cluster will be mapping this account.
  labels:
    nats_cluster: example-nats
spec:
  permissions:
    publish: [""foo.*"", ""foo.bar.quux""]
    subscribe: [""foo.bar""]
---
apiVersion: nats.io/v1alpha2
kind: NatsServiceRole
metadata:
  name: nats-admin-user
  namespace: nats-io
  labels:
    nats_cluster: example-nats
spec:
  permissions:
    publish: ["">""]
    subscribe: ["">""]
The above will create two different Secrets which can then be mounted as volumes
for a Pod.
$ kubectl -n nats-io get secrets
NAME                                       TYPE          DATA      AGE
...
nats-admin-user-example-nats-bound-token   Opaque        1         43m
nats-user-example-nats-bound-token         Opaque        1         43m
An example of mounting the secret in a Pod can be found below:
apiVersion: v1
kind: Pod
metadata:
  name: nats-user-pod
  labels:
    nats_cluster: example-nats
spec:
  volumes:
    - name: ""token""
      projected:
        sources:
        - secret:
            name: ""nats-user-example-nats-bound-token""
            items:
              - key: token
                path: ""token""
  restartPolicy: Never
  containers:
    - name: nats-ops
      command: [""/bin/sh""]
      image: ""wallyqs/nats-ops:latest""
      tty: true
      stdin: true
      stdinOnce: true
      volumeMounts:
      - name: ""token""
        mountPath: ""/var/run/secrets/nats.io""
Then within the Pod the token can be used to authenticate against
the server using the created token.
$ kubectl -n nats-io attach -it nats-user-pod

/go # nats-sub -s nats://nats-user:`cat /var/run/secrets/nats.io/token`@example-nats:4222 hello.world
Listening on [hello.world]
^C
/go # nats-sub -s nats://nats-admin-user:`cat /var/run/secrets/nats.io/token`@example-nats:4222 hello.world
Can't connect: nats: authorization violation
Using a single secret with the explicit configuration.
Authorization can also be set for the server by using a secret
where the permissions are defined in JSON:
{
  ""users"": [
    { ""username"": ""user1"", ""password"": ""secret1"" },
    { ""username"": ""user2"", ""password"": ""secret2"",
      ""permissions"": {
	""publish"": [""hello.*""],
	""subscribe"": [""hello.world""]
      }
    }
  ],
  ""default_permissions"": {
    ""publish"": [""SANDBOX.*""],
    ""subscribe"": [""PUBLIC.>""]
  }
}
Example of creating a secret to set the permissions:
kubectl create secret generic nats-clients-auth --from-file=clients-auth.json
Now when creating a NATS cluster it is possible to set the permissions as
in the following example:
apiVersion: ""nats.io/v1alpha2""
kind: ""NatsCluster""
pmetadata:
  name: ""example-nats-auth""
spec:
  size: 3
  version: ""1.1.0""

  auth:
    # Definition in JSON of the users permissions
    clientsAuthSecret: ""nats-clients-auth""

    # How long to wait for authentication
    clientsAuthTimeout: 5

Configuration Reload
On Kubernetes v1.12+ clusters it is possible to enable on-the-fly reloading of configuration for the servers that are part of the cluster.
This can also be combined with the authorization support, so in case the user permissions change, then the servers will reload and apply the new permissions.
apiVersion: ""nats.io/v1alpha2""
kind: ""NatsCluster""
metadata:
  name: ""example-nats-auth""
spec:
  size: 3
  version: ""1.1.0""

  pod:
    # Enable on-the-fly NATS Server config reload
    # NOTE: Only supported in Kubernetes v1.12+.
    enableConfigReload: true

    # Possible to customize version of reloader image
    reloaderImage: connecteverything/nats-server-config-reloader
    reloaderImageTag: ""0.2.2-v1alpha2""
    reloaderImagePullPolicy: ""IfNotPresent""
  auth:
    # Definition in JSON of the users permissions
    clientsAuthSecret: ""nats-clients-auth""

    # How long to wait for authentication
    clientsAuthTimeout: 5
Connecting operated NATS clusters to external NATS clusters
By using the extraRoutes field on the spec you can make the operated
NATS cluster create routes against clusters outside of Kubernetes:
apiVersion: ""nats.io/v1alpha2""
kind: ""NatsCluster""
metadata:
  name: ""nats""
spec:
  size: 3
  version: ""1.4.1""

  extraRoutes:
    - route: ""nats://nats-a.example.com:6222""
    - route: ""nats://nats-b.example.com:6222""
    - route: ""nats://nats-c.example.com:6222""
It is also possible to connect to another operated NATS cluster as follows:
apiVersion: ""nats.io/v1alpha2""
kind: ""NatsCluster""
metadata:
  name: ""nats-v2-2""
spec:
  size: 3
  version: ""1.4.1""

  extraRoutes:
    - cluster: ""nats-v2-1""
Development
Building the Docker Image
To build the nats-operator Docker image:
$ docker build -f docker/operator/Dockerfile . <image:tag>
To build the nats-server-config-reloader:
$ docker build -f docker/reloader/Dockerfile . <image:tag>
You'll need Docker 17.06.0-ce or higher.
",274
facebookincubator/mvfst,C++,"
Introduction
mvfst (Pronounced move fast) is a client and server implementation of IETF QUIC protocol in C++ by Facebook. QUIC is a UDP based reliable, multiplexed transport protocol that will become an internet standard. The goal of mvfst is to build a performant implementation of the QUIC transport protocol that applications could adapt for use cases on both the internet and the data-center. mvfst has been tested at scale on android, iOS apps, as well as servers and has several features to support large scale deployments.
Features
Server features:

Multi-threaded UDP socket server with a thread local architecture to be able to scale to multi-core servers
Customizable Connection-Id routing. The default Connection-Id routing implementation integrates seamlessly with katran
APIs to enable zero-downtime restarts of servers, so that applications do not have to drop connections when restarting.
APIs to expose transport and server statistics for debuggability
Zero-Rtt connection establishment and customizable zero rtt path validation
Support for UDP Generic segmentation offloading (GSO) for faster UDP writes.

Client features:

Native happy eyeballs support between ipv4 and ipv6 so that applications do not need to implement it themselves
Pluggable congestion control and support for turning off congestion control to plug in application specific control algorithms
QUIC trace APIs to retrieve deep transport level stats.

Source Layout

quic/api:         Defines API that applications can use to interact with the QUIC transport layer.
quic/client:      Client transport implementation
quic/codec:       Read and write codec implementation for the protocol
quic/common:      Implementation of common utility functions
quic/congestion_control: Implementation of different congestion control algorithms such as Cubic and Copa
quic/flowcontrol: Implementations of flow control functions
quic/handshake:   Implementations cryptographic handshake layer
quic/happyeyeballs: Implementation of mechanism to race IPV4 and IPV6 connection and pick a winner
quic/logging:     Implementation of logging framework
quic/loss:        Implementations of different loss recovery algorithms
quic/samples:     Example client and server
quic/server:      Server transport implementation
quic/state:       Defines and implements both connection and stream level state artifacts and state machines

Dependencies
mvfst largely depends on two libraries: folly and fizz.
Building mvfst
Ubuntu 16+
To begin, you should install the dependencies we need for build. This largely
consists of dependencies from folly as well as
fizz.
sudo apt-get install         \
    g++                      \
    cmake                    \
    libboost-all-dev         \
    libevent-dev             \
    libdouble-conversion-dev \
    libgoogle-glog-dev       \
    libgflags-dev            \
    libiberty-dev            \
    liblz4-dev               \
    liblzma-dev              \
    libsnappy-dev            \
    make                     \
    zlib1g-dev               \
    binutils-dev             \
    libjemalloc-dev          \
    libssl-dev               \
    pkg-config               \
    libsodium-dev

Then, build and install folly and fizz
Alternatively, run the helper script build_helper.sh in this subdirectory.
It will install and link the required dependencies and also build folly and fizz.
This may take several minutes the first time.
./build_helper.sh

After building, the directory _build/ will contain the dependencies
(under _build/deps) whereas _build/build will contain all the
built libraries and binaries for mvfst.
You can also install mvfst as well as its dependencies folly and fizz
to a custom directory using the build script, by supplying an INSTALL_PREFIX
env var.
./build_helper.sh -i /usr/local

See ./build_helper.sh --help for more options
You might need to run the script as root to install to certain directories.
By default the build script build_helper.sh enables the building of test target (i.e. runs with -DBUILD_TEST=ON option). Since some of tests in mvfst require some test artifacts of Fizz, it is necessary to supply the path of the Fizz src directory (via option DFIZZ_PROJECT) to correctly build all test targets in mvfst.
Run a sample client and server
Building the test targets of mvfst (or via build_helper.sh) should automatically build the sample client and server binaries as well. You can then spin a simple echo server by running:
./quic/samples/echo -mode=server -host=<ip> -port=<port>

and to run a client:
./quic/samples/echo -mode=client  -host=<ip> -port=<port>

For more options, see
./quic/samples/echo --help

Contributing
We'd love to have your help in making mvfst better. If you're interested, please
read our guide to guide to contributing
Please also join our
slack to ask questions or start discussions.
License
mvfst is MIT licensed, as found in the LICENSE file.
API
The API should be considered in alpha. We can't predict all the use cases that
people will have, so we're waiting some time before declaring a more stable API.
We are open to have several different APIs for different constraints.
Reporting and Fixing Security Issues
Please do not open GitHub issues or pull requests - this makes the problem
immediately visible to everyone, including malicious actors. Security issues in
mvfst can be safely reported via Facebook's Whitehat Bug Bounty program:
https://www.facebook.com/whitehat
Facebook's security team will triage your report and determine whether or not is
it eligible for a bounty under our program.
",263
nats-io/nats-operator,Go,"NATS Operator



NATS Operator manages NATS clusters atop Kubernetes, automating their creation and administration.
Requirements

Kubernetes v1.10+.

Configuration reloading is only supported in Kubernetes v1.12+.
Authentication using service accounts is only supported in Kubernetes v1.12+ having the TokenRequest API enabled.



Introduction
NATS Operator provides a NatsCluster Custom Resources Definition (CRD) that models a NATS cluster.
This CRD allows for specifying the desired size and version for a NATS cluster, as well as several other advanced options:
apiVersion: nats.io/v1alpha2
kind: NatsCluster
metadata:
  name: example-nats-cluster
spec:
  size: 3
  version: ""1.4.0""
NATS Operator monitors creation/modification/deletion of NatsCluster resources and reacts by attempting to perform the any necessary operations on the associated NATS clusters in order to align their current status with the desired one.
Installing
NATS Operator supports two different operation modes:

Namespace-scoped (classic): NATS Operator manages NatsCluster resources on the Kubernetes namespace where it is deployed.
Cluster-scoped (experimental): NATS Operator manages NatsCluster resources across all namespaces in the Kubernetes cluster.

The operation mode must be chosen when installing NATS Operator and cannot be changed later.
Namespace-scoped installation
To perform a namespace-scoped installation of NATS Operator in the Kubernetes cluster pointed at by the current context, you may run:
$ kubectl apply -f https://github.com/nats-io/nats-operator/releases/download/v0.4.5/00-prereqs.yaml
$ kubectl apply -f https://github.com/nats-io/nats-operator/releases/download/v0.4.5/10-deployment.yaml
This will, by default, install NATS Operator in the default namespace and observe NatsCluster resources created in the default namespace, alone.
In order to install in a different namespace, you must first create said namespace and edit the manifests above in order to specify its name wherever necessary.
WARNING: To perform multiple namespace-scoped installations of NATS Operator, you must manually edit the nats-operator-binding cluster role binding in deploy/00-prereqs.yaml file in order to add all the required service accounts.
Failing to do so may cause all NATS Operator instances to malfunction.
WARNING: When performing a namespace-scoped installation of NATS Operator, you must make sure that all other namespace-scoped installations that may exist in the Kubernetes cluster share the same version.
Installing different versions of NATS Operator in the same Kubernetes cluster may cause unexpected behavior as the schema of the CRDs which NATS Operator registers may change between versions.
Alternatively, you may use Helm to perform a namespace-scoped installation of NATS Operator.
To do so you may go to helm/nats-operator and use the Helm charts found in that repo.
Cluster-scoped installation (experimental)
Cluster-scoped installations of NATS Operator must live in the nats-io namespace.
This namespace must be created beforehand:
$ kubectl create ns nats-io
Then, you must manually edit the manifests in deployment/ in order to reference the nats-io namespace and to enable the ClusterScoped feature gate in the NATS Operator deployment.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nats-operator
  namespace: nats-io
spec:
  (...)
    spec:
      containers:
      - name: nats-operator
        (...)
        args:
        - nats-operator
        - --feature-gates=ClusterScoped=true
        (...)
Once you have done this, you may install NATS Operator by running:
$ kubectl apply -f https://raw.githubusercontent.com/nats-io/nats-operator/master/deploy/00-prereqs.yaml
$ kubectl apply -f https://raw.githubusercontent.com/nats-io/nats-operator/master/deploy/10-deployment.yaml
WARNING: When performing a cluster-scoped installation of NATS Operator, you must make sure that there are no other deployments of NATS Operator in the Kubernetes cluster.
If you have a previous installation of NATS Operator, you must uninstall it before performing a cluster-scoped installation of NATS Operator.
Creating a NATS cluster
Once NATS Operator has been installed, you will be able to confirm that two new CRDs have been registered in the cluster:
$ kubectl get crd
NAME                       CREATED AT
natsclusters.nats.io       2019-01-11T17:16:36Z
natsserviceroles.nats.io   2019-01-11T17:16:40Z
To create a NATS cluster, you must create a NatsCluster resource representing the desired status of the cluster.
For example, to create a 3-node NATS cluster you may run:
$ cat <<EOF | kubectl create -f -
apiVersion: nats.io/v1alpha2
kind: NatsCluster
metadata:
  name: example-nats-cluster
spec:
  size: 3
  version: ""1.3.0""
EOF
NATS Operator will react to the creation of such a resource by creating three NATS pods.
These pods will keep being monitored (and replaced in case of failure) by NATS Operator for as long as this NatsCluster resource exists.
Listing NATS clusters
To list all the NATS clusters:
$ kubectl get nats --all-namespaces
NAMESPACE   NAME                   AGE
default     example-nats-cluster   2m
TLS support
By using a pair of opaque secrets (one for the clients and then another for the routes),
it is possible to set TLS for the communication between the clients and also for the
transport between the routes:
apiVersion: ""nats.io/v1alpha2""
kind: ""NatsCluster""
metadata:
  name: ""nats""
spec:
  # Number of nodes in the cluster
  size: 3
  version: ""1.3.0""

  tls:
    # Certificates to secure the NATS client connections:
    serverSecret: ""nats-clients-tls""

    # Certificates to secure the routes.
    routesSecret: ""nats-routes-tls""
In order for TLS to be properly established between the nodes, it is
necessary to create a wildcard certificate that matches the subdomain
created for the service from the clients and the one for the routes.
By default, the routesSecret has to provide the files: ca.pem, route-key.pem, route.pem,
for the CA, server private and public key respectively.
$ kubectl create secret generic nats-routes-tls --from-file=ca.pem --from-file=route-key.pem --from-file=route.pem

Similarly, by default the serverSecret has to provide the files: ca.pem, server-key.pem, and server.pem
for the CA, server private key and public key used to secure the connection
with the clients.
$ kubectl create secret generic nats-clients-tls --from-file=ca.pem --from-file=server-key.pem --from-file=server.pem

NATS also supports kubernetes.io/tls secrets (like the ones managed by cert-manager) and any secrets containing a CA, private and public keys with arbitrary names.
It is possible to overwrite the default names as follows:
apiVersion: ""nats.io/v1alpha2""
kind: ""NatsCluster""
metadata:
  name: ""nats""
spec:
  # Number of nodes in the cluster
  size: 3
  version: ""1.3.0""

  tls:
    # Certificates to secure the NATS client connections:
    serverSecret: ""nats-clients-tls""
    # Name of the CA in serverSecret
    serverSecretCAFileName: ""ca.crt""
    # Name of the key in serverSecret
    serverSecretKeyFileName: ""tls.key""
    # Name of the certificate in serverSecret
    serverSecretCertFileName: ""tls.crt""

    # Certificates to secure the routes.
    routesSecret: ""nats-routes-tls""
    # Name of the CA in routesSecret
    routesSecretCAFileName: ""ca.crt""
    # Name of the key in routesSecret
    routesSecretKeyFileName: ""tls.key""
    # Name of the certificate in routesSecret
    routesSecretCertFileName: ""tls.crt""
Cert-Manager
If cert-manager is available in your cluster, you can easily generate TLS certificates for NATS as follows:
Create a self-signed cluster issuer (or namespace-bound issuer) to create NATS' CA certificate:
apiVersion: certmanager.k8s.io/v1alpha1
kind: ClusterIssuer
metadata:
  name: selfsigning
spec:
  selfSigned: {}
Create your NATS cluster's CA certificate using the new selfsigning issuer:
apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: nats-ca
spec:
  secretName: nats-ca
  duration: 8736h # 1 year
  renewBefore: 240h # 10 days
  issuerRef:
    name: selfsigning
    kind: ClusterIssuer
  commonName: nats-ca
  organization:
  - Your organization
  isCA: true
Create your NATS cluster issuer based on the new nats-ca CA:
apiVersion: certmanager.k8s.io/v1alpha1
kind: Issuer
metadata:
  name: nats-ca
spec:
  ca:
    secretName: nats-ca
Create your NATS cluster's server certificate (assuming NATS is running in the nats-io namespace, otherwise, set the commonName and dnsNames fields appropriately):
apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: nats-server-tls
spec:
  secretName: nats-server-tls
  duration: 2160h # 90 days
  renewBefore: 240h # 10 days
  issuerRef:
    name: nats-ca
    kind: Issuer
  organization:
  - Your organization
  commonName: nats.nats-io.svc.cluster.local
  dnsNames:
  - nats.nats-io.svc
Create your NATS cluster's routes certificate (assuming NATS is running in the nats-io namespace, otherwise, set the commonName and dnsNames fields appropriately):
apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: nats-routes-tls
spec:
  secretName: nats-routes-tls
  duration: 2160h # 90 days
  renewBefore: 240h # 10 days
  issuerRef:
    name: nats-ca
    kind: Issuer
  organization:
  - Your organization
  commonName: ""*.nats-mgmt.nats-io.svc.cluster.local""
  dnsNames:
  - ""*.nats-mgmt.nats-io.svc""
Authorization

Using ServiceAccounts
The NATS Operator can define permissions based on Roles by using any present ServiceAccount in a namespace.
This feature requires a Kubernetes v1.12+ cluster having the TokenRequest API enabled.
To try this feature using minikube v0.30.0+, you can configure it to start as follows:
$ minikube start \
  --extra-config=apiserver.service-account-signing-key-file=/var/lib/minikube/certs/apiserver.key \
  --extra-config=apiserver.service-account-issuer=api \
  --extra-config=apiserver.service-account-api-audiences=api \
  --kubernetes-version=v1.12.4
Please note that availability of this feature across Kubernetes offerings may vary widely.
ServiceAccounts integration can then be enabled by setting the
enableServiceAccounts flag to true in the NatsCluster configuration.
apiVersion: nats.io/v1alpha2
kind: NatsCluster
metadata:
  name: example-nats
spec:
  size: 3
  version: ""1.3.0""

  pod:
    # NOTE: Only supported in Kubernetes v1.12+.
    enableConfigReload: true
  auth:
    # NOTE: Only supported in Kubernetes v1.12+ clusters having the ""TokenRequest"" API enabled.
    enableServiceAccounts: true
Permissions for a ServiceAccount can be set by creating a
NatsServiceRole for that account.  In the example below, there are
two accounts, one is an admin user that has more permissions.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nats-admin-user
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nats-user
---
apiVersion: nats.io/v1alpha2
kind: NatsServiceRole
metadata:
  name: nats-user
  namespace: nats-io

  # Specifies which NATS cluster will be mapping this account.
  labels:
    nats_cluster: example-nats
spec:
  permissions:
    publish: [""foo.*"", ""foo.bar.quux""]
    subscribe: [""foo.bar""]
---
apiVersion: nats.io/v1alpha2
kind: NatsServiceRole
metadata:
  name: nats-admin-user
  namespace: nats-io
  labels:
    nats_cluster: example-nats
spec:
  permissions:
    publish: ["">""]
    subscribe: ["">""]
The above will create two different Secrets which can then be mounted as volumes
for a Pod.
$ kubectl -n nats-io get secrets
NAME                                       TYPE          DATA      AGE
...
nats-admin-user-example-nats-bound-token   Opaque        1         43m
nats-user-example-nats-bound-token         Opaque        1         43m
An example of mounting the secret in a Pod can be found below:
apiVersion: v1
kind: Pod
metadata:
  name: nats-user-pod
  labels:
    nats_cluster: example-nats
spec:
  volumes:
    - name: ""token""
      projected:
        sources:
        - secret:
            name: ""nats-user-example-nats-bound-token""
            items:
              - key: token
                path: ""token""
  restartPolicy: Never
  containers:
    - name: nats-ops
      command: [""/bin/sh""]
      image: ""wallyqs/nats-ops:latest""
      tty: true
      stdin: true
      stdinOnce: true
      volumeMounts:
      - name: ""token""
        mountPath: ""/var/run/secrets/nats.io""
Then within the Pod the token can be used to authenticate against
the server using the created token.
$ kubectl -n nats-io attach -it nats-user-pod

/go # nats-sub -s nats://nats-user:`cat /var/run/secrets/nats.io/token`@example-nats:4222 hello.world
Listening on [hello.world]
^C
/go # nats-sub -s nats://nats-admin-user:`cat /var/run/secrets/nats.io/token`@example-nats:4222 hello.world
Can't connect: nats: authorization violation
Using a single secret with the explicit configuration.
Authorization can also be set for the server by using a secret
where the permissions are defined in JSON:
{
  ""users"": [
    { ""username"": ""user1"", ""password"": ""secret1"" },
    { ""username"": ""user2"", ""password"": ""secret2"",
      ""permissions"": {
	""publish"": [""hello.*""],
	""subscribe"": [""hello.world""]
      }
    }
  ],
  ""default_permissions"": {
    ""publish"": [""SANDBOX.*""],
    ""subscribe"": [""PUBLIC.>""]
  }
}
Example of creating a secret to set the permissions:
kubectl create secret generic nats-clients-auth --from-file=clients-auth.json
Now when creating a NATS cluster it is possible to set the permissions as
in the following example:
apiVersion: ""nats.io/v1alpha2""
kind: ""NatsCluster""
pmetadata:
  name: ""example-nats-auth""
spec:
  size: 3
  version: ""1.1.0""

  auth:
    # Definition in JSON of the users permissions
    clientsAuthSecret: ""nats-clients-auth""

    # How long to wait for authentication
    clientsAuthTimeout: 5

Configuration Reload
On Kubernetes v1.12+ clusters it is possible to enable on-the-fly reloading of configuration for the servers that are part of the cluster.
This can also be combined with the authorization support, so in case the user permissions change, then the servers will reload and apply the new permissions.
apiVersion: ""nats.io/v1alpha2""
kind: ""NatsCluster""
metadata:
  name: ""example-nats-auth""
spec:
  size: 3
  version: ""1.1.0""

  pod:
    # Enable on-the-fly NATS Server config reload
    # NOTE: Only supported in Kubernetes v1.12+.
    enableConfigReload: true

    # Possible to customize version of reloader image
    reloaderImage: connecteverything/nats-server-config-reloader
    reloaderImageTag: ""0.2.2-v1alpha2""
    reloaderImagePullPolicy: ""IfNotPresent""
  auth:
    # Definition in JSON of the users permissions
    clientsAuthSecret: ""nats-clients-auth""

    # How long to wait for authentication
    clientsAuthTimeout: 5
Connecting operated NATS clusters to external NATS clusters
By using the extraRoutes field on the spec you can make the operated
NATS cluster create routes against clusters outside of Kubernetes:
apiVersion: ""nats.io/v1alpha2""
kind: ""NatsCluster""
metadata:
  name: ""nats""
spec:
  size: 3
  version: ""1.4.1""

  extraRoutes:
    - route: ""nats://nats-a.example.com:6222""
    - route: ""nats://nats-b.example.com:6222""
    - route: ""nats://nats-c.example.com:6222""
It is also possible to connect to another operated NATS cluster as follows:
apiVersion: ""nats.io/v1alpha2""
kind: ""NatsCluster""
metadata:
  name: ""nats-v2-2""
spec:
  size: 3
  version: ""1.4.1""

  extraRoutes:
    - cluster: ""nats-v2-1""
Development
Building the Docker Image
To build the nats-operator Docker image:
$ docker build -f docker/operator/Dockerfile . <image:tag>
To build the nats-server-config-reloader:
$ docker build -f docker/reloader/Dockerfile . <image:tag>
You'll need Docker 17.06.0-ce or higher.
",274
facebook/fbzmq,C++,"fbzmq

fbzmq provides a framework for writing services in C++ while leveraging the
awesomeness of libzmq (message passing semantics). At a high level it provides

Lightweight C++ wrapper over libzmq which leverages newer C++ constructs
and stricter type checking. Most notably it provides the ability to
send/receive thrift objects as messages over wire without worrying about
wire encoding/decoding protocols.
Powerful Async Framework with EventLoop, Timeouts, SignalHandler and more
to enable developers to write asynchronous applications efficiently.
Suite of monitoring tools that make it easy to add logging and counters to
your service.

Examples
Here is a simple example demonstrating some powerful abstractions of fbzmq
which makes writing asynchronous applications easier on top of libzmq
// Create context
fbzmq::Context context{1};

// Create eventloop
fbzmq::ZmqEventLoop evl;

// Create thrift serializer
apache::thrift::CompactSerializer serializer;

// Create REP and PUB server sockets
fbzmq::Socket<ZMQ_REP, ZMQ_SERVER> reqSocket;
fbzmq::Socket<ZMQ_PUB, ZMQ_SERVER> pubSocket;
reqSocket.bind(""tcp://[::1]:12345"");
pubSocket.bind(""tcp://[::1]:12346"");

// Attach callback on reqSocket
evl.addSocket(RawZmqSocketPtr{*reqSocket}, ZMQ_POLLIN, [&](int evts) noexcept {
  // It must be POLLIN event
  CHECK(evts | ZMQ_POLLIN);

  // Read request
  auto request = reqSocket.recvThriftObj<thrift::Request>(serializer);

  // Send response
  thrift::Response response;
  response.request = request;
  response.success = true;
  reqSocket.sendThriftObj(response, serializer);
});

// Create periodic timeout to send publications and schedule it every 10s
auto timeout = fbzmq::ZmqTimeout::make(&evl, [&]() noexcept {
  thrift::User user;
  user.name = ""TestPerson"";
  user.email = ""test@person.com"";
  pubSocket.sendThriftObj(nodeData, serializer);
});
timeout->scheduleTimeout(std::chrono::seconds(10), true /* periodic */);

// Let the magic begin
evl.run()


Requirements
We have tried fbzmq on Ubuntu-14.04, Ubuntu-16.04 and CentOS-7. This
should work on all Linux based platforms without any issues.

Compiler supporting C++14 or higher
libzmq-4.0.6 or greater

Building fbzmq
Repo Directory Structure
At the top level of this repo are the build and fbzmq directories. Under the
former is a tool, fbcode_builder, that contains scripts for generating a
docker context to build the project. The fbzmq directory contains the
source for the project.
Dependencies

cmake
gflags
gtest
libsodium
libzmq
zstd
folly
fbthrift

One Step Build - Ubuntu-16.04
We've provided a script, build/build_fbzmq.sh, well tested on
Ubuntu-16.04, to install all necessary dependencies, compile fbzmq and install
C++ binaries as well as python tools. Please modify the script as needed for
your platform. Also, note that some library dependencies require a newer version
than provided by the default package manager on the system and hence we are
compiling them from source instead of installing via the package manager. Please
see the script for those instances and the required versions.
Build using Docker
Learn more here.
Build Steps
// Step into `build` directory
cd build

//  Install dependencies and fbzmq
sudo bash ./build_fbzmq.sh

make test

Installing fbzmq
fbzmq builds a static library and install steps installs library as well all
header files to /usr/local/lib/ and /usr/local/include/ (under fbzmq sub
directory)
sudo make install

Build and Install python libraries
cd fbzmq/fbzmq/py
python setup.py build
sudo python setup.py install

How fbzmq works
zmq/* are a straight forward C++ wrappers over raw libzmq objects (like
zmq_msg_t, zmq_ctx_t, etc)  in C. ZmqEventLoop is an event loop which allows
you to attach different event callbacks on sockets as well as schedule timeouts.
Internally it maintains a dynamic poll-list which is updated only when new
socket/fd is added/removed and uses zmq_poll APIs for polling the list. It
also maintains internal heap of timeouts ordered on their expiry times.
Full documentation
To understand how to use library, take a look at examples/ directory. All of
our code is very well documented and refer to appropriate header files for up to
date and detailed documentation of various APIs/functionalities.
License
fbzmq is MIT-licensed.
",200
json-schema-org/JSON-Schema-Test-Suite,Python,"JSON Schema Test Suite 
This repository contains a set of JSON objects that implementors of JSON Schema
validation libraries can use to test their validators.
It is meant to be language agnostic and should require only a JSON parser.
The conversion of the JSON objects into tests within your test framework of
choice is still the job of the validator implementor.
Structure of a Test
If you're going to use this suite, you need to know how tests are laid out. The
tests are contained in the tests directory at the root of this repository.
Inside that directory is a subdirectory for each draft or version of the
schema.
If you look inside the draft directory, there are a number of .json files,
which logically group a set of test cases together. Often the grouping is by
property under test, but not always, especially within optional test files
(discussed below).
Inside each .json file is a single array containing objects. It's easiest to
illustrate the structure of these with an example:
    {
        ""description"": ""the description of the test case"",
        ""schema"": {""the schema that should"" : ""be validated against""},
        ""tests"": [
            {
                ""description"": ""a specific test of a valid instance"",
                ""data"": ""the instance"",
                ""valid"": true
            },
            {
                ""description"": ""another specific test this time, invalid"",
                ""data"": 15,
                ""valid"": false
            }
        ]
    }
So a description, a schema, and some tests, where tests is an array containing
one or more objects with descriptions, data, and a boolean indicating whether
they should be valid or invalid.
Coverage
Drafts 03, 04, 06, and 07 should have full coverage, with drafts 06 and 07
being considered current and actively supported.  Bug fixes will be made as
needed for draft-04 as it is still the most widely used, while draft-03
is long since deprecated.
If you see anything missing from the current supported drafts, or incorrect
on any draft still accepting bug fixes, please file an issue or submit a PR.
Who Uses the Test Suite
This suite is being used by:
Coffeescript

jsck

C++

Modern C++ JSON schema validator

Dart

json_schema

Elixir

ex_json_schema

Erlang

jesse

Go

gojsonschema
validate-json

Haskell

aeson-schema
hjsonschema

Java

json-schema-validator
everit-org/json-schema
networknt/json-schema-validator
Justify

JavaScript

json-schema-benchmark
direct-schema
is-my-json-valid
jassi
JaySchema
json-schema-valid
Jsonary
jsonschema
request-validator
skeemas
tv4
z-schema
jsen
ajv
djv

Node.js
The JSON Schema Test Suite is also available as an
npm package.
Node-specific support is maintained on the node branch.
See NODE-README.md
for more information.
.NET

Newtonsoft.Json.Schema
Manatee.Json

PHP

json-schema
json-guard

Python

jsonschema
fastjsonschema

Ruby

json-schema

Rust

valico

Swift

JSONSchema

Clojure

json-schema

PostgreSQL

postgres-json-schema
is_jsonb_valid

If you use it as well, please fork and send a pull request adding yourself to
the list :).
Contributing
If you see something missing or incorrect, a pull request is most welcome!
There are some sanity checks in place for testing the test suite. You can run
them with bin/jsonschema_suite check && npm test or tox && npm test. They will be run automatically by
Travis CI as well.
",298
OwlSoul/YandexTransportWebdriverAPI-Python,Python,"Yandex Transport Webdriver API
Build status: 
Note: Ужасно мерзкий проект для поддержки, его нельзя просто написать, сказать что оно Stable и забыть. Яндекс все время что-то у себя меняет, это надо отслеживать и исправлять здесь. В среднем раз в месяц случается ""большая взбучка"", и что-то отваливается.

A ""sort of API"" to access Yandex Transport/Masstransit data, designed to work in conjunction with YandexTransportProxy.
Своеобразное ""API"" для доступа к данным Яндекс Транспорт (Masstransit API), предназначено для работы в паре с YandexTransportProxy.
This project is for ""Yandex.Maps"" and ""Yandex.Transport"" services, so it's expected that majority of potential users are from Russian Federation, thus the README is written in russian language.

Статус тестов поддерживаемых функций



Метод
01
02
03
04
05
06
Комментарий




getStopInfo






-


getRouteInfo






Уничтожен Яндексом 15-05-2019, замена - getLine


getLine






-


getVehiclesInfo






Риск DEPRECATED


getVehiclesInfoWithRegion






-


getRegionLayers






-



 - тест пройден 
 - тест неприменим к данному URL 
 - тест не пройден
URL каждого теста можно узнать нажав на его номер в таблице.
Предназначение проекта
Данное ""API"" позволяет автоматизировать получение данных от Яндекс.Транспорт Masstransit API (закрытого на данный момент для сторонних пользователей). Получить данные вроде ""покажи координаты всего общественного транспорта в таком-то районе"" или ""выдай мне данные о координатах транспорта по всему городу в данный момент"" с помощью этой штуковины просто так нельзя. Зато с ее помощью можно автоматизировать получение данных  по конкретной остановке, или по конкретному маршруту, и получить их именно в том формате в котором Яндекс их пересылает - здоровенные такие JSON-структуры (до 150 килобайт и больше).
Полученные данные можно использовать для сбора статистики по конкретному маршруту, создания своего собственного табло для конкретной остановки, и автоматизации каких-либо событий на основе данных о транспорте (редко ходящий, но жизненно важный автобус вышел на маршрут - включаем будильник).
Принцип работы
У каждой остановки или маршрута в Яндекс.Картах есть свой URL, узнать его просто, достаточно просто нажать на интересующую остановку/маршрут и посмотреть что будет в адресной строке.
Примеры URL:

Остановка ""Метро Бауманская"", Москва:
https://yandex.ru/maps/213/moscow/?ll=37.678708,55.772438&masstransit[stopId]=stop__9643291&mode=stop&z=19
Здесь самая важная часть - ""masstransit[stopId]=stop__9643291"". Это ID нашей остановки, в целом достаточно только его, ссылка https://yandex.ru/maps/213/moscow/?masstransit[stopId]=stop__9643291 точно также будет работать в браузере.

Маршрут ""Трамвай Б"", Москва:
https://yandex.ru/maps/213/moscow/?ll=37.679549,55.772203&masstransit[lineID]=B_tramway_default&masstransit[stopId]=stop__9643291&masstransit[threadId]=BA_tramway_default&mode=stop&z=18
Здесь важная часть - ""masstransit[lineID]=B_tramway_default"", и ссылка https://yandex.ru/maps/213/moscow/?masstransit%5BlineID%5D=B_tramway_default точно так же будет работать в браузере.

Подобные URL и используются в работе данного API. Хотя с ""короткими"" URL проблем пока не было, на всякий случай параноидально рекомендуется использовать ""длинные"".
При выполнении этих запросов в браузере производятся несколько AJAX запросов, в которых приходит вся необходимая информация по транспорту, в формате JSON. За получение этих запросов и передачу в данный API отвечает YandexTransportProxy, данный же API является простой оболочкой, позволяющей выполнить, например вот такие вещи:
# Получение информации об остановке, прокси-сервер находится по адресу 127.0.0.1:25555
proxy = YandexTransportProxy('127.0.0.1', 25555)
stop_url = https://yandex.ru/maps/213/moscow/?ll=37.678708%2C55.772438&masstransit%5BstopId%5D=stop__9643291&mode=stop&z=19
result_json = proxy.getStopInfo(stop_url)

# Получение информации о маршруте и транспорте, прокси-сервер находится по адресу 127.0.0.1:25555
proxy = YandexTransportProxy('127.0.0.1', 25555)
route_url = https://yandex.ru/maps/213/moscow/?masstransit%5BlineID%5D=B_tramway_default
result_json_route = proxy.getLineInfo(route_url)
result_json_vehicles = proxy.getVehiclesInfo(route_url)

Результатом подобных запросов будут как раз те самые данные в формате JSON, приходящие от Yandex Transport/Masstransit API. Примеры таких ответов можно посмотреть в YandexTransportProxy wiki.
Прокси-сервер YandexTransportProxy реализует из себя очередь, где полученные от данного API команды выполняются в один поток, с определенной задержкой между запросами. Такое поведение заложено по дизайну, для максимального имитирования работы обычного пользователя, и снижения потенциальной нагрузки на сервера Яндекс (в том числе с целью избежания временного бана за слишком частые запросы, и мы не хотим его злить).
JSON данные от Яндекса очень таких приличных размеров, и самый простой способ посмотреть что именно приходит по запросу - просто сохранить данные в какой-либо файл, и потом визуализировать, например здесь:
#!/usr/bin/env python3

# Basic example, get stop info and save it to a file

import json
from yandex_transport_webdriver_api import YandexTransportProxy

proxy = YandexTransportProxy('127.0.0.1', 25555)
url = ""https://yandex.ru/maps/213/moscow/?ll=37.742975%2C55.651185&masstransit%5BstopId%5D=stop__9647487&mode=stop&z=18""
data = proxy.get_stop_info(url)
with open('data.json', 'w') as file:
    file.write(json.dumps(data,indent=4, separators=(',', ': ')))

Данных приходит очень много, и лучше просто один раз взглянуть на них, чем пытаться здесь все задокументировать.
Примеры приходящих данных можно посмотреть в wiki к YandexProxyServer:
Установка
Проект можно поставить через pip3:
pip3 install yandex-transport-webdriver-api

Либо просто скачав данную библиотеку из релизов или через ""git clone"".
Каких-то важных зависимостей у библиотеки нет, только стандартные библиотеки Python3.
Не забывайте, что для работы нужен запущенный и доступный по сети сервер YandexTransportProxy, хотя бы один (а можно и Kubernetes кластер из них создать).
Реализованные функции
Функции для получения информации от Яндекс.Транспорта могут работать как в блокирующем (синхронном), так и в неблокирующем (асинхронном) режимах.
Функции данного API имеют общую структуру,обычно в виде get_something(params, query_id=None, blocking=True, timeout=0, callback=None)
Параметры, общие для всех функций:

query_id - строка (string),ID данного запроса, все ответы на него будут содержать данный ID. Важно для асинхронного режима, для синхронного можно опустить (сгенерируется автоматически случайным образом).
blocking - если True, функция блокирует до окончания выполнения, или до истечения таймаута (следующий параметр). Если False - функция завершается немедленно, и при каждом полученном ответе будет вызываться функция callback.
timeout - целое число (int), таймаут в секундах. Если в течении данного промежутка времени никакого ответа от сервера не будет получено, будет брошено исключение. Важно помнить что у прокси-сервера может быть приличный промежуток между выполнениями запросов, если указывается данный параметр. При значении = 0 таймаут не учитывается, и функция будет ждать бесконечно, или пока не придут данные сигнализирующие что ""обработка запроса окончена, новых данных можн оне ждать"".
callback - callback функция, будет вызываться каждый раз при получении данных в асинхронном режиме.




Метод
Соответствующий метод Yandex Masstransit API
Назначение




get_echo
-
Тестовая функция, помещает команду getEcho в очередь YandexTransportProxy, и возвращает переданную строку (text) при выполнении


get_stop_info
getStopInfo
Получить данные об остановке (проходящие маршруты, время прибытия и т.д.)


get_line
getLine
Получение информации о маршруте


get_vehicles_info
getVehiclesInfo
Получить информацию о транспорте на маршруте


get_vehicles_info_with_region
getVehiclesInfoWithRegion
Получение информации о транспорте на маршруте, с дополнительной информацией о регионе. Возможно скоро полностью сменит предыдущий метод.


get_layer_regions
getLayerRegions
""Running Gag"" этого проекта, не ясно за что оно отвечает.


get_all_info
-
Универсальный метод, просто выдает все возможные методы по скормленому ему URL



F.A.Q.
Q: get_vehicles_info (getVehiclesInfo) не работает, хотя в браузере трнспорт на аршруте отображается. 
A: Возможно на данном маршруте getVehiclesInfo уже не применятся, стоит попробовать get_vehicles_info_with_region (getVehiclesInfoWithRegion), или get_all_info и просто посмотреть какие именно запросы Yandex Masstransit API выполняются по переданному URL.
Обратная связь
Гарантий что эта штука будет работать долго и счастливо - никаких. Яндекс может в любой момент устроить что-нибудь что сделает работу этого проекта невозможным. Проект находится на постоянной системе мониторинга, и если что-то отваливается или перестает работать - автор об этом оперативно узнает, и поправит, если это возможно.
В любом случае, сообщить о возникшей проблеме всегда можно здесь: 
https://github.com/OwlSoul/YandexTransportWebdriverAPI-Python/issues/new
Лицензия / License
Исходный код распространяется под лицензией MIT, ""как есть (as is)"", автор ответственности за возможные проблемы при его использовании не несет (но будет глубоко расстроен).
The code is distributed under MIT licence, AS IS, author do not bear any responsibility for possible problems with usage of this project (but he will be very sad).
Зал славы / Credits
Project author: Yury D. (TheOwlSoul@gmail.com) 
PEP-8 Evangelist, Good Cop: Pavel Lutskov 
PEP-8 Evangelist, Bad Cop: Yury Alexeev

",5
matthiasn/meins,Clojure,"meins
The open-source application meins is an experimentation toolkit for designing your life. It helps you collect relevant information,  design, and then implement change. Most importantly, it does so without leaking data, because everything stays within your realm of control, and you can always verify this claim in the source code. Please have a look at the manual to find out more about what it does. The same text is available in the application under the help menu.
Here's how that currently looks like:

Installers
You can download a beta version of the application for Linux, Mac, and Windows on GitHub, where you want the highest existing version for your platform. The Mac version is usually the newest, and the others can lag. You only have to download the binary once, as there are automatic updates. You can also build the application yourself, with a simple make command, plus an unknowable amount of time for getting your development environment right.
All of these provide auto-update functionality, which can be accessed through ""Check for Updates"" in the application menu. In addition, checks for a newer version run once every 24 hours.
Motivation
See this blog post for the background. Back then it was called meo, but has since been renamed to meins.
Components
meins consists of a Clojure and ClojureScript system spanning an Electron application and a backend that runs on the JVM. There's also a mobile companion app written in ClojureScript on top of React Native, see ./rn-app in this repository.
All subsystems in meins make use of the systems-toolbox for their architecture.
Here's how the app currently looks like:

Installation
There is a Makefile that contains all the build targets. You will obviously need GNU make to run the targets. Alternatively, you can run the commands in there individually. Please have a look at the Makefile to see what the commands are.
You also need JDK 10+, for example OpenJDK 11. Please make sure that the $JAVA_HOME is set up and that you can run link from the command line.
Then you need Node.js. I use nvm. After installing it, run
$ nvm install 8

To install the required global npm dependencies on a Mac, you can run
$ make deps-mac

For Ubuntu, run
$ make deps-ubuntu

If anything is missing or redundant, please submit a pull request, I am not running these often. Leiningen itself is missing, not sure how to best install that from here. Maybe it's best to just use the commands in the install targets as a template for what you need to install. Please have a look at what the target for your platform does before blindly running it.
Once all the dependencies installed already, you can create a packaged version of meins running
$ make package 

This will download dependencies, both Clojure and npm, and then run tests, build, and package the entire application for the platform you are running on. This will take The backend of the application is a standalone uberjar that runs with a packaged custom Java runtime that is generated when building meins using jlink. The resulting runtime is only a fraction of the size of a JDK. Packaging the runtime is more reliable than trying to rely on a recent JRE on a non-developer machine. This packaging mechanism is provided by Project Jigsaw,
Icons
Convert using electron-icon-generator, then copy to build/ and rename.
$ npm install -g electron-icon-generator
$ electron-icon-generator ~/Downloads/Logo_meins_RZ.png

Development
For development, you need to install the JavaScript dependencies:
$ yarn install

Then, you need to build the JavaScript for both the Electron Main and Render processes:
$ shadow-cljs watch main
$ shadow-cljs watch renderer

The task above need to be running in separate terminals, as they watching the file system for changes. Next, you need to compile the SCSS files into CSS:
$ lein sass4clj auto

Once you have completed all the steps in the previous section, all you need to do is the following, once again in separate terminals:
$ lein run
$ npm start

Now you should have an environment running where any change in the code (including the SCSS) reloads the content of the Electron application.
Packaging
meins is released as follows:
$ make release

This will package and sign the application, for the platform it runs on. Then, it will upload the resulting package(s) to the releases section on the project page. Note that the proper GH_TOKEN environment variable must be set for this.
Tests
$ lein test

or
$ lein test2junit



How to help
Contributions and pull requests are very welcome. Please check the open issues for where help is required the most, and file issues for anything else that you find.
License
Copyright © 2016-2019 Matthias Nehlsen. Distributed under the GNU AFFERO PUBLIC LICENSE, Version 3. See separate LICENSE file.
",162
PDXPythonPirates/pythonpirates.org,HTML,"Portland Python Pirates Website
This is the home for the content files used to create the PDX Python Pirates website.
The content is written in Markdown and rendered to static HTML using Hugo, a static site generation tool.
How to Contribute: Overview

Fork this repo to your GitHub account.
Clone to your local system and make content changes (see note on git submodules below)
Run Hugo on your local system to verify your changes.
Commit changes to your fork and open a pull request.


Please note this repository make use of git submodules which requires an extra flag --recurse-submodules to fetch when cloning.

git clone --recurse-submodules git@github.com:GITHUB_USER/pythonpirates.org.git

If you missed the submodules in the initial clone you can update using the following:
git submodule update --init --recurse

Verify Changes Locally
When making edits it is recommended you run a local installation of Hugo to verify the pages render as expected.
Fortunately, Hugo is not difficult to install and run.
See the official Hugo docs to:

Install Hugo for your operating system.
Verify the executable is found in your PATH.

Once installed, start hugo from a command prompt with the project root as the current directory.
# repo was cloned to directory 'pythonpirates.org'
cd /path/to/git-projects/pythonpirates.org

hugo server -D

This starts up a Hugo server listening on http://localhost:1313/

The -D flag is optional and tells Hugo to include pages marked as ""draft"" status.

Make your edits & when finished:

commit your changes locally... you made a branch, right? ;-)
push to your fork on GitHub
open a pull request
A preview of the site will be linked from the pull request ticket
notify someone in Slack to review the change and merge the pull request

A Note on Front Matter and Drafts
Each content page starts with a section called front matter.  This is easily identified at the top of the file within a section fenced by --- or +++.
The front matter serves as a place to record metadata for the page such as author, background image, keyword tags, and more.  Most fields are self explanatory.
Front matter is also the place where a page is marked as a draft.  This is notable because Hugo does not publish pages marked as draft to our website.
You can override this behavior in your local installation by running hugo with the -D flag.  This is useful when authoring content that is not quite ready to be published but you want to check the rendered result.
Example front matter:
---
# comments starting with '#' are allowed in front matter
title: ""Use Python to Take Over the World""
date:  2018-11-22
# page is marked as a draft and will not publish to the live site
draft: true
---

When you are ready to publish the page, update the value draft: false, or delete the draft attribute line.

The draft attribute is the first thing to check if a page isn't showing up on the website that you expect.

Updating Your Clone
Your cloned copy can easily fall out of date when regular updates are being merged into the main website repository.
Because of this possibility, it's a good practice to update your local copy of master before creating a new branch to make edits.
Check if you already have a remote defined for upstream:
git remote -v

If upstream isn't in the list of remotes then add it:
git add remote upstream https://github.com/PDXPythonPirates/pythonpirates.org.git

Update your local master so it's caught up with the upstream master:
git checkout master
git pull upstream master

Now you can create your branch and make your killer contribution:
git checkout -b my-awesome-website-contribution

",3
pvtl/voyager-forms,PHP,"Voyager Forms
The Missing Form Module for The Missing Laravel Admin.
This Laravel package adds dynamic form creation and shortcode insertion to a Voyager project.

Create & manage forms and their fields (add fields, drag/drop into order etc)
Output forms on the frontend with an simple shortcode ({!! forms(<FORM_ID>) !!})
Each form's output on the frontend are overridable with custom layouts
All submissions are emailed inside overridable HTML email templates
All submissions are backed up to the database and accessible under Voyager Admins > Forms > Enquiries

Built by Pivotal Agency.

Prerequisites

Node & NPM Installed
Composer Installed
Install Laravel
Install Voyager
Install Voyager Front-end


Installation
# 1. Require this Package in your fresh Laravel/Voyager project
composer require pvtl/voyager-forms

# 2. Run the Installer
composer dump-autoload && php artisan voyager-forms:install

# 3. Configure to/from addresses
        -> Navigate to Admin -> Settings -> 'Forms' tab
        -> Adjust values
        
# 4. Configure ""MAIL"" environment variables

# 5. (optional) Add Google invisible reCAPTCHA
        -> Navigate to Admin -> Settings -> 'Admin' tab
        -> Insert Google reCATPCHA keys 


Displaying Forms
You can easily display your created forms on the front-end in any kind of output - we use shortcodes to render our forms so go ahead and add {!! forms(1) !!} to a page/post to see the default Contact form appear.

Form Hooks
You may also wish to include custom logic and functionality when your form has been submitted (but before the submission has saved to the DB - eg. so that you can execute custom validation). This can be done with a Form Hook Block - simply specify your controllers namespace'd path and the method you wish to call and the Voyager Forms module will automatically execute it upon submission. For example:
Pvtl\AwesomeModule\Somewhere\ClassName::anExampleHey('hello world')
Note that in the above example, the first param of the actual method will be the submission data and the second param will be 'hello world'

Custom Form Output
This module outputs forms on the frontend in a basic structure. However you have the ability to build your own form layouts very easily.
A completely custom layout:

Create a new blade template in views/vendor/voyager-forms/layouts
Edit the form in Voyager Admin and select the new layout you created

To get a completely custom output, you'll likely need to define the <form> html including each form field individually.
Override fields
You also have the ability to override views/vendor/voyager-forms/forms/render.blade.php to change the way form fields are styled.

Custom Email Templates
This module sends a generic looking email with each submission. However you have the ability to build your own email templates very easily.

Create a new blade template in views/vendor/voyager-forms/email-templates (you can also simply override default.blade.php in the same location)
Edit the form in Voyager Admin and select the appropriate email template

",29
zonyitoo/rust-ini,Rust,"INI in Rust





INI is an informal standard for configuration files for some platforms or software. INI files are simple text files with a basic structure composed of ""sections"" and ""properties"".
This is an INI file parser in Rust.
[dependencies]
rust-ini = ""0.13""

Usage

Create a Ini configuration file.

extern crate ini;
use ini::Ini;

fn main() {
    let mut conf = Ini::new();
    conf.with_section(None)
        .set(""encoding"", ""utf-8"");
    conf.with_section(Some(""User"".to_owned()))
        .set(""given_name"", ""Tommy"")
        .set(""family_name"", ""Green"")
        .set(""unicode"", ""Raspberry树莓"");
    conf.with_section(Some(""Book"".to_owned()))
        .set(""name"", ""Rust cool"");
    conf.write_to_file(""conf.ini"").unwrap();
}
Then you will get conf.ini
encoding=utf-8

[User]
given_name=Tommy
family_name=Green
unicode=Raspberry\x6811\x8393

[Book]
name=Rust cool

Read from file conf.ini

extern crate ini;
use ini::Ini;

fn main() {
    let conf = Ini::load_from_file(""conf.ini"").unwrap();

    let section = conf.section(Some(""User"".to_owned())).unwrap();
    let tommy = section.get(""given_name"").unwrap();
    let green = section.get(""family_name"").unwrap();

    println!(""{:?} {:?}"", tommy, green);

    // iterating
    for (sec, prop) in &conf {
        println!(""Section: {:?}"", sec);
        for (key, value) in prop {
            println!(""{:?}:{:?}"", key, value);
        }
    }
}

More details could be found in examples.


License
The MIT License (MIT)
Copyright (c) 2014 Y. T. CHUNG
Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the ""Software""), to deal in
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
the Software, and to permit persons to whom the Software is furnished to do so,
subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
",105
googleapis/google-api-php-client-services,PHP,"Google PHP API Client Services
Requirements
Google API PHP Client
Usage in v2 of Google API PHP Client
This library is automatically updated daily with new API changes, and tagged weekly.
It is installed as part of the
Google API PHP Client
library via Composer, which will pull down the most recent tag.
Usage in v1
If you are currently using the v1-master
branch of the client library, but want to use the latest API services, you can
do so by requiring this library directly into your project via the same composer command:
composer require google/apiclient-services:dev-master
",580
pvtl/voyager-forms,PHP,"Voyager Forms
The Missing Form Module for The Missing Laravel Admin.
This Laravel package adds dynamic form creation and shortcode insertion to a Voyager project.

Create & manage forms and their fields (add fields, drag/drop into order etc)
Output forms on the frontend with an simple shortcode ({!! forms(<FORM_ID>) !!})
Each form's output on the frontend are overridable with custom layouts
All submissions are emailed inside overridable HTML email templates
All submissions are backed up to the database and accessible under Voyager Admins > Forms > Enquiries

Built by Pivotal Agency.

Prerequisites

Node & NPM Installed
Composer Installed
Install Laravel
Install Voyager
Install Voyager Front-end


Installation
# 1. Require this Package in your fresh Laravel/Voyager project
composer require pvtl/voyager-forms

# 2. Run the Installer
composer dump-autoload && php artisan voyager-forms:install

# 3. Configure to/from addresses
        -> Navigate to Admin -> Settings -> 'Forms' tab
        -> Adjust values
        
# 4. Configure ""MAIL"" environment variables

# 5. (optional) Add Google invisible reCAPTCHA
        -> Navigate to Admin -> Settings -> 'Admin' tab
        -> Insert Google reCATPCHA keys 


Displaying Forms
You can easily display your created forms on the front-end in any kind of output - we use shortcodes to render our forms so go ahead and add {!! forms(1) !!} to a page/post to see the default Contact form appear.

Form Hooks
You may also wish to include custom logic and functionality when your form has been submitted (but before the submission has saved to the DB - eg. so that you can execute custom validation). This can be done with a Form Hook Block - simply specify your controllers namespace'd path and the method you wish to call and the Voyager Forms module will automatically execute it upon submission. For example:
Pvtl\AwesomeModule\Somewhere\ClassName::anExampleHey('hello world')
Note that in the above example, the first param of the actual method will be the submission data and the second param will be 'hello world'

Custom Form Output
This module outputs forms on the frontend in a basic structure. However you have the ability to build your own form layouts very easily.
A completely custom layout:

Create a new blade template in views/vendor/voyager-forms/layouts
Edit the form in Voyager Admin and select the new layout you created

To get a completely custom output, you'll likely need to define the <form> html including each form field individually.
Override fields
You also have the ability to override views/vendor/voyager-forms/forms/render.blade.php to change the way form fields are styled.

Custom Email Templates
This module sends a generic looking email with each submission. However you have the ability to build your own email templates very easily.

Create a new blade template in views/vendor/voyager-forms/email-templates (you can also simply override default.blade.php in the same location)
Edit the form in Voyager Admin and select the appropriate email template

",29
zonyitoo/rust-ini,Rust,"INI in Rust





INI is an informal standard for configuration files for some platforms or software. INI files are simple text files with a basic structure composed of ""sections"" and ""properties"".
This is an INI file parser in Rust.
[dependencies]
rust-ini = ""0.13""

Usage

Create a Ini configuration file.

extern crate ini;
use ini::Ini;

fn main() {
    let mut conf = Ini::new();
    conf.with_section(None)
        .set(""encoding"", ""utf-8"");
    conf.with_section(Some(""User"".to_owned()))
        .set(""given_name"", ""Tommy"")
        .set(""family_name"", ""Green"")
        .set(""unicode"", ""Raspberry树莓"");
    conf.with_section(Some(""Book"".to_owned()))
        .set(""name"", ""Rust cool"");
    conf.write_to_file(""conf.ini"").unwrap();
}
Then you will get conf.ini
encoding=utf-8

[User]
given_name=Tommy
family_name=Green
unicode=Raspberry\x6811\x8393

[Book]
name=Rust cool

Read from file conf.ini

extern crate ini;
use ini::Ini;

fn main() {
    let conf = Ini::load_from_file(""conf.ini"").unwrap();

    let section = conf.section(Some(""User"".to_owned())).unwrap();
    let tommy = section.get(""given_name"").unwrap();
    let green = section.get(""family_name"").unwrap();

    println!(""{:?} {:?}"", tommy, green);

    // iterating
    for (sec, prop) in &conf {
        println!(""Section: {:?}"", sec);
        for (key, value) in prop {
            println!(""{:?}:{:?}"", key, value);
        }
    }
}

More details could be found in examples.


License
The MIT License (MIT)
Copyright (c) 2014 Y. T. CHUNG
Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the ""Software""), to deal in
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
the Software, and to permit persons to whom the Software is furnished to do so,
subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
",105
googleapis/google-api-php-client-services,PHP,"Google PHP API Client Services
Requirements
Google API PHP Client
Usage in v2 of Google API PHP Client
This library is automatically updated daily with new API changes, and tagged weekly.
It is installed as part of the
Google API PHP Client
library via Composer, which will pull down the most recent tag.
Usage in v1
If you are currently using the v1-master
branch of the client library, but want to use the latest API services, you can
do so by requiring this library directly into your project via the same composer command:
composer require google/apiclient-services:dev-master
",580
D-Programming-GDC/gcc,C,"The GDC D Compiler  
GDC is the GCC-based D language compiler, integrating the open source DMDFE D front end
with GCC as the backend. The GNU D Compiler (GDC) project was originally started by David Friedman
in 2004 until early 2007 when he disappeared from the D scene, and was no longer able to maintain GDC.
Following a revival attempt in 2008, GDC is now under the lead of Iain Buclaw who has been steering the
project since 2009 with the assistance of its contributors, without them the project would not have been
nearly as successful as it has been.
Documentation on GDC is available from the wiki. Any bugs or issues found with using GDC should
be reported at the GCC bugzilla site with the bug component set to d. For help with GDC, the
D.gnu mailing list is the place to go with questions or problems. There's also a GDC IRC
channel at #d.gdc on FreeNode. Any questions which are related to the D language, but not directly to
the GDC compiler, can be asked in the D forums. You can find more information about D, including
example code, API documentation, tutorials and these forums at the main D website.
Building GDC
Stable GDC releases for production usage should be obtained by downloading stable GCC sources
from the GCC downloads site.
For the latest experimental development version, simply download a GCC snapshot or
checkout the GCC SVN or git mirror repository. Most GDC development directly targets the GCC SVN repository,
so the latest GDC version is always available in the GCC SVN.
Do not use the trunk-ci branch in this repository, as it is rebased regularly and contains exclusively
CI related changes.
During certain development phases (e.g. when GCC is in a feature freeze) larger GDC changes may be staged
to the trunk-next branch. This branch is rebased irregularly, do not rely on the commit ids to be
stable.
If you need to clone this repo for some reason, you may want to do a shallow clone using the
--depth 1 --no-single-branch git options, as this repository is large. To compile GDC, add --enable-languages=d to the GCC configure flags and start building.
Using GDC
Usage information can be found at ...
Contributing to GDC
Starting with GCC 9.0.0, GDC has been merged into upstream GCC and all GDC development now follows the usual
GCC development process. Changes to GDC and related code can therefore be submitted
to the gcc-patches mailing list for review.
It is possible to directly post patches to the mailing list and not to use this repository at all.
We however recommend using this repository to make use of the CI checks and the github review workflow.
Submitting Changes
To submit changes to GDC, simply fork this repository, create a new feature branch based on the trunk-ci branch,
then open a pull request against the trunk branch. We recommend using full clones for development, allthough
using shallow clones should also be possible. In code:
# Initial one time setup:
# For repository on github, then clone your fork
git clone git@github.com:[you]/gcc.git
cd gcc
# Add the gdc repository as a remote
git remote add gdc git@github.com:D-Programming-GDC/gcc.git

# Do this for every patch:
# Fetch latest upstream changes
git remote update
# Base a new branch on gdc/trunk
git checkout gdc/trunk-ci
git checkout -b pr12345
# Make changes, commit
git commit [...]
git push origin pr12345:pr12345
# Open a pull request on github, target branch: trunk
Opening a pull request will automatically trigger our CI and test your changes on various machines.
Changelogs
The GCC project requires keeping changes in the Changelog files. GCC ships a script which can generate
Changelog templates for us if we feed it a diff:
git diff gdc/trunk-ci | ./contrib/mklog
Note: The above command generates the diff between gdc/trunk-ci and your local branch. If gdc/trunk-ci was
updated and you did a git remote update, gdc/trunk may have changes which are not yet in your branch.
In that case, rebase onto gdc/trunk first.
The command outputs something like this:
ChangeLog:

2019-02-03  Johannes Pfau  <johannespfau@example.com>

	* test.d: New file.

gcc/d/ChangeLog:

2019-02-03  Johannes Pfau  <johannespfau@example.com>

	* dfile.txt: New file.

libphobos/ChangeLog:

2019-02-03  Johannes Pfau  <johannespfau@example.com>

	* phobosfile.txt: New file.


The ChangeLog:, libphobos/ChangeLog: part gives the file into which the following changes need to be added.
Complete the changelog text and use the existing entries in the files for reference or see
the GCC and GNU documentation. Also make sure to adhere to the line length limit of 80 characters. Then make the changelog somehow available for review:
Either commit the files, or preferable, just copy and paste the edited text output of mklog into your
pull request description.
Getting Changes Into GCC SVN
After changes have been reviewed on github, they have to be pushed into the GCC SVN. Pull requests will
not get merged into this repository. The following steps can be handled by GDC maintainers, although it is
possible to perform these by yourself as well.
Sumbitting to the gcc-patches Mailing List
Once the review and CI have passed on the github pull request page, the changes need to be submitted to the
gcc-patches mailing list. This can easily be done using git send-email:

You might want to squash the commits. Each commit will become one email/patch so it might make sense
to combine commits here.
The changelog should preferrably be pasted into the email text, so do not include
commits modifying the changelog files.
If you had to regenerate any autogenerated files (e.g. configure from configure.ac)
you may keep these changes out of the patch for simplified review. The generated files
should still be present in the changelog.

You'll have to configure git send-email once after you checked out the repository:
git config sendemail.to gcc-patches@gcc.gnu.org
If you never used git send-email before, you'll also have to setup the SMTP settings once.
See here for details.
Now to send the patches:
# Check which commits will be sent:
git log gdc/trunk-ci..
# Check the complete diff which will be sent:
git diff gdc/trunk-ci..
# Dry run to verify everything again
git send-email gdc/trunk-ci --annotate --dry-run
# Send the patches
git send-email gdc/trunk-ci --annotate
If you send multiple patches and want to write an introduction email, use the --compose argument for
git send-email. You can also generate patch files like this:
git format-patch gdc/trunk-ci..
# Edit the *.patch files, add messages etc.
# Now send the patches
git send-email *.patch --dry-run
git send-email *.patch
Pushing Changes to SVN
This section is only relevant for GDC maintainers with GCC SVN write access. There are certain rules when
pushing to SVN, usually you're only allowed to push after the patches have been reviewed on the mailing list.
Refer to the GCC documentation for details.
# We need the full repository for SVN integration
git remote add gcc git://gcc.gnu.org/git/gcc.git
git config --add remote.gcc.fetch 'refs/remotes/*:refs/remotes/svn/*'
git remote update
git svn init -s --prefix=svn/ svn+ssh://username@gcc.gnu.org/svn/gcc
# Update trunk branch to latest SVN commit
git checkout -b trunk svn/trunk
git fetch
git svn rebase

git checkout myfix
# Rebase myfix branch to get rid of CI commits
git rebase gdc/trunk-ci --onto trunk
# And merge
git checkout trunk
git merge --ff-only myfix
# See commits we're going to commit
git log -p @{u}..

# And finally commit
git svn dcommit

Repository Information
This repository is a fork of the GCC git mirror. As the GCC main version control system is
currently still using SVN, all GIT repositories are git-svn bridges. This needs some special attention when
performing some git operations.
Directory Structure
All code branches contain the complete GCC tree. D sources are in gcc/d for the compiler
and in libphobos for the runtime library. Changes to files in gcc/d/dmd or libphobos
should be submitted to the upstream dlang repositories first if possible.
Refer to gcc/d/README.gcc for more details.
Branches
Branches in this repository are organized in the following way:

CI branches: The trunk-ci branch and release branches gcc-*-branch-ci are based on the same
branches in the upstream GCC git repository. The only changes compared to the upstream branches
are CI-related setup commits. CI branches are updated automatically to be kept in sync with
upstream and are rebased onto the upstream changes. These branches are effectively readonly:
We never merge into the branches in this repository. The CI related changes make it possible
to run CI based tests for any PR based on these branches, which is their sole purpose.
The trunk-next branch: If GCC is in a late development stage this branch can accumulate
changes for the GCC release after the next one. It is essentially used to allow periodic merges from
upstream DMD when GCC development is frozen. Changes in the GCC trunk branch
are manually merged into this branch. When GCC enters stage 1 development again, this branch will be
rebased and pushed to upstream trunk. After that, the branch in this repository will be rebased
to trunk.
Backport branches: The gcc-*-bp branches contain D frontend and library feature updates for released GCC versions.
Regression fixes should target the main gcc-*-branch branches instead, according to GCC rules.

",10
facebook/fbthrift,C++,"Facebook Thrift 
Thrift is a serialization and RPC framework for service communication. Thrift enables these features in all major languages, and there is strong support for C++, Python, Hack, and Java. Most services at Facebook are written using Thrift for RPC, and some storage systems use Thrift for serializing records on disk.
Facebook Thrift is not a distribution of Apache Thrift. This is an evolved internal branch of Thrift that Facebook re-released to open source community in February 2014. Facebook Thrift was originally released closely tracking Apache Thrift but is now evolving in new directions. In particular, the compiler was rewritten from scratch and the new implementation features a fully asynchronous Thrift server. Read more about these improvements in the ThriftServer documentation.
You can also learn more about this project in the original Facebook Code blog post.
Table of Contents (ToC):

The Three Things About Thrift

A Code Generator
A Serialization Framework
An RPC Framework


Building

Dependencies
Build
Thrift Files


C++ Static Reflection

About Thrift
At a high level, Thrift is three major things:
A Code Generator
Thrift has a code generator which generates data structures that can be serialized using Thrift, and client and server stubs for RPC, in different languages.
A Serialization Framework
Thrift has a set of protocols for serialization that may be used in different languages to serialize the generated structures created from the code generator.
An RPC Framework
Thrift has a framework to frame messages to send between clients and servers, and to call application-defined functions when receiving messages in different languages.
There are several key goals for these components:


Ease of use:
Thrift takes care of the boilerplate of serialization and RPC, and enables the developer to focus on the schema of the system's serializable types and on the interfaces of system's RPC services.


Cross language support:
Thrift enables intercommunication between different languages. For example, a Python client communicating with a C++ server.


Performance:
Thrift structures and services enable fast serialization and deserialization, and its RPC protocol and frameworks are designed with performance as a feature.


Backwards compatibility:
Thrift allows fields to be added to and removed from serializable types in a manner that preserves backward and forward compatibility.



Building
Dependencies
Please install the following dependencies before building Facebook Thrift:
System:
Bison 3.1 or later,
Boost,
CMake,
Flex,
OpenSSLv1.0.2g,
PThreads,
Zlib
External:
Double Conversion,
{fmt},
GFlags,
GLog,
Facebook:
Fizz,
Folly,
Wangle,
Zstd
Build
git clone https://github.com/facebook/fbthrift
cd build
cmake .. # Add -DOPENSSL_ROOT_DIR for macOS. Usually in /usr/local/ssl
make # or cmake --build .

This will create:

thrift/bin/thrift1: The Thrift compiler binary to generate client and
server code.
thrift/lib/libthriftcpp2.so: Runtime library for clients and servers.

Thrift Files
When using thrift and the CMake build system, include: ThriftLibrary.cmake in
your project. This includes the following macro to help building Thrift files:
thrift_library(
  #file_name
  #services
  #language
  #options
  #file_path
  #output_path
)

This generates a library called file_name-<language>. That is, for
Test.thrift compiled as cpp2, it will generate the library Test-cpp2.
This should be added as a dependency to any source or header file that contains
an include to generated code.

C++ Static Reflection
Information regarding C++ Static Reflection support can be found under the static reflection library directory, in the corresponding README file.


",1674
hunzhiwange/queryphp,PHP,"












English | 中文

The QueryPHP Application

This is the QueryPHP application, the core framework can be found here Framework.

QueryPHP is a modern, high performance PHP 7 resident framework, with engineer user experience as its historical mission, let every PHP application have a good framework.
A hundred percent coverage of the unit tests to facing the bug,based on Zephir implemented framework resident,with Swoole ecology to achieve business resident,
now or in the future step by step. Our vision is USE LEEVEL WITH SWOOLE DO BETTER, let your business to support more user services.
The PHP Framework For Code Poem As Free As Wind, Starts From This Moment With QueryPHP.

Site: https://www.queryphp.com/
API: https://api.queryphp.com
Document: https://www.queryphp.com/docs/

The core packages

QueryPHP On Github: https://github.com/hunzhiwange/queryphp/
QueryPHP On Gitee: https://gitee.com/dyhb/queryphp/
Framework On Github: https://github.com/hunzhiwange/framework/
Framework On Gitee: https://gitee.com/dyhb/framework/
Leevel On Github: https://github.com/hunzhiwange/leevel/
Leevel On Gitee: https://gitee.com/dyhb/leevel
Tests: https://github.com/leevels/tests/
Packages: https://github.com/leevels/
Packages From Hunzhiwange: https://packagist.org/packages/hunzhiwange/
Packages From Leevel: https://packagist.org/packages/leevel/

How to install
Base use
composer create-project hunzhiwange/queryphp myapp dev-master

Visite it

php leevel server <Visite http://127.0.0.1:9527/>


Home http://127.0.0.1:9527/
Mvc router http://127.0.0.1:9527/api/test
Mvc restful router http://127.0.0.1:9527/restful/123
Mvc restful router with method http://127.0.0.1:9527/restful/123/show
Annotation router http://127.0.0.1:9527/api/v1/petLeevelForApi/helloworld
Annotation router with bind http://127.0.0.1:9527/api/v2/withBind/foobar
php leevel link:public http://127.0.0.1:9527/public/css/page.css
php leevel link:storage http://127.0.0.1:9527/storage/logo.png
php leevel link:apis http://127.0.0.1:9527/apis/
php leevel link:debugbar http://127.0.0.1:9527/debugbar/debugbar.css

Connect database
First to create a database.
CREATE DATABASE IF NOT EXISTS myapp DEFAULT CHARSET utf8 COLLATE utf8_general_ci;

Then modify .env
vim .env

...
// Database
DATABASE_DRIVER = mysql
DATABASE_HOST = 127.0.0.1
DATABASE_PORT = 3306
DATABASE_NAME = queryphp_development_db
DATABASE_USER = root
DATABASE_PASSWORD =
...

to

...
// Database
DATABASE_DRIVER = mysql
DATABASE_HOST = 127.0.0.1
DATABASE_PORT = 3306
DATABASE_NAME = myapp
DATABASE_USER = root
DATABASE_PASSWORD = 123456
...


Migrate
php leevel migrate:migrate
php leevel server

Test with database
http://127.0.0.1:9527/api/entity
{
    count: 4,
    :trace: {
        ...
    }
}

Login to QueryVue
Install frontend
First to install the frontend,see more detail on frontend/README.md.
cd frontend
npm install -g cnpm --registry=https://registry.npm.taobao.org // Just once
cnpm install
npm run serve # npm run dev

Login
Then visite it. http://127.0.0.1:9528/#/login

user: admin
password: 123456

Run Tests
First to create a test database.
CREATE DATABASE IF NOT EXISTS test DEFAULT CHARSET utf8 COLLATE utf8_general_ci;

Then modify .testing
vim .testing

...
// Database
DATABASE_DRIVER = mysql
DATABASE_HOST = 127.0.0.1
DATABASE_PORT = 3306
DATABASE_NAME = test
DATABASE_USER = root
DATABASE_PASSWORD =
...

to

...
// Database
DATABASE_DRIVER = mysql
DATABASE_HOST = 127.0.0.1
DATABASE_PORT = 3306
DATABASE_NAME = test
DATABASE_USER = root
DATABASE_PASSWORD = 123456
...


Migrate
php leevel migrate:migrate -e testing

Run
_____________                           _______________
 ______/     \__  _____  ____  ______  / /_  _________
  ____/ __   / / / / _ \/ __`\/ / __ \/ __ \/ __ \___
   __/ / /  / /_/ /  __/ /  \  / /_/ / / / / /_/ /__
     \_\ \_/\____/\___/_/   / / .___/_/ /_/ .___/
        \_\                /_/_/         /_/

$cd /data/codes/queryphp/
$vim .testing # modify database redis and other
$php leevel migrate:migrate -e testing
$php vendor/bin/phpunit tests

Production optimization
Close Debug
Modify .env or runtime/bootstrap/option.php.
// Environment production、testing and development
ENVIRONMENT = production

// Debug
DEBUG = false
DEBUG_JSON = false 
DEBUG_CONSOLE = false
DEBUG_JAVASCRIPT = false

Optimize Commands
The commands below can make queryphp faster.
php leevel router:cache
php leevel option:cache
php leevel i18n:cache
php leevel view:cache
php leevel autoload (Equivalent to `composer dump-autoload --optimize --no-dev`)

Or
php leevel production

Development
Open Debug
Modify .env or runtime/bootstrap/option.php.
// Environment production、testing and development
ENVIRONMENT = development

// Debug
DEBUG = true
DEBUG_JSON = true 
DEBUG_CONSOLE = true
DEBUG_JAVASCRIPT = true

Clears Commands
php leevel i18n:clear
php leevel log:clear
php leevel option:clear
php leevel router:clear
php leevel session:clear
php leevel view:clear
php leevel autoload --dev (Equivalent to `composer dump-autoload --optimize`)

Or
php leevel development

USE LEEVEL DO BETTER
Windows
Need to tests.
Linux
You can download the source code.
git clone git@github.com:hunzhiwange/leevel.git
cd ext

Then compile it.
$/path/to/phpize
$./configure --with-php-config=/path/to/php-config
$make && make install

Then add extension to your php.ini,you can see if installation is successful by command php -m.
extension = leevel.so

Use Swoole With Ultra High Performance
Http server
php leevel http:server # php leevel http:server -d
php leevel http:reload
php leevel http:stop
php leevel http:status

The same with php-fpm
root@vagrant-ubuntu-10-0-2-5:/data/codes/queryphp# php leevel http:server
_____________                           _______________
 ______/     \__  _____  ____  ______  / /_  _________
  ____/ __   / / / / _ \/ __`\/ / __ \/ __ \/ __ \___
   __/ / /  / /_/ /  __/ /  \  / /_/ / / / / /_/ /__
     \_\ \_/\____/\___/_/   / / .___/_/ /_/ .___/
        \_\                /_/_/         /_/
Http Server Version 1.0.0

+-----------------------+---------------------------------+
| Item                  | Value                           |
+-----------------------+---------------------------------+
| host                  | 0.0.0.0                         |
| port                  | 9501                            |
| process_name          | leevel.http                     |
| pid_path              | @path/runtime/protocol/http.pid |
| worker_num            | 8                               |
| daemonize             | 0                               |
| enable_static_handler | 1                               |
| document_root         | @path/www                       |
| task_worker_num       | 4                               |
+-----------------------+---------------------------------+


Home http://127.0.0.1:9501/
Mvc router http://127.0.0.1:9501/api/test
Mvc restful router http://127.0.0.1:9501/restful/123
Mvc restful router with method http://127.0.0.1:9501/restful/123/show
Annotation router http://127.0.0.1:9501/api/v1/petLeevelForApi/helloworld
Annotation router with bind http://127.0.0.1:9501/api/v2/withBind/foobar
php leevel link:public http://127.0.0.1:9501/public/css/page.css
php leevel link:storage http://127.0.0.1:9501/storage/logo.png
php leevel link:apis http://127.0.0.1:9501/apis/
php leevel link:debugbar http://127.0.0.1:9501/debugbar/debugbar.css

Websocket server
php leevel websocket:server # php leevel websocket:server -d
php leevel websocket:reload
php leevel websocket:stop
php leevel websocket:status

A chat room demo
root@vagrant-ubuntu-10-0-2-5:/data/codes/queryphp# php leevel websocket:server
_____________                           _______________
 ______/     \__  _____  ____  ______  / /_  _________
  ____/ __   / / / / _ \/ __`\/ / __ \/ __ \/ __ \___
   __/ / /  / /_/ /  __/ /  \  / /_/ / / / / /_/ /__
     \_\ \_/\____/\___/_/   / / .___/_/ /_/ .___/
        \_\                /_/_/         /_/
Websocket Server Version 1.0.0

+-----------------+--------------------------------------+
| Item            | Value                                |
+-----------------+--------------------------------------+
| host            | 0.0.0.0                              |
| port            | 9502                                 |
| process_name    | leevel.websocket                     |
| pid_path        | @path/runtime/protocol/websocket.pid |
| worker_num      | 8                                    |
| daemonize       | 0                                    |
| task_worker_num | 4                                    |
+-----------------+--------------------------------------+

Visite http://127.0.0.1:9527/websocket/chat

Rpc server
php leevel rpc:server # php leevel rpc:server -d
php leevel rpc:reload
php leevel rpc:stop
php leevel rpc:status

Rpc thrift protocol
Leevel/Protocol/Thrift/Struct/leevel.thrift
namespace php Leevel.Protocol.Thrift.Service

/**
 * ---------------------------------------------------------------
 * 定义一个请求包结构
 * ---------------------------------------------------------------
 *
 * 约定请求数据包，方便只定义一个结构全自动调用 MVC 服务
 */
struct Request
{
  // call 为字符串类型，是指 Service 接口的名称
  // 例如：home:blog/info@get 为调用 mvc 接口中的数据
  1: required string call;

  // params 为 list 类型数据，一个元素可重复的有序列表，C++ 之 vector，Java 之 ArrayList，PHP 之 array
  2: list<string> params;

  // 服务端客户端共享自定义共享数据
  // 相当于 PHP 中的关联数组
  3: map<string,string> metas;
}

/**
 * ---------------------------------------------------------------
 * 定义一个响应包结构
 * ---------------------------------------------------------------
 *
 * 通用响应接口，数据以 JSON 进行交互
 */
struct Response
{
  // status 为响应状态，200 表示成功，其他参考 HTTP 状态
  1: required i16 status;

  // code 为 JSON 字符串，客户端自主进行解析
  2: required string data;
}

/**
 * ---------------------------------------------------------------
 * 定义一个通用的服务
 * ---------------------------------------------------------------
 *
 * 通用调用服务，通过一个 call
 */
service Thrift
{
    Response call(1: Request request)
}

A rpc demo
root@vagrant-ubuntu-10-0-2-5:/data/codes/queryphp# php leevel rpc:server
_____________                           _______________
 ______/     \__  _____  ____  ______  / /_  _________
  ____/ __   / / / / _ \/ __`\/ / __ \/ __ \/ __ \___
   __/ / /  / /_/ /  __/ /  \  / /_/ / / / / /_/ /__
     \_\ \_/\____/\___/_/   / / .___/_/ /_/ .___/
        \_\                /_/_/         /_/
Rpc Server Version 1.0.0

+-----------------------+--------------------------------+
| Item                  | Value                          |
+-----------------------+--------------------------------+
| host                  | 0.0.0.0                        |
| port                  | 1355                           |
| process_name          | leevel.rpc                     |
| pid_path              | @path/runtime/protocol/rpc.pid |
| worker_num            | 8                              |
| daemonize             | 0                              |
| dispatch_mode         | 1                              |
| open_length_check     | 1                              |
| package_max_length    | 8192000                        |
| package_length_type   | N                              |
| package_length_offset | 0                              |
| package_body_offset   | 4                              |
| task_worker_num       | 4                              |
+-----------------------+--------------------------------+

Visite http://127.0.0.1:9527/api/rpc
<?php

declare(strict_types=1);

/*
 * This file is part of the your app package.
 *
 * The PHP Application For Code Poem For You.
 * (c) 2018-2099 http://yourdomian.com All rights reserved.
 *
 * For the full copyright and license information, please view the LICENSE
 * file that was distributed with this source code.
 */

namespace App\App\Controller\Api;

use Leevel\Http\IResponse;
use Leevel\Protocol\Proxy\Rpc as Rpcs;

/**
 * rpc tests.
 *
 * @author Name Your <your@mail.com>
 *
 * @since 2018.08.31
 *
 * @version 1.0
 */
class Rpc
{
    /**
     * 默认方法.
     *
     * @return \Leevel\Http\IResponse
     */
    public function handle(): IResponse
    {
        return Rpcs::call('api/rpc/rpc-result', ['foo', 'bar'], ['arg1' => 'hello', 'arg2' => 'world']);
    }

    /**
     * RPC 结果.
     *
     * @return array
     */
    public function rpcResult(string $arg1, string $arg2, array $metas): array
    {
        return ['arg1' => $arg1, 'arg2' => $arg2, 'metas' => $metas];
    }
}

// The result
// {""arg1"":""foo"",""arg2"":""bar"",""metas"":{""arg1"":""hello"",""arg2"":""world""}}
RoadRunner Supported
RoadRunner is an open source high-performance PHP application server, load balancer and process manager. It supports running as a service with the ability to extend its functionality on a per-project basis.
Install RoadRunner
You can download the binary file.
cd /data/server
wget https://github.com/spiral/roadrunner/releases/download/v1.3.5/roadrunner-1.3.5-darwin-amd64.zip
unzip roadrunner-1.3.5-darwin-amd64.zip
cd /data/codes/queryphp

Roadrunner server
/data/server/roadrunner-1.3.5-darwin-amd64/rr serve -d -v # -d = debug
/data/server/roadrunner-1.3.5-darwin-amd64/rr http:reset
/data/server/roadrunner-1.3.5-darwin-amd64/rr http:workers -i

The same with php-fpm
root@vagrant-ubuntu-10-0-2-5:/data/codes/queryphp# /data/server/roadrunner-1.3.5-darwin-amd64/rr serve -d -v
DEBU[0000] [static]: disabled
DEBU[0000] [rpc]: started
DEBU[0000] [http]: started
INFO[0060] 127.0.0.1 {23.1ms} 200 GET http://127.0.0.1:9601/api/test


Home http://127.0.0.1:9601/
Mvc router http://127.0.0.1:9601/api/test
Mvc restful router http://127.0.0.1:9601/restful/123
Mvc restful router with method http://127.0.0.1:9601/restful/123/show
Annotation router http://127.0.0.1:9601/api/v1/petLeevelForApi/helloworld
Annotation router with bind http://127.0.0.1:9601/api/v2/withBind/foobar
php leevel link:public http://127.0.0.1:9601/public/css/page.css
php leevel link:storage http://127.0.0.1:9601/storage/logo.png
php leevel link:apis http://127.0.0.1:9601/apis/
php leevel link:debugbar http://127.0.0.1:9601/debugbar/debugbar.css

Unified Code Style
Install PHP Coding Standards Fixer
https://github.com/friendsofphp/php-cs-fixer
Base use
$cd /data/codes/queryphp
$php-cs-fixer fix --config=.php_cs.dist

With Git hooks
Add a pre-commit for it.
cp ./build/pre-commit.sh ./.git/hooks/pre-commit
chmod 777 ./.git/hooks/pre-commit

Pass hook
# git commit -h
# git commit -n -m 'pass hook' #bypass pre-commit and commit-msg hooks

Thanks
Thanks my colleague John.mao for your selfless help in the development of this project and and let me have a new understanding, it makes QueryPHP more beautiful.
Thanks for these excellent projects, we have absorbed a lot of excellent design and ideas, standing on the shoulders of giants for innovation.

QeePHP: https://github.com/dualface/qeephp2_x/
Swoole: https://github.com/swoole/
JeCat: https://github.com/JeCat/
ThinkPHP: https://github.com/top-think/
Laravel: https://github.com/laravel/
Symfony: https://github.com/symfony/
Doctrine: https://github.com/doctrine/
Phalcon: https://github.com/phalcon/

License
The QueryPHP framework is open-sourced software licensed under the MIT license.
",112
stoplightio/json,TypeScript,"@stoplight/json
 
Useful functions when working with JSON.

Explore the interfaces: TSDoc
View the changelog: Releases

Installation
Supported in modern browsers and node.
# latest stable
yarn add @stoplight/json
Usage

decycle: Remove circular references with support for an optional replacer.
parseWithPointers: Like JSON.parse(val) but also returns parsing errors as well as full ast with line information.
pathToPointer: Turns an array of path segments into a json pointer IE ['paths', '/user', 'get'] -> #/paths/~1/user/get.
pointerToPath: Turns a json pointer into an array of path segments IE #/paths/~1/user/get -> ['paths', '/user', 'get'].
safeParse: Like JSON.parse(val) but does not throw on invalid JSON.
safeStringify: Like JSON.stringify(val) but handles circular references.
startsWith: Like native JS x.startsWith(y) but works with strings AND arrays.
trimStart: Like lodash.startsWith(x, y) but works with strings AND arrays.
getJsonPathForPosition: Computes JSON path for given position.
getLocationForJsonPath: Retrieves location of node matching given JSON path.

Example parseWithPointers
Note: Unlike most of the other functions, parseWithPointers is not exported from root. You must import by name.
import { parseWithPointers } from ""@stoplight/json/parseWithPointers"";

const result = parseWithPointers('{""foo"": ""bar""}');

console.log(result.data); // => the {foo: ""bar""} JS object
console.log(result.pointers); // => the source map with a single ""#/foo"" pointer that has position info for the foo property
// basic example of getJsonPathForPosition and getLocationForJsonPath
import { getJsonPathForPosition, getLocationForJsonPath, parseWithPointers } from ""@stoplight/json"";

const result = parseWithPointers(`{
  ""hello"": ""world"",
  ""address"": {
    ""street"": 123
  }
}`);

const path = getJsonPathForPosition(result, { line: 3, character: 15 }); // line and character are 0-based
console.log(path); // -> [""address"", ""street""];

const position = getLocationForJsonPath(result, [""address""]);
console.log(position.range.start); // { line: 2, character: 13 } line and character are 0-based
console.log(position.range.end); // { line: 4, character: 3 } line and character are 0-based
Contributing

Clone repo.
Create / checkout feature/{name}, chore/{name}, or fix/{name} branch.
Install deps: yarn.
Make your changes.
Run tests: yarn test.prod.
Stage relevant files to git.
Commit: yarn commit. NOTE: Commits that don't follow the conventional format will be rejected. yarn commit creates this format for you, or you can put it together manually and then do a regular git commit.
Push: git push.
Open PR targeting the next branch.

",2
jlord/patchwork,HTML,"Patchwork

This repository is a website hosted by GitHub Pages. It goes along with Git-it, a desktop application for learning Git and GitHub.
Users fork this repository and learn things like (forking) branching, adding collaborators, pulling in changes, pushing to a remote branch and submitting pull request. Once their pull request comes in, @reporobot rewrites the index.html here to include the user.
So much social coding goodness! 
",873
0xProject/0x-monorepo,TypeScript,"

0x is an open protocol that facilitates trustless, low friction exchange of Ethereum-based assets. For more information on how it works, check out the 0x protocol specification.
This repository is a monorepo including the 0x protocol smart contracts and numerous developer tools. Each public sub-package is independently published to NPM.
If you're developing on 0x now or are interested in using 0x infrastructure in the future, please join our developer mailing list for updates.





Packages
Visit our developer portal for a comprehensive list of core & community maintained packages. All packages maintained with this monorepo are listed below.
Python Packages



Package
Version
Description




0x-contract-addresses

A tiny utility library for getting known deployed contract addresses for a particular network


0x-contract-artifacts

0x smart contract compilation artifacts


0x-json-schemas

0x-related JSON schemas


0x-order-utils

A set of utilities for generating, parsing, signing and validating 0x orders


0x-sra-client

A Python client for interacting with servers conforming to the Standard Relayer API specification



Solidity Packages
These packages are all under development. See /contracts/README.md for a list of deployed packages.



Package
Version
Description




@0x/contracts-asset-proxy

AssetProxy contracts used within the protocol


@0x/contracts-erc20

Implementations of various ERC20 tokens


@0x/contracts-erc721

Implementations of various ERC721 tokens


@0x/contracts-erc1155

Implementations of various ERC1155 tokens


@0x/contracts-exchange

The Exchange contract used for settling trades within the protocol


@0x/contracts-exchange-forwarder

A Forwarder contract used to simplify UX for interacting with the protocol


@0x/contracts-exchange-libs

Protocol specific libraries used within the Exchange contract


@0x/contracts-extensions

Contracts that interact with and extend the functionality of the core protocol


@0x/contracts-multisig

Various implementations of multisignature wallets, including the AssetProxyOwner contract that has permissions to upgrade the protocol


@0x/contracts-test-utils

Typescript/Javascript shared utilities used for testing contracts


@0x/contracts-utils

Generic libraries and utilities used throughout all of the contracts


@0x/contracts-coordinator

A contract that allows users to execute 0x transactions with permission from a Coordinator



Typescript/Javascript Packages
0x-specific packages



Package
Version
Description




0x.js

An aggregate package combining many smaller utility packages for interacting with the 0x protocol


@0x/contract-addresses

A tiny utility library for getting known deployed contract addresses for a particular network.


@0x/contract-wrappers

JS/TS wrappers for interacting with the 0x smart contracts


@0x/order-utils

A set of utilities for generating, parsing, signing and validating 0x orders


@0x/json-schemas

0x-related JSON schemas


@0x/order-watcher

An order watcher daemon that watches for order validity


@0x/migrations

Migration tool for deploying 0x smart contracts on private testnets


@0x/contract-artifacts

0x smart contract compilation artifacts


@0x/abi-gen-wrappers

Low-level 0x smart contract wrappers generated using @0x/abi-gen


@0x/sra-spec

OpenAPI specification for the Standard Relayer API


@0x/connect

An HTTP/WS client for interacting with the Standard Relayer API


@0x/asset-buyer

Convenience package for discovering and buying assets with Ether



Ethereum tooling



Package
Version
Description




@0x/web3-wrapper

An Ethereum JSON RPC client


@0x/sol-compiler

A wrapper around solc-js that adds smart re-compilation, ability to compile an entire project, Solidity version specific compilation, standard input description support and much more.


@0x/sol-coverage

A solidity test coverage tool


@0x/sol-profiler

A solidity gas cost profiler


@0x/sol-trace

A solidity stack trace tool


@0x/sol-resolver

Import resolver for smart contracts dependencies


@0x/subproviders

Web3 provider middlewares (e.g. LedgerSubprovider)


@0x/sol-doc

Solidity documentation generator



Utilities



Package
Version
Description




@0x/abi-gen

Tool to generate TS wrappers from smart contract ABIs


@0x/tslint-config

Custom TSLint rules used by the 0x core team


@0x/types

Shared type declarations


@0x/typescript-typings

Repository of types for external packages


@0x/utils

Shared utilities


@0x/react-docs

React documentation component for rendering TypeDoc & sol-doc generated JSON


@0x/react-shared

0x shared react components


@0x/assert

Type and schema assertions used by our packages


@0x/base-contract

BaseContract used by auto-generated abi-gen wrapper contracts


@0x/dev-utils

Dev utils to be shared across 0x packages


@0x/fill-scenarios

0x order fill scenario generator



Private Packages



Package
Description




@0x/instant
A free and flexible way to offer simple crypto purchasing in any app or website.


@0x/testnet-faucets
A faucet micro-service that dispenses test ERC20 tokens or Ether


@0x/website
0x website



Usage
Node version >= 6.12 is required.
Most of the packages require additional typings for external dependencies.
You can include those by prepending the @0x/typescript-typings package to your typeRoots config.
""typeRoots"": [""node_modules/@0x/typescript-typings/types"", ""node_modules/@types""],
Contributing
We strongly recommend that the community help us make improvements and determine the future direction of the protocol. To report bugs within this package, please create an issue in this repository.
Read our contribution guidelines.
Install dependencies
Make sure you are using Yarn v1.9.4. To install using brew:
brew install yarn@1.9.4
Then install dependencies
yarn install
Build
To build all packages:
yarn build
To build a specific package:
PKG=@0x/web3-wrapper yarn build
To build all contracts packages:
yarn build:contracts
Watch
To re-build all packages on change:
yarn watch
To watch a specific package and all it's dependent packages:
PKG=[NPM_PACKAGE_NAME] yarn watch

e.g
PKG=@0x/web3-wrapper yarn watch
Clean
Clean all packages:
yarn clean
Clean a specific package
PKG=0x.js yarn clean
Rebuild
To re-build (clean & build) all packages:
yarn rebuild
To re-build (clean & build) a specific package & it's deps:
PKG=0x.js yarn rebuild
Lint
Lint all packages:
yarn lint
Lint a specific package:
PKG=0x.js yarn lint
Run Tests
Run all tests:
yarn test
Run a specific package's test:
PKG=@0x/web3-wrapper yarn test
Run all contracts packages tests:
yarn test:contracts
",1040
PassiveModding/RavenBOT,C#,"

RavenBOT






A discord Example bot that uses RavenDB for document storage.
Features
Custom Context, Database Support, Interactive Commands, Sharding & More
Contributing
Feel free to make a pull request, report issues etc. I would love to see everyone's input!
",6
Lombiq/Orchard-Azure-Indexing,C#,"Orchard Azure Search Indexing Readme
Project Description
Orchard module with a search indexing implementation that stores Lucene indices in Azure Blob storage.
Documentation
The module uses AzureDirectory to store Lucene search indices in Azure Blob storage (with a local cache) so it isn't stored on the web server's local storage. It contains a search index provider that extends and overrides the default Lucene provider. Thus after enabling the features of this module indices will be stored in Blob storage, but first you have to configure the storage: take a look at the Constants class and add entries to the appSettings or connectionStrings in the Web.config (or through the Azure Portal) corresponding to those configuration keys.
AzureDirectory is included as source to avoid a mismatch of assemblies (the project used a previous version of Azure assemblies). The actual code that's included is from https://github.com/richorama/AzureDirectory.
The module is also available for DotNest sites.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/hosting-azure-indexing (Mercurial repository)
https://github.com/Lombiq/Orchard-Azure-Indexing (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",3
bappogroup/bappo-components,JavaScript,"bappo-components
Cross-platform React components for building Bappo apps
Installation
npm i --save bappo-components
Usage
import React from 'react';
import { styled, Text, View } from 'bappo-components';

class MyComponent extends React.Component {
  render() {
    return (
      <Container>
        <Text>Hello World</Text>
      </Container>
    );
  }
}

const Container = styled(View)`
  flex: 1;
  background-color: white;
`;
Using built-in Icon components
Web (with webpack)
In webpack config file, use url-loader or file-loader to handle ttf files:
{
  test: /\.ttf$/,
  loader: 'url-loader',
  include: path.resolve(__dirname, 'node_modules/bappo-components'), // path to bappo-components
}
Then in your JavaScript entry point, inject a style tag:
import fontAwesome from 'bappo-components/fonts/FontAwesome.ttf';
const fontStyles = `@font-face { src:url(${fontAwesome});font-family: FontAwesome; }`;

// create stylesheet
const style = document.createElement('style');
style.type = 'text/css';
if (style.styleSheet) {
  style.styleSheet.cssText = fontStyles;
} else {
  style.appendChild(document.createTextNode(fontStyles));
}

// inject stylesheet
document.head.appendChild(style);
iOS

Browse to node_modules/bappo-components and drag the folder fonts to your project in Xcode. Make sure your app is checked under ""Add to targets"" and that ""Create groups"" is checked.
Edit Info.plist and add a property called Fonts provided by application (or UIAppFonts if Xcode won't autocomplete/not using Xcode) and type in the files you just added.
Note: you need to recompile your project after adding new fonts, also ensure that they also appear under Copy Bundle Resources in Build Phases.

Android
Edit android/app/build.gradle ( NOT android/build.gradle ) and add the following:
apply from: ""../../node_modules/bappo-components/fonts.gradle""
To customize the files being copied, add the following instead:
project.ext.vectoricons: [
    iconFontNames: [""FontAwesome.ttf""] // Name of the font files you want to copy
]
apply from: ""../../node_modules/bappo-components/fonts.gradle""
Credits

This library is inspired by ReactXP and React Primitives.
This library's built-in Icon components are inspired by react-native-vector-icons.

",2
Julian/jsonschema,Python,"jsonschema

 
   
jsonschema is an implementation of JSON Schema
for Python (supporting 2.7+ including Python 3).
>>> from jsonschema import validate

>>> # A sample schema, like what we'd get from json.load()
>>> schema = {
...     ""type"" : ""object"",
...     ""properties"" : {
...         ""price"" : {""type"" : ""number""},
...         ""name"" : {""type"" : ""string""},
...     },
... }

>>> # If no exception is raised by validate(), the instance is valid.
>>> validate(instance={""name"" : ""Eggs"", ""price"" : 34.99}, schema=schema)

>>> validate(
...     instance={""name"" : ""Eggs"", ""price"" : ""Invalid""}, schema=schema,
... )                                   # doctest: +IGNORE_EXCEPTION_DETAIL
Traceback (most recent call last):
    ...
ValidationError: 'Invalid' is not of type 'number'
It can also be used from console:
$ jsonschema -i sample.json sample.schema

Features

Full support for
Draft 7,
Draft 6,
Draft 4
and
Draft 3
Lazy validation
that can iteratively report all validation errors.
Programmatic querying
of which properties or items failed validation.


Installation
jsonschema is available on PyPI. You can install using pip:
$ pip install jsonschema

Demo
Try jsonschema interactively in this online demo:

Online demo Notebook will look similar to this:


Release Notes
Version 3.0 brings support for Draft 7 (and 6). The interface for redefining
types has also been majorly overhauled to support easier redefinition of the
types a Validator will accept or allow.
jsonschema is also now tested under Windows via AppVeyor.
Thanks to all who contributed pull requests along the way.

Running the Test Suite
If you have tox installed (perhaps via pip install tox or your
package manager), running tox in the directory of your source
checkout will run jsonschema's test suite on all of the versions
of Python jsonschema supports. If you don't have all of the
versions that jsonschema is tested under, you'll likely want to run
using tox's --skip-missing-interpreters option.
Of course you're also free to just run the tests on a single version with your
favorite test runner. The tests live in the jsonschema.tests package.

Benchmarks
jsonschema's benchmarks make use of perf.
Running them can be done via tox -e perf, or by invoking the perf
commands externally (after ensuring that both it and jsonschema itself are
installed):
$ python -m perf jsonschema/benchmarks/test_suite.py --hist --output results.json

To compare to a previous run, use:
$ python -m perf compare_to --table reference.json results.json

See the perf documentation for more details.

Community
There's a mailing list
for this implementation on Google Groups.
Please join, and feel free to send questions there.

Contributing
I'm Julian Berman.
jsonschema is on GitHub.
Get in touch, via GitHub or otherwise, if you've got something to contribute,
it'd be most welcome!
You can also generally find me on Freenode (nick: tos9) in various
channels, including #python.
If you feel overwhelmingly grateful, you can woo me with beer money via
Google Pay with the email in my GitHub profile.
",2270
haskell-works/hw-kafka-avro,Haskell,"kafka-avro-serialiser
Avro serialiser/deserialiser for Kafka messages. Uses SchemaRegistry for schema compatibility and discoverability functionality.
This library is meant to be compatible (on both sending and receiving sides) with Java kafka/avro serialiser (written by Confluent).
",9
groonga/groonga,C,"README
Groonga is an open-source fulltext search engine and column store.
Reference manual
See doc/source/ directory or http://groonga.org/docs/.
Here are shortcut links:

How to install: http://groonga.org/docs/install.html
Tutorial: http://groonga.org/docs/tutorial.html
How to build as a developer: http://groonga.org/docs/contribution/development/build.html

Community

@groonga on Twitter
Groonga page on Facebook
Mailing list on SourceForge.net
Chat room on Gitter

Bundled software
mruby

Path: vendor/mruby-source
License: The MIT license. See vendor/mruby-source/MITL for details.

Onigmo

Path: vendor/onigmo-source
License: BSD license. See vendor/onigmo-source/COPYING for details.

nginx

Path: vendor/nginx-${VERSION}
License: BSD license. See vendor/nginx-${VERSION}/LICENSE for details.

Authors
Primary authors

Daijiro MORI 
Tasuku SUENAGA 
Yutaro Shimamura 
Kouhei Sutou 
Kazuho Oku 
Moriyoshi Koizumi 

Patches and modules from
TODO: Update or use
https://github.com/groonga/groonga/graphs/contributors instead.

Daisuke Maki 
Kazuhiro Osawa 
Hiroyuki OYAMA 
Nguyen Anh Phu 
Hideyuki KUROSU <hideyuki. kurosu at gmail. com>
Takuo Kitame 
Yoshihiro Oyama 
cZfSunOs.U 

",487
closeio/closeio-api-scripts,Python,"closeio-api-scripts
Example Python scripts for interacting with Close through its API
using the closeio_api Python client.
Install basic dependencies
Before you start, you should already have git, python (2.7) and virtualenv installed. For OS X users, we recommend Homebrew.
Setup

git clone https://github.com/closeio/closeio-api-scripts.git
cd closeio-api-scripts
virtualenv venv
. venv/bin/activate
pip install -r requirements.txt

Running a script
Example:
python scripts/run_leads_deleted_report.py -k MYAPIKEY 
...

If you have any questions, please contact support@close.com.

Documentation for individual scripts
How to run the CSV importing script
The script will look for your CSV to have specific column names (case insensitive). All columns are optional. All columns not listed below will be imported as custom fields.

company (multiple contacts will be grouped if rows have the same company
url (must start with http:// or https://)
status (defaults to ""potential"")
contact (full name of contact)
title (job title of contact)
email (must be a valid email address)
phone (must be a valid phone number, and must start with a ""+"" if it's a non-US number)
mobile_phone
fax
address (street address)
city
state (2 letter abbreviation)
zip
country (2 letter abbreviation)
(any additional fields will be added as custom fields)

Multiple contacts will be grouped in the same lead if multiple rows have the same value in the ""company"" column.


Make sure (if you haven't already in Setup) you're in the closeio-api directory and you have activated your virtual environment by running . venv/bin/activate.


Run the import script: ./scripts/csv_to_cio.py --api_key YOUR_API_KEY_HERE ~/path/to/your/leads.csv


You can generate an API Key from Settings in Close.
",7
systemd/systemd,C,"systemd - System and Service Manager









Details
General information about systemd can be found in the systemd Wiki.
Information about build requirements is provided in the README file.
Consult our NEWS file for information about what's new in the most recent systemd versions.
Please see the Hacking guide for information on how to hack on systemd and test your modifications.
Please see our Contribution Guidelines for more information about filing GitHub Issues and posting GitHub Pull Requests.
When preparing patches for systemd, please follow our Coding Style Guidelines.
If you are looking for support, please contact our mailing list or join our IRC channel.
Stable branches with backported patches are available in the stable repo.
",4813
IronsDu/gayrpc,C++,"gayrpc
基于Protobuf协议的跨平台(Linux和Windows)全双工双向(异步)RPC系统,也即通信两端都可以同时作为服务方和客户端,彼此均可请求对方的服务.
Windows :  Linux:
动机

目前的RPC系统大多用于互联网行业后端系统，他们之间更像一个单向图，但游戏等行业中很常见两个节点之间互相主动请求数据。
因此我们需要一个全双工RPC，在一个“链接”（虚拟概念，不一定基于TCP，且两者之间只存在逻辑链接而没有网络直连）的两端都可以开启服务或到对端的客户端。
因为目前很多RPC系统都不是C++写的，而我常用语言是C++，且觉得目前版本C++的开发效率和细节控制非常不错，决定写一个试试。
有朋友觉得我之前基于C++泛型开发的RPC必定没法做得太强大，因此改为gayrpc这种基于代码生成来做，试试看。

设计准则

RPC支持拦截器，能够对Request或Response做一些处理(比如监控、认证、加解密)
RPC核心不依赖网络和网络传输协议，即：我们可以开发任何网络应用和逻辑来开启RPC两端，将“收到”的消息丢给RPC核心，并通过某个出站拦截器来实现/决定把Request或Response以何种方式传递给谁。
此RPC是基于异步回调的，我认为这是目前C++里比较安全和靠谱的方式，除了回调地狱让人恶心……，不过可以通过将Lambda抽离出来而不是嵌套稍微好看点吧？
RPC系统核心（以及接口）是线程安全的，可以在任意线程调用RPC；且可以在任意线程使用XXXReply::PTR对象返回Response。
RPC是并行的，也即：客户端可以随意发送Request而不必等待之前的完成。 且允许先收到后发出的Request的Response。或许这会让某些业务编写困难，又会陷入回调地狱……
RPC系统会为每一个“链接”生成一个XXXService对象，这样可以让不同的“链接”绑定/持有各自的业务对象（session），这点可以在下面的服务范例中看到。（而不是像grpc等系统那样，一个服务只存在一个service，而RPC调用则是类似短链接：收到请求返回数据即可）

依赖
Windows下可使用 vcpkg 进行安装以下依赖库.

protobuf
brynet

请注意,当使用Windows时,务必使用vcpkg install brynet --head安装brynet.
且务必根据自身系统中的protoc版本对gayrpc_meta.proto和gayrpc_option.proto预先生成代码，请在 src目录里执行:
 protoc --cpp_out=. ./gayrpc/core/gayrpc_meta.proto ./gayrpc/core/gayrpc_option.proto
代码生成工具
地址：https://github.com/IronsDu/protoc-gen-gayrpc，由liuhan编写完成。
首先将插件程序放到系统 PATH路径下(比如Linux下的/usr/bin)，然后执行代码生成，比如（在具体的服务目录里，比如gayrpc/examples/echo/pb）:
 protoc  -I. -I../../../src --cpp_out=. echo_service.proto
 protoc  -I. -I../../../src --gayrpc_out=. echo_service.proto
Benchmark
connection num:5000
took 20349ms, for 2000000 requests
throughput  (TPS):100000
mean:46 ms ,46509491 ns
median:42 ms ,42219427 ns
max:336 ms ,336657507 ns
min:0 ms ,21056 ns
p99:45 ms ,45382276 ns
协议
目前RPC通信协议底层采用两层协议.
第一层采用二进制协议,且字节序统一为大端.
通信格式如下:
[data_len | op | data]
字段解释:
data_len : uint64_t;
op       : uint32_t;
data     : char[data_len];

当op值为1时表示RPC消息,此为第二层协议!这时第一层协议中的data的内存布局则为:
[meta_size | data_size | meta | data]
字段解释:
meta_size  : uint32_t;
data_size  : uint64_t;
meta       : char[meta_size];
data       : char[data_size];

其中meta为 RpcMata的binary.data为某业务上的Protobuf Request或Response类型对象的binary或JSON.
RpcMata的proto定义如下:
syntax = ""proto3"";

package gayrpc.core;

message RpcMeta {
    enum Type {
        REQUEST = 0;
        RESPONSE = 1;
    };

    enum DataEncodingType {
        BINARY = 0;
        JSON = 1;
    };

    message Request {
        // 请求的服务函数
        uint64  method = 1;
        // 请求方是否期待服务方返回response
        bool    expect_response = 2;
        // 请求方的序号ID
        uint64  sequence_id = 3;
    };

    message Response {
        // 请求方的序号ID
        uint64  sequence_id = 1;
        // 执行是否成功
        bool    failed = 2;
        // (当failed为true)错误码
        int32   error_code = 3;
        // (当failed为true)错误原因
        string  reason = 4;
    };
    
    // Rpc类型(请求、回应)
    Type    type = 1;
    // RpcData的编码方式
    DataEncodingType encoding = 2;
    // 请求元信息
    Request request_info = 3;
    // 回应元信息
    Response response_info = 4;
}
服务描述文件范例
以下面的服务定义为例:
syntax = ""proto3"";

package dodo.test;

message EchoRequest {
    string message = 1;
}

message EchoResponse {
    string message = 1;
}

service EchoServer {
    rpc Echo(EchoRequest) returns(EchoResponse){
        option (gayrpc.core.message_id)= 1 ;//设定消息ID,也就是rpc协议中request_info的method
    };
}
处理请求或Response的实现原理

编写第一层通信协议的编解码
将第一层中的data作为第二层协议数据,反序列化其中的meta作为RpcMeta对象
判断RpcMata中的type

如果为REQUEST则根据request_info中的元信息调用method所对应的服务函数.
此时第二层协议中的data则为服务函数的请求请求对象(比如EchoRequest).
如果为RESPONSE则根据response_info中的元信息调用sequence_id对应的回调函数.
此时第二层协议中的data则为服务方返回的Response(比如EchoResponse)



发送请求的实现原理
以client->echo(echoRequest, responseCallback)为例
参考代码:GayRpcClient.h

客户端分配 sequence_id,以它为key将 responseCallback保存起来.
将echoRequest序列化为binary作为第二层协议中的data
构造RpcMeta对象,将echo函数对应的id号作为request_info的method,并设置sequence_id.
将RpcMeta对象的binary作为第二层协议中的meta
用第二层协议的数据写入第一层协议进行发送给服务端.

发送Response的实现原理
以replyObj->reply(echoResponse)为例
参考代码:GayRpcReply.h

首先replyObj里(拷贝)储存了来自于请求中的RpcMata对象.
将echoResponse序列化为binary作为第二层协议中的data
构造RpcMeta对象,将备份的RpcMeta对象中的sequence_id设置到前者中response_info的sequence_id.
将RpcMeta对象的binary作为第二层协议中的meta
用第二层协议的数据写入第一层协议进行发送给服务端.

编解码参考

对于发送请求或Response都可以走出站拦截器,用于统一的发送消息,最终的序列化参考代码：
UtilsDataHandler.h
对于接收请求或Response在编解码之后都可以交给入站拦截器,解码参考代码:
OpPacket.h

注意点

通信协议的第一层并不是重点,RPC核心在实现上尽量不要依赖它(目前的第一层协议只是某一种范例)
同样,RPC核心并不依赖通信采用的传输协议,可以是TCP也可以是UDP或者WebSocket
RPC服务方的reply顺序与客户端的调用顺序无关,也就是可能后发起的请求先得到返回.
目前RPC系统没有提供超时控制

",36
ElisaMin/Manual-For-LGG7-Chinese-Guys,None,"Another Github Pages
",3
damng/hackernews-rss-with-inlined-content,Python,"hackernews-rss-inlined-content
Loads the hackerness rss and inlines the contents of the pages. Chrome with Selenium loads the page, dom-distiller makes the contents like they're in firefox's reader mode, and the resulting html is served as the entry description. The 300 or so entries become about 5mb. PDFs and things that yield no usible text preview will remain as they were on the old feed.
I invoke it as:
  xvfb-run python main.py

I used xvfb-run instead of headless mode because extensions are not supported in headless. One directory up, in ""../dat"", is a chrome user profile data directory that is copied for each instance of the browser run and deleted when finished. You can initialize it and add whatever extensions you want. The resulting rss file is then commited/pushed on here and served via gitpages.
This is hack level code.
The feed is available at https://damng.github.io/hackernews-rss-with-inlined-content/output.rss
If you find this useful, don't give me anything. Instead, go to http://templeos.org, donate to the TempleOS project and become a Templar. Dontate to the Brain and Behavior Research Foundation (https://www.bbrfoundation.org/).
",10
ejolie/moviemovie,HTML,"🎞 미음비읍
SSAFY 마지막 프로젝트
 
Getting Started
 
Built with
Frontend

Vue : ^2.6.10
Bulma
Font Awesome

Backend

Django : 2.1
Django REST framework : 3.9.4
Python : 3.6.7

 
Deploy
",2
learningequality/studio,Python,"Kolibri Studio

Check out our beta site!
Kolibri Studio is a web application designed to deliver educational materials to Kolibri.
Kolibri Studio supports the following workflows:


Organizing and publishing content channels in the format suitable for import from Kolibri.


Curating content and remixing of existing channels into custom channels aligned to various educational standards, country curricula, and special needs.


Creating learning pathways and assessments.


Uploading new content through the web interface or programatically using ricecooker-powered content import scripts.


Kolibri Studio uses Django for the backend and is transitioning from Backbone.js to Vue.js for the frontend.
Developer Instructions
Follow the instructions below to setup your dev environment and get started. (Note: docs/docker_setup has instructions for setting up your environment using docker, but this is currently a work in progress.)
0. Get the code


Fork the studio repo to create a copy of the studio repository under your own github username.
cd <project directory>
git clone git@github.com:<yourusername>/studio.git


The folder <project directory>/studio now contains the latest Studio code.


For more information on using git, please check out docs/git_setup


1. Install software prerequisites
You need the following software installed on your machine to run Studio:

python (2.7)
python-pip
nodejs (10.x)
Postgres DB
redis
minio server
nginx
ffmpeg
python-tk
libmagickwand-dev
yarn

You can also use nodeenv (which is included as a python development dependency below) or nvm to install Node.js 10.x if you need to maintain multiple versions of node:

http://ekalinin.github.io/nodeenv/
https://github.com/creationix/nvm

Ubuntu or Debian
You can install all the necessary packages using these commands (you may need to add sudo if you receive Permission Denied errors:
# Install minio
wget https://dl.minio.io/server/minio/release/linux-amd64/minio -O /usr/local/bin/minio
chmod +x /usr/local/bin/minio

# Install node PPA
curl -sL https://deb.nodesource.com/setup_10.x | bash -

# Install packages
apt-get install -y  python python-pip python-dev python-tk \
    postgresql-server-dev-all postgresql-contrib postgresql-client postgresql \
    ffmpeg nodejs libmagickwand-dev nginx redis-server wkhtmltopdf
Mac OS X
You can install the corresponding packages using Homebrew:
brew install  postgresql@9.6 redis node ffmpeg imagemagick@6 gs
brew install minio/stable/minio
brew link --force postgresql@9.6
brew link --force imagemagick@6
Windows
Windows is no longer supported due to incompatibilities with some of the required packages.
2. Set up python dependencies through pipenv
If you haven't installed pipenv,
pip install -U pipenv
Then set up:
# Create virtual environment
pipenv shell

# Ensure your environment matches the one specified in Pipfile.lock
pipenv sync
Exit the virtual environment by running exit. Reactivate it by running pipenv shell again.
3. Set up pre-commit hooks
We use pre-commit to help ensure consistent, clean code. The pip package should already be installed from a prior setup step, but you need to install the git hooks using this command.
pre-commit install
Note: you may need to run pip install pre-commit if you see pre-commit command not found
4. Install javascript dependencies
As described above, Kolibri Studio has dependencies that rely on Node.js version 10.x. You'll also need yarn installed.
All the javascript dependencies are listed in package.json. To install them run the following yarn command:
yarn install
This may take a long time.
If you encounter a ESOCKETTIMEDOUT error related to material-design-icons, you can increase your timeout by setting network-timeout 600000 inside ~/.yarnrc.
5. Set up the database and start redis
Install postgres if you don't have it already. If you're using a package manager, you need to make sure you install the following packages: postgresql, postgresql-contrib, and postgresql-server-dev-all which will be required to build psycopg2 python driver.
Make sure postgres is running:
service postgresql start
# alternatively: pg_ctl -D /usr/local/var/postgresql@9.6 start
Start the client with:
sudo su postgres  # switch to the postgres account
psql  # mac: psql postgres
Create a database user with username learningequality and password kolibri:
CREATE USER learningequality with NOSUPERUSER INHERIT NOCREATEROLE CREATEDB LOGIN NOREPLICATION NOBYPASSRLS PASSWORD 'kolibri';
Create a database called kolibri-studio:
CREATE DATABASE ""kolibri-studio"" WITH TEMPLATE = template0 ENCODING = ""UTF8"" OWNER = ""learningequality"";
Press Ctrl+D to exit the psql client. Finally
exit  # leave the postgres account
6. Run all database migrations and load constants
These commands setup the necessary tables and contents in the database.
In one terminal, run all external services:
yarn run services
In another terminal, run devsetup to create all the necessary tables and buckets:
yarn run devsetup
When this completes, close the second tab and kill the services.
7. Start the dev server
You're all set up now, and ready to start the Studio local development server:
On Macs only run this in another terminal first:
yarn run services
Start the server:
yarn run devserver
Once you see the following output in your terminal, the server is ready:
Starting development server at http://0.0.0.0:8080/
Quit the server with CONTROL-C.

You should be able to login at http://127.0.0.1:8080 using email a@a.com, password a.
Note: If you are using a Linux environment, you may need to increase the amount of listeners to allow the watch command to automatically rebuild static assets when you edit them. Please see here for instructions on how to do so.
Running tests
You can run tests using the following command:
yarn run test
For more testing tips, please check out docs/running_tests.
Linting
Front-end linting is run using:
yarn run lint-all
Some linting errors can be fixed automatically by running:
yarn run lint-all:fix
Make sure you've set up pre-commit hooks by following the instructions here.  This will ensure that linting is automatically run on staged changes before every commit.
",16
EMFTeam/EMF,Python,"EMF
Extended Mechanics & Flavor
",23
vipickering/vincentp,HTML,"Vincent Pickering [blog]
   

Requirements
This blog runs on Jekyll.
Node packages are used with PostCSS to compile the CSS.
It uses Bundler to package the Gems.
Deployment and hosting is via Netlify.
Mastr Cntrl supplies webmentions and other indieweb content such as Micropub, syndication and a media endpoint.
Ruby version 2.4.3 is a minimum requirement.
Setup
Clone the directory and install the Node packages:
npm install

Install the gems you need:
bundle install

Running
To run in the development environment run the command:
npm run dev

If you need to modify the CSS in any way, you can run the command:
npm run watch:css

To watch the CSS for any changes and recompile automatically.
If you just want to (re)build the CSS run:
npm run build:css

Configuration

Post CSS is configured in the postcss.config.js file.
Stylelint is configured in the .stylelintrc file and postcss.config.js file.
Netlify configuration is done via netlify.toml file.

Deploy
The site is served by Netlify. Deployment happens automatically when the master branch is pushed to GitHub.
If you want to manually see the live compiled site with HTTPS URLs run:
jekyll build

Syndication
Content is syndicated to Medium and pushed to a draft via Zapier and a custom Javascript.
Webmentions


Brid.gy


Webmention.io - Webmentions are managed by Webmention.io and sent to Mastr Cntrl; Which creates a unique file for each Webmention in the data folder.


Webmentions can be disabled on a per page basis by adding disableWebmention: true in the frontmatter


Publish Elsewhere, Syndicate (to your) Own Site(PESOS) Content
Backfeed content is provided by ownyourswarm and ownyourgram and published to the site via the Micropub API in Mastr Cntrl
License

",5
tensorflow/tensorflow,C++,"






Documentation








TensorFlow is an open source software library for numerical computation
using data flow graphs. The graph nodes represent mathematical operations, while
the graph edges represent the multidimensional data arrays (tensors) that flow
between them. This flexible architecture enables you to deploy computation to
one or more CPUs or GPUs in a desktop, server, or mobile device without
rewriting code. TensorFlow also includes
TensorBoard, a data visualization
toolkit.
TensorFlow was originally developed by researchers and engineers
working on the Google Brain team within Google's Machine Intelligence Research
organization for the purposes of conducting machine learning and deep neural
networks research.  The system is general enough to be applicable in a wide
variety of other domains, as well.
TensorFlow provides stable Python and C APIs as well as non-guaranteed backwards
compatible API's for C++, Go, Java, JavaScript, and Swift.
Keep up to date with release announcements and security updates by
subscribing to
announce@tensorflow.org.
Installation
To install the current release for CPU-only:
pip install tensorflow

Use the GPU package for CUDA-enabled GPU cards:
pip install tensorflow-gpu

See Installing TensorFlow for detailed
instructions, and how to build from source.
People who are a little more adventurous can also try our nightly binaries:
Nightly pip packages * We are pleased to announce that TensorFlow now offers
nightly pip packages under the
tf-nightly and
tf-nightly-gpu project on PyPi.
Simply run pip install tf-nightly or pip install tf-nightly-gpu in a clean
environment to install the nightly TensorFlow build. We support CPU and GPU
packages on Linux, Mac, and Windows.
Try your first TensorFlow program
$ python
>>> import tensorflow as tf
>>> tf.enable_eager_execution()
>>> tf.add(1, 2).numpy()
3
>>> hello = tf.constant('Hello, TensorFlow!')
>>> hello.numpy()
'Hello, TensorFlow!'
Learn more examples about how to do specific tasks in TensorFlow at the
tutorials page of tensorflow.org.
Contribution guidelines
If you want to contribute to TensorFlow, be sure to review the contribution
guidelines. This project adheres to TensorFlow's
code of conduct. By participating, you are expected to
uphold this code.
We use GitHub issues for
tracking requests and bugs, please see
TensorFlow Discuss
for general questions and discussion, and please direct specific questions to
Stack Overflow.
The TensorFlow project strives to abide by generally accepted best practices in open-source software development:

Continuous build status
Official Builds



Build Type
Status
Artifacts




Linux CPU

pypi


Linux GPU

pypi


Linux XLA

TBA


MacOS

pypi


Windows CPU

pypi


Windows GPU

pypi


Android




Raspberry Pi 0 and 1
 
Py2 Py3


Raspberry Pi 2 and 3
 
Py2 Py3



Community Supported Builds



Build Type
Status
Artifacts




IBM s390x

TBA


Linux ppc64le CPU Nightly

Nightly


Linux ppc64le CPU Stable Release

Release


Linux ppc64le GPU Nightly

Nightly


Linux ppc64le GPU Stable Release

Release


Linux CPU with Intel® MKL-DNN Nightly

Nightly


Linux CPU with Intel® MKL-DNN  Supports Python 2.7, 3.4, 3.5, and 3.6

1.13.1 pypi


Red Hat® Enterprise Linux® 7.6 CPU & GPU  Python 2.7, 3.6

1.13.1 pypi



For more information

TensorFlow Website
TensorFlow Tutorials
TensorFlow Model Zoo
TensorFlow Twitter
TensorFlow Blog
TensorFlow Course at Stanford
TensorFlow Roadmap
TensorFlow White Papers
TensorFlow YouTube Channel
TensorFlow Visualization Toolkit

Learn more about the TensorFlow community at the community page of tensorflow.org for a few ways to participate.
License
Apache License 2.0
",127703
Kasugaccho/DungeonTemplateLibrary,C++,"DTL (Dungeon Template Library) 

>> Japanese(日本語ページはこちら)
Download
>> SDK Downloads
>> Web Trial (~0.2.4)
Overview
Version 0.4.7 [ C++11/14/17 ]



Compiler
C++17
C++14




MSVC
14.10-passing
14.10-passing


GCC
unknown
5.1.0-passing


Clang
5.0.0-passing
3.8.1-passing


Zapcc
unknown
1.0.1-passing


ICC
unknown
unknown



API reference
>> API reference
>> Roadmap

License
Copyright (c) 2017-2019 Kasugaccho.
Copyright (c) 2018-2019 As Project.
Distributed under the Boost Software License, Version 1.0.(See accompanying file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)
Contact



E-mail:
wanotaitei@gmail.com




Twitter:
@wanotaitei



Made by As Project.



Made by Gaccho.



",97
haskell-works/hw-kafka-avro,Haskell,"kafka-avro-serialiser
Avro serialiser/deserialiser for Kafka messages. Uses SchemaRegistry for schema compatibility and discoverability functionality.
This library is meant to be compatible (on both sending and receiving sides) with Java kafka/avro serialiser (written by Confluent).
",9
groonga/groonga,C,"README
Groonga is an open-source fulltext search engine and column store.
Reference manual
See doc/source/ directory or http://groonga.org/docs/.
Here are shortcut links:

How to install: http://groonga.org/docs/install.html
Tutorial: http://groonga.org/docs/tutorial.html
How to build as a developer: http://groonga.org/docs/contribution/development/build.html

Community

@groonga on Twitter
Groonga page on Facebook
Mailing list on SourceForge.net
Chat room on Gitter

Bundled software
mruby

Path: vendor/mruby-source
License: The MIT license. See vendor/mruby-source/MITL for details.

Onigmo

Path: vendor/onigmo-source
License: BSD license. See vendor/onigmo-source/COPYING for details.

nginx

Path: vendor/nginx-${VERSION}
License: BSD license. See vendor/nginx-${VERSION}/LICENSE for details.

Authors
Primary authors

Daijiro MORI 
Tasuku SUENAGA 
Yutaro Shimamura 
Kouhei Sutou 
Kazuho Oku 
Moriyoshi Koizumi 

Patches and modules from
TODO: Update or use
https://github.com/groonga/groonga/graphs/contributors instead.

Daisuke Maki 
Kazuhiro Osawa 
Hiroyuki OYAMA 
Nguyen Anh Phu 
Hideyuki KUROSU <hideyuki. kurosu at gmail. com>
Takuo Kitame 
Yoshihiro Oyama 
cZfSunOs.U 

",487
haskell-works/hw-xml,Haskell,"hw-xml

hw-xml is a high performance XML parsing library. It uses
succinct data-structures to allow traversal of large XML
strings with minimal memory overhead.
For an example, see app/Main.hs
Notes

Semi-Indexing Semi-Structured Data in Tiny Space
Space-Efficient, High-Performance Rank & Select Structures on Uncompressed Bit Sequences

",8
closeio/closeio-api-scripts,Python,"closeio-api-scripts
Example Python scripts for interacting with Close through its API
using the closeio_api Python client.
Install basic dependencies
Before you start, you should already have git, python (2.7) and virtualenv installed. For OS X users, we recommend Homebrew.
Setup

git clone https://github.com/closeio/closeio-api-scripts.git
cd closeio-api-scripts
virtualenv venv
. venv/bin/activate
pip install -r requirements.txt

Running a script
Example:
python scripts/run_leads_deleted_report.py -k MYAPIKEY 
...

If you have any questions, please contact support@close.com.

Documentation for individual scripts
How to run the CSV importing script
The script will look for your CSV to have specific column names (case insensitive). All columns are optional. All columns not listed below will be imported as custom fields.

company (multiple contacts will be grouped if rows have the same company
url (must start with http:// or https://)
status (defaults to ""potential"")
contact (full name of contact)
title (job title of contact)
email (must be a valid email address)
phone (must be a valid phone number, and must start with a ""+"" if it's a non-US number)
mobile_phone
fax
address (street address)
city
state (2 letter abbreviation)
zip
country (2 letter abbreviation)
(any additional fields will be added as custom fields)

Multiple contacts will be grouped in the same lead if multiple rows have the same value in the ""company"" column.


Make sure (if you haven't already in Setup) you're in the closeio-api directory and you have activated your virtual environment by running . venv/bin/activate.


Run the import script: ./scripts/csv_to_cio.py --api_key YOUR_API_KEY_HERE ~/path/to/your/leads.csv


You can generate an API Key from Settings in Close.
",7
systemd/systemd,C,"systemd - System and Service Manager









Details
General information about systemd can be found in the systemd Wiki.
Information about build requirements is provided in the README file.
Consult our NEWS file for information about what's new in the most recent systemd versions.
Please see the Hacking guide for information on how to hack on systemd and test your modifications.
Please see our Contribution Guidelines for more information about filing GitHub Issues and posting GitHub Pull Requests.
When preparing patches for systemd, please follow our Coding Style Guidelines.
If you are looking for support, please contact our mailing list or join our IRC channel.
Stable branches with backported patches are available in the stable repo.
",4813
IronsDu/gayrpc,C++,"gayrpc
基于Protobuf协议的跨平台(Linux和Windows)全双工双向(异步)RPC系统,也即通信两端都可以同时作为服务方和客户端,彼此均可请求对方的服务.
Windows :  Linux:
动机

目前的RPC系统大多用于互联网行业后端系统，他们之间更像一个单向图，但游戏等行业中很常见两个节点之间互相主动请求数据。
因此我们需要一个全双工RPC，在一个“链接”（虚拟概念，不一定基于TCP，且两者之间只存在逻辑链接而没有网络直连）的两端都可以开启服务或到对端的客户端。
因为目前很多RPC系统都不是C++写的，而我常用语言是C++，且觉得目前版本C++的开发效率和细节控制非常不错，决定写一个试试。
有朋友觉得我之前基于C++泛型开发的RPC必定没法做得太强大，因此改为gayrpc这种基于代码生成来做，试试看。

设计准则

RPC支持拦截器，能够对Request或Response做一些处理(比如监控、认证、加解密)
RPC核心不依赖网络和网络传输协议，即：我们可以开发任何网络应用和逻辑来开启RPC两端，将“收到”的消息丢给RPC核心，并通过某个出站拦截器来实现/决定把Request或Response以何种方式传递给谁。
此RPC是基于异步回调的，我认为这是目前C++里比较安全和靠谱的方式，除了回调地狱让人恶心……，不过可以通过将Lambda抽离出来而不是嵌套稍微好看点吧？
RPC系统核心（以及接口）是线程安全的，可以在任意线程调用RPC；且可以在任意线程使用XXXReply::PTR对象返回Response。
RPC是并行的，也即：客户端可以随意发送Request而不必等待之前的完成。 且允许先收到后发出的Request的Response。或许这会让某些业务编写困难，又会陷入回调地狱……
RPC系统会为每一个“链接”生成一个XXXService对象，这样可以让不同的“链接”绑定/持有各自的业务对象（session），这点可以在下面的服务范例中看到。（而不是像grpc等系统那样，一个服务只存在一个service，而RPC调用则是类似短链接：收到请求返回数据即可）

依赖
Windows下可使用 vcpkg 进行安装以下依赖库.

protobuf
brynet

请注意,当使用Windows时,务必使用vcpkg install brynet --head安装brynet.
且务必根据自身系统中的protoc版本对gayrpc_meta.proto和gayrpc_option.proto预先生成代码，请在 src目录里执行:
 protoc --cpp_out=. ./gayrpc/core/gayrpc_meta.proto ./gayrpc/core/gayrpc_option.proto
代码生成工具
地址：https://github.com/IronsDu/protoc-gen-gayrpc，由liuhan编写完成。
首先将插件程序放到系统 PATH路径下(比如Linux下的/usr/bin)，然后执行代码生成，比如（在具体的服务目录里，比如gayrpc/examples/echo/pb）:
 protoc  -I. -I../../../src --cpp_out=. echo_service.proto
 protoc  -I. -I../../../src --gayrpc_out=. echo_service.proto
Benchmark
connection num:5000
took 20349ms, for 2000000 requests
throughput  (TPS):100000
mean:46 ms ,46509491 ns
median:42 ms ,42219427 ns
max:336 ms ,336657507 ns
min:0 ms ,21056 ns
p99:45 ms ,45382276 ns
协议
目前RPC通信协议底层采用两层协议.
第一层采用二进制协议,且字节序统一为大端.
通信格式如下:
[data_len | op | data]
字段解释:
data_len : uint64_t;
op       : uint32_t;
data     : char[data_len];

当op值为1时表示RPC消息,此为第二层协议!这时第一层协议中的data的内存布局则为:
[meta_size | data_size | meta | data]
字段解释:
meta_size  : uint32_t;
data_size  : uint64_t;
meta       : char[meta_size];
data       : char[data_size];

其中meta为 RpcMata的binary.data为某业务上的Protobuf Request或Response类型对象的binary或JSON.
RpcMata的proto定义如下:
syntax = ""proto3"";

package gayrpc.core;

message RpcMeta {
    enum Type {
        REQUEST = 0;
        RESPONSE = 1;
    };

    enum DataEncodingType {
        BINARY = 0;
        JSON = 1;
    };

    message Request {
        // 请求的服务函数
        uint64  method = 1;
        // 请求方是否期待服务方返回response
        bool    expect_response = 2;
        // 请求方的序号ID
        uint64  sequence_id = 3;
    };

    message Response {
        // 请求方的序号ID
        uint64  sequence_id = 1;
        // 执行是否成功
        bool    failed = 2;
        // (当failed为true)错误码
        int32   error_code = 3;
        // (当failed为true)错误原因
        string  reason = 4;
    };
    
    // Rpc类型(请求、回应)
    Type    type = 1;
    // RpcData的编码方式
    DataEncodingType encoding = 2;
    // 请求元信息
    Request request_info = 3;
    // 回应元信息
    Response response_info = 4;
}
服务描述文件范例
以下面的服务定义为例:
syntax = ""proto3"";

package dodo.test;

message EchoRequest {
    string message = 1;
}

message EchoResponse {
    string message = 1;
}

service EchoServer {
    rpc Echo(EchoRequest) returns(EchoResponse){
        option (gayrpc.core.message_id)= 1 ;//设定消息ID,也就是rpc协议中request_info的method
    };
}
处理请求或Response的实现原理

编写第一层通信协议的编解码
将第一层中的data作为第二层协议数据,反序列化其中的meta作为RpcMeta对象
判断RpcMata中的type

如果为REQUEST则根据request_info中的元信息调用method所对应的服务函数.
此时第二层协议中的data则为服务函数的请求请求对象(比如EchoRequest).
如果为RESPONSE则根据response_info中的元信息调用sequence_id对应的回调函数.
此时第二层协议中的data则为服务方返回的Response(比如EchoResponse)



发送请求的实现原理
以client->echo(echoRequest, responseCallback)为例
参考代码:GayRpcClient.h

客户端分配 sequence_id,以它为key将 responseCallback保存起来.
将echoRequest序列化为binary作为第二层协议中的data
构造RpcMeta对象,将echo函数对应的id号作为request_info的method,并设置sequence_id.
将RpcMeta对象的binary作为第二层协议中的meta
用第二层协议的数据写入第一层协议进行发送给服务端.

发送Response的实现原理
以replyObj->reply(echoResponse)为例
参考代码:GayRpcReply.h

首先replyObj里(拷贝)储存了来自于请求中的RpcMata对象.
将echoResponse序列化为binary作为第二层协议中的data
构造RpcMeta对象,将备份的RpcMeta对象中的sequence_id设置到前者中response_info的sequence_id.
将RpcMeta对象的binary作为第二层协议中的meta
用第二层协议的数据写入第一层协议进行发送给服务端.

编解码参考

对于发送请求或Response都可以走出站拦截器,用于统一的发送消息,最终的序列化参考代码：
UtilsDataHandler.h
对于接收请求或Response在编解码之后都可以交给入站拦截器,解码参考代码:
OpPacket.h

注意点

通信协议的第一层并不是重点,RPC核心在实现上尽量不要依赖它(目前的第一层协议只是某一种范例)
同样,RPC核心并不依赖通信采用的传输协议,可以是TCP也可以是UDP或者WebSocket
RPC服务方的reply顺序与客户端的调用顺序无关,也就是可能后发起的请求先得到返回.
目前RPC系统没有提供超时控制

",36
ElisaMin/Manual-For-LGG7-Chinese-Guys,None,"Another Github Pages
",3
AdobeDocs/adobeio-runtime,None,"Adobe I/O Runtime Developer Guide
This guide will give you an overview of Adobe I/O Runtime, explain how it works, and get you started with developing your own integrations.
Contents
Overview

What is Adobe I/O Runtime
Use Cases for Adobe I/O Runtime
How Adobe I/O Runtime Works
Adobe I/O Runtime Entities

Quickstarts

Setting up Your Environment
Deploying your First Adobe I/O Runtime Function
Debug Your Actions

Guides

Creating Actions: actions, web actions, invoking and managing, setting parameters
Throughput Tuning: how to maximize the number of action invocations
Security Guide: discover potential security issues and how to address them
Securing Web Actions: learn how to control the access to web actions
Creating REST APIs: learn to create REST APIs from web actions
Using Packages: Working with packages
Logging and Monitoring: learn how to troubleshoot your actions
System Settings: see the system settings and constraints
CI/CD Pipeline: understand the tools you have to create a CI/CD Pipeline

Reference Documentation

aio CLI: how to use aio CLI
wsk CLI: how to use wsk CLI
Multiple Regions: where we run your actions
Pre-installed Packages: what packages are pre-installed
Runtimes: details about the available runtimes
API Reference: I/O Management API
Triggers & Rules: working with triggers and rules
Packages: working with packages
Feeds: working with feeds

Tools

aio CLI - this tool helps you manage your namespaces and the authentication for the wsk CLI
wsk CLI - this tool is the main interface for managing your actions/packages/rules/triggers and getting access to activation results/errors/logs
wskdeploy CLI - this tool helps you deploy multiple actions and packages

Resources and Support

FAQ

",4
prestodb/RPresto,R,"RPresto
RPresto is a DBI-based adapter for
the open source distributed SQL query engine Presto
for running interactive analytic queries.
Installation
RPresto is both on CRAN
and github.
For the CRAN version, you can use
install.packages('RPresto')
You can install the github development version via
devtools::install_github('prestodb/RPresto')
Examples
The standard DBI approach works with RPresto:
library('DBI')

con <- dbConnect(
  RPresto::Presto(),
  host='http://localhost',
  port=7777,
  user=Sys.getenv('USER'),
  schema='<schema>',
  catalog='<catalog>',
  source='<source>'
)

res <- dbSendQuery(con, 'SELECT 1')
# dbFetch without arguments only returns the current chunk, so we need to
# loop until the query completes.
while (!dbHasCompleted(res)) {
    chunk <- dbFetch(res)
    print(chunk)
}

res <- dbSendQuery(con, 'SELECT CAST(NULL AS VARCHAR)')
# Due to the unpredictability of chunk sizes with presto, we do not support
# custom number of rows
# testthat::expect_error(dbFetch(res, 5))

# To get all rows using dbFetch, pass in a -1 argument
print(dbFetch(res, -1))

# An alternative is to use dbGetQuery directly

# `source` for iris.sql()
source(system.file('tests', 'testthat', 'utilities.R', package='RPresto'))

iris <- dbGetQuery(con, paste(""SELECT * FROM"", iris.sql()))

dbDisconnect(con)
We also include dplyr integration.
library(dplyr)

db <- src_presto(
  host='http://localhost',
  port=7777,
  user=Sys.getenv('USER'),
  schema='<schema>',
  catalog='<catalog>',
  source='<source>'
)

# Assuming you have a table like iris in the database
iris <- tbl(db, 'iris')

iris %>%
  group_by(species) %>%
  summarise(mean_sepal_length = mean(as(sepal_length, 0.0))) %>%
  arrange(species) %>%
  collect()
How RPresto works
Presto exposes its interface via a REST based API1. We utilize the
httr package to make the API calls and
use jsonlite to reshape the
data into a data.frame. Note that as of now, only read operations are
supported.
RPresto has been tested on Presto 0.100.
License
RPresto is BSD-licensed. We also provide an additional patent grant.
[1] See https://github.com/prestodb/presto/wiki/HTTP-Protocol for a
description of the API.
",96
Izheil/Quantum-Nox-Firefox-Dark-Full-Theme,CSS,"
Previously known as ""Firefox 57+ full dark theme with scrollbars and multirow tabs"", I decided to give it an actual name instead of leaving it as just a description.
This theme is mainly intended for the stable release of Firefox (This means that while it will most probably work with nightly and ESR for the most part, it may have less support for those versions).
You can use it to fully change the colors of most of firefox UI to dark-gray colors (with #222-#444 colors mostly), including scrollbars, tooltips, sidebar, as well as dialogs. With the files here you can also as remove some context menu options, enable multirow tabs, change the font of the url bar...
Of course... you could as well use these files to color your firefox any way you wanted, the only thing you'd have to do is change the correct values (what each class or id does is commented above each) in the .css files (as far as you know some 
basic css or color coding, it shouldn't be too hard) using notepad, or some code editing program (such as notepad++ on Windows).
To change these you will have to use the right hex codes. You can find a color picker to hex code in this page.
If you want to edit a file and you want to use notepad (windows), you may see that all code is a wall of text without any line break (the files get compressed like that when uploaded, so there isn't much to do there), in which case you can always drag & drop the file you want to modify into any internet browser window (like firefox) to see the actual code with line breaks, and then copy & paste it back to the open file with notepad, making it regain the line breaks on notepad again.

This problem doesn't happen if you use a code editor such as notepad++, atom, sublime text...
Last update: 12/05/2019
Files updated:

Userchrome.css: Fixed white flashes on page pre-loading without affecting pages that use about:blank.

Pre-Last update: 06/05/2019
Files updated:

Usercontent.css: Added a missing textarea box on mozilla addons page.
Addons.css: Themed Ant Video Downloader addon, and removed Flash Video Downloader (Mozilla took the original one down from their addons page).


A note on people looking to replace some Tab Mix Plus features
As for using this theme to replace some functions of Tab Mix Plus, I'll keep the functions that can be done through CSS here, and I'll try to point addons that cover some of the missing functions, but as of right now, the files in this repository covers multi-row tabs, keep the close button on tabs always visible, and color-coding tabs when they are loaded, unloaded, etc...
Most other functions of Tab Mix Plus can already be ""simulated"" changing some about:config settings:

To keep FF open even after closing the last tab -> browser.tabs.closeWindowWithLastTab to false.
To open a search result typed on the URL bar on a new tab -> browser.urlbar.openintab to true.
To open a search result typed on the search bar on a new tab -> browser.search.openintab to true.
To open bookmarks on a new tab instead of the current tab -> browser.tabs.loadBookmarksInTabs to true.
To force popups on new tabs instead of windows -> browser.link.open_newwindow.restriction to 0 (should be 2 by default).
Open related tabs (the ones you open) as the last tab in the tab bar -> browser.tabs.insertRelatedAfterCurrent to false.

... or through extensions (not a comprehensive list, only the ones themed here are mentioned), like Tab session manager, Undo closed tabs button.

Installation
Main browser UI


If you are on windows and only want the theme or multirow, you can use the batch file installers inside the installers folder.
If you are using Linux or Mac, or want to add some more functionability (like deleting some useless context menu commands), you will have to use the methods described inside one of the 3 main folders of this repository:
Short review of each folder:

CSS tweaks: Enables removal of context menu items, multirow bookmarks, changing tab bar position (so that it could be under the bookmarks bar for example)
Full dark theme: Gives dark colors to firefox UI, including the scrollbars and the tooltips. Can also change the background image of about:home and the header image used as a persona.
Multirow and other functions: You can find the JS files that add extra functionability to Firefox that couldn't be done with CSS alone.


You can turn the features you want on or off changing the commented lines on userchrome.css (To change them you just have to open it with notepad or any code editor, and encase between ""/*"" and ""*/"" (without the quotation marks) the lines you don't want to take effect). Of course, if you think that you are NEVER going to use certain feature, you can always delete the specific lines you don't want without any other side-effect.
You can find a video tutorial on how to install the theme without installers here.
General sites dark theme
You can apply the global dark userstyle found inside the Global dark userstyle folder to theme general sites with an all-around CSS stylesheet. You need stylus addon to use it.
While it's not perfect (meaning that you should still use per-site styles for the sites you visit often), it can help to darken most sites when browsing around general sites that you don't visit often, and thus don't want/can't find a specific userstyle for them.


It is recommended that you check the Global dark userstyle readme to know how to add site exclusions to the global userstyle.
Addon dark themes
You can apply a dark theme to certain addons changing the UUID's of them inside the addons.css file inside the ""Full dark theme"" folder (more instructions on how to do that inside the addons file).

Here is a list of the themed addons:

Ant Video Downloader
Cookie autodelete
Download Manager (S3)
History autodelete
HTTPS Everywhere
Noscript
Notifier for Gmail (restartless)
Multi-accounts containers
OneTab
Popup Blocker Ultimate
Privacy badger
Tab session manager
Temporary containers
Ublock Origin
Undo closed tabs button
Video Download Helper
Viewhance

You might have noticed that we no longer have Lastpass dark theme inside addons.css anymore. This is because at the time that addon was themed, I didn't know much about it. After some research it seems like Lastpass has had a history of security issues (in 2011, 2015, 2016, and 2017), as well as there being other open source alternatives out there that seem to be more reliable, like Bitwarden (it also has a built-in dark mode) which is available for all major browsers.
The scrollbars
This theme colors scrollbars using usercontent.css to give them a basic re-color.

If you want a different style on the scrollbars, you can try using the scrollbars.as.css file inside the Alternative scrollbars folder, which will make the scrollbars look more rounded and will have some sort of ""puffy"" effect when clicking them.

If instead you just don't want scrollbars to show at all but keep scrollability, you can do this through usercontent.css setting the variable --scrollbars-width to none (should be the first rule on the :root section (almost at the start)), and deleting scrollbars.as.css.
If you aren't using the usercontent provided here for some reason, you can always just add this code to your self-created usercontent.css:*|* {scrollbar-width: none !important}

FAQ:
How do I submit an issue?
As of 26/03/2019 I stopped offering active support for new features or issues. This doesn't mean that I won't be mantaining the project, it just means I won't be taking feature requests nor unrelated issues to the functionability offered by the files inside this repository anymore.
If you find some problem that is directly related with any of the functions offered by any of the files in this repository, you can comment it inside the relevant commit that you think may have affected the function that is giving you trouble. If you can't tell which, comment in the last one. Comments about new functionability or things that aren't related to the actual functionability of the files will be ignored (You can already ask about problems you may have with firefox on r/firefox or r/firefoxCSS subreddits, or on Firefox support pages).
The pre-loading screen of websites is still white, how can I change this?
The fastest way to solve the ""blinking"" white screen is to change the default web background color on Firefox settings > General > ""Colors..."" button > Background, which will make the blinking dissapear and be changed to the color you set up. This, although, can cause some issues on some very few and specific pages like BBC, where they don't set a background color to override the one set here (the number of sites with this problem is very small, most sites override the background color set by this setting).
The synced tabs sidebar isn't themed.
Since it's anonymous content of the browser we can't theme it through userChrome or userContent, which is why you will have to apply the fix inside Sync-tabs-sidebar.as.css. It requires the use of external CSS files loading, which is enabled thorugh userchrome.css and userchrome.xml.
Some context menu commands dissapear after installing userchrome.
If you only want the dark theme, use the default userchrome.css file inside ""Full dark theme"", which won't make the context menu commands dissapear. In case you want to use the features part of the theme, just delete everything after the line that says /* CONTEXT MENU COMMANDS */ (you can find it using the search option on notepad, or the code editor you are using).
In case you still want to delete some commands but not all, just comment out the ones that you want to appear, and leave as active the ones that you want to dissapear.
For example, this is active, so the command is hidden:
/* Send image... */
#context-sendimage,
...But this is commented out, so the command will show on the context menu:
/* Send image... *//*
#context-sendimage,*/
You will see that the ones that I have commented out by default only have the starting ""/*"" of the comment after the description of what they are, since the closing ""*/"" would come from the next description comment below them.
The bookmarks toolbar text/tabs text color is black and I can't see the letters over the dark background.
This is caused by your persona (lightweight theme), and while you could change these settings inside userchrome, I thought it was better to just change the settings on the persona directly (since not all personas will look the same). To do so you'd have to open about:config, and search for lightweightThemes.usedThemes. Once there, find the ""textcolor"" setting and type any color you'd want to use instead of black or the color being used by the theme (use #fff for white). The persona you are currently using should be in the first place in the list.
The bookmarks multirow shows an empty scrollbar when enabled.
If you are using an old version of the scrollbars, or you are just plain not using the scrollbars here, you will have to add some code to delete the empty scrollbars that show on the bookmark toolbars. You have to use this code on a ""*.ac.css"" file (so you would need to have firefox patched with the method explained on the Multirow and other functions folder), since otherwise it won't work:
/* This deletes the scrollbar from bookmarks toolbar when using multirow bookmarks */
#PlacesToolbarItems scrollbar {display: none !important}

I only want to use the multirow/(Any other) feature, but not the other ones!
You only need to modify userChrome.css, deleting the lines that you don't want to apply (Every function has a comment above it saying what each ruleset does), or if you think you may want them later, just encase the feature parts that you don't want to apply between /* and */:
/* This is an example of a comment that wouldn't be read on a .css file */
I'm opening web files locally (as in opening html pages that you have created or downloaded) and the background is not the color it should be.
To change the directory browsing page and change how .css or some .txt files appear when opened with Firefox, I had to specify it to affect urls that start with ""file:///"", meaning that any file opened with Firefox will get overriden with those rules as well. To prevent this, go to userContent.css, and comment out the lines that affect this url (This rule should be exactly under the color variables at the start of the file).
I placed userchrome.css inside my chrome folder and I still don't have multi-row tabs!
While we only needed to use CSS to enable multi-row tabs, this breaks tabs draggability, making reordering tabs when it was enabled a bit erratic, so to fix this, I decided to put all multi-row tabs code inside the MultiRowTabLiteforFx.uc.js file. This means that now Multi-row tabs can be enabled following the method described inside the Multirow tabs folder. If you were using CSS code on your userchrome.css to enable multirow tabs, delete (or comment it out) for the js file to take effect.
Why use this method instead of using Stylus?
The main reason is that you can't style firefox about: pages nor dialog windows with just stylus.
The theme is making the text of some addon popups unreadable, how do I fix this?
The theme is made so that it changes most background colors, including the one of the popups that don't have any background color specified by their original creator. Sadly it doesn't change the text of these by default, so you may have to do it manually, or report the addon you want themed here, or just use the fix inside userchrome.css (at around lines 926-929) to turn the addons back to their white background color.
Credits
The original code for the custom scrollbars which we modified here belongs to Arty2, and you can find it here.
The original code for the multirow tabs (the CSS part) was written by Andreicristianpetcu, and you can find it here, or for just the code, here. The fix of multi-row tabs draggability was made by TroudhuK, and you can find the original one here.
The original code for the multirow bookmarks toolbar belongs to the original creator mentioned in this reddit thread, whose code was fixed by jscher2000 to use in our current firefox version.
The fix to be able to theme unread tabs again after FF61 (see bug 1453957 on bugzilla) as mentioned in this reddit thread, was made by moko1960 to use in Firefox 61+.
The code to be able to edit anonymous content (in our case the scrollbars and tooltips) was created thanks to the efforts of Zeniko, Nichu, and Sporif.
Special thanks to Messna for noting the class turning into an ID on FF58, and Snwflake for fixing Firefox root folder location on MacOS.
Also thanks to BelladonnavGF, Demir-delic, DallasBelt, Gitut2007, Hakerdefo, Tkhquang and YiannisNi for noting some issues with the theme, and the requests for new features that extended this project.
Donations
If you want to support this project, consider buying me a coffee to motivate me keep this repository up and running.
​

",135
MicrosoftDocs/dynamics-365-customer-engagement,C#,"Legal Notices
Microsoft and any contributors grant you a license to the Microsoft documentation and other content
in this repository under the Creative Commons Attribution 4.0 International Public License,
see the LICENSE file, and grant you a license to any code in the repository under the MIT License, see the
LICENSE-CODE file.
Microsoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation
may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.
The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.
Microsoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.
Privacy information can be found at https://privacy.microsoft.com/en-us/
Microsoft and any contributors reserve all others rights, whether under their respective copyrights, patents,
or trademarks, whether by implication, estoppel or otherwise.
",86
gowerrobert/StochOpt.jl,Julia,"warning package under development: it will break. But renewed shall be the code that was broken, the crashless again shall compile.
Download the datasets
bash download_datasets
If it crashes, data can be downloaded manually here. Then, datasets .jld files should be placed in the directory ./data.
Dependencies
Launch the  Julia REPL and press ] to enter the package REPL mode, then install the required packages with
(v1.0) pkg> add JLD
(v1.0) pkg> add Plots
(v1.0) pkg> add StatsBase
(v1.0) pkg> add Match
(v1.0) pkg> add Combinatorics
(v1.0) pkg> add Formatting
(v1.0) pkg> add LaTeXStrings
(v1.0) pkg> add PyPlot
(v1.0) pkg> add Distributions
...
(v1.0) pkg> add Distributed
if any problem with PyPlot, try the following manipulation
$ julia
julia> ENV[""PYTHON""]=""""
julia> ]
(v1.0) pkg> build PyCall
StochOpt
A suite of stochastic optimization methods for solving the empirical risk minimization problem. 
Demo
For a simple demo of the use of the package
julia ./test/demo.jl
For a demo of the SVRG2 type methods from [1]
julia ./test/test/demo_SVRG2.jl
For a demo of the BFGS and accelerated BFGS methods from [2]
julia ./test/demo_BFGS.jl
For a demo of SAGA with optimized probabilities from [4]
julia ./test/demo_SAGA.jl
For a demo of SAGA nice from [5]
julia ./test/demo_SAGA_nice.jl
Repeating paper results
To re-generate all of the experiments from [1]
julia ./repeat_paper_experiments/repeat_SVRG2_paper_experiments.jl
To re-generate all of the experiments from [2]
julia ./repeat_paper_experiments/repeat_BFGS_accel_paper_results.jl
To re-generate the experiments from Section 6.1 of [4]
julia ./repeat_paper_experiments/compare_SAGA_importance_opt_Lis.jl
To re-generate the experiments from Section 6.1 of [4]
julia ./repeat_paper_experiments/test_optimal_minibatch_SAGA_nice.jl
To re-generate the experiments from Section 5.1 & 5.2 of [5] (~1h 30min)
julia ./repeat_paper_experiments/repeat_optimal_minibatch_step_sizes_SAGA_paper_experiment_1_and_2.jl all_problems
setting all_problems to false to run the code only on the first two problem, unscaled uniform synthetic dataset with $\lambda =10^{-1}$), (~7min) or to true to run it on all of them (~1h 22min).
Or using the parallel implementation
julia -p <number_of_processors_to_add> ./repeat_paper_experiments/repeat_optimal_minibatch_step_sizes_SAGA_paper_experiment_1_and_2_parallel.jl all_problems
setting all_problems to false to run the code only on the first problem, unscaled uniform synthetic dataset with $\lambda =10^{-1}$, (~XXXXXmin) or to true to run it on all of them (~XXXXXXXXh XXXXXmin).
To re-generate experiments from Section 5.3 of [5]
julia ./repeat_paper_experiments/repeat_optimal_minibatch_step_sizes_SAGA_paper_experiment_3.jl all_problems
setting all_problems to false to run the code only on the first problem, scaled ijcnn1_full with $\lambda =10^{-1}$, (~1min) or to true to run it on all of them (~2h 09min).
Or using the parallel implementation
julia -p <number_of_processors_to_add> ./repeat_paper_experiments/repeat_optimal_minibatch_step_sizes_SAGA_paper_experiment_3_parallel.jl all_problems
setting all_problems to false to run the code only on the first problem, scaled ijcnn1_full with $\lambda =10^{-1}$, (~1min) or to true to run it on all of them (~1h 30min).
To re-generate the experiments from Section 5.4 of [5]
julia ./repeat_paper_experiments/repeat_optimal_minibatch_step_sizes_SAGA_paper_experiment_4.jl all_problems
setting all_problems to false to run the code only on the first problem, scaled ijcnn1_full with $\lambda =10^{-1}$, (~2min) or to true to run it on all of them (~XXh XXmin).
Or using the parallel implementation
julia -p <number_of_processors_to_add> ./repeat_paper_experiments/repeat_optimal_minibatch_step_sizes_SAGA_paper_experiment_4_parallel.jl all_problems
setting all_problems to false to run the code only on the first problem, scaled ijcnn1_full with $\lambda =10^{-1}$, (~2min) or to true to run it on all of them (~XXh XXmin).
Methods implemented
SVRG, the original SVRG algorithm. 
SVRG2, which tracks the gradients using the full Hessian. 
2D, which tracks the gradients using the diagonal of the Hessian. 
2Dsec, which tracks the gradients using the robust secant equation. 
SVRG2emb, which tracks the gradients using a low-rank approximation of the Hessians. 
CM, which tracks the gradients using the low-rank curvature matching approximation of the Hessian. 
AM, which uses the low-rank action matching approximation of the Hessian. 
BFGS, the standard, full memory BFGS method. 
BFGS_accel, an accelerated BFGS method. 
SAGA, stochastic average gradient descent, with several options of samplings (including optimal probabilities). 
SAGA nice, mini-batch version of SAGA with nice sampling. 
More details on the methods can be found in [1], [2], [4] and [5] 
Code Philosophy
To provide en enviroment where competing stochastic methods can be compared on equal footing. This is why all methods are called by the same wrapper function ""minimizeFunc"" (or it's extension minimizeFunc_grid_stepsize). All performance measures such as time taken, test error or epochs are calculated by these wrapper functions. Each new method need only supply a stepmethod and a bootmethod. The stepmethod returns an update vector d which is then added to x_k to give the next iterate x_{k+1}. The bootmethod is called once to initialize the method.
Adding more data
To test a new data set, download the raw data of a binary classifiction fomr LIBSVM [3] and place it in the folder ./data.
Then replace ""liver-disorders"" in the code src/load_new_LIBSVM_data.jl and execute. In other words, run the code
include(""dataLoad.jl"")
initDetails()

datasets = [""liver-disorders""]
for  dataset in datasets
transformDataJLD(dataset)
X,y = loadDataset(dataset)
showDetails(dataset)
end
where ""liver-disorders"" has been replaced with the name of the new raw data file.
Adding new loss functions
to include new objective function, see load_logistic.jl and copy the same structure
Adding new methods
to include a new method X, you need to write a descent_X.jl and boot_X.jl function. See descent_grad and boot_grad for an example. I also recommend writing your type and including it in StochOpt or using one of the types there defined already.
References
[1]  Tracking the gradients using the Hessian: A new look at variance reducing stochastic methods 
RMG, Nicolas Le Roux and Francis Bach.
To appear in AISTATS 2018
[2] Accelerated stochastic matrix inversion: general theory and speeding up BFGS rules for faster second-order optimization 
RMG, Filip Hanzely, P. Richtárik and S. Stich.
arXiv:1801.05490, 2018
[3]  LIBSVM : a library for support vector machines. 
Chih-Chung Chang and Chih-Jen Lin, ACM Transactions on Intelligent Systems and Technology, 2:27:1--27:27, 2011. 
Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm
[4] Stochastic Quasi-Gradient Methods:
Variance Reduction via Jacobian Sketching 
RMG, Peter Richtárik, Francis Bach
[5] Optimal mini-batch and step sizes for SAGA 
Nidham Gazagnadou, Robert M. Gower and Joseph Salmon.
arXiv:1902.00071, 2019
For up-to-date references see https://perso.telecom-paristech.fr/rgower/publications.html
TODO

change output type to have a testerrors field. Think where best to load a test problem. Probably outside of minimizeFunc. Where best to place code for test_error ?  Probably best to start a new src file for error calculations? or testing related things?
Implement the calculation of the Jacobian.
The code for SVRG2 type methods (AMprev, AMgauss, CMprev, CMgauss) should have their own type. Right now they are definied using the generic Method type, which is why the code for these functions is illegible.

",10
bcgov/clus,R,"Caribou and Land Use Simulator
Usage
The code here is currently in development and likely will not function. Contact tyler.muhly@gov.bc.ca for questions.
Requirements
N/A
Getting Help or Reporting an Issue
Use the Issues tab to get help or report any issues.
Descriptions:
License
Copyright 2018-2019 Province of British Columbia
Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
",5
HumanCellAtlas/data-portal,JavaScript,"HCA Data Portal
Developer Workspace
Requirements


Node.js (https://nodejs.org/en/), version 10.0.0.


We recommend using n (https://github.com/tj/n) as the Node.js package manger.


npm (https://www.npmjs.com/) is bundled with Node.js and is required to manage application dependencies.


Setup
Clone Repo
Clone the data-portal repo:
git@github.com:HumanCellAtlas/data-portal.git

Install Gatsby Command Line Tool
The Gatsby command line tool is used to develop, build and serve (locally) the Data Portal.
npm install --global gatsby-cli

Install Packages
Run the following command from the project's root directory to install the required packages:
npm install

Development Server
Run the following command from the root directory:
gatsby develop
The development server can be viewed at:
localhost:8000
Deployment
Run the following command to build the application:
gatsby build
Local Production Version
Run the following command to view a built version of the application, locally:
gatsby serve
The built version can be viewed at:
localhost:9000
Application Dependencies
Material Components
HCA Data Preview uses Material Components (https://material.io/components/web/) for its Material Design library.
",3
ashafer01/laurelin-server,Python,"laurelin-server
This is in very early stages of development as you can see.
This will be a simple, lightweight LDAP-speaking frontend server which will support a variety of data backends.
Currently the following are planned:

Ephemeral/in-memory (no backend)
In-memory with persistence in JSON, LDIF, etc., supporting local filesystem storage at minimum, and eventually cloud
storage such as S3. With JSON, MongoDB and perhaps other NoSQLs will probably come cheap, too.
SQL, utilizing a good ORM/abstraction layer to allow for maximum DBMS compatibility.
Zookeeper - conceptually almost identical to LDAP already.

Overarching goals will be maximum backend compatibility in all senses of the phrase, and thus making it as easy as
possible to add support for more backends.
",2
kubernetes/kubernetes,Go,"Kubernetes
 


Kubernetes is an open source system for managing containerized applications
across multiple hosts; providing basic mechanisms for deployment, maintenance,
and scaling of applications.
Kubernetes builds upon a decade and a half of experience at Google running
production workloads at scale using a system called Borg,
combined with best-of-breed ideas and practices from the community.
Kubernetes is hosted by the Cloud Native Computing Foundation (CNCF).
If you are a company that wants to help shape the evolution of
technologies that are container-packaged, dynamically-scheduled
and microservices-oriented, consider joining the CNCF.
For details about who's involved and how Kubernetes plays a role,
read the CNCF announcement.

To start using Kubernetes
See our documentation on kubernetes.io.
Try our interactive tutorial.
Take a free course on Scalable Microservices with Kubernetes.
To start developing Kubernetes
The community repository hosts all information about
building Kubernetes from source, how to contribute code
and documentation, who to contact about what, etc.
If you want to build Kubernetes right away there are two options:
You have a working Go environment.
go get -d k8s.io/kubernetes
cd $GOPATH/src/k8s.io/kubernetes
make

You have a working Docker environment.
git clone https://github.com/kubernetes/kubernetes
cd kubernetes
make quick-release

For the full story, head over to the developer's documentation.
Support
If you need support, start with the troubleshooting guide,
and work your way through the process that we've outlined.
That said, if you have questions, reach out to us
one way or another.

",52691
microsoft/vscode-sublime-keybindings,TypeScript,"Sublime Importer for VS Code
This extension imports keybindings and settings from Sublime Text to VS Code.
Getting Started
Keymappings
This extension ports the most popular Sublime Text keyboard shortcuts to Visual Studio Code.
Just restart VS Code after the installation of this extension and your favorite Sublime Text keyboard shortcuts will be available in VS Code.
Importing settings
The first time the extension is launched a prompt is shown that let's you import your Sublime Settings.
If you want to import your settings at a later time use the Sublime Text Keymap: Import Sublime Text Settings command from the Command Palette (F1).

FAQ
What keyboard shortcuts are included?
The included keyboard shortcuts can be looked up in the contribution list.

Why don't all Sublime Text commands work?
VS Code has not implemented all features. Head on over to this GitHub issue and let the VS Code team know what you'd like to see.
You can install an extension for many of these features:

Expand Selection To Scope
FontSize Shortcuts
change case
expand-region
transpose
Close HTML/XML tag

Contributing
How do I contribute a keyboard shortcut?
We may have missed a keyboard shortcut. If we did please help us out! It is very easy to make a PR.

Head over to our GitHub repository.
Open the package.json file.
Add a JSON object to contributes.keybindings as seen below.
Open a pull request.

{
    ""mac"": ""<keyboard shortcut for mac>"",
    ""linux"": ""<keyboard shortcut for linux"",
    ""win"": ""<keyboard shortcut for windows"",
    ""key"": ""<default keyboard shortcut>"",
    ""command"": ""<name of the command in VS Code""
}
How do I contribute a Sublime setting?
There are two different types of settings files: The mappings file holds the information on how a sublime setting can be mapped to a VS Code setting. The defaults file contains default Sublime settings that are not explicitly set in the Sublime settings file (e.g. the Monokai theme).
To make a Pull Request:

Head over to our GitHub repository.
Open the settings/mappings.json or the settings/defaults.json file.
Add your setting
Open a pull request.

License
MIT
",149
department-of-veterans-affairs/vets.gov-status,HTML,"Vets.gov Executive Scorecard

A scorecard for vets.gov projects

The executive scorecard is meant to provide a simple overview of the vets.gov project to external audiences. It is often used in briefings but should be able to stand-alone with enough context offered for a visitor unfamiliar with vets.gov to navigate it and understand it.
Structure
Landing page
The executive scorecard has a main landing page index.html that provides the vets.gov vision statement as context for the project.
The six most important data points on the impact of vets.gov are presented in a set of ""tiles."" These are rotated and may be customized for ahead of briefings to key stakeholders.
A set of summary metrics over time are conveyed in series of charts in a tab group. The metrics are presented as weekly data to smooth out some of the day-to-day variance and provide a long-term trend view. The tabs are used to conserve visual space. The data for the charts is pulled in at build time from csv files in the _data directory. Those csv files are updated using Python scripts in scripts directory.
Each significant feature or function of vets.gov gets its own ""tile"" in the project section. They are grouped by which of the parts of vision statement they fulfill. Completed features are links to detailed scorecard boards. ""Coming soon"" features can also be displayed. This section is constructed at build time from the contents of the _board directory.
There are special call-out sections for the human-centered design work and progress of migrations.
Detailed Project Boards
Each feature or function gets its own board in _board directory. It has a project overview section, followed by up to three data 'tiles' of key facts, a set of charts, and before/after screenshots.
The boards are generated from the yaml front matter in each Markdown file. The actual content in the Markdown is not used and should be omitted.
Feature/Function Board Fields

title: The name of the feature/function [Required]
date_added: When the feature/function launched [Optional]
vetsdotgov_url: URL on production site [Optional]
status: normal for launched items, progress for 'coming soon' items [Required]
category: Which of the vision statement categories it belongs in: Discover, Apply, Track, Manage [Required]
description: One sentence description, will display on the landing page [Required]
extended_description: Will replace description on detailed page if supplied [Optional]
-screenshot: The file base name for screenshots. If basename is entered here, there should be assets\img\basename.png for the after and assets\img\basename_old.png provided. placeholder can be used if no screenshots are ready. [Required]
-tiles: A list of up to three data tiles to display. Some examples are in showcase.html or check the actual html for each in _includes\tiles. [Required]
-clicks: Will display an ""Outbound links"" chart for tracking traffic sent to other sites
-charts: The file base name for the data charts in _data and should match what is used in scripts\config.json

Chart data
The charts are powered by the vets.gov Google Analytics account. The Python scripts in scripts pull data from the Google Analytics account and create a set of updated CSV files in _data that are then used by Jekyll to build the actual charts.
Once deployed the data is static until the next deploy. Because the executive scorecard is meant for external audiences, this ensures that the data is available and can be quality controlled prior to putting it in front of an audience. Once deployed, we do not have to worry about data abnormalities or failures appearing.
Getting started

Install Ruby if needed (MacOS has by default)
Install Jekyll
Clone this repository to your local computer
Run bundle to install gems
Serve the project locally

bundle exec jekyll serve

Deploying
Our deployments are handled by Jenkins using the Jenkinsfile. We deploy by committing to the production branch. We use the demo branch to deploy to our development server to internally demo new boards or tile updates without blocking the data update path from master to production.
Previous repo
This repo previously held a now defunct dashboard. The prior work is archived as a release on this repo in case that work needs to be revisited.
",6
oracc/oracc,C,"oracc sources
Sources for Oracc.
Setup
Please refer this setup guide for setting it locally and this setup guide for setting it up on a server.
",8
facebook/hhvm,C++,"HHVM
HHVM page |
HHVM documentation |
Hacklang page |
General group |
Dev group |
Twitter
HHVM is an open-source virtual machine designed for executing programs written in Hack. HHVM uses a just-in-time (JIT) compilation approach to achieve superior performance while maintaining the development flexibility that PHP provides.
HHVM is intended for Hack projects, and also supports a large subset of PHP 7 that is required by common tools and libraries. We no longer recommend using HHVM for purely PHP projects.
HHVM should be used together with a webserver like the built in, easy to deploy Proxygen, or a FastCGI-based webserver on top of nginx or Apache.
Installing
If you're new, try our getting started guide.
You can install a prebuilt package or compile from source.
Running
You can run standalone programs just by passing them to hhvm: hhvm example.hack.
If you want to host a website:

Install your favorite webserver. Proxygen is built in to HHVM, fast and easy to deploy.
Install our package
Start your webserver
Run sudo /etc/init.d/hhvm start
Visit your site at http://.../main.hack

Our getting started guide provides a slightly more detailed introduction as well as links to more information.
Contributing
We'd love to have your help in making HHVM better. If you're interested, please read our guide to contributing.
License
HHVM is licensed under the PHP and Zend licenses except as otherwise noted.
The Hack typechecker is licensed under the MIT License except as otherwise noted.
Reporting Crashes
See Reporting Crashes for helpful tips on how to report crashes in an actionable manner.
Reporting and Fixing Security Issues
Please do not open GitHub issues or pull requests - this makes the problem
immediately visible to everyone, including malicious actors. Security issues in
HHVM can be safely reported via HHVM's Whitehat Bug Bounty program:
https://www.facebook.com/whitehat
Facebook's security team will triage your report and determine whether or not
is it eligible for a bounty under our program.
FAQ
Our user FAQ has answers to many common questions about HHVM, from general questions to questions geared towards those that want to use.
There is also a FAQ for contributors to HHVM.
",16009
funeralchris/csgo-list-of-cvars,None,"csgo-list-of-cvars
Complete list
You can find the complete list here: https://github.com/funeralchris/csgo-list-of-cvars/blob/master/cvars_all.log
Recent changes
Updates recently changed:



Update
Description
Changes




16.9.2018
Initial release
Link


3.10.2018
""panorama_dump_events_backlog 0"" and ""cash_team_winner_bonus_consecutive_rounds"" added, ""snd_surround_speakers"" no longer devonly, ""windows_speaker_config"" removed, ""mp_solid_teammates"" changed (2 added as value)
Link


4.10.2018
""sv_load_random_client_names_file"" added
Link


9.10.2018
""snd_hrtf_voice_delay"", ""sv_talk_after_dying_time"", ""voice_positional"", ""voice_positional_seconds_after_death"" added
Link


23.10.2018
""cl_scoreboard_survivors_always_on"" added, ""sv_holiday_mode"" on 1 now (was 0), ""sv_maxunlag"" no longer devonly and 0.200 now (was 1.0)
Link


30.10.2018
""dsp_room"" is now a float
Link


31.10.2018
""dsp_room"" is no longer a float / revert of the last update
Link


6.11.2018
""dsp_room"" is a float again, ""cl_clock_24hour"" was removed
Link


15.11.2018
""dsp_room"" is an integer again, ""mp_disconnect_kills_bots"" and ""mp_disconnect_kills_players"" added
Link


16.11.2018
""dsp_room"" is a float again
Link


29.11.2018
""dsp_room"" is an int again
Link


6.12.2018
potential fix for mouse escaping fullscreen application with ""m_limitedcapture_workaround"", survival-related changes
Link


7.12.2018
""mat_resolveFullFrameDepth"" now ""devonly"" instead of ""cheat"", value 0 removed
Link


8.12.2018
""snd_dzmusic_volume"" added
Link


8.12.2018 #2
various ""(sv_)dz"" related cvars added
Link


18.12.2018
""@panorama_canvas_fxaa"" and ""@panorama_canvas_ssaa"" added including options, ""sv_snowball_strength"" and ""ammo_grenade_limit_snowballs"" added, ""sv_holiday_mode 2""
Link


11.1.2019
""sv_holiday_mode 0""
Link


15.1.2019
""sv_ledge_mantle_helper_dzonly"" added
Link


25.1.2019
""r_flushlod"" is now cheat-protected
Link


7.2.2019
""tv_broadcast_keyframe_interval1"" and ""tv_broadcast_max_requests1"" added
Link


25.2.2019
""blackbox_dump"" now devonly, ""sv_dz_spec_group_teams"" and ""ui_playsettings_survival_solo"" added
Link


6.3.2019
""cl_hide_avatar_images"", ""cl_sanitize_player_names"", ""cl_show_enemy_avatar_colors"", ""mat_using_d3d9ex"" and ""spec_dz_group_teams"" added
Link


7.3.2019 - 23.3.2019
""fs_allow_unsafe_writes"" and ""mp_consecutive_loss_aversion"" added
Link


23.4.2019
""mat_using_d3d9ex 1"" (was 0)
Link


1.5.2019
""quickinv"", ""ammo_grenade_limit_bumpmine"", ""cl_player_ping_mute"", ""cl_quickinventory_line_update_speed"", ""cl_radial_radio_tab"", ""cl_radialmenu_deadzone_size"", ""exojump"", ""mp_shield_speed_deployed"", ""mp_shield_speed_holstered"", ""player_ping"", ""player_ping_throttle_decay"", ""r_drawplayers"", ""r_drawshieldstencil"", ""r_drawshieldstencil_debug"", ""r_drawunderwatercap"", ""snd_mute_mvp_music_live_players"", ""sv_bumpmine_arm_delay"", ""sv_bumpmine_detonate_delay"", ""sv_dz_enable_respawn"", ""sv_dz_exploration_payment_amount_bonus"", ""viewmodel_offset_randomize""  and other various DZ-related cvars added, ""cl_quickinventory_deadzone_size"" removed and ""sv_showbullethits"" modified
Link


7.5.2019
""sv_shield_explosive_damage_..."" added
Link


14.5.2019
""lobby_default_privacy_bits1"" removed, ""lobby_default_privacy_bits2"", ""ui_nearbylobbies_filter2"", ""ui_new_events_alert_seen"", ""sv_dz_respawn_additional_wait_time_solo"", ""ui_setting_advertiseforhire_auto"", ""ui_setting_advertiseforhire_auto_last"", added
Link



",24
sitkatech/neptune,C#,"neptune
Neptune is the internal project name for the Orange County Stormwater Tools. The Orange County Stormwater Tools application is built by Sitka Technology Group (https://sitkatech.com) in partnership with Geosyntech Consultants (https://geosyntec.com) for Orange County Public Works. The Orange County Stormwater Tools application is derived from the Tahoe Regional Planning Agency's Lake Tahoe Info Stormwater Tools (https://stormwater.laketahoeinfo.org). This program is free software; you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation
Copyright (C) Tahoe Regional Planning Agency and Sitka Technology Group
",3
uZer/.minimics,Shell,".minimics

Use at your own risk
The bundle is not complete

Minimalistic dotfiles for linux environment.
Easy to use, trying to avoid overkill plugins I'll never find the usage more than
one time in a year.
How to use this and not to use this
I recommend you not to use this repository, but to build your own fork with your
settings. This repository is not maintained for stability but only to fulfill my
needs at a specific moment.
If you just plan to git clone and say yolo as many of my friends did over these
last years, please make sure to run setup.sh or replace manually my name in
.gitconfig file...
Content
This bundle contains:

Git dotfile model (you should customize it with setup script or edit the file)
Vim configuration with vundle, neocomplete and syntastic (disabled by default)
ZSH configuration files with oh-my-zsh autoinstall
Custom aliases
Solarized dircolors submodule (git submodule init / git submodule update)
i3 configuration files and other X and Archlinux related configuration files
Cygwin configuration files with solarized colorscheme
Other experimental stuff I use on my computers and servers

Non-solarized terminal are supported with some minor changes in colorschemes.
Non-powerline fonts are supported, but by default powerline chars are used in vim
and zsh. You can ajust this according to your needs.
This bundle uses:

Sexy visual bar for vim vim-airline
Bundle management solution for vim vundle
gitconfig from scyn-conf

This bundle can include:

solarize dircolors for solarized terminals (facultative)
powerline fonts (facultative)

Installation
.minimics is very easy to install. You just need to have git installed on you
system. There is an autoinstall script in your .minimics folder. To install,
just perform :
git clone https://github.com/uZer/.minimics.git ~/.minimics
~/.minimics/setup.sh

A backup of your old configuration files is automatically made in your minimics
folder.
If you plan to use minimics for your desktop/i3 environment, you will have extra
packages to compile/install.
Usage and configuration
Vim
There are plenty of usefull keyboard mappings in the vimrc configuration file.
My vim configuration will remove end-of-line trailing whitespaces each time you
save a document. If you want to disable this feature, edit your .vimrc and
comment the line containing:
autocmd BufWrite * :call DeleteTrailingWS()

Leader command is <space> by default.
To update vundle plugins, you can just run :PluginUpdate
Useful shorcuts and various caveats:
The <leader> key is super key on your keyboard (also known as windows key).
To paste a block of text, use paste mode: <leader>pp
To enable syntax check, please type <leader>sc out of edit mode.
Tab management:    <leader>tn (tab new) and <leader>tc (tab close)
Window management: <leader>wn (win new) and <leader>wc (win close)
Move from tab to tab / window to window with <leader>j or k, h, l.
Yes, this means if your using vim in a Windows environment (Cygwin...) you may
lock yourself out trying to move to the window on the left.
Bash
I don't.
ZSH
Using oh-my-zsh bundle and custom aliases.
Also using powerline zsh configuration file, if you don't have any powerline
installed please comment the appropriated line in your .zshrc
Git config
See the .gitconfig file in your home directory for usefull git shocuts.
Please make sure you changed the name of your user in your gitrc file
Colors
If you use solarized terminal configurations, you can enable solarized colors in
the setup script (comment or uncomment the variables). At any time if your
unhappy with some color configuration, you can edit the configuration.
Uninstall
If for any reason you want to uninstall .minimics partially or completely, you
just have to unlink the unwanted configuration files in your home directory.
unlink ~/.bashrc

",2
openfl/openfl,Haxe,"     


Purpose
Interactive application and game developers need access to productive tools for forging bitmap, vector, text, sound and video together. The modern-day web browser provides many of these features, but performance for animated content, and support for hardware graphics (while still supporting software caching and fallback) is not readily available. OpenFL combines a proven set of tools for development of games and rich interactive content, going back to the first renaissance innovators on the web.
Two Versions
There are two versions of OpenFL, the first is primarily distributed using haxelib, and blends native support for Windows, macOS, Linux, iOS, Android, Flash, HTML5 and WebAssembly. You can read more about the haxelib distributed version of OpenFL, here.
The second edition of OpenFL is distributed using NPM, and is designed for use from TypeScript, JavaScript (EcmaScript 5 or 6+) or Haxe, the latter of which can be used in both versions of OpenFL. The NPM version of OpenFL is designed to be used in a browser environment. The NPM version also has (beta) support for ActionScript 3.0.
Getting Started
The simplest way to get started is to use Yeoman to create a new project:
npm install -g yo generator-openfl
mkdir NewProject
cd NewProject
yo openfl
You will have the opportunity to choose TypeScript, Haxe, ES6 or ES5 as the source language for your new project.
The template project will include configuration files for Webpack, as well as a source code entry point where you can begin writing a new project. In order to begin using OpenFL, you can try adding support for loading and displaying an image (continued below).
Features
The DOM (Document Object Model) is a convenient method of nesting and arranging visual content, but it is known to be slow. Use of the DOM is discouraged for animated content, unless steps are taken to limit the number of reflows. Normally to improve performance, a developer is forced to use either canvas 2D or WebGL, creating a new problem with writing new rendering code, and losing what made the DOM easy to work with.
OpenFL provides a standard object model, along with additional features useful for animation and interactive development.
Rendering

WebGL 1 and 2
Canvas 2D
CSS 2D transforms (DOM)

Object Model

Matrix transforms
Color transforms
Hit testing
Event propagation
Bitmap caching
Filters (limited)
Masking and scroll rectangles

Vector Graphics

Solid, bitmap and gradient fills
Quadratic and cubic bézier curves
Ellipses, circles and paths
Rectangles and rounded rectangles
Lines with cap, joint and miter styles

Bitmap Data

Seamless support for image, canvas and typed array pixel stores
Transparency and premultiplied alpha
Get, set and copy pixels
Fill and flood fill
Color bounds calculation
Threshold operations
Render-to-texture
Output PNG and JPEG bytes
Channel blending between images
Noise and perlin noise (limited)
Palette swapping
Difference images
Scrolling

Text Support

Font, color and alignment
Selectable text input
Auto-size and alignment
Background and border
Plain or simple HTML text
Multi-line, restrict or password
Character metrics
Selection
Text replacement

Sound Support

Sound playback
Global sound mixing
Time, loops, sound transforms

Geometry Types

2D (3x3) matrix
3D (4x4) matrix
Orientation and perspective
Points and vectors
Rectangle

Networking

Save data to disk
Local storage
Web sockets
HTTP requests

Input

Mouse and touch
Keyboard
Gamepad

Other Features

Batched tile rendering
Video rendering
Asset management
MovieClip animations

Displaying a Bitmap
Create a new project using yo openfl
mkdir DisplayingABitmap
cd DisplayingABitmap
yo openfl
Next, download openfl.png and save it your new ""dist"" directory.
Next, use Visual Studio Code or another code editor to open ""src/app.ts"", ""src/app.js"" or ""src/App.hx"", depending upon the language type you used when you created the project. We will need to add a couple more imports, and a little code to load and display an image.
TypeScript
At the top of the file, add new imports:
import Bitmap from ""openfl/display/Bitmap"";
import BitmapData from ""openfl/display/BitmapData"";
Then extend the constructor method so it looks like this:
constructor () {
	
	super ();
	
	BitmapData.loadFromFile (""openfl.png"").onComplete ((bitmapData) => {
		
		var bitmap = new Bitmap (bitmapData);
		this.addChild (bitmap);
		
	});
	
}
Haxe
At the top of the file, add new imports:
import openfl.display.Bitmap;
import openfl.display.BitmapData;
Then extend the new method so it looks like this:
public function new () {
	
	super ();
	
	BitmapData.loadFromFile (""openfl.png"").onComplete (function (bitmapData) {
		
		var bitmap = new Bitmap (bitmapData);
		addChild (bitmap);
		
	});
	
}
ES6 JavaScript
At the top of the file, add new imports:
import Bitmap from ""openfl/display/Bitmap"";
import BitmapData from ""openfl/display/BitmapData"";
Then extend the constructor method so it looks like this:
constructor () {
	
	super ();
	
	BitmapData.loadFromFile (""openfl.png"").onComplete ((bitmapData) => {
		
		var bitmap = new Bitmap (bitmapData);
		this.addChild (bitmap);
		
	});
	
}
ES5 JavaScript
At the top of the file, add new require statements:
var Bitmap = require (""openfl/display/Bitmap"").default;
var BitmapData = require (""openfl/display/BitmapData"").default;
Then extend the App constructor so it looks like this:
var App = function () {
	
	Sprite.call (this);
	
	BitmapData.loadFromFile (""openfl.png"").onComplete (function (bitmapData) {
		
		var bitmap = new Bitmap (bitmapData);
		this.addChild (bitmap);
		
	}.bind (this));
	
}
Running the Project
You can start a development server by going to the root directory of your project, and running npm start. In addition to compiling your application, it will open a new window in your web browser, with hot reloading enabled. This means that if you edit the app.ts, app.js or App.hx source file, the server will automatically compile your changes, and reload the current window, speeding up development. Now we can making more changes.
Adding Changes
You can continue make changes to your app.ts, app.js or App.hx file, to manipulate your bitmap after it is loaded.
For example:
bitmap.x = 10;
bitmap.y = 200;
bitmap.rotation = 45;
bitmap.alpha = 0.5;
Other Samples
There are more sample projects with additional features (such as sound, animation and video) in each of the OpenFL samples repositories:

https://github.com/openfl/openfl-samples-ts
https://github.com/openfl/openfl-samples-haxe
https://github.com/openfl/openfl-samples-es6
https://github.com/openfl/openfl-samples-es5
https://github.com/openfl/openfl-samples-as3

Each of the samples can be tested using npm install then npm start
Additional Reading
Go to http://www.openfl.org for more information on OpenFL, and visit http://community.openfl.org to ask questions and get help!
License
OpenFL is free, open-source software under the MIT license.
Development Builds
Clone the OpenFL repository:
git clone https://github.com/openfl/openfl
Using OpenFL with NPM
First, install any NPM dependencies:
cd openfl
npm install
Optionally, you may choose to link with a clone of a dependency library (such as lime):
cd path/to/lime
npm link

cd path/to/openfl
npm link lime
Build OpenFL:
npm run build -s
Once built, you may want to npm link to use your version with other projects:
npm link

cd path/to/your-project
npm link openfl
Using OpenFL with Haxelib
First, tell haxelib where your development copy of OpenFL is installed:
haxelib dev openfl openfl

Second, you may want to build the OpenFL tools for processing SWF assets:
openfl rebuild tools

Later, if you decide to return to release builds:
haxelib dev openfl

You may also need a development build of Lime installed.
",1381
dotclear/dotclear,JavaScript,"README
WHAT IS DOTCLEAR ?
Dotclear is an open-source web publishing software.
Take control over your blog!
Dotclear project's purpose is to provide a user-friendly
tool allowing anyone to publish on the web, regardless of
their technical skills.
Features:

Easy publication
Fully customizable theme
User-friendly administration
Flexible template system
Media management
Choose from several editing syntax (wiki, markdown, textile or directly in wysiwyg)
Flexible comment system
Built-in antispam
Localization
Presentation widgets
Themes and plugins
Pages
Tags and categories
Automated installation
Support for several database types
Multiblog
Multi-user with permissions
Standards compliant
Accessible
Importing / exporting
Naturally optimized for search engines
Syndication feeds
Complete trackback support
Full Unicode support
XML/RPC client support
Extensible
Performance and scalability
Twice free

REQUIREMENTS
In order to run Dotclear you need:

A web server (Apache, Cherokee, Nginx, lighttpd, etc.)
PHP 5.6 with the following modules:

mbstring
iconv
simplexml
mysql, mysqli, postgresql or sqlite


A database server (MySQL or PostgreSQL) or SQLite.

INSTALLATION
Automatic installation
The easiest way to install the blog engine is automatic installation.
Download the one minute install file, upload it to your web space. Then open it in your favorite browser. You'll only have to follow the instructions on screen. See the documentation for more information.
Standard installation.
You need to download Dotclear archive, extract it then upload your files to your web space using an FTP client.
Then open your favorite browser and go to http://your.host.name/dotclear/admin/install/. A message alerts you that you haven't got a configuration file and offers to run the wizard. Click this link.
DOCUMENTATION
Still unsure if you want to move? A ""guided tour"" is what you need.
Dotclear is fully documented:

If you have moved in already, the User Manual is there for you.
The managers will turn to the Administration Guide.
Decorators and craftsmen will surely enjoy reading the Developer and designer resources.

Dotclear documentation uses a wiki. Feel free to contribute.
License
Copyright Olivier Meunier & Association Dotclear
GPL-2.0-only (https://www.gnu.org/licenses/gpl-2.0.html)
This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; version 2 of the License.
This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
CONTRIBUTING
Dotclear is an open source project. If you'd like to contribute, please read the CONTRIBUTING file.
You can submit a pull request, or feel free to use any other way you'd prefer.
Repositories:
https://hg.dotclear.org/dotclear/ (official)
https://bitbucket.org/dotclear/dotclear (Bitbucket)
https://github.com/dotclear/dotclear (Github, sync'ed with Bitbucket)
",11
aceiro/atm_2019,C++,"ATM 2019
CI-APP: UM SISTEMA DE COMUNICADO INTERNO PARA A FACULDADE ASSER
Introdução
As Atividades Multidisciplinares do Curso de Graduação em Sistemas de Informação da Escola Superior de Tecnologia e Educação de Rio Claro (ESRC) são partes integrantes do currículo e estão previstas no Projeto Pedagógico, aprovado no âmbito do Colegiado de Curso, sendo esta versão vigente a partir do mês de fevereiro de 2018 (vide respectivo regimento disponível em [1]).
Essa atividade será desenvolvida nos moldes de uma Webquest [2] que originalmente foi proposta por Bernie Dodge, professor estadual da Califórnia (EUA) tendo como foco uma metodoligia baseada no da Internet de forma criativa. A Webquest é uma atividade investigativa onde as informações com as quais os alunos interagem provêm da internet.
portanto, para desenvolver as atividades siga as Sessões seguintes para poder compreender a atividade que será desenvolida.
Em síntese, essa Webquest envolve as seguintes disciplinas do semestre atual de 2019: (i) Cálculo Diferencial e Integral II; (ii) Programação Orientada a Objetos; (iii) Programação de Computadores II; (iv) Economia; (v) Circuitos Lógicos; (vi) Algebra Linear; (vii) Relações Etnicas Raciais.
Esse projeto vista o desenvolvimento de um sistema para controle de comucação interna da Faculdade realizada através do formulário – CI (COMUNICADO INTERNO).
A Figura abaixo ilustra o modelo de CI usado atualmente.

Cronograma
A Tabela a seguir ilustra as datas que devem ser organizas através do Kanban Cultura Ágil. Para isso, sugerimos a criação de atividades no menu Issues



Data
Fase




20/04/2019
Atividade 1


04/05/2019
Atividade 2


18/05/2019
Atividade 3


01/06/2019
Atividade 4


08/06/2019
Entrega



Fontes

WebQuest em PDF
GitHub Issues
GitHub Issues video (inglês)

",2
KarlOfDuty/SCPDiscord,C#,"SCPDiscord
SCPDiscord links features from your SCP:SL server console to Discord channels, such as printing out server events to Discord and using server commands through Discord. It is incredibly customisable as you can read more about below.
Features:


Ability to log any ServerMod event to Discord. All are optional and can be toggled individually.


Complete multi-channel support letting you post different events to different channels. All can be individually re-routed.


Support for MultiAdmin managed servers, each sub-server can post to the same channel, different channels or even different bots and different Discord servers.


You can for instance have one public channel for each of your servers where things like player joins, kills, round starts and round ends are posted. You could then add one channel for each server visible only to moderators, showing things like admin actions and logging who attacks who on each server to check for teamkillers.


Ability to use different commands through Discord, currently kick, ban and unban. To my knowledge this is the only plugin that currently offers banning offline players, so hackers leaving as soon as they see an admin coming on is no longer an issue. It is also the only plugin I'm aware of that lets you unban players.


The ability to completely customise every single message from the plugin and use different languages. More info here.


Player count is displayed in the bot activity field. The bot's status changes from dnd when the scp server is on but the bot server is on, away when there are no players on a server and online when a server has players.


An auto-updating channel topic with information such as number of players, server uptime, ip and gameplay information.


You can sync ranks from discord to the game, letting you automate things like patreon rewards and moderator positions.


Installation and configuration
Check out the Wiki tab for all guides on installation and configs.
Usage
Languages
Commands and Permissions
",12
forcedotcom/salesforcedx-vscode,TypeScript,"Salesforce Extensions for VS Code



Introduction
This repository contains the source code for Salesforce Extensions for VS Code: the Visual Studio Code (VS Code) extensions for Salesforce DX.
Currently, we have the following extensions:

salesforcedx-vscode
A top-level extension pack that automatically installs the following extensions for you.
salesforcedx-vscode-core
This extension interacts with the Salesforce CLI to provide basic Salesforce DX functionality.
salesforcedx-vscode-apex
This extension uses the Apex Language Server to provide features such as syntax highlighting and code completion.
salesforcedx-vscode-apex-debugger
This extension enables VS Code to use the real-time Apex Debugger with your scratch orgs and to use ISV Customer Debugger with your subscribers’ sandbox orgs.
salesforcedx-vscode-apex-replay-debugger
This extension enables VS Code to replay Apex execution from Apex debug logs.
salesforcedx-vscode-lightning
This extension supports Aura component bundles. It uses the HTML language server from VS Code.
salesforcedx-vscode-visualforce
This extension supports Visualforce pages and components. It uses the HTML language server from VS Code.

Be an Efficient Salesforce Developer with VS Code
Dreamforce 2018 session on how to use Visual Studio Code and Salesforce Extensions for VS Code:

Getting Started
If you are interested in contributing, please take a look at the CONTRIBUTING guide.
If you are interested in building the extensions locally, please take a look at the publishing doc.
You can find more information about developing Salesforce Extensions for VS Code in the docs folder. If the docs don’t cover what you are looking for, please feel free to open an issue.
For information about using the extensions, consult the README.md file for each package.
",379
haskell-works/hw-kafka-conduit,Haskell,"kafka-client-conduit

Conduit based API for kafka-client.
Ecosystem
HaskellWorks Kafka ecosystem is described here: https://github.com/haskell-works/hw-kafka
Example
A working example can be found at example/Main.hs
Prerequisites
Running an example requires Kafka to be available at localhost:9092
docker-compose can be used for spinning up Kafka environment:
$ docker-compose up

Running the example
To build and run the example project:
$ stack build --flag hw-kafka-conduit:examples

or
$ stack build --exec kafka-conduit-example --flag hw-kafka-conduit:examples

",15
haskell-works/hw-kafka-client,Haskell,"hw-kafka-client

Kafka bindings for Haskell backed by the
librdkafka C module.
Credits
This project is inspired by Haskakafka
which unfortunately doesn't seem to be actively maintained.
Ecosystem
HaskellWorks Kafka ecosystem is described here: https://github.com/haskell-works/hw-kafka
Consumer
High level consumers are supported by librdkafka starting from version 0.9.
High-level consumers provide an abstraction for consuming messages from multiple
partitions and topics. They are also address scalability (up to a number of partitions)
by providing automatic rebalancing functionality. When a new consumer joins a consumer
group the set of consumers attempt to ""rebalance"" the load to assign partitions to each consumer.
Example:
$ stack build --flag hw-kafka-client:examples

or
$ stack build --exec kafka-client-example --flag hw-kafka-client:examples

A working consumer example can be found here: ConsumerExample.hs
To run an example please compile with the examples flag.
import Control.Exception (bracket)
import Data.Monoid ((<>))
import Kafka
import Kafka.Consumer

-- Global consumer properties
consumerProps :: ConsumerProperties
consumerProps = brokersList [BrokerAddress ""localhost:9092""]
             <> groupId (ConsumerGroupId ""consumer_example_group"")
             <> noAutoCommit
             <> logLevel KafkaLogInfo

-- Subscription to topics
consumerSub :: Subscription
consumerSub = topics [TopicName ""kafka-client-example-topic""]
           <> offsetReset Earliest

-- Running an example
runConsumerExample :: IO ()
runConsumerExample = do
    res <- bracket mkConsumer clConsumer runHandler
    print res
    where
      mkConsumer = newConsumer consumerProps consumerSub
      clConsumer (Left err) = return (Left err)
      clConsumer (Right kc) = (maybe (Right ()) Left) <$> closeConsumer kc
      runHandler (Left err) = return (Left err)
      runHandler (Right kc) = processMessages kc

-------------------------------------------------------------------
processMessages :: KafkaConsumer -> IO (Either KafkaError ())
processMessages kafka = do
    mapM_ (\_ -> do
                   msg1 <- pollMessage kafka (Timeout 1000)
                   putStrLn $ ""Message: "" <> show msg1
                   err <- commitAllOffsets OffsetCommit kafka
                   putStrLn $ ""Offsets: "" <> maybe ""Committed."" show err
          ) [0 .. 10]
    return $ Right ()
Producer
kafka-client producer supports sending messages to multiple topics.
Target topic name is a part of each message that is to be sent by produceMessage.
A working producer example can be found here: ProducerExample.hs
Delivery reports
Kafka Producer maintains its own internal queue for outgoing messages. Calling produceMessage
does not mean that the message is actually written to Kafka, it only means that the message is put
to that outgoing queue and that the producer will (eventually) push it to Kafka.
However, it is not always possible for the producer to send messages to Kafka. Network problems
or Kafka cluster being offline can prevent the producer from doing it.
When a message cannot be sent to Kafka for some time (see message.timeout.ms configuration option),
the message is dropped from the outgoing queue and the delivery report indicating an error is raised.
It is possible to configure hw-kafka-client to set an infinite message timeout so the message is
never dropped from the queue:
producerProps :: ProducerProperties
producerProps = brokersList [BrokerAddress ""localhost:9092""]
             <> sendTimeout (Timeout 0)           -- for librdkafka ""0"" means ""infinite"".

Delivery reports provide the way to detect when producer experiences problems sending messages
to Kafka.
Currently hw-kafka-client only supports delivery error callbacks:
producerProps :: ProducerProperties
producerProps = brokersList [BrokerAddress ""localhost:9092""]
             <> setCallback (deliveryErrorsCallback print)

In the example above when the producer cannot deliver the message to Kafka,
the error will be printed (and the message will be dropped).
When sendTimeout is not configured to Timeout 0 (infinite), no error callbacks will be delivered.
This is because no message will ever be timing out for sending.
Example
{-# LANGUAGE OverloadedStrings #-}
import Control.Exception (bracket)
import Control.Monad (forM_)
import Data.ByteString (ByteString)
import Kafka.Producer

-- Global producer properties
producerProps :: ProducerProperties
producerProps = brokersList [BrokerAddress ""localhost:9092""]
             <> logLevel KafkaLogDebug

-- Topic to send messages to
targetTopic :: TopicName
targetTopic = TopicName ""kafka-client-example-topic""

-- Run an example
runProducerExample :: IO ()
runProducerExample =
    bracket mkProducer clProducer runHandler >>= print
    where
      mkProducer = newProducer producerProps
      clProducer (Left _)     = return ()
      clProducer (Right prod) = closeProducer prod
      runHandler (Left err)   = return $ Left err
      runHandler (Right prod) = sendMessages prod

sendMessages :: KafkaProducer -> IO (Either KafkaError ())
sendMessages prod = do
  err1 <- produceMessage prod (mkMessage Nothing (Just ""test from producer"") )
  forM_ err1 print

  err2 <- produceMessage prod (mkMessage (Just ""key"") (Just ""test from producer (with key)""))
  forM_ err2 print

  return $ Right ()

mkMessage :: Maybe ByteString -> Maybe ByteString -> ProducerRecord
mkMessage k v = ProducerRecord
                  { prTopic = targetTopic
                  , prPartition = UnassignedPartition
                  , prKey = k
                  , prValue = v
                  }
Installation
Installing librdkafka
Although librdkafka is available on many platforms, most of
the distribution packages are too old to support kafka-client.
As such, we suggest you install from the source:
git clone https://github.com/edenhill/librdkafka
cd librdkafka
./configure
make && make install

Sometimes it is helpful to specify openssl includes explicitly:
LDFLAGS=-L/usr/local/opt/openssl/lib CPPFLAGS=-I/usr/local/opt/openssl/include ./configure

Installing Kafka
The full Kafka guide is at http://kafka.apache.org/documentation.html#quickstart
Alternatively docker-compose can be used to run Kafka locally inside a Docker container.
To run Kafka inside Docker:
$ docker-compose up

",70
pivotal-cf/docs-pks,HTML,"Enterprise Pivotal Container Service (Enterprise PKS) Documentation
This repository contains the content for the documentation for Enterprise Pivotal Container Service (Enterprise PKS).
The content in this repository publishes to the Enterprise PKS documentation site at https://docs.pivotal.io/runtimes/pks/index.html.
How To Contribute
Please help us improve the accuracy and completeness of the Enterprise PKS documentation by contributing.
The easiest way to contribute is to file a pull request through GitHub.
Every topic in the Enterprise PKS documentation site has a corresponding file in this repository. Locate the correct file and make a pull request against it. You can also navigate to the topic in the Enterprise PKS documentation site and click ""View the source for this page in GitHub"" at the bottom of the topic.
Versions and Branching



Branch Name
Content
Location




master
Enterprise PKS 1.5 pre-release content
N/A


1.4
Enterprise PKS 1.4.x released content
https://docs.pivotal.io/runtimes/pks/1-4/index.html


1.4.x-patch-releases
Enterprise PKS 1.4.x pre-release content
N/A


1.3
Enterprise PKS 1.3.x released content
https://docs.pivotal.io/runtimes/pks/1-3/index.html


1.3.x-patch-releases
Enterprise PKS 1.3.x pre-release content
N/A


1.2
Enterprise PKS 1.2.x released content
https://docs.pivotal.io/runtimes/pks/1-2/index.html


1.2.x-patch-releases
Enterprise PKS 1.2.x pre-release content
N/A


1.1
Not in use
N/A (PDF available)


1.0-publish
Not in use
N/A (PDF available)


0.8
Not in use
N/A



master: The master branch is used to publish the pre-release v1.5 version of the site. Create pull requests on master to contribute bug fixes or correct technical inaccuracies in the v1.5 documentation.
This documentation requires validation. Contact #pcf-docs on Pivotal Slack for access.
1.4: The 1.4 branch is used to publish the live v1.4 version of the site. Create pull requests on 1.4 to contribute bug fixes or correct technical inaccuracies in the v1.4 documentation.
1.4.x-patch-releases: The 1.4.x-patch-releases branch is used to work on documentation for the pre-release v1.4.x version of Enterprise PKS. This branch publishes to an internal staging site only. Create pull requests on 1.4.x-patch-releases to contribute content or correct technical inaccuracies in the v1.4.x pre-release documentation.
This documentation requires validation. Contact #pcf-docs on Pivotal Slack for access.
1.3: The 1.3 branch is used to publish the live v1.3 version of the site. Create pull requests on 1.3 to contribute bug fixes or correct technical inaccuracies in the v1.3 documentation.
1.3.x-patch-releases TThe 1.3.x-patch-releases branch is used to work on documentation for the pre-release v1.3.x version of Enterprise PKS. This branch publishes to an internal staging site only. Create pull requests on 1.3.x-patch-releases to contribute content or correct technical inaccuracies in the v1.3.x pre-release documentation.
This documentation requires validation. Contact #pcf-docs on Pivotal Slack for access.
1.2: The 1.2 branch is used to publish the live v1.2 version of the site. Create pull requests on 1.2 to contribute bug fixes or correct technical inaccuracies in the v1.2 documentation.
1.2.x-patch-releases TThe 1.2.x-patch-releases branch is used to work on documentation for the pre-release v1.2.x version of Enterprise PKS. This branch publishes to an internal staging site only. Create pull requests on 1.2.x-patch-releases to contribute content or correct technical inaccuracies in the v1.2.x pre-release documentation.
This documentation requires validation. Contact #pcf-docs on Pivotal Slack for access.
The 1.1 branch is no longer maintained. A PDF of the Enterprise PKS v1.0 documentation is available at https://docs.pivotal.io/archives/pks-docs-1.1.pdf.
The 1.0-publish and 0.8 branches are no longer maintained. A PDF of the Enterprise PKS v1.0 documentation is available at https://docs.pivotal.io/archives/pks-docs-1.0.pdf.
How To Use Bookbinder To View Your Documentation
Bookbinder is a command-line utility for stitching Markdown documents into a hostable web app. The PCF Documentation Team uses Bookbinder to publish our documentation sites, but you can also use Bookbinder to view a live version of your documentation on your local machine.
Bookbinder draws the content for the site from this repository, the left navigation menu (""subnav"") from docs-book-pks, and various layout configuration and assets from docs-layout-repo.
To use Bookbinder to view your documentation, perform the following steps:

Clone this repository to the ~/workspace directory on your local machine.
Clone docs-book-pks and docs-layout-repo to the ~/workspace directory on your local machine.
Change into the docs-book-pks directory.
Run bundle install to install all of the necessary gems, including Bookbinder.
Build your documentation site with bookbinder in one of the two following ways:


Run bundle exec bookbinder watch to build an interactive version of the documentation and navigate to localhost:4567/runtime/pks/1-4/index.html in a browser. (It may take a moment for the site to load at first.) This builds a site from your content repository at docs-content, and then watches that repository to update the site if you make any changes to the repository.
Run bundle exec bookbinder bind local to build a Rack web-app of the book. After the bind has completed, cd into the final_app directory and run rackup. Then navigate to localhost:9292/runtime/pks/1-4/index.html in a browser.

",12
Lombiq/Orchard-Azure-Indexing,C#,"Orchard Azure Search Indexing Readme
Project Description
Orchard module with a search indexing implementation that stores Lucene indices in Azure Blob storage.
Documentation
The module uses AzureDirectory to store Lucene search indices in Azure Blob storage (with a local cache) so it isn't stored on the web server's local storage. It contains a search index provider that extends and overrides the default Lucene provider. Thus after enabling the features of this module indices will be stored in Blob storage, but first you have to configure the storage: take a look at the Constants class and add entries to the appSettings or connectionStrings in the Web.config (or through the Azure Portal) corresponding to those configuration keys.
AzureDirectory is included as source to avoid a mismatch of assemblies (the project used a previous version of Azure assemblies). The actual code that's included is from https://github.com/richorama/AzureDirectory.
The module is also available for DotNest sites.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/hosting-azure-indexing (Mercurial repository)
https://github.com/Lombiq/Orchard-Azure-Indexing (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",3
bappogroup/bappo-components,JavaScript,"bappo-components
Cross-platform React components for building Bappo apps
Installation
npm i --save bappo-components
Usage
import React from 'react';
import { styled, Text, View } from 'bappo-components';

class MyComponent extends React.Component {
  render() {
    return (
      <Container>
        <Text>Hello World</Text>
      </Container>
    );
  }
}

const Container = styled(View)`
  flex: 1;
  background-color: white;
`;
Using built-in Icon components
Web (with webpack)
In webpack config file, use url-loader or file-loader to handle ttf files:
{
  test: /\.ttf$/,
  loader: 'url-loader',
  include: path.resolve(__dirname, 'node_modules/bappo-components'), // path to bappo-components
}
Then in your JavaScript entry point, inject a style tag:
import fontAwesome from 'bappo-components/fonts/FontAwesome.ttf';
const fontStyles = `@font-face { src:url(${fontAwesome});font-family: FontAwesome; }`;

// create stylesheet
const style = document.createElement('style');
style.type = 'text/css';
if (style.styleSheet) {
  style.styleSheet.cssText = fontStyles;
} else {
  style.appendChild(document.createTextNode(fontStyles));
}

// inject stylesheet
document.head.appendChild(style);
iOS

Browse to node_modules/bappo-components and drag the folder fonts to your project in Xcode. Make sure your app is checked under ""Add to targets"" and that ""Create groups"" is checked.
Edit Info.plist and add a property called Fonts provided by application (or UIAppFonts if Xcode won't autocomplete/not using Xcode) and type in the files you just added.
Note: you need to recompile your project after adding new fonts, also ensure that they also appear under Copy Bundle Resources in Build Phases.

Android
Edit android/app/build.gradle ( NOT android/build.gradle ) and add the following:
apply from: ""../../node_modules/bappo-components/fonts.gradle""
To customize the files being copied, add the following instead:
project.ext.vectoricons: [
    iconFontNames: [""FontAwesome.ttf""] // Name of the font files you want to copy
]
apply from: ""../../node_modules/bappo-components/fonts.gradle""
Credits

This library is inspired by ReactXP and React Primitives.
This library's built-in Icon components are inspired by react-native-vector-icons.

",2
Julian/jsonschema,Python,"jsonschema

 
   
jsonschema is an implementation of JSON Schema
for Python (supporting 2.7+ including Python 3).
>>> from jsonschema import validate

>>> # A sample schema, like what we'd get from json.load()
>>> schema = {
...     ""type"" : ""object"",
...     ""properties"" : {
...         ""price"" : {""type"" : ""number""},
...         ""name"" : {""type"" : ""string""},
...     },
... }

>>> # If no exception is raised by validate(), the instance is valid.
>>> validate(instance={""name"" : ""Eggs"", ""price"" : 34.99}, schema=schema)

>>> validate(
...     instance={""name"" : ""Eggs"", ""price"" : ""Invalid""}, schema=schema,
... )                                   # doctest: +IGNORE_EXCEPTION_DETAIL
Traceback (most recent call last):
    ...
ValidationError: 'Invalid' is not of type 'number'
It can also be used from console:
$ jsonschema -i sample.json sample.schema

Features

Full support for
Draft 7,
Draft 6,
Draft 4
and
Draft 3
Lazy validation
that can iteratively report all validation errors.
Programmatic querying
of which properties or items failed validation.


Installation
jsonschema is available on PyPI. You can install using pip:
$ pip install jsonschema

Demo
Try jsonschema interactively in this online demo:

Online demo Notebook will look similar to this:


Release Notes
Version 3.0 brings support for Draft 7 (and 6). The interface for redefining
types has also been majorly overhauled to support easier redefinition of the
types a Validator will accept or allow.
jsonschema is also now tested under Windows via AppVeyor.
Thanks to all who contributed pull requests along the way.

Running the Test Suite
If you have tox installed (perhaps via pip install tox or your
package manager), running tox in the directory of your source
checkout will run jsonschema's test suite on all of the versions
of Python jsonschema supports. If you don't have all of the
versions that jsonschema is tested under, you'll likely want to run
using tox's --skip-missing-interpreters option.
Of course you're also free to just run the tests on a single version with your
favorite test runner. The tests live in the jsonschema.tests package.

Benchmarks
jsonschema's benchmarks make use of perf.
Running them can be done via tox -e perf, or by invoking the perf
commands externally (after ensuring that both it and jsonschema itself are
installed):
$ python -m perf jsonschema/benchmarks/test_suite.py --hist --output results.json

To compare to a previous run, use:
$ python -m perf compare_to --table reference.json results.json

See the perf documentation for more details.

Community
There's a mailing list
for this implementation on Google Groups.
Please join, and feel free to send questions there.

Contributing
I'm Julian Berman.
jsonschema is on GitHub.
Get in touch, via GitHub or otherwise, if you've got something to contribute,
it'd be most welcome!
You can also generally find me on Freenode (nick: tos9) in various
channels, including #python.
If you feel overwhelmingly grateful, you can woo me with beer money via
Google Pay with the email in my GitHub profile.
",2270
haskell-works/hw-kafka-avro,Haskell,"kafka-avro-serialiser
Avro serialiser/deserialiser for Kafka messages. Uses SchemaRegistry for schema compatibility and discoverability functionality.
This library is meant to be compatible (on both sending and receiving sides) with Java kafka/avro serialiser (written by Confluent).
",9
groonga/groonga,C,"README
Groonga is an open-source fulltext search engine and column store.
Reference manual
See doc/source/ directory or http://groonga.org/docs/.
Here are shortcut links:

How to install: http://groonga.org/docs/install.html
Tutorial: http://groonga.org/docs/tutorial.html
How to build as a developer: http://groonga.org/docs/contribution/development/build.html

Community

@groonga on Twitter
Groonga page on Facebook
Mailing list on SourceForge.net
Chat room on Gitter

Bundled software
mruby

Path: vendor/mruby-source
License: The MIT license. See vendor/mruby-source/MITL for details.

Onigmo

Path: vendor/onigmo-source
License: BSD license. See vendor/onigmo-source/COPYING for details.

nginx

Path: vendor/nginx-${VERSION}
License: BSD license. See vendor/nginx-${VERSION}/LICENSE for details.

Authors
Primary authors

Daijiro MORI 
Tasuku SUENAGA 
Yutaro Shimamura 
Kouhei Sutou 
Kazuho Oku 
Moriyoshi Koizumi 

Patches and modules from
TODO: Update or use
https://github.com/groonga/groonga/graphs/contributors instead.

Daisuke Maki 
Kazuhiro Osawa 
Hiroyuki OYAMA 
Nguyen Anh Phu 
Hideyuki KUROSU <hideyuki. kurosu at gmail. com>
Takuo Kitame 
Yoshihiro Oyama 
cZfSunOs.U 

",487
haskell-works/hw-xml,Haskell,"hw-xml

hw-xml is a high performance XML parsing library. It uses
succinct data-structures to allow traversal of large XML
strings with minimal memory overhead.
For an example, see app/Main.hs
Notes

Semi-Indexing Semi-Structured Data in Tiny Space
Space-Efficient, High-Performance Rank & Select Structures on Uncompressed Bit Sequences

",8
althea-mesh/althea_rs,Rust,"Althea_rs
This contains many (although confusingly not all) of the Rust components for the Althea firmware. The only separated components are guac_rs which we want to be easily used externally as a Rust Payment channel light client, Clarity a lightweight transaction generation library for Ethereum, and num256 a architecture portable fixed 256 bit integer implementation.
The primary binary crate in this repo is 'rita' which produces two binaries 'rita' and 'rita_exit'
see the file headers for descriptions.
This is primarily an infrastructure repo, to get a working version of Althea you should look at installer for desktop linux and althea-firmware for OpenWRT.
Building
Debian:
sudo apt-get install build-essential libssl-dev libsqlite3-dev pkg-config postgresql-dev

Ubuntu:
sudo apt-get install build-essential libssl-dev libsqlite3-dev pkg-config postgresql-dev

Centos:
sudo yum install gcc gcc-c++ openssl-devel sqlite-devel make postgresql-devel

Fedora:
sudo dnf install gcc gcc-c++ openssl-devel sqlite-devel make postgresql-devel

Arch:
sudo pacman -S gcc gcc-libs openssl sqlite postgressql

Finally install Rust and add Rustup to your PATH
You are now ready to build code from this Rust repository by running
cargo build --all

If you want to build a development build that contains unsafe options that are not suitable for production usage:
cargo build --all --features development

Development
Please install required git hooks before contributing. Those hooks are responsible for making the codebase consistent.
rustup component add rustfmt-preview --toolchain nightly
cd .git/hooks && ln -s ../../scripts/.git-hooks/pre-commit
Components
Rita
The only binary output of this repo that ends up on routers and the 'main' Crate. The Rita binary is run as a daemon on the mesh nodes as well as the exit nodes in an Althea network.
Status:

Discovering Peers: done
Opening Wireguard tunnels with Peers: done
Contacting the Exit server to negotiate credentials: done
Opening a Wireguard tunnel to the exit: done
Setting the user traffic route to the exit tunnel: Partially complete, needs ipv6
Accepting commands from the user configuration dashboard and applying them: Done
Accounts for bandwidth used and required payment: Has known bugs
Communicates with Babeld to get mesh info: done
Communicates with Babeld to detect fraud: in progress
Makes payments: Will mostly be contained in the Guac_rs repo

althea_kernel_interface
Handles interfacing with the kernel networking stack. Right now it does this by shelling out to common Linux commands like 'ip', 'iptables', 'ebtables', etc. This will some day be replaced with calls in the native Netlink api for greater stability.
Status: Feature Complete
babel_monitor
Communicates with Babel's local configuration API to list routes along with their quality and price.
Status: Needs improvements to fraud detection, possibly rescue cases for crashes
bounty_hunter
A separate daemon from Rita designed to be run by channel bounty hunters on the internet. In a production Alteha network mesh devices would periodically upload their channel states to a bounty hunter. The bounty hunter will then watch the blockchain state and publish these channel states if an attempt at fraud was made. Claiming a small bounty and preventing channel fraud even when a device is knocked offline.
Status: Needs Clarity integration and Blockchain watching
clu
Manages things like exit tunnel setup, key generation, and other more using facing tasks.
Status: Feature complete
Settings
Manages the settings file, including loading/saving and updating the file.
Status: Feature complete
Cross building
Rita is designed to run on OpenWRT and other embedded devices, if you need to test your code on a router you can use the build scripts in the scripts/ folder. These will download a pre-built cross-compiler toolchain from updates.altheamesh.com and then use that to build your local althea_rs repository for the specified architecture. If you wish to build the cross-compilation toolchain yourself see the Althea firmware builder. Either way you need to be on Linux, or running Windows Linux compatibility layer.
Setting up the router
First download the latest nightly firmware for your device. Follow the OpenWRT wiki link in the table for flashing instructions. If you have a pretty recent version of the firmware you should be fine, but upgrade if see strange behavior.
Once you have a device running edit the scripts/openwrt_upload.sh script to match your device ip and target. If you have an n600 or n750 then the default settings are correct. Review the nightly firmware download table to determine the correct target name for other devices.
The router ip address is by default 192.168.10.1, if your home network is on that same ip range (probable) you may have trouble reaching the router, plug into the device directly and disable wifi or connect to the Althea-Home wifi network to make sure there's no confusion about which device you are talking to.
Finally run bash scripts/openwrt_upload.sh Rust should take a few minutes to build and then Rita should start scrolling logs on your screen. The build will take longer than your normal debugging builds because the binary needs to be much smaller to fit on most embedded devices. If you have any problems refer to the firmware debugging instructions. If that also proves unhelpful drop by our matrix chat and ask.
",40
danielocallaghan/dotfiles,Vim script,"Dan's Dotfiles
Various vim/env related configuration files that can be symlinked to the current users profile/home dir by running ./createsymlinks
Vim Settings
Plugins & extensions managed by Vundle:

https://github.com/gmarik/vundle

Mappings, functions & bindings and general guidance pulled together from various sources such as:

https://github.com/skwp/dotfiles
http://mislav.uniqpath.com/2011/12/vim-revisited/
https://github.com/garybernhardt/dotfiles/blob/master/.vimrc
https://github.com/r00k/dotfiles

",2
Lombiq/Orchard-Scripting-Extensions,C#,"Orchard Scripting Extensions Readme
Project Description
Core module for running scripts inside Orchard.
Documentation
This module depends on Helpful Libraries. Please install it first!
WARNING: This module is only compatible with Orchard 1.6 or greater!
Scripting Extensions enhances Orchard's scripting capabilities by adding numerous new features for scripting support. Together with PHP Scripting Extensions you can use it to run PHP scripts inside Orchard for example.
Note: this module only contains common services but no scripting runtime. Check out the following engines and install one or more of them:

PHP
.NET (C#/VB)
JavaScript

Module overview
This module consists of two features:

Orchard Scripting Extensions: Core (OrchardHUN.Scripting): contains the core of scripting extensions
Orchard Scripting Extensions: Rules (OrchardHUN.Scripting.Rules): adds script running rules to the Orchard Rules engine

Core feature
This feature adds basic services for script management and also adds a scripting testbed to the dashboard.
Generic services
Orchard Scripting Extensions include multiple generic services (described below) that automatically use every scripting engine existing and enabled in the Orchard instance. By implementing the simple IScriptingRuntime interface you can extend the set of scripting engines with your own.
To use scripting engines from your own code, use IScriptingManager.
Script content types
Scripts are regular content items. There are two script content types:

Script: this one stores a script source code written in the selected language.
Composite Script: Composite Scripts can reference other script content items. When a Composite Script is run, all the other scripts referenced by it will be run in the order which they are referenced.

Scripts' editors are syntax-highlighted thanks to the ACE editor.
Testbed
The testbed is a script editor that you can access from under the ""Scripting"" menu on the admin site. You can just write a script there or select an existing script and run it for testing purposes.
The testbed uses dynamic pages from Helpful Libraries. This makes pages like the testbed fully extensible and customizable like content items.
Scripting Rules
This feature adds scripting-related extensions to the Rules engine.
Script Execution Action
This action can be used to run an arbitrary script when an event fires. You can e.g. hook into the content item life cycle and run a script when a content item is published.
Adding your own scripting engine
By implementing the simple IScriptingRuntime interface you can write your own interface that can then used with all the common scripting features. Take a look at PHP Scripting Extensions for an example.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/orchard-scripting-extensions (Mercurial repository)
https://github.com/Lombiq/Orchard-Scripting-Extensions (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",4
realguoshuai/hadoop_study,Java,"
hadoop

hadoop科普
大数据书籍
大数据学习路线图

How Study

阅读官方文档
看大牛博客

hadoop 导图笔记

hadoop伪分布式搭建
hadoop介绍
hadoop经典面试题及解题思路

hive

阅读官方文档
导图笔记
hive必会50道sql
hive在离线计算用的比较多,工作好找,有sql基础入门比较快

Scala

阅读官方文档
导图笔记
scala 工作中spark代码一般都是使用 Scala 实现 优先学!

Flink

阅读官方文档
流计算利器,但是资料比较少,建议直接看官方文档 我会将自己的翻译的文档不定期上传到Flink的目录下
Flink 阿里直播(每周四晚20:00-21:00))
QA

Spark

阅读官方文档
导图笔记

SparkStreaming

阅读官方文档
导图笔记

Spark Structured Streaming

阅读官方文档
导图笔记

HBase(phoenix)

文档笔记
见HBase目录,不定期更新

kafka

阅读官方文档
导图笔记
开源的消息队列,流计算架构一定要有的一个组件

Eleasticsearch

占坑,可以直接使用dbms+es实现几千万数据可视化展示 汇总,底层跟solr一样,都是基于lucene

solr

阅读官方文档
导图笔记
全文检索 solr+kerberos验证(巨坑,文档稍后上传)

redis

Redis命令参考
导图笔记

kerberos

这个真恶心 人机/机机
2019-5-5 更新 Fusioninsigh下的所有组件互通已经调试好

springboot

用空还是要学一下 最近做了个springboot实现solr rest服务 套的公司应用层模板

持续更新..

会不定期的将在工作中接触大数据组件时做的去敏测试代码上传到对应的文件夹下供初学者参考,少走弯路    包括自己每天更新的大数据印象笔记  更新的进度和规划在issues 中
由于精力有限 今年计划重心是在实时计算上  flink 和 sparkStreaming   spark结构化流公司不用  有空再去搞
最近忙到爆炸 停更三周

",93
DavidFW1960/Aussie-Broadband-Usage-Meter,Visual Basic,"Aussie Broadband Usage Meter
AussieBroadband usage meter skin for Rainmeter.
Credits and information
Originally made by Kanine and adapted for Aussie Broadband by HD, jandakot11, Protogen and Big Kahuna on the whirlpool forums
Original Inspiration: Kanine's Bigpond Skin
HD's ABB Original
Jandakot11's ABB Modified
HD's and Jayden's skins are distributed in the RMSKIN file as well.
Thanks also to nclemeur for identifying and fixing a password issue with complex passwords. (Original ABB Portal)
BIG THANKS to Protogen for updating the scripts to work with the new Aussie Broadband Portal.
Installation steps

Install Rainmeter (Minimum Version 4.1)
Download Aussie Broadband Usage 0.71.rmskin
Run the .rmskin to install it with Rainmeter, use Manage Rainmeter to add it to desktop, you will be prompted for your Aussie Broadband login details that will be encrypted and stored locally.





There will be seven .ini skin files:
To change colours and other options, see the notes below for version 0.34 Or just use one of these 7 provided skins.

There are variants with a solid bar (see above), 5px and 8px as well as dashed bar 5px (see below) and 8px. The Data used bar is graded in colour from green to red. Also included are HD's original and Jayden's modified skin. The ABB.ini skin is the same as the 5px image one and is my current favourite skin.
I also added a mid (12 point) and a large (16 point) font variant that is a larger meter and has different graphics. The bar style and height can be changed in the same way as the other skins.
The only difference between these is I have edited the variable barStyleSize to select the right image or colour bar size as per the below but otherwise all of the variants are identical (except for HD and Jayden's originals of course)
The progressive image skins look like this:

The Manage Skin Screen looks like this:

In Rainmeter, select Manage Skins and in the Aussie Broadband folder select abb.ini and load it (see above graphic).
Note you can change transparency in that panel.
On the main skin, clicking the ABB icon will load the customer portal at Aussie.
On first load it will prompt for ABB username and password
In this version, I added a calculation to take into account how much of the current day has been used for days remaining and also days used and am using that to give a more accurate estimate of daily use. I am also now showing the Allowance per day at start of month and remaining with the other information in the tooltip as per the image here.
If you are over your quota for this stage of the month the percent used bar will be red.
Note that if you set the nominalAllowance to 100000 it imitates an unlimited plan (see variables below). If this is set to 0 it will use the value it reads from Aussie Broadband.
Unlimited plans don't show the lower data percent used bar and don't show any of the allowance statistics in the tooltip. Set the nominalAllowance to 100000 to see this if you are not on an umlimited plan.
NOTE: Some users have problems with default secure protocols (I've had reports from users running Windows 7, 64 bit) Applying a Microsoft hotfix has been reported to fix this
Security and Password Encryption/Encoding
The Username and Password are no longer stored as we obtain a cookie and refresh token as per the changelog. IF you change your password, the credentials will be invalidated and you will be prompted to gothrough the initial setup again.
Changelog
0.71 Sunday 14th April 2019 Remove debug and restore refresh token to 1/2 cookie life.
New in version 0.70
We are now using the new Aussie Broadband Portal which uses a cookie and a refresh token to download usage once you have authenticated. The cookie is valid for 1 year. The first time you open the new skin, you will be required to enter your ABB Username and Password to authenticate. This will then download a cookie and refresh token from ABB. When the cookie is at it's half life, the script will automatically use the refresh token to get a new cookie and refresh token. You will also be prompted to enter a description for your service. This description and service ID will be shown on the skin. This is also to facilitate an upcoming change in mid May to allow multiple services to be monitored.
New in version 0.34
So I made some major changes in this version. I have moved all the colours and images from being scattered in the .ini file to now all being specified in the variables section. If you edit the abb.ini file (or any of the others except HD's and Jayden's) scroll down to line 21. It looks like this:
[Variables]
fontName=Trebuchet MS
textSize=8
colorBarDays=8,71,174,255
colorBarData=34,177,76,255
colorBarOver=255,0,0,255
colorText=255,255,255,205
imageBarDays5px=#@#Images\DaysRemaining-5px.png
imageBarDays8px=#@#Images\DaysRemaining-8px.png
imageBarData5px=#@#Images\DataUsedProgress-5px.png
imageBarData8px=#@#Images\DataUsedProgress-8px.png
imageBarDataOver5px=#@#Images\DataUsedOver-5px.png
imageBarDataOver8px=#@#Images\DataUsedOver-8px.png
fontEffectColorSet=0,0,0,20
solidColorSet=0,0,0,255
;Set the bar size and type. Valid Options are solid5px, solid8px, image5px, image8px
barStyleSize=image5px
;Set the nominal allowance if required. 0 means value is read from ABB otherwise enter GB allowance (so you see data used bar)
nominalAllowance=0

Here, you can set the bar colours for a solid bar for Days remaining, Data Used and a data used colour that indicates you will go over your allowance based on usage in the month to date. As a default I'm using blue (8,71,174), green (34,177,76) and red (255,0,0) for these as above. The 4th term in the colour is the alpha value, 255 for each.
Also if using an image, you can define what image to use here. If you create your own, make sure they are 183px wide and either 5px or 8px high.
Another change is that if you are on an umlimited plan, the data used meter will not show unless you manually overide the allowance by specifying nominalAllowance=1000 (say) as per above. Note to 'simulate' an unlimited plan in the meter, Aussie Broadband sets a data allowance of 100000 for unlimited plans so if you enter that as the nominalAllowance, the skin will switch to an unlimited plan and show as unlimited. To use the ABB allowance as defined by ABB, just set the nominalAllowance to 0.
On unlimited plans, a tooltip calculates an extimate of usage projected to the end of the cycle.
I am also calculating a Quota Remaining Today number in the tooltip so you know how much quota you can still use for the rest of today and not go into the red zone.
I have also made it possible to set the meter to use either a solid bar or an image and to set the size of these to 5px or 8px. Set the variable barStyleSize to one of the 4 VALID options as shown. You can change the solid bar colours as above and if you wish to change the image, you can make your own and replace or add to my images. All references to the bar or image used are in the variables section as shown above.
",4
leanprover-community/mathlib,Lean,"Lean mathlib



Mathlib is a user maintained library for the Lean theorem prover.
It contains both programming infrastructure and mathematics, as well as tactics that use the former and allow to develop the later.
Installation
You can find detailed instructions to install Lean, mathlib, and supporting tools:

On Debian-derived Linux (Debian, Ubuntu, LMDE...)
On other Linux distributions
On MacOS
On Windows

Documentation
Besides the installation guides above and Lean's general
documentation, the documentation
of mathlib consists of:

A description of currently covered theories,
as well as an overview for mathematicians.
A couple of tutorials
Some extra Lean documentation not specific to mathlib
A description of tactics introduced in mathlib,
and available hole commands.
Documentation for people who would like to contribute to mathlib

Much of the discussion surrounding mathlib occurs in a
Zulip chat room. Since this
chatroom is only visible to registered users, we provide an
openly accessible archive
of the public discussions. This is useful for quick reference; for a
better browsing interface, and to participate in the discussions, we strongly
suggest joining the chat. Questions from users at all levels of expertise are
welcomed.
Maintainers:

Jeremy Avigad (@avigad): analysis
Reid Barton (@rwbarton): category theory, topology
Mario Carneiro (@digama0): all (lead maintainer)
Johan Commelin (@jcommelin): algebra
Sébastien Gouëzel (@sgouezel): topology, calculus
Simon Hudon (@cipher1024): all
Chris Hughes (@ChrisHughes24): group theory, ring theory, field theory
Robert Y. Lewis (@robertylewis): all
Patrick Massot (@patrickmassot): documentation, topology

",118
openweave/openweave-core,C++,"





What is OpenWeave?

Weave is the network application layer that provides a secure, reliable
communications backbone for Nest's products. OpenWeave is the open source
release of Weave.
This initial release makes available some of Weave’s core components. With
OpenWeave core, you can preview the technology by inspecting the code, learning
about the architecture, and understanding the security model.
At Nest, we believe the core technologies that underpin connected home products
need to be open and accessible. Alignment around common fundamentals will help
products securely and seamlessly communicate with one another. Nest's first open
source release was our implementation of Thread, OpenThread. OpenWeave can run
on OpenThread, and is another step in the direction of making our core
technologies more widely available.
OpenWeave Features


Low Overhead
Whereas many approaches to the Internet of Things attempt to start from
server-based or -grown technologies, struggling to scale down, Weave starts
with the perspective that if you can tackle the toughest Internet of Things
problems on the constrained product front, you can successfully scale up from
there to any product.


Pervasive architecture
By building an application architecture on top of IP and starting with a low
overhead architecture, we have achieved a holistic, common, and consistent
architecture from low-power, sleepy Thread end nodes to cloud services and
mobile devices with end-to-end addressability, reachability, and security,
making it easier on your developers and helping instill customer confidence in
Weave-enabled products.


Robust
Weave leverages and takes advantages of underlying network attributes, such as
Thread’s no single point-of-failure and mesh resiliency. In addition, Weave
devices are auto-configuring and enjoy the ability to route among and across
different network link technologies.


Secure
Weave employs security as a fundamental and core design tenet with:

Defense in depth with end-to-end application security, independent of underlying network
Tiered trust domains with application-specific groupwise keys



Easy to Use
Out-of-box experience is key to avoiding a connected product return and an
impact on your product’s profitability. Weave’s focus on a smooth, cohesive,
integrated provisioning and out-of-box experience ensures returns are
minimized.  In addition to offering simple setup and administration for the
end user, Weave also offers a straightforward but capable platform for the
application developer.


Versatile
OpenWeave has been designed to scale to home area networks containing low to
mid-100s of devices. The communication support for variety of interaction
models / patterns such as device-to-device, device-to-service,
device-to-mobile/PC (remote and local).


Getting started with OpenWeave Core
All end-user documentation and guides are located at openweave.io.
Learn how Weave works with our Weave Primer,
or dig right in and learn how to:

Build the standalone OpenWeave distribution, or for
iOS or Android project linking
Run test scripts and explore the applications that the
OpenWeave team uses to test the codebase
Use tools such as Weave Device Manager and Mock Device,
and test Weave protocols such as Heartbeat and Echo
Build simulated IoT networks with our Happy tool

Additional build information can also be found in BUILDING.md.
Need help?
There are numerous avenues for OpenWeave support:

Bugs and feature requests — submit to the Issue Tracker
Stack Overflow — post questions using the openweave tag
Google Groups — discussion and announcements at openweave-users

The openweave-users Google Group is the recommended place for users to
discuss OpenWeave and interact directly with the OpenWeave team.
Directory Structure
The OpenWeave repository is structured as follows:



File / Folder
Contents




aclocal.m4
GNU autotools auto-generated file containing autoconf macros used by the OpenWeave build system.


bootstrap
GNU autotools bootstrap script for the OpenWeave build system.


bootstrap-configure
Convenience script that will bootstrap the OpenWeave build system, via bootstrap, and invoke configure.


build/
OpenWeave-specific build system support content.


BUILDING.md
More detailed information on configuring and building OpenWeave for different targets.


certs/
OpenWeave-related security certificate material.


CHANGELOG
Description of changes to OpenWeave from release-to-release.


configure
GNU autotools configuration script for OpenWeave.


configure.ac
GNU autotools configuration script source file for OpenWeave.


CONTRIBUTING.md
Guidelines for contributing to OpenWeave


.default-version
Default OpenWeave version if none is available via source code control tags, .dist-version, or .local-version.


doc/
Documentation and Doxygen build file.


LICENSE
OpenWeave license file (Apache 2.0)


Makefile.am
Top-level GNU automake makefile source.


Makefile-Android
Convenience makefile for building OpenWeave against Android.


Makefile.in
Top-level GNU autoconf makefile source.


Makefile-iOS
Convenience makefile for building OpenWeave against iOS.


Makefile-Standalone
Convenience makefile for building OpenWeave as a standalone package on desktop and server systems.


README.md
This file.


src/
Implementation of OpenWeave, including unit- and functional-tests, tools, and utilities.


src/ble/
Definition and implementation of OpenWeave over BLE


src/device-manager/
Library for interacting with a remote OpenWeave device, implementing pairing flows. Bindings are provided for Python, iOS, Java and Android.


src/examples/
Example code


src/include/
Build support for external headers


src/inet/
Abstraction of the IP stack. Currently supports socket- and LWIP-based implementation.


src/lib/
Core OpenWeave implementation.


src/lwip
LwIP configuration for the standalone OpenWeave build.


src/platform
Additional platform support code required to bind the BLE support to BlueZ libraries.


src/ra-daemon/
Route advertisement daemon implementation


src/system/
Abstraction of the required system support, such as timers, network buffers, and object pools.


src/test-apps
Unit tests and test apps exercising Weave functionality


src/tools
Tools for generating Weave security material, and for device support


src/warm/
OpenWeave address and routing module


src/wrappers
Java wrappers for OpenWeave library


third_party/
Third-party code used by OpenWeave.



Contributing
We would love for you to contribute to OpenWeave and help make it even
better than it is today! See the CONTRIBUTING.md
file for more information.
Versioning
OpenWeave follows the Semantic Versioning guidelines for release cycle transparency and to maintain backwards compatibility.
License
License and the Weave and OpenWeave Brands
The OpenWeave software is released under the Apache 2.0
license, which does not extend a license for the use of
the Weave and OpenWeave name and marks beyond what is required for
reasonable and customary use in describing the origin of the software
and reproducing the content of the NOTICE file.
The Weave and OpenWeave name and word (and other trademarks, including
logos and logotypes) are property of Nest Labs, Inc. Please refrain
from making commercial use of the Weave and OpenWeave names and word
marks and only use the names and word marks to accurately reference
this software distribution. Do not use the names and word marks in any
way that suggests you are endorsed by or otherwise affiliated with
Nest without first requesting and receiving a written license from us
to do so. See our Trademarks and General
Principles page
for additional details. Use of the Weave and OpenWeave logos and
logotypes is strictly prohibited without a written license from us to
do so.
",115
openbmc/meta-phosphor,BitBake,"OpenBMC
meta-phosphor is the OpenBMC layer. This layer should be included for all
OpenBMC systems. The OpenBMC layer contains content which is shared between all
OpenBMC systems.
",6
makowis/teijigo-beer-time,Vue,"teijigo-beer-time

定時後ビールタイム公式サイト





Build Setup
# install dependencies
npm install

# serve with hot reload at localhost:8080
npm run dev

# build for production with minification
npm run build

# build for production and view the bundle analyzer report
npm run build --report

# run unit tests
npm run unit

# run e2e tests
npm run e2e

# run all tests
npm test
For a detailed explanation on how things work, check out the guide and docs for vue-loader.
",6
Izheil/Quantum-Nox-Firefox-Dark-Full-Theme,CSS,"
Previously known as ""Firefox 57+ full dark theme with scrollbars and multirow tabs"", I decided to give it an actual name instead of leaving it as just a description.
This theme is mainly intended for the stable release of Firefox (This means that while it will most probably work with nightly and ESR for the most part, it may have less support for those versions).
You can use it to fully change the colors of most of firefox UI to dark-gray colors (with #222-#444 colors mostly), including scrollbars, tooltips, sidebar, as well as dialogs. With the files here you can also as remove some context menu options, enable multirow tabs, change the font of the url bar...
Of course... you could as well use these files to color your firefox any way you wanted, the only thing you'd have to do is change the correct values (what each class or id does is commented above each) in the .css files (as far as you know some 
basic css or color coding, it shouldn't be too hard) using notepad, or some code editing program (such as notepad++ on Windows).
To change these you will have to use the right hex codes. You can find a color picker to hex code in this page.
If you want to edit a file and you want to use notepad (windows), you may see that all code is a wall of text without any line break (the files get compressed like that when uploaded, so there isn't much to do there), in which case you can always drag & drop the file you want to modify into any internet browser window (like firefox) to see the actual code with line breaks, and then copy & paste it back to the open file with notepad, making it regain the line breaks on notepad again.

This problem doesn't happen if you use a code editor such as notepad++, atom, sublime text...
Last update: 12/05/2019
Files updated:

Userchrome.css: Fixed white flashes on page pre-loading without affecting pages that use about:blank.

Pre-Last update: 06/05/2019
Files updated:

Usercontent.css: Added a missing textarea box on mozilla addons page.
Addons.css: Themed Ant Video Downloader addon, and removed Flash Video Downloader (Mozilla took the original one down from their addons page).


A note on people looking to replace some Tab Mix Plus features
As for using this theme to replace some functions of Tab Mix Plus, I'll keep the functions that can be done through CSS here, and I'll try to point addons that cover some of the missing functions, but as of right now, the files in this repository covers multi-row tabs, keep the close button on tabs always visible, and color-coding tabs when they are loaded, unloaded, etc...
Most other functions of Tab Mix Plus can already be ""simulated"" changing some about:config settings:

To keep FF open even after closing the last tab -> browser.tabs.closeWindowWithLastTab to false.
To open a search result typed on the URL bar on a new tab -> browser.urlbar.openintab to true.
To open a search result typed on the search bar on a new tab -> browser.search.openintab to true.
To open bookmarks on a new tab instead of the current tab -> browser.tabs.loadBookmarksInTabs to true.
To force popups on new tabs instead of windows -> browser.link.open_newwindow.restriction to 0 (should be 2 by default).
Open related tabs (the ones you open) as the last tab in the tab bar -> browser.tabs.insertRelatedAfterCurrent to false.

... or through extensions (not a comprehensive list, only the ones themed here are mentioned), like Tab session manager, Undo closed tabs button.

Installation
Main browser UI


If you are on windows and only want the theme or multirow, you can use the batch file installers inside the installers folder.
If you are using Linux or Mac, or want to add some more functionability (like deleting some useless context menu commands), you will have to use the methods described inside one of the 3 main folders of this repository:
Short review of each folder:

CSS tweaks: Enables removal of context menu items, multirow bookmarks, changing tab bar position (so that it could be under the bookmarks bar for example)
Full dark theme: Gives dark colors to firefox UI, including the scrollbars and the tooltips. Can also change the background image of about:home and the header image used as a persona.
Multirow and other functions: You can find the JS files that add extra functionability to Firefox that couldn't be done with CSS alone.


You can turn the features you want on or off changing the commented lines on userchrome.css (To change them you just have to open it with notepad or any code editor, and encase between ""/*"" and ""*/"" (without the quotation marks) the lines you don't want to take effect). Of course, if you think that you are NEVER going to use certain feature, you can always delete the specific lines you don't want without any other side-effect.
You can find a video tutorial on how to install the theme without installers here.
General sites dark theme
You can apply the global dark userstyle found inside the Global dark userstyle folder to theme general sites with an all-around CSS stylesheet. You need stylus addon to use it.
While it's not perfect (meaning that you should still use per-site styles for the sites you visit often), it can help to darken most sites when browsing around general sites that you don't visit often, and thus don't want/can't find a specific userstyle for them.


It is recommended that you check the Global dark userstyle readme to know how to add site exclusions to the global userstyle.
Addon dark themes
You can apply a dark theme to certain addons changing the UUID's of them inside the addons.css file inside the ""Full dark theme"" folder (more instructions on how to do that inside the addons file).

Here is a list of the themed addons:

Ant Video Downloader
Cookie autodelete
Download Manager (S3)
History autodelete
HTTPS Everywhere
Noscript
Notifier for Gmail (restartless)
Multi-accounts containers
OneTab
Popup Blocker Ultimate
Privacy badger
Tab session manager
Temporary containers
Ublock Origin
Undo closed tabs button
Video Download Helper
Viewhance

You might have noticed that we no longer have Lastpass dark theme inside addons.css anymore. This is because at the time that addon was themed, I didn't know much about it. After some research it seems like Lastpass has had a history of security issues (in 2011, 2015, 2016, and 2017), as well as there being other open source alternatives out there that seem to be more reliable, like Bitwarden (it also has a built-in dark mode) which is available for all major browsers.
The scrollbars
This theme colors scrollbars using usercontent.css to give them a basic re-color.

If you want a different style on the scrollbars, you can try using the scrollbars.as.css file inside the Alternative scrollbars folder, which will make the scrollbars look more rounded and will have some sort of ""puffy"" effect when clicking them.

If instead you just don't want scrollbars to show at all but keep scrollability, you can do this through usercontent.css setting the variable --scrollbars-width to none (should be the first rule on the :root section (almost at the start)), and deleting scrollbars.as.css.
If you aren't using the usercontent provided here for some reason, you can always just add this code to your self-created usercontent.css:*|* {scrollbar-width: none !important}

FAQ:
How do I submit an issue?
As of 26/03/2019 I stopped offering active support for new features or issues. This doesn't mean that I won't be mantaining the project, it just means I won't be taking feature requests nor unrelated issues to the functionability offered by the files inside this repository anymore.
If you find some problem that is directly related with any of the functions offered by any of the files in this repository, you can comment it inside the relevant commit that you think may have affected the function that is giving you trouble. If you can't tell which, comment in the last one. Comments about new functionability or things that aren't related to the actual functionability of the files will be ignored (You can already ask about problems you may have with firefox on r/firefox or r/firefoxCSS subreddits, or on Firefox support pages).
The pre-loading screen of websites is still white, how can I change this?
The fastest way to solve the ""blinking"" white screen is to change the default web background color on Firefox settings > General > ""Colors..."" button > Background, which will make the blinking dissapear and be changed to the color you set up. This, although, can cause some issues on some very few and specific pages like BBC, where they don't set a background color to override the one set here (the number of sites with this problem is very small, most sites override the background color set by this setting).
The synced tabs sidebar isn't themed.
Since it's anonymous content of the browser we can't theme it through userChrome or userContent, which is why you will have to apply the fix inside Sync-tabs-sidebar.as.css. It requires the use of external CSS files loading, which is enabled thorugh userchrome.css and userchrome.xml.
Some context menu commands dissapear after installing userchrome.
If you only want the dark theme, use the default userchrome.css file inside ""Full dark theme"", which won't make the context menu commands dissapear. In case you want to use the features part of the theme, just delete everything after the line that says /* CONTEXT MENU COMMANDS */ (you can find it using the search option on notepad, or the code editor you are using).
In case you still want to delete some commands but not all, just comment out the ones that you want to appear, and leave as active the ones that you want to dissapear.
For example, this is active, so the command is hidden:
/* Send image... */
#context-sendimage,
...But this is commented out, so the command will show on the context menu:
/* Send image... *//*
#context-sendimage,*/
You will see that the ones that I have commented out by default only have the starting ""/*"" of the comment after the description of what they are, since the closing ""*/"" would come from the next description comment below them.
The bookmarks toolbar text/tabs text color is black and I can't see the letters over the dark background.
This is caused by your persona (lightweight theme), and while you could change these settings inside userchrome, I thought it was better to just change the settings on the persona directly (since not all personas will look the same). To do so you'd have to open about:config, and search for lightweightThemes.usedThemes. Once there, find the ""textcolor"" setting and type any color you'd want to use instead of black or the color being used by the theme (use #fff for white). The persona you are currently using should be in the first place in the list.
The bookmarks multirow shows an empty scrollbar when enabled.
If you are using an old version of the scrollbars, or you are just plain not using the scrollbars here, you will have to add some code to delete the empty scrollbars that show on the bookmark toolbars. You have to use this code on a ""*.ac.css"" file (so you would need to have firefox patched with the method explained on the Multirow and other functions folder), since otherwise it won't work:
/* This deletes the scrollbar from bookmarks toolbar when using multirow bookmarks */
#PlacesToolbarItems scrollbar {display: none !important}

I only want to use the multirow/(Any other) feature, but not the other ones!
You only need to modify userChrome.css, deleting the lines that you don't want to apply (Every function has a comment above it saying what each ruleset does), or if you think you may want them later, just encase the feature parts that you don't want to apply between /* and */:
/* This is an example of a comment that wouldn't be read on a .css file */
I'm opening web files locally (as in opening html pages that you have created or downloaded) and the background is not the color it should be.
To change the directory browsing page and change how .css or some .txt files appear when opened with Firefox, I had to specify it to affect urls that start with ""file:///"", meaning that any file opened with Firefox will get overriden with those rules as well. To prevent this, go to userContent.css, and comment out the lines that affect this url (This rule should be exactly under the color variables at the start of the file).
I placed userchrome.css inside my chrome folder and I still don't have multi-row tabs!
While we only needed to use CSS to enable multi-row tabs, this breaks tabs draggability, making reordering tabs when it was enabled a bit erratic, so to fix this, I decided to put all multi-row tabs code inside the MultiRowTabLiteforFx.uc.js file. This means that now Multi-row tabs can be enabled following the method described inside the Multirow tabs folder. If you were using CSS code on your userchrome.css to enable multirow tabs, delete (or comment it out) for the js file to take effect.
Why use this method instead of using Stylus?
The main reason is that you can't style firefox about: pages nor dialog windows with just stylus.
The theme is making the text of some addon popups unreadable, how do I fix this?
The theme is made so that it changes most background colors, including the one of the popups that don't have any background color specified by their original creator. Sadly it doesn't change the text of these by default, so you may have to do it manually, or report the addon you want themed here, or just use the fix inside userchrome.css (at around lines 926-929) to turn the addons back to their white background color.
Credits
The original code for the custom scrollbars which we modified here belongs to Arty2, and you can find it here.
The original code for the multirow tabs (the CSS part) was written by Andreicristianpetcu, and you can find it here, or for just the code, here. The fix of multi-row tabs draggability was made by TroudhuK, and you can find the original one here.
The original code for the multirow bookmarks toolbar belongs to the original creator mentioned in this reddit thread, whose code was fixed by jscher2000 to use in our current firefox version.
The fix to be able to theme unread tabs again after FF61 (see bug 1453957 on bugzilla) as mentioned in this reddit thread, was made by moko1960 to use in Firefox 61+.
The code to be able to edit anonymous content (in our case the scrollbars and tooltips) was created thanks to the efforts of Zeniko, Nichu, and Sporif.
Special thanks to Messna for noting the class turning into an ID on FF58, and Snwflake for fixing Firefox root folder location on MacOS.
Also thanks to BelladonnavGF, Demir-delic, DallasBelt, Gitut2007, Hakerdefo, Tkhquang and YiannisNi for noting some issues with the theme, and the requests for new features that extended this project.
Donations
If you want to support this project, consider buying me a coffee to motivate me keep this repository up and running.
​

",135
nnhubbard/ZSSRichTextEditor,Objective-C,"HELP NEEDED
I no longer have the time needed to work on this project. If anyone is interested in helping or taking over this project please contact me @zedsaid.
ZSSRichTextEditor
The Editor
ZSSRichTextEditor is a beautiful Rich Text WYSIWYG Editor for iOS. It includes all of the standard editor tools one would expect from a WYSIWYG editor as well as an amazing source view with syntax highlighting.

The editor is used how any other text input area in iOS is used. A selection of text or content is made, then tapping on the toolbar item below will apply that style. A Source View is also included, you can make changes and this will be reflected in the editor preview.

Colors
We wanted to have a really beautiful color picker to make changing colors really simple. So, we used the open-source HRColorPicker which was exactly what we were looking for. Choosing colors for text or background is simple and seamless.

How It Works
Just subclass ZSSRichTextEditor and use the following:
// HTML Content to set in the editor
NSString *html = @""<!-- This is an HTML comment -->""
""<p>This is a test of the <strong>ZSSRichTextEditor</strong> by <a title=\""Zed Said\"" href=\""http://www.zedsaid.com\"">Zed Said Studio</a></p>"";

// Set the base URL if you would like to use relative links, such as to images.
self.baseURL = [NSURL URLWithString:@""http://www.zedsaid.com""];

// If you want to pretty print HTML within the source view.
self.formatHTML = YES;

// set the initial HTML for the editor
[self setHTML:html];
If you want to retrieve the HTML from the editor:
// Returns an NSString
[self getHTML];
Insert HTML at the current caret position:
NSString *html = @""<strong>I love cats!</strong>"";
[self insertHTML:html];
Change the tint color of the toolbar buttons:
// Set the toolbar item color
self.toolbarItemTintColor = [UIColor greenColor];

// Set the toolbar selected color
self.toolbarItemSelectedTintColor = [UIColor redColor];
Show only specified buttons in the toolbar:
self.enabledToolbarItems = @[ZSSRichTextEditorToolbarBold, ZSSRichTextEditorToolbarH1, ZSSRichTextEditorToolbarParagraph];
Always show the toolbar even when the keyboard is hidden:
self.alwaysShowToolbar = YES;
Set A Placeholder
[self setPlaceholder:@""This is a placeholder that will show when there is no content(html)""];

Insert Link and Insert Image
If you want to manually insert a link or image where the cursor is, you can use the following methods:
Insert Image
- (void)insertImage:(NSString *)url alt:(NSString *)alt;
Insert Link
- (void)insertLink:(NSString *)url title:(NSString *)title;
Custom Pickers
You can implement your own pickers for images and links if you have an alternate method that you are wanting to use. E.g., uploading an image from your camera roll then inserting the URL.
When the alternate picker icon (crosshair) is tapped it will call the corresponding method, which you need to override in your ZSSRichTextEditor subclass (see example project):
- (void)showInsertURLAlternatePicker {
    
    [self dismissAlertView];
    
    // Show your custom picker
    
}


- (void)showInsertImageAlternatePicker {
    
    [self dismissAlertView];
    
    // Show your custom picker
    
}
Custom Toolbar Buttons
UIButton *myButton = [[UIButton alloc] initWithFrame:CGRectMake(0.0f, 0.0f, buttonWidth, 28.0f)];
[myButton setTitle:@""My Button"" forState:UIControlStateNormal];
[myButton addTarget:self
             action:@selector(didTapCustomToolbarButton:)
   forControlEvents:UIControlEventTouchUpInside];

[self addCustomToolbarItemWithButton:myButton];

Custom CSS
NSString *customCSS = @""a {text-decoration:none;} a:hover {color:#FF0000;}"";
[self setCSS:customCSS];

Receive Editor Did Change Events
Add the following to your viewDidLoad method:
self.receiveEditorDidChangeEvents = YES;
Then you will receive events in the following method:
- (void)editorDidChangeWithText:(NSString *)text andHTML:(NSString *)html {
    
    NSLog(@""Text Has Changed: %@"", text);
    
    NSLog(@""HTML Has Changed: %@"", html);
    
}
Receive Hashtag & Mention Events
Hashtags:
- (void)hashtagRecognizedWithWord:(NSString *)word {
    
    NSLog(@""Hashtag has been recognized: %@"", word);
    
}
Mentions:
- (void)mentionRecognizedWithWord:(NSString *)word {
    
    NSLog(@""Mention has been recognized: %@"", word);
    
}
Supported Functions
ZSSRichTextEditor has the following functions:

Bold
Italic
Subscript
Superscript
Strikethrough
Underline
Remove Formatting
Font
Justify Left
Justify Center
Justify Right
Justify Full
Paragraph
Heading 1
Heading 2
Heading 3
Heading 4
Heading 5
Heading 6
Undo
Redo
Unordered List
Ordered List
Indent
Outdent
Insert Image
Insert Link
Quick Link
Unlink
Horizontal Rule
View Source
Text Color
Background Color

Installation
You can use CocoaPods or manually using the following instructions:
ZSSRichTextEditor requires iOS7 as well as CoreGraphics.framework and CoreText.framework.

Copy the Source folder to your project.
Subclass ZSSRichTextEditor and implement the methods as mentioned above.

When using ZSSRichTextEditor in your own project, XCode will automatically add ZSSRichTextEditor.js to compile sources under build phases, this will cause ZSSRichTextEditor to not work correctly as the javascript file won't be included in your app. Instead, remove it from compile sources and add it to copy bundle resources.
Attribution
ZSSRichTextEditor uses portions of code from the following sources:



Component
Description
License




CYRTextView
CYRTextView is a UITextView subclass that implements a variety of features that are relevant to a syntax or code text view.
MIT


HRColorPicker
Simple color picker for iPhone
BSD


jQuery
jQuery is a fast, small, and feature-rich JavaScript library.
MIT


JS Beautifier
Makes ugly Javascript pretty
MIT



Contact
Visit us online at http://www.zedsaid.com or @zedsaid.
",3192
gaogaotiantian/ColorFightIIServer,Python,"This is the new version of ColorFight
The client should use WebSocket to communicate with the server.
Two WebSocket instances are used for the game.
Game Channel
The first one is on game_channel, which collects information from the
game. The client should keep this ws alive, this channel is read only, meaning
the client should wait for the server to publish data.
Data Format
Data is in json format.
TODO: detailed description.
Action Channel
The second one is on action_channel, which is used for clients to send
actions to the server.
Action Format
The client should send a string representing a json object.
{'action': action, **kwargs}
action is a string representing the kind of action. It could be

register

{'action': 'register', 'username': username, 'password': password}


command

{'action': 'command', 'cmd_list': cmd_list}



Command List
The client interacts with the server with command list. The command list should
be a list of commands in a single turn.
A command is a string with arguments separated by spaces
ex a 2 3 200
The first argument represents for the kind of command.

'a' for Attack

'a x y energy'
Attack (x, y) with energy



",2
UCLA-Creative-Labs/flux,JavaScript,"Setup
First install all dependencies by running:
yarn

Now you can run a local server with:
yarn start

",4
Julian/Filesystems,Python,"Filesystems

 
  
",2
agrande/lba2remake,JavaScript,"LBA2 Remake




A Little Big Adventure 2 / Twinsen's Odyssey reimplementation in JavaScript / Three.js / React
Live demo (or Editor mode)
Vision / goals
Phase 1: Reimplement the LBA2 game engine

Should look and play like the original
Full re-engineering instead of dissassembly-based approach
Focus on code quality, make it easy to read and modify
Milestones: Gameplay 1, Gameplay 2, Gameplay 3, Gameplay 4

Phase 2: Make a HD version of the game

Improve graphics and gameplay in a way that preserves the original look & feel
More of a remastered version than a remake
Support more platforms and ways of playing
Milestones: HD Version, Social gaming, Virtual Reality, Alternative versions

Phase 3: Become a platform for modding and action/adventure game development

Progressively add tools to the editor to support mods
Have all tools grouped in a single integrated platform
Milestones: Modding 1, Modding 2, Modding 3

Status
Currently you can walk around every island and buildings of the original game.
Most of the graphic elements are implemented and part of the gameplay. It is not however completable at this point.
Check out the gameplay milestones to see what needs to be done before the game becomes playable from start to end.

The editor mode allows navigating around the game scenes, inspecting scene content and variables, inspecting the game engine internal variables and debugging scripts (you can set breakpoints on actor scripts).
Progress on the editor is tracked by the modding milestones.

For more information, check the FAQ.
Contributing
Any help is very much appreciated!
Check the How to Contribute guide to know how to do that.
A list of current and past contributors can be found here.
Getting started
(more details here)

Make sure you own a copy of the original game: GOG or Steam
Copy the *.HQR, *.OBL, *.ILE files and the VOX folder from the original game install folder to www/data
(TODO) Import the musics and videos as mp4 (no script available for that at this time)
Download Node.js LTS (if you already have node.js installed, make sure it is a recent version: >= 8.x.x)
Run npm install
Run npm run dev
Fire up your browser at page http://localhost:8080
Enjoy!

Community

Slack - For development related discussions
Discord
MBN forum
Reddit: r/linux_gaming or r/gamedev
Facebook

License
While the original Little Big Adventure 2 game executable, assets and intellectual property belong to Didier Chanfray SARL, the code within this project is released under the MIT License.
That being said, if you do anything interesting with this code, please let us know, we'd love to see it!
",51
microsoft/SparseSC,Python,"Sparse Synthetic Controls
TL;DR:
The fit() function can be used to create a set of weights and returns a
fitted model which can be used to create synthetic units using it's
.predict() method:
from SparseSC import fit

# fit the model:
fitted_model = fit(X,Y,...)

# make for the in-sample data
in_sample_predictions = fitted_model.predict()

# make predictions for a held out set of fetures (Y_hat) within the
# original set of units
additional_predictions = fitted_model.predict(Y_additional)
Overview
When estimating synthetic controls, units of observation are divided into
control and treated units. Data collected on these units may include
observations of the outcome of interest, as well as other characteristics
of the units (termed ""covariates"", herein). Outcomes may be observed both
before and after an intervention on the treated units.
To maintain independence of the fitted synthetic controls and the
post-intervention outcomes of interest of treated units, the
post-intervention outcomes from treated units are not used in the fitting
process. There are two cuts from the remaining data that may be used to
fit synthetic controls, and each has it's advantages and disadvantages.
Fitting a synthetic control model
Data and Model Type
The parameters X and Y should be numeric matrices containing data on
the features and target variables, respectively, with one row per unit
of observation, and one column per feature or target variable.
There area 4 model types that can be fit using the fit() function which
can be selected by passing one of the following values to the model_type parameter:


""retrospective"": In this model, data are assumed to be collected
retrospectively, sometime after an intervention or event has taken
place in a subset of the subjects/units, typically with the intent of
estimating the effect of the intervention.
In this model, Y should contain target variables recorded after the
event of interest and X may contain a combination of target variables
recorded prior to the event of interest and other predictors /
covariates known prior to the event. In addition, the rows in X and
Y which contain units that were affected by the intervention
(""treated units"") should be indicated using the treated_units
parameter.


""prospective"": In a prospective analysis, a subset of units have been designated to
receive a treatment but the treatment has not yet occurred and the
designation of the treatment may be correlated with a (possibly unobserved)
feature of the treatment units. In this scenario, all data are
collected prior to the treatment intervention, and data on the outcome
of interested are divided in two, typically divided in two subsets
taken before and after a particular point in time.
In this model, Y should contain only target variables and X may
contain a combination of target variables and other predictors /
covariates. The parameters treated_units should be used to indicate
the units which will or will not receive treatment.


""prospective-restricted"": This is motivated by the same example as the
previous sample. It requires a larger set of treated units for similar
levels of precision, with the benefit of substantially faster running
time.


""full"": This model is motivated by the need for prospective failure
detection, and is not used in the context of a historical event or
treatment intervention.
like the prospective models, data on the outcome of interested are
divided in two, typically divided in two subsets taken before and after
a particular point in time, and Y should contain only target
variables and X may contain a combination of target variables and
other predictors / covariates. The parameter treated_units is unused.


More details on the above parameters can be found in file fit.md in the
root of this git repository.
Penalty Parameters
The fitted synthetic control weights depend on the penalties applied to the V and W
matrices (v_pen and w_pen, respectively), and the fit() function will
attempt to find an optimal pair of penalty parameters. Users can modify the selection
process or simply provide their own values for the penalty parameters, for
example to optimize these parameters on their own, with one of the
following methods:
1. Passing v_pen and w_pen as floats:
When single values are passed in the to the v_pen and w_pen, a fitted
synthetic control model is returned using the provided penalties.
2. Passing v_pen as a value and w_pen as a vector, or vice versa:
When either v_pen or w_pen are passed a vector of values, fit()
will iterate over the vector of values and return the model with an optimal
out of sample prediction error using cross validation. The choice of model
can be controlled with the choice parameter which has the options of
""min"" (default) which selects the model with the smallest out of sample
error, ""1se"" which implements the 'one standard-error' rule, or a
function which implements a custom selection rule.
Note that passing vectors to both v_pen and w_pen is assumed to be
inefficient and fit will raise an error. If you wish to evaluate over a N x N
grid of penalties, use:
from intertools import product
fitted_models = [ fit(..., v_pen=v, w_pen=w) for v,w in product(v_pen,w_pen)]
3. Modifying the default search
By default fit() picks an arbitrary value for w_pen and creates a grid
of values for v_pen over which to search, picks the optimal for v_pen
from the set of parameters, and then repeats the process alternating
between a fixed v_pen and array of values w_pen and vice versa until
stopping rule is reached.
The grid over which each penalty parameter is searched is determined by the
value of the other (fixed) penalty parameter. For example, for a given
value of w_pen there is a maximum value of v_pen which does not result
in a null model (i.e. when the V matrix would be identically 0 and W would
be identically 1/N), and the same logic applies in both scenarios (i.e.
when w_pen is fixed).
The search grid is therefor bounded between 0 and the maximum referenced
above. By default the grid consists of 20 points log-linearly spaced
between 0 and the maximum. The number of points in the grid can be
controlled with the grid_length parameters, and the bounds are controlled
via the grid_min and grid_max parameters. Alternatively, an array of
values between 0 and 1 can be passed to the grid parameter and will be
multiplied by the relevant grid_max to determine the search grid at each
iteration of the alternating coordinate descent.
Finally, the parameter stopping_rule determines how long the coordinate
descent will alternate between searching over a grid of V and W penalties.
(see the Big list of parameters for details)
Advanced Topics
Custom Donor Pools
By default all control units are allowed to be donors for all other units.
There are cases where this is not desired and so the user can pass in a
matrix specifying a unit-specific donor pool via a N x C matrix of booleans.
Constraining the V matrix
In the current implementation, the V matrix is a diagonal matrix, and the
individual elements of V are constrained to be positive, as negative values
would be interpreted as two units would considered to more similar when
their observed values for a particular feature are more different.
Additionally, the V matrix may be constrained to the standard simplex.
which tends to minimize out of sample of error relative to the model
constrained to the nonnegative
orthant in some cases. V is
constrained to the either the simplex or the nonnegative orthant by passing
either ""simplex"" or ""orthant"" to the constrain parameter.
Fold Parameters
The data are split into folds both purpose of calculating the cross fold
validation (out-of-sample) errors and for K-fold gradient descent, a
technique used to speed up the model fitting process. The parameters
cv_fold and gradient_fold can be passed either an integer number of
folds or an list-of-lists which indicate the units (rows) which are
allocated to each fold.
In the case that an integer is passed, the scikit-learn function
kfold
is used internally to split the data into random folds. For consistency
across calls to fit, the cv_seed and gradient_seed parameters are
passed to Kfold(..., random_state=seed).
Parallelization
If you have the BLAS/LAPACK libraries installed and available to Python,
you should not need to do any further optimization to ensure that maximum
number of processors are used during the execution of fit().  If
not, seting the parameter paralell=True when you call
fit() which will split the work across N - 2 sub-processes where N
is the number of cores in your
machine.
Note that setting paralell=True when the BLAS/LAPACK are available will
tend to increase running times. Also, this is considered an experimenatl
stub. While it works, parallel processing spends most of the time passing
repeatedly sending a relatively small amount of data, which could be (but
currently is not) initialized in each worker at the start. If this a
priority for your team, feel free to submit a PR or feature request.
Gradient Descent in feature space
Currently a custom gradient descent method called cdl_search (imported
from SparseSC.optimizers.cd_line_search import. ) is used which which
performs the constrained gradient descent. An alternate gradient descent
function may be supplied to the method parameter, and any additional
keyword arguments passed to fit() are passed along to whichever gradient
descent function is used. (see the Big list of
parameters for details)
Big list of parameters


X (Matrix of flaots): Matrix of features variables with one row per unit of observation and
one column per covariate / feature.


Y (Matrix of flaots): Matrix of targets variables with one row per unit of observation and
one column per target variable.


model_type (string, default = ""retrospective""): Type of model
being fit. One of ""retrospective"", ""prospective"",
""prospective-restricted"" or ""full"" See above
for details.


treated_units (int[]|boolean[]): A list of integers or array of
booleans indicating the rows of X and Y which contain data from
treated units.


w_pen (float | float[], optional): Penalty / penalties applied to the
difference between the fitted weights (W) and the null weights (1/n),
See above for details.


v_pen (float | float[], optional): Penalty / penalties applied to the
difference between the fitted weights (W) and the null weights (1/n).
See above for details.


grid: (float[], optional). See above for details.


grid_min (float, default = 1e-6): Lower bound for grid when
grid are not provided. Must be in the range (0,1)


grid_max (float, default = 1): Upper bound for grid when
v_pen and grid are not provided. Must be in the range (0,1]


grid_length (int, default = 20): number of points in the grid
parameter when v_pen and grid are not provided


stopping_rule (int|float|function, optional): A stopping rule less
than one is interpreted as the percent improvement in the out-of-sample
squared prediction error required between the current and previous
iteration in order to continue with the coordinate descent. A stopping
rule of one or greater is interpreted as the number of iterations of
the coordinate descent (rounded down to the nearest Int).
Alternatively, stopping_rule may be a function which will be passed
the current model fit, the previous model fit, and the iteration number
(depending on it's signature), and should return a truthy value if the
coordinate descent should stop and a falsey value if the coordinate
descent should continue.


choice (string|function, default =""min""): Method for selecting the
optimal penalty parameter from an array of penalty parameters, from the
out-of-sample error estimates and standard errors of the estimates.
When either v_pen or w_pen are passed a vector of values, fit()
will iterate over the vector of values and return the model with an
optimal out of sample prediction error using cross validation. The
choice of model can be controlled with the choice parameter which has
the options of ""min"" (default) which selects the model with the
smallest out of sample error, ""1se"" which implements the 'one
standard-error' rule, or a function which implements a custom
selection rule


cv_folds (int[]|int[][], default = 10): An integer number of Cross
Validation folds passed to sklearn.model_selection.KFold, or an
explicit list of train validation folds


gradient_folds (int[]|int[][], default = 10): An integer
number of Gradient folds passed to sklearn.model_selection.KFold, or
an explicit list of train validation folds. Not used when model_type
is ""prospective-restricted""


cv_seed (int, default = 10101): passed to sklearn.model_selection.KFold
to allow for consistent cross validation folds across calls to fit()


gradient_seed (int, default = 110011): passed to sklearn.model_selection.KFold
to allow for consistent gradient folds across calls to fit()


progress (boolean, default = True): Controls the level of
verbosity. If True, the messages indication the progress are printed
to the console at each iteration of the gradient descent in the feature
space (stdout).


verbose (boolean, default = False): Controls the level of
verbosity. If True, the messages indication the progress are printed
to the console at each calculation of the partial gradient (stdout).
partial gradients are  calculated h * c times in the leave-one-out
gradient descent, and h * k times in the k-fold gradient descent,
where h is the number of cross-validation folds , c is the number
of controls, and k is the number of gradient folds. In short, this is
level of messaging is typically excessive.


custom_donor_pool (boolean matrix, default = None): By default all
control units are allowed to be donors for all units. There are cases
where this is not desired and so the user can pass in a matrix
specifying a unit-specific donor pool (NxC matrix of booleans).
Common reasons for restricting the allowability: (a) When we would like
to reduce interpolation bias by restricting the donor pool to those
units similar along certain features. (b) If units are not completely
independent (for example there may be contamination between neighboring
units). This is a violation of the Single Unit Treatment Value
Assumption (SUTVA). Note: These are not used in the fitting stage (of
V and penalties) just in final unit weight determination.


parallel (boolean, default=false): split the gradient descent
across multiple sub-processes.  This is currently an experimental stub
and tends to increase running time. See notes above.


method (string|function, default=SparseSC.optimizers.cd_line_search.cdl_search):
The method or function responsible for performing gradient descent in
the feature space. If method is a string, it is passed as the
method argument to scipy.optimize.minimize. Otherwise, method
must be a function with a signature compatible with
scipy.optimize.minimize (method(fun,x0,grad,**kwargs)) which
returns an object having x and fun attributes.


kwargs: Additional arguments passed to the optimizer (i.e. method or scipy.optimize.minimize). Additional arguments for the
default optimizer include:


constrain (string): The value ""orthant"" constrains V
to the non-negative orthant, and ""simplex"" constrains V to the
standard simplex.


learning_rate (float, default = 0.2): The initial learning rate
which determines the initial step size, which is set to learning_rate * null_model_error / gradient. Must be between 0 and 1.


learning_rate_adjustment (float, default = 0.9): Adjustment factor
applied to the learning rate applied between iterations when the
optimal step size returned by scipy.optimize.line_search is greater
less than 1, else the step size is adjusted by
1/learning_rate_adjustment. Must be between 0 and 1,


tol (float, default = 0.0001): Tolerance used for the stopping rule
based on the proportion of the in-sample residual error reduced in the
last step of the gradient descent.




The Model Object:
fit() returns an object of type SparseSCFit which contains the details
of the fitted model.
Attributes:
Input Parameters:

X: A reference to the input paremeter X
Y: A reference to the input paremeter Y
control_units: A reference to the input paremeter control_units
treated_units: A reference to the input paremeter treated_units
model_type: A reference to the input paremeter model_type
initial_w_pen: A reference to the input paremeter w_pen
initial_v_pen: A reference to the input paremeter v_pen

Fitted values:

fitted_w_pen: The selected w_pen value.
fitted_v_pen: The selected v_pen value.
V: The fitted matrix of feature weights.
sc_weights: The fitted synthetic control weights matrix W
score: Squared out-of-sample error from cross validation of the
for the model associated with the selected penalty parameters.
trivial_units: An array of booleans indicating which (if any) units
have zeros for all targets (outcomes) and all non-trivial features
(features with a non-zero weight in the fitted V matrix). These are
important as the penalties will tend to set their weights to 1/N for
all synthetics units for which they may be included. (This is
anticipated to be very rare in real life datasets)

Methods:


model.get_weights (include_trivial_donors=False): Returns the synthetic
control weights, optionally setting the contributions of trivial units
to the predicted values of non-trivial units to zero.


model.predict (Y=None,include_trivial_donors=False): Returns matrix
of synthetic units, optionally applying the synthetic control weights
to a new set of features Y (e.g. for prospective use-cases).


model.str(): Brief summary of model fit.


model.summary(): Provides a summary of the coordinate descent steps
in the search for an optimal pair of penalty parameters.  Return a list
with one pandas DataFrame (if installed) per direction of the
coordinate descent.


Developer Notes
Performance Notes
The function get_max_lambda() requires a single calculation of the
gradient using all of the available data.  In contrast,  SC.CV_score()
performs gradient descent within each validation-fold of the data.
Furthermore, in the 'pre-only' scenario the gradient is calculated once for
each iteration of the gradient descent, whereas in the 'controls-only'
scenario the gradient is calculated once for each control unit.
Specifically, each control unit is excluded from the set of units that can
be used to predict it's own post-intervention outcomes, resulting in
leave-one-out gradient descent.
For large sample sizes in the 'controls-only' scenario, it may be
sufficient to divide the non-held out control units into ""gradient folds"", such
that controls within the same gradient-fold are not used to predict the
post-intervention outcomes of other control units in the same fold.  This
result's in K-fold gradient descent, which improves the speed of
calculating the overall gradient by a factor slightly greater than c/k
(where c is the number of control units) with an even greater reduction
in memory usage.
K-fold gradient descent is enabled by passing the parameter grad_splits
to CV_score(), and for consistency across calls to CV_score() it is
recommended to also pass a value to the parameter random_state, which is
used in selecting the gradient folds.
Additional Performance Considerations
If you have the BLAS/LAPACK libraries installed and available to Python,
you should not need to do any further optimization to ensure that maximum
number of processors are used during the execution of CV_score().  If
not, you may wish to set the parameter parallel=True when you call
CV_score() which will split the work across N - 2 sub-processes where N
is the number of cores in your
machine.
(Note that setting parallel=True when the BLAS/LAPACK are available will
tend to increase running times.)
Documentation
You can read these online at Read the
Docs.
To build the
documentation locally, you will need sphinx, recommonmark, and
sphinx-markdown-tables (to incorporate .md files)
The documentation can be built locally using the (n)make target
htmldocs and is generated in docs/build/html/index.html.
Contributing
This project welcomes contributions and suggestions.  Most contributions
require you to agree to a Contributor License Agreement (CLA) declaring
that you have the right to, and actually do, grant us the rights to use
your contribution. For details, visit https://cla.microsoft.com.
When you submit a pull request, a CLA-bot will automatically determine
whether you need to provide a CLA and decorate the PR appropriately (e.g.,
label, comment). Simply follow the instructions provided by the bot. You
will only need to do this once across all repos using our CLA.
This project has adopted the Microsoft Open Source Code of
Conduct.  For more
information see the Code of Conduct
FAQ or contact
opencode@microsoft.com with any additional
questions or comments.
",7
Connormiha/jest-css-modules-transform,JavaScript,"
jest-css-modules-transform
Preprocessor css modules for Jest test framework
This preprocessor converts css files in modules like Webpack.
If we have css files
.class1, .class2, .class3 {
    display: block;
}
Webpack will transfrom it to object.
{
    class1: 'class1', //value may be different. It depends of localIndentName property
    class2: 'class2',
    class3: 'class3',
}
In testing you need to mock all css modules to aviod exception. But use pure object {} for mocking is bad idea, because it makes wrong classNames in components. This preprocessor makes correct modules as if Webpack did.
Usage
In Jest config add option
""transform"": {
  "".+\\.(css|styl|less|sass|scss)$"": ""<rootDir>/node_modules/jest-css-modules-transform""
},
It supports pure CSS, SCSS, SASS, STYLUS and LESS.
Options
You can save preproccessor options in file jest-css-modules-transform-config.js in root of your project(Where is the file package.json).
You can pass options for your preprocessors.
example:
const path = require('path');  
const additionalResolvePath = path.resolve(__dirname, 'src', 'additional_modules');

module.exports = {
    sassConfig: {
        includePaths: [additionalResolvePath],
        precision: 5,
    },
    lessConfig: {
        paths: [additionalResolvePath],
    },
    stylusConfig: {
        paths: [additionalResolvePath],
    },
    postcssConfig: {
        plugins: [
            require('autoprefixer')({
                browsers: ['Chrome 68', 'Firefox 62', 'Safari 12']
            })
        ]
    },
};
For all preprocessor options see offical documentations for Sass, Less, Stylus.
Install
npm i jest-css-modules-transform

",16
Alluxio/alluxio,Java,"






What is Alluxio
Alluxio (formerly known as Tachyon)
is a virtual distributed storage system. It bridges the gap between
computation frameworks and storage systems, enabling computation applications to connect to
numerous storage systems through a common interface. Read more about
Alluxio Overview.
The Alluxio project originated from a research project called Tachyon at AMPLab, UC Berkeley,
which was the data layer of the Berkeley Data Analytics Stack (BDAS).
For more details, please refer to Haoyuan Li's PhD dissertation
Alluxio: A Virtual Distributed File System.
Who Uses Alluxio
Alluxio is used in production to manage Petabytes of data in many leading companies, with
the largest deployment exceeding 1300 nodes. Find more use cases at
Powered by Alluxio.
Community and Events
Please use the following to reach members of the community:

Slack: alluxio-community channel
Community Events: upcoming online office hours, meetups and webinars
Mailing List: alluxio-users
Meetup Groups: Bay Area Meetup,
New York Meetup,
Beijing Alluxio Meetup
Twitter: @alluxio

Download Alluxio
Binary download
Prebuilt binaries are available to download at https://www.alluxio.io/download .
Docker
Download and start an Alluxio master and a worker. More details can be found in documentation.
# launch a master
$ docker run -d --net=host\
    -v /mnt/data:/opt/alluxio/underFSStorage\
    alluxio/alluxio master
# launch a worker
$ docker run -d --net=host --shm-size=1G\
    -e ALLUXIO_WORKER_MEMORY_SIZE=1G\
    -v /mnt/data:/opt/alluxio/underFSStorage\
    -e ALLUXIO_MASTER_HOSTNAME=localhost\
    alluxio/alluxio worker
MacOS Homebrew
$ brew install alluxio
Quick Start
Please follow the Guide to Get Started
to run a simple example with Alluxio.
Report a Bug
To report bugs, suggest improvements, or create new feature requests, please open a Github Issue. Our previous Alluxio JIRA system has been deprecated since December 2018.
Depend on Alluxio
For Alluxio versions 1.4 or earlier, use the alluxio-core-client artifact.
For Alluxio versions 1.5 or later, Alluxio provides several different client artifacts. The Alluxio
file system interface provided by the alluxio-core-client-fs artifact is recommended for the best
performance and access to Alluxio-specific functionality. If you want to use other interfaces,
include the appropriate client artifact. For example, alluxio-core-client-hdfs provides a client
implementing HDFS's file system API.
Apache Maven
<dependency>
  <groupId>org.alluxio</groupId>
  <artifactId>alluxio-core-client-fs</artifactId>
  <version>1.8.1</version>
</dependency>
SBT
libraryDependencies += ""org.alluxio"" % ""alluxio-core-client-fs"" % ""1.8.1""

Contributing
Contributions via GitHub pull requests are gladly accepted from their original author. Along with
any pull requests, please state that the contribution is your original work and that you license the
work to the project under the project's open source license. Whether or not you state this
explicitly, by submitting any copyrighted material via pull request, email, or other means you agree
to license the material under the project's open source license and warrant that you have the legal
authority to do so.
For a more detailed step-by-step guide, please read
how to contribute to Alluxio.
For new contributor, please take 2 new contributor tasks.
Useful Links

Alluxio Website
Downloads
Releases and Notes
Documentation

",4120
microsoft/SparseSC,Python,"Sparse Synthetic Controls
TL;DR:
The fit() function can be used to create a set of weights and returns a
fitted model which can be used to create synthetic units using it's
.predict() method:
from SparseSC import fit

# fit the model:
fitted_model = fit(X,Y,...)

# make for the in-sample data
in_sample_predictions = fitted_model.predict()

# make predictions for a held out set of fetures (Y_hat) within the
# original set of units
additional_predictions = fitted_model.predict(Y_additional)
Overview
When estimating synthetic controls, units of observation are divided into
control and treated units. Data collected on these units may include
observations of the outcome of interest, as well as other characteristics
of the units (termed ""covariates"", herein). Outcomes may be observed both
before and after an intervention on the treated units.
To maintain independence of the fitted synthetic controls and the
post-intervention outcomes of interest of treated units, the
post-intervention outcomes from treated units are not used in the fitting
process. There are two cuts from the remaining data that may be used to
fit synthetic controls, and each has it's advantages and disadvantages.
Fitting a synthetic control model
Data and Model Type
The parameters X and Y should be numeric matrices containing data on
the features and target variables, respectively, with one row per unit
of observation, and one column per feature or target variable.
There area 4 model types that can be fit using the fit() function which
can be selected by passing one of the following values to the model_type parameter:


""retrospective"": In this model, data are assumed to be collected
retrospectively, sometime after an intervention or event has taken
place in a subset of the subjects/units, typically with the intent of
estimating the effect of the intervention.
In this model, Y should contain target variables recorded after the
event of interest and X may contain a combination of target variables
recorded prior to the event of interest and other predictors /
covariates known prior to the event. In addition, the rows in X and
Y which contain units that were affected by the intervention
(""treated units"") should be indicated using the treated_units
parameter.


""prospective"": In a prospective analysis, a subset of units have been designated to
receive a treatment but the treatment has not yet occurred and the
designation of the treatment may be correlated with a (possibly unobserved)
feature of the treatment units. In this scenario, all data are
collected prior to the treatment intervention, and data on the outcome
of interested are divided in two, typically divided in two subsets
taken before and after a particular point in time.
In this model, Y should contain only target variables and X may
contain a combination of target variables and other predictors /
covariates. The parameters treated_units should be used to indicate
the units which will or will not receive treatment.


""prospective-restricted"": This is motivated by the same example as the
previous sample. It requires a larger set of treated units for similar
levels of precision, with the benefit of substantially faster running
time.


""full"": This model is motivated by the need for prospective failure
detection, and is not used in the context of a historical event or
treatment intervention.
like the prospective models, data on the outcome of interested are
divided in two, typically divided in two subsets taken before and after
a particular point in time, and Y should contain only target
variables and X may contain a combination of target variables and
other predictors / covariates. The parameter treated_units is unused.


More details on the above parameters can be found in file fit.md in the
root of this git repository.
Penalty Parameters
The fitted synthetic control weights depend on the penalties applied to the V and W
matrices (v_pen and w_pen, respectively), and the fit() function will
attempt to find an optimal pair of penalty parameters. Users can modify the selection
process or simply provide their own values for the penalty parameters, for
example to optimize these parameters on their own, with one of the
following methods:
1. Passing v_pen and w_pen as floats:
When single values are passed in the to the v_pen and w_pen, a fitted
synthetic control model is returned using the provided penalties.
2. Passing v_pen as a value and w_pen as a vector, or vice versa:
When either v_pen or w_pen are passed a vector of values, fit()
will iterate over the vector of values and return the model with an optimal
out of sample prediction error using cross validation. The choice of model
can be controlled with the choice parameter which has the options of
""min"" (default) which selects the model with the smallest out of sample
error, ""1se"" which implements the 'one standard-error' rule, or a
function which implements a custom selection rule.
Note that passing vectors to both v_pen and w_pen is assumed to be
inefficient and fit will raise an error. If you wish to evaluate over a N x N
grid of penalties, use:
from intertools import product
fitted_models = [ fit(..., v_pen=v, w_pen=w) for v,w in product(v_pen,w_pen)]
3. Modifying the default search
By default fit() picks an arbitrary value for w_pen and creates a grid
of values for v_pen over which to search, picks the optimal for v_pen
from the set of parameters, and then repeats the process alternating
between a fixed v_pen and array of values w_pen and vice versa until
stopping rule is reached.
The grid over which each penalty parameter is searched is determined by the
value of the other (fixed) penalty parameter. For example, for a given
value of w_pen there is a maximum value of v_pen which does not result
in a null model (i.e. when the V matrix would be identically 0 and W would
be identically 1/N), and the same logic applies in both scenarios (i.e.
when w_pen is fixed).
The search grid is therefor bounded between 0 and the maximum referenced
above. By default the grid consists of 20 points log-linearly spaced
between 0 and the maximum. The number of points in the grid can be
controlled with the grid_length parameters, and the bounds are controlled
via the grid_min and grid_max parameters. Alternatively, an array of
values between 0 and 1 can be passed to the grid parameter and will be
multiplied by the relevant grid_max to determine the search grid at each
iteration of the alternating coordinate descent.
Finally, the parameter stopping_rule determines how long the coordinate
descent will alternate between searching over a grid of V and W penalties.
(see the Big list of parameters for details)
Advanced Topics
Custom Donor Pools
By default all control units are allowed to be donors for all other units.
There are cases where this is not desired and so the user can pass in a
matrix specifying a unit-specific donor pool via a N x C matrix of booleans.
Constraining the V matrix
In the current implementation, the V matrix is a diagonal matrix, and the
individual elements of V are constrained to be positive, as negative values
would be interpreted as two units would considered to more similar when
their observed values for a particular feature are more different.
Additionally, the V matrix may be constrained to the standard simplex.
which tends to minimize out of sample of error relative to the model
constrained to the nonnegative
orthant in some cases. V is
constrained to the either the simplex or the nonnegative orthant by passing
either ""simplex"" or ""orthant"" to the constrain parameter.
Fold Parameters
The data are split into folds both purpose of calculating the cross fold
validation (out-of-sample) errors and for K-fold gradient descent, a
technique used to speed up the model fitting process. The parameters
cv_fold and gradient_fold can be passed either an integer number of
folds or an list-of-lists which indicate the units (rows) which are
allocated to each fold.
In the case that an integer is passed, the scikit-learn function
kfold
is used internally to split the data into random folds. For consistency
across calls to fit, the cv_seed and gradient_seed parameters are
passed to Kfold(..., random_state=seed).
Parallelization
If you have the BLAS/LAPACK libraries installed and available to Python,
you should not need to do any further optimization to ensure that maximum
number of processors are used during the execution of fit().  If
not, seting the parameter paralell=True when you call
fit() which will split the work across N - 2 sub-processes where N
is the number of cores in your
machine.
Note that setting paralell=True when the BLAS/LAPACK are available will
tend to increase running times. Also, this is considered an experimenatl
stub. While it works, parallel processing spends most of the time passing
repeatedly sending a relatively small amount of data, which could be (but
currently is not) initialized in each worker at the start. If this a
priority for your team, feel free to submit a PR or feature request.
Gradient Descent in feature space
Currently a custom gradient descent method called cdl_search (imported
from SparseSC.optimizers.cd_line_search import. ) is used which which
performs the constrained gradient descent. An alternate gradient descent
function may be supplied to the method parameter, and any additional
keyword arguments passed to fit() are passed along to whichever gradient
descent function is used. (see the Big list of
parameters for details)
Big list of parameters


X (Matrix of flaots): Matrix of features variables with one row per unit of observation and
one column per covariate / feature.


Y (Matrix of flaots): Matrix of targets variables with one row per unit of observation and
one column per target variable.


model_type (string, default = ""retrospective""): Type of model
being fit. One of ""retrospective"", ""prospective"",
""prospective-restricted"" or ""full"" See above
for details.


treated_units (int[]|boolean[]): A list of integers or array of
booleans indicating the rows of X and Y which contain data from
treated units.


w_pen (float | float[], optional): Penalty / penalties applied to the
difference between the fitted weights (W) and the null weights (1/n),
See above for details.


v_pen (float | float[], optional): Penalty / penalties applied to the
difference between the fitted weights (W) and the null weights (1/n).
See above for details.


grid: (float[], optional). See above for details.


grid_min (float, default = 1e-6): Lower bound for grid when
grid are not provided. Must be in the range (0,1)


grid_max (float, default = 1): Upper bound for grid when
v_pen and grid are not provided. Must be in the range (0,1]


grid_length (int, default = 20): number of points in the grid
parameter when v_pen and grid are not provided


stopping_rule (int|float|function, optional): A stopping rule less
than one is interpreted as the percent improvement in the out-of-sample
squared prediction error required between the current and previous
iteration in order to continue with the coordinate descent. A stopping
rule of one or greater is interpreted as the number of iterations of
the coordinate descent (rounded down to the nearest Int).
Alternatively, stopping_rule may be a function which will be passed
the current model fit, the previous model fit, and the iteration number
(depending on it's signature), and should return a truthy value if the
coordinate descent should stop and a falsey value if the coordinate
descent should continue.


choice (string|function, default =""min""): Method for selecting the
optimal penalty parameter from an array of penalty parameters, from the
out-of-sample error estimates and standard errors of the estimates.
When either v_pen or w_pen are passed a vector of values, fit()
will iterate over the vector of values and return the model with an
optimal out of sample prediction error using cross validation. The
choice of model can be controlled with the choice parameter which has
the options of ""min"" (default) which selects the model with the
smallest out of sample error, ""1se"" which implements the 'one
standard-error' rule, or a function which implements a custom
selection rule


cv_folds (int[]|int[][], default = 10): An integer number of Cross
Validation folds passed to sklearn.model_selection.KFold, or an
explicit list of train validation folds


gradient_folds (int[]|int[][], default = 10): An integer
number of Gradient folds passed to sklearn.model_selection.KFold, or
an explicit list of train validation folds. Not used when model_type
is ""prospective-restricted""


cv_seed (int, default = 10101): passed to sklearn.model_selection.KFold
to allow for consistent cross validation folds across calls to fit()


gradient_seed (int, default = 110011): passed to sklearn.model_selection.KFold
to allow for consistent gradient folds across calls to fit()


progress (boolean, default = True): Controls the level of
verbosity. If True, the messages indication the progress are printed
to the console at each iteration of the gradient descent in the feature
space (stdout).


verbose (boolean, default = False): Controls the level of
verbosity. If True, the messages indication the progress are printed
to the console at each calculation of the partial gradient (stdout).
partial gradients are  calculated h * c times in the leave-one-out
gradient descent, and h * k times in the k-fold gradient descent,
where h is the number of cross-validation folds , c is the number
of controls, and k is the number of gradient folds. In short, this is
level of messaging is typically excessive.


custom_donor_pool (boolean matrix, default = None): By default all
control units are allowed to be donors for all units. There are cases
where this is not desired and so the user can pass in a matrix
specifying a unit-specific donor pool (NxC matrix of booleans).
Common reasons for restricting the allowability: (a) When we would like
to reduce interpolation bias by restricting the donor pool to those
units similar along certain features. (b) If units are not completely
independent (for example there may be contamination between neighboring
units). This is a violation of the Single Unit Treatment Value
Assumption (SUTVA). Note: These are not used in the fitting stage (of
V and penalties) just in final unit weight determination.


parallel (boolean, default=false): split the gradient descent
across multiple sub-processes.  This is currently an experimental stub
and tends to increase running time. See notes above.


method (string|function, default=SparseSC.optimizers.cd_line_search.cdl_search):
The method or function responsible for performing gradient descent in
the feature space. If method is a string, it is passed as the
method argument to scipy.optimize.minimize. Otherwise, method
must be a function with a signature compatible with
scipy.optimize.minimize (method(fun,x0,grad,**kwargs)) which
returns an object having x and fun attributes.


kwargs: Additional arguments passed to the optimizer (i.e. method or scipy.optimize.minimize). Additional arguments for the
default optimizer include:


constrain (string): The value ""orthant"" constrains V
to the non-negative orthant, and ""simplex"" constrains V to the
standard simplex.


learning_rate (float, default = 0.2): The initial learning rate
which determines the initial step size, which is set to learning_rate * null_model_error / gradient. Must be between 0 and 1.


learning_rate_adjustment (float, default = 0.9): Adjustment factor
applied to the learning rate applied between iterations when the
optimal step size returned by scipy.optimize.line_search is greater
less than 1, else the step size is adjusted by
1/learning_rate_adjustment. Must be between 0 and 1,


tol (float, default = 0.0001): Tolerance used for the stopping rule
based on the proportion of the in-sample residual error reduced in the
last step of the gradient descent.




The Model Object:
fit() returns an object of type SparseSCFit which contains the details
of the fitted model.
Attributes:
Input Parameters:

X: A reference to the input paremeter X
Y: A reference to the input paremeter Y
control_units: A reference to the input paremeter control_units
treated_units: A reference to the input paremeter treated_units
model_type: A reference to the input paremeter model_type
initial_w_pen: A reference to the input paremeter w_pen
initial_v_pen: A reference to the input paremeter v_pen

Fitted values:

fitted_w_pen: The selected w_pen value.
fitted_v_pen: The selected v_pen value.
V: The fitted matrix of feature weights.
sc_weights: The fitted synthetic control weights matrix W
score: Squared out-of-sample error from cross validation of the
for the model associated with the selected penalty parameters.
trivial_units: An array of booleans indicating which (if any) units
have zeros for all targets (outcomes) and all non-trivial features
(features with a non-zero weight in the fitted V matrix). These are
important as the penalties will tend to set their weights to 1/N for
all synthetics units for which they may be included. (This is
anticipated to be very rare in real life datasets)

Methods:


model.get_weights (include_trivial_donors=False): Returns the synthetic
control weights, optionally setting the contributions of trivial units
to the predicted values of non-trivial units to zero.


model.predict (Y=None,include_trivial_donors=False): Returns matrix
of synthetic units, optionally applying the synthetic control weights
to a new set of features Y (e.g. for prospective use-cases).


model.str(): Brief summary of model fit.


model.summary(): Provides a summary of the coordinate descent steps
in the search for an optimal pair of penalty parameters.  Return a list
with one pandas DataFrame (if installed) per direction of the
coordinate descent.


Developer Notes
Performance Notes
The function get_max_lambda() requires a single calculation of the
gradient using all of the available data.  In contrast,  SC.CV_score()
performs gradient descent within each validation-fold of the data.
Furthermore, in the 'pre-only' scenario the gradient is calculated once for
each iteration of the gradient descent, whereas in the 'controls-only'
scenario the gradient is calculated once for each control unit.
Specifically, each control unit is excluded from the set of units that can
be used to predict it's own post-intervention outcomes, resulting in
leave-one-out gradient descent.
For large sample sizes in the 'controls-only' scenario, it may be
sufficient to divide the non-held out control units into ""gradient folds"", such
that controls within the same gradient-fold are not used to predict the
post-intervention outcomes of other control units in the same fold.  This
result's in K-fold gradient descent, which improves the speed of
calculating the overall gradient by a factor slightly greater than c/k
(where c is the number of control units) with an even greater reduction
in memory usage.
K-fold gradient descent is enabled by passing the parameter grad_splits
to CV_score(), and for consistency across calls to CV_score() it is
recommended to also pass a value to the parameter random_state, which is
used in selecting the gradient folds.
Additional Performance Considerations
If you have the BLAS/LAPACK libraries installed and available to Python,
you should not need to do any further optimization to ensure that maximum
number of processors are used during the execution of CV_score().  If
not, you may wish to set the parameter parallel=True when you call
CV_score() which will split the work across N - 2 sub-processes where N
is the number of cores in your
machine.
(Note that setting parallel=True when the BLAS/LAPACK are available will
tend to increase running times.)
Documentation
You can read these online at Read the
Docs.
To build the
documentation locally, you will need sphinx, recommonmark, and
sphinx-markdown-tables (to incorporate .md files)
The documentation can be built locally using the (n)make target
htmldocs and is generated in docs/build/html/index.html.
Contributing
This project welcomes contributions and suggestions.  Most contributions
require you to agree to a Contributor License Agreement (CLA) declaring
that you have the right to, and actually do, grant us the rights to use
your contribution. For details, visit https://cla.microsoft.com.
When you submit a pull request, a CLA-bot will automatically determine
whether you need to provide a CLA and decorate the PR appropriately (e.g.,
label, comment). Simply follow the instructions provided by the bot. You
will only need to do this once across all repos using our CLA.
This project has adopted the Microsoft Open Source Code of
Conduct.  For more
information see the Code of Conduct
FAQ or contact
opencode@microsoft.com with any additional
questions or comments.
",7
Connormiha/jest-css-modules-transform,JavaScript,"
jest-css-modules-transform
Preprocessor css modules for Jest test framework
This preprocessor converts css files in modules like Webpack.
If we have css files
.class1, .class2, .class3 {
    display: block;
}
Webpack will transfrom it to object.
{
    class1: 'class1', //value may be different. It depends of localIndentName property
    class2: 'class2',
    class3: 'class3',
}
In testing you need to mock all css modules to aviod exception. But use pure object {} for mocking is bad idea, because it makes wrong classNames in components. This preprocessor makes correct modules as if Webpack did.
Usage
In Jest config add option
""transform"": {
  "".+\\.(css|styl|less|sass|scss)$"": ""<rootDir>/node_modules/jest-css-modules-transform""
},
It supports pure CSS, SCSS, SASS, STYLUS and LESS.
Options
You can save preproccessor options in file jest-css-modules-transform-config.js in root of your project(Where is the file package.json).
You can pass options for your preprocessors.
example:
const path = require('path');  
const additionalResolvePath = path.resolve(__dirname, 'src', 'additional_modules');

module.exports = {
    sassConfig: {
        includePaths: [additionalResolvePath],
        precision: 5,
    },
    lessConfig: {
        paths: [additionalResolvePath],
    },
    stylusConfig: {
        paths: [additionalResolvePath],
    },
    postcssConfig: {
        plugins: [
            require('autoprefixer')({
                browsers: ['Chrome 68', 'Firefox 62', 'Safari 12']
            })
        ]
    },
};
For all preprocessor options see offical documentations for Sass, Less, Stylus.
Install
npm i jest-css-modules-transform

",16
Alluxio/alluxio,Java,"






What is Alluxio
Alluxio (formerly known as Tachyon)
is a virtual distributed storage system. It bridges the gap between
computation frameworks and storage systems, enabling computation applications to connect to
numerous storage systems through a common interface. Read more about
Alluxio Overview.
The Alluxio project originated from a research project called Tachyon at AMPLab, UC Berkeley,
which was the data layer of the Berkeley Data Analytics Stack (BDAS).
For more details, please refer to Haoyuan Li's PhD dissertation
Alluxio: A Virtual Distributed File System.
Who Uses Alluxio
Alluxio is used in production to manage Petabytes of data in many leading companies, with
the largest deployment exceeding 1300 nodes. Find more use cases at
Powered by Alluxio.
Community and Events
Please use the following to reach members of the community:

Slack: alluxio-community channel
Community Events: upcoming online office hours, meetups and webinars
Mailing List: alluxio-users
Meetup Groups: Bay Area Meetup,
New York Meetup,
Beijing Alluxio Meetup
Twitter: @alluxio

Download Alluxio
Binary download
Prebuilt binaries are available to download at https://www.alluxio.io/download .
Docker
Download and start an Alluxio master and a worker. More details can be found in documentation.
# launch a master
$ docker run -d --net=host\
    -v /mnt/data:/opt/alluxio/underFSStorage\
    alluxio/alluxio master
# launch a worker
$ docker run -d --net=host --shm-size=1G\
    -e ALLUXIO_WORKER_MEMORY_SIZE=1G\
    -v /mnt/data:/opt/alluxio/underFSStorage\
    -e ALLUXIO_MASTER_HOSTNAME=localhost\
    alluxio/alluxio worker
MacOS Homebrew
$ brew install alluxio
Quick Start
Please follow the Guide to Get Started
to run a simple example with Alluxio.
Report a Bug
To report bugs, suggest improvements, or create new feature requests, please open a Github Issue. Our previous Alluxio JIRA system has been deprecated since December 2018.
Depend on Alluxio
For Alluxio versions 1.4 or earlier, use the alluxio-core-client artifact.
For Alluxio versions 1.5 or later, Alluxio provides several different client artifacts. The Alluxio
file system interface provided by the alluxio-core-client-fs artifact is recommended for the best
performance and access to Alluxio-specific functionality. If you want to use other interfaces,
include the appropriate client artifact. For example, alluxio-core-client-hdfs provides a client
implementing HDFS's file system API.
Apache Maven
<dependency>
  <groupId>org.alluxio</groupId>
  <artifactId>alluxio-core-client-fs</artifactId>
  <version>1.8.1</version>
</dependency>
SBT
libraryDependencies += ""org.alluxio"" % ""alluxio-core-client-fs"" % ""1.8.1""

Contributing
Contributions via GitHub pull requests are gladly accepted from their original author. Along with
any pull requests, please state that the contribution is your original work and that you license the
work to the project under the project's open source license. Whether or not you state this
explicitly, by submitting any copyrighted material via pull request, email, or other means you agree
to license the material under the project's open source license and warrant that you have the legal
authority to do so.
For a more detailed step-by-step guide, please read
how to contribute to Alluxio.
For new contributor, please take 2 new contributor tasks.
Useful Links

Alluxio Website
Downloads
Releases and Notes
Documentation

",4120
twilio/voice-quickstart-android,Java,"
NOTE: This sample application uses the Programmable Voice Android 3.x APIs. For an example using
our 2.x APIs, please see the 2.x
branch. If you are using SDK 2.x, we highly recommend planning your migration to 3.0 as soon as possible. Support for 2.x will cease 1/1/2020. Until then, SDK 2.x will only receive fixes for critical or security related issues.

Twilio Voice Quickstart for Android
Get started with Voice on Android:

Quickstart - Run the quickstart app
New Features - New features in 3.0
Migration Guide - Migrating from 2.x to 3.x
Emulator Support - Android emulator support
Reducing APK Size - Use ABI splits to reduce APK size
Access Tokens - Using access tokens
Managing Push Credentials - Managing Push Credentials
Troubleshooting Audio - Troubleshooting Audio
More Documentation - More documentation related to the Voice Android SDK
Twilio Helper Libraries - TwiML quickstarts.
Issues & Support - Filing issues and general support

Quickstart
To get started with the Quickstart application follow these steps. Steps 1-6 will allow you to make a call. The remaining steps 7-8 will enable push notifications using FCM.

Open this project in Android Studio
Create a Voice API key
Configure a server to generate an access token to use in the app
Create a TwiML application
Configure your application server
Run the app
Generate google-services.json
Add a Push Credential using your FCM Server API Key
Receiving an Incoming Notification
Make client to client call
Make client to PSTN call

1. Open the project in Android Studio

2. Create a Voice API key
Go to the API Keys page and create a new API key.

Save the generated API_KEY and API_KEY_SECRET in your notepad. You will need them in the next step.
3. Configure a server to generate an access token to use in the app
Download one of the starter projects for the server.

voice-quickstart-server-java
voice-quickstart-server-node
voice-quickstart-server-php
voice-quickstart-server-python

Follow the instructions in the server's README to get the application server up and running locally and accessible via the public Internet. For now just add the Twilio Account SID that you can obtain from the console, and  the API_KEY and API_SECRET you obtained in the previous step.
ACCOUNT_SID = 'AC***'
API_KEY = 'SK***'
API_KEY_SECRET = '***'

4. Create a TwiML application
Next, we need to create a TwiML application. A TwiML application identifies a public URL for retrieving TwiML call control instructions. When your Android app makes a call to the Twilio cloud, Twilio will make a webhook request to this URL, your application server will respond with generated TwiML, and Twilio will execute the instructions you’ve provided.
To create a TwiML application, go to the TwiML app page. Create a new TwiML application, and use the public URL of your application server’s /makeCall endpoint as the Voice Request URL (If your app server is written in PHP, then you need .php extension at the end).

As you can see we’ve used our ngrok public address in the Request URL field above.
Save your TwiML Application configuration, and grab the TwiML Application SID (a long identifier beginning with the characters ""AP"").
5. Configure your application server
Put the remaining APP_SID configuration info into your application server by setting the following constants with the information you gathered above.
ACCOUNT_SID = 'AC***'
API_KEY = 'SK***'
API_KEY_SECRET = '***'
APP_SID = 'AP***'

Once you’ve done that, restart the server so it uses the new configuration info. Now it's time to test.
Open up a browser and visit the URL for your application server's Access Token endpoint: https://{YOUR_SERVER_URL}/accessToken (If your app server is written in PHP, then you need .php extension at the end). If everything is configured correctly, you should see a long string of letters and numbers, which is a Twilio Access Token. Your Android app will use a token like this to connect to Twilio.
6. Run the app
Paste the public URL of your application server’s https://{YOUR_SERVER_URL}/accessToken endpoint into TWILIO_ACCESS_TOKEN_SERVER_URL in VoiceActivity.java. Make sure to include /accessToken in the URL path.

Run the quickstart app on an Android device

Press the call button to open the call dialog.

Leave the dialog text field empty and press the call button to start a call. You will hear the congratulatory message. Support for dialing another client or number is described in steps 10 and 11.

7. Generate google-services.json
The Programmable Voice Android SDK uses Firebase Cloud Messaging push notifications to let your application know when it is receiving an incoming call. If you want your users to receive incoming calls, you’ll need to enable FCM in your application.
Follow the steps under Use the Firebase Assistant in the Firebase Developers Guide. Once you connect and sync to Firebase successfully, you will be able to download the google-services.json for your application.
Login to Firebase console and make a note of generated Server API Key and Sender ID in your notepad. You will need them in the next step.
""
Make sure the generated google-services.json is downloaded to the app directory of the quickstart project to replace the existing app/google-services.json stub json file. If you are using the Firebase plugin make sure to remove the stub google-services.json file first.
As a final step re-run the application from Android Studio to ensure the APK now has the latest google-services.json file.
8. Add a Push Credential using your FCM Server API Key
You will need to store the FCM Server API Key with Twilio so that we can send push notifications to your app on your behalf. Once you store the API Key with Twilio, it will get assigned a Push Credential SID so that you can later specify which key we should use to send push notifications.
Go to the Push Credentials page and create a new Push Credential.
Paste in the Server API Key and press Save.
""
9. Receiving an Incoming Notification
Put the PUSH_CREDENTIAL_SID configuration info into your application server by setting the following constants with the information you gathered above.
ACCOUNT_SID = 'AC***'
API_KEY = 'SK***'
API_KEY_SECRET = '***'
PUSH_CREDENTIAL_SID = 'CR***'
APP_SID = 'AP***'

Once you’ve done that, restart the server so it uses the new configuration info. Now it's time to test. Use your browser to initiate an incoming call by navigating to the public URL of your application server’s https://{YOUR_SERVER_URL}/placeCall endpoint (If your app server is written in PHP, then you need .php extension at the end). This will trigger a Twilio REST API request that will make an inbound call to your mobile app.
Your application will be brought to the foreground and you will see an alert dialog. The app will be brought to foreground even when your screen is locked.
""
You will receive an incoming call notification as well. If you pull down the notification drawer, you will be able to view the notification.
""
Once your app accepts the call, you should hear a congratulatory message.
10. Make client to client call
To make client to client calls, you need the application running on two devices. To run the application on an additional device, make sure you use a different identity in your access token when registering the new device. For example, change the identity field to bob and run the application.

Press the call button to open the call dialog.

Enter the client identity of the newly registered device to initiate a client to client call from the first device.


11. Make client to PSTN call
A verified phone number is one that you can use as your Caller ID when making outbound calls with Twilio. This number has not been ported into Twilio and you do not pay Twilio for this phone number.
To make client to number calls, first get a valid Twilio number to your account via https://www.twilio.com/console/phone-numbers/verified. Update your server code and replace CALLER_NUMBER with the verified number. Restart the server so that it uses the new value.
Press the call button to open the call dialog.

Enter a PSTN number and press the call button to place a call.

Emulator Support
The SDK supports using emulators except in the following known cases:

Emulators with API 22 or lower have bad audio emulation, the sound is generally inaudible
Emulators must have Google Play services support to use FCM to receive call invites
Running on x86 API 25 emulators results in application crashes

In general we advise using a real device when doing development with our SDK since real-time audio is a performance oriented operation.
Reducing APK Size
Our library is built using native libraries. As a result, if you use the default gradle build you will generate an APK with all four architectures(armeabi-v7a, arm64-v8a, x86, x86_64) in your APK.
APK splits allow developers to build multiple APKs for different screen sizes and ABIs. Enabling APK splits ensures that the minimum amount of files required to support a particular device are packaged into an APK.
The following snippet shows an example build.gradle with APK splits enabled.
apply plugin: 'com.android.application'

android {
    // Specify that we want to split up the APK based on ABI
    splits {
        abi {
            // Enable ABI split
            enable true

            // Clear list of ABIs
            reset()

            // Specify each architecture currently supported by the Voice SDK
            include ""armeabi-v7a"", ""arm64-v8a"", ""x86"", ""x86_64""

            // Specify that we do not want an additional universal SDK
            universalApk false
        }
    }
}

The adoption of APK splits requires developers to submit multiple APKs to the Play Store. Refer to Google’s documentation  for how to support this in your application.
Access Tokens
The access token generated by your server component is a jwt that contains a grant for Programmable Voice, an identity that you specify, and a time-to-live that sets the lifetime of the generated access token. The default time-to-live is 1 hour and is configurable up to 24 hours using the Twilio helper libraries.
Uses
In the Android SDK the access token is used for the following:

To make an outgoing call via Voice.call(Context context, String accessToken, String twiMLParams, Call.Listener listener)
To register or unregister for incoming notifications via GCM or FCM via Voice.register(String accessToken, Voice.RegistrationChannel registrationChannel, String registrationToken, RegistrationListener listener) and Voice.unregister(String accessToken, Voice.RegistrationChannel registrationChannel, String registrationToken, RegistrationListener listener). Once registered, incoming notifications are handled via a CallInvite where you can choose to accept or reject the invite. When accepting the call an access token is not required. Internally the CallInvite has its own accessToken that ensures it can connect to our infrastructure.

Managing Expiry
As mentioned above, an access token will eventually expire. If an access token has expired, our infrastructure will return error EXCEPTION_INVALID_ACCESS_TOKEN_EXPIRY/20104 via a CallException or a RegistrationException.
There are number of techniques you can use to ensure that access token expiry is managed accordingly:

Always fetch a new access token from your access token server before making an outbound call.
Retain the access token until getting a EXCEPTION_INVALID_ACCESS_TOKEN_EXPIRY/20104 error before fetching a new access token.
Retain the access token along with the timestamp of when it was requested so you can verify ahead of time whether the token has already expired based on the time-to-live being used by your server.
Prefetch the access token whenever the Application, Service, Activity, or Fragment associated with an outgoing call is created.

Managing Push Credentials
A Push Credential is a record for a push notification channel, for Android this Push Credential is a push notification channel record for FCM or GCM. Push Credentials are managed in the console under Mobile Push Credentials.
Whenever a registration is performed via Voice.register(…) in the Android SDK the identity and the Push Credential SID provided in the JWT based access token, along with the FCM/GCM token are used as a unique address to send push notifications to this application instance whenever a call is made to reach that identity. Using Voice.unregister(…) removes the association for that identity.
Updating a Push Credential
If you need to change or update your server key token provided by Firebase (under Project Settings → Cloud Messaging → Server key) you can do so by selecting the Push Credential in the console and adding your new Server key in the text box provided on the Push Credential page shown below:

Deleting a Push Credential
We do not recommend that you delete a Push Credential unless the application that it was created for is no longer being used.
When a Push Credential is deleted any associated registrations made with this Push Credential will be deleted. Future attempts to reach an identity that was registered using the Push Credential SID of this deleted push credential will fail.
If you are certain you want to delete a Push Credential you can click on Delete this Credential on the console page of the selected Push Credential.
Please ensure that after deleting the Push Credential you remove or replace the Push Credential SID when generating new access tokens.
Troubleshooting Audio
The following sections provide guidance on how to ensure optimal audio quality in your applications.
Configuring AudioManager
The AudioManager configuration guidance below is meant to provide optimal audio experience when in a Call. This configuration is inspired by the WebRTC Android example and the Android documentation.
 private void setAudioFocus(boolean setFocus) {
    if (audioManager != null) {
        if (setFocus) {
            savedAudioMode = audioManager.getMode();
            // Request audio focus before making any device switch.
            if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.O) {
                AudioAttributes playbackAttributes = new AudioAttributes.Builder()
                        .setUsage(AudioAttributes.USAGE_VOICE_COMMUNICATION)
                        .setContentType(AudioAttributes.CONTENT_TYPE_SPEECH)
                        .build();
                AudioFocusRequest focusRequest = new AudioFocusRequest.Builder(AudioManager.AUDIOFOCUS_GAIN_TRANSIENT)
                        .setAudioAttributes(playbackAttributes)
                        .setAcceptsDelayedFocusGain(true)
                        .setOnAudioFocusChangeListener(new AudioManager.OnAudioFocusChangeListener() {
                            @Override
                            public void onAudioFocusChange(int i) {
                            }
                        })
                        .build();
                audioManager.requestAudioFocus(focusRequest);
            } else {
                int focusRequestResult = audioManager.requestAudioFocus(new AudioManager.OnAudioFocusChangeListener() {

		                                       @Override
		                                       public void onAudioFocusChange(int focusChange) {
		
		                                       }
		                                   }, AudioManager.STREAM_VOICE_CALL,
                        AudioManager.AUDIOFOCUS_GAIN_TRANSIENT);
            }
            /*
             * Start by setting MODE_IN_COMMUNICATION as default audio mode. It is
             * required to be in this mode when playout and/or recording starts for
             * best possible VoIP performance. Some devices have difficulties with speaker mode
             * if this is not set.
             */
            audioManager.setMode(AudioManager.MODE_IN_COMMUNICATION);
        } else {
            audioManager.setMode(savedAudioMode);
            audioManager.abandonAudioFocus(null);
        }
    }
}

Handling Low Headset Volume
If your application experiences low playback volume, we recommend the following snippets:
Android N and Below
int focusRequestResult = audioManager.requestAudioFocus(new AudioManager.OnAudioFocusChangeListener() {
    
                                   @Override
                                   public void onAudioFocusChange(int focusChange) {
                                   }
                               }, AudioManager.STREAM_VOICE_CALL,
        AudioManager.AUDIOFOCUS_GAIN_TRANSIENT);

Android O and Up :
AudioAttributes playbackAttributes = new AudioAttributes.Builder()
        .setUsage(AudioAttributes.USAGE_ASSISTANCE_SONIFICATION)
        .setContentType(AudioAttributes.CONTENT_TYPE_SPEECH)
        .build();
AudioFocusRequest focusRequest = new AudioFocusRequest.Builder(AudioManager.AUDIOFOCUS_GAIN_TRANSIENT)
        .setAudioAttributes(playbackAttributes)
        .setAcceptsDelayedFocusGain(true)
        .setOnAudioFocusChangeListener(new AudioManager.OnAudioFocusChangeListener() {
            @Override
            public void onAudioFocusChange(int i) {
            }
        })
        .build();

More Documentation
You can find more documentation on getting started as well as our latest Javadoc below:

Getting Started
Javadoc

Twilio Helper Libraries
To learn more about how to use TwiML and the Programmable Voice Calls API, check out our TwiML quickstarts:

TwiML Quickstart for Python
TwiML Quickstart for Ruby
TwiML Quickstart for PHP
TwiML Quickstart for Java
TwiML Quickstart for C#

Issues and Support
Please file any issues you find here on Github.
For general inquiries related to the Voice SDK you can file a support ticket.
Please ensure that you are not sharing any
Personally Identifiable Information(PII)
or sensitive account information (API keys, credentials, etc.) when reporting an issue.
License
MIT
",95
Julian/venvs,Python,"venvs

 
  
venvs is a tool for configuring, in a single file, a set of virtualenvs,
which packages to install into each, and any binaries to make globally
available from within.

Installation
The usual:
$ pip install venvs


Usage
The best way to use venvs is by creating a file named
~/.local/share/virtualenvs/virtualenvs.toml. Here's an example of what goes
in it:
[virtualenv.development]
install = [
    ""pudb"",
    ""twisted"",
]
link = [""trial""]

[virtualenv.app]
install = [""$DEVELOPMENT/myapp""]

After creating the above, running convergeenvs will create 2 virtualenvs,
one called ""development"" with pudb and twisted installed into it and trial
linked from within it onto your PATH, and a second called ""app"" installing
the corresponding directory.
That's about all you need to know. If you insist on reading further though,
venvs has an older, not-very-recommended mutable interface which allows you to
create virtualenvs in a central location without tracking them in a config file
(or converging them). For that, usage is similar to mkvirtualenv,
although venvs passes arguments directly through to virtualenv:
$ venvs nameofvenv -- -p pypy
will create a virtual environment in an appropriate platform-specific
data directory, or in the directory specified by WORKON_HOME for
compatibility with virtualenvwrapper.

Single-Purpose Virtualenvs
A common use case for virtualenvs is for single-purpose installations, e.g.:
""I want to install fabric and give it its own virtualenv so that its
dependencies can be independently upgraded, all while still being able to use
the fab binary globally"".
venvs supports a --link option for this use case:
$ venvs -i fabric --link fab
will create a virtualenv for fabric (in the same normal location), but will
symlink the fab binary from within the virtualenv into your
~/.local/bin directory.
(You may have heard of pipsi which is a
similar tool for this use case, but with less customization than I would have
liked.)

Temporary Virtualenvs
I also find mktmpenv useful for quick testing. To support its use case,
venvs currently supports a different but similar style of temporary
virtualenv.
Invoking:
$ venv=$(venvs -t)

in your shell will create (or re-create) a global temporary virtualenv,
and print its bin/ subdirectory (which in this case will be then
stored in the venv variable). It can subsequently be used by, e.g.:
$ $venv/python

or:
$ $venv/pip ...

et cetera.
You may prefer using:
$ cd $(venvs -t)

as your temporary venv workflow if you're into that sort of thing instead.
The global virtualenv is cleared each time you invoke venvs -t.
Unless you care, unlike virtualenvwrapper's mktmpenv, there's no
need to care about cleaning it up, whenever it matters for the next
time, it will be cleared and overwritten.
venvs may support the more similar ""traditional"" one-use virtualenv in the
future, but given that it does not activate virtualenvs by default (see below),
the current recommendation for this use case would be to simply use the
virtualenv binary directly.

The 5 Minute Tutorial
Besides the venvs for named-virtualenv creation and venvs -t for
temporary-virtualenv creation described above:
$ findenv name foo

will output (to standard output) the path to a virtualenv with the given name
(see also --existing-only), and:
$ rmenv foo

will remove it.
There are a number of other slight variants, see the --help information for
each of the three binaries.
Real documentation to come (I hope)

Why don't I use virtualenvwrapper?
virtualenvwrapper is great! I've used it for a few years. But I've
slowly settled on a much smaller subset of its functionality that I like
to use. Specifically:


I don't like activating virtualenvs.
virtualenvs are magical and hacky enough on their own, and piling
activation on top just makes things even more messy for me, especially
when moving around between different projects in a shell.  Some people
use cd tricks to solve this, but I just want simplicity.

I don't need project support.
I've never attached a project to a virtualenv. I just use a naming
convention, naming the virtualenv with the name of the repo (with simple
coersion), and then using dynamic directory expansion in my shell
to handle association.



Basically, I just want a thing that is managing a central repository of
virtualenvs for me. So that's what venvs does.
",10
bioconda/bioconda.github.io,HTML,"Bioconda Documentation
The Bioconda Documentation is built with
Sphinx from sources located at
bioconda/bioconda-utils/docs/source.
If you have found a mistake in our documentation, please open an
issue,
or better yet, edit the file on Github and open a PR.
The package index is regenerated daily and may not reflect the most
current uploads to the Bioconda channel.
",18
pytorch/tvm,Python,"Pytorch TVM Extension


Please note that this is a work in progress.
Build
For improved performance, you'll need to build PyTorch on top of this PR: https://github.com/pytorch/pytorch/pull/20284
cd pytorch
git fetch origin pull/20284/head:tvm_dev
git checkout tvm_dev
python setup.py install

Otherwise, install the latest Nightly build of PyTorch.
Then, build this repo
# Make sure the right llvm-config is in your PATH
python setup.py install

Test
python setup.py test 

Usage
This package transparently hooks into PyTorch's JIT, so the same tooling is applicable (see @torch.jit.script, torch.jit.trace and graph_for).  See below for an example.
import torch_tvm

torch_tvm.enable()

# The following function will be compiled with TVM
@torch.jit.script
def my_func(a, b, c):
    return a * b + c

To disable the JIT hooks, use torch_tvm.disable().
Code Layout

register.cpp: Sets up pybind bindings and invokes the registration of a TVM backend.
compiler.{h,cpp}: Main logic to compile a PyTorch JIT graph with TVM.
operators.{h,cpp}: Location of mapping from JIT IR to TVM operators.


v0.1 Roadmap
Below, in order, is a prioritized list of tasks for this repository.

 End to end build and runtime
 Operator translation

 Add
 Multiply
 Convolution
 BatchNorm
 Relu
 AveragePool
 MaxPool
 Linear
 Reshape
 AdaptiveAveragePool


 Tooling

 Model coverage checks
 Benchmarks for master


 User exposed configurations

 Backend selection (CPU/Cuda/OpenCL)
 Optimization level


 Custom TVM operator registration

Enable Python/C++ mechanism to use custom TVM operators and schedules


 Bail-out mechanism

When TVM cannot compile a subgraph, invoke PyTorch JIT fallback



v0.2 Plan

 View support
 Zero copy set_input
 Subsystem integration

 Threadpool integration
 Allocator integration
 Distributed communication


 Advanced IR integration

 Control flow
 Aliasing



",148
oracc/oracc,C,"oracc sources
Sources for Oracc.
Setup
Please refer this setup guide for setting it locally and this setup guide for setting it up on a server.
",8
facebook/hhvm,C++,"HHVM
HHVM page |
HHVM documentation |
Hacklang page |
General group |
Dev group |
Twitter
HHVM is an open-source virtual machine designed for executing programs written in Hack. HHVM uses a just-in-time (JIT) compilation approach to achieve superior performance while maintaining the development flexibility that PHP provides.
HHVM is intended for Hack projects, and also supports a large subset of PHP 7 that is required by common tools and libraries. We no longer recommend using HHVM for purely PHP projects.
HHVM should be used together with a webserver like the built in, easy to deploy Proxygen, or a FastCGI-based webserver on top of nginx or Apache.
Installing
If you're new, try our getting started guide.
You can install a prebuilt package or compile from source.
Running
You can run standalone programs just by passing them to hhvm: hhvm example.hack.
If you want to host a website:

Install your favorite webserver. Proxygen is built in to HHVM, fast and easy to deploy.
Install our package
Start your webserver
Run sudo /etc/init.d/hhvm start
Visit your site at http://.../main.hack

Our getting started guide provides a slightly more detailed introduction as well as links to more information.
Contributing
We'd love to have your help in making HHVM better. If you're interested, please read our guide to contributing.
License
HHVM is licensed under the PHP and Zend licenses except as otherwise noted.
The Hack typechecker is licensed under the MIT License except as otherwise noted.
Reporting Crashes
See Reporting Crashes for helpful tips on how to report crashes in an actionable manner.
Reporting and Fixing Security Issues
Please do not open GitHub issues or pull requests - this makes the problem
immediately visible to everyone, including malicious actors. Security issues in
HHVM can be safely reported via HHVM's Whitehat Bug Bounty program:
https://www.facebook.com/whitehat
Facebook's security team will triage your report and determine whether or not
is it eligible for a bounty under our program.
FAQ
Our user FAQ has answers to many common questions about HHVM, from general questions to questions geared towards those that want to use.
There is also a FAQ for contributors to HHVM.
",16009
sidooms/MovieTweetings,None,"MovieTweetings
Some stats



Metric
Value




Total number of ratings
778,902


Number of unique users
57,272


Number of unique items
33,385


These stats were last autocalculated on Fri May 17 01:49:32 CEST 2019  (more stats here)




",218
efskap/sefr,Rust,"sefr (Search Engine FRontend)
 
Terminal program for interactively opening search engines / parametric URLs.
It's kinda like surfraw but with interactive suggestions (via parsing opensearch json).

Motivation
I use custom url bar search engines a lot, but browser support for them is frustrating.

Firefox has a really obtuse way of defining them, doesn't let you specify suggestion endpoints, and still doesn't sync them.
Chrome makes defining them easy, syncs them, but doesn't let you specify suggestion endpoints.
Vivaldi makes defining them easy, lets you specify suggestion endpoints, but doesn't sync them.

e.g. in stock Firefox, you can't create a search engine that, when you type ""r foo"" in your url bar, automatically goes to ""reddit.com/r/foo"".
You have to manually write the URL, and you don't even get completions!
This is meant to be a customizable crossplatform solution, and since it uses your default browser (more details), you can integrate it into a GUI workflow with a global hotkey (see below).
Installation
There are two ways to install sefr:

Clone this repository, install the Rust toolchain, and either call cargo run in the cloned directory to try it out, or cargo build to create a binary located at target/debug/sefr.
Install via cargo by calling cargo install sefr. This should make it runnable from anywhere.

A convenient way to integrate it into your desktop environment is by mapping a global hotkey to launch it in a lightweight terminal. For example, bind this to a convenient hotkey in your DE or WM and change the last argument to point at the binary.
uxterm  -geometry 60x20+620+200 -fa 'Monospace' -fs 13 -bg grey27 -fg white -e ~/sefr/target/debug/sefr
Configuration  / Customization
Config file
On its first startup, sefr will automatically generate a TOML configuration file in the config directory provided by the directories crate. Any subsequent changes should be made inside it.
e.g. For Linux, the config file will be found in ~/.config/sefr/config.toml.
Adding new engines
Warning: The current configuration format might be changed in the future!
New engines can be added for use by sefr by adding them to the config.toml file.
A basic engine definition looks like this:
[engines.yt]
name = ""YouTube""
search_url = ""https://www.youtube.com/results?q=%s""
suggestion_url = ""http://suggestqueries.google.com/complete/search?client=firefox&ds=yt&q=%s""

[engines.PREFIX] defines what prefix (also known as a keyword or trigger) activates the engine.
name is the name of the engine, used for the prompt text if not defined in the prompt section (more on that later).
search_url is opened in your browser with %s replaced by the search term when enter is pressed.
suggestion_url (optional) is the endpoint queried for suggestions (with %s replaced by the search term) while typing. It must return  OpenSearch suggestions schema json.
space_becomes (optional, + by default) is what spaces are replaced with before search_url is opened.
In the default config:

engines.wkt (Wiktionary) and engines.w (Wikipedia) have it set to _, because that's how Wikis encode spaces in their URLs.
engines.r (Subreddit) has it set to a blank string, because subreddits can't have spaces in their names (note that this value prevents spaces from being entered into the input buffer when the engine is selected so that space can be used to select a suggestion without performing a search).



The engine used when no prefix is entered is defined as _default in the config, and it is obligatory for the program to start. Example:
[engines._default]
name = ""Google""
search_url = ""https://www.google.com/search?q=%s""
suggestion_url = ""https://www.google.com/complete/search?client=chrome&q=%s""
Along with this, there is also an optional prompt section which handles the prompt displayed when the engine is called. It will usually look like this:
[engines.yt.prompt]
icon = "" ▶ ""
icon_bg = ""Red""
icon_fg = ""White""
text = "" Youtube ""
text_bg = ""White""
text_fg = ""Black""
The following fields are supported, and all are optional:

icon: the icon displayed in the prompt
icon_bg: background color of the icon
icon_fg: foreground color of the icon
text: The text displayed after the icon in the prompt
text_bg: background color for the text
text_fg: foreground color for the text

Note that icon and text are padded with whitespace for aesthetics in the example configuration, but this is not required.
The fields are all strings except for colors (*_bg, *_fg). They can be strings (corresponding to the color names here), 8-bit numbers (corresponding to Ansi color codes), or 8-bit RGB tuples like [255,255,255]
If this section is left out for a particular engine, a basic prompt displaying the engine's name will be used.
Keybindings
For the time being, keybindings are hardcoded and they can be found below:

Tab/Down/Ctrl-N => Select next suggestion
Shift-Tab/Up/Ctrl-P => Select previous suggestion
Ctrl-W => Delete last word
Ctrl-C => Exit
Enter => Perform selected search in browser

Progress
This project is currently in its alpha stage but is relatively stable.

 Prompt
 Suggestions request / json parse
 Definable engines with prefixes, prompts, and endpoints
 Browser launching
 Selection of suggestions w/ prefix edge cases
 TOML file config
 Use real cursor for rendering input buffer, and be able to move it
 Configurable keybindings
 Better feedback for when suggestion endpoints misbehave
 CLI args, e.g. providing the initial input buffer through an argument for aliasing.

",2
InvestmentSystems/static-frame,Python,"















static-frame
The StaticFrame library consists of the Series and Frame, immutable data structures for one- and two-dimensional calculations with self-aligning, labelled axes. StaticFrame offers an alternative to Pandas. While many interfaces for data extraction and manipulation are similar to Pandas, StaticFrame deviates from Pandas in many ways: all data is immutable, and all indices must be unique; all vector processing uses NumPy, and the full range of NumPy data types is preserved; the implementation is concise and lightweight; consistent naming and interfaces are used throughout; and flexible approaches to iteration and function application, with built-in options for parallelization, are provided.
Code: https://github.com/InvestmentSystems/static-frame
Docs: http://static-frame.readthedocs.io
Packages: https://pypi.org/project/static-frame

Installation
Install StaticFrame via PIP:
pip install static-frame

Or, install StaticFrame via conda:
conda install -c conda-forge static-frame


Dependencies
StaticFrame requires Python 3.6+ and NumPy 1.14.1+.

Quick-Start Guide
StaticFrame provides numerous methods for loading and creating data, either as a 1D Series or a 2D Frame. All creation routines are exposed as alternate constructors on the desired class, such as Frame.from_records(), Frame.from_csv() or Frame.from_pandas().
For example, we can load JSON data from a URL using Frame.from_json_url(), and then use Frame.head() to reduce the displayed output to just the first five rows. (Passing explicit dtypes is only necessary on Windows.)
>>> import numpy as np
>>> import static_frame as sf
>>> frame = sf.Frame.from_json_url('https://jsonplaceholder.typicode.com/photos', dtypes=dict(albumId=np.int64, id=np.int64))
>>> frame.head()
<Frame>
<Index> albumId id      title                url                  thumbnailUrl         <<U12>
<Index>
0       1       1       accusamus beatae ... https://via.place... https://via.place...
1       1       2       reprehenderit est... https://via.place... https://via.place...
2       1       3       officia porro iur... https://via.place... https://via.place...
3       1       4       culpa odio esse r... https://via.place... https://via.place...
4       1       5       natus nisi omnis ... https://via.place... https://via.place...
<int64> <int64> <int64> <<U86>               <<U38>               <<U38>

Note
The Pandas CSV reader far out-performs the NumPy-based reader in StaticFrame: thus, for now, using Frame.from_pandas(pd.read_csv(fp)) is recommended for loading CSV files.
For more information on Series and Frame constructors, see Container Import & Creation.

As with a NumPy array, the Frame exposes common attributes of shape and size.
>>> frame.shape
(5000, 5)
>>> frame.size
25000
>>> frame.nbytes
3320000
Unlike a NumPy array, a Frame stores heterogeneous types, where each column is a single type. StaticFrame preserves the full range of NumPy types, including fixed-size character strings. Character strings can be converted to Python objects or other types as needed with the Frame.astype interface, which exposes a __getitem__ style interface for selecting columns to convert. As with all similar functions, a new Frame is returned.
>>> frame.dtypes
<Series>
<Index>
albumId      int64
id           int64
title        <U86
url          <U38
thumbnailUrl <U38
<<U12>       <object>
>>> frame.astype['title':](object).dtypes
<Series>
<Index>
albumId      int64
id           int64
title        object
url          object
thumbnailUrl object
<<U12>       <object>
Utility functions common to Pandas users are available on Frame and Series, such as Series.unique(), Series.isna(), and Series.any().
>>> frame['albumId'].unique()
array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,
        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,
        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,
        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,
        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,
        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,
        92,  93,  94,  95,  96,  97,  98,  99, 100])
>>> frame['id'].isna().any()
False

Note
For more information on Series and Frame utility functions, see Transformations & Utilities.

StaticFrame interfaces for extracting data will be familiar to Pandas users, though with a number of interface refinements to remove redundancies and increase consistency. On a Frame, __getitem__ is (exclusively) a column selector; loc and iloc are (with one argument) row selectors or (with two arguments) row and column selectors.
For example we can select a single column with __getitem__:
>>> frame['albumId'].tail()
<Series: albumId>
<Index>
4995              100
4996              100
4997              100
4998              100
4999              100
<int64>           <int64>
Consistent with other __getitem__ style selectors, a slice or a list can be used to select columns:
>>> frame['id':'title'].head()
<Frame>
<Index> id      title                <<U12>
<Index>
0       1       accusamus beatae ...
1       2       reprehenderit est...
2       3       officia porro iur...
3       4       culpa odio esse r...
4       5       natus nisi omnis ...
<int64> <int64> <<U86>
The loc interface, with one argument, returns a Series for the row found at the given index label.
>>> frame.loc[4]
<Series: 4>
<Index>
albumId      1
id           5
title        natus nisi omnis ...
url          https://via.place...
thumbnailUrl https://via.place...
<<U12>       <object>
With two arguments, loc can select both rows and columns at the same time:
>>> frame.loc[4:8, ['albumId', 'title']]
<Frame>
<Index> albumId title                <<U12>
<Index>
4       1       natus nisi omnis ...
5       1       accusamus ea aliq...
6       1       officia delectus ...
7       1       aut porro officii...
<int64> <int64> <<U86>
Where the loc interface uses index and column labels, the iloc interface uses integer offsets from zero, just as if the Frame were a NumPy array. For example, we can select the last row with -1:
>>> frame.iloc[-1]
<Series: 4999>
<Index>
albumId        100
id             5000
title          error quasi sunt ...
url            https://via.place...
thumbnailUrl   https://via.place...
<<U12>         <object>
Or, using two arguments, we can select the first two columns of the last two rows:
>>> frame.iloc[-2:, 0:2]
<Frame>
<Index> albumId id      <<U12>
<Index>
4998    100     4999
4999    100     5000
<int64> <int64> <int64>
Just as with Pandas, expressions can be used in __getitem__, loc, and iloc statements to create more narrow selections. For example, we can select all ""albumId"" greater than or equal to 98.
>>> frame.loc[frame['albumId'] >= 98, ['albumId', 'title']].head()
<Frame>
<Index> albumId title                <<U12>
<Index>
4850    98      aut aut nulla vol...
4851    98      ducimus neque del...
4852    98      fugit officiis su...
4853    98      pariatur temporib...
4854    98      qui inventore inc...
<int64> <int64> <<U86>
However, unlike Pandas, __getitem__, loc, and iloc cannot be used for assignment or in-place mutation on a Frame or Series. Throughout StaticFrame, all underlying NumPy arrays, and all container attributes, are immutable. Making data and objects immutable reduces opportunities for coding errors and offers, in some situations, greater efficiency by avoiding defensive copies.
>>> frame.loc[4854, 'albumId']
98
>>> frame.loc[4854, 'albumId'] = 200
Traceback (most recent call last):
TypeError: 'GetItem' object does not support item assignment
>>> frame.values[4854, 0] = 200
Traceback (most recent call last):
ValueError: assignment destination is read-only

Note
For more information on Series and Frame selection interfaces, see Selection.

Instead of in-place assignment, an assign interface object (similar to the Frame.astype interface shown above) is provided to expose __getitem__, loc, and iloc interfaces that, when called with an argument, return a new object with the desired changes. These interfaces expose the full range of expressive assignment-like idioms found in Pandas and NumPy. Arguments can be single values, or Series and Frame objects, where assignment will align on the Index.
>>> frame_new = frame.assign.loc[4854, 'albumId'](200)
>>> frame_new.loc[4854, 'albumId']
200
This pattern of specialized interfaces is used throughout StaticFrame, such as with the Frame.mask and Frame.drop interfaces. For example, Frame.mask can be used to create a Boolean Frame that sets rows to True if their ""id"" is even:
>>> frame.mask.loc[frame['id'] % 2 == 0].head()
<Frame>
<Index> albumId id     title  url    thumbnailUrl <<U12>
<Index>
0       False   False  False  False  False
1       True    True   True   True   True
2       False   False  False  False  False
3       True    True   True   True   True
4       False   False  False  False  False
<int64> <bool>  <bool> <bool> <bool> <bool>
Or, using the Frame.drop interface, a new Frame can be created by dropping rows with even ""id"" values and dropping URL columns specified in a list:
>>> frame.drop.loc[frame['id'] % 2 == 0, ['thumbnailUrl', 'url']].head()
<Frame>
<Index> albumId id      title                <<U12>
<Index>
0       1       1       accusamus beatae ...
2       1       3       officia porro iur...
4       1       5       natus nisi omnis ...
6       1       7       officia delectus ...
8       1       9       qui eius qui aute...
<int64> <int64> <int64> <<U86>

Note
For more information on Series and Frame interfaces, see Assignment / Dropping / Masking.

Iteration of rows, columns, and elements, as well as function application on those values, is unified under a family of generator interfaces. These interfaces are distinguished by the form of the data iterated (Series, namedtuple, or array) and whether key-value pairs (e.g., Frame.iter_series_items()) or just values (e.g., Frame.iter_series()) are yielded. For example, we can iterate over each row of a Frame and yield a corresponding Series:
>>> next(iter(frame.iter_series(axis=1)))
<Series>
<Index>
albumId      1
id           1
title        accusamus beatae ...
url          https://via.place...
thumbnailUrl https://via.place...
<<U12>       <object>
Or we can iterate over rows as named tuples, applying a function that matches a substring of the ""title"" or returns None, then drop those None records:
>>> frame.iter_tuple(axis=1).apply(lambda r: r.title if 'voluptatem' in r.title else None).dropna().head()
<Series>
<Index>
19       assumenda volupta...
27       non neque eligend...
29       odio enim volupta...
31       ad enim dignissim...
40       in voluptatem dol...
<int64>  <object>
Element iteration and function application works the same way as for rows or columns (though without an axis argument). For example, here each URL is processed with the same string transformation function:
>>> frame[['thumbnailUrl', 'url']].iter_element().apply(lambda c: c.replace('https://', '')).iloc[-4:]
<Frame>
<Index> thumbnailUrl         url                  <<U12>
<Index>
4996    via.placeholder.c... via.placeholder.c...
4997    via.placeholder.c... via.placeholder.c...
4998    via.placeholder.c... via.placeholder.c...
4999    via.placeholder.c... via.placeholder.c...
<int64> <object>             <object>
Group-by functionality is exposed in a similar manner with Frame.iter_group_items() and Frame.iter_group().
>>> next(iter(frame.iter_group('albumId', axis=0))).shape
(50, 5)
Function application to a group Frame can be used to produce a Series indexed by the group label. For example, a Series, indexed by ""albumId"", can be produced to show the number of unique titles found per album.
>>> frame.iter_group('albumId', axis=0).apply(lambda g: len(g['title'].unique())).head()
<Series>
<Index>
1        50
2        50
3        50
4        50
5        50
<int64>  <int64>

Note
For more information on Series and Frame iterators and tools for function application, see Iterators.

If performing calculations on a Frame that result in a Series with a compatible Index, a grow-only FrameGO can be used to add Series as new columns. This limited form of mutation, i.e., only the addition of columns, provides a convenient compromise between mutability and immutability. (Underlying NumPy array data always remains immutable.)
A FrameGO can be efficiently created from a Frame, as underling NumPy arrays do not have to be copied:
>>> frame_go = frame.to_frame_go()
We can obtain a track number within each album, assuming the records are sorted, by creating the following generator expression pipe-line. Using a Frame grouped by ""albumId"", zip together as pairs the Frame.index and a contiguous integer sequence via range(); chain all of those iterables, and then pass the resulting generator to Series.from_items(). (As much as possible, StaticFrame supports generators as arguments wherever an ordered sequence is expected.)
>>> from itertools import chain
>>> index_to_track = chain.from_iterable(zip(g.index, range(len(g))) for g in frame_go.iter_group('albumId'))
>>> frame_go['track'] = sf.Series.from_items(index_to_track) + 1
>>> frame_go.iloc[45:55]
<FrameGO>
<IndexGO> albumId id      title                url                  thumbnailUrl         track   <<U12>
<Index>
45        1       46      quidem maiores in... https://via.place... https://via.place... 46
46        1       47      et soluta est        https://via.place... https://via.place... 47
47        1       48      ut esse id           https://via.place... https://via.place... 48
48        1       49      quasi quae est mo... https://via.place... https://via.place... 49
49        1       50      et inventore quae... https://via.place... https://via.place... 50
50        2       51      non sunt voluptat... https://via.place... https://via.place... 1
51        2       52      eveniet pariatur ... https://via.place... https://via.place... 2
52        2       53      soluta et harum a... https://via.place... https://via.place... 3
53        2       54      ut ex quibusdam d... https://via.place... https://via.place... 4
54        2       55      voluptatem conseq... https://via.place... https://via.place... 5
<int64>   <int64> <int64> <<U86>               <<U38>               <<U38>               <int64>
Unlike with Pandas, StaticFrame Index objects always enforce uniqueness (there is no ""verify_integrity"" option: integrity is never optional). Thus, an index can never be set from non-unique data:
>>> frame_go.set_index('albumId')
Traceback (most recent call last):
KeyError: 'labels have non-unique values'
For a data set such as the one used in this example, a hierarchical index, by ""albumId"" and ""track"", is practical. StaticFrame implements hierarchical indices as IndexHierarchy objects. The Frame.set_index_hierarchy() method, given columns in a Frame, can be used to create a hierarchical index:
>>> frame_h = frame_go.set_index_hierarchy(['albumId', 'track'], drop=True)
>>> frame_h.head()
<FrameGO>
<IndexGO>                id      title                url                  thumbnailUrl         <<U12>
<IndexHierarchy>
1                1       1       accusamus beatae ... https://via.place... https://via.place...
1                2       2       reprehenderit est... https://via.place... https://via.place...
1                3       3       officia porro iur... https://via.place... https://via.place...
1                4       4       culpa odio esse r... https://via.place... https://via.place...
1                5       5       natus nisi omnis ... https://via.place... https://via.place...
<int64>          <int64> <int64> <<U86>               <<U38>               <<U38>
Hierarchical indices permit specifying selectors, per axis, at each hierarchical level. To distinguish hierarchical levels from axis arguments in a loc expression, the HLoc wrapper, exposing a __getitem__ interface, can be used. For example, we can select, from all albums, the second and fifth track, and then only the ""title"" and ""url"" columns.
>>> frame_h.loc[sf.HLoc[:, [2,5]], ['title', 'url']].head()
<FrameGO>
<IndexGO>                title                url                  <<U12>
<IndexHierarchy>
1                2       reprehenderit est... https://via.place...
1                5       natus nisi omnis ... https://via.place...
2                2       eveniet pariatur ... https://via.place...
2                5       voluptatem conseq... https://via.place...
3                2       eaque iste corpor... https://via.place...
<int64>          <int64> <<U86>               <<U38>
Just as a hierarchical selection can reside in a loc expression with an HLoc wrapper, an integer index selection can reside in a loc expression with an ILoc wrapper. For example, the previous row selection is combined with the selection of the last column:
>>> frame_h.loc[sf.HLoc[:, [2,5]], sf.ILoc[-1]].head()
<Series: thumbnailUrl>
<IndexHierarchy>
1                      2       https://via.place...
1                      5       https://via.place...
2                      2       https://via.place...
2                      5       https://via.place...
3                      2       https://via.place...
<int64>                <int64> <<U38>

Note
For more information on Index and IndexHierarchy, see Index Manipulation.

While StaticFrame offers many of the features of Pandas and similar data structures, exporting directly to NumPy arrays (via the .values attribute) or to Pandas is supported for functionality not found in StaticFrame or compatibility with other libraries. For example, a Frame can export to a Pandas DataFrame with Frame.to_pandas().
>>> df = frame_go.to_pandas()
",107
awslabs/deeplearning-cfn,Python,"Distributed Deep Learning on AWS Using MXNet and TensorFlow
AWS CloudFormation, which creates and configures Amazon Web Services resources with a template, simplifies the process of setting up a distributed deep learning cluster. The AWS CloudFormation Deep Learning template uses the Amazon Deep Learning AMI (which provides MXNet, TensorFlow, Caffe, Theano, Torch, and CNTK frameworks) to launch a cluster of EC2 instances and other AWS resources needed to perform distributed deep learning.
With this template, we continue with our mission to make distributed deep learning easy. AWS CloudFormation creates all resources in the customer account.
What's New?
We've updated the AWS CloudFormation Deep Learning template to add some exciting new features and capabilities.
May 16 2019

Updated AWS DLAMI Conda version in CFN template file to v23.0. Check release notes for details.

April 30 2019

Add Support to Amazon Linux 2. Check product overview for details.
Updated AWS DLAMI Conda version in CFN template file to v22.0 release notes for details.

Feb 12 2019

Updated AWS DLAMI Conda version in CFN template file to v21.2. Check release notes for details.

Dec 19 2018

Updated AWS DLAMI Conda version in CFN template file to v20.0. Check release notes for details.
Updated instructions to use tensorflow with horovod for distributed training.

Nov 7 2018


Introduce AWS DLAMI Conda v16.0 - For developers who want pre-installed pip packages of deep learning frameworks in separate virtual environments, the Conda-based AMI is available in Ubuntu and  Amazon Linux. Check AWS DLAMI Official webpage for more details.


We now support 11 AWS regions- us-east-1, us-west-2, eu-west-1, us-east-2, ap-southeast-2, ap-northeast-1, ap-northeast-2, ap-south-1, eu-central-1,ap-southeast-1, us-west-1.


We now support g3 and c5 instances, and remove g2 instances.


Update MXNet submodule to 1.3.0.


Mar 22 2018


We now support 10 AWS regions - us-east-1, us-west-2, eu-west-1, us-east-2, ap-southeast-2, ap-northeast-1, ap-northeast-2, ap-south-1, eu-central-1,ap-southeast-1.


We now support p3 instances.


Older Release Notes


We now support 5 AWS regions - us-east-1, us-east-2, us-west-2, eu-west-1 and ap-southeast-2.


We've enhanced the AWS CloudFormation Deep Learning template with automation that continues stack creation even if the provisioned number of worker instances falls short of the desired count. In the previous version of the template, if one of the worker instances failed to be provisioned, for example, if it a hit account limit, AWS CloudFormation rolled back the stack and required you to adjust your desired count and restart the stack creation process. The new template includes a function that automatically adjusts the count down and proceeds with setting up the rest of the cluster (stack).


We now support creating a cluster of CPU Amazon EC2 instance types.


We've also added Amazon Elastic File System (Amazon EFS) support for the cluster created with the template.

Amazon EFS is automatically mounted on all worker instances during startup.
Amazon EFS allows sharing of code, data, and results across worker instances.
Using Amazon EFS doesn't degrade performance for densely packed files (for example, .rec files containing image data).



We now support creating a cluster of instances running Ubuntu. See the Ubuntu Deep Learning AMI.


EC2 Cluster Architecture
The following architecture diagram shows the EC2 cluster infrastructure.

Resources Created by the Deep Learning Template
The Amazon Deep Learning template creates a stack that contains the following resources:

A VPC in the customer account.
The requested number or available number of worker instances in an Auto Scaling group within the VPC. These worker instances are launched in a private subnet.
A master instance in a separate Auto Scaling group that acts as a proxy to enable connectivity to the cluster with SSH. AWS CloudFormation places this instance within the VPC and connects it to both the public and private subnets. This instance has both public IP addresses and DNS.
An Amazon EFS file storage system configured in General Purpose performance mode.
A mount target to mount Amazon EFS on the instances.
A security group that allows external SSH access to the master instance.
A security group that allows the master and worker instances to mount and access Amazon EFS through NFS port 2049.
Two security groups that open ports on the private subnet for communication between the master and workers.
An AWS Identity and Access Management (IAM) role that allows instances to poll Amazon Simple Queue Service (Amazon SQS) and access and query Auto Scaling groups and the private IP addresses of the EC2 instances.
A NAT gateway used by the instances within the VPC to talk to the outside world.
Two Amazon SQS queues to configure the metadata at startup on the master and the workers.
An AWS Lambda function that monitors the Auto Scaling group's launch activities and modifies the desired capacity of the Auto Scaling group based on availability.
An Amazon Simple Notification Service (Amazon SNS) topic to trigger the Lambda function on Auto Scaling events.
AWS CloudFormation WaitCondition and WaitHandler, with a stack creation timeout of 55 minutes to complete metadata setup.

How the Deep Learning Template Works
The startup script enables SSH forwarding on all hosts. Enabling SSH agent forwarding is essential because frameworks such as MXNet use SSH for communication between master and worker instances during distributed training.
The startup script on the master polls the master SQS queue for messages confirming that Auto Scaling setup is complete. The Lambda function sends two messages, one when the master Auto Scaling group is successfully set up, and a second when either the requested capacity is satisfied or when instances fail to launch on the worker Auto Scaling group. When instance launch fails on the worker Auto Scaling group, the Lambda function modifies the desired capacity to the number of instances that have been successfully launched.
Upon receiving messages on the Amazon SQS master queue, the setup script on the master configures all of the necessary worker metadata (IP addresses of the workers, GPU count, etc.,) and broadcasts the metadata on the worker SQS queue. Upon receiving this message, the startup script on the worker instances that are polling the SQS worker queue configure this metadata on the workers.
The following environment variables are set up on all the instances:


$DEEPLEARNING_WORKERS_PATH: The file path that contains the list of workers


$DEEPLEARNING_WORKERS_COUNT: The total number of workers


$DEEPLEARNING_WORKER_GPU_COUNT: The number of GPUs on the instance


$EFS_MOUNT: The directory where Amazon EFS is mounted


Setting Up a Deep Learning Stack
To set up a deep learning AWS CloudFormation stack, follow Using the AWS CloudFormation Deep Learning Template.
Running Distributed Training
To demonstrate how to run distributed training using MXNet and Tensorflow frameworks, we use the standard CIFAR-10 model.  CIFAR-10 is a sufficiently complex network that benefits from a distributed setup and that can be quickly trained on such a setup.
Log in to the Master Instance
Follow Step 3 in Using the AWS CloudFormation Deep Learning Template.
Clone the awslabs/deeplearning-cfn repo that contains the examples onto the EFS mount
Note: This could take a few minutes.
git clone https://github.com/awslabs/deeplearning-cfn $EFS_MOUNT/deeplearning-cfn && \
cd $EFS_MOUNT/deeplearning-cfn && \
#
#fetches dmlc/mxnet and tensorflow/models repos as submodules
git submodule update --init $EFS_MOUNT/deeplearning-cfn/examples/tensorflow/models && \
git submodule update --init $EFS_MOUNT/deeplearning-cfn/examples/incubator-mxnet && \
cd $EFS_MOUNT/deeplearning-cfn/examples/incubator-mxnet && \
git submodule update --init $EFS_MOUNT/deeplearning-cfn/examples/incubator-mxnet/3rdparty/dmlc-core
# We also need to pull latest dmlc-core code to make it work in conda environment
cd $EFS_MOUNT/deeplearning-cfn/examples/incubator-mxnet/3rdparty/dmlc-core
git fetch origin master
git checkout -b master origin/master

Running Distributed Training on MXNet
The following example shows how to run CIFAR-10 with data parallelism on MXNet. Note the use of the DEEPLEARNING_* environment variables.
#terminate all running Python processes across workers
while read -u 10 host; do ssh -o ""StrictHostKeyChecking no"" $host ""pkill -f python"" ; \
done 10<$DEEPLEARNING_WORKERS_PATH

#navigate to the MXNet image-classification example directory \
cd $EFS_MOUNT/deeplearning-cfn/examples/incubator-mxnet/example/gluon/

#run the CIFAR10 distributed training example in mxnet conda environment(Change mxnet_p36 to mxnet_p27 if you wnat to run in python27 environment)\
../../tools/launch.py -n $DEEPLEARNING_WORKERS_COUNT -H $DEEPLEARNING_WORKERS_PATH ""source activate mxnet_p36 && python image_classification.py --dataset cifar10 --model vgg11 --epochs 1 --kvstore dist_device_sync""

We were able to run the training for 100 epochs in 25 minutes on 2 P2.8x EC2 instances and achieve a training accuracy of 92%.
These steps summarize how to get started. For more information about running distributed training on MXNet, see Run MXNet on Multiple Devices.
Running Distributed Training on TensorFlow with Horovod
Horovod is a distributed training framework to make distributed deep learning easy and fast. You can get more information about the advantages of using Horovod to do distributed training with Tensorflow or other framework.
Starting from DLAMI Conda v19.0, it comes with example pre-configured scripts to show how you can train model with Tensorflow and Horovod on multi gpus/machines.
The following example shows how to train a ResNet-50 model with synthetic data.
cd ~/examples/horovod/tensorflow
vi hosts

You should be able to see localhost slots=8 in your hosts file. The number of slots means how many GPUs you want to use in that machine to train your model. Also, you should append your worker nodes to the hosts file, and assign GPU number to it. To know how many GPUs available in your instance, run nvidia-smi. After the change, your hosts file should look like
localhost slots=<#GPUs>
<worker node private ip> slots=<#GPUs>
......

You can easily calculate the number of GPUs you'll use to train the model by summing up the slots available on each machine. Note that: the argument passed to the train_synthetic.sh script below is passed to -np parameter of mpirun. The -np argument represents the total number of processes and the slots argument in hostfile represents the split of those processes per machine.
Then, just  run
./train_synthetic.sh 24  or replace 24 with number of GPUs you use.
",231
AgentManny/qLib,Java,"qLib
This has everything you need to make a minecraft or roblox server
Credits to @AlfieC & @bizarre
Follow https://twitter.com/alfie_cleveland <3
Scoreboard example of what you can do with q""L""ib

",2
michelegera/dotfiles,Shell,"michelegera’s dotfiles
These are the base dotfiles that I start with when I set up a
new environment. For more specific local needs I use the .local
files described in the Local Settings section.
Setup
To setup the dotfiles just run the following command in the
terminal:
(DO NOT run the command if you don’t fully
understand what it does. Seriously, DON’T!)
bash -c ""$(curl -LsS https://raw.github.com/michelegera/dotfiles/master/dotfiles)""
That’s it!
The setup process will:

Download the dotfiles on your computer (by default it will suggest
~/.dotfiles)
Create some additional directories
Symlink the
git, and
shell files
Install applications / command-line tools for
macOS
Install the
Bash shell
Set custom
macOS preferences

Customize
Local Settings
The dotfiles can be easily extended to suit additional local
requirements by using the following files:
~/.gitconfig.local
If the ~/.gitconfig.local file exists, it will be automatically
included after the configurations from ~/.gitconfig, thus, allowing
its content to overwrite or add to the existing git configurations.
Note: Use ~/.gitconfig.local to store sensitive information such
as the git user credentials, e.g.:
[user]
  name = John Appleseed
  email = john.appleseed@apple.com
  signingkey = XXXXXXXX
Forks
If you decide to fork this project, don’t forget to substitute my
username with your own in the setup snippets, in the
dotfiles script
and in the iTerm preferences.
Update
To update the dotfiles you can either run the dotfiles
script or, if you want to just update one particular part,
run the appropriate os script.
Acknowledgements
Inspiration and code was taken from many sources, including:

Cătălin Mariș’
dotfiles
Mathias Bynens’
dotfiles
Nicolas Gallagher’s
dotfiles

License
The code is available under the MIT license.
",3
zeal-dev/zeal-website,CSS,"How to get going
This site is built using hugo.  Installation instructions can be found here.
Once you've got hugo running, you can start the site with the command
hugo server -D

",2
jeffotoni/s3godo,Go,"s3godo
client Digital Ocean in Go
Godo is a sdk, a client so we can work natively using Go.
The lib is very light and lean, we managed to handle via API all the features we have in Cloud DigitalOcean.
We were able to create, remove, list Droplets dynamically, provision them as needed using Godo, and integrate with Terraform or create their own apis.
Possibilities of working with Load Balance, Network, Spaces, Volumes, and so on.
The API is fantastic, network latency makes it all work like a rocket.
The programs in Go fit like a glove due to its simplicity and performance, consuming little hardwares we have lean costs for our projects.
package main

import (
	""context""
	""os""

	""github.com/digitalocean/godo""
	""golang.org/x/oauth2""
)

var (
	pat = os.Getenv(""DO_PAT"")
)

type TokenSource struct {
	AccessToken string
}

func (t *TokenSource) Token() (*oauth2.Token, error) {
	token := &oauth2.Token{
		AccessToken: t.AccessToken,
	}
	return token, nil
}

func main() {

	tokenSource := &TokenSource{
		AccessToken: pat,
	}

	oauthClient := oauth2.NewClient(context.Background(), tokenSource)
	client := godo.NewClient(oauthClient)
}
",4
RuneStar/client,Kotlin,"RuneStar Client for Old School RuneScape


Download
Browse Plugins
",50
MatterHackers/MatterControl,C#,"MatterControl




Master




Linux



Windows




MatterControl is an open-source program designed to control and enhance the desktop 3D printing experience. It's designed to help you get the most out of your 3D printer - making it easy to track, preview, and print your 3D parts. Development of MatterControl is sponsored by MatterHackers and it's partners.

Features

Integrated slicing engine MatterSlice
Library for managing your STL files
Built in profiles for a plethora of different printers.
Built in editing tools along with plugins for creating text, images, and braille.
Queue of items you are going to print, and history of items you have printed.
2D/3D preview of the sliced object.
Advanced printer controls, including the ability to make adjustments while printing.
Software based print leveling.
Remote monitoring of your printer, along with SMS/email notifications when your print is completed.

Download

Windows
Mac
Linux

Release Notes
Building from Source
MatterControl is written in C#. It uses the agg-sharp GUI abstraction layer. See this wiki article if you want to contribute code.


Checkout the latest source code and submodules:
 git clone --recursive https://github.com/MatterHackers/MatterControl.git
 cd MatterControl



Install MonoDevelop and Nuget.
 sudo apt-get install monodevelop nuget



Add Mono SSL Support - Copy in Mozilla Root certificates to enable NuGet and MatterControl SSL requests
 mozroots --import --sync



Restore NuGet packages - On MonoDevelop 4.0 or older you can install NuGet Addin. If you are on Mint, also install libmono-cairo2.0-cil. Alternatively you can run the command line NuGet application to restore the project packages:
 nuget restore MatterControl.sln



Optionally switch to a target branch
 git checkout master
 git submodule update --init --recursive

As a single command line statement:
 targetBranch=master && git checkout $targetBranch && git submodule update --init --recursive



Build MatterControl
 mdtool build -c:Release MatterControl.sln

or
 xbuild /p:Configuration=Release MatterControl.sln



Link the StaticData from your source directory to the build directory
 ln -s ../../StaticData bin/Release/StaticData



After MatterControl has been built in MonoDevelop it is recommended that you run the application via command line or via a shell script to invoke mono.
 mono bin/Release/MatterControl.exe

If you'd like to log errors for troubleshooting
 mono bin/Release/MatterControl.exe > log.txt

If you want detailed error logging and tracing
 MONO_LOG_LEVEL=debug mono bin/Release/MatterControl.exe > log.txt



In order for MatterControl to access the serial ports, you will need to give your user the appropriate permissions. On Debian based distros, add yourself to the dialout group. On Arch, add yourself the the uucp and lock groups instead.
 gpasswd -a $USER dialout



Serial Helper


Change to the SerialHelper directory
 cd Submodules/agg-sharp/SerialPortCommunication/SerialHelper



Run the build script
 ./build.sh



If your receive errors you may need to install libc6-dev-i386 for x86 compilation
 sudo apt-get install libc6-dev-i386



Help, Bugs, Feedback
For information on using MatterControl, check the MatterControl Wiki. If you have questions or feedback, feel free to post on the MatterHackers Forums or send an email to support@matterhackers.com. To report a bug, file an issue on GitHub.
",243
esp8266/Arduino,C,"Arduino core for ESP8266 WiFi chip
Quick links

Latest release documentation
Current ""git version"" documentation
Install git version (sources)

Arduino on ESP8266
This project brings support for ESP8266 chip to the Arduino environment. It lets you write sketches using familiar Arduino functions and libraries, and run them directly on ESP8266, no external microcontroller required.
ESP8266 Arduino core comes with libraries to communicate over WiFi using TCP and UDP, set up HTTP, mDNS, SSDP, and DNS servers, do OTA updates, use a file system in flash memory, work with SD cards, servos, SPI and I2C peripherals.
Contents

Installing options:

Using Boards Manager
Using git version
Using PlatformIO
Building with make


Documentation
Issues and support
Contributing
License and credits

Installing with Boards Manager
Starting with 1.6.4, Arduino allows installation of third-party platform packages using Boards Manager. We have packages available for Windows, Mac OS, and Linux (32 and 64 bit).

Install the current upstream Arduino IDE at the 1.8.7 level or later. The current version is at the Arduino website.
Start Arduino and open Preferences window.
Enter https://arduino.esp8266.com/stable/package_esp8266com_index.json into Additional Board Manager URLs field. You can add multiple URLs, separating them with commas.
Open Boards Manager from Tools > Board menu and install esp8266 platform (and don't forget to select your ESP8266 board from Tools > Board menu after installation).

Latest release 
Boards manager link: https://arduino.esp8266.com/stable/package_esp8266com_index.json
Documentation: https://arduino-esp8266.readthedocs.io/en/2.5.1/
Using git version (basic instructions)


Install the current upstream Arduino IDE at the 1.8 level or later. The current version is at the Arduino website.
Go to Arduino directory

For Mac OS X, it is Arduino.app showing as the Arduino icon.
This location may be your ~/Downloads, ~/Desktop or even /Applications.
cd <application-directory>/Arduino.app/Contents/Java

For Linux, it is ~/Arduino by default.
cd ~/Arduino



Clone this repository into hardware/esp8266com/esp8266 directory (or clone it elsewhere and create a symlink)

cd hardware
mkdir esp8266com
cd esp8266com
git clone https://github.com/esp8266/Arduino.git esp8266
cd esp8266
git submodule update --init

Download binary tools (you need Python 2.7)

cd esp8266/tools
python get.py

Restart Arduino

Using PlatformIO
PlatformIO is an open source ecosystem for IoT
development with cross platform build system, library manager and full support
for Espressif (ESP8266) development. It works on the popular host OS: macOS, Windows,
Linux 32/64, Linux ARM (like Raspberry Pi, BeagleBone, CubieBoard).

What is PlatformIO?
PlatformIO IDE
PlatformIO Core (command line tool)
Advanced usage -
custom settings, uploading to SPIFFS, Over-the-Air (OTA), staging version
Integration with Cloud and Standalone IDEs -
Cloud9, Codeanywhere, Eclipse Che (Codenvy), Atom, CLion, Eclipse, Emacs, NetBeans, Qt Creator, Sublime Text, VIM, Visual Studio, and VSCode
Project Examples

Building with make
makeEspArduino is a generic makefile for any ESP8266 Arduino project.
Using make instead of the Arduino IDE makes it easier to do automated and production builds.
Documentation
Documentation for latest development version: https://arduino-esp8266.readthedocs.io/en/latest/
Issues and support
ESP8266 Community Forum is a well established community for questions and answers about Arduino for ESP8266. If you need help, have a ""How do I..."" type question, have a problem with a 3rd party lib not hosted in this repo, or just want to discuss how to approach a problem , please ask there.
If you find the forum useful, please consider supporting it with a donation. 

If you encounter an issue which you think is a bug in the ESP8266 Arduino Core or the associated libraries, or if you want to propose an enhancement, you are welcome to submit it here on Github: https://github.com/esp8266/Arduino/issues.
Please provide as much context as possible, as well as the information requested in the issue template:

ESP8266 Arduino core version which you are using (you can check it in Boards Manager)
your sketch code; please wrap it into a code block, see Github markdown manual
when encountering an issue which happens at run time, attach serial output. Wrap it into a code block, just like the code.
for issues which happen at compile time, enable verbose compiler output in the IDE preferences, and attach that output (also inside a code block)
ESP8266 development board model
IDE settings (board choice, flash size)
etc

Contributing
For minor fixes of code and documentation, please go ahead and submit a pull request.
Check out the list of issues which are easy to fix — easy issues pending. Working on them is a great way to move the project forward.
Larger changes (rewriting parts of existing code from scratch, adding new functions to the core, adding new libraries) should generally be discussed by opening an issue first.
Feature branches with lots of small commits (especially titled ""oops"", ""fix typo"", ""forgot to add file"", etc.) should be squashed before opening a pull request. At the same time, please refrain from putting multiple unrelated changes into a single pull request.
License and credits
Arduino IDE is developed and maintained by the Arduino team. The IDE is licensed under GPL.
ESP8266 core includes an xtensa gcc toolchain, which is also under GPL.
Esptool written by Christian Klippel is licensed under GPLv2, currently maintained by Ivan Grokhotkov: https://github.com/igrr/esptool-ck.
Espressif SDK included in this build is under Espressif MIT License.
ESP8266 core files are licensed under LGPL.
SPI Flash File System (SPIFFS) written by Peter Andersson is used in this project. It is distributed under MIT license.
umm_malloc memory management library written by Ralph Hempel is used in this project. It is distributed under MIT license.
SoftwareSerial library and examples written by Peter Lerup. Distributed under LGPL 2.1.
axTLS library written by Cameron Rich, built from https://github.com/igrr/axtls-8266, is used in this project. It is distributed under BSD license.
BearSSL library written by Thomas Pornin, built from https://github.com/earlephilhower/bearssl-esp8266, is used in this project.  It is distributed under the MIT License.
",9422
cloudendpoints/esp,C++,"The Extensible Service Proxy
Extensible Service Proxy, a.k.a. ESP is a proxy which enables API management
capabilities for JSON/REST or gRPC API services. The current implementation is
based on an NGINX HTTP reverse proxy server.
ESP provides:


Features: authentication (auth0, gitkit), API key validation, JSON to gRPC
transcoding, as well as  API-level monitoring, tracing and logging. More
features coming in the near future: quota, billing, ACL, etc.


Easy Adoption: the API service can be implemented in any coding language
using any IDLs.


Platform flexibility: support the deployment on any cloud or on-premise
environment.


Superb performance and scalability: low latency and high throughput


ESP can Run Anywhere
However, the initial development was done on Google App Engine Flexible
Environment, GCE and GKE for API services using Open API
Specification and so our instructions
and samples are focusing on these platforms. If you make it work on other
infrastructure and IDLs please let us know and contribute instructions/code.
Prerequisites
Common prerequisites used irrespective of operating system and build tool
chain are:

Git
Node.js is required for running included example
Endpoints bookstore application.

Getting ESP
To download the Extensible Service Proxy source code, clone the ESP repository:
# Clone ESP repository
git clone https://github.com/cloudendpoints/esp

# Initialize Git submodules.
git -C esp submodule update --init --recursive

Released ESP docker images
ESP docker images are released regularly. The regular images are named as gcr.io/endpoints-release/endpoints-runtime:MAJOR_VERSION.MINOR_VERSION.PATCH_NUMBER. For example, gcr.io/endpoints-release/endpoints-runtime:1.30.0 has MAJOR_VERSION=1, MINOR_VERSION=30 and PATCH_NUMBER=0.
Symbolically linked images:

MAJOR_VERSION is linked to the latest image with same MAJOR_VERSION.

For example, gcr.io/endpoints-release/endpoints-runtime:1 is always pointed to the latest image with ""1"" major version.
Secure image:
Normally ESP container runs as root, it is deemed as not secure. To make ESP container secure, it should be run as non-root and its root file system should be read-only. Normal docker images can be made to run as non-root, but such change may break some existing users. Starting 1.31.0, a new secure image is built with suffix ""-secure"" in the image name, e.g. gcr.io/endpoints-release/endpoints-runtime-secure:1.31.0.  It will be run as non-root.
You can switch to use the secure images if the followings are satisfied:

Nginx is not listening on ports requiring root privilege (ports < 1024).
Not use custom nginx config. The server_config path is hard coded to /etc/nginx/ folder in the custom nginx config. The secure image moved the server_config to /home/nginx. Please modify your custom nginx config before using the secure image.

If some folders can be mounted externally, the root system can be made read-only. Please see this GKE deployment yaml file as example on how to make root system read-only.
Repository Structure

doc: Documentation
docker: Scripts for packaging ESP in a Docker image.
include: Extensible Service Proxy header files.
src: Extensible Service Proxy source.
google and third_party: Git submodules containing
dependencies of ESP, including NGINX.
script: Scripts used for build, test, and continuous integration.
test: Applications and client code used for end-to-end testing.
tools: Assorted tooling.
start_esp: A Python start-up script for the ESP proxy. The script includes a generic nginx configuration template and fetching logic to retrieve service configuration from Google Service Management service.

ESP Tutorial
To find out more about building, running, and testing ESP, please review

Build ESP on Ubuntu 16.04
ESP Tutorial
Testing ESP with Bazel
Run ESP on Kubernetes

Contributing
Your contributions are welcome. Please follow the contributor
guidlines.
",162
flutter/engine,C++,"Flutter Engine

Flutter is a new way to build high-performance, cross-platform mobile apps.
Flutter is optimized for today's, and tomorrow's, mobile devices. We are
focused on low-latency input and high frame rates on Android and iOS.
The Flutter Engine is a portable runtime for hosting
Flutter applications.  It implements Flutter's core
libraries, including animation and graphics, file and network I/O,
accessibility support, plugin architecture, and a Dart runtime and compile
toolchain. Most developers will interact with Flutter via the Flutter
Framework, which provides a modern,
reactive framework, and a rich set of platform, layout and foundation widgets.
If you are new to Flutter, then you will find more general information
on the Flutter project, including tutorials and samples, on our Web
site at Flutter.dev. For specific information
about Flutter's APIs, consider our API reference which can be found at
the docs.flutter.dev.
If you intend to contribute to Flutter, welcome! You are encouraged to
start with our contributor
guide,
which helps onboard new team members.
",2960
FISCO-BCOS/spring-boot-starter,Java,"English / 中文
Spring Boot Starter



The sample spring boot project is based on Web3SDK, which provides the basic framework and basic test cases for blockchain application and helps developers to quickly develop applications based on the FISCO BCOS blockchain. The version only supports FISCO BCOS 2.0.
Quickstart
Precodition
Build FISCO BCOS blockchain, please check out here。
Configuration
Download
$ git clone https://github.com/FISCO-BCOS/spring-boot-starter.git

Certificate Configuration
Copy the ca.crt, node.crt, and node.key files in the node's directory nodes/${ip}/sdk to the project's src/main/resources directory.
Settings
The application.yml of the spring boot project is shown below, and the commented content is modified according to the blockchain node configuration.
encryptType: 0  # 0:standard, 1:guomi
groupChannelConnectionsConfig:
  allChannelConnections:
  - groupId: 1  #group ID
    connectionsStr:
                    - 127.0.0.1:20200  # node listen_ip:channel_listen_port
                    - 127.0.0.1:20201
  - groupId: 2
    connectionsStr:
                    - 127.0.0.1:20202
                    - 127.0.0.1:20203
channelService:
  groupId: 1 # The specified group to which the SDK connects
  orgID: fisco # agency name
A detail description of the SDK configuration for the project, please checkout  here。
Run
Compile and run test cases:
$ ./gradlew build

When all test cases run successfully, it means that the blockchain is running normally,and the project is connected to the blockchain through the SDK. You can develop your blockchain application based on the project。
Test Case Introduction
The sample project provides test cases for developers to use. The test cases are mainly divided into tests for Web3j API, Precompiled Serveice API, Solidity contract file to Java contract file, deployment and call contract.
Web3j API Test
Provide Web3jApiTest class to test the Web3j API. The sample test is as follows:
@Test
public void getBlockNumber() throws IOException {
    BigInteger blockNumber = web3j.getBlockNumber().send().getBlockNumber();
    System.out.println(blockNumber);
    assertTrue(blockNumber.compareTo(new BigInteger(""0""))>= 0);
}

Tips: The Application class initializes the Web3j object, which can be used directly in the way where the business code needs it. The usage is as follows:
@Autowired
private Web3j web3j

Precompiled Service API Test
Provide PrecompiledServiceApiTest class to test the Precompiled Service API。The sample test is as follows:
@Test
public void testSystemConfigService() throws Exception {
    SystemConfigSerivce systemConfigSerivce = new SystemConfigSerivce(web3j, credentials);
    systemConfigSerivce.setValueByKey(""tx_count_limit"", ""2000"");
    String value = web3j.getSystemConfigByKey(""tx_count_limit"").send().getSystemConfigByKey();
    System.out.println(value);
    assertTrue(""2000"".equals(value));
}

Solidity contract file to Java contract file Test
Provide SolidityFunctionWrapperGeneratorTest class to test contract compilation. The sample test is as follows:
@Test
public void compileSolFilesToJavaTest() throws IOException {
    File solFileList = new File(""src/test/resources/contract"");
    File[] solFiles = solFileList.listFiles();

    for (File solFile : solFiles) {

        SolidityCompiler.Result res = SolidityCompiler.compile(solFile, true, ABI, BIN, INTERFACE, METADATA);
        System.out.println(""Out: '"" + res.output + ""'"");
        System.out.println(""Err: '"" + res.errors + ""'"");
        CompilationResult result = CompilationResult.parse(res.output);
        System.out.println(""contractname  "" + solFile.getName());
        Path source = Paths.get(solFile.getPath());
        String contractname = solFile.getName().split(""\\."")[0];
        CompilationResult.ContractMetadata a = result.getContract(solFile.getName().split(""\\."")[0]);
        System.out.println(""abi   "" + a.abi);
        System.out.println(""bin   "" + a.bin);
        FileUtils.writeStringToFile(new File(""src/test/resources/solidity/"" + contractname + "".abi""), a.abi);
        FileUtils.writeStringToFile(new File(""src/test/resources/solidity/"" + contractname + "".bin""), a.bin);
        String binFile;
        String abiFile;
        String tempDirPath = new File(""src/test/java/"").getAbsolutePath();
        String packageName = ""org.fisco.bcos.temp"";
        String filename = contractname;
        abiFile = ""src/test/resources/solidity/"" + filename + "".abi"";
        binFile = ""src/test/resources/solidity/"" + filename + "".bin"";
        SolidityFunctionWrapperGenerator.main(Arrays.asList(
                ""-a"", abiFile,
                ""-b"", binFile,
                ""-p"", packageName,
                ""-o"", tempDirPath
        ).toArray(new String[0]));
    }
    System.out.println(""generate successfully"");
}

This test case converts all Solidity contract files (HelloWorld contract provided by default) in the src/test/resources/contract directory to the corresponding abi and bin files, and save them in the src/test/resources/solidity directory. Then convert the abi file and the corresponding bin file combination into a Java contract file, which is saved in the src/test/java/org/fisco/bcos/temp directory. The SDK will use the Java contract file for contract deployment and invocation.
Deployment and Invocation Contract Test
Provide ContractTest class to test deploy and call contracts. The sample test is as follows:
@Test
public void deployAndCallHelloWorld() throws Exception {
    //deploy contract
    HelloWorld helloWorld = HelloWorld.deploy(web3j, credentials, new StaticGasProvider(gasPrice, gasLimit)).send();
    if (helloWorld != null) {
        System.out.println(""HelloWorld address is: "" + helloWorld.getContractAddress());
        //call set function
        helloWorld.set(""Hello, World!"").send();
        //call get function
        String result = helloWorld.get().send();
        System.out.println(result);
        assertTrue( ""Hello, World!"".equals(result));
    }
}

Developing & Contributing

Star our Github.
Pull requests. See CONTRIBUTING.
Ask questions.
Discuss in WeChat group  or Gitter.

Related Links

For FISCO BCOS project, please check out FISCO BCOS Documentation。
For Web3SDK project, please check out Web3SDK Documentation。
For Spring Boot applications, please check out Spring Boot。

Community
By the end of 2018, Financial Blockchain Shenzhen Consortium (FISCO) has attracted and admitted more than 100 members from 6 sectors including banking, fund management, securities brokerage, insurance, regional equity exchanges, and financial information service companies. The first members include the following organizations: Beyondsoft, Huawei, Shenzhen Securities Communications, Digital China, Forms Syntron, Tencent, WeBank, Yuexiu FinTech.


Join our WeChat 


Discuss in 


Read news by 


Mail us at 


",13
percyliang/refdb,Ruby,"This package manages a shared database of bibliography references which can be
used to generate bibtex (to include in your paper) and HTML files (for
your publications page).
If you're in a hurry and just want a bib file, here you go:
https://raw.githubusercontent.com/percyliang/refdb/master/all.bib

Adding new entries to the database
To add a new entry, append to data/<username>.rb, for example:
entry!('liang11dcs',
  author('Percy Liang and Michael I. Jordan and Dan Klein'),
  title('Learning Dependency-Based Compositional Semantics'),
  acl(2011),
  pages(590, 599),
nil)

Fields like author, title, and pages are what you'd expect from bibtex.
You can also use macros such as acl(2011) (defined in data/venues.rb) to
make it easier to type and to maintain consistency.  Consistency of
capitalization and duplicate entries are automatically checked when you run
./generate.rb.
You can also import from existing bibtex (either from a file or stdin):
./import.rb

Paste in your bibtex format and the corresponding Ruby code will be appended to
data/<username>.rb.
Note: when you have finished pasting your bibtex at the command line, type
Ctrl-D to terminate stdin. The script will then continue.
There might be errors so it's a good idea to double check
what's been added.
After you do this, make sure you rebuild all.bib:
make

Then git add data/<username>.rb if necessary.  To commit your changes, do
git commit -am ""add"", git push or make a pull request.
Printing/querying the database
To output a bibtex file:
./generate.rb bib out=all.bib

To output an HTML file:
./generate.rb html out=all.html

To filter the entries:
./generate.rb html author='Percy Liang' title=Publications out=pliang.html
./generate.rb bib search='hidden markov model'
./generate.rb bib tags='semantic parsing'

",21
opnfv/nfvbench,Python,"NFVbench: A Network Performance Benchmarking Tool for NFVi Full Stacks
The NFVbench tool provides an automated way to measure the network performance for the most common data plane packet flows
on any NFVi system viewed as a black box (NFVi Full Stack).
An NFVi full stack exposes the following interfaces:
- an OpenStack API for those NFVi platforms based on OpenStack
- an interface to send and receive packets on the data plane (typically through top of rack switches

while simpler direct wiring to a looping device would also work)
The NFVi full stack does not have to be supported by the OPNFV ecosystem and can be any functional OpenStack system that provides
the above interfaces.
NFVbench can also be used without OpenStack on any networking device that can handle L2 forwarding or L3 routing.
NFVbench can be installed standalone (in the form of a single Docker container) and is fully functional without
the need to install any other OPNFV framework.
It is designed to be easy to install and easy to use by non experts (no need to be an expert in traffic generators and data plane
performance benchmarking).

Online Documentation
The latest version of the NFVbench documentation is available online at:
https://opnfv-nfvbench.readthedocs.io/en/latest/testing/user/userguide/index.html

Contact Information
Inquiries and questions: send an email to opnfv-tech-discuss@lists.opnfv.org with a Subject line starting with ""#nfvbench""
Open issues or submit an issue or enhancement request: https://jira.opnfv.org/projects/NFVBENCH/issues (this requires an OPNFV Linux Foundation login).
",11
sveinbjornt/searchfs,Objective-C,"searchfs

searchfs is a Mac command line tool to quickly search by filename on entire APFS and HFS+ volumes. Searching takes place at the driver level using the file system catalog. This means the volume's directory tree can be scanned much faster than with a standard recursive filename search using find.
Search is case-insensitive by default. Matching files are printed to standard output in the order they are found in the catalog. See the man page for details.
KatSearch is a graphical application built on top of searchfs.
Download

⇩ Download latest searchfs binary (v0.3, <20 KB, Intel 64-bit, macOS 10.7 or later)
searchfs man page (HTML)

Build & Install

git clone https://github.com/sveinbjornt/searchfs.git
cd searchfs
make
make install

Installs binary into /usr/local/bin/. Man page goes into /usr/local/share/man/man1/.
Performance
According to my benchmarks, searchfs runs about 35-50% faster than find on APFS filesystems and many times faster on HFS+.
The following are benchmark results on a 2012 Retina MacBook Pro with an Apple-supplied 512 GB SSD running an APFS file system containing about 2 million files:
searchfs
$ time searchfs ""something""
0,01s user 33,15s system 32% cpu 1:23,59 total
find
$ time find / -name ""*something*""
9,53s user 67,64s system 49% cpu 2:37,39 total
Although I have yet to test this properly, searchfs is probably much faster than find on hard disk drives, which have higher seek times. It is also very fast indeed on file systems with a small number of files.
History
Apple added file system catalog search to Mac OS with the introduction of the Hiearchical File System (HFS) back in 1985. HFS replaced the previous flat table structure in the old MFS file system with a catalog file using a B-tree structure. Unlike Windows' FAT file system, HFS (and later, HFS+) thus arranged the entire directory tree into one large file on the disk, with interlinked nodes that did not match the hierarchical folder structure. This meant that volumes could be searched very quickly regardless of size.
The Classic Mac OS exposed this functionality via the FSCatalogSearch() function, which iterated efficiently over the nodes, thus minimizing disk seek times. In the pre-SSD era, this gave the Mac a significant performance advantage over Windows when it came to full-volume search. For a long time, FSCatalogSearch continued to be available in Mac OS X / macOS via the Carbon APIs but it has now been deprecated and does not support APFS, Apple's new file system.
However, catalog search for both HFS+ and APFS is available in Darwin's low-level system libraries via the searchfs() function. The searchfs program makes use of this function.
TODO

The searchfs API supports searching the catalog for files based on size, owner, group, creation, modification or access date, finder flags, deprecated old-school file and creator types, and so on. Add that.

Version History
11/05/2019 - 0.3

New -l flag lists all mounted volumes that support catalog search.
Volume to search can now be specified via device name as well as mount path.
Regex modifiers ^ and $ can now be used to match only at the start or end of a filenames.

26/04/2019 - 0.2

Fixed issue which prevented searchfs from working on older versions of macOS.
Now fails silently when path lookup fails for a file system object ID.
Now runs on macOS 10.7 or later.

14/07/2018 - 0.1

Initial release

BSD License
Copyright © 2017-2019 Sveinbjorn Thordarson <sveinbjorn@sveinbjorn.org>
Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:


Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.


Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.


Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.


THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
",26
whonore/Coqtail,Python,"Coqtail


Interactive Coq Proofs in Vim
Coqtail enables interactive Coq proof development in Vim similar to
CoqIDE
or ProofGeneral.
It supports:

Coq 8.4 - 8.9
Having multiple Coq buffers open
Python 2 and 3

Installation and Requirements
Using
pathogen:
cd ~/.vim/bundle && git clone https://github.com/whonore/coqtail.git

Using
Vundle:
Plugin 'whonore/coqtail' (in .vimrc)
:PluginInstall

Using
VimPlug:
Plug 'whonore/coqtail' (in .vimrc)
:PlugInstall

Coqtail requires:

Vim compiled with either +python or +python3
vimbufsync
Coq 8.4,
8.5,
8.6,
8.7,
8.8, or
8.9

If you are using pathogen, documentation can be generated by
:call pathogen#helptags(). Alternatively, you can do :helptags {path-to-coqtail}/doc.
Usage
Coqtail provides the following commands (see :help coqtail for more details):
Starting and Stopping



Command
Mapping
Description




CoqStart
<leader>cc
Launch Coqtail for the current buffer


CoqStop
<leader>cq
Quit Coqtail for the current buffer




Movement



Command
Mapping
Description




CoqNext
<leader>cj
Check the next sentence with Coq


CoqUndo
<leader>ck
Step back one sentence


CoqToCursor
<leader>cl
Check/rewind all sentences up to the cursor position


CoqToTop
<leader>cT
Rewind to the beginning of the file


CoqJumpToEnd
<leader>cG
Move the cursor to the end of the checked region


CoqGotoDef <arg>
<leader>cg
Populate the quickfix list with possible locations of the definition of <arg>




Queries



Command
Mapping
Description




Coq <args>

Send arbitrary queries to Coq (e.g. Check, About, Print, etc.)


Coq Check <arg>
<leader>ch
Show the type of <arg> (the mapping will use the term under the cursor)


Coq About <arg>
<leader>ca
Show information about <arg>


Coq Print <arg>
<leader>cp
Show the definition of <arg>


Coq Locate <arg>
<leader>cf
Show where <arg> is defined


Coq SearchAbout <args>
<leader>cs
Show theorems about <args>



The mappings shown above are set by default, but you can disable them all and
define your own by setting g:coqtail_nomap = 1 in your .vimrc.
Alternatively, you can choose to only remap specific commands and the defaults
will still be used for the rest.
Coqtail also comes with an ftdetect script for Coq, as well as modified
versions of Vincent Aravantinos'
syntax and
index scripts for Coq.
These scripts are used by default but can be disabled by setting
g:coqtail_nosyntax = 1 and g:coqtail_noindent = 1 respectively.
Thanks
Parts of Coqtail were originally inspired by/adapted from
Coquille
(MIT License, Copyright (c) 2013, Thomas Refis).
",22
rench/shoutao,HTML,"shoutao
shoutao
node-canvas
Compiling
If you don't have a supported OS or processor architecture, or you use --build-from-source, the module will be compiled on your system. This requires several dependencies, including Cairo and Pango.
For detailed installation information, see the wiki. One-line installation instructions for common OSes are below. Note that libgif/giflib, librsvg and libjpeg are optional and only required if you need GIF, SVG and JPEG support, respectively. Cairo v1.10.0 or later is required.



OS
Command




OS X
Using Homebrew:brew install pkg-config cairo pango libpng jpeg giflib librsvg


Ubuntu
sudo apt-get install build-essential libcairo2-dev libpango1.0-dev libjpeg-dev libgif-dev librsvg2-dev


Fedora
sudo yum install gcc-c++ cairo-devel pango-devel libjpeg-turbo-devel giflib-devel


Solaris
pkgin install cairo pango pkg-config xproto renderproto kbproto xextproto


OpenBSD
doas pkg_add cairo pango png jpeg giflib


Windows
See the wiki


Others
See the wiki



Mac OS X v10.11+: If you have recently updated to Mac OS X v10.11+ and are experiencing trouble when compiling, run the following command: xcode-select --install. Read more about the problem on Stack Overflow.
If you have xcode 10.0 or higher installed, in order to build from source you need NPM 6.4.1 or higher.
",3
Brewskey/Brewskey.App,JavaScript,"Brewskey.App
Development

Clone this repository
Install Node/NPM
npm i -g react-native-cli
Navigate to the project from command line
Run npm install

",2
daverodal/wargaming,PHP,"
WebWargaming Markup
This is a clone of the Javascript code found at
http://www.webwargaming.org/
It's being released under the license found in the code
This has since become a large php/composer/laravel project. Supporting many games of
different historical eras.
It connects via guzzelHttp to couchDB on the back end.
It's multiplayer, no computer AI.
The origin copyright was from Mark Butler below.
Copyright (c) 2008-2011 Mark Butler
This program is free software; you can redistribute it
and/or modify it under the terms of the GNU General Public License
as published by the Free Software Foundation;
either version 2 of the License, or (at your option) any later version.

Copyright (c) 2012-2016 David Rodal
",2
cp3402-students/a2-cp3402-2019-team20,PHP,"CoffeeCan Wordpress Site
Contents of this readme

Prerequisites
Setting up a local environment
Theme Details
Workflow
Wordpress
Contributors
Dev Diaries

Prerequisites

Vagrant

Setting up a local environment
Installing the environment:

Navigate to the directory where you want your local environment
Open a Git Bash terminal in that directory (Can right click and select from context menu if Git is installed)
Run the command git clone https://github.com/flurinduerst/WPDistillery.git cp3402_20
Run the command vagrant up and wait for it to finish installing
Navigate to the URL 192.168.33.10/wp-admin and login (Username/Password: admin)
Navigate to Plugins -> Add new and search for All-in-One WP Migration and install it

Setting up the environment:

Open a Git Bash terminal in the Public directory
Run the command touch .htaccess
Run the command nano .htaccess
Add these 5 lines of code within the IfModule block
php_value upload_max_filesize 128M
php_value post_max_size 128M
php_value memory_limit 256M
php_value max_execution_time 300`
php_value max_input_time 300


Save (Ctrl + X Then Y)

Putting the staging site into local:

Navigate to the staging server and export the site as a file using the migration plugin (Located in left nav)
Navigate to 192.168.33.10/wp-admin and import the file using the migration plugin
Run the command rm -r wp-content in a Git Bash terminal within the Public directory
Run the command git init
Run the command git remote add origin https://github.com/cp3402-students/a2-cp3402-2019-team20.git
Run the command rm -r index.php
Run the command git pull origin master

That's it! You should now have a local environment replica of the staging site that is connected to this repository
Theme Details:
To Be Added
Workflow:
Development:

PHP Storm

Collaboration:

Slack + Github Integration
Trello + Slack Integration
Github

Deployment:

Git command line
Webhooks
Wordmove

Changes to staging:
Content changes:

Work on staging server backend directly

Theme changes:

Commit to this repository
Webhooks automatically pull in changes

Changes to production:



Wordpress:

Version: 5.0.4
Themes: Underscores & CoffeeCan (Developed for website)
Staging Site: 178.128.223.156
Production Site: 128.199.252.211

Deployment tools setup:

Instructions on how to setup webhooks can be found Here
Instructions on using wordmove can be found Here

Contributors:

Craig Morris - Theme Design
Matthew Lewandowski - Theme Design
Nicholas Barty - Content Design
Kyle Ormonde - Content Design
Peter Menzel - Site Features / PHP

Dev Diaries
These diaries are for team members who's work does not involve pushing to github and serve as a means of keeping track of contributions to the project

Nicholas Barty Dev Diary
Kyle Ormonde Dev Diary

",2
collin80/can_common,C++,"can_common
A set of shared structures, classes, and functionality that multiple CAN libraries
might need. Created to unify the due_can and mcp2515 libraries on the Due such that
both can be used interchangeably. This library isn't solely responsible for this
but does include CAN frame structures and other common functionality.
",14
WildGums/CsvTextEditor,C#,"CsvTextEditor
Is a free, simple to use csv text editor application.
It is built with:

Orchestra
Orc.CsvTextEditor
AvalonEdit
Loosely implements Elastic-tabstops in the background (i.e. instead of tabstops we use commas as the separators).

We have purposely kept the control simple.
We welcome all pull requests, however please discuss large feature implementations ahead of time.
Download
Download the latest version from the release page
Requirements
.NET 4.6.2 or above
Goal
We work with a lot of csv files on a daily basis and wanted a no-nonsense, quick and simple tool to edit csv files.
We found that Excel reformats csv files  when saving a file, which causes us a lot of issues. (Like changing date formats and stripping out leading '0').
Essentially CsvTextEditor is a simple ""text editor"" with some extra features that make it easy to edit csv files and will not reformat the file in anyway.
Limitations
Please read this section carefully.
Speed
The control is effective with relatively small csv files. It is not designed to deal with large files with more than 100K rows or over 1MB in size.
Our use case is typically 20 columns (or less) and a few hundred rows.
We would welcome any efforts to improve the performance if someone was willing to tackle this issue.
Csv format
We only support simple comma separated files. (i.e. we expect the csv files to be fairly clean and do not support quotes, imbedded commas or new lines within the text.)
We would also welcome a PR that allowed CsvTextEditor to handle more cases.
Features


All the features available in AvalonEdit are also available in CsvTextEditor


Display the columns using elastic formating


Line and column number are in the status bar (bottom right corner)


Highlight word (Just select some characters or double click on a word and the same occurrences will be highlighted.)


Easy navigation between ""cells"" (Arrows, Tab, SHIFT + Tab)


Undo/Redo


Column widths will automatically re-adjust after editing


Syntax highlighting (Numbers are shown in blue font)


Automatically highlight ""True/False"" and  ""Yes/No"" values


Word hints based on existing values in the column will be shown as you type


ShortCut Keys:

Delete lines (""CTRL +L"")
Duplicate lines (""CTRL + D"")
Add columns ("","")
Delete columns (""CTL + ,"")
Search and replace (""CTRL + F"") (Search will also highlight all occurrences in the file)



Commands:

Remove duplicate rows
Remove blank lines
Trim (remove whitespaces from the beginning or end of) cell values



Screenshots



License
MIT License
",33
haskell-works/hw-simd,Haskell,"hw-simd


",8
microsoftgraph/microsoft-graph-docs.pt-BR,PowerShell,"Documentação do Microsoft Graph
Agradecemos seu interesse na documentação do Microsoft Graph! Para ter a melhor experiência, recomendamos examinar esse conteúdo no Portal do desenvolvedor do Microsoft Graph.
Envie-nos os seus comentários
Seus comentários são importantes para nós.

Para nos informar sobre dúvidas ou problemas encontrados na documentação, envie uma questão neste repositório.
Recomendamos que você faça a bifurcação, a correção e uma solicitação de pull das suas alterações propostas. Confira CONTRIBUTING.md para obter as diretrizes de contribuição.
Para nos informar sobre sua experiência de programação, o que você gostaria de ver em versões futuras, exemplos de código, entre outros, insira suas sugestões e ideias no UserVoice da Plataforma do Desenvolvedor do Office.

Código de conduta
Este projeto adotou o Código de Conduta do Código Aberto da Microsoft. Para saber mais, confira as Perguntas frequentes do Código de Conduta ou contate opencode@microsoft.com se tiver outras dúvidas ou comentários.
",2
openml/openml-tutorial,HTML,"OpenML Tutorial


Learn how to use OpenML for reproducible and collaborative machine learning projects
Tutorial website
To prepare for the tutorial, please check the Prerequisites in the Python API Tutorial
All materials are CC-0 licenced. You can use them however you like.
",3
microsoftgraph/microsoft-graph-docs.pt-BR,PowerShell,"Documentação do Microsoft Graph
Agradecemos seu interesse na documentação do Microsoft Graph! Para ter a melhor experiência, recomendamos examinar esse conteúdo no Portal do desenvolvedor do Microsoft Graph.
Envie-nos os seus comentários
Seus comentários são importantes para nós.

Para nos informar sobre dúvidas ou problemas encontrados na documentação, envie uma questão neste repositório.
Recomendamos que você faça a bifurcação, a correção e uma solicitação de pull das suas alterações propostas. Confira CONTRIBUTING.md para obter as diretrizes de contribuição.
Para nos informar sobre sua experiência de programação, o que você gostaria de ver em versões futuras, exemplos de código, entre outros, insira suas sugestões e ideias no UserVoice da Plataforma do Desenvolvedor do Office.

Código de conduta
Este projeto adotou o Código de Conduta do Código Aberto da Microsoft. Para saber mais, confira as Perguntas frequentes do Código de Conduta ou contate opencode@microsoft.com se tiver outras dúvidas ou comentários.
",2
openml/openml-tutorial,HTML,"OpenML Tutorial


Learn how to use OpenML for reproducible and collaborative machine learning projects
Tutorial website
To prepare for the tutorial, please check the Prerequisites in the Python API Tutorial
All materials are CC-0 licenced. You can use them however you like.
",3
bg1bgst333/Sample,Java,"Sample
Sample Program
",4
NervanaSystems/ngraph,C++,"
 


Architecture & features | Ecosystem | Release notes |  Documentation |  Contribution guide


Quick start
To begin using nGraph with popular frameworks to accelerate deep learning
workloads on CPU for inference, please refer to the links below.



Framework (Version)
Installation guide
Notes




TensorFlow* 1.13.1
Pip install or Build from source
20 Validated workloads


MXNet* 1.3
Pip install or Build from source
18 Validated workloads


ONNX 1.4
Pip install
17 Validated workloads



Python wheels for nGraph
The Python wheels for nGraph have been tested and are supported on the following
64-bit systems

Ubuntu 16.04 or later
CentOS 7.6
Debian 10
macOS 10.14.3 (Mojave)

Frameworks using nGraph Compiler stack to execute workloads have shown
up to 45X
performance boost when compared to native framework implementations. We've also
seen performance boosts running workloads that are not included on the list of
Validated workloads, thanks to nGraph's powerful subgraph pattern matching.
Additionally we have integrated nGraph with PlaidML to provide deep learning
performance acceleration on Intel, nVidia, & AMD GPUs. More details on current
architecture of the nGraph Compiler stack can be found in Architecture and features,
and recent changes to the stack are explained in Release Notes.
What is nGraph Compiler?
nGraph Compiler aims to accelerate developing AI workloads using any deep learning
framework and deploying to a variety of hardware targets. We strongly believe in
providing freedom, performance, and ease-of-use to AI developers.
The diagram below shows deep learning frameworks and hardware targets
supported by nGraph. NNP-L and NNP-I in the diagram refer to Intel's next generation
deep learning accelerators: Intel® Nervana™ Neural Network Processor for Learning and
Inference respectively.  Future plans for supporting addtional deep learning frameworks
and backends are outlined in the ecosystem section.

While the ecosystem shown above is all functioning, we have validated
performance for deep learning inference on CPU processors, such as Intel® Xeon®
for the Beta release of nGraph. The Gold release is targeted for June 2019; it
will feature broader workload coverage including quantized graphs (int8) and
will implement support for dynamic shapes.
Our documentation has extensive information about how to use nGraph Compiler
stack to create an nGraph computational graph, integrate custom frameworks,
and to interact with supported backends. If you wish to contribute to the
project, please don't hesitate to ask questions in GitHub issues after
reviewing our contribution guide below.
How to contribute
We welcome community contributions to nGraph. If you have an idea how
to improve it:

See the contrib guide for code formatting and style guidelines.
Share your proposal via GitHub issues.
Ensure you can build the product and run all the examples with your patch.
In the case of a larger feature, create a test.
Submit a pull request.
Make sure your PR passes all CI tests. Note: our Travis-CI service
runs only on a CPU backend on Linux. We will run additional tests
in other environments.
We will review your contribution and, if any additional fixes or
modifications are necessary, may provide feedback to guide you. When
accepted, your pull request will be merged to the repository.

",954
haskell-works/hw-rankselect,Haskell,"hw-rankselect


Efficient rank and select operations on large bit-vectors based on the paper
""Space-Efficient, High-Performance Rank & Select Structures on Uncompressed Bit
Sequences"".  This library does not yet implement the full cspoppy implementation.
Notably, it still uses the sub-optimal ""Straw man"" design for ""Combined Sampling""
on page 10.
This library will use support for some BMI2 CPU instructions on some x86 based
CPUs if compiled with the appropriate flags on ghc-8.4.1 or later.
Rank and select
This library provides the following functions on indexed bit-vectors:

rank1
rank0
select1
select0

The supported indexed bit-vector types are:

Poppy512
CsPoppy

Constructing and using an indexed bit-vector in the repl
The example below constructs an indexed bit-vector from a string and runs
rank and select query operations on it.  The bits in a string are in
little-endian and can be of arbitrary length.  The resulting bit-vector
will be padded with 0-bits until the next 64-bit boundary.
$ stack repl --package hw-rankselect --flag bits-extra:bmi2 --flag hw-rankselect-base:bmi2 --flag hw-rankselect:bmi2
λ> import Data.Maybe
λ> import HaskellWorks.Data.Bits.BitRead
λ> import HaskellWorks.Data.RankSelect.Base.Rank1
λ> import HaskellWorks.Data.RankSelect.Base.Select1
λ> import HaskellWorks.Data.RankSelect.CsPoppy
λ> let bs = fromJust $ bitRead ""10010010"" :: CsPoppy
bs :: CsPoppy
λ> select1 bs 1
1
λ> select1 bs 2
4
λ> select1 bs 3
7
λ> rank1 bs 7
3
λ> rank1 bs 4
2
λ> rank1 bs 1
1

A valid bit string contains zero or more characters.  Characters other than 1 and 0 are
permitted, but are ignored.  For example spaces can be used to group bits for clarity.
λ> let bs = fromJust $ bitRead """" :: CsPoppy
bs :: CsPoppy
λ> let bs = fromJust $ bitRead ""10010010 10010010"" :: CsPoppy
bs :: CsPoppy

Whilst the use of a bit string is convenient for the repl, for performance reasons, it
is more typical to construct an indexed bit-vector from a 64-bit word vector:
> import qualified Data.Vector.Storable as DVS
λ> let bs = makeCsPoppy (DVS.fromList [23, 73, 55])
bs :: CsPoppy

Working with files
Bit strings are stored in files as a string of bits (little-endian, which is native for
Intel platforms) padded to the nearest word8 (byte) without any additional structure.
Query such a structure directly is slow, so it is possible to load it into memory by
way of memory mapping then constructing an additional Rank-Select-Bit-String index.
The following code shows how to query such bit vectors and run simple queries:
λ> import Data.Maybe
λ> import HaskellWorks.Data.Bits.BitRead
λ> import qualified HaskellWorks.Data.FromForeignRegion as IO
λ> v :: CsPoppy <- IO.mmapFromForeignRegion ""data/sample-000.idx""
λ> rank1 v 100
8
λ> select1 v 8
95
Here the
Compilation
It is sufficient to build, test and benchmark the library as follows
for basic performance.  The library will be compiled to use broadword
implementation of rank & select, which has reasonable performance.
stack build
stack test
stack bench

To target the BMI2 instruction set, add the bmi2 flag:
stack build --flag bits-extra:bmi2 --flag hw-rankselect-base:bmi2 --flag hw-rankselect:bmi2
stack test  --flag bits-extra:bmi2 --flag hw-rankselect-base:bmi2 --flag hw-rankselect:bmi2
stack bench --flag bits-extra:bmi2 --flag hw-rankselect-base:bmi2 --flag hw-rankselect:bmi2

Benchmark results
The following benchmark shows the kinds of performance gain that can
be expected from enabling the BMI2 instruction set for CPU targets
that support them.  Benchmarks were run on 2.9 GHz Intel Core i7,
macOS High Sierra.
With BMI2 disabled:
benchmarking data/example.ib/CsPoppy Select1
time                 3.341 μs   (3.312 μs .. 3.385 μs)
                     0.999 R²   (0.998 R² .. 1.000 R²)
mean                 3.357 μs   (3.331 μs .. 3.403 μs)
std dev              109.2 ns   (81.13 ns .. 151.9 ns)
variance introduced by outliers: 42% (moderately inflated)

With BMI2 enabled:
benchmarking data/example.ib/CsPoppy Select1
time                 1.907 μs   (1.902 μs .. 1.911 μs)
                     1.000 R²   (1.000 R² .. 1.000 R²)
mean                 1.906 μs   (1.901 μs .. 1.911 μs)
std dev              17.94 ns   (14.44 ns .. 21.87 ns)

References

Space-Efficient, High-Performance Rank & Select Structures on Uncompressed Bit Sequences

",17
paritytech/substrate,Rust,"Substrate

Table of Contents

1. Intro in one sentence
2. Description
3. Usage

3.1. The Basics of Substrate
3.2. Extrinsics
3.3. Runtime and API
3.4. Inherent Extrinsics
3.5. Block-authoring Logic


4. Roadmap

4.1. So far
4.2. In progress
4.3. The future


5. Trying out Substrate Node

5.1. On Mac and Ubuntu


6. Building

6.1. Hacking on Substrate
6.2. Joining the Flaming Fir Testnet
6.3. Joining the Emberic Elm Testnet


7. Documentation

7.1. Viewing documentation for Substrate packages
7.2. Contributing to documentation for Substrate packages
7.3. Contributing to documentation (tests, extended examples, macros) for Substrate packages


8. Contributing

8.1. Contributing Guidelines
8.2. Contributor Code of Conduct


9. License



1. Intro in one sentence


Substrate is a next-generation framework for blockchain innovation.




2. Description


At its heart, Substrate is a combination of three technologies: WebAssembly, Libp2p and GRANDPA Consensus. About GRANDPA, see this definition, introduction and formal specification. It is both a library for building new blockchains and a ""skeleton key"" of a blockchain client, able to synchronize to any Substrate-based chain.


Substrate chains have three distinct features that make them ""next-generation"": a dynamic, self-defining state-transition function; light-client functionality from day one; and a progressive consensus algorithm with fast block production and adaptive, definite finality. The STF, encoded in WebAssembly, is known as the ""runtime"". This defines the execute_block function, and can specify everything from the staking algorithm, transaction semantics, logging mechanisms and procedures for replacing any aspect of itself or of the blockchain’s state (""governance""). Because the runtime is entirely dynamic all of these can be switched out or upgraded at any time. A Substrate chain is very much a ""living organism"".


See also https://www.parity.io/what-is-substrate/.




3. Usage


Substrate is still an early stage project, and while it has already been used as the basis of major projects like Polkadot, using it is still a significant undertaking. In particular, you should have a good knowledge of blockchain concepts and basic cryptography. Terminology like header, block, client, hash, transaction and signature should be familiar. At present you will need a working knowledge of Rust to be able to do anything interesting (though eventually, we aim for this not to be the case).


Substrate is designed to be used in one of three ways:




Trivial: By running the Substrate binary substrate and configuring it with a genesis block that includes the current demonstration runtime. In this case, you just build Substrate, configure a JSON file and launch your own blockchain. This affords you the least amount of customisability, primarily allowing you to change the genesis parameters of the various included runtime modules such as balances, staking, block-period, fees and governance.


Modular: By hacking together modules from the Substrate Runtime Module Library into a new runtime and possibly altering or reconfiguring the Substrate client’s block authoring logic. This affords you a very large amount of freedom over your own blockchain’s logic, letting you change datatypes, add or remove modules and, crucially, add your own modules. Much can be changed without touching the block-authoring logic (since it is generic). If this is the case, then the existing Substrate binary can be used for block authoring and syncing. If the block authoring logic needs to be tweaked, then a new altered block-authoring binary must be built as a separate project and used by validators. This is how the Polkadot relay chain is built and should suffice for almost all circumstances in the near to mid-term.


Generic: The entire Substrate Runtime Module Library can be ignored and the entire runtime designed and implemented from scratch. If desired, this can be done in a language other than Rust, providing it can target WebAssembly. If the runtime can be made to be compatible with the existing client’s block authoring logic, then you can simply construct a new genesis block from your Wasm blob and launch your chain with the existing Rust-based Substrate client. If not, then you’ll need to alter the client’s block authoring logic accordingly. This is probably a useless option for most projects right now, but provides complete flexibility allowing for a long-term far-reaching upgrade path for the Substrate paradigm.




3.1. The Basics of Substrate

Substrate is a blockchain platform with a completely generic state transition function. That said, it does come with both standards and conventions (particularly regarding the Runtime Module Library) regarding underlying data structures. Roughly speaking, these core datatypes correspond to +trait+s in terms of the actual non-negotiable standard and generic +struct+s in terms of the convention.



Header := Parent + ExtrinsicsRoot + StorageRoot + Digest
Block := Header + Extrinsics + Justifications




3.2. Extrinsics

Extrinsics in Substrate are pieces of information from ""the outside world"" that are contained in the blocks of the chain. You might think ""ahh, that means transactions"": in fact, no. Extrinsics fall into two broad categories of which only one is transactions. The other is known as inherents. The difference between these two is that transactions are signed and gossiped on the network and can be deemed useful per se. This fits the mold of what you would call transactions in Bitcoin or Ethereum.


Inherents, meanwhile, are not passed on the network and are not signed. They represent data which describes the environment but which cannot call upon anything to prove it such as a signature. Rather they are assumed to be ""true"" simply because a sufficiently large number of validators have agreed on them being reasonable.


To give an example, there is the timestamp inherent, which sets the current timestamp of the block. This is not a fixed part of Substrate, but does come as part of the Substrate Runtime Module Library to be used as desired. No signature could fundamentally prove that a block were authored at a given time in quite the same way that a signature can ""prove"" the desire to spend some particular funds. Rather, it is the business of each validator to ensure that they believe the timestamp is set to something reasonable before they agree that the block candidate is valid.


Other examples include the parachain-heads extrinsic in Polkadot and the ""note-missed-proposal"" extrinsic used in the Substrate Runtime Module Library to determine and punish or deactivate offline validators.



3.3. Runtime and API

Substrate chains all have a runtime. The runtime is a WebAssembly ""blob"" that includes a number of entry-points. Some entry-points are required as part of the underlying Substrate specification. Others are merely convention and required for the default implementation of the Substrate client to be able to author blocks.


If you want to develop a chain with Substrate, you will need to implement the Core trait. This Core trait generates an API with the minimum necessary functionality to interact with your runtime. A special macro is provided called impl_runtime_apis! that help you implement runtime API traits. All runtime API trait implementations need to be done in one call of the impl_runtime_apis! macro. All parameters and return values need to implement parity-codec to be encodable and decodable.


Here’s a snippet of the Polkadot API implementation as of PoC-3:



impl_runtime_apis! {
	impl client_api::Core<Block> for Runtime {
		fn version() -> RuntimeVersion {
			VERSION
		}

		fn execute_block(block: Block) {
			Executive::execute_block(block)
		}

		fn initialize_block(header: <Block as BlockT>::Header) {
			Executive::initialize_block(&header)
		}
	}
	// ---snip---
}




3.4. Inherent Extrinsics

The Substrate Runtime Module Library includes functionality for timestamps and slashing. If used, these rely on ""trusted"" external information being passed in via inherent extrinsics. The Substrate reference block authoring client software will expect to be able to call into the runtime API with collated data (in the case of the reference Substrate authoring client, this is merely the current timestamp and which nodes were offline) in order to return the appropriate extrinsics ready for inclusion. If new inherent extrinsic types and data are to be used in a modified runtime, then it is this function (and its argument type) that would change.



3.5. Block-authoring Logic

In Substrate, there is a major distinction between blockchain syncing and block authoring (""authoring"" is a more general term for what is called ""mining"" in Bitcoin). The first case might be referred to as a ""full node"" (or ""light node"" - Substrate supports both): authoring necessarily requires a synced node and, therefore, all authoring clients must necessarily be able to synchronize. However, the reverse is not true. The primary functionality that authoring nodes have which is not in ""sync nodes"" is threefold: transaction queue logic, inherent transaction knowledge and BFT consensus logic. BFT consensus logic is provided as a core element of Substrate and can be ignored since it is only exposed in the SDK under the authorities() API entry.


Transaction queue logic in Substrate is designed to be as generic as possible, allowing a runtime to express which transactions are fit for inclusion in a block through the initialize_block and apply_extrinsic calls. However, more subtle aspects like prioritization and replacement policy must currently be expressed ""hard coded"" as part of the blockchain’s authoring code. That said, Substrate’s reference implementation for a transaction queue should be sufficient for an initial chain implementation.


Inherent extrinsic knowledge is again somewhat generic, and the actual construction of the extrinsics is, by convention, delegated to the ""soft code"" in the runtime. If ever there needs to be additional extrinsic information in the chain, then both the block authoring logic will need to be altered to provide it into the runtime and the runtime’s inherent_extrinsics call will need to use this extra information in order to construct any additional extrinsic transactions for inclusion in the block.





4. Roadmap


4.1. So far



0.1 ""PoC-1"": PBFT consensus, Wasm runtime engine, basic runtime modules.


0.2 ""PoC-2"": Libp2p





4.2. In progress



AfG consensus


Improved PoS


Smart contract runtime module





4.3. The future



Splitting out runtime modules into separate repo


Introduce substrate executable (the skeleton-key runtime)


Introduce basic but extensible transaction queue and block-builder and place them in the executable.


DAO runtime module


Audit







5. Trying out Substrate Node


Substrate Node is Substrate’s pre-baked blockchain client. You can run a development node locally or configure a new chain and launch your own global testnet.


5.1. On Mac and Ubuntu

To get going as fast as possible, there is a simple script that installs all required dependencies and installs Substrate into your path. Just open a terminal and run:



curl https://getsubstrate.io -sSf | bash



You can start a local Substrate development chain with running substrate --dev.


To create your own global network/cryptocurrency, you’ll need to make a new Substrate Node chain specification file (""chainspec"").


First let’s get a template chainspec that you can edit. We’ll use the ""staging"" chain, a sort of default chain that the node comes pre-configured with:



substrate build-spec --chain=staging > ~/chainspec.json



Now, edit ~/chainspec.json in your editor. There are a lot of individual fields for each module, and one very large one which contains the Webassembly code blob for this chain. The easiest field to edit is the block period. Change it to 10 (seconds):



     ""timestamp"": {
        ""period"": 10
      },



Now with this new chainspec file, you can build a ""raw"" chain definition for your new chain:



substrate build-spec --chain ~/chainspec.json --raw > ~/mychain.json



This can be fed into Substrate:



substrate --chain ~/mychain.json



It won’t do much until you start producing blocks though, so to do that you’ll need to use the --validator option together with passing the seed for the account(s) that is configured to be the initial authorities:



substrate --chain ~/mychain.json --validator --key ...



You can distribute mychain.json so that everyone can synchronize and (depending on your authorities list) validate on your chain.





6. Building


6.1. Hacking on Substrate

If you’d actually like to hack on Substrate, you can just grab the source code and
build it. Ensure you have Rust and the support software installed:


6.1.1. Linux and Mac

For Unix-based operating systems, you should run the following commands:



curl https://sh.rustup.rs -sSf | sh

rustup update nightly
rustup target add wasm32-unknown-unknown --toolchain nightly
rustup update stable
cargo install --git https://github.com/alexcrichton/wasm-gc



You will also need to install the following packages:




Linux:


sudo apt install cmake pkg-config libssl-dev git clang libclang-dev




Mac:


brew install cmake pkg-config openssl git llvm






To finish installation of Substrate, jump down to shared steps.



6.1.2. Windows

If you are trying to set up Substrate on Windows, you should do the following:




First, you will need to download and install ""Build Tools for Visual Studio:""



You can get it at this link: https://aka.ms/buildtools


Run the installation file: vs_buildtools.exe


Please ensure the Windows 10 SDK component is included when installing the Visual C++ Build Tools.





Restart your computer.





Next, you need to install Rust:



Detailed instructions are provided by the Rust Book.


Download from: https://www.rust-lang.org/tools/install


Run the installation file: rustup-init.exe
> Note that it should not prompt you to install vs_buildtools since you did it in step 1.


Choose ""Default Installation.""


To get started, you need Cargo’s bin directory (%USERPROFILE%\.cargo\bin) in your PATH environment variable. Future applications will automatically have the correct environment, but you may need to restart your current shell.





Then, you will need to run some commands in CMD to set up your Wasm Build Environment:


rustup update nightly
rustup update stable
rustup target add wasm32-unknown-unknown --toolchain nightly




Next, you install wasm-gc, which is used to slim down Wasm files:


cargo install --git https://github.com/alexcrichton/wasm-gc --force




Then, you need to install LLVM: https://releases.llvm.org/download.html


Next, you need to install OpenSSL, which we will do with vcpkg:


mkdir \Tools
cd \Tools
git clone https://github.com/Microsoft/vcpkg.git
cd vcpkg
.\bootstrap-vcpkg.bat
.\vcpkg.exe install openssl:x64-windows-static




After, you need to add OpenSSL to your System Variables:


$env:OPENSSL_DIR = 'C:\Tools\vcpkg\installed\x64-windows-static'
$env:OPENSSL_STATIC = 'Yes'
[System.Environment]::SetEnvironmentVariable('OPENSSL_DIR', $env:OPENSSL_DIR, [System.EnvironmentVariableTarget]::User)
[System.Environment]::SetEnvironmentVariable('OPENSSL_STATIC', $env:OPENSSL_STATIC, [System.EnvironmentVariableTarget]::User)




Finally, you need to install cmake: https://cmake.org/download/





6.1.3. Shared Steps

Then, grab the Substrate source code:



git clone https://github.com/paritytech/substrate.git
cd substrate



Then build the code:



./scripts/build.sh          # Builds the WebAssembly binaries
cargo build                 # Builds all native code



You can run all the tests if you like:



cargo test --all



Or just run the tests of a specific package (i.e. cargo test -p srml-assets)


You can start a development chain with:



cargo run \-- --dev



Detailed logs may be shown by running the node with the following environment variables set: RUST_LOG=debug RUST_BACKTRACE=1 cargo run -- --dev.


If you want to see the multi-node consensus algorithm in action locally, then you can create a local testnet with two validator nodes for Alice and Bob, who are the initial authorities of the genesis chain specification that have been endowed with a testnet DOTs. We’ll give each node a name and expose them so they are listed on Telemetry . You’ll need two terminals windows open.


We’ll start Alice’s substrate node first on default TCP port 30333 with her chain database stored locally at /tmp/alice. The Bootnode ID of her node is QmQZ8TjTqeDj3ciwr93EJ95hxfDsb9pEYDizUAbWpigtQN, which is generated from the --node-key value that we specify below:



cargo run --release \-- \
  --base-path /tmp/alice \
  --chain=local \
  --alice \
  --node-key 0000000000000000000000000000000000000000000000000000000000000001 \
  --telemetry-url ws://telemetry.polkadot.io:1024 \
  --validator



In the second terminal, we’ll run the following to start Bob’s substrate node on a different TCP port of 30334, and with his chain database stored locally at /tmp/bob. We’ll specify a value for the --bootnodes option that will connect his node to Alice’s Bootnode ID on TCP port 30333:



cargo run --release \-- \
  --base-path /tmp/bob \
  --bootnodes /ip4/127.0.0.1/tcp/30333/p2p/QmQZ8TjTqeDj3ciwr93EJ95hxfDsb9pEYDizUAbWpigtQN \
  --chain=local \
  --bob \
  --port 30334 \
  --telemetry-url ws://telemetry.polkadot.io:1024 \
  --validator



Additional Substate CLI usage options are available and may be shown by running cargo run -- --help.




6.2. Joining the Flaming Fir Testnet

Flaming Fir is the new testnet for Substrate master (2.0). Please note that master is not compatible with the BBQ-Birch, Charred-Cherry, Dried-Danta or Emberic-Elm testnets. Ensure you have the dependencies listed above before compiling.
The master branch might have breaking changes as development progresses, therefore you should make sure you have a reasonably updated client when trying to sync Flaming Fir.



git clone https://github.com/paritytech/substrate.git
cd substrate



You can run the tests if you like:



cargo test --all



Start your node:



cargo run --release \--



To see a list of command line options, enter:



cargo run --release \-- --help



For example, you can choose a custom node name:



cargo run --release \-- --name my_custom_name



If you are successful, you will see your node syncing at https://telemetry.polkadot.io/#/Flaming%20Fir



6.3. Joining the Emberic Elm Testnet

Emberic Elm is the testnet for Substrate 1.0. Please note that 1.0 is not compatible with the BBQ-Birch, Charred-Cherry, Dried-Danta or Flaming-Fir testnets.
In order to join the Emberic Elm testnet you should build from the v1.0 branch. Ensure you have the dependencies listed above before compiling.



git clone https://github.com/paritytech/substrate.git
cd substrate
git checkout -b v1.0 origin/v1.0



You can then follow the same steps for building and running as described above in Joining the Flaming Fir Testnet.





7. Documentation


7.1. Viewing documentation for Substrate packages

You can generate documentation for a Substrate Rust package and have it automatically open in your web browser using rustdoc with Cargo,
(of the The Rustdoc Book), by running the the following command:



cargo doc --package <spec> --open



Replacing <spec> with one of the following (i.e. cargo doc --package substrate --open):




All Substrate Packages


substrate




Substrate Core


substrate, substrate-cli, substrate-client, substrate-client-db,
substrate-consensus-common, substrate-consensus-rhd,
substrate-executor, substrate-finality-grandpa, substrate-keyring, substrate-keystore, substrate-network,
substrate-network-libp2p, substrate-primitives, substrate-rpc, substrate-rpc-servers,
substrate-serializer, substrate-service, substrate-service-test, substrate-state-db,
substrate-state-machine, substrate-telemetry, substrate-test-client,
substrate-test-runtime, substrate-transaction-graph, substrate-transaction-pool,
substrate-trie




Substrate Runtime


sr-api, sr-io, sr-primitives, sr-sandbox, sr-std, sr-version




Substrate Runtime Module Library (SRML)


srml-assets, srml-balances, srml-consensus, srml-contract, srml-council, srml-democracy, srml-example,
srml-executive, srml-metadata, srml-session, srml-staking, srml-support, srml-system, srml-timestamp,
srml-treasury




Node


node-cli, node-consensus, node-executor, node-network, node-primitives, node-runtime




Subkey


subkey







7.2. Contributing to documentation for Substrate packages

Document source code for Substrate packages by annotating the source code with documentation comments.


Example (generic):



/// Summary
///
/// Description
///
/// # Panics
///
/// # Errors
///
/// # Safety
///
/// # Examples
///
/// Summary of Example 1
///
/// ```rust
/// // insert example 1 code here
/// ```
///





Important notes:



Documentation comments must use annotations with a triple slash ///


Modules are documented using //!








//! Summary (of module)
//!
//! Description (of module)





Special section header is indicated with a hash #.



Panics section requires an explanation if the function triggers a panic


Errors section is for describing conditions under which a function of method returns Err(E) if it returns a Result<T, E>


Safety section requires an explanation if the function is unsafe


Examples section includes examples of using the function or method





Code block annotations for examples are included between triple graves, as shown above.
Instead of including the programming language to use for syntax highlighting as the annotation
after the triple graves, alternative annotations include the ignore, text, should_panic, or no_run.


Summary sentence is a short high level single sentence of its functionality


Description paragraph is for details additional to the summary sentence


Missing documentation annotations may be used to identify where to generate warnings with ![warn(missing_docs)]
or errors ![deny(missing_docs)]


Hide documentation for items with #[doc(hidden)]





7.3. Contributing to documentation (tests, extended examples, macros) for Substrate packages

The code block annotations in the # Example section may be used as documentation as tests and for extended examples.




Important notes:



Rustdoc will automatically add a main() wrapper around the code block to test it


Documenting macros.


Documentation as tests examples are included when running cargo test










8. Contributing


8.1. Contributing Guidelines

CONTRIBUTING.adoc



8.2. Contributor Code of Conduct

CODE_OF_CONDUCT.adoc





9. License


LICENSE


",1077
haskell-works/hw-prim,Haskell,"hw-prim


Primitive types library.
Report dependency build errors: https://github.com/haskell-infra/hackage-trustees
",2
corda/samples,Kotlin,"This repository contains sample CorDapps created to show developers how to implement specific functionality, one per folder. These samples are all Apache 2.0 licensed, so feel free to use them as the basis for your own CorDapps.
General

yo-cordapp: A simple CorDapp that allows you to send Yo’s! to other Corda nodes
cordapp-example: Models IOUs (I Owe yoUs) between Corda nodes (also in Java)
obligation-cordapp: A more complex version of the IOU CorDapp (also in Java) Handles the transfer and settlement of obligations Retains participant anonymity using confidential identities (i.e. anonymous public keys)
negotiation-cordapp: Shows how multi-party negotiation is handled on the Corda ledger, in the absence of an API for user interaction

Observers

observable-states: Use the observers feature to allow regulators to track regulated activity

Attachments

blacklist: Use an attachment to blacklist specific nodes from signing agreements

Confidential identities

whistleblower: Use confidential identities (i.e. anonymous public keys) to whistle-blow on other nodes anonymously

Oracles

oracle-example: Use an oracle to attest to the prime-ness of integers in transaction

Scheduled activities

heartbeat: Use scheduled states to cause your node to emit a heartbeat every second

Accessing external data

flow-http: Make an HTTP request in a flow to retrieve the Bitcoin readme from a webserver
flow-db: Access the node’s database in flows to store and read cryptocurrency values

Upgrading Cordapps

explicit-cordapp-upgrades: A client for upgrading contracts using the Contract Upgrade Flow
implicit-cordapp-upgrades: An app with a number of different versions, showing how to carry out various upgrade procedures

Interacting with your node
Web servers

pigtail: A node web-server using Braid and Node.js
spring-webserver: A node web-server using Spring that provides generic REST endpoints for interacting with a node via RPC and can be extended to work with specific CorDapps

Command-line clients

corda-nodeinfo: A command-line client for retrieving information from a running node Useful for checking that a node is running and is accessible from another host via RPC

Reference States

reference-states: A cordapp demonstrating the use of Reference States to extend the IOU cordapp-example cordapp

",23
vsl-lang/libvsl,None,"libvsl



    —
    Documentation


    VSL Standard Library
  

VSL standard type library. The VSL standard library includes most functions for
a variety of systems including WASM which provides critical behaviors such as
the Object class along with the integer classes and classes used for memory
allocations.
VSL's standard type explicitly supports alongside POSIX:

Windows (win32)
WASM (wasm)

VSL's standard type also assumes any system that is not Windows or WASM to be
POSIX compliant unless stated otherwise.
",2
vsl-lang/libvsl,None,"libvsl



    —
    Documentation


    VSL Standard Library
  

VSL standard type library. The VSL standard library includes most functions for
a variety of systems including WASM which provides critical behaviors such as
the Object class along with the integer classes and classes used for memory
allocations.
VSL's standard type explicitly supports alongside POSIX:

Windows (win32)
WASM (wasm)

VSL's standard type also assumes any system that is not Windows or WASM to be
POSIX compliant unless stated otherwise.
",2
acharyaks90/ipriksha,TypeScript,"Ipriksha
This project was generated with Angular CLI version 7.0.2.
Development server
Run ng serve for a dev server. Navigate to http://localhost:4200/. The app will automatically reload if you change any of the source files.
Code scaffolding
Run ng generate component component-name to generate a new component. You can also use ng generate directive|pipe|service|class|guard|interface|enum|module.
Build
Run ng build to build the project. The build artifacts will be stored in the dist/ directory. Use the --prod flag for a production build.
Running unit tests
Run ng test to execute the unit tests via Karma.
Running end-to-end tests
Run ng e2e to execute the end-to-end tests via Protractor.
Further help
To get more help on the Angular CLI use ng help or go check out the Angular CLI README.
",3
Lombiq/Arithmetics,C#,"Unum - Proof of concept readme
This project was developed as part of Hastlayer, the .NET HLS tool that converts .NET programs into equivalent logic hardware implementations.
Its goal is to implement a Unum proof of concept: the number type and an example using it, all transformable with Hastlayer.
The project's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/arithmetics (Mercurial repository)
https://github.com/Lombiq/Arithmetics (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub. Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
About Unum
Unum is a new number format invented by Dr. John L. Gustafson that can be used to store any number with exact precision (or known error). It can be used to achieve better range and accuracy than IEEE floating point formats while eliminating the algebraic errors that the IEEE floats are prone to.
For more about its advantages see: http://ubiquity.acm.org/article.cfm?id=2913029.
",6
pothi/wp-in-a-box,Shell,"WP In A Box
Script/s to install LEMP in a linux box. This LEMP stack is fine-tuned towards WordPress installations. It may work for other PHP based applications, too. For more details, please see the blog post at https://www.tinywp.in/wp-in-a-box/.
There are a number of similar scripts available on the internet. The unique feature of this repo is in security considerations.
Supported Platforms

Ubuntu Bionic Beaver (18.04.x)
Debian Stretch (9.x)
Ubuntu Xenial Xerus (16.04.x)

Generic Goals
In sync with WordPress philosophy of “decision, not options”.
Performance Considerations

No added bloatware
Redis for object cache (available as an optional package)
Full page cache support (WP Super Cache, WP Rocket and WP Fastest Cache)
PHP 7.x
Nginx (no Apache, sorry)
Varnish (planned, but no ETA)
Swap

Security Considerations

No phoning home.
No external dependencies (such as third-party repositories, unless there is a strong reason to use it).
Automatic security updates (with an option to update everything).
Disable password authentication for root.
Nginx (possibly with Naxsi WAF when h2 issue is resolved).
Umask 027 or 077.
ACL integration.
Weekly logwatch (if email is supplied).
Isolated user for PhpMyAdmin.
PHP user and Nginx user run under different username.
Only ports 80, 443, and port for SSH are open.

Implementation Details

Agentless.
Idempotent.
Random username (like GoDaddy generates).
Automatic restart of MySQL (and Varnish) upon failure.
Integrated wp-cli.
Support for version control (git, hg).
Composer pre-installed.
Auto-update of almost everything (wp-cli, composer, certbot certs, etc).

Roadmap

Automatic Certbot / LetsEncrypt installation and renewal (like Caddy).
Automated setup of sites using backups.
Web interface (planned, but no ETA).
Automatic backup of site/s (files and DB) to AWS S3 or to GCP.

Install procedure

Rename .envrc-sample file as .envrc and insert as much information as possible
Download bootstrap.sh and execute it.

# as root

apt install curl screen -y

# optional steps
# curl -LO https://github.com/pothi/wp-in-a-box/raw/master/.envrc-sample
# cp .envrc-sample .envrc
# nano .envrc

# download the bootstrap script
curl -LO https://github.com/pothi/wp-in-a-box/raw/master/bootstrap.sh

# please do not trust any script on the internet or github
# so, please go through it!
nano ~/bootstrap.sh

# execute it and wait for some time
# screen bash bootstrap.sh
# or simply
bash bootstrap.sh

# wait for the installation to get over.
# it can take approximately 5 minutes on a 2GB server
# it depends on CPU power too

# we no longer needs bootstrap.sh file
rm bootstrap.sh

# to see the credentials to log in to the server from now
# this is the important step. you can't login as root from now on
cat ~/.envrc

What you get at the end of the installation

a SSH user (prefixed with ssh_) with root privileges (use it only to manage the server such as to create a new MySQL database or to create a new vhost entry for Nginx)
a chrooted SFTP user, prefixed with sftp_web_, with its home directory at /home/web along with some common directories(such as ~/log, ~/sites, etc) created already. (you may give it to your developer to access the file system such as to upload a new theme, etc)

Where to install WordPress & How to install it

PHP runs as SFTP user. So, please install WordPress as SFTP user at /home/web/sites/example.com/public.
Configure Nginx using pre-defined templates that can be found at the companion repo WordPress-Nginx. That repo is already installed. You just have to copy / paste one of the templates to fit your domain name.
If you wish to deploy SSL, a Let's Encrypt client is already installed. Please use the command certbot certonly --webroot -w /home/web/sites/example.com/public -d example.com -d www.example.com. The renewal script is already in place as a cron entry. So, you don't have to create a new entry. To know more about this client library and to know more about the available options, please visit https://certbot.eff.org/ .

Known Limitations

SFTP user can not create or upload new files and folders at $HOME, but can create or upload inside other existing directories. This is a known limitation when we use SFTP capability of built-in OpenSSH server.

Wiki
For more documentation, information, supported/tested hosts, todo, etc, please see the WP-In-A-Box wiki.
",26
WildGums/LogViewer,C#,"LogViewer

LogViewer is a simple application used to read log files created by the Catel logger.
It is also used as an example on how to build an application using Orchestra and the various Orc.* libraries.
Features

Build-in filesystem browser with multiselection support:

Easily switch between log files
View multiple log files together


Search (with highlighting)
Fitler by:

Log level (Error, warning, info, debug)
Date range
Keyword



How to get it
The tool is available in several ways:

Build it from source
Download it here: (includes automatic updates):

Stable with support for updates
Beta with support for updates
Alpha with support for updates



Note that you can switch the update channel in the settings
Screenshots

License
MIT license
",51
Velliz/pukoconsole,PHP,"Puko Console
Advanced console util that make pukoframework get things done on the fly.
Usage:
php puko
Finished:

Auth
Controller
Database
Serve
Routes
Secure
Element

Todo:

Automated Docs

Future todo:

Migrate database
Generic database

",2
DanEngelbrecht/bikeshed,C++,"


Branch
OSX / Linux / Windows




master



master




bikeshed
Lock free hierarchical work scheduler
Builds with MSVC, Clang and GCC, header only, C99 compliant, MIT license.
See github for latest version: https://github.com/DanEngelbrecht/bikeshed
See design blogs at: https://danengelbrecht.github.io
Version history
Version v0.3 1/5 2019
Pre-release 3
Fixes

Ready callback is now called when a task is readied via dependency resolve
Tasks are readied in batch when possible

Version v0.2 29/4 2019
Pre-release 2
Fixes

Internal cleanups
Fixed warnings and removed clang warning suppressions

-Wno-sign-conversion
-Wno-unused-macros
-Wno-c++98-compat
-Wno-implicit-fallthrough


Made it compile cleanly with clang++ on Windows

Version v0.1 26/4 2019
Pre-release 1
Features

Generic tasks scheduling with dependecies between tasks

Each task has zero or many dependecies (as defined by user)
User should Ready any tasks that can execute (has zero dependencies)
Automatic ready of tasks that reaches zero dependecies
Automatic free of tasks that has completed


A task can have many parents and many child dependecies
Task channels - execute tasks based on task channel
No memory allocations once shed is created
Minimal dependencies
Memory allocation and threading are users responsability
Lifetime of data associated with tasks is users responsability
Configurable and optional assert (fatal error) behavior
Configurable platform dependant functions with default implementation provided
Header only - define BIKESHED_IMPLEMENTATION in one compilation unit and include bikeshed.h

Non-features

Cyclic dependency detection and resolving

API is designed to help user avoid cyclic dependecies but does not do any analisys


Built in threading or syncronization code - API to add it is available
Unlimited number of active tasks - currently limited to 8 388 607 active tasks
Cancelling of tasks
Tagging of tasks

Dependencies
Minimal dependecies with default overridable method for atomic operations.

<stdint.h>
<string.h>
The default (optional) MSVC implementation depends on <Windows.h>.

Optional default methods
The default implementations for the atomic functions can be overridden with your own implementation by overriding the macros:

BIKESHED_ATOMICADD Atomically adds a 32-bit signed integer to another 32-bit signed integer and returns the result
BIKESHED_ATOMICCAS Atomically exchange a 32-bit signed integer with another 32-bit signed integer if the value to be swapped matches the provided compare value, returns the old value.

Test code dependecies
Test code has dependencies added as drop-in headers from

https://github.com/JCash/jctest for unit test validation

Test code has dependencies added as git sub-modules from

https://github.com/DanEngelbrecht/nadir for threading and syncronization

",15
mishas/wstunnel,Go,"wstunnel
Tunneling SOCKS5 proxy over Websockets.
This project contains two binaries: client and server.
Running them in tandem creates a tunnel over Websocket protocol that proxies any information.
Example
Alice wants to connect to Bob's computer via SSH (port 22), but Alice is connected to Eve's Wifi,
and Eve has a firewall in place that blocks port 22. In fact, Eve's firewall only lets web traffic
on ports 80 and 443 go through.
Alice can ask Faythe, who has no firewall, to set up the server (from this package) on her computer:
bazel run :server

And run the client locally:
bazel run :client -- -host=faythe.com

Now, any SOCKS5 message sent to localhost:8080 will be tunneled via websockets to Faythe's computer.
If Alice wants to SSH Bob now, she can simply do:
ssh -o ""ProxyCommand=nc -X 5 -x localhost:8080 %h %p"" bob.com

If, Except for the Firewall, Alice's connection to the internet must go through a SOCKS5 proxy, she
can run the client locally using:
all_proxy=socks5://outbound.alice.com:12345/ bazel run :client -- -host=faythe.com

",14
openpowerquality/opq,C,"Welcome to the master repository for OPQ technology.
For details on OPQ, please see http://openpowerquality.org.
",9
openpowerquality/opq,C,"Welcome to the master repository for OPQ technology.
For details on OPQ, please see http://openpowerquality.org.
",9
clearlinux/docker-brew-clearlinux,Dockerfile,"docker-brew-clearlinux
Dockerhub image snapshots for Clear Linux
",18
PythonistaMX/py101,Jupyter Notebook,"
py101 Introducción a Python 3.
Temario:

Introducción al lenguaje Python.
Palabras reservadas y espacio de nombres.
Expresiones y declaraciones.
Números, cadenas de caracteres, tipos y operadores.
Orientación a objetos e introspección.
Entrada y salida estándar.
Bloques, comentarios y condicionales.
Ciclos, iteraciones e interrupciones de ejecución.
Objetos tipo list y tipo tuple.
Objetos tipo dict.
Objetos tipo str.
Objetos tipo set y frozenset.
Funciones.
Gestión de excepciones.
Iteradores y generadores.
Completado de elementos.
Entrada y salida de archivos.
Módulos y paquetes.
Gestión de módulos y paquetes con pip.
Creación de paquetes con setuptools.
Entornos virtuales.

Descarga de los apuntes.
Para descargar los apuntes ejecute el siguiente comando:
git clone https://github.com/PythonistaMX/py101.git

La máquina virtual de Pythonista®
Para poder aprovechar por completo el código de los apuntes, se recomienda descargar y ejecutar desde VirtualBox la imagen que se encuentra disponible en https://pythonista.io/descargas/base/view.
Esta obra está bajo una Licencia Creative Commons Atribución 4.0 Internacional.
© José Luis Chiquete Valdivieso. 2019.
",2
vim-jp/vimdoc-ja-working,Vim script,"vimdoc-ja-working

Vim 付属のヘルプを日本語に翻訳するためのプロジェクトです。
ヘルプを利用したい
配布専用のページにアクセスしてください!
ヘルプの翻訳に協力したい
作業手順については wiki をご覧ください。
間違いを見付けた場合、issueトラッカー でお知らせください。プルリクエストも大歓迎です。
GitHub のアカウントをお持ちでない場合は、次の方法による連絡も可能です。

メーリングリスト https://groups.google.com/group/vimdoc-ja

姉妹プロジェクトがあります！
Vim 本体のメッセージ、GUIメニュー、チュートリアル、man ファイルなどの日本語訳については vim-jp/lang-ja でやっています。お気軽にどうぞ。
注意事項
README-dist.md は vim-jp/vimdoc-ja の README.md として配布されます。
内容を変更する際はそれに相応しい変更であるよう留意してください。
",35
sdbxpjzq/Planet,HTML,"Planet
平时的一些整理, 还有很多不足之处, 还请大家多多指教.

阅读插件推荐
Github Toc
Octotree
",3
kdri-genomics/Hayai-Annotation-Plants,R,"Hayai-Annotation Plants v1.0.1
R-package for an Ultra-Fast and Comprehensive Gene Annotation in Plants
Description
Hayai-Annotation-Plants is an R-package with a shiny-browser interface for a highly accurate and fast gene annotation system for plant species (Embryophyta).
Hayai-Annotation-Plants provides five levels of annotation:

Protein Name;
Gene Ontology (GO) consisting of three main domains (Biological Process, Molecular Function and Cellular Component);
Enzyme Commission Code - EC;
Protein Existence Level;
Evidence Code for GO annotation.

Documentation
For downloading, installing and running Hayai-Annotation Plants please see Hayai-Annotation-Plants wiki
Reference
Hayai-Annotation Plants: an ultra-fast and comprehensive functional gene annotation system in plants 
Andrea Ghelfi, Kenta Shirasawa, Hideki Hirakawa, Sachiko Isobe 
Bioinformatics, btz380, https://doi.org/10.1093/bioinformatics/btz380
Updates
2019/04/09. Beta Version: Hayai-Annotation Plants Docker Container 
2019/01/04. Low number of input sequences crash - Fixed. Now, graphics are generated if number of annotated queries are higher than 500.
2019/01/04. Hayai-Annotation Plants run with even if USEARCH is installed in another directory (requires symbolic link).
",2
HN67/spook-fighters,Python,"Spook Fighters
A repository for the fighter game Spook Fighters, built with Pygame. This is a hobby project under volatile development.
Play
To play Spook Fighters, download main.exe.
Alternatively, to natively run the game, install Python 3.7+ and Pygame 1.9+. Download the repository and run the main.py file to play the native, potentially experiemental version.

Gameplay
Controls



Action
Player 1
Player 2




Jump
W
↑


Move Left
A
←


Move Right
D
→


Fast Fall
S
↓


Stun
C
I


Basic Attack
V
O



Mechanics
Health

Take damage from attacks
Knockback scales from damage

Attacks

Basic stun
Class standard attack
Class special attack
Class ultimate

",3
SeryioGonzalez/azure-pricer,Python,"Azure Quote Estimation Tool
The tool generates a spreadsheet like Azure-Quote-Tool-20190517.xlsx
Unofficial tool for an estimation on Azure VM migration projects.
It generates a spreadsheet for performing calculations
For creating a new spreadsheet:
#ONLY IF YOU HAD NOT INSTALLED PIP3 AND THE XLSWRITER LIBRARY
$ sudo apt install -y python3-pip
$ pip3 install xlsxwriter

#COMMAND REQUIRED FOR CREATING A SPREADSHEET
# python3 xlsGenerator.py [nameOfOutputXLSFile]
$ python3 xlsGenerator.py Azure-Quote-Tool-$(date +%d%m%y).xlsx
$ ls -1
Azure-Quote-Tool-020618.xlsx

It works for Excel 2016 and beyond
Considerations


This is a unofficial tool for providing a first order estimation on Azure migration projects


It contains Azure pricing information read from Azure pricing APIs at the day of the spreadsheet creation


The tool consider IaaS projects and estimates compute, storage and ASR costs


Services considered for a quote

Managed Disks for OS and data drives
Premium or Standard Managed Disk for OS and data drives

OS Disk sizes limited to P4, P4, P10, S4, S6 or S10


ASR Costs per VM
ASR Disk duplication costs
PAYG(hour), 3 or 1 year Reserved Instance pricing models
VMs certified for SAP NetWeaver loads
VMs equipped with GPU cards
Usage of burstable VMs
Percent performance improvement in Azure

If I input 20% performance win per CPU and memory on Azure, a VM request for 100 GB RAM will look VMs sizes starting at 80 GB


Selection based on CPU and Memory or just Memory

Additional points to be added for a full quote

VPN or Express Route (ER) Gateway costs
If ER is used, the cost of the ER circuit in Azure and telco costs
Outgoing bandwidth costs
VM Backups
Cost for services other than compute, managed disks and ASR

",8
HN67/spook-fighters,Python,"Spook Fighters
A repository for the fighter game Spook Fighters, built with Pygame. This is a hobby project under volatile development.
Play
To play Spook Fighters, download main.exe.
Alternatively, to natively run the game, install Python 3.7+ and Pygame 1.9+. Download the repository and run the main.py file to play the native, potentially experiemental version.

Gameplay
Controls



Action
Player 1
Player 2




Jump
W
↑


Move Left
A
←


Move Right
D
→


Fast Fall
S
↓


Stun
C
I


Basic Attack
V
O



Mechanics
Health

Take damage from attacks
Knockback scales from damage

Attacks

Basic stun
Class standard attack
Class special attack
Class ultimate

",3
SeryioGonzalez/azure-pricer,Python,"Azure Quote Estimation Tool
The tool generates a spreadsheet like Azure-Quote-Tool-20190517.xlsx
Unofficial tool for an estimation on Azure VM migration projects.
It generates a spreadsheet for performing calculations
For creating a new spreadsheet:
#ONLY IF YOU HAD NOT INSTALLED PIP3 AND THE XLSWRITER LIBRARY
$ sudo apt install -y python3-pip
$ pip3 install xlsxwriter

#COMMAND REQUIRED FOR CREATING A SPREADSHEET
# python3 xlsGenerator.py [nameOfOutputXLSFile]
$ python3 xlsGenerator.py Azure-Quote-Tool-$(date +%d%m%y).xlsx
$ ls -1
Azure-Quote-Tool-020618.xlsx

It works for Excel 2016 and beyond
Considerations


This is a unofficial tool for providing a first order estimation on Azure migration projects


It contains Azure pricing information read from Azure pricing APIs at the day of the spreadsheet creation


The tool consider IaaS projects and estimates compute, storage and ASR costs


Services considered for a quote

Managed Disks for OS and data drives
Premium or Standard Managed Disk for OS and data drives

OS Disk sizes limited to P4, P4, P10, S4, S6 or S10


ASR Costs per VM
ASR Disk duplication costs
PAYG(hour), 3 or 1 year Reserved Instance pricing models
VMs certified for SAP NetWeaver loads
VMs equipped with GPU cards
Usage of burstable VMs
Percent performance improvement in Azure

If I input 20% performance win per CPU and memory on Azure, a VM request for 100 GB RAM will look VMs sizes starting at 80 GB


Selection based on CPU and Memory or just Memory

Additional points to be added for a full quote

VPN or Express Route (ER) Gateway costs
If ER is used, the cost of the ER circuit in Azure and telco costs
Outgoing bandwidth costs
VM Backups
Cost for services other than compute, managed disks and ASR

",8
nteract/nteract,TypeScript,"nteract 








|| Basics • Users || Contributors • Development • Maintainers || Sponsors • Made possible by ||
Basics
nteract is first and foremost a dynamic tool to give you flexibility when
writing code, exploring data, and authoring text to share insights about the
data.
Edit code, write prose, and visualize.

Share documents understood across the Jupyter ecosystem, all in the comfort of a desktop app.
Explore new ways of working with compute and playing with data.
Bring the nteract experience to your web-based Jupyter installation with the nteract server extension.

We support Jupyter kernels
locally on your system and on remote JupyterHubs via Binder.
Users
Installing the nteract desktop application
If you're here to install the nteract desktop app, visit
nteract.io to download a binary and install or visit the
releases page. The nteract desktop app ships with a NodeJS kernel by default. If you'd like to use other Jupyter kernels, you will need to install them.
Installing nteract web
Our current flavor of nteract web runs on top of the Jupyter server. Install it with pip:
pip install nteract_on_jupyter
jupyter serverextension enable nteract_on_jupyter

Now, run jupyter nteract and you're running nteract on the Jupyter web application!
Try the nteract playground
The nteract playground is a web-based application that allows you to write code and execute code in a web-based editor supported by Binder and Jupyter kernels. You can learn more about in the nteract play GitHub repo.
User Guide
To learn more about using the nteract desktop app, please check out the user guide

Contributors
The contributors are listed in the contributors page on GitHub.
To learn how to contribute to nteract, head on over to our contributing guide.
This project adheres to the Contributor Covenant code of conduct.
By participating, you are expected to uphold this code. Please report unacceptable behavior to rgbkrk@gmail.com.
Feel free to post issues on GitHub or chat with us in Slack (request an invite) if you need help or have
questions. If you have trouble creating an account on Slack, please post an issue on GitHub.
Development
Overview of nteract's monorepo
This repository is a monorepo, which basically
means that the repository hosts more than one module or application. In our
case, we have two main directories:
packages/ -- components used as an individual library
applications/ -- all the user facing applications (i.e. desktop)

The packages directory has the components needed to build new applications,
and the applications has the desktop app and the Jupyter extension.
Why have a monorepo? The monorepo contains many components and packages that
can be mixed and remixed to create new applications. The monorepo keeps these
elements together so they are easy to discover and use. Another benefit
is that the monorepo makes it easy to iterate on applications that share
common components. For example, if we update a component, such as the Jupyter
message handling, and happen to introduce an issue when making a change to the
desktop app, we would
notice the issue in tandem.
Getting Started
To get started developing, set up the nteract monorepo.
Set the monorepo up in dev mode
Requires Node.js and yarn.

Fork this repo
Clone your fork or this repo git clone https://github.com/nteract/nteract
cd to the directory where you cloned it
yarn install

To keep up-to-date with changes to the root nteract/nteract branch:

Set the root as a remote: git remote add upstream https://github.com/nteract/nteract.git

When changes are made to the root nteract/nteract, they can then be pulled from the root and merged to your master branch:

git pull upstream master
yarn clean
yarn install

Building a specific package
In some cases you'll want to modify an individual base package (i.e. commutable
or transforms) and not rebuild all of the other packages. To target a build of a
specific package, use this command, replacing packageName with the fully qualified name of the package you
want to hack on:
yarn build:only packageName

For example, to hack on the transforms package, use
yarn build:only @nteract/transforms

Hacking on the Desktop application
Quick and dirty (manual)
yarn app:desktop

As you make changes, you will have to close the entire app (CMD-q on macOS or
CNTL-c at the terminal) and then run yarn app:desktop again to see the
changes.
Progressive Webpack build (automatic)
In separate terminals run:
yarn build:desktop:watch

and
yarn spawn

This progressive webpack build will keep rebuilding as you modify the source
code. When you open a new notebook, you'll get the fresh, up-to-date copy of
the notebook app.
Logging
console.log statements in the main Electron process are piped to stdout.
console.log statements in the Electron renderer process go to the
regular Dev Tools console (accessible from the View menu). Set
ELECTRON_ENABLE_LOGGING=1 to pipe renderer console.log to the launching
terminal as well. This is useful for debugging crashes and notebook closing
behaviors.
Troubleshooting

I upgraded my developer installation and things are broken!


Try yarn clean && yarn


I want to debug redux actions and state changes.


Enable redux-logger by
spawning the application with yarn spawn:debug.


I keep getting a pop-up asking: Do you want the application ""nteract Helper.app"" to accept
incoming network connections? while developing or using a custom build of
nteract on macOS.



This is how the the macOS firewall behaves for unsigned apps. On a signed app,
the dialog won't show up again after approving it the first time. If you're
using a custom build of nteract, run:
sudo codesign --force --deep --sign - /Applications/nteract.app

You will have to do this again every time you rebuild the app.



Sponsors
Work on the nteract notebook is currently sponsored by

We're on a common mission to build a great interactive computing experience. You can help by:

QAing the desktop application and helping create meaningful bug reports
Providing support on GitHub issues or the Slack team
Donating money to nteract, which is a non-profit fiscally sponsored by NumFOCUS
Contributing your organization's engineering hours to the nteract project

Made possible by
The nteract project was made possible with the support of

If your employer allows you to work on nteract during the day and would like
recognition, feel free to add them to this ""Made possible by"" list.
|| Basics • Users || Contributors • Development • Maintainers || Sponsors • Made possible by ||
",4051
gatsbyjs/gatsby,JavaScript,"





  Gatsby v2


⚛️ 📄 🚀


  Fast in every way that matters


  Gatsby is a free and open source framework based on React that helps developers build blazing fast websites and apps

























Quickstart
 · 
Tutorial
 · 
Plugins
 · 
Starters
 · 
Showcase
 · 
Contribute
 · 
  Support: Spectrum
 & 
Discord

Gatsby is a modern framework for blazing fast websites.


Go Beyond Static Websites. Get all the benefits of static websites with none of the
limitations. Gatsby sites are fully functional React apps so you can create high-quality,
dynamic web apps, from blogs to e-commerce sites to user dashboards.


Use a Modern Stack for Every Site. No matter where the data comes from, Gatsby sites are
built using React and GraphQL. Build a uniform workflow for you and your team, regardless of
whether the data is coming from the same backend.


Load Data From Anywhere. Gatsby pulls in data from any data source, whether it’s Markdown
files, a headless CMS like Contentful or WordPress, or a REST or GraphQL API. Use source plugins
to load your data, then develop using Gatsby’s uniform GraphQL interface.


Performance Is Baked In. Ace your performance audits by default. Gatsby automates code
splitting, image optimization, inlining critical styles, lazy-loading, and prefetching resources,
and more to ensure your site is fast — no manual tuning required.


Host at Scale for Pennies. Gatsby sites don’t require servers so you can host your entire
site on a CDN for a fraction of the cost of a server-rendered site. Many Gatsby sites can be
hosted entirely free on services like GitHub Pages and Netlify.


Learn how to use Gatsby for your next project.
What’s In This Document

Get Up and Running in 5 Minutes
Learning Gatsby
Migration Guides
How to Contribute
License
Thanks to Our Contributors and Sponsors

🚀 Get Up and Running in 5 Minutes
You can get a new Gatsby site up and running on your local dev environment in 5 minutes with these four steps:


Install the Gatsby CLI.
npm install -g gatsby-cli



Create a Gatsby site from a Gatsby starter.
Get your Gatsby blog set up in a single command:
# create a new Gatsby site using the default starter
gatsby new my-blazing-fast-site


Start the site in develop mode.
Next, move into your new site’s directory and start it up:
cd my-blazing-fast-site/
gatsby develop


Open the source code and start editing!
Your site is now running at http://localhost:8000. Open the my-blazing-fast-site directory in your code editor of choice and edit src/pages/index.js. Save your changes, and the browser will update in real time!


At this point, you’ve got a fully functional Gatsby website. For additional information on how you can customize your Gatsby site, see our plugins and the official tutorial.
🎓 Learning Gatsby
Full documentation for Gatsby lives on the website.


For most developers, we recommend starting with our in-depth tutorial for creating a site with Gatsby. It starts with zero assumptions about your level of ability and walks through every step of the process.


To dive straight into code samples head to our documentation. In particular, check out the “Guides”, “API Reference”, and “Advanced Tutorials” sections in the sidebar.


We welcome suggestions for improving our docs. See the “how to contribute” documentation for more details.
Start Learning Gatsby: Follow the Tutorial · Read the Docs
💼 Migration Guides
Already have a Gatsby site? These handy guides will help you add the improvements of Gatsby v2 to your site without starting from scratch!

Migrate a Gatsby site from v1 to v2
Still on v0? Start here: Migrate a Gatsby site from v0 to v1

❗ Code of Conduct
Gatsby is dedicated to building a welcoming, diverse, safe community. We expect everyone participating in the Gatsby community to abide by our Code of Conduct. Please read it. Please follow it. In the Gatsby community, we work hard to build each other up and create amazing things together. 💪💜
🤝 How to Contribute
Whether you're helping us fix bugs, improve the docs, or spread the word, we'd love to have you as part of the Gatsby community! 💪💜
Check out our Contributing Guide for ideas on contributing and setup steps for getting our repositories up and running on your local machine.
A note on how this repository is organized
This repository is a monorepo managed using Lerna. This means there are multiple packages managed in this codebase, even though we publish them to NPM as separate packages.
Contributing to Gatsby v1
We are currently only accepting bug fixes for Gatsby v1. No new features will be accepted.
📝 License
Licensed under the MIT License.
💜 Thanks to Our Contributors and Sponsors
Thanks to our many contributors and sponsors as well as the companies sponsoring
our testing and hosting infrastructure: Circle CI, Appveyor, and Netlify.
",34692
Rsullivan00/apollo-link-json-api,TypeScript,"JSON API Link  
Purpose
An Apollo Link to easily use GraphQL with a JSON API
compliant server.
Installation
npm install apollo-link-json-api apollo-link graphql graphql-anywhere qs --save # or `yarn add apollo-link-rest apollo-link graphql graphql-anywhere qs`
apollo-link, graphql, qs, humps, and graphql-anywhere are peer dependencies needed by apollo-link-json-api.
Usage
Basics
import { JsonApiLink } from ""apollo-link-json-api"";
// Other necessary imports...

// Create a JsonApiLink for the JSON API
// If you are using multiple link types, jsonApiLink should go before httpLink,
// as httpLink will swallow any calls that should be routed through jsonApi!
const jsonApiLink = new JsonApiLink({
  uri: 'http://jsonapiplayground.reyesoft.com/v2/',
});

// Configure the ApolloClient with the default cache and JsonApiLink
const client = new ApolloClient({
  link: jsonApiLink,
  cache: new InMemoryCache(),
});

// A simple query to retrieve data about the first author
const query = gql`
  query firstAuthor {
    author @jsonapi(path: ""authors/1"") {
      name
    }
  }
`;

// Invoke the query and log the person's name
client.query({ query }).then(response => {
  console.log(response.data.name);
});
Advanced Querying
JSON API Link supports unpacking related resources
into a friendlier GraphQL query structure.
const query = gql`
  query firstAuthor {
    author @jsonapi(path: ""authors/1?include=series,series.books"") {
      name
      series {
        title
        books {
          title
        }
      }
    }
  }
`;

While JSON API Link does support running multiple nested queries, prefer
sideloading resources in a single request by using the ?include parameter if
your JSON API server supports it.
// Avoid this
const badQuery = gql`
  query firstAuthor {
    author @jsonapi(path: ""authors/1"") {
      name
      series @jsonapi(path: ""authors/1/series"") {
        title
      }
    }
  }
`;

// Prefer this
const query = gql`
  query firstAuthor {
    author @jsonapi(path: ""authors/1?include=series"") {
      name
      series {
        title
      }
    }
  }
`;
Mutations
import React from 'react'
import gql from 'graphql-tag'
import { Mutation } from 'react-apollo'

export const UPDATE_BOOK_TITLE = gql`
  mutation UpdateBookTitle($input: UpdateBookTitleInput!) {
    book(input: $input) @jsonapi(path: ""/books/{args.input.data.id}"", method: ""PATCH"") {
      title
    }
  }
`

const UpdateBookTitleButton = ({ videoId, children }) => (
  <Mutation
    mutation={UPDATE_BOOK_TITLE}
    update={(store, { data: { book } }) => {
      // Update your Apollo cache with result
      console.log(book.title)
    }}
  >
    {mutate => (
      <button onClick={() => 
        mutate({
          variables: {
            input: {
              data: {
                id: bookId,
                type: 'books',
                attributes: { title: 'Changed title!' }
              }
            }
          },
          optimisticResponse: {
            book: {
              __typename: 'Books',
              title: 'Changed title!'
            }
          }
        })
        }>
        Update your book title!
        </button>
    )}
  </Mutation>
)
Options
JSON API Link takes an object with some options on it to customize the behavior of the link. The options you can pass are outlined below:

uri: the URI key is a string endpoint (optional when endpoints provides a default)
endpoints: root endpoint (uri) to apply paths to or a map of endpoints
customFetch: a custom fetch to handle REST calls
headers: an object representing values to be sent as headers on the request
credentials: a string representing the credentials policy you want for the fetch call
fieldNameNormalizer: function that takes the response field name and converts it into a GraphQL compliant name

Context
JSON API Link uses the headers field on the context to allow passing headers to the HTTP request. It also supports the credentials field for defining credentials policy.

headers: an object representing values to be sent as headers on the request
credentials: a string representing the credentials policy you want for the fetch call

Contributing
This project uses TypeScript to bring static types to JavaScript and uses Jest for testing. To get started, clone the repo and run the following commands:
npm install # or `yarn`

npm test # or `yarn test` to run tests
npm test -- --watch # run tests in watch mode

npm run check-types # or `yarn check-types` to check TypeScript types
To run the library locally in another project, you can do the following:
npm link

# in the project you want to run this in
npm link apollo-link-json-api
",3
o1-labs/snarky,HTML,"snarky
snarky is an OCaml front-end for writing R1CS SNARKs.
It is modular over the backend SNARK library, and comes with backends
from libsnark.
Disclaimer: This code has not been thoroughly audited and should not
be used in production systems.
Getting started

First install libsnark's dependencies by running scripts/depends.sh, or following the instructions here.
Then, make sure you have opam installed.
Finally, install snarky and its dependencies by running

opam pin add git@github.com:o1-labs/snarky.git
and answering yes to the prompts.
The best place to get started learning how to use the library are the annotated examples.

Election: shows how to use Snarky to verify an election was run honestly.
Merkle update: a simple example updating a Merkle tree.

Design
The intention of this library is to allow writing snarks by writing what look
like normal programs (whose executions the snarks verify). If you're an experienced
functional programmer, the basic idea (simplifying somewhat) is that there is a monad
Checked.t so that a value of type 'a Checked.t is an 'a whose computation is
certified by the snark. For example, we have a function
mul : var -> var -> (var, _) Checked.t.
Given v1, v2 : var, mul v1 v2 is a variable containg the product of v1 and v2,
and the snark will ensure that this is so.
Example: Merkle trees
One computation useful in snarks is verifying membership in a list. This is
typically accomplished using authentication paths in Merkle trees. Given a
hash entry_hash, an address (i.e., a list of booleans) addr0 and an
authentication path (i.e., a list of hashes) path0, we can write a checked
computation for computing the implied Merkle root:
  let implied_root entry_hash addr0 path0 =
    let rec go acc addr path =
      let open Let_syntax in
      match addr, path with
      | [], [] -> return acc
      | b :: bs, h :: hs ->
        let%bind l = Hash.if_ b ~then_:h ~else_:acc
        and r = Hash.if_ b ~then_:acc ~else_:h
        in
        let%bind acc' = Hash.hash l r in
        go acc' bs hs
      | _, _ -> failwith ""Merkle_tree.Checked.implied_root: address, path length mismatch""
    in
    go entry_hash addr0 path0
The type of this function is
val implied_root : Hash.var -> Boolean.var list -> Hash.var list -> (Hash.var, 'prover_state) Checked.t
The return type (Hash.var, 'prover_state) Checked.t indicates that the function
returns a ""checked computation"" producing a variable containing a hash, and can be
run by a prover with an arbitrary state type 'prover_state.
Compare this definition to the following ""unchecked"" OCaml function (assuming a function hash):
let implied_root_unchecked entry_hash addr0 path0 =
  let rec go acc addr path =
    match addr, path with
    | [], [] -> acc
    | b :: bs, h :: hs ->
      let l = if b then h else acc
      and r = if b then acc else h
      in
      let acc' = hash l r in
      go acc' bs hs
    | _, _ ->
      failwith ""Merkle_tree.implied_root_unchecked: address, path length mismatch""
  in
  go entry_hash addr0 path0
;;
The two obviously look very similar, but the first one can be run to generate an R1CS
(and also an ""auxiliary input"") to verify that computation.
Implementation
Currently, the library uses a free-monad style AST to represent the snark computation.
This may change in future versions if the overhead of creating the AST is significant.
Most likely it will stick around since the overhead doesn't seem to be too bad and it
enables optimizations like eliminating equality constraints.
",215
katspaugh/wavesurfer.js,JavaScript,"wavesurfer.js

 
Interactive navigable audio visualization using Web Audio and Canvas.

See a tutorial and examples on wavesurfer-js.org.
Browser support
wavesurfer.js works only in modern browsers supporting Web Audio.
It will fallback to Audio Element in other browsers (without graphics). You can also try wavesurfer.swf which is a Flash-based fallback.
FAQ
Can the audio start playing before the waveform is drawn?
Yes, if you use the backend: 'MediaElement' option. See here: https://wavesurfer-js.org/example/audio-element/. The audio will start playing as you press play. A thin line will be displayed until the whole audio file is downloaded and decoded to draw the waveform.
Can drawing be done as file loads?
No. Web Audio needs the whole file to decode it in the browser. You can however load pre-decoded waveform data to draw the waveform immediately. See here: https://wavesurfer-js.org/example/audio-element/ (the ""Pre-recoded Peaks"" section).
API in examples
Choose a container:
<div id=""waveform""></div>
Create an instance, passing the container selector and options:
var wavesurfer = WaveSurfer.create({
    container: '#waveform',
    waveColor: 'violet',
    progressColor: 'purple'
});
Subscribe to some events:
wavesurfer.on('ready', function () {
    wavesurfer.play();
});
Load an audio file from a URL:
wavesurfer.load('example/media/demo.wav');
Documentation
See the documentation on all available methods, options and events on the homepage.
Upgrade
See the upgrade document if you're upgrading from a previous version of wavesurfer.js.
Using with a module bundler
Wavesurfer can be used with a module system like this:
// import
import WaveSurfer from 'wavesurfer.js';

// commonjs/requirejs
var WaveSurfer = require('wavesurfer.js');

// amd
define(['WaveSurfer'], function(WaveSurfer) {
  // ... code
});

Related projects
For a list of  projects using wavesurfer.js, check out
the projects page.
Development



Install development dependencies:
npm install

Development tasks automatically rebuild certain parts of the library when files are changed (start – wavesurfer, start:plugins – plugins). Start a dev task and go to localhost:8080/example/ to test the current build.
Start development server for core library:
npm run start

Start development server for plugins:
npm run start:plugins

Build all the files. (generated files are placed in the dist directory.)
npm run build

Running tests only:
npm run test

Build documentation with esdoc (generated files are placed in the doc directory.)
npm run doc

If you want to use the VS Code - Debugger for Chrome, there is already a launch.json with a properly configured sourceMapPathOverrides for you.
Editing documentation
The homepage and documentation files are maintained in the gh-pages branch. Contributions to the documentation are especially welcome.
Credits
Initial idea by Alex Khokhulin. Many
thanks to
the awesome contributors!
License

This work is licensed under a
BSD 3-Clause License.
",3617
metabake/mbake,None,"Metabake
'All my friends KNOW the low-coder'
Metabake is open source and extensible low-code productivity tool for programmers.
Overview
Metabake(mbake) provides all the necessary tools for a developer to write cleaner code faster, with pug templating and livereload out of the box.
You can gradually adopt it while it allows you to develop faster - and with clean simplicity - Static Websites, Web Components, custom CMS/E-Commerce, CRUD and all sorts of dynamic web-apps.
Prerequisites
You should know HTML, CSS and JavaScript - that is all we use. If you need to catch up, we recommend this book: 'Design and Build Websites' by Jon Duckett.
Quick start
yarn global add mbake
mbake -w . /* for a base website */
cd website
mbakeX -w . /* to run the watcher/livereload */
Metabake in 4 Minutes
Building sites take a few moments, just add index.pug and dat.yaml files in the folder, and compile it with mbake . from the root folder of your site.
Example
Create a folder called 'one'.
In the folder 'one', create file index.pug
header
body
   p Hello #{key1}

and create file dat.yaml
key1: World

Note: to create a new page|screen in mbake, create a new folder with an index.pug and day.yaml in each folder.

Now make with mbake:
mbake .
This will create index.html. Of course you can use regular Pug syntax to include other Pug files or Markdown. (Metabake Markdown flavor includes CSS support):
body
   div
      include:metaMD comment.md
And example Markdown file with CSS nested classes. Title is nested in 2 levels, .column class CSS and second level .stick CSS class
:::: column col-2
::: stick
Title 
:::
::::


So if you write a Markdown file comment.md, it will be included in index.html
Watcher
This will start a webserver and auto-refresh browser, and watch for file changes to auto build:
mbakeX -w .
Instead of . you can specify any path.
Also, the fact that we are generating this static content allows us to have the entire webapp served by a CDN
SASS
CSS can be hard to work with, so people use Sass/Scss. Create a style.scss file:
$font-stack: Helvetica, sans-serif;
$primary-color: #333;

body {
   font: 100% $font-stack;
   color: $primary-color;
}
Create file assets.yaml in assets folder, to compile your scss to css
css:
- style.scss

and run
mbake -s .
It will create a css file in assets/css with auto-prefixes.
So the structure of asset folder should look something like that:
assets/
   css/style.css /* this is going to be compiled from style.scss */
   scss/style.scss /* your working area */
   assets.yaml /* with `scss` files that need to be compiled */
	...

TypeScript
TypeScript is supper-set of JavaScript. Write a ts file, like foo.ts:
foo(i:number) {
	console.log('oh hi')
}
and run
mbake -t .
It will create a .js and min.js files. It will output ES5 to support IE11, so feel free to use class { } syntax.
If there is no .ts, than it will simply slightly mimifify js files into min.js (but no ES5 conversion).
Lots of time you use .ts to call DB services: such as Google FireStore.
Examples - Website
There are 12 very different examples included in the mbake CLI. One is just a website:
mbake -w
That will extract an example website in the current folder. ( Obviously you can create any layout with any combination of css and other libraries, but here is how we laid out an example/starter website).
Dynamic data/CRUD/'ViewModel'
This relates to dynamic data, not static content (static eg: CMS or eCommerce). To extract an example CRUD web-app in the current folder:
mbake -u
It has a README.md in root of the website that you can glance.
CMS/Itemize example
Itemize (eg CMS)
Lets build a folder called Items and in that folder create a blank file dat_i.yaml, with nothing there.


In the folder called Items create folder Page1 and folder Page2. In each page folder create index.pug and dat.yaml. So you have Page1 and Page2 folder under Items.


In each Page's dat.yaml add


title: Page name

And add a few more key/value words in each dat.yaml, but make each pages values a bit different.

And now, in the folder Items run

mbake -i .
It will create items.json. This allows you to fetch that json and search for content, CMS, items, etc.
MetaBake FrameWork(FW)/Application Architecture(AA)
There is not much to ours.
mbake -f .
This emits a Pug file that you should include in your Pug's layout head section.
In turn, the included file calls a 'setup-deffs' js file that defines and show you how to define things you can require later.
Extras and next steps
Now that you know mbake foundation, here are some choices for next things to learn in the advanced docs, pick and chose:

CMS: an admin panel that you can host to can use as is; or as a base to build commercial grade CMS or eCommerce site, including browser plugin.
MetaCake: plugin components, makes it easy for designers to write real web-apps. Developed with RIOTjs, easier than Reactjs (commercial license optional)
AMP
SPA router: with page transition effects and state machine (needed for cross-platform development)
Cross platform development with real single code base development: single code base for Web, AMP, Electron and PhoneGap/Crodova
VS code from the Cloud: multiple developers using a browser against same VS Code host in the cloud

Other examples include:

Using markdown CSS effect: allows non-programmers to write interactive stories
Slide show with markdown
Dashboard example
Ads example

Links

Full Docs: docs.Metabake.org
Metabake.org
blog.Metabake.net
Github
Check for the latest version of mbake: npm.js

",12
okta/okta-developer-docs,JavaScript,"
 
Okta Developer Site
The Okta developer site serves Okta's API documentation and guides, including:

API references
SDK references and sample code
Authentication quickstarts
Guides
Developer Blog (not published from this repo, see https://github.com/okta/okta.github.io)

If you have questions or need help with Okta's APIs or SDKs, visit the Developer Forum. You can also email developers@okta.com to create a support ticket.
Getting Started
Okta's developer documentation (this repo) is built using the VuePress site generator.
There are currently 2 parts to the site, the content and the theming/plugins.
Requirements

Node: 8+
Yarn: 1.7+

Before getting started, open a terminal window and make sure these commands work:
node --version

yarn --version
Local setup


Fork this repository


Clone (use the Clone or download button above)


Install the dependencies with yarn:


cd okta-developer-docs

yarn install
This will install everything you need to build the documentation on your machine.
Previewing the site
With the above steps completed, you can start a preview server by running this command inside the cloned directory:
yarn dev
This starts a preview server on your machine, and watches all files for changes. Open http://localhost:8080/documentation/ to view the documentation.

Note: if you try to visit the root, you will get a 404 page.  You must visit a path corresponding to a directory under vuepress-site, like /documentation.

The preview server supports hot reloading. Once the server is running on your machine, any changes you make to Markdown content will appear automatically in your browser within a few seconds. Note that changes to page frontmatter or site configuration require you to stop and start the preview server.
Adding and editing pages
Documentation pages are stored as Markdown files in the /packages/@okta/vuepress-site directory.
As an example, lets say you want to edit the Users API page. The public path of this page is /docs/api/resources/users/.
To edit this page, you would navigate to /packages/@okta/vuepress-site/docs/api/resources/users/index.md and edit that Markdown file.
An index.md file in a directory like users will be served as /users/ when the site is live. If you name the file anything other than index.md, you will need to include .html in the URL when you view the page in your browser.
More information about writing content for VuePress can be found in our VuePress Authoring Guidelines. There you will also find our Style Guide.
What About Building the Site Before Committing?
There is no need to build the rendered site before committing and submitting a PR. This will all happen on the CI side to test and build the rendered site.
Running tests
Running the tests before committing should be done and can be accomplished by running yarn test from the terminal. This will run a series of tests to make sure that everything is working as expected and that your changes did not affect anything that was not planned.

Note: If you're already running the preview server locally, you can run yarn test-only instead. This skips starting up the preview server.

If your test run fails unexpectedly, try executing yarn stop and running the tests again.
Theme and Plugin Contribution
The theme and plugins are in separate packages from content. All of the theme files live in /packages/@okta/vuepress-theme-default – see that package's readme for more info.
Updating Guides
Guides Directory Structure
Summary
Each guide URL:

/guides/NAME-OF-GUIDE/NAME-OF-FRAMEWORK/NAME-OF-SECTION
Maps to a directory tree:
guides/NAME-OF-GUIDE/NAME-OF-SECTION/index.md
Framework-specific text/code bits are in separate files named 'snippets' that are referenced in the content by their snippet name.  Their content is found in:
guides/NAME-OF-GUIDE/NAME-OF-SECTION/NAME-OF-FRAMEWORK/SNIPPET-NAME.md

Details
Every guide is based in a subdirectory under guides/ in packages/@okta/vuepress-site/.  This directory name is used in the url of the guide, so follow best practices for URLs (human-readable, lowercase, no spaces, no special characters other than '-').
The file guides/index.md contains the meta-data for the guides overall in the front matter, notably, the ordered list of guides to offer and the ordered list of ""featured"" guides for the main Guides page. (TODO: Move content from GuidesOverview to this index.md file) If a guide is not listed here, it IS on the site but is NOT linked to.
Each guide directory will have a number of section subdirectories.  These are used in the urls for each section of the guide, so follow best practices for URLs in naming these directories.  Each guide directory has an index.md file that holds meta-data for the guide, notably the ordered list of section directories.  Any section directory not listed can be accessed on the site but will not be linked to.  Each section directory has an index.md file that holds the content for that section.
If a guide section has framework-specific content, you can use <StackSelector snippet=""SNIPPET-NAME""/> where SNIPPET-NAME is a section-specific indicator of your choice.  This does NOT appear in a url, but please follow common filename conventions.
Content for the StackSelector snippets are found in guides/NAME-OF-GUIDE/NAME-OF-SECTION/NAME-OF-FRAMEWORK/SNIPPET-NAME.md files.
Writing a guide

Create the guide directory
Create the guide index.md
Create a subdirectory for every section
Put your content into the index.md file for each section subdirectory
For any snippets you declare, create the NAME-OF-FRAMEWORK/NAME-OF-SNIPPET.md files in the section subdirectory
Make sure the index.md file for the section includes the title of the section
Make sure the index.md file for the guide includes the title for the guide and the list of all section subdirectories (in order)
Make sure the main guides index.md file lists your guide in the desired position in the order

Using the ""Stack Selector""
Many guide sections will have one or more areas of framework-specific content.  This can be code, instructions, or a mix of the two.  When a guide section has need of such content, simply use <StackSelector snippet=""SNIPPET-NAME""/> in your section index.md content where you want some framework-specific content to appear.  The create the relevant content in NAME-OF-FRAMEWORK/SNIPPET-NAME.md files for every relevant framework.
Writing framework-specific content
Each guide should have the same list of frameworks for all StackSelectors in all sections of that guide, however each individual guide can have a distinct list of frameworks that does not have to be the same as different guides. For example, a mobile guide might have ios, android, and reactnative frameworks, while a front-end web guide might have react, angular, and vue options.
The framework names in the directories should be lowercased and without special characters.  The list of frameworks supported with icons and human names are:

android
ios
reactnative
xamarin
angular
preact
react
vue
go
node
java
php
python
spring
dotnet
dotnetcore
aspnet
aspnetcore

Linking between sections and guides
Always have a trailing slash at the end of your guides link.  (Example: /guides/NAME-OF-GUIDE/-/NAME-OF-SECTION/)
Links and Guides have some complications because of the ""magic"" nature of the selected framework.  For links within or between guides, follow these examples:

Linking to a different section of the same guide:

Use <GuideLink> and always go ""up"" one directory
<GuideLink link=""../NAME-OF-SECTION"">Text to show</GuideLink>
This will maintain the selected framework


Linking to a different guide:

Normal markdown links work
[Text to Show](/guides/NAME-OF-GUIDE/)
This will select the first framework and first section and update the url to match.


Linking to a specific section of a different guide:

Normal markdown links work
use - in place of a framework if you aren't linking to a specific framework
[Text to Show](/guides/NAME-OF-GUIDE/-/NAME-OF-SECTION/)
This will select the first framework and update the url to match


Linking to another section as part of the guide navigation

<NextSectionLink/> - Provides a ""button"" link to the next section
<NextSectionLink>Some Example Text</NextSectionLink> - Provides the ""button"" with different text
<NextSectionLink name=""next-steps""/> - Provides the ""button"" to link to the named section of the guide (doesn't have to be the ""next"" section)



",9
collin80/due_can,C++,"due_can
Object oriented canbus library for Arduino Due compatible boards
Implements both CAN buses exposed by Due hardware.
This library requires the can_common library now. That library
is a common base that other libraries can be built off of to allow
a more universal API for CAN.
The needed can_common library is found here:
https://github.com/collin80/can_common
",149
voidsatisfaction/TIL,Jupyter Notebook,"Today I Learned(Since 2017. 2. 3.)
Special thanks to milooy
voidsatisfaction의 개인 wiki형식의 배움의 놀이터 입니다.
프로그래밍 뿐 아니라, AI / 금융 / 철학 / 종교학 등의 다양한 지식을 넣어두는 보물 창고 입니다.
내가 추구하는 것
아키텍트

아키텍트란

문제의 본질을 포괄적으로 이해
다양한 도구의 본질을 이해하고 어느 상황에서 어떤 타이밍에 사용해야할지 앎
배움을 즐김
자기자신을 사랑하고, 마음의 평온을 이루어 냄



아키텍트는 프로그래머가 아닙니다. 내가 나 자신을 프로그래머라고 규정하는 순간, 망치를 든 사람은 모든것이 못으로 보이듯이, 모든 문제를 프로그래밍적으로 해결하려고 무의식적으로 생각이 들기 때문입니다.
그래서 나의 정체성은 프로그래머가 아닌, 아키텍트이며, 문제 해결사 라고 생각합니다. 이렇게 사고를 확장시키면, 어떠한 문제에 대해서 다양한 도구를 바탕으로 다양한 시각에서 문제를 해결할 수 있게 됩니다.

제가 생각하는 아키텍트의 정의는, ""문제의 본질을 포괄적이고 논리적으로 이해할 수 있으며, 그러한 문제들을 모델링 할 수 있고, 그에 적절한 해결책을 논리적 이유와 함께 제시할 수 있는 사람"" 이라고 생각합니다.

문제의 본질을 포괄적이고 논리적으로 이해하고 있기 때문에 10살 꼬마아이가 쉽게 이해할 수 있도록 그러한 문제의 본질을 설명할 수 있을것입니다.

좋은 아키텍트는 다양한 도구들의 본질을 파악하여, 어떠한 상황과 타이밍에 사용할지를 정확히 알고, 행동을 할 때에는 과감한 가설검증의 프로세스를 반복해 나가는 사람입니다. 처음부터 완벽한 솔루션이 아닌, 살아있는 솔루션, 즉, 점점 발달해가는 솔루션을 빠르게 개발나가는 사람입니다. 이는 좋은 아키텍트는 우리가 세상에 대해서 잘 모르고, 세상은 계속 변화한다는 점을 이해하고 있기 때문입니다.

그리고 좋은 아키텍트는 배우는 것을 즐기는 사람입니다. 배움의 대상은 언제나 열려있습니다. 나보다 경력 많고 지혜로운 다른 아키텍트 뿐 아니라, 나와 동년배의 같은 프로그래머, 다른 직업을 갖고있는 친구들, 3살짜리 어린아이에게도 배울 점이 있을 것이고, 그것을 흡수하는 것을 즐깁니다. 자기자신이 하루에 한 발자국이라도 성장해 가는것을 즐길 수 있는 사람입니다. 그리고 언제나 그들로부터 배웠다는 것 즉, 자신은 수 많은 거인덕분에 거인의 어깨의 위에서 세상을 볼 수 있었다는 것을 기억하는 사람입니다.
마지막으로 좋은 아키텍트는 자기자신을 세상 무엇보다도 사랑하는 사람이어야 합니다. 자기자신의 마음의 안정과 존중 없이는 세상의 문제를 다양한 관점에서 바라볼 수 없습니다. 그 무엇을 하더라도 자기자신을 잃어버리게 된다면 그것은 모든것을 잃어버리는 것과 마찬가지 입니다. 그러므로 좋은 아키텍트는 자기자신이 충분히 잘 하고 있다는 것을 인정합니다. 또한 자신의 밖에서 어떠한 일이 닥치더라도, 모든것은 마음이 만들어낸 것이라는 진리를 이해하고 있으며, 그렇기 때문에 마음이 언제나 고요한 밤 바다처럼 평정을 유지할 것입니다.



16 voidsatisfaction(텅 빈 충만)



",5
istio/istio,Go,"Istio





An open platform to connect, manage, and secure microservices.

For in-depth information about how to use Istio, visit istio.io
To ask questions and get assistance from our community, visit discuss.istio.io
To learn how to participate in our overall community, visit our community page

In this README:

Introduction
Repositories
Issue management

In addition, here are some other documents you may wish to read:

Istio Community - describes how to get involved and contribute to the Istio project
Istio Developer's Guide - explains how to set up and use an Istio development environment
Project Conventions - describes the conventions we use within the code base
Creating Fast and Lean Code - performance-oriented advice and guidelines for the code base

You'll find many other useful documents on our Wiki.
Introduction
Istio is an open platform for providing a uniform way to integrate
microservices, manage traffic flow across microservices, enforce policies
and aggregate telemetry data. Istio's control plane provides an abstraction
layer over the underlying cluster management platform, such as Kubernetes.
Istio is composed of these components:


Envoy - Sidecar proxies per microservice to handle ingress/egress traffic
between services in the cluster and from a service to external
services. The proxies form a secure microservice mesh providing a rich
set of functions like discovery, rich layer-7 routing, circuit breakers,
policy enforcement and telemetry recording/reporting
functions.

Note: The service mesh is not an overlay network. It
simplifies and enhances how microservices in an application talk to each
other over the network provided by the underlying platform.



Mixer - Central component that is leveraged by the proxies and microservices
to enforce policies such as authorization, rate limits, quotas, authentication, request
tracing and telemetry collection.


Pilot - A component responsible for configuring the proxies at runtime.


Citadel - A centralized component responsible for certificate issuance and rotation.


Citadel Agent - A per-node component responsible for certificate issuance and rotation.


Galley- Central component for validating, ingesting, aggregating, transforming and distributing config within Istio.


Istio currently supports Kubernetes and Consul-based environments. We plan support for additional platforms such as
Cloud Foundry, and Mesos in the near future.
Repositories
The Istio project is divided across a few GitHub repositories.


istio/istio. This is the main repository that you are
currently looking at. It hosts Istio's core components and also
the sample programs and the various documents that govern the Istio open source
project. It includes:

security. This directory contains security related code,
including Citadel (acting as Certificate Authority), citadel agent, etc.
pilot. This directory
contains platform-specific code to populate the
abstract service model, dynamically reconfigure the proxies
when the application topology changes, as well as translate
routing rules into proxy specific configuration.
istioctl. This directory contains code for the
istioctl command line utility.
mixer. This directory
contains code to enforce various policies for traffic passing through the
proxies, and collect telemetry data from proxies and services. There
are plugins for interfacing with various cloud platforms, policy
management services, and monitoring services.



istio/api. This repository defines
component-level APIs and common configuration formats for the Istio platform.


istio/proxy. The Istio proxy contains
extensions to the Envoy proxy (in the form of
Envoy filters), that allow the proxy to delegate policy enforcement
decisions to Mixer.


Issue management
We use GitHub combined with ZenHub to track all of our bugs and feature requests. Each issue we track has a variety of metadata:


Epic. An epic represents a feature area for Istio as a whole. Epics are fairly broad in scope and are basically product-level things.
Each issue is ultimately part of an epic.


Milestone. Each issue is assigned a milestone. This is 0.1, 0.2, ..., or 'Nebulous Future'. The milestone indicates when we
think the issue should get addressed.


Priority/Pipeline. Each issue has a priority which is represented by the Pipeline field within GitHub. Priority can be one of
P0, P1, P2, or >P2. The priority indicates how important it is to address the issue within the milestone. P0 says that the
milestone cannot be considered achieved if the issue isn't resolved.


We don't annotate issues with Releases; Milestones are used instead. We don't use GitHub projects at all, that
support is disabled for our organization.
",17540
StanfordCS194/Team-7,JavaScript,"Team-7: Stanford Insider

A MERN stack app that provides insider information on finding the best kept secrets on campus. This app includes crowdsourced information for live updates. You can find a User Guide and documentation of our stack on our Wiki!
Team Members



Member
Email
Photo




Edward Guzman
eguzman3@stanford.edu



Krishan Kumar
krishank@stanford.edu



Ben Jun Xiong Wu
benjxwu@stanford.edu



Jerry Chen
jchen98@stanford.edu




Team Skills Matrix



Member
Skills
Personal Traits
Desired Growth
Weaknesses




Edward
Systems, back-end, math, some AI, some Android
Methodical, fast learner, good memory
Product design, project management
Time management, UI/UX


Krishan
Python programming, statistical modeling, data science
User-oriented and data-driven
Understanding of the full stack
Web dev


Ben
OpenGL, C++, Python, Maya/Blender, Algorithms, Graphics Rendering, matrix methods
open-minded, abstract/creative ideas, attentive to small details, impulsive
Prototyping, testing, management
Indecisive, hindered productivity under extreme stress, reaching out for help


Jerry
Algorithms, Physical prototyping (machining and woodworking), Good memory
Flexible, adjustable sleep schedule, devil's advocate
General experience
Idea generation



Team Communication

Intra-team communication via Messenger.
To contact us, email: eguzman3@stanford.edu, krishank@stanford.edu, benjxwu@stanford.edu, jchen98@stanford.edu.

",2
MatterHackers/MatterControl,C#,"MatterControl




Master




Linux



Windows




MatterControl is an open-source program designed to control and enhance the desktop 3D printing experience. It's designed to help you get the most out of your 3D printer - making it easy to track, preview, and print your 3D parts. Development of MatterControl is sponsored by MatterHackers and it's partners.

Features

Integrated slicing engine MatterSlice
Library for managing your STL files
Built in profiles for a plethora of different printers.
Built in editing tools along with plugins for creating text, images, and braille.
Queue of items you are going to print, and history of items you have printed.
2D/3D preview of the sliced object.
Advanced printer controls, including the ability to make adjustments while printing.
Software based print leveling.
Remote monitoring of your printer, along with SMS/email notifications when your print is completed.

Download

Windows
Mac
Linux

Release Notes
Building from Source
MatterControl is written in C#. It uses the agg-sharp GUI abstraction layer. See this wiki article if you want to contribute code.


Checkout the latest source code and submodules:
 git clone --recursive https://github.com/MatterHackers/MatterControl.git
 cd MatterControl



Install MonoDevelop and Nuget.
 sudo apt-get install monodevelop nuget



Add Mono SSL Support - Copy in Mozilla Root certificates to enable NuGet and MatterControl SSL requests
 mozroots --import --sync



Restore NuGet packages - On MonoDevelop 4.0 or older you can install NuGet Addin. If you are on Mint, also install libmono-cairo2.0-cil. Alternatively you can run the command line NuGet application to restore the project packages:
 nuget restore MatterControl.sln



Optionally switch to a target branch
 git checkout master
 git submodule update --init --recursive

As a single command line statement:
 targetBranch=master && git checkout $targetBranch && git submodule update --init --recursive



Build MatterControl
 mdtool build -c:Release MatterControl.sln

or
 xbuild /p:Configuration=Release MatterControl.sln



Link the StaticData from your source directory to the build directory
 ln -s ../../StaticData bin/Release/StaticData



After MatterControl has been built in MonoDevelop it is recommended that you run the application via command line or via a shell script to invoke mono.
 mono bin/Release/MatterControl.exe

If you'd like to log errors for troubleshooting
 mono bin/Release/MatterControl.exe > log.txt

If you want detailed error logging and tracing
 MONO_LOG_LEVEL=debug mono bin/Release/MatterControl.exe > log.txt



In order for MatterControl to access the serial ports, you will need to give your user the appropriate permissions. On Debian based distros, add yourself to the dialout group. On Arch, add yourself the the uucp and lock groups instead.
 gpasswd -a $USER dialout



Serial Helper


Change to the SerialHelper directory
 cd Submodules/agg-sharp/SerialPortCommunication/SerialHelper



Run the build script
 ./build.sh



If your receive errors you may need to install libc6-dev-i386 for x86 compilation
 sudo apt-get install libc6-dev-i386



Help, Bugs, Feedback
For information on using MatterControl, check the MatterControl Wiki. If you have questions or feedback, feel free to post on the MatterHackers Forums or send an email to support@matterhackers.com. To report a bug, file an issue on GitHub.
",243
esp8266/Arduino,C,"Arduino core for ESP8266 WiFi chip
Quick links

Latest release documentation
Current ""git version"" documentation
Install git version (sources)

Arduino on ESP8266
This project brings support for ESP8266 chip to the Arduino environment. It lets you write sketches using familiar Arduino functions and libraries, and run them directly on ESP8266, no external microcontroller required.
ESP8266 Arduino core comes with libraries to communicate over WiFi using TCP and UDP, set up HTTP, mDNS, SSDP, and DNS servers, do OTA updates, use a file system in flash memory, work with SD cards, servos, SPI and I2C peripherals.
Contents

Installing options:

Using Boards Manager
Using git version
Using PlatformIO
Building with make


Documentation
Issues and support
Contributing
License and credits

Installing with Boards Manager
Starting with 1.6.4, Arduino allows installation of third-party platform packages using Boards Manager. We have packages available for Windows, Mac OS, and Linux (32 and 64 bit).

Install the current upstream Arduino IDE at the 1.8.7 level or later. The current version is at the Arduino website.
Start Arduino and open Preferences window.
Enter https://arduino.esp8266.com/stable/package_esp8266com_index.json into Additional Board Manager URLs field. You can add multiple URLs, separating them with commas.
Open Boards Manager from Tools > Board menu and install esp8266 platform (and don't forget to select your ESP8266 board from Tools > Board menu after installation).

Latest release 
Boards manager link: https://arduino.esp8266.com/stable/package_esp8266com_index.json
Documentation: https://arduino-esp8266.readthedocs.io/en/2.5.1/
Using git version (basic instructions)


Install the current upstream Arduino IDE at the 1.8 level or later. The current version is at the Arduino website.
Go to Arduino directory

For Mac OS X, it is Arduino.app showing as the Arduino icon.
This location may be your ~/Downloads, ~/Desktop or even /Applications.
cd <application-directory>/Arduino.app/Contents/Java

For Linux, it is ~/Arduino by default.
cd ~/Arduino



Clone this repository into hardware/esp8266com/esp8266 directory (or clone it elsewhere and create a symlink)

cd hardware
mkdir esp8266com
cd esp8266com
git clone https://github.com/esp8266/Arduino.git esp8266
cd esp8266
git submodule update --init

Download binary tools (you need Python 2.7)

cd esp8266/tools
python get.py

Restart Arduino

Using PlatformIO
PlatformIO is an open source ecosystem for IoT
development with cross platform build system, library manager and full support
for Espressif (ESP8266) development. It works on the popular host OS: macOS, Windows,
Linux 32/64, Linux ARM (like Raspberry Pi, BeagleBone, CubieBoard).

What is PlatformIO?
PlatformIO IDE
PlatformIO Core (command line tool)
Advanced usage -
custom settings, uploading to SPIFFS, Over-the-Air (OTA), staging version
Integration with Cloud and Standalone IDEs -
Cloud9, Codeanywhere, Eclipse Che (Codenvy), Atom, CLion, Eclipse, Emacs, NetBeans, Qt Creator, Sublime Text, VIM, Visual Studio, and VSCode
Project Examples

Building with make
makeEspArduino is a generic makefile for any ESP8266 Arduino project.
Using make instead of the Arduino IDE makes it easier to do automated and production builds.
Documentation
Documentation for latest development version: https://arduino-esp8266.readthedocs.io/en/latest/
Issues and support
ESP8266 Community Forum is a well established community for questions and answers about Arduino for ESP8266. If you need help, have a ""How do I..."" type question, have a problem with a 3rd party lib not hosted in this repo, or just want to discuss how to approach a problem , please ask there.
If you find the forum useful, please consider supporting it with a donation. 

If you encounter an issue which you think is a bug in the ESP8266 Arduino Core or the associated libraries, or if you want to propose an enhancement, you are welcome to submit it here on Github: https://github.com/esp8266/Arduino/issues.
Please provide as much context as possible, as well as the information requested in the issue template:

ESP8266 Arduino core version which you are using (you can check it in Boards Manager)
your sketch code; please wrap it into a code block, see Github markdown manual
when encountering an issue which happens at run time, attach serial output. Wrap it into a code block, just like the code.
for issues which happen at compile time, enable verbose compiler output in the IDE preferences, and attach that output (also inside a code block)
ESP8266 development board model
IDE settings (board choice, flash size)
etc

Contributing
For minor fixes of code and documentation, please go ahead and submit a pull request.
Check out the list of issues which are easy to fix — easy issues pending. Working on them is a great way to move the project forward.
Larger changes (rewriting parts of existing code from scratch, adding new functions to the core, adding new libraries) should generally be discussed by opening an issue first.
Feature branches with lots of small commits (especially titled ""oops"", ""fix typo"", ""forgot to add file"", etc.) should be squashed before opening a pull request. At the same time, please refrain from putting multiple unrelated changes into a single pull request.
License and credits
Arduino IDE is developed and maintained by the Arduino team. The IDE is licensed under GPL.
ESP8266 core includes an xtensa gcc toolchain, which is also under GPL.
Esptool written by Christian Klippel is licensed under GPLv2, currently maintained by Ivan Grokhotkov: https://github.com/igrr/esptool-ck.
Espressif SDK included in this build is under Espressif MIT License.
ESP8266 core files are licensed under LGPL.
SPI Flash File System (SPIFFS) written by Peter Andersson is used in this project. It is distributed under MIT license.
umm_malloc memory management library written by Ralph Hempel is used in this project. It is distributed under MIT license.
SoftwareSerial library and examples written by Peter Lerup. Distributed under LGPL 2.1.
axTLS library written by Cameron Rich, built from https://github.com/igrr/axtls-8266, is used in this project. It is distributed under BSD license.
BearSSL library written by Thomas Pornin, built from https://github.com/earlephilhower/bearssl-esp8266, is used in this project.  It is distributed under the MIT License.
",9422
cloudendpoints/esp,C++,"The Extensible Service Proxy
Extensible Service Proxy, a.k.a. ESP is a proxy which enables API management
capabilities for JSON/REST or gRPC API services. The current implementation is
based on an NGINX HTTP reverse proxy server.
ESP provides:


Features: authentication (auth0, gitkit), API key validation, JSON to gRPC
transcoding, as well as  API-level monitoring, tracing and logging. More
features coming in the near future: quota, billing, ACL, etc.


Easy Adoption: the API service can be implemented in any coding language
using any IDLs.


Platform flexibility: support the deployment on any cloud or on-premise
environment.


Superb performance and scalability: low latency and high throughput


ESP can Run Anywhere
However, the initial development was done on Google App Engine Flexible
Environment, GCE and GKE for API services using Open API
Specification and so our instructions
and samples are focusing on these platforms. If you make it work on other
infrastructure and IDLs please let us know and contribute instructions/code.
Prerequisites
Common prerequisites used irrespective of operating system and build tool
chain are:

Git
Node.js is required for running included example
Endpoints bookstore application.

Getting ESP
To download the Extensible Service Proxy source code, clone the ESP repository:
# Clone ESP repository
git clone https://github.com/cloudendpoints/esp

# Initialize Git submodules.
git -C esp submodule update --init --recursive

Released ESP docker images
ESP docker images are released regularly. The regular images are named as gcr.io/endpoints-release/endpoints-runtime:MAJOR_VERSION.MINOR_VERSION.PATCH_NUMBER. For example, gcr.io/endpoints-release/endpoints-runtime:1.30.0 has MAJOR_VERSION=1, MINOR_VERSION=30 and PATCH_NUMBER=0.
Symbolically linked images:

MAJOR_VERSION is linked to the latest image with same MAJOR_VERSION.

For example, gcr.io/endpoints-release/endpoints-runtime:1 is always pointed to the latest image with ""1"" major version.
Secure image:
Normally ESP container runs as root, it is deemed as not secure. To make ESP container secure, it should be run as non-root and its root file system should be read-only. Normal docker images can be made to run as non-root, but such change may break some existing users. Starting 1.31.0, a new secure image is built with suffix ""-secure"" in the image name, e.g. gcr.io/endpoints-release/endpoints-runtime-secure:1.31.0.  It will be run as non-root.
You can switch to use the secure images if the followings are satisfied:

Nginx is not listening on ports requiring root privilege (ports < 1024).
Not use custom nginx config. The server_config path is hard coded to /etc/nginx/ folder in the custom nginx config. The secure image moved the server_config to /home/nginx. Please modify your custom nginx config before using the secure image.

If some folders can be mounted externally, the root system can be made read-only. Please see this GKE deployment yaml file as example on how to make root system read-only.
Repository Structure

doc: Documentation
docker: Scripts for packaging ESP in a Docker image.
include: Extensible Service Proxy header files.
src: Extensible Service Proxy source.
google and third_party: Git submodules containing
dependencies of ESP, including NGINX.
script: Scripts used for build, test, and continuous integration.
test: Applications and client code used for end-to-end testing.
tools: Assorted tooling.
start_esp: A Python start-up script for the ESP proxy. The script includes a generic nginx configuration template and fetching logic to retrieve service configuration from Google Service Management service.

ESP Tutorial
To find out more about building, running, and testing ESP, please review

Build ESP on Ubuntu 16.04
ESP Tutorial
Testing ESP with Bazel
Run ESP on Kubernetes

Contributing
Your contributions are welcome. Please follow the contributor
guidlines.
",162
flutter/engine,C++,"Flutter Engine

Flutter is a new way to build high-performance, cross-platform mobile apps.
Flutter is optimized for today's, and tomorrow's, mobile devices. We are
focused on low-latency input and high frame rates on Android and iOS.
The Flutter Engine is a portable runtime for hosting
Flutter applications.  It implements Flutter's core
libraries, including animation and graphics, file and network I/O,
accessibility support, plugin architecture, and a Dart runtime and compile
toolchain. Most developers will interact with Flutter via the Flutter
Framework, which provides a modern,
reactive framework, and a rich set of platform, layout and foundation widgets.
If you are new to Flutter, then you will find more general information
on the Flutter project, including tutorials and samples, on our Web
site at Flutter.dev. For specific information
about Flutter's APIs, consider our API reference which can be found at
the docs.flutter.dev.
If you intend to contribute to Flutter, welcome! You are encouraged to
start with our contributor
guide,
which helps onboard new team members.
",2960
FISCO-BCOS/spring-boot-starter,Java,"English / 中文
Spring Boot Starter



The sample spring boot project is based on Web3SDK, which provides the basic framework and basic test cases for blockchain application and helps developers to quickly develop applications based on the FISCO BCOS blockchain. The version only supports FISCO BCOS 2.0.
Quickstart
Precodition
Build FISCO BCOS blockchain, please check out here。
Configuration
Download
$ git clone https://github.com/FISCO-BCOS/spring-boot-starter.git

Certificate Configuration
Copy the ca.crt, node.crt, and node.key files in the node's directory nodes/${ip}/sdk to the project's src/main/resources directory.
Settings
The application.yml of the spring boot project is shown below, and the commented content is modified according to the blockchain node configuration.
encryptType: 0  # 0:standard, 1:guomi
groupChannelConnectionsConfig:
  allChannelConnections:
  - groupId: 1  #group ID
    connectionsStr:
                    - 127.0.0.1:20200  # node listen_ip:channel_listen_port
                    - 127.0.0.1:20201
  - groupId: 2
    connectionsStr:
                    - 127.0.0.1:20202
                    - 127.0.0.1:20203
channelService:
  groupId: 1 # The specified group to which the SDK connects
  orgID: fisco # agency name
A detail description of the SDK configuration for the project, please checkout  here。
Run
Compile and run test cases:
$ ./gradlew build

When all test cases run successfully, it means that the blockchain is running normally,and the project is connected to the blockchain through the SDK. You can develop your blockchain application based on the project。
Test Case Introduction
The sample project provides test cases for developers to use. The test cases are mainly divided into tests for Web3j API, Precompiled Serveice API, Solidity contract file to Java contract file, deployment and call contract.
Web3j API Test
Provide Web3jApiTest class to test the Web3j API. The sample test is as follows:
@Test
public void getBlockNumber() throws IOException {
    BigInteger blockNumber = web3j.getBlockNumber().send().getBlockNumber();
    System.out.println(blockNumber);
    assertTrue(blockNumber.compareTo(new BigInteger(""0""))>= 0);
}

Tips: The Application class initializes the Web3j object, which can be used directly in the way where the business code needs it. The usage is as follows:
@Autowired
private Web3j web3j

Precompiled Service API Test
Provide PrecompiledServiceApiTest class to test the Precompiled Service API。The sample test is as follows:
@Test
public void testSystemConfigService() throws Exception {
    SystemConfigSerivce systemConfigSerivce = new SystemConfigSerivce(web3j, credentials);
    systemConfigSerivce.setValueByKey(""tx_count_limit"", ""2000"");
    String value = web3j.getSystemConfigByKey(""tx_count_limit"").send().getSystemConfigByKey();
    System.out.println(value);
    assertTrue(""2000"".equals(value));
}

Solidity contract file to Java contract file Test
Provide SolidityFunctionWrapperGeneratorTest class to test contract compilation. The sample test is as follows:
@Test
public void compileSolFilesToJavaTest() throws IOException {
    File solFileList = new File(""src/test/resources/contract"");
    File[] solFiles = solFileList.listFiles();

    for (File solFile : solFiles) {

        SolidityCompiler.Result res = SolidityCompiler.compile(solFile, true, ABI, BIN, INTERFACE, METADATA);
        System.out.println(""Out: '"" + res.output + ""'"");
        System.out.println(""Err: '"" + res.errors + ""'"");
        CompilationResult result = CompilationResult.parse(res.output);
        System.out.println(""contractname  "" + solFile.getName());
        Path source = Paths.get(solFile.getPath());
        String contractname = solFile.getName().split(""\\."")[0];
        CompilationResult.ContractMetadata a = result.getContract(solFile.getName().split(""\\."")[0]);
        System.out.println(""abi   "" + a.abi);
        System.out.println(""bin   "" + a.bin);
        FileUtils.writeStringToFile(new File(""src/test/resources/solidity/"" + contractname + "".abi""), a.abi);
        FileUtils.writeStringToFile(new File(""src/test/resources/solidity/"" + contractname + "".bin""), a.bin);
        String binFile;
        String abiFile;
        String tempDirPath = new File(""src/test/java/"").getAbsolutePath();
        String packageName = ""org.fisco.bcos.temp"";
        String filename = contractname;
        abiFile = ""src/test/resources/solidity/"" + filename + "".abi"";
        binFile = ""src/test/resources/solidity/"" + filename + "".bin"";
        SolidityFunctionWrapperGenerator.main(Arrays.asList(
                ""-a"", abiFile,
                ""-b"", binFile,
                ""-p"", packageName,
                ""-o"", tempDirPath
        ).toArray(new String[0]));
    }
    System.out.println(""generate successfully"");
}

This test case converts all Solidity contract files (HelloWorld contract provided by default) in the src/test/resources/contract directory to the corresponding abi and bin files, and save them in the src/test/resources/solidity directory. Then convert the abi file and the corresponding bin file combination into a Java contract file, which is saved in the src/test/java/org/fisco/bcos/temp directory. The SDK will use the Java contract file for contract deployment and invocation.
Deployment and Invocation Contract Test
Provide ContractTest class to test deploy and call contracts. The sample test is as follows:
@Test
public void deployAndCallHelloWorld() throws Exception {
    //deploy contract
    HelloWorld helloWorld = HelloWorld.deploy(web3j, credentials, new StaticGasProvider(gasPrice, gasLimit)).send();
    if (helloWorld != null) {
        System.out.println(""HelloWorld address is: "" + helloWorld.getContractAddress());
        //call set function
        helloWorld.set(""Hello, World!"").send();
        //call get function
        String result = helloWorld.get().send();
        System.out.println(result);
        assertTrue( ""Hello, World!"".equals(result));
    }
}

Developing & Contributing

Star our Github.
Pull requests. See CONTRIBUTING.
Ask questions.
Discuss in WeChat group  or Gitter.

Related Links

For FISCO BCOS project, please check out FISCO BCOS Documentation。
For Web3SDK project, please check out Web3SDK Documentation。
For Spring Boot applications, please check out Spring Boot。

Community
By the end of 2018, Financial Blockchain Shenzhen Consortium (FISCO) has attracted and admitted more than 100 members from 6 sectors including banking, fund management, securities brokerage, insurance, regional equity exchanges, and financial information service companies. The first members include the following organizations: Beyondsoft, Huawei, Shenzhen Securities Communications, Digital China, Forms Syntron, Tencent, WeBank, Yuexiu FinTech.


Join our WeChat 


Discuss in 


Read news by 


Mail us at 


",13
Lombiq/Orchard-Liquid-Markup,C#,"Orchard Liquid Markup Readme
Project Description
Orchard module for adding support for templates written in Liquid Markup (http://liquidmarkup.org/). Uses DotLiquid (http://dotliquidmarkup.org/).
Documentation
Overview
This module adds the ability to use shape templates written in Liquid Markup and uses the DotLiquid library. See what Liquid offers. The module is also available for DotNest sites for templating.
Naming conventions are C#-style! This means that all the properties you can access on viewmodels from your templates will have the same names as usual but built-in Liquid filters (like upcase) will also behave in the same way (i.e. you'll be able to use it as Upcase).
There are the following features in the module:

Liquid Markup: doesn't provide any user-accessible features, just basic services.
Liquid Markup Templates: extends Orchard.Templates so you can write Liquid templates from the admin.
Liquid Markup View Engine: adds a view engine that enables you to use .liquid Liquid-formatted templates in your themes and modules. You can use this to develop Liquid templates quickly from an IDE.

Note that since Liquid was designed to be safe you can't call arbitrary methods on services that may be accessible from templates.
Examples
Do note the following:

Although presented here with C#-style notation, custom tags are usable all lowercase too (tags are conventionally all lowercase in Liquid). E.g. these are both valid: {% Display User %} and {% display User %}.
While strings are wrapped in double quotes here single quotes (') work equally.
When passing parameters to tags you can not just pass simple strings but also variable references. E.g. these will both work: {% Display User, Parameter1: ""some value"" %} and  {% Display User, Parameter1: Model.WorkContext.CurrentUser.UserName %}.

Accessing the model
Accessing shape properties:
{{ Model.Id }}
{{ Model.Metadata.Type }}

Accessing the viewmodel from the admin User shape:
{{ Model.CurrentUser.UserName }}
{{ Model.CurrentUser.Email }}

Dynamic expressions on ContentItems work, e.g. you can do this from the Content shape or content part shapes:
{{ Model.ContentItem.TitlePart.Title }}

Accessing array or list elements work as well. E.g. if you add a MediaLibraryPickerField on the Page content type with the name Images you'll be able to access the attached Image items (given that there are at least two) like following:
{{ Model.ContentItem.Page.Images.MediaParts[0].MediaUrl } <- First image.
{{ Model.ContentItem.Page.Images.MediaParts[1].MediaUrl } <- Second image.
{{ Model.ContentItem.Page.Images.Ids[0] }} <-- ID of the first image.
{{ Model.ContentItem.Page.Images.Ids[1] }} <-- ID of the second image.

Accessing the WorkContext
The properties on the WorkContext (and the properties of those objects) are also accessible:
Accessing the currently authenticated user's name: 
{% if Model.WorkContext.CurrentUser != null %}
  {{ Model.WorkContext.CurrentUser.UserName }}
{% else %}
  Please log in!
{% endif %}

Using the HttpContext:
{{ Model.WorkContext.HttpContext.Request.Url.AbsoluteUri }}

Including static resources
Including stylesheets and scripts:
{% Style ""/url/to/stylesheet.css"" %}
You can omit the single quotes or use double quotes instead if you wish.
Note that relative virtual paths (like ""~/Themes/MyTheme/Styles/styles.css"") will work too as usual.

{% Script ""/url/to/script.js"", head %}
The second parameter is the script location: head or foot. The default is foot so you can omit the parameter if you want to include the script in the footer.

You can also reference resources by their names if they are defined in an enabled feature:
{% ScriptRequire ""jQueryUI"", head %}
{% StyleRequire ""jQueryUI_Orchard"" %}

Working with shapes
Displaying shapes from the viewmodel with the Display filter, e.g. from the Content shape:
<article>
{{ Model.Content | Display }}
</article>
Note that there are no quotes around Model.Content!

Displaying any shape with the display tag, here the User shape:
{% Display User %}

You can also give the shape input parameters as from C#:
{% Display User, Parameter1: ""some value"", Parameter2: ""some other value"" %}
These then can be uses from inside the User shape as Model.Parameter1 and Model.Parameter2 respectively.

CSS classes can be added to shapes much like how Model.Classes.Add(""pagination""); works in Razor:
{% AddClassToCurrentShape ""pagination"" %}

Changing global properties of the HTML document
Adds a <link> tag to the head of the document.
{% RegisterLink, Condition: ""gte IE 7"", Href: ""https://en.wikipedia.org/static/favicon/wikipedia.ico"", Rel: ""shortcut icon"", Title: ""favicon"", Type: ""image/x-icon"" %}
The same as the following in Razor: 
RegisterLink(new Orchard.UI.Resources.LinkEntry
	{
		Condition = ""gte IE 7"",
		Href = ""https://en.wikipedia.org/static/favicon/wikipedia.ico"",
		Rel = ""shortcut icon"",
		Title = ""favicon"",
		Type = ""image/x-icon""
	});

Adds a <meta> tag to the head of the document (or modifies an existing one).
{% SetMeta, Charset: ""utf-8"", Content: ""Wordpress"", HttpEquiv: ""X-Generator"", Name: ""generator"" %}
The same as the following in Razor:
SetMeta(new Orchard.UI.Resources.MetaEntry
	{
		Charset = ""utf-8"",
		Content = ""Wordpress"",
		HttpEquiv = ""X-Generator"",
		Name = ""generator""
	});

Sets the title of the current page. Equivalent to using Html.Title(""Title comes here""); in Razor.
{% PageTitle ""Title comes here"" %}

Set and output page classes like Html.ClassForPage(); would do in Razor:
{% ClassForPage, ""custom-class"" %}
Or multiple classes:
{% ClassForPage, ""custom-class1"", ""custom-class2"" %}
Can be also used to simply output the page classes:
{% ClassForPage %}

Sets page classes like Html.AddPageClassNames(); in Razor:
{% AddPageClassNames, ""custom-class"" %}
Or multiple classes:
{% AddPageClassNames, ""custom-class1"", ""custom-class2"" %}

Accessing the antiforgery token
Displays a hidden form field with the antiforgery token. Equivalent to using Html.AntiForgeryTokenOrchard(); in Razor.
{% AntiForgeryTokenOrchard %}
Displays the value of the antiforgery token. Equivalent to using Html.AntiForgeryTokenValueOrchard(); in Razor.
{% AntiForgeryTokenValueOrchard %}

Helpers
Converts an URL to an app-relative one, similar to Href() in Razor.
{% Href ""~/Admin"" %}
If the site root URL is example.com then this will produce ""/Admin"", if there is an app path like example.com/Orchard then this will produce ""/Orchard/Admin"". These would do exactly the same:
{% Href ""/Admin"" %}
{% Href ""Admin"" %}

So by utilizing the standard Liquid capture tag you can build dynamic URLs like following:
{% capture profileUrl %}~/Profile/{{ Model.WorkContext.CurrentUser.UserName }}{% endcapture %}
{% Href profileUrl %}
Or even with multiple parameters:
{% Href ""~/Profile"", Model.WorkContext.CurrentUser.UserName %}
Or observe how we utilize capture, the Href tag and the RegisterLink to register a favicon with a dynamic URL:
{% capture faviconUrl %}{% Href ""~/Themes/MyTheme/Images/favicon.ico"" %}{% endcapture %}
{% RegisterLink, Href: faviconUrl, Rel: ""shortcut icon"", Title: ""favicon"", Type: ""image/x-icon"" %}

Contribution notes
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/orchard-liquid-markup (Mercurial repository)
https://github.com/Lombiq/Orchard-Liquid-Markup (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",5
aclarkxyz/web_molkit,TypeScript,"WebMolKit
Cheminformatics toolkit built with TypeScript & jQuery. Can be used to carry out some fairly sophisticated cheminformatics tasks on a contemporary web browser, such as rendering molecules for display, format conversions, calculations, interactive sketching, among other things.
License
Copyright © 2010-2016 Molecular Materials Informatics, Inc.
http://molmatinf.com
All rights reserved


General availability: The WebMolKit library may be used by anyone under the terms set by the general Gnu Public License (v3.0). The synopsis of this license is that if you use any part of the source code in your own project, you are contractually bound to make the entire project available under a similar license whenever you distribute it, i.e. it's the viral version of the GPL. Not to be confused with the lesser version (LGPL). Also, if you are considering asserting software patents, you'd best read the GPL v3.0 terms very carefully.


Special exception 1: An exception to the default licensing is made for Collaborative Drug Discovery, Inc (CDD). CDD is permitted to use this toolkit under the terms of the Apache License 2.0, which is a non-viral license. This exception applies to any version of the toolkit in which this notice appears, i.e. if this exception is ever removed, then CDD may continue to use the version immediately prior to its removal, and is permitted to fork the code under the terms of the Apache License.


Special exception 2: An exception to the default licensing is made for Collaborations Pharmaceuticals, Inc (CP). CP is permitted to use this toolkit under the terms of the Apache License 2.0, which is a non-viral license. This exception applies to any version of the toolkit in which this notice appears, i.e. if this exception is ever removed, then CP may continue to use the version immediately prior to its removal, and is permitted to fork the code under the terms of the Apache License.


If the copyright owner adds additional exceptions in later releases, they will take effect as of that version.


Owner availability: The copyright owner, Molecular Materials Informatics, Inc., can and will exercise the liberty of using this toolkit in proprietary and/or open source software.


Codebase
Written in TypeScript. Requires the TypeScript compiler (tsc) to cross-compile into JavaScript. Developed using Visual Studio Code.
Status
As of November 2016, the toolkit is openly available on GitHub. Check the VERSION file: until such time as the number reaches 1.0.0, you must consider this project to be a work in progress, i.e. if you wish to use it, you need to keep the source files uptodate in order to collect features are they are completed and bugfixes as they are rolled out, but you also need to watch for API changes that can happen suddenly and without warning.
For documentation purposes, the best place to look is in the val/html folder. The val/html/sketcher.html file is as good a place as any to start: it imports the cross-compiled JavaScript, defines the ancilliary resource location, unpacks a Molecule object, and then plops it into an interactive sketcher widget.
Note that trying out the page by opening the file with a browser probably won't work because of security issues accessing a file from a parallel directory hierarchy, so you will need to fire up a trivial HTTP server (see the run_server file) to test it. If you cut & paste the necessary files into a current web server, and make sure the references are pointing to the correct URLs, then it should work.
The source files are moderately well commented, but proper documentation is a to-do item that will happen when it happens.
A note when perusing the source code: this project actually goes back to ca. 2010, and was originally constructed using Google Closure, which is a JavaScript wrapper that was designed to workaround the fact that the so-called standard web runtime was a complete and utter trainwreck until quite recently. Google's solution was unwieldy and ugly as hell, but that is what had to be done to make it work. The toolkit was originally designed to delegate anything complicated to a server (Java EE) via remote procedure calls, which worked well enough. Fast forward to 2016, the lowest common denominator of modern browsers is finally adequate for high quality applications; the jQuery library has become virtually universal; TypeScript has matured into a very practical solution to the many design flaws of JavaScript; and Visual Studio Code is a remarkably pleasant way to work (and despite the fact that it comes from Microsoft, I only use it on Mac & Linux). For these reasons, the original Closure/Java split has been morphed into what you see now: a self-contained library that runs entirely on the browser. Be warned, though, that the transformation is not entirely complete, and so you will see some evidence of its earlier form (e.g. the src/rpc source files, which you can safely ignore).
If you have any questions, comments or inquiries, feel free to write to aclark@molmatinf.com. Serious people welcome; trolls, spammers and haters not so much.
",7
NCAR/dset-web-accessible-folder-dev,None,"dset-web-accessible-folder-dev
This is the DSET repository for the CKAN web accessible folder used by the DASH Search development application.
",2
obynio/certbot-plugin-gandi,Python,"Certbot plugin for authentication using Gandi LiveDNS
This is a plugin for Certbot that uses the Gandi
LiveDNS API to allow Gandi
customers to prove control of a domain name.
Usage


Obtain a Gandi API token (see Gandi LiveDNS API)


Install the plugin using pip install certbot-plugin-gandi


Create a gandi.ini config file with the following contents and apply chmod 600 gandi.ini on it:
certbot_plugin_gandi:dns_api_key=APIKEY

Replace APIKEY with your Gandi API key and ensure permissions are set
to disallow access to other users.


Run certbot and direct it to use the plugin for authentication and to use
the config file previously created:
certbot certonly -a certbot-plugin-gandi:dns --certbot-plugin-gandi:dns-credentials gandi.ini -d domain.com

Add additional options as required to specify an installation plugin etc.


Please note that this solution is usually not relevant if you're using Gandi's web hosting services as Gandi offers free automated certificates for all simplehosting plans having SSL in the admin interface.
Updates
This plugin can be updated by running:
pip install certbot-plugin-gandi --upgrade

Wildcard certificates
This plugin is particularly useful when you need to obtain a wildcard certificate using dns challenges:
certbot certonly -a certbot-plugin-gandi:dns --certbot-plugin-gandi:dns-credentials gandi.ini -d domain.com -d \*.domain.com --server https://acme-v02.api.letsencrypt.org/directory

Automatic renewal
You can setup automatic renewal using crontab with the following job for weekly renewal attempts:
0 0 * * 0 certbot renew -q -a certbot-plugin-gandi:dns --certbot-plugin-gandi:dns-credentials /etc/letsencrypt/gandi/gandi.ini --server https://acme-v02.api.letsencrypt.org/directory

",17
zhaoolee/ChineseBQB,Python,"Chinese sticker pack / 中国的表情包
表情包目录(共收录1033张表情包)Emoticon package directory (commonly included 1033 emoticon pack)



Example(示例)
链接(Entrance link)





001Funny_滑稽大佬😏BQB(已收录35张)



002CuteGirl_可爱的女孩纸👧BQB(已收录185张)



003CuteBoy_可爱男孩纸👶BQB(已收录23张)



004SmirkBoy_假笑男孩👦BQB(已收录23张)



005ShowerheadBoy_莲蓬头男孩👲BQB(已收录8张)



006Hamster_仓鼠🐹BQB(已收录1张)



007Tiger_胖虎🐯BQB(已收录21张)



008HappyDuck_开心鸭🐥BQB(已收录7张)



009KumamotoBear_熊本熊🐻BQB(已收录31张)



010Cat_猫🐱BQB(已收录38张)



011Dog_狗🐶BQB(已收录40张)



012Parrot_鹦鹉🐦BQB(已收录16张)



013PigPecs_小猪佩奇👑BQB(已收录20张)



014猪_Pig🐖BQB(已收录5张)



015Golden_Curator_Panda金馆长熊猫🐼BQB(已收录175张)



016spray_喷雾🚿BQB(已收录15张)



017Playing_演奏🎻BQB(已收录23张)



018WangEgg_汪蛋🥚BQB(已收录48张)



019Green_Hat绿帽子🖼BQB(已收录23张)



020TATAN🤷‍♂️BQB(已收录31张)



021TongfuInn_同福客栈🏫BQB(已收录101张)



022SuDaqiang_苏大强👴BQB(已收录26张)



023emoji🤠BQB(已收录122张)



024程序员_Programmer💻BQB(已收录6张)



025superHeroes超级英雄们BQB(已收录10张)




Data generation time (数据生成时间): 2019-05-17

BQBEND


The sticker pack is Chinese and the world!
表情包是中国的, 也是世界的!




This is a warehouse dedicated to the collection of stickers, all included stickers can be viewed online!
这是一个专门用于收录表情包的仓库, 所有收录的表情包均可在线查看!




Welcome to the masters of the fight to contribute stickers, if you can not use git to send the sticker to my mailbox zhaoolee@gmail.com
欢迎斗图高手们贡献表情包, 如果不会使用git可以将表情包发送到我的邮箱 zhaoolee@gmail.com




All image index data is automatically generated by the python program, the program is `build.py in the project.
所有图片索引数据由python程序自动生成, 程序为项目中的build.py




Sticker package online view entry page: https://zhaoolee.github.io/ChineseBQB/
表情包在线查看入口页面: https://zhaoolee.github.io/ChineseBQB/


How to get an sticker package? / 如何获取表情包?


Get the section with the Chrome extension GitZip for github (Recommended)
借助Chrome扩展程序GitZip for github 获取部分 (推荐)





GitZip for github 使用方法: 《GitZip for github》从Github批量下载表情包
GitZip for github How to use: [GitZip for github downloads stickers from Github in bulk] (https://zhaoolee.gitbooks.io/chrome/content/040gitzip-for-github.html)




通过git获取全部表情包
Get all stickers via git


git clone --depth 1 https://github.com/zhaoolee/ChineseBQB

Contribution method  / 贡献方法


If you are a Daxie and have a github account, you are welcome to contribute the sticker pack to the root of the https://github.com/zhaoolee/ChineseBQB/000Contribution_贡献🇨🇳


如果你是斗图大佬, 又有github账号, 欢迎将表情包贡献到项目https://github.com/zhaoolee/ChineseBQB根目录下的000Contribution_贡献🇨🇳


If you are a Daxie, don't have a github account, please send the sticker pack to my email address zhaoolee@gmail.com, letter **Contribute stickers **


如果你是斗图大佬, 没有github账号, 欢迎将表情包发到我的邮箱 zhaoolee@gmail.com , 来信注明 贡献表情包


If you feel that the first two methods are too much trouble, just want to make friends with the author, welcome to add me wechat (WeChat): jianzhao111, after the friend passes, you can send me the sticker package directly~ (Crystal: BQB)


如果你感觉前两种方法太麻烦, 只是想和作者交个朋友, 欢迎加我 wechat(微信): jianzhao111 , 好友通过后, 可以直接把表情包发我哦~ (暗号: BQB)





Handing cola to Daxie / 给大佬递可乐
Giving tea to Daxie / 给大佬端茶
Give a cigarette to Daxie / 给大佬点烟











If you are an sticker author, you can also contact me. This warehouse can help you promote stickers and enhance your visibility.
如果你是表情包作者, 也可以联系我, 这个仓库可以帮你推广表情包, 提升你的知名度

Is this something useful? / 这个东西有啥用么?


The sticker pack is the source of happiness!
表情包是快乐之源!



// The following paragraph from zhihu.com
// 下面这段话是我从知乎抄的

Remember, when you pass an sticker
记住，当你传递一张表情包的时候

You are not passing a green stalk
你不是在传递一张发绿的老梗

You passed
你传递的

It is the last torch of the classical Internet spirit!
是古典互联网精神最后的火炬！ 

",97
JDMCreator/LaTeXTableEditor,JavaScript,"LaTeX Table Editor
Version 1.6.3
This is the GitHub page of an Open Source WYSIWYG table editor that exports to multiples languages including LaTeX, ConTeXt, Plain TeX, CSV, HTML, BBCode, Eplain, PreTeXt, Markdown and WML.
Use
You can use it here.
Dependencies
You need Bootstrap 3, ntc.js, SheetJS and JQuery. See the online example.
Formats supported

LaTeX
ConTeXt
Plain TeX
PreTeXt
Markdown
HTML
CSV
BBCode
Textile
Eplain
PreTeXt
WML
Text (coming soon)
MediaWiki (coming soon)

Status
The LaTeX Table Editor is now a stable release. No public API is documented for now.
Report an issue
Please report any issue or bugs via this Github page or contact me at info @ latex-tables [DOT] com.
License
This project is licensed under the terms of the MIT license.
",16
haskell-works/hw-hspec-hedgehog,Shell,"hw-hspec-hedgehog

",5
lpii-2019-1/gerador_de_provas,Java,"Gerador de Provas
Autores
Bruno Geovane(https://github.com/brunogeovane)
Gustavo Faquim(https://github.com/faquimgustavo)
Ricardo Takayuki(https://github.com/ricardotakayuki)
",3
openssh/openssh-portable,C,"Portable OpenSSH
OpenSSH is a complete implementation of the SSH protocol (version 2) for secure remote login, command execution and file transfer. It includes a client ssh and server sshd, file transfer utilities scp and sftp as well as tools for key generation (ssh-keygen), run-time key storage (ssh-agent) and a number of supporting programs.
This is a port of OpenBSD's OpenSSH to most Unix-like operating systems, including Linux, OS X and Cygwin. Portable OpenSSH polyfills OpenBSD APIs that are not available elsewhere, adds sshd sandboxing for more operating systems and includes support for OS-native authentication and auditing (e.g. using PAM).
Documentation
The official documentation for OpenSSH are the man pages for each tool:

ssh(1)
sshd(8)
ssh-keygen(1)
ssh-agent(1)
scp(1)
sftp(1)
ssh-keyscan(8)
sftp-server(8)

Stable Releases
Stable release tarballs are available from a number of download mirrors. We recommend the use of a stable release for most users. Please read the release notes for details of recent changes and potential incompatibilities.
Building Portable OpenSSH
Dependencies
Portable OpenSSH is built using autoconf and make. It requires a working C compiler, standard library and headers, as well as zlib and libcrypto from either LibreSSL or OpenSSL to build. Certain platforms and build-time options may require additional dependencies.
Building a release
Releases include a pre-built copy of the configure script and may be built using:
tar zxvf openssh-X.Y.tar.gz
cd openssh
./configure # [options]
make && make tests

See the Build-time Customisation section below for configure options. If you plan on installing OpenSSH to your system, then you will usually want to specify destination paths.
Building from git
If building from git, you'll need autoconf installed to build the configure script. The following commands will check out and build portable OpenSSH from git:
git clone https://github.com/openssh/openssh-portable # or https://anongit.mindrot.org/openssh.git
cd openssh-portable
autoreconf
./configure
make && make tests

Build-time Customisation
There are many build-time customisation options available. All Autoconf destination path flags (e.g. --prefix) are supported (and are usually required if you want to install OpenSSH).
For a full list of available flags, run configure --help but a few of the more frequently-used ones are described below. Some of these flags will require additional libraries and/or headers be installed.



Flag
Meaning




--with-pam
Enable PAM support. OpenPAM, Linux PAM and Solaris PAM are supported.


--with-libedit
Enable libedit support for sftp.


--with-kerberos5
Enable Kerberos/GSSAPI support. Both Heimdal and MIT Kerberos implementations are supported.


--with-selinux
Enable SELinux support.



Development
Portable OpenSSH development is discussed on the openssh-unix-dev mailing list (archive mirror). Bugs and feature requests are tracked on our Bugzilla.
Reporting bugs
Non-security bugs may be reported to the developers via Bugzilla or via the mailing list above. Security bugs should be reported to openssh@openssh.com.
",696
lefex/LeetCodeGraphically,C++,"
图解 LeetCode 算法，让算法""活""起来。
数据结构和算法是面试非常重要的内容，有些算法非常难以理解，仅靠文字描述很难让读者明白算法思想。如果通过图示的方式来描述算法和数据结构，会形象很多，读者也容易理解算法的真实含义。
这个项目目的是让学习数据结构和算法变得更轻松，我会逐步解析LeetCode上的题目，在学习算法的同时也会讲解设计到的数据结构。
博客地址
超越技术
公众号
图解算法相关的文章每天都会在公众号发布，欢迎关注我的公众号「超越技术」，和我一同学习！！！

图解排序算法
代码位置



图解排序 1/10 - 冒泡排序
冒泡排序是通过比较两个相邻元素的大小实现排序，如果前一个元素大于后一个元素，就交换这两个元素





图解排序 2/10 - 插入排序
选择排序的核心思想是把一个待排序序列，分成 2 部分，前半部分为有序序列，后半部分为无序序列，遍历后半部分数据，插入到前半部分已经排序好的序列，最终得到一个有序序列



图解排序 3/10 - 希尔排序
把一个序列分组，对分组后的内容进行插入排序，这里的分组只是逻辑上的分组，不会重新开辟存储空间



图解排序 4/10 - 快速排序
对待排序序列通过一个「支点」（支点就是序列中的一个元素，别把它想的太高大上）进行拆分，使得左边的数据小于支点，右边的数据大于支点。然后把左边和右边再做一次递归，直到递归结束



图解排序 5/10 - 归并排序
归并排序，采用分治思想，先把待排序序列拆分成一个个子序列，直到子序列只有一个元素，停止拆分，然后对每个子序列进行边排序边合并



图解排序 6/10 - 堆排序
堆排序需要借助于一种数据结构「堆」，注意下文说的都是 「大根堆」。排序的过程中需要不断进行重组堆（heapify 阶段）



图解排序 7/10 - 计数排序
计数排序的核心思想是把一个无序序列 A 转换成另一个有序序列 B，从 B 中逐个“取出”所有元素，取出的元素即为有序序列



图解排序 8/10 - 桶排序
桶排序的核心思想是把数据分到若干个“桶”中，对“桶”中的元素进行排序，最终把“桶”中已排序好的数据合并为一个有序序列。



图解排序 9/10 - 基数排序
基数排序是从待排序序列找出可以作为排序的「关键字」，按照「关键字」进行多次排序，最终得到有序序列。



图解排序 10/10 - 选择排序
选择排序的思想是，依次从「无序列表」中找到一个最小的元素放到「有序列表」的最后面




图解LeetCode链表相关算法
代码位置



图解 LeetCode 链表: 83. Remove Duplicates from Sorted List
给定一个「有序」的链表，去掉重复的节点，每个节点只能出现一次





图解 LeetCode 链表: 82. Remove Duplicates from Sorted List II
移除链表中出现过多次的节点，解这道题的思路也是「水管思路」，对水管进行拆分，重组。链表中可能出现多个重复节点，需要把这些重复的节点全部干掉




更多算法题，关注公众号超越技术，或者前往博客 超越技术
",7
kubernetes-sigs/controller-tools,Go,"

Kubernetes controller-tools Project
The Kubernetes controller-tools Project is a set of go libraries for building Controllers.
Development
Clone this project, and iterate on changes by running ./test.sh.
This project uses Go modules to manage its dependencies, so feel free to work from outside
of your GOPATH. However, if you'd like to continue to work from within your GOPATH, please
export GO111MODULE=on.
Community, discussion, contribution, and support
Learn how to engage with the Kubernetes community on the community page.
controller-tools is a subproject of the kubebuilder project
in sig apimachinery.
You can reach the maintainers of this project at:

Slack channel: #kubebuilder
Google Group: kubebuilder@googlegroups.com

Code of conduct
Participation in the Kubernetes community is governed by the Kubernetes Code of Conduct.
",55
hdl4fpga/hdl4fpga,VHDL,"The code that is on the reposiroty is not working now. You can find the latest stable release following the next link https://github.com/hdl4fpga/hdl4fpga/releases/tag/arty

scopeio : an Embedded Measurement System

What is it ?
Simulation tools can't help you and you cannot guess what is happening inside
of the FPGA anymore. You need to see it now. You need an Embedded Measurement
System in the FPGA that lets you see as much data as you need using as few
resources as possible.

How it was born
I found myself in a situation in which I had to debug a high-performance open
source portable DDR Core. I needed to capture a lot of data to see as many
events as possible. There weren¿t just a few signals, but more than sixteen to
understand what was happening. That is how ScopeIO was born.
Its goals were:

Small footprint to embed it.
Portability.
VGA to display data.
Block RAM requirement only.
UDP to communicate with the computer.


Small footprint to embed it
ADSL, DVB-T,  ISDB-T, LTE, WiFi and others are OFDM signals. In those cases, a
self-correlation function should be calculated to know when packets start or
end. That is not difficult to do. However, if the measurement instrument lacks
a self-correlation function it is not possible to trigger the signal at its
boundaries. One can see that it is important that ScopeIO be as small as
possible to embed it in the design to be debugged.
The table below shows the resources used by both implementations that are on
the youtube channel.


Kit
Channels
Samples per Channel
Video Resolution
FPGA Slices
Block Rams
Project



Artix-7 35T Arty
9
6400
1920x1080
1791(8.5%)
23.5(47%)
arty/vivado/scopeio/scopeio.xpr

Spartan 3E Starter
2
960
800x600
710(2%)
17(20%)
s3estarter/ise/scopeio/scopeio.xise




VGA to display data
VGA is a well known video standard that is easy to implement, and it is pretty
much available on every monitor. Many monitors that have a DVI port are easy to
connect to a VGA port too through a mechanical adapter. A VGA port needs a
minimum of GND, VSYNC, HSYNC and RGB pins. Four wires are more than enough if
you don¿t mind a monochrome image, or six to have eight colors. But if you have
video dacs in the development kit, ScopeIO can display as many colors as there
are available.

Block RAM requirement only
FPGAs have embedded memory blocks that are fast and easy to use but they are
small compared to dynamic ram. As much as possible, the memory is used to
capture the signal data to display. The memory is not used for video.

UDP to send data to the computer.
Commands to control ScopeIO are sent by UDP/IP. The commands are detected every
time a packet with the corresponding MAC address : ¿00:00:00:01:02:03¿ - not
much imagination there -  is received. A configuration in the PC is required as
described in the video. A simple JavaScript Application controls everything
else. Node,js and Nw.js are required to run it.

Portability
Portability is also one of the main goals. While videos on the channels are for
Xilinx's FPGA kits: Artix-7 35T Arty FPGA Evaluation Kit and Spartan 3E Starter
kit board, ScopeIO is easy to port to other manufactures' devices and kit
boards. You can find a porting to Latticesemi ECP3Versa at
ecp3versa/diamond/ecp3versa.ldf, kit to which I have access.
",20
k1LoW/awspec,Ruby,"awspec  

RSpec tests for your AWS resources.

Resource Types | Contributing
Installation
Add this line to your application's Gemfile:
gem 'awspec'
And then execute:
$ bundle

Or install it yourself as:
$ gem install awspec

Getting Started
STEP 1. Generate awspec init files
If you're starting on a fresh RSpec project, you can use awspec to generate your init files:
$ awspec init

If you're working on an exisitng RSpec project, you will need to add the following lines to your spec_helper.rb file:
require 'awspec'
Awsecrets.load(secrets_path: File.expand_path('./secrets.yml', File.dirname(__FILE__)))
STEP 2. Set AWS config
2-1. Use Shared Credentials like AWS CLI
$ aws configure

...
(See http://docs.aws.amazon.com/ja_jp/cli/latest/userguide/cli-chap-getting-started.html#config-settings-and-precedence)
2-2. Use secrets.yml
$ cat <<EOF > spec/secrets.yml
region: ap-northeast-1
aws_access_key_id: XXXXXXXXXXXXXXXXXXXX
aws_secret_access_key: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
EOF
STEP 3. Write spec/*_spec.rb
require 'spec_helper'

describe ec2('my-ec2-tag-name') do
  it { should be_running }
  its(:instance_id) { should eq 'i-ec12345a' }
  its(:image_id) { should eq 'ami-abc12def' }
  its(:public_ip_address) { should eq '123.0.456.789' }
  it { should have_security_group('my-security-group-name') }
  it { should belong_to_vpc('my-vpc') }
  it { should belong_to_subnet('subnet-1234a567') }
  it { should have_eip('123.0.456.789') }
  it { should be_disabled_api_termination }
end
Using shared_context: region
require 'spec_helper'

describe sqs('my-sqs-queue'), region: 'us-west-2' do
  it { should exist }
  its(:queue_url) { should eq 'https://sqs.us-west-2.amazonaws.com/xxxxxxxxxxxx/my-sqs-queue' }
  its(:queue_arn) { should eq 'arn:aws:sqs:us-west-2:xxxxxxxxxxxx:my-sqs-queue' }
  its(:visibility_timeout) { should eq '30' }
  its(:maximum_message_size) { should eq '256000' }
  its(:message_retention_period) { should eq '86400' }
  its(:delay_seconds) { should eq '0' }
  its(:receive_message_wait_time_seconds) { should eq '10' }
end
STEP 4. Run tests
Add gem ""rake"" in your Gemfile if you are starting a blank project.
$ bundle exec rake spec

Advanced Tips: Spec generate command
Generate spec from AWS resources already exists.
$ awspec generate ec2 vpc-ab123cde >> spec/ec2_spec.rb
Make sure you have added in your spec file
require 'spec_helper'
Advanced Tips: Use Shared Credentials (~/.aws/config ~/.aws/credentials)
$ awspec generate ec2 vpc-ab123cde --profile mycreds
$ AWS_PROFILE=mycreds bundle exec rake spec
Support AWS Resources
Resource Types information here
awspec AWS key/secrets precedence
Dependent on awsecrets.
References
awspec is inspired by Serverspec.

Original idea (code / architecture) -> Serverspec
AWS + Serverspec original concept -> https://github.com/marcy-terui/awspec
Serverspec book

",838
Jmeyer1292/opw_kinematics,C++,"Intro
A simple, analytical inverse kinematic library for industrial robots with parallel bases and
spherical wrists. Based on the paper An Analytical Solution of the Inverse Kinematics Problem of Industrial Serial Manipulators with an Ortho-parallel Basis and a Spherical Wrist by
Mathias Brandstötter, Arthur Angerer, and Michael Hofbaur.
Purpose
This package is meant to provide a simpler alternative to IK-Fast based solutions in situations
where one has an industrial robot with a parallel base and spherical wrist. This configuration
is extremely common in industrial robots.
The kinematics are parameterized by 7 primary values taken directly from the robot's spec sheet
and a set of joint-zero offsets. Given this structure, no other setup is required.
Parameters
This library makes use of 7 kinematic parameters (a1, a2, b, c1, c2, c3, and c4) defined in the paper An Analytical Solution of the Inverse Kinematics Problem of Industrial Serial Manipulators with an Ortho-parallel Basis and a Spherical Wrist. See the paper for details.
This paper assumes that the arm is at zero when all joints are sticking straight up in the air as seen in the image below. It also assumes that all rotations are positive about the base axis of the robot.

To use the library, fill out an opw_kinematics::Parameters<T> data structure with the appropriate values for the 7 kinematic parameters and any joint offsets required to bring the paper's zero position (arm up in Z) to the manufacturers position. Additionally, there are 6 ""sign correction"" parameters (-1 or 1) that should be specified if your robot's axes do not match the convention in the paper.
For example, the ABB IRB2400 has the following values:
  Parameters<T> p;
  p.a1 = T(0.100);
  p.a2 = T(-0.135);
  p.b =  T(0.000);
  p.c1 = T(0.615);
  p.c2 = T(0.705);
  p.c3 = T(0.755);
  p.c4 = T(0.085);

  p.offsets[2] = -M_PI / 2.0;

  p.sign_corrections[0] = 1; // Already 1 by default; just an example
Note that the offset of the third joint is -90 degrees, bringing the joint from the upright position to parallel with the ground at ""zero"".
You can find other examples (many un-tested) taken from the source paper in include/opw_kinematics/opw_parameters_examples.h.
Example
#include ""opw_kinematics/opw_kinematics.h""
#include ""opw_kinematics/opw_parameters_examples.h"" // for makeIrb2400_10<double>()
#include ""opw_kinematics/opw_utilities.h"" // for optional checking
#include <array>

int main()
{
  const auto abb2400 = opw_kinematics::makeIrb2400_10<double>();

  // Inverse kinematics
  auto pose = opw_kinematics::Transform<double>::Identity();
  pose.translation() = Eigen::Vector3d(1.3, 0.2, 0);

  // Up to 8 solutions exist
  // NaN indicates a solution did not exist
  std::array<double, 6 * 8> sols; // You could also use a std::vector or c-array of the appropriate size (6*8)
  opw_kinematics::inverse(abb2400, pose, sols.data());

  // Forward kinematics
  Eigen::Affine3d forward_pose = opw_kinematics::forward(abb2400, &sols[6 * 0]);

  // Optionally, check for validity (this just makes sure there are no Nans in a solution)
  bool second_sol_is_valid = opw_kinematics::isValid(&sols[6 * 1]);

  // Optionally, harmonize the result toward zero in place
  // So if a joint is greater than PI or less than -PI, we add -2PI or +2PI respectively to move the joint solution closer to zero.
  opw_kinematics::harmonizeTowardZero(&sols[6 * 2]); // Harmonizes the third solution.

  return 0;
}

Notes
The library returns the 8 kinematically unique solutions for a given pose. Note that:

These solutions ARE NOT the ONLY solutions. For each joint that can rotate more than 2 * Pi, there exists redundant solutions. For example, if joint 6 can rotate -2Pi to 2Pi then a solution with joint 6 at 4.0 radians also has a solution with joint 6 at -2.28 radians and all other values the same.
This library has no concept of LIMITS! Check your own limits. Be sure to check the redundant solutions to see if they are in limits. Consider calling opw_kinematics::harmonizeTowardZero(T* qs) in opw_kinematics/opw_utilities.h to help check these.

",16
Azure/azure-rest-api-specs,TypeScript,"
Azure REST API Specifications
Description
This repository is the canonical source for REST API specifications for Microsoft Azure.
Basics
If you're a spec author looking for information about all of the repositories and steps in the pipeline, go to the adx-documentation-pr repository. Make sure to join the Github Azure organization to get access to that repo.
Latest improvement: Microsoft employees can try out our new experience at OpenAPI Hub - online experience for using our validation tools and finding your workflow.
Please check the announcements page for any new updates since your last visit.
Getting started

Our Contribution guidelines walks you through the process of contributing to this repository.
The /documentation folder contains reference documentation for all aspects of Swagger and our recommended patterns. Start with the Creating Swagger page.

Directory Structure
The structure of the directory should strictly follow these rules:


Profile: The profile holder contains the profiles' definition MD files. these files will contain information and references to the snapshots of the RPs' Resource types or Dataplane API versions that represent a specific profile.


Specification: This folder the is root folder for all Specs (Management and Dataplane) related docs.


{RP-Name} Folders - each RP will have a separate folder


'resource-manager' and 'data-plane' Folders: the RPs can put specs in one of two categories: resource-manager (for ARM resources) and data-plane (for everything else) . The autorest configuration file (readme.md) for the RP should be inside this folder


'preview' and 'stable' Folders: Varying levels of stability exist in our repository. Each API Version folder should be categorized as either still accepting breaking changes, or no longer accepting breaking changes. This is not a direct analog for whether or not an API Version has the ""-preview"" suffix or not. SDKs that are generated from 'preview' folder items should indicate to their customers in the most idiomatic way that breaking changes may still be coming.


API versions: this folder will be the direct child of the category folder. there will be one such folder per resource type or dataplane service version. This folder will contain the OpenAPI validation Specs (Swaggers previously) and the examples folder.


Examples: the example folder will contain the x-ms-examples files. it will reside under the APIs or Resources' version folders as different APIs or Resource types version can have different examples.


Notes:

folder names should be singular (ie, 'profile' not 'profiles' ) -- this removes ambiguity for some non-english speakers.
generic folder names should be lower-case
proper-name/product name/namespace folders can be PascalCased (ie, ""KeyVault"")
files are whatever case you think is good for your soul.



The structure should appear like so:
.
\---specification
|    +---automation
|    |   \---resource-manager
|    |       \---Microsoft.Automation
|    |           \---stable
|    |               \---2015-10-31
|    |                   \---examples
|    +---batch
|    |   +---data-plane
|    |   |   \---Microsoft.Batch
|    |   |       +---stable
|    |   |       |   +---2015-12-01.2.2
|    |   |       |   +---2016-02-01.3.0
|    |   |       |   +---2016-07-01.3.1
|    |   |       |   +---2017-01-01.4.0
|    |   |       |       \---examples
|    |   |       \---preview
|    |   |           \---2017-05-01.5.0
|    |   \---resource-manager
|    |       \---Microsoft.Batch
|    |           +---stable
|    |           |   +---2015-12-01
|    |           |   +---2017-01-01
|    |           |       \---examples
|    |           \---2017-05-01
|    |               \---examples
|    +---billing
|        \---resource-manager
|            \---Microsoft.Billing
|                \---stable
|                |   +---2017-02-27-preview
|                |       \---examples
|                +---preview
|                    \---2017-04-24-preview
|                        \---examples
\--- readme.md
Currently, the specifications are expected to be in Swagger JSON format
Next steps
The next step in the process after a spec is completed is to generate SDKs and API reference documentation. Go to the Azure Developer Experience guide for more information.

This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.
",457
couchbase/java-dcp-client,Java,"Couchbase Java DCP Client
This repository contains a purely java-based implementation for a Couchbase
DCP (Database Change Protocol) client. It is currently a work in progress.
It replaces the previous experimental support inside Couchbase core-io.
It supports:

 Low Overhead Streaming
 Async from top to bottom
 Stream specific vbuckets
 Manual Start/Stop of streams
 Noop Acknowledgements and Dead Connection Detection
 Flow Control
 Session Management for restartability
 Rebalance Support
 Export current session state for durability (ships with JSON)
 Start and Restart from specific session state (import durable state)
 Pausing and Restarts
 Stream up to a specific point in time for a vbucket and then stop
 Proper shutdown/disconnect and cleanup

Installation
We publish the releases (including pre-releases to maven central):
<dependency>
    <groupId>com.couchbase.client</groupId>
    <artifactId>dcp-client</artifactId>
    <version>0.22.0</version>
</dependency>
If you want the bleeding edge, you can check
out the project from github and build it on your own. It is a maven-based
project so simply run
$ git clone https://github.com/couchbase/java-dcp-client.git
$ cd java-dcp-client
$ mvn install

This local build will install the com.couchbase.client:dcp-client artifact
with the next SNAPSHOT version. You can then depend on it in your
project.
Basic Usage
The simplest way is to initiate a stream against localhost and open
all streams available. You always need to attach a callback for both the
config and the data events - in the simplest case all the messages are
just discarded. It's important to release the buffers!
Please check out the examples!
The following example connects to the travel-sample bucket and prints
out all subsequent mutations and deletions that occur.
// Connect to localhost and use the travel-sample bucket
final Client client = Client.configure()
    .hostnames(""localhost"")
    .bucket(""travel-sample"")
    .build();

// Don't do anything with control events in this example
client.controlEventHandler(new ControlEventHandler() {
    @Override
    public void onEvent(ChannelFlowController flowController, ByteBuf event) {
        event.release();
    }
});

// Print out Mutations and Deletions
client.dataEventHandler(new DataEventHandler() {
    @Override
    public void onEvent(ChannelFlowController flowController, ByteBuf event) {
        if (DcpMutationMessage.is(event)) {
            System.out.println(""Mutation: "" + DcpMutationMessage.toString(event));
            // You can print the content via DcpMutationMessage.content(event).toString(CharsetUtil.UTF_8);
        } else if (DcpDeletionMessage.is(event)) {
            System.out.println(""Deletion: "" + DcpDeletionMessage.toString(event));
        }
        event.release();
    }
});

// Connect the sockets
client.connect().await();

// Initialize the state (start now, never stop)
client.initializeState(StreamFrom.NOW, StreamTo.INFINITY).await();

// Start streaming on all partitions
client.startStreaming().await();

// Sleep for some time to print the mutations
// The printing happens on the IO threads!
Thread.sleep(TimeUnit.MINUTES.toMillis(10));

// Once the time is over, shutdown.
client.disconnect().await();
Dealing with Messages and ByteBufs
To save allocations the actual data you are interacting with are raw
netty ByteBufs that may be pooled, depending on the configuration. So
it is always important to release() them when not needed anymore.
Since working with the raw buffers is not fun, the client provides
flyweights that allow you to extract the information out of the buffers
easily. Consult the docs for information on which message types to expect
when, but as an example if you want to print the key and content of an
incoming mutation in the data handler you can do it like this:
if (DcpMutationMessage.is(event)) {
    String key = DcpMutationMessage.key(event).toString(CharsetUtil.UTF_8);
    String content = DcpMutationMessage.content(event).toString(CharsetUtil.UTF_8);
    System.out.println(""Found Key "" + key + "" with Content "" + content);
}
Advanced Usage
Flow Control
To handle slow clients better and to make it possible that the client signals
backpressure to the server (that it should stop sending new data when the
client is busy processing the previous ones) flow control tuneables are
available.
Handling flow control consist of two stages: first, you need to enable
it during bootstrap and then acknowledge specific message types as soon
as you are done processing them.
Configuring Flow Control
To activate flow control, the DcpControl.Names.CONNECTION_BUFFER_SIZE
control param needs to be set to a value greater than zero. A reasonable
start value to test would be ""10240"" (10K).
Next, you also need to set the bufferAckWatermark to a value which is
equal or smaller than the connection buffer size. Every time a message
is acknowledged the client accumulates up to the watermark and only if
the watermark is exceeded the acknowledgement is sent. This helps with
cutting down on network traffic and to reduce the workload on the server
side for accounting.
Acknowledging Messages
If you do not acknowledge the bytes read for specific messages, the server
will stop streaming new messages when the CONNECTION_BUFFER_SIZE is
reached.
The following messages need to be acknowledged by the user:

DcpSnapshotMarkerRequest (on the ControlEventHandler)
DcpMutationMessage (on the DataEventHandler)
DcpDeletionMessage (on the DataEventHandler)
DcpExpirationMessage (on the DataEventHandler)

Acknowledging works by calling the ChannelFlowController#ack method.
A simple way to do this is the following:
flowController.ack(event);
This method extracts the number of readable bytes out of it.
When you already did consume the bytes and the reader index
of the buffer is not the number of bytes orginally, you can fall back to
the lower level API:
flowController.ack(numBytes);
SSL (Couchbase Enterprise feature)
Read in details about SSL in Couchbase on
our documentation.
Here we will just post quick start steps:


Download and store in file cluster certificate from ""Security"" -> ""Root Certificate"" section on Admin Console.


Import this certificate using keytool:
 keytool -importcert -keystore /tmp/keystore \
                     -storepass secret \
                     -file /tmp/cluster.cert



And update configuration of the DCP client:
final Client client = Client.configure()
        .hostnames(""localhost"")
        .bucket(""travel-sample"")
        .sslEnabled(true)
        .sslKeystoreFile(""/tmp/keystore"")
        .sslKeystorePassword(""secret"")
        .build();


System Events
Since the 0.7.0 release, the client implements a notification service, which allows you to react on events, which are
not tied directly to protocol and data transmission.  For example, connection errors, or notifications about stream
completion when the end sequence number wasn't set to infinity. The following example subscribes a handler to system
events to find out when partition 42 is done with data transmission:
client.systemEventHandler(new SystemEventHandler() {
    @Override
    public void onEvent(CouchbaseEvent event) {
        if (event instanceof StreamEndEvent) {
            StreamEndEvent streamEnd = (StreamEndEvent) event;
            if (streamEnd.partition() == 42) {
                System.out.println(""Stream for partition 42 has ended (reason: "" + streamEnd.reason() + "")"");
            }
        }
    }
});
",26
SciSharp/TensorFlow.NET,C#,"TensorFlow.NET
TensorFlow.NET (TF.NET) provides a .NET Standard binding for TensorFlow. It aims to implement the complete Tensorflow API in CSharp which allows .NET developers to develop, train and deploy Machine Learning models with the cross-platform .NET Standard framework.






TF.NET is a member project of SciSharp STACK.

Why TensorFlow.NET ?
SciSharp STASK's mission is to bring popular data science technology into the .NET world and to provide .NET developers with a powerful Machine Learning tool set without reinventing the wheel. Scince the APIs are kept as similar as possible you can immediately adapt any existing Tensorflow code in C# with a zero learning curve. Take a look at a comparison picture and see how comfortably a   Tensorflow/Python script translates into a C# program with TensorFlow.NET.

SciSharp's philosophy allows a large number of machine learning code written in Python to be quickly migrated to .NET, enabling .NET developers to use cutting edge machine learning models and access a vast number of Tensorflow resources which would not be possible without this project.
In comparison to other projects, like for instance TensorFlowSharp which only provide Tensorflow's low-level C++ API and can only run models that were built using Python, Tensorflow.NET also implements Tensorflow's high level API where all the magic happens. This computation graph building layer is still under active development. Once it is completely implemented you can build new Machine Learning models in C#.
How to use
Install TF.NET through NuGet.
PM> Install-Package TensorFlow.NET
If you are using Linux or Mac OS, please download the pre-compiled dll here and place it in the working folder. This is only need for Linux and Mac OS, and already packed into NuGet for Windows.
Import TF.NET.
using Tensorflow;
Add two constants:
// Create a Constant op
var a = tf.constant(4.0f);
var b = tf.constant(5.0f);
var c = tf.add(a, b);

using (var sess = tf.Session())
{
    var o = sess.run(c);
}
Feed placeholder:
// Create a placeholder op
var a = tf.placeholder(tf.float32);
var b = tf.placeholder(tf.float32);
var c = tf.add(a, b);

using(var sess = tf.Session())
{
    var o = sess.run(c, new FeedItem(a, 3.0f), new FeedItem(b, 2.0f));
}
Linear Regression:
// We can set a fixed init value in order to debug
var W = tf.Variable(-0.06f, name: ""weight"");
var b = tf.Variable(-0.73f, name: ""bias"");

// Construct a linear model
var pred = tf.add(tf.multiply(X, W), b);

// Mean squared error
var cost = tf.reduce_sum(tf.pow(pred - Y, 2.0f)) / (2.0f * n_samples);

// Gradient descent
// Note, minimize() knows to modify W and b because Variable objects are trainable=True by default
var optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost);

// Initialize the variables (i.e. assign their default value)
var init = tf.global_variables_initializer();

// Start training
with(tf.Session(), sess => 
{
    // Run the initializer
    sess.run(init);
    
    // Fit all training data
    for (int epoch = 0; epoch < training_epochs; epoch++)
    {
        foreach (var (x, y) in zip<float>(train_X, train_Y))
            sess.run(optimizer, new FeedItem(X, x), new FeedItem(Y, y));
        
        // Display logs per epoch step
        if ((epoch + 1) % display_step == 0)
        {
            var c = sess.run(cost, new FeedItem(X, train_X), new FeedItem(Y, train_Y));
            Console.WriteLine($""Epoch: {epoch + 1} cost={c} "" + $""W={sess.run(W)} b={sess.run(b)}"");
        }
        
        Console.WriteLine(""Optimization Finished!"");
        var training_cost = sess.run(cost, new FeedItem(X, train_X), new FeedItem(Y, train_Y));
        Console.WriteLine($""Training cost={training_cost} W={sess.run(W)} b={sess.run(b)}"");
        
        // Testing example
        var test_X = np.array(6.83f, 4.668f, 8.9f, 7.91f, 5.7f, 8.7f, 3.1f, 2.1f);
        var test_Y = np.array(1.84f, 2.273f, 3.2f, 2.831f, 2.92f, 3.24f, 1.35f, 1.03f);
        Console.WriteLine(""Testing... (Mean square loss Comparison)"");
        
        var testing_cost = sess.run(tf.reduce_sum(tf.pow(pred - Y, 2.0f)) / (2.0f * test_X.shape[0]), new FeedItem(X, test_X), new FeedItem(Y, test_Y));
        Console.WriteLine($""Testing cost={testing_cost}"");
        
        var diff = Math.Abs((float)training_cost - (float)testing_cost);
        Console.WriteLine($""Absolute mean square loss difference: {diff}"");
    }
});
Read the docs & book The Definitive Guide to Tensorflow.NET.
More examples:


Hello World


Basic Operations


Linear Regression


Logistic Regression


Nearest Neighbor


Naive Bayes Classification


Image Recognition


K-means Clustering


NN XOR


Object Detection


Text Classification


CNN Text Classification


Named Entity Recognition


Contribute:
Feel like contributing to one of the hottest projects in the Machine Learning field? Want to know how Tensorflow magically creates the computational graph? We appreciate every contribution however small. There are tasks for novices to experts alike, if everyone tackles only a small task the sum of contributions will be huge.
You can:

Let everyone know about this project (trivial)
Port Tensorflow unit tests from Python to C# (easy)
Port missing Tensorflow code from Python to C# (easy)
Port Tensorflow examples to C# and raise issues if you come accross missing parts of the API (easy)
Debug one of the unit tests that is marked as Ignored to get it to work (can be challenging)
Debug one of the not yet working examples and get it to work (hard)

How to debug unit tests:
The best way to find out why a unit test is failing is to single step it in C# and its pendant Python at the same time to see where the flow of execution digresses or where variables exhibit different values. Good Python IDEs like PyCharm let you single step into the tensorflow library code.
Git Knowhow for Contributors
Add SciSharp/TensorFlow.NET as upstream to your local repo ...
git remote add upstream git@github.com:SciSharp/TensorFlow.NET.git

Please make sure you keep your fork up to date by regularly pulling from upstream.
git pull upstream master

Contact
Feel free to star or raise issue on Github.
Join our chat on Gitter or
Scan QR code to join Tencent TIM group:

",365
yzyly1992/Website_Aodabo,HTML,"Awesome David Blog (Aodabo) 凹大卜
A Personal Webapp written by David Yang. Referred from Micheal Liao's tutorial.
The website is mainly built by Python3.
Components:
Database: MySQL;
Services: nginx, supervisor, python3, mysql-server;
Libraries in python3: aiomysql, aiohttp, jinja2;
Front-end framework: UIkit;
JavaScript framework: Vue.js;
Writing format syntax: MarkDown;
Features:

Post Blog with title, summary, content, tag
Comment System (Valine.js)
Catagorize blogs through tags
In-site Search
Show multi-media through MarkDown
Edit and manage Blogs
Manage Users and Comments
Read Count System (Bosuanzi.js)
Mobile Adjustable Layout

Features will add:

Share system
Pay system
Safety System
Author Resume

",4
yzyly1992/Website_Aodabo,HTML,"Awesome David Blog (Aodabo) 凹大卜
A Personal Webapp written by David Yang. Referred from Micheal Liao's tutorial.
The website is mainly built by Python3.
Components:
Database: MySQL;
Services: nginx, supervisor, python3, mysql-server;
Libraries in python3: aiomysql, aiohttp, jinja2;
Front-end framework: UIkit;
JavaScript framework: Vue.js;
Writing format syntax: MarkDown;
Features:

Post Blog with title, summary, content, tag
Comment System (Valine.js)
Catagorize blogs through tags
In-site Search
Show multi-media through MarkDown
Edit and manage Blogs
Manage Users and Comments
Read Count System (Bosuanzi.js)
Mobile Adjustable Layout

Features will add:

Share system
Pay system
Safety System
Author Resume

",4
formalabstracts/formalabstracts,Lean,"Formal Abstracts
This repository contains the current state of the Formal Abstracts Project. We are currently in the design phase, and figuring out how to structure the project. We are not actively seeking contributions at this time.
Contributors, please use git pull -r to pull and use git rebase instead of git merge.
",52
rust-lang-nursery/futures-rs,Rust,"



  Zero-cost asynchronous programming in Rust











    Documentation
   | 
    Website
  

Usage
Add this to your Cargo.toml:
[dependencies]
futures-preview = ""0.3.0-alpha.16""
Now, you can use futures-rs:
use futures::future::Future; // Note: It's not `futures_preview`
The current version of futures-rs requires Rust nightly 2019-05-09 or later.
Feature std
Futures-rs works without the standard library, such as in bare metal environments.
However, it has a significantly reduced API surface. To use futures-rs in
a #[no_std] environment, use:
[dependencies]
futures-preview = { version = ""=0.3.0-alpha.16"", default-features = false }
License
This project is licensed under either of

Apache License, Version 2.0, (LICENSE-APACHE or
http://www.apache.org/licenses/LICENSE-2.0)
MIT license (LICENSE-MIT or
http://opensource.org/licenses/MIT)

at your option.
Contribution
Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in futures-rs by you, as defined in the Apache-2.0 license, shall be
dual licensed as above, without any additional terms or conditions.
",2608
PolymerLabs/arcs,TypeScript,"

arcs
A hosted version of Arcs is available in both tagged and bleeding edge forms.
Neither is stable -- the runtime, database and front-end are all iterating rapidly.
TypeDoc
generated documentation is available for Arcs Runtime.
Tagged Releases
Tagged release URLs have the form
https://cdn.rawgit.com/PolymerLabs/arcs-live/<release_number>/shells/web-shell
(the list of releases is
here). A tagged release (with an older
path due to a previous version of shell code) is
v0.4.1.
Bleeding edge often works and is available on:
https://live.arcs.dev/
Install
Arcs is developed with a recent version of Node (v10.0.0 at the time of this
writing), in particular as we use new ES6 features. You can check our Travis
config to see what
version is used for automated build status. More recent versions should work,
but if for example you see test errors on a version that's a full release later
(ex. v11+) you may want to try rolling back to an earlier version. We welcome
patches that will allow more recent versions to operate, ideally without
requiring an upgrade to our current version.
Installing from scratch


Install nvm.
As per the installation instructions,
download and run the installation script directly in your terminal (yes, you
read that correctly):
$ curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.34.0/install.sh | bash

If you're using zsh you may need to source ~/.nvm/nvm.sh after this.


Install node.
$ nvm install 10



If you need to update npm to a later version (our build checks for the
minimum required version):
$ npm install -g npm   # can use npm@6.3.0 to install a specific version



Installing within the Arcs project:
$ npm install
$ ./tools/sigh

npm install is required on a fresh checkout. After that it only needs to be
re-run infrequently as new dependencies are included, and usually a build
failure will be the signal for that.
Git Setup
You may also find it helpful to have sigh lint and type check your work locally
before you commit it. To do this, you can setup git to run presubmit checks
using:
$ git config core.hooksPath tools/hooks

Windows Installation Notes

Git for Windows is one of many Git options.
Consider using nvm-windows
to allow more easily switching between Node versions.
As part of npm install you'll need to build fibers which uses node-gyp
which in turn requires windows-build-tools. Follow the node-gyp Windows
build instructions. If option
1 hangs or otherwise hits issues, you can try option 2. Note the
Microsoft Build Tools 2015 can be
downloaded separately from Visual Studio (and the links in the node-gyp
documentation are stale), but you'll still need to do the
npm config set msvs_version 2015 bit, and similar for Python if you
install that manually per node-gyp option 2 instructions.

Starting Arcs
After the full build (npm install && tools/sigh) run: (note that npm start will block, so you'll have to run the second command in a new shell):
$ npm start

Then open http://localhost:8786/shells/web-shell in a web browser
(or, on MacOS, use open 'http://localhost:8786/shells/web-shell).
Subprojects
Subcomponents have more detailed descriptions. Particularly the extensions
also have individual installation steps.
Chrome Extension
See extension.
Chrome Developer Tools Extension
See devtools.
Testing
The simplest way to run tests is to let the targets do all the work. These
commands will install all packages, run a build, start a background server,
run all the tests, and kill the background server:
$ npm install
$ npm run test-with-start

There are additional targets provided to run subsets of those commands.

npm start: spins up a server (and blocks), serving on port 8786.
./tools/sigh: run a subset of tests and build packed artifacts.
npm test: run all tests (using currently built artifacts) against an
already-running server (assumed to be port 8786).
npm run test-no-web: run all non-web tests.

To run a specific Selenium test using Mocha's 'grep' capability:

In one terminal: npm start
In another: npm run test-wdio-shells -- --mochaOpts.grep 'regex'

This also works for unit tests: ./tools/sigh test --grep 'regex'. In addition,
for unit tests you can run only a single test case by using it.only() instead
of it(), or a single suite using describe.only() instead of describe().
Debugging tests
If you see errors like
ERROR: connect ECONNREFUSED 127.0.0.1:9515
chrome
    at new RuntimeError (...\node_modules\webdriverio\build\lib\utils\ErrorHandler.js:144:12)
    at Request._callback (...\node_modules\webdriverio\build\lib\utils\RequestHandler.js:327:43)

It may indicate that chromedriver hasn't been installed completely. Run the install script:
node node_modules\chromedriver\install.js

Debugging unit tests in Chrome
You can attach the Chrome debugger to debug your unit tests using the
--inspect flag:
./tools/sigh test --inspect

It will wait for you to attach your debugger before running the tests. Open
[chrome://inspect] and look for the ""inspect"" button under the ""Remote Target""
heading. You can use Ctrl-P to open files (you may need to add the build
folder to your workspace first). Hit ""resume"" to start running the unit tests.
Debugging Selenium Failures
Selenium failures are often easy to cause due to seemingly unrelated changes,
and difficult to diagnose.
There are 2 main avenues to debug them in this system. The first is to have
the browser run in a graphical manner (as opposed to the default headless
configuration). The second is to actually debug the running selenium instance.
There are some debugging hints (code and configuration you can uncomment to
make debugging easier) in test/specs/starter-test.js and test/wdio.conf.js
marked with the phrase debug hint.
To activate a sane set of helpful debugging flags, there's a wdio-debug
command line argument that you can pass in. This will run Chrome in a
non-headless fashion, and will increase timeouts.
Through npm: npm run test-wdio-shells --wdio-debug=true (or npm test --wdio-debug=true).  Directly through wdio: node_modules/.bin/wdio --wdio-debug=true shell/test/wdio.conf.js.
Webdriver takes screenshots of failures, which are saved to the
./shells/test/errorShots/ directory. When running on Travis, the screenshots
are uploaded to the Arcs Webdriver Screenshots team drive.
Graphical (non-headless)
It may be easiest to see the problem in a browser window to diagnose it. Edit
wdio.conf.js in the branch with failures, comment out the '--headless'
option and increase the mocha timeout. In combination, these two changes will
allow you to see what's happening on the screen, and will give you enough time
to debug the situation.
arcs/shells> vi test/wdio.conf.js
arcs/shells> git diff test/wdio.conf.js
diff --git a/test/wdio.conf.js b/test/wdio.conf.js
index 0e36452..8ecf3d6 100644
--- a/test/wdio.conf.js
+++ b/test/wdio.conf.js
@@ -50,7 +50,7 @@ exports.config = {
       chromeOptions: {
         args: [
           // arcs note: comment this out to see the system running
-          '--headless'
+          // '--headless'
         ]
       }
     }
@@ -139,7 +139,7 @@ exports.config = {
   mochaOpts: {
     ui: 'bdd',
     // arcs note: increase this timeout for debugging
-    timeout: 20003
+    timeout: 2000003
   }
   //
   // =====

Then, in your test, you can add a breakpoint (via browser.debug();) to pause
execution so you can debug in the browser. It may be worthwhile to add several
browser.debug() invocations through your flow to trace execution (.exit
will exit the debugger and continue execution of the test).
At that point you can open up DevTools in the browser to debug the current
state, or inspect it visually. Some utilities (those in selenium-utils.js,
including pierceShadows) have already been loaded.
There are also some commands available natively at that point, including
.help and the browser variable (including methods such as
browser.execute()).
Attaching a Debugger
To attach a debugger, uncomment the execArgv --inspect configuration option.
It's likely that you'll still want to have increased the mochaTimeout and to
be running graphically, so those are in the example as well:
arcs/shells> git diff test/wdio.conf.js
diff --git a/test/wdio.conf.js b/test/wdio.conf.js
index 0e36452..4240c0a 100644
--- a/test/wdio.conf.js
+++ b/test/wdio.conf.js
@@ -50,11 +50,12 @@ exports.config = {
       chromeOptions: {
         args: [
           // arcs note: comment this out to see the system running
-          '--headless'
+          // '--headless'
         ]
       }
     }
   ],
+  execArgv: ['--inspect'],
   //
   // ===================
   // Test Configurations
@@ -139,7 +140,7 @@ exports.config = {
   mochaOpts: {
     ui: 'bdd',
     // arcs note: increase this timeout for debugging
-    timeout: 20003
+    timeout: 2000003
   }
   //
   // =====

When starting, you should see log item like debugger listening on ws://127.0.0.1:9229/.. as normally appears for node
debugging. Passing the --inspect
argument will also enable the V8 Inspector
Integration which may be easier to use
(to activate this, look for a node icon in a Chrome DevTools process).
Adding debugger; statements may be the easiest way to activate the debugger.
Using browser.debug(); statements to pause execution to give you time to
attach a debugger may be helpful as well.
Releasing
Our release process is pretty minimal, but requires a few steps across the
arcs and
arcs-live repositories.
Our standard is to have the stable versions start with clean (empty)
databases, but to continue a single mainline/unstable database.


Decide what your new mainline and stable versions will be. For an example
here, I'll use 0.3.5-alpha as the old mainline, 0.3.6-alpha as the new
mainline, and 0.3.5 as the new stable version.


In order to keep the mainline data roughly consistent, clone the data at
the current firebase key to the new mainline release number. To do this, I
used the firebase web interface to ""Export JSON"" for the current tree, and
""Import JSON"" to the new tree.


For example, clone from <snip>/database/arcs-storage/data/0_3_5-alpha to
<snip>/database/arcs-storage/data/0_3_6-alpha.
If the web interface is read-only due to too many nodes, you can visit the
new version's URL directly to Import JSON.


Update the version in shell/apps/common/firebase-config.js to a
reasonable stable version (in our example, 0.3.5). See
#1114 for an example.
Update the links README.md (this file) to reflect this new version.


Once the deploy is done to
arcs-live, create a new
release. Note that we
remap the versions slightly between the two systems for legibility in
different systems - a version of 0_3_5 (in firebase-config.js) becomes
v0.3.5 (in the arcs-live repo).


Update the version in shell/apps/common/firebase-config.js to the
new mainline development version (perhaps using the -alpha suffix; in our
example, 0.3.6-alpha).  See
#1155 for an example.


",39
wenhaoliang/learn-python,Python,"学习python的旅程
",4
nukosuke/go-zendesk,Go,"go-zendesk






Zendesk API client library for Go

Reference

Installation
$ go get github.com/nukosuke/go-zendesk
Usage
package main

import (
    ""context""

    ""github.com/nukosuke/go-zendesk/zendesk""
)

func main() {
    // You can set custom *http.Client here
    client := zendesk.NewClient(nil)

    // example.zendesk.com
    client.SetSubdomain(""example"")

    // Authenticate with API token
    client.SetCredential(zendesk.NewAPITokenCredential(""john.doe@example.com"", ""apitoken""))

    // Authenticate with agent password
    client.SetCredential(zendesk.NewBasicAuthCredential(""john.doe@example.com"", ""password""))

    // Create resource
    client.CreateGroup(context.Background(), zendesk.Group{
        Name: ""support team"",
    })
}
Authors

nukosuke
tamccall

License
MIT License.
See the file LICENSE.
",2
greatfire/wiki,None,"
立即下载自由浏览 无需翻墙，畅快浏览谷歌、推特、脸书。
重要提示：自由浏览3.1.0发布(2019/3/19更新)，修复网络问题，无法自动升级的用户请手动下载安装包。














如何获取自由浏览？
您可以直接点击链接或者扫描下面的二维码下载最新版自由浏览：



自由浏览








更多的自由-自由有声


立即下载自由有声
自由有声是一个便捷的安卓端禁书阅读器
拥有书籍封面、轻量化预览、整合有声书与文字书三大特点
内置书库有数百本PDF和有声书
您可以使用它随时随地阅读和分享中国的禁书。















您可以直接点击链接或者扫描下面的二维码下载最新版自由有声：



自由有声








使用应用时遇到了问题？
如果您在使用我们的产品时遇到了问题，您可以按照下面的格式发送邮件到support@greatfire.org

安卓系统版本，或第三方ROM名称/版本（如Android 5.1 MIUI 6.7.14）
使用何种网络 如（中国联通、中国移动4G）
详细描述发生问题时的情况

GreatFire运营的项目

GreatFire翻墙中心 - 测试和销售墙内可用的高级VPN服务。
自由浏览 - 自由浏览是一个免费的安卓应用，可以提供不经审查的互联网接入。
自由有声 - 阅读中国的禁书。
自由微博 - 自由微博提供匿名和不受屏蔽的新浪微博搜索。
GreatFire Analyzer - 一直关注中国的网络审查，带给您中国防火长城的最新资讯。
GreatFire 博客 - 关于国内审查情况的新闻。
泡泡 - 未经审查的互联网信息。



",6743
nukosuke/go-zendesk,Go,"go-zendesk






Zendesk API client library for Go

Reference

Installation
$ go get github.com/nukosuke/go-zendesk
Usage
package main

import (
    ""context""

    ""github.com/nukosuke/go-zendesk/zendesk""
)

func main() {
    // You can set custom *http.Client here
    client := zendesk.NewClient(nil)

    // example.zendesk.com
    client.SetSubdomain(""example"")

    // Authenticate with API token
    client.SetCredential(zendesk.NewAPITokenCredential(""john.doe@example.com"", ""apitoken""))

    // Authenticate with agent password
    client.SetCredential(zendesk.NewBasicAuthCredential(""john.doe@example.com"", ""password""))

    // Create resource
    client.CreateGroup(context.Background(), zendesk.Group{
        Name: ""support team"",
    })
}
Authors

nukosuke
tamccall

License
MIT License.
See the file LICENSE.
",2
greatfire/wiki,None,"
立即下载自由浏览 无需翻墙，畅快浏览谷歌、推特、脸书。
重要提示：自由浏览3.1.0发布(2019/3/19更新)，修复网络问题，无法自动升级的用户请手动下载安装包。














如何获取自由浏览？
您可以直接点击链接或者扫描下面的二维码下载最新版自由浏览：



自由浏览








更多的自由-自由有声


立即下载自由有声
自由有声是一个便捷的安卓端禁书阅读器
拥有书籍封面、轻量化预览、整合有声书与文字书三大特点
内置书库有数百本PDF和有声书
您可以使用它随时随地阅读和分享中国的禁书。















您可以直接点击链接或者扫描下面的二维码下载最新版自由有声：



自由有声








使用应用时遇到了问题？
如果您在使用我们的产品时遇到了问题，您可以按照下面的格式发送邮件到support@greatfire.org

安卓系统版本，或第三方ROM名称/版本（如Android 5.1 MIUI 6.7.14）
使用何种网络 如（中国联通、中国移动4G）
详细描述发生问题时的情况

GreatFire运营的项目

GreatFire翻墙中心 - 测试和销售墙内可用的高级VPN服务。
自由浏览 - 自由浏览是一个免费的安卓应用，可以提供不经审查的互联网接入。
自由有声 - 阅读中国的禁书。
自由微博 - 自由微博提供匿名和不受屏蔽的新浪微博搜索。
GreatFire Analyzer - 一直关注中国的网络审查，带给您中国防火长城的最新资讯。
GreatFire 博客 - 关于国内审查情况的新闻。
泡泡 - 未经审查的互联网信息。



",6743
grirgz/param,SuperCollider,"param
Param Quark for SuperCollider
The goal of this quark is to ease the controlling of sounds objects parameters (Ndef, Pdef, etc) using GUI, MIDI, and OSC.
(
Ndef(\ndef_scalar, { arg freq=200, pan=0, amp=0.1;
	var sig;
	sig = SinOsc.ar(freq);
	sig = Pan2.ar(sig, pan, amp);
}).play;
);

// create a reference to a parameter of a Ndef
~p = Param(Ndef(\ndef_scalar), \freq, \freq.asSpec);

// now the slider control the Ndef parameter \freq
Slider.new.mapParam(~p);

// now the MIDI knob number 16 controls the Ndef parameter \freq
MIDIMap([16], ~p); 

The benefit is you have the same API for controlling Ndef, Pdef, Synth, Bus, ... and you can easily control arrays and envelopes in the same way. Setting .action and updating of GUI, and freeing resources like MIDIFunc and buses are done for you in the background.
Using this library, you can quickly write custom GUI, but more importantly, you can build complex GUI which are completely independent of the sounds objects you control, just pass to your GUI a list a Param you want to control. Mapping any of theses parameters with MIDI is easy too.
// if you have a GUI with 8 sliders, you can simply send it the list of parameters you want to control:
~mygui.set_params( [
	[Pdef(\plop), \freq],
	[Pdef(\plop), \lpfreq],
	[Pdef(\plop), \rq],
	[Ndef(\echo), \shift1],
	[Ndef(\echo), \shift2],
] )

Dependencies : please install the following quark before trying Param:

JITLibExtensions

Current features:

map any Ndef of Pdef parameter to a GUI object, including arrays and envelopes parameters
map any Ndef of Pdef parameter to a MIDI control
write easily a GUI showing current parameters mapped to your MIDI controls
save and load presets, persistent across SC reboot
morph between selected presets
switch quickly between normal mode and bus mode in patterns (bus mode is the way to continuously control a parameter)
block the MIDI until the MIDI value match the Param value to avoid sudden value jump
GUI can be updated in synchronous or polling mode
replace default GUI used by .edit

Controlled objects

Ndef
Pdef
Ndef volume
Volume (eg: s.volume)
TempoClock

Planned features:

control others objects:

Bus
Synth
Instr
list (to have an array of slider sequencing the sound in a pattern for example)


map Param to others GUI objects like PopupMenu
integration with Modality toolkit
easy step sequencer creation with visual feedback
OSC mapping

",7
Lombiq/Orchard-Security,C#,"Lombiq Security Orchard module Readme
Project Description
An Orchard module to enhance security.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/orchard-security (Mercurial repository)
https://github.com/Lombiq/Orchard-Security (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",2
sfcta/prospector,JavaScript,"SFCTA Prospector: Data Warehouse and Visualization Platform
(c) 2018 San Francisco County Transportation Authority and contribution authors.
This software is licensed under the Apache License 2.0. See LICENSE.txt for details.
Full documentation is at:
https://github.com/sfcta/prospector/wiki
",2
swaldman/sbt-ethereum,Scala,"SBT-Ethereum
An SBT-based development environment and command line for Ethereum
Please see the documentation at www.sbt-ethereum.io
",56
swesust/backEnd,Python,"💀 backEnd (SWE Society Website)
developing on django (2.0 or up)

Dependency:


Django 2.15 or up


Python 3.6.0 or up


Database: PostgreSQL 10.0 or up


User: swe


Password: pass1234


Name: swedb





Deployment:


Install the packages
pip3 install -r requirements.txt


Migrattion the project with database


python3 manage.py makemigrations


python3 manage.py migrate




Create Admin User

python3 manage.py createsuperuser



UserId: 2016831035






Name: Rafiul Islam






Email: user@mail.com






Is Student: True






Password: #######






Confirm Password: #######






Run the Server

python3 manage.py runserver 0.0.0.0:8000




Workflow:
urls:


example.com/ (template = index.html)


example.com/login (template = login.html) [not dialog view]no registration page


example.com/opensource (template = opensource.html) [view is not comfirmed yet, will be updated]


example.com/id (template = profile.html) see profile structure


example.com/batch (template = batchlist.html) [all batch list]


example.com/batch/2017 (template = batch.html)[every particular batch students]


example.com/faculty (template = faculty.html) [teachers list]


templates:
base.html will carry the base header and a content block. Every extended or inherited .html file will set on the content block of the base view
prototype of base.html
<!DOCTYPE html>
<html>
<head>

	<link rel=""stylesheet"" type=""text/css"" href="""">
	<script type=""text/javascript""></script>

	{% block extra_head %}

	<!-- extended page header -->

	{% endblock %}

</head>
<body>
 	
 	<div class=""header""></div>

 	{% block contents %}

 	<!-- extended page contents -->

 	{% endblock %}

 	<div class=""footer""></div>

</body>
</html>

prototype of other templates
{% extends 'base.html' %}
{% load staticfiles %}

{% extra_head %}
	
	<title></title>
	<link rel=""stylesheet"" type=""text/css"" href="""">
	<script type=""text/javascript""></script>

{% endblock %}


{% block contents %}

<div class=""contents"">
	
	<!-- page contents -->

</div>

{% endblock %}


User profile and authentication:

User types



General
Staff
Super




Can only log in into site have no permission to visit admin panel
Can log in into site and have the permission to visit admin panel and also have some limited permission on admin panel functionality
Can log in into site and admin panel and have every permission. Super user will destroy after deploying. Available in developing version.



",4
certbot/certbot,Python,"Certbot is part of EFF’s effort to encrypt the entire Internet. Secure communication over the Web relies on HTTPS, which requires the use of a digital certificate that lets browsers verify the identity of web servers (e.g., is that really google.com?). Web servers obtain their certificates from trusted third parties called certificate authorities (CAs). Certbot is an easy-to-use client that fetches a certificate from Let’s Encrypt—an open certificate authority launched by the EFF, Mozilla, and others—and deploys it to a web server.
Anyone who has gone through the trouble of setting up a secure website knows what a hassle getting and maintaining a certificate is. Certbot and Let’s Encrypt can automate away the pain and let you turn on and manage HTTPS with simple commands. Using Certbot and Let's Encrypt is free, so there’s no need to arrange payment.
How you use Certbot depends on the configuration of your web server. The best way to get started is to use our interactive guide. It generates instructions based on your configuration settings. In most cases, you’ll need root or administrator access to your web server to run Certbot.
Certbot is meant to be run directly on your web server, not on your personal computer. If you’re using a hosted service and don’t have direct access to your web server, you might not be able to use Certbot. Check with your hosting provider for documentation about uploading certificates or using certificates issued by Let’s Encrypt.
Certbot is a fully-featured, extensible client for the Let's
Encrypt CA (or any other CA that speaks the ACME
protocol) that can automate the tasks of obtaining certificates and
configuring webservers to use them. This client runs on Unix-based operating
systems.
To see the changes made to Certbot between versions please refer to our
changelog.
Until May 2016, Certbot was named simply letsencrypt or letsencrypt-auto,
depending on install method. Instructions on the Internet, and some pieces of the
software, may still refer to this older name.

Contributing
If you'd like to contribute to this project please read Developer Guide.
This project is governed by EFF's Public Projects Code of Conduct.

How to run the client
The easiest way to install and run Certbot is by visiting certbot.eff.org,
where you can find the correct instructions for many web server and OS
combinations.  For more information, see Get Certbot.

Understanding the client in more depth
To understand what the client is doing in detail, it's important to
understand the way it uses plugins.  Please see the explanation of
plugins in
the User Guide.

Links
Documentation: https://certbot.eff.org/docs
Software project: https://github.com/certbot/certbot
Notes for developers: https://certbot.eff.org/docs/contributing.html
Main Website: https://certbot.eff.org
Let's Encrypt Website: https://letsencrypt.org
Community: https://community.letsencrypt.org
ACME spec: http://ietf-wg-acme.github.io/acme/
ACME working area in github: https://github.com/ietf-wg-acme/acme
 
  

System Requirements
See https://certbot.eff.org/docs/install.html#system-requirements.

Current Features

Supports multiple web servers:
apache/2.x
nginx/0.8.48+
webroot (adds files to webroot directories in order to prove control of
domains and obtain certs)
standalone (runs its own simple webserver to prove you control a domain)
other server software via third party plugins


The private key is generated locally on your system.
Can talk to the Let's Encrypt CA or optionally to other ACME
compliant services.
Can get domain-validated (DV) certificates.
Can revoke certificates.
Adjustable RSA key bit-length (2048 (default), 4096, ...).
Can optionally install a http -> https redirect, so your site effectively
runs https only (Apache only)
Fully automated.
Configuration changes are logged and can be reverted.
Supports an interactive text UI, or can be driven entirely from the
command line.
Free and Open Source Software, made with Python.

For extensive documentation on using and contributing to Certbot, go to https://certbot.eff.org/docs. If you would like to contribute to the project or run the latest code from git, you should read our developer guide.
",25028
ibcom/1blankspace,JavaScript,"1blankspace
All the 1blankspace javascript files that support https://app.1blankspace.com.
You can fork the repo to create your own app or generate a pull request for any fixes / improvements.
1blankspace namespace documentation.
Scripts are highly dependent on jQuery and jQueryUI/themeroller.
about.json
Updates:
1blankspace.com/updates
Dependencies:
/jscripts/fullcalendar-1.5.4/fullcalendar.css
/jscripts/1blankspace-2.0.4.css
/jscripts/jquery-1.8.3.min.js
/jscripts/lodash-4.13.1.min.js
/jscripts/jqueryui/jqueryui-1.8.12.min.js
/jscripts/jqueryui/jquery-ui-1.8.11.custom.min.js
/jscripts/jqueryui/jqueryui-timepicker.js
/jscripts/jquery.cookie.js
/jscripts/modernizr-load.js
/jscripts/md5-min.js
/jscripts/cryptojs-3.1.2/pbkdf2.js
/jscripts/cryptojs-3.1.2/aes.js
/jscripts/tinymce-4.1.5/tinymce.min.js
/jscripts/fullcalendar.min.js
/jscripts/date-2.0.0.js
/jscripts/moment.min.js
/jscripts/accounting.min.js
/jscripts/toword.js
/jscripts/chart.min.js
/jscripts/chart.HorizontalBar.js
/jscripts/FileSaver.min.js
/jscripts/numeral.min.js

",4
wallabyjs/ngCliWebpackSample,TypeScript,"
Wallaby.js
To get wallaby.js working after you have generated your project with angular-cli, you'll need to do the following:

Add the wallaby.js config file to the project.
Add the wallabyTest.ts bootstrap file to the project.
Exclude the src/wallabyTest.ts file in the tsconfig.json as it may affect Angular AOT compilation.
Run npm install wallaby-webpack angular2-template-loader --save-dev.

Note that the sample is using Chrome (headless) runner.
Alternatively, you may use Electron test runner. In this case you may  change the env setting to env: {kind: 'electron'}, and run npm i electron --save-dev.
You may use PhantomJs runner if you like, to do that you may remove the env setting. In this case, you will not need to npm i electron (and can remove electron from the package.json dependencies), however will need to uncomment core-js polyfills and Intl polyfill, so that PhantomJs may work.
Wallaby configuation for Jest
If you are looking for a way to use wallaby.js with Jest for angular-cli generated project, then you may find the working config in this docs section.
",63
magmo/apps,TypeScript,"
 
Magmo Apps

Welcome to the Magmo mono-repo, home of several proof-of-concept applications built on our state channel protocols.
For more information
On our website you will find links to our whitepapers and contact information. Whether you simply want to try
out our apps, or get involved more deeply we would love to hear your thoughts. Deployed versions of our games may be accessed with these links:

Rock Paper Scissors (RPS)
Tic Tac Toe (TTT)

Getting Started
Setting up development environment and running a game application
You will need yarn installed (see here for instructions). After cloning the code,


In the top directory, run yarn install.


Run npx lerna bootstrap.


Start ganache by running yarn ganache:start in one of the package directories.


(In a new shell) Run the wallet via yarn start in the wallet package directory


(In a new shell) Run a game (either RPS or TTT) via yarn start in the relevant package directory.


Add MetaMask to your browser, and point it to localhost:3001 to view the application. You will need to import one of our (testnet-only) seed accounts into metamask to have funds to transact.

Open the metamask browser extension
Click on the account icon (circle in the top right)
Select ""Import""
Paste in one of these secret keys.



You may visit the app in two different browsers in order to play against yourself. We use Redux DevTools to develop and test our apps.
Configuration
All default configuration values are located in the .env and .env.production files.
These can be overridden by adding a .env.local or .env.production.local and specifying values there.
To run storybook
We use Storybook to view our react components during development. You can start Storybook by running:
yarn storybook

in the relevant package directory. This will fire up the Storybook panel inside a browser.
To create an optimized production build:


Optionally override the TARGET_NETWORK by setting the value in your .env.production.local file. Otherwise the application will be built against the ropsten test network.


Build the application:
yarn run build



To deploy smart contracts

Add test eth to your account for the deployment using an eth faucet: https://faucet.ropsten.be/ or https://faucet.metamask.io.
Set TARGET_NETWORK in your .env.local file to the network you want to deploy to: either 'development', 'ropsten', 'kovan' or 'rinkeby'.
Deploy the contracts to the network:
yarn deployContracts
Alternatively, simply run, e.g. TARGET_NETWORK=ropsten yarn deployContracts.

Running Tests specific to a certain app
From the relevant subdirectory...

To run application tests in watch mode:

yarn test:app


To run smart contract tests:

yarn test:contracts


To run all tests relating (before submitting a PR):

yarn test


To update dependencies:

npx lerna bootstrap


To add a dependency:

npx lerna add [dependency name] --scope=[target package]

This installs the latest version of the dependency to the target package (ttt, rps or wallet). Use --dev flag to add the new package to devDependencies instead of dependencies.

To update the version of a dependency:

yarn upgrade [package-name@version-number]

Documentation
We are working hard to produce documenation for our applications. In the interim, please see our Developer Handbook, which as some of the hints and tips
for developing on ethereum that we have used to develop our apps. You will also find some information in the /notes/ subdirectory of each app.
Problems?
Frequently, problems can be sorted by one or more of the following steps:

Resetting your MetaMask account (i.e. deleting transaction history), or simply switching to a different network and back again.
Restarting ganache
Running npx lerna bootstrap if you changed any dependencies

Otherwise, please check issues, someone else may have had the same experience. You may find a solution -- if not, please add to or create an issue.
Contributing
We welcome contributions! If you want to contribute, you should be aware of the following conventions:
Prettier
Prettier is configured via .prettierrc.
Tests will fail if code does not satisfy the rules specificied in .prettierrc.
We suggest that you configure your editor to auto-format using prettier,
or that you run it in a pre-commit git hook.
You can also run yarn prettier:write.
Naming

Directories and files should be kebab-cased.

// ok
foo/bar/foo-bar/foo-bar.ts

// ok
foo/bar/foo-bar/foo-bar.tsx

// tslint error
foo/bar/foo-bar/fooBar.ts

// tslint error
foo/bar/foo-bar/FooBar.tsx

// no tslint error, but please avoid this
foo/bar/fooBar/foo-bar.svg

// no tslint error, but please avoid this
foo/bar/foo-bar/FooBar.svg

",6
Thilas/chocolatey-packages,PowerShell,"Thilas Packages


Update status

chocolatey/Thilas
This repository contains chocolatey automatic packages.
The repository is setup so that you can manage your packages entirely from the GitHub web interface (using AppVeyor to update and push packages) and/or using the local repository copy.
Reminders

All packages in this repo should be in conformity with the contributing guidelines.
Get the production url (for icons) by pasting the raw url here and using the right (CDN) result.

Prerequisites
To run locally you will need:

Powershell 5+: cinst powershell.
Chocolatey Automatic Package Updater Module: Install-Module au or cinst au.

In order to setup AppVeyor update runner please take a look at the AU wiki AppVeyor section.
Create a package
To create a new package see Creating the package updater script.
Testing the package
In a package directory run: Test-Package. This function can be used to start testing in chocolatey-test-environment via Vagrant parameter or it can test packages locally.
Automatic package update
Single package
Run from within the directory of the package to update that package:
cd <package_dir>
./update.ps1

If this script is missing, the package is not automatic.
Set $au_Force = $true prior to script call to update the package even if no new version is found.
Multiple packages
To update all packages run ./update_all.ps1. It accepts few options:
./update_all.ps1 -Name a*                         # Update all packages which name start with letter 'a'
./update_all.ps1 -ForcedPackages 'cpu-z copyq'    # Update all packages and force cpu-z and copyq
./update_all.ps1 -ForcedPackages 'copyq:1.2.3'    # Update all packages but force copyq with explicit version
./update_all.ps1 -Root 'c:\packages'              # Update all packages in the c:\packages folder
The following global variables influence the execution of update_all.ps1 script if set prior to the call:
$au_NoPlugins = $true        #Do not execute plugins
$au_Push      = $false       #Do not push to chocolatey
You can also call AU method Update-AUPackages (alias updateall) on its own in the repository root. This will just run the updater for the each package without any other option from update_all.ps1 script. For example to force update of all packages with a single command execute:
updateall -Options ([ordered]@{ Force = $true })

Testing all packages
You can force the update of all or subset of packages to see how they behave when complete update procedure is done:
./test_all.ps1                            # Test force update on all packages
./test_all.ps1 'cdrtfe','freecad', 'p*'   # Test force update on only given packages
./test_all.ps1 'random 3'                 # Split packages in 3 groups and randomly select and test 1 of those each time
Note: If you run this locally your packages will get updated. Use git reset --hard after running this to revert the changes.
Pushing To Community Repository Via Commit Message
You can force package update and push using git commit message. AppVeyor build is set up to pass arguments from the commit message to the ./update_all.ps1 script.
If commit message includes [AU <forced_packages>] message on the first line, the forced_packages string will be sent to the updater.
Examples:

[AU pkg1 pkg2]
Force update ONLY packages pkg1 and pkg2.
[AU pkg1:ver1 pkg2 non_existent]
Force pkg1 and use explicit version ver1, force pkg2 and ignore non_existent.

To see how versions behave when package update is forced see the force documentation.
You can also push manual packages with command [PUSH pkg1 ... pkgN]. This works for any package anywhere in the file hierarchy and will not invoke AU updater at all.
If there are no changes in the repository use --allow-empty git parameter:
git commit -m '[AU copyq less:2.0]' --allow-empty
git push

Start using AU with your own packages
To use this system with your own packages do the following steps:

Fork this project. If needed, rename it to au-packages.
Delete all existing packages.
Edit the README.md header with your repository info.
Set your environment variables. See AU wiki for details.

Add your own packages now, with this in mind:

You can keep both manual and automatic packages together. To get only AU packages any time use Get-AUPackages function (alias lsau or gau)
Keep all package additional files in the package directory (icons, screenshots etc.). This keeps everything related to one package in its own directory so it is easy to move it around or remove it.

",5
elixir-lang/elixir,Elixir,"


Elixir is a dynamic, functional language designed for building scalable
and maintainable applications.
For more about Elixir, installation and documentation,
check Elixir's website.
Announcements
New releases are announced in the
announcements mailing list.
All security releases will be tagged with [security].
Compiling from source
For the many different ways to install Elixir,
see our installation instructions on the website.
To compile from source, you can follow the steps below.
First, install Erlang. Then clone this repository to your machine, compile and test it:
git clone https://github.com/elixir-lang/elixir.git
cd elixir
make clean test

Note: if you are running on Windows,
this article includes important notes for compiling Elixir from source
on Windows.

If Elixir fails to build (specifically when pulling in a new version via
git), be sure to remove any previous build artifacts by running
make clean, then make test.
If tests pass, you can Interactive Elixir by running bin/iex in your terminal.
However, if tests fail, it is likely you have an outdated Erlang/OTP version
(Elixir requires Erlang/OTP 20.0 or later). You can check your Erlang/OTP version
by calling erl in the command line. You will see some information as follows:
Erlang/OTP 20 [erts-9.0] [smp:2:2] [async-threads:10] [kernel-poll:false]

If you have properly set up your dependencies and tests still fail,
you may want to open up a bug report, as explained next.
Bug reports
For reporting bugs, visit our issues tracker and follow the steps
for reporting a new issue. Please disclose security vulnerabilities
privately at elixir-security@googlegroups.com.
Proposing new features
For proposing new features, please start a discussion in the
Elixir Core mailing list. Keep in mind that it is your responsibility
to argue and explain why a feature is useful and how it will impact the
codebase and the community.
Once a proposal is accepted, it will be added to the issues tracker.
The issues tracker focuses on actionable items and it holds a list of
upcoming enhancements and pending bugs. All entries in the tracker are
tagged for clarity and to ease collaboration.
Features and bug fixes that have already been merged and will be included
in the next release are marked as ""closed"" in the issues tracker and are
added to the CHANGELOG.
Finally, remember all interactions in our official spaces follow our
Code of Conduct.
Contributing
We welcome everyone to contribute to Elixir. To do so, there are a few
things you need to know about the code. First, Elixir code is divided
in applications inside the lib folder:


elixir - Contains Elixir's kernel and stdlib


eex - Template engine that allows you to embed Elixir


ex_unit - Simple test framework that ships with Elixir


iex - IEx, Elixir's interactive shell


logger - The built-in logger


mix - Elixir's build tool


You can run all tests in the root directory with make test and you can
also run tests for a specific framework make test_#{NAME}, for example,
make test_ex_unit. If you just changed something in the Elixir's standard
library, you can run only that portion through make test_stdlib.
If you are changing just one file, you can choose to compile and run tests only
for that particular file for fast development cycles. For example, if you
are changing the String module, you can compile it and run its tests as:
bin/elixirc lib/elixir/lib/string.ex -o lib/elixir/ebin
bin/elixir lib/elixir/test/elixir/string_test.exs
To recompile (including Erlang modules):
make compile
After your changes are done, please remember to run mix format to guarantee
all files are properly formatted and then run the full suite with
make test.
If your contribution fails during the bootstrapping of the language,
you can rebuild the language from scratch with:
make clean_elixir compile
Similarly, if you can't get Elixir to compile or the tests to pass after
updating an existing checkout, run make clean compile. You can check
the official build status on Travis-CI.
More tasks can be found by reading the Makefile.
With tests running and passing, you are ready to contribute to Elixir and
send a pull request.
We have saved some excellent pull requests we have received in the past in
case you are looking for some examples:

Implement Enum.member? - Pull Request
Add String.valid? - Pull Request
Implement capture_io for ExUnit - Pull Request

Reviewing changes
Once a pull request is sent, the Elixir team will review your changes.
We outline our process below to clarify the roles of everyone involved.
All pull requests must be approved by two committers before being merged into
the repository. If any changes are necessary, the team will leave appropriate
comments requesting changes to the code. Unfortunately we cannot guarantee a
pull request will be merged, even when modifications are requested, as the Elixir
team will re-evaluate the contribution as it changes.
Committers may also push style changes directly to your branch. If you would
rather manage all changes yourself, you can disable ""Allow edits from maintainers""
feature when submitting your pull request.
The Elixir team may optionally assign someone to review a pull request.
If someone is assigned, they must explicitly approve the code before
another team member can merge it.
When the review finishes, your pull request will be squashed and merged
into the repository. If you have carefully organized your commits and
believe they should be merged without squashing, leave a comment.
Building documentation
Building the documentation requires ExDoc
to be installed and built alongside Elixir:
# After cloning and compiling Elixir, in its parent directory:
git clone git://github.com/elixir-lang/ex_doc.git
cd ex_doc && ../elixir/bin/mix do deps.get, compile
Now go back to Elixir's root directory and run:
make docs                  # to generate HTML pages
make docs DOCS_FORMAT=epub # to generate EPUB documents
This will produce documentation sets for elixir, mix, etc. under
the doc directory. If you are planning to contribute documentation,
please check our best practices for writing documentation.
Development links

Elixir Documentation
Elixir Core Mailing list (development)
Issues tracker
Code of Conduct
#elixir-lang on Freenode IRC

License
""Elixir"" and the Elixir logo are copyright (c) 2012 Plataformatec.
Elixir source code is released under Apache 2 License.
Check NOTICE and LICENSE files for more information.
",15257
Civ13-SS13/Civ13,DM,"Civilization 13
WHAT IS THIS?
Civ13 (formerly 1713) is a game based on Space Station 13 code, which features several epochs of human history. (hence the name). It features both RP and Combat maps and gamemodes.



Civ13 Discord

Official Server
http://1713.eu/server
",13
regro/libcfgraph,None,"libcfgraph
Cron Status: 
Graph Data for Conda Forge Library
This repository houses metadata for all of conda-forge's artifacts and is updated hourly.
It is intendeded to be used in conjunction with libcflib which can convert the mountains
of json into something a little nicer to work with
",5
XerTheSquirrel/SquirrelJME,Java,"SquirrelJME
SquirrelJME is intended to be a Java ME 8 compatible environment for
strange and many other devices. That is, you would be able to have a Java
environment on a wide range of Internet of Things devices, older computers
(such as the Amiga), embedded devices, and mobile devices from old to new.
It intends to be 99.9% compatible with Java ME 8 and older J2ME standards. It
should be able to run a wide range of older J2ME software which would enable
this software to be used for archival purposes. It is written purely in Java
and requires only a Java compiler and virtual machine for it to be built.
Note that SquirrelJME is not a Java SE virtual machine nor does it intend
to be one at all. As such, it will not be able to run desktop software unless
it is ported to Java ME. However some programs may run since Java ME is a
subset of Java SE, so provided the desktop software does not use what is
missing or not supported it will run.

Copyright (C) 2013-2019 Stephanie Gawroriski
xer@multiphasicapps.net
""SquirrelJME"" and ""Lex"" are trademarked by Stephanie Gawroriski 2016-2019
Lex and The SquirrelJME Logo were illustrated by...

Kat Adam-MacEwen
Kat@CMYKat Designs & Illustrations http://www.cmykat-designs.com/



Please support me on Patreon!
Badges:



Information:

Code of Conduct
License

Source: GNU General Public License, Version 3 or later
Assets: Creative Commons CC-BY-SA 4.0
Fonts : SIL Open Font License Version 1.1


Contributing to SquirrelJME
End-User Documents

Building
Changelog
Compatibility
History
Release Roadmap
Usage


Developer Resources

CircleCI Status
Developer Guide
Developer Notes
Design Document
Project Scope
SquirrelJME As A Runtime
CLDC 1.8 API (docs.oracle.com)



Also check out SquirrelJME's sister project Mu at:

https://github.com/meepingsnesroms/Mu

I Am Open For Hiring
SquirrelJME is a hobby project (which means I work on it in my spare time)
and as such does not sustain me monetarily. If you do like my work please also
be advised that I am available for hire to work on a number of other projects
potentially. Most of my experience for languages is in C, Java, and
POSIX Shell. Although I do not mainly use other languages they can be learned
by adapting my current experience with those languages. SquirrelJME itself
mostly relates to embedded, compiler, and Java development. I prefer remote
development as it allows me to work from home rather than spending time
commuting every day (which wastes time). Interest can be e-mailed to me
(Stephanie Gawroriski) at xerthesquirrel@gmail.com.
Repository

Online repository
(HTTPS,
Tor (A),
Tor (B))
SquirrelJME uses fossil http://fossil-scm.org/.
The repository may be cloned using Fossil:

One of these commands:

fossil clone -u http://squirreljme.cc/ squirreljme.fossil
fossil clone -u https://squirreljme.cc/ squirreljme.fossil
fossil clone -u http://squirrelzarhffxz.onion/ squirreljme.fossil
fossil clone -u http://squirrelmfbf2udn.onion/ squirreljme.fossil


mkdir squirreljme
cd squirreljme
fossil open ../squirreljme.fossil


Alternatively via GitHub:

git clone https://github.com/XerTheSquirrel/SquirrelJME
cd SquirrelJME


Trunk Source Snapshot (Main website or fossil serve only):

TAR.GZ
ZIP



Links

My Portfolio
My Public PGP Key
SquirrelJME Twitter
Locations where you can get support:

Discord


Front facing social media:

YouTube


Personal Accounts (If you wish to contact me this way)

GitHub
Keybase
LinkedIn
Mastodon (awoo.space)
Twitter
E-Mail: xer@multiphasicapps.net



Goals

To be self-hosting - it can build itself while running on itself, provided
there is a filesystem and enough memory.
To be small - Smaller programs are easier to maintain and are usually
simpler.
To be fast - With the potential of AOT/JIT, systems that do not normally
have a port of an existing virtual machine will usually only have an
interpreter.
To be compatible - So that existing Java ME 8 applications can run on this
virtual machine, along with J2ME applications when required. This has the
added bonus of allowing one to use older software to catalog for
preservation.
To compile once and run anywhere - Current and up to date Java
implementations are limited only to a few select systems (Solaris,
Linux, BSD, Mac OS X, and Windows). Java ME 8 as of this writing
only supports the FRDM-K64F and the Raspberry Pi. There are multitudes
of devices that support J2ME, however that is horribly out of date when
compared with the Java that is used today (J2ME is equal to Java 1.4
which was released in 2002).

Donations
You may donate to SquirrelJME to which would be used by the author to keep the
web server online, the purchasing of new hardware/software, and for the
purchasing of food items for consumption.

Ko-Fi
Patreon
BitCoin (BTC/XBT):
3NEF6Pyt2JfWnb6hn7WFcwH5jpkHYPkTNj
BitCoin Cash (BCH):
qrgww4z9aalgxyyddh5ax8jyy34c89phxuklp3hcwd
Ethereum (ETH):
0x7C2316f5336C63855C199784a6e145921145B74a
Ethereum Classic (ETC):
0x1b30e6411bd7da6dd72cA35dabD5b98BA25A0Ae7
LiteCoin (LTC):
MAogNXdjPQ7ZABtpeXztCHf4UXvfiwTuGX
Stellar Lumens (XLM):
GB4O2MAMTINKS6GM6RH34KO32TJMF4QZKZTFJBE4HIPFMKRBXFQ7IAF6.
ZCash (ZEC):
t1cBD4dJYNWs5TQk1JKFNy1Qkg3TBx8noXQ

Important Statements
JAVA AND ALL OTHER RELATED TRADEMARKS AND COPYRIGHTS ARE OWNED BY ORACLE
CORPORATION http://www.oracle.com/.
SQUIRRELJME IS NEITHER PRODUCED BY NOR ENDORSED BY ORACLE CORPORATION.
ALL SOURCE CODE IS WRITTEN BY MYSELF WITH FUNCTIONALITY DETERMINED BY THE
DOCUMENTED APIS AND RUN-TIME BEHAVIOR. I HAVE NOT LOOKED AT THE SOURCE CODE FOR
OTHER IMPLEMENTATIONS, NEITHER FREE SOFTWARE NOR PROPRIETARY IMPLEMENTATIONS.
ESSENTIALLY SQUIRRELJME IS A CLEAN-ROOM IMPLEMENTATION OF THE DESIGN OF JAVA
ME 8. THE IMPLEMENTATIONS OF THE JAVA ME 8
APIS AND ASSOCIATED JSRS/JEPS IS CONSIDERED BY MYSELF TO BE IN THE FAIR USE
(AND IT ALSO HELPS THE JAVA ECOSYSTEM BY PERMITTING IT TO RUN IN MORE PLACES
WHERE IT IS NOT SUPPORTED BY ORACLE).
THIS SOFTWARE IS INCOMPLETE AND IN ITS CURRENT STATE IS NOT CURRENTLY
100% COMPATIBLE WITH JAVA ME 8. THERE ARE CURRENTLY NO STABLE RELEASED VERSIONS
OF THIS SOFTWARE. AS SUCH THE FIRST MAJOR VERSION RELEASE WILL BE WHEN IT IS
COMPATIBLE SO AS TO NOT VIOLATE POTENTIAL LICENSES, AND IF POSSIBLE BE VERIFIED
WITH ANY APPLICABLE TCKS (SO IT WOULD BE AN ""OFFICIAL"" IMPLEMENTATION).
",47
Preetam/mini-lisp,Go,"mini-lisp
A small Lisp implementation in Go.
Examples
;; Factorial
(define fact (lambda (n) (if (<= n 1) 1 (* n (fact (- n 1))))))
(fact 5)
; 120

;; sum2 demonstrating tail recursion optimization
(define sum2 (lambda (n acc) (if (= n 0) acc (sum2 (- n 1) (+ n acc)))))
(sum2 1000 0)
; 500500
Features

REPL
Lambdas
Tail recursion optimization

References

mal - Make a Lisp
(How to Write a (Lisp) Interpreter (in Python))

",5
TeslaCloud/flux-ce,Lua,"
Flux is a WIP gamemode framework designed with performance and convenience in mind.
Alpha release
Current version of Flux is currently in active development as an open alpha. This means that you can install it and it will run, but there will almost inevitably be bugs and issues, as well as a lot of missing features. If you are not a developer, it is probably better for you to wait until Flux is in beta.
Installation
If you want to just get Flux up and running, simply download this repo, as well as the reborn schema, and match the repo folders with your Garry's Mod dedicated server folders. You can also clone this repo directly and then install the server on top, so that you get an easy way to update without hassle.
Flux is only guaranteed to work on dedicated servers (srcds). We do not support ""listen"" servers (launching from Garry's Mod client).
Database setup
Depending on your use case, you may want to setup a database. An SQLite is the default option and requires no further setup. It is perfect if you simply want to take a look at Flux and how it works. If you want to run Flux in production, however, you should consider setting up a MySQL (MariaDB) or PostgreSQL database.
Follow the instructions in /garrysmod/gamemodes/flux/config/database.yml to learn more.
Environment
By default, Flux comes with production environment pre-chosen. It is good if you don't want to write code. If you plan on writing plugins, schemas or modifying the framework, you should set your environment to development. No other environments are supported yet! If you wish to change your environment, copy the gamemodes/flux/config/environment.lua file as environment.local.lua and change production to development inside that file.
What is the difference between production and development?
In production, code runs a little bit faster, but it sacrifices error-tolerance and refreshability. It it perfect when you are running your server properly, because in that case you don't want to refresh the code anyway (since it causes a lot of lag).
In development, code runs slower, but is a lot more tolerant to errors. It uses safe mode on hooks and print lots of useful debug information, such as load order. Due to the speed sacrifice, it is only practical to run development when actually developing.
Upgrading
During Alpha, the database may break between versions. This will be different in beta and beyond, but until then, if you are upgrading Flux you need to recreate the database manually every time.
To do that, simply follow the steps below:

Delete the /garrysmod/gamemodes/**your_schema**/db/ folder.
Follow the database-specific instructions below:

SQLite

Simply delete the /garrysmod/sv.db file.
Restart the server.

MariaDB (MySQL)

Open the MySQL console (mysql command on Linux) or any other means of managing your database.
Drop the table specified in /garrysmod/gamemodes/flux/config/database[.local].yml. To do that from console, simply run DROP DATABASE database_name_here;, replace database_name_here with your database name.
Create a new database. To do that, run CREATE DATABASE database_name_here;, replace database_name_here with your database name.
Restart the server.

PostgreSQL

Open psql or pgAdmin (sudo -u postgres psql).
Drop the table specified in /garrysmod/gamemodes/flux/config/database[.local].yml. To do that from psql, simply run DROP DATABASE database_name_here;, replace database_name_here with your database name.
Create a new database. To do that, run CREATE DATABASE database_name_here;, replace database_name_here with your database name.
Restart the server.

Playing
If you wish to play the gamemode, you should install the content addon to prevent purple-black checkers where the materials should be. You can find it here: https://steamcommunity.com/sharedfiles/filedetails/?id=1518849094
Other info
For more info or technical support, please visit our forums: http://f.teslacloud.net/
",12
dagster-io/dagster,Python,"






Introduction
Dagster is a system for building modern data applications. Combining an elegant programming model and beautiful tools, Dagster allows infrastructure engineers, data engineers, and data scientists to seamlessly collaborate to process and produce the trusted, reliable data needed in today's world.
pip install dagster dagit and jump immediately to our tutorial
Or read our complete documentation
For details on contributing or running the project for development, read here.
This repository contains a number of distinct subprojects.
Top Level Tools:

dagster: The core programming model and abstraction stack; stateless, single-node,
single-process and multi-process execution engines; and a CLI tool for driving those engines.
dagit: A rich development environment for Dagster, including a DAG browser, a type-aware config editor,
and a streaming execution interface.
dagstermill: Built on the papermill library (https://github.com/nteract/papermill) Dagstermill is meant for integrating productionized Jupyter notebooks into dagster pipelines.
dagster-airflow: Allows Dagster pipelines to be scheduled and executed, either containerized or uncontainerized, as Apache Airflow DAGs (https://github.com/apache/airflow)

Supporting Libraries:

libraries/dagster-aws: Dagster solids and tools for interacting with Amazon Web Services.
libraries/dagster-ge: A Dagster integration with Great Expectations. (see
https://github.com/great-expectations/great_expectations)
dagster-pandas: A Dagster integration with Pandas.
dagster-pyspark: A Dagster integration with Pyspark.
dagster-snowflake: A Dagster integration with Snowflake.
dagster-spark: A Dagster integration with Spark.

Example Projects:

airline-demo: A substantial demo project illustrating how these tools can be used together
to manage a realistic data pipeline.
event-pipeline-demo: A substantial demo project illustrating a typical web event processing
pipeline with Spark and Scala.

Internal Libraries;

js_modules/dagit - The web UI for dagit
dagster-graphql: A GraphQL-based interface for interacting with the Dagster engines and
repositories of Dagster pipelines.

Come join our slack!: https://tinyurl.com/dagsterslack
",173
dagster-io/dagster,Python,"






Introduction
Dagster is a system for building modern data applications. Combining an elegant programming model and beautiful tools, Dagster allows infrastructure engineers, data engineers, and data scientists to seamlessly collaborate to process and produce the trusted, reliable data needed in today's world.
pip install dagster dagit and jump immediately to our tutorial
Or read our complete documentation
For details on contributing or running the project for development, read here.
This repository contains a number of distinct subprojects.
Top Level Tools:

dagster: The core programming model and abstraction stack; stateless, single-node,
single-process and multi-process execution engines; and a CLI tool for driving those engines.
dagit: A rich development environment for Dagster, including a DAG browser, a type-aware config editor,
and a streaming execution interface.
dagstermill: Built on the papermill library (https://github.com/nteract/papermill) Dagstermill is meant for integrating productionized Jupyter notebooks into dagster pipelines.
dagster-airflow: Allows Dagster pipelines to be scheduled and executed, either containerized or uncontainerized, as Apache Airflow DAGs (https://github.com/apache/airflow)

Supporting Libraries:

libraries/dagster-aws: Dagster solids and tools for interacting with Amazon Web Services.
libraries/dagster-ge: A Dagster integration with Great Expectations. (see
https://github.com/great-expectations/great_expectations)
dagster-pandas: A Dagster integration with Pandas.
dagster-pyspark: A Dagster integration with Pyspark.
dagster-snowflake: A Dagster integration with Snowflake.
dagster-spark: A Dagster integration with Spark.

Example Projects:

airline-demo: A substantial demo project illustrating how these tools can be used together
to manage a realistic data pipeline.
event-pipeline-demo: A substantial demo project illustrating a typical web event processing
pipeline with Spark and Scala.

Internal Libraries;

js_modules/dagit - The web UI for dagit
dagster-graphql: A GraphQL-based interface for interacting with the Dagster engines and
repositories of Dagster pipelines.

Come join our slack!: https://tinyurl.com/dagsterslack
",173
kaiwahour/home,JavaScript,"Kaiwa Hour Website
Public facing website for Kaiwa Hour built in React. This website is current
under construction. View the latest (pre-)deployment here.

Photo credit: Bernard Hermant on Unsplash
Getting started
To build the website, first git clone this repository. Then from the command
line, run:
yarn install

Start the development server with
yarn start
Problems with yarn?
First, try deleting yarn.lock and the /node_modules directory (if it exists). Then run yarn, and if that succeeds, run yarn update.
Getting involved
Want to get involved? Check out the open
Issues for this
project. More detailed guidelines and instructions on contributing will soon
be added.
",4
jpedrodias/MicroPython,Python,"MicroPython
Some tools to help when using MicroPython (tested on Wemos D1 Mini - esp8266)

how to use WLAN Manager
how to use MQTT Manager
how to use Sensors Manager
how to use Board Manager (at work)
how to use Robot Manager (at work) 

PS: My personilized version of MicroPython in compiled folder, already has this files (wlan_manager, mqtt_manager, sensor_manager, board_manager and robot_manager)
Wemos D1 mini :: GPIO MAP

PIN: D0D1D2D3D4D5D6D7D8
GPIO:16 5 4 0 214121315
PWM:  N Y Y Y Y Y Y Y Y

Wemos D1 mini :: Boot Mode Options

GPIO15GPIO0GPIO2Mode Comment
   D8  D3  D4  Comment
  L     H    H    Flashboot from SPI Flash  
  L     L    H    UARTProgram via UART (TX/RX)
  H     any  any  SDIOBoot from SD card

WLAN Manager :: Setup
Send wlan_manager.py to board using:
ampy -p /dev/ttyUSB0 put wlan_manager.py

The first time you need to run the setup() function. This function will creat the file wlan_manager.json to store SSID and password
from wlan_manager import WLAN_Manager
wlan_client = WLAN_Manager()
wlan_client.setup() # creates wlan_manager.json file to store your SSID and password
wlan_client.setup('HOME', 'password', append=False) # overwrite the file and store this settings
wlan_client.setup('WORK', 'password', append=True)  # appends this settings to the file
wlan_client.start()

MQTT Manager :: Setup
Send mqtt_manager.py and mqtt_manager.json (change here your mqtt setting before send) to board using:
ampy -p /dev/ttyUSB0 put mqtt_manager.py
ampy -p /dev/ttyUSB0 put mqtt_manager.json

from mqtt_manager import MQTT_Manager
mqtt_client = MQTT_Manager()
mqtt_client.setup() # creates mqtt_manager.json file to store your broker settings
print( 'client_id:', mqtt_client.CONFIG['client_id'] )
WLAN and MQTT Manager :: main loop example
def reconnect():
  wlan_client.start()
  success = wlan_client.check() and mqtt_client.check()
  if success:
    mqtt_client.broker.subscribe(TOPIC_SUB)
  return success
  
from wlan_manager import WLAN_Manager
wlan_client = WLAN_Manager()

from mqtt_manager import MQTT_Manager
mqtt_client = MQTT_Manager()

TOPIC_SUB = mqtt_client.get_topic('control') # You talking to the sensor
TOPIC_PUB = mqtt_client.get_topic('status')  # The sensor talking to you
chatty_client =  bool(mqtt_client.CONFIG.get('chatty', True))
mqtt_client.broker.set_callback(mqtt_callback)
print( 'client_id:', mqtt_client.CONFIG['client_id'] )

connected = reconnect()
if connected:
  mqtt_client.send('debug', TOPIC_SUB)
  mqtt_client.send('debug', TOPIC_PUB)
  mqtt_client.send('debug', app_name)

Sensor Manager :: Setup
Send sensor_manager.py to board using:
ampy -p /dev/ttyUSB0 put sensors_manager.py

Sensors Manager :: Using DHT22 (or DHT11) example (temperature and humidity sensor)
import machine, time

from sensor_manager import Sensor_DHT22 # or DHT11
sensor = Sensor_DHT22( 5 ) # Pin 5 = D1 

while True:
  sensor.read()
  print(sensor.values, sensor.values_dict)
  time.sleep(1)

Sensor Manager :: Using DS18B20 example (temperature sensor)
import machine, time

from sensor_manager import Sensor_DS18B20
sensor = Sensor_DS18B20( 5 ) # Pin 5 = D1

while True:
  sensor.read()
  print(sensor.values, sensor.values_dict)
  time.sleep(1)

Sensor Manager :: example using the BME280 (pressure, temperature and humidity sensor)
import machine, time

i2c = machine.I2C(scl=machine.Pin(5), sda=machine.Pin(4)) # Pin 5 = D1 | Pin 4 = D2
from sensor_manager import Sensor_BME280
sensor = Sensor_BME280(i2c=i2c, address=0x76) # to find address use i2c.scan()

while True:
  sensor.read()
  print(sensor.values, sensor.values_dict)
  time.sleep(1)

Note: also need to put the file bme280.py (or bme280.mpy) in the root folder using:
ampy -p /dev/ttyUSB0 put bme280.py bme280.py

Sensor Manager :: example using the HC-SR04 (UltraSonic distance sensor)
import machine, time

from sensor_manager import Sensor_HCSR04
sensor = Sensor_HCSR04(trigger=5, echo=4)

while True:
  sensor.read()
  print(sensor.values, sensor.values_dict, sensor.distance_mm, sensor.distance_cm)
  time.sleep(1)

Sensor Manager :: example using the VL53L0X (Light distance sensor)
import machine, time

i2c = machine.I2C(scl=machine.Pin(5), sda=machine.Pin(4)) # Pin 5 = D1 | Pin 4 = D2
from sensor_manager import Sensor_VL53L0X
sensor = Sensor_VL53L0X(i2c=i2c, address=0x29) # to find address use i2c.scan()

while True:
  sensor.read()
  print(sensor.values, sensor.values_dict)
  time.sleep(1)

Sensor Manager :: example using the BH1750FVI (Lux sensor)
import machine, time

i2c = machine.I2C(scl=machine.Pin(5), sda=machine.Pin(4)) # Pin 5 = D1 | Pin 4 = D2
from sensor_manager import Sensor_BH1750FVI
sensor = Sensor_BH1750FVI(i2c=i2c, address=0x23) # to find address use i2c.scan()

while True:
  sensor.read()
  print(sensor.values, sensor.values_dict)
  time.sleep(1)

End of File
",2
hardenedlinux/Debian-GNU-Linux-Profiles,Shell,"Debian-GNU-Linux-Profiles
Done
DNS
Basic bind9 configuration for lan
Domain based routing on openwrt
HA
Using UPS with NUT
Harbian QA
Benchmarking PaX/Grsecurity kernel on Debian GNU/Linux
Syzkaller crash DEMO
Kernel QA with syzkaller and qemu
Hardened boot
Ways to build your own trustchain for secureboot
Debian Hardened boot
Grub for Coreboot
Grub for Secure boot
Preparation for Secure Boot on Key Management Server
Set Up Unrestricted Secure Boot On supporting machine
IDS
Deploy Bro as an IDS
IPSEC
Building IPSEC VPN via strongswan
Security Operation Center
Using Logstash/Elasticsearch/Grafana to build a small SOC(Security Operation Center) from scratch
SOC Overview
ELK with Bro-based Application Layer Packet Classifier
Storage
Manually deploy ceph cluster step by step
SSH and Cluster
Powerful ssh(1) options you don't know
Ways to authenticate yourself to a remote virtual machine host
Recommended way to use ssh(1) for cluster management
TLS
TLS Mutual Authentication in Webdav
TLS Mutual Authentication in Gitlab
OpenConnect Server For Anyconnect Compatible Service
MAC/RBAC
Grsecurity RBAC system with nginx practice
Grsecurity RBAC system with ceph
Separating the three powers with grsecurity RBAC system

NGINX Hardening
Pretty config for nginx
Unclassified
Small-scale Enterprise KVM Deployments With Kimchi
The recommended configs of host computers and management console running Debian GNU/Linux within clusters
Todo-list
DRBD in HA
LVM Best Practice
Soft Raid in Debian
Gitlab Hardening
Exploring the Cross-platform File Sharing Service
Nginx Tuning in Debian
Apache Tuning in Debian
Puppet on the Run
Large-scale Enterprise KVM Deployments in Debian
Binary Dispatch in Automated Operations
Automated Operations in Debian
Iptables in Practice
Apparmor Best Practice
",116
optimizely/objective-c-sdk,Objective-C,"Objective-C SDK








This repository houses the Objective-C SDK for use with Optimizely Full Stack and Optimizely Rollouts for Mobile and OTT.
Optimizely Full Stack is A/B testing and feature flag management for product development teams. Experiment in any application. Make every feature on your roadmap an opportunity to learn. Learn more at https://www.optimizely.com/platform/full-stack/, or see the documentation.
Optimizely Rollouts is free feature flags for development teams. Easily roll out and roll back features in any application without code deploys. Mitigate risk for every feature on your roadmap. Learn more at https://www.optimizely.com/rollouts/, or see the documentation.
Getting Started
Using the SDK
See the Mobile developer documentation or OTT developer documentation to learn how to set
up an Optimizely X project and start using the SDK.
Requirements

iOS 8.0+ / tvOS 9.0+

Installing the SDK
Please note below that <platform> is used to represent the platform on which you are building your app. Currently, we support iOS and tvOS platforms.
**note: if you or another framework are using sqlite, then you should probably add compiler options for thead safe sqlite: SQLITE_THREADSAFE=1
https://www.sqlite.org/threadsafe.html
Cocoapod

Add the following lines to the Podfile:use_frameworks!
pod 'OptimizelySDK<platform>', '~> 3.1.0'


Run the following command: pod install

Further installation instructions for Cocoapods: https://guides.cocoapods.org/using/getting-started.html
Carthage

Add the following lines to the Cartfile:github ""optimizely/objective-c-sdk"" ~> 3.1.0



Run the following command:carthage update


Link the frameworks to your project. Go to your project target's Link Binary With Libraries and drag over the following from the Carthage/Build/<platform> folder: OptimizelySDKCore.framework
OptimizelySDKDatafileManager.framework
OptimizelySDKEventDispatcher.framework
OptimizelySDKShared.framework
OptimizelySDKUserProfileService.framework
OptimizelySDK<platform>.framework


To ensure that proper bitcode-related files and dSYMs are copied when archiving your app, you will need to install a Carthage build script:

Add a new Run Script phase in your target's Build Phase.
In the script area include:/usr/local/bin/carthage copy-frameworks
Add the paths to the frameworks to the Input Files list:$(SRCROOT)/Carthage/Build/<platform>/OptimizelySDKCore.framework
$(SRCROOT)/Carthage/Build/<platform>/OptimizelySDKDatafileManager.framework
$(SRCROOT)/Carthage/Build/<platform>/OptimizelySDKEventDispatcher.framework
$(SRCROOT)/Carthage/Build/<platform>/OptimizelySDKShared.framework
$(SRCROOT)/Carthage/Build/<platform>/OptimizelySDKUserProfileService.framework
$(SRCROOT)/Carthage/Build/<platform>/OptimizelySDK<platform>.framework
Add the paths to the copied frameworks to the Output Files list:$(BUILT_PRODUCTS_DIR)/$(FRAMEWORKS_FOLDER_PATH)/OptimizelySDKCore.framework
$(BUILT_PRODUCTS_DIR)/$(FRAMEWORKS_FOLDER_PATH)/OptimizelySDKDatafileManager.framework
$(BUILT_PRODUCTS_DIR)/$(FRAMEWORKS_FOLDER_PATH)/OptimizelySDKEventDispatcher.framework
$(BUILT_PRODUCTS_DIR)/$(FRAMEWORKS_FOLDER_PATH)/OptimizelySDKShared.framework
$(BUILT_PRODUCTS_DIR)/$(FRAMEWORKS_FOLDER_PATH)/OptimizelySDKUserProfileService.framework
$(BUILT_PRODUCTS_DIR)/$(FRAMEWORKS_FOLDER_PATH)/OptimizelySDK<platform>.framework



Futher installation instructions for Carthage: https://github.com/Carthage/Carthage
Manual Installation
The universal framework can be used in an application without the need for a third-party dependency manager. The universal framework packages up all Optimizely X Mobile modules, which include:OptimizelySDKCore
OptimizelySDKShared
OptimizelySDKDatafileManager
OptimizelySDKEventDispatcher
OptimizelySDKUserProfileService
The universal framework for iOS includes builds for the following architectures:i386
x86_64
ARMv7
ARMv7s
ARM64
The universal framework for tvOS includes builds for the following architectures:x86_64
ARM64
Bitcode is enabled for both the iOS and tvOS universal frameworks.
In order to install the universal framework, follow the steps below:


Download the iOS or tvOS framework.


Unzip the framework, then drag the framework to your project in Xcode; Xcode should prompt you to select a target. Go to Build Phases and make sure that the framework is under the Link Binary with Libraries section.


Go to the General tab and add the framework to the Embedded Binaries section. If the Embedded Binaries section is not visible, add the framework in the Copy Files section (you can add this section in Build Settings).


The Apple store will reject your app if you have the universal framework installed as it includes simulator binaries. Therefore, a script to strip the extra binaries needs to be run before you upload the app. To do this, go to Build Phases and add a Run Script section by clicking the + symbol. Copy and paste the following script (make sure you replace the FRAMEWORK_NAME with the proper framework name!
):
FRAMEWORK=""FRAMEWORK_NAME""
FRAMEWORK_EXECUTABLE_PATH=""${BUILT_PRODUCTS_DIR}/${FRAMEWORKS_FOLDER_PATH}/$FRAMEWORK.framework/$FRAMEWORK""
EXTRACTED_ARCHS=()
for ARCH in $ARCHS
do
   lipo -extract ""$ARCH"" ""$FRAMEWORK_EXECUTABLE_PATH"" -o ""$FRAMEWORK_EXECUTABLE_PATH-$ARCH""
   EXTRACTED_ARCHS+=(""$FRAMEWORK_EXECUTABLE_PATH-$ARCH"")
done
lipo -o ""$FRAMEWORK_EXECUTABLE_PATH-merged"" -create ""${EXTRACTED_ARCHS[@]}""
rm ""${EXTRACTED_ARCHS[@]}""
rm ""$FRAMEWORK_EXECUTABLE_PATH""
mv ""$FRAMEWORK_EXECUTABLE_PATH-merged"" ""$FRAMEWORK_EXECUTABLE_PATH""



If you choose to build the universal framework yourself, you can do so by running the OptimizelySDKiOS-Universal or OptimizelySDKTVOS-Universal schemes. After building these schemes, the frameworks are output in the OptimizelySDKUniversal/generated-frameworks folder.
Contributing
Please see CONTRIBUTING.
Credits
First-party code (under OptimizelySDKCore/, OptimizelySDKDatafileManager/, OptimizelySDKEventDispatcher/, OptimizelySDKiOS/, OptimizelySDKShared/, OptimizelySDKTVOS/, OptimizelySDKUniversal/, OptimizelySDKUserProfileService/, ) is copyright Optimizely, Inc. and contributors, licensed under Apache 2.0.
Additional Code
FMDB https://github.com/ccgus/fmdb
License (MIT):: https://github.com/ccgus/fmdb/blob/master/LICENSE.txt
Modfied: Yes
Distributed: Yes
Distribution: Binary
SQLITE3 https://www.sqlite.org/index.html
License (Public Domain):: https://www.sqlite.org/copyright.html
Modfied: Yes
Distributed: Yes
Distribution: Binary
JSONModel https://github.com/jsonmodel/jsonmodel
License (MIT):: https://github.com/jsonmodel/jsonmodel/blob/master/LICENSEl
Modfied: Yes
Distributed: Yes
Distribution: Binary
murmur3 https://github.com/PeterScott/murmur3l
License (Public Domain):: https://github.com/PeterScott/murmur3l
Modfied: No
Distributed: Yes
Distribution: Binary
",14
rc9000/wiewarm-data,None,"wiewarm-data
About the Data
Full dataset of the wiewarm.ch website, containing many millions of temperature points from pools, rivers and lakes all over Switzerland.
The data is collected on the website http://www.wiewarm.ch either by manual entry or from sensors. Entries go back to 2001 when the website launched.
The temperature values are in degree celsius, and the file encoding is UTF-8 with ""Unix""-style line feeds. Date format is YYYY-MM-DD hh:mm:ss. The CSV of the current year will be updated daily.
License
You can freely use this data according to Creatvie Commons Attribution-ShareAlike 3.0. See the wiewarm.ch info page for instructions how to properly attribute the source.
",2
magroski/frogg,PHP,"Frogg




A library with some quality of life classes and syntactic sugar for some Phalcon Framework structures.
Authors:

@magroski
@tassogama

See Wiki for project info and documentation.
",3
devinit/D-Portal,JavaScript,"Issues
Please be aware that we use Github Issues for discussion and not every issue represents a bug.
If you have any questions about d-portal, please raise an issue. We very much welcome comments and feedback!

Search IATI data on development and humanitarian activities on specific events or themes more easily on d-portal.
Visit the site at http://d-portal.org/
We are currently in BETA. While d-portal is publicly available, it is not considered finished. You may experience bugs and missing features, but that is why we need your feedback and support.
Features

Explore IATI data for both countries and publishers.
Updates everyday GMT +0 with new data from the IATI Registry.
DStore is an
optimized nodejs + PostgreSQL + SQLite database for use in real time queries.
Q allows queries via simple but complex filters.
SAVi xml simplifies IATI xml to aid legibility for casual users.
Localization ready means adding translations of different languages a breeze.
Themeing options for customised versions of d-portal.
Chart.js for fully customisable graphs.
Generator allows easy embedding of IATI content in blog posts and websites.
Dash explores the gaps, highlights quality of data being published and displayed on d-portal.org
Easily create news posts using Markdown.
Install your own d-portal.
Sitemap of the site.
Open source with The MIT License.

Directory Structure
/dstore contains server side javascript for xml manipulation and
parsing of iati data.  See the readme in that directory for more
information. This is needed to run the Q queries on your own host.
/ctrack contains client side javascript and css for displaying
information direct from the datastore in browser. See the README in
that directory for more information. This is needed to build and
deploy a customized d-portal browser tool.
/dportal contains javascript that builds the static information and
example site you will find deployed at http://d-portal.org/
/bin contains helper scripts.
/documents contains some documentation.
Updates
d-portal.org is currently being developed and designed so things
are constantly changing. Thank you for your patience and understanding.
If you have a suggestion or feedback, or would just like to partake in discussions on d-portal, join in by creating a new issue here.
If you are interested in finding out more or part-funding d-portal, please contact support@iatistandard.org
Database Logs
We are tracking the nightly imports of IATI data being
published to d-portal.org here.
Service status on Uptime Robot.
",21
devinit/D-Portal,JavaScript,"Issues
Please be aware that we use Github Issues for discussion and not every issue represents a bug.
If you have any questions about d-portal, please raise an issue. We very much welcome comments and feedback!

Search IATI data on development and humanitarian activities on specific events or themes more easily on d-portal.
Visit the site at http://d-portal.org/
We are currently in BETA. While d-portal is publicly available, it is not considered finished. You may experience bugs and missing features, but that is why we need your feedback and support.
Features

Explore IATI data for both countries and publishers.
Updates everyday GMT +0 with new data from the IATI Registry.
DStore is an
optimized nodejs + PostgreSQL + SQLite database for use in real time queries.
Q allows queries via simple but complex filters.
SAVi xml simplifies IATI xml to aid legibility for casual users.
Localization ready means adding translations of different languages a breeze.
Themeing options for customised versions of d-portal.
Chart.js for fully customisable graphs.
Generator allows easy embedding of IATI content in blog posts and websites.
Dash explores the gaps, highlights quality of data being published and displayed on d-portal.org
Easily create news posts using Markdown.
Install your own d-portal.
Sitemap of the site.
Open source with The MIT License.

Directory Structure
/dstore contains server side javascript for xml manipulation and
parsing of iati data.  See the readme in that directory for more
information. This is needed to run the Q queries on your own host.
/ctrack contains client side javascript and css for displaying
information direct from the datastore in browser. See the README in
that directory for more information. This is needed to build and
deploy a customized d-portal browser tool.
/dportal contains javascript that builds the static information and
example site you will find deployed at http://d-portal.org/
/bin contains helper scripts.
/documents contains some documentation.
Updates
d-portal.org is currently being developed and designed so things
are constantly changing. Thank you for your patience and understanding.
If you have a suggestion or feedback, or would just like to partake in discussions on d-portal, join in by creating a new issue here.
If you are interested in finding out more or part-funding d-portal, please contact support@iatistandard.org
Database Logs
We are tracking the nightly imports of IATI data being
published to d-portal.org here.
Service status on Uptime Robot.
",21
botandrose/capybara-headless_chrome,Ruby,"Capybara::HeadlessChrome

A nice and tidy Capybara driver for headless Chrome. Even supports file downloads!
Usage
Capybara Setup
Just require ""capybara/headless_chrome"" somewhere in your test setup. This will register the :chrome driver, and make it  Capybara's default.
Options
If you want to change some of the options Chrome is started with, just reregister the driver:
Capybara.register_driver :chrome do |app|
  Capybara::HeadlessChrome::Driver.new(app, lang: ""es_MX"", headless: false, window_size: [1024,768])
end
Consult https://peter.sh/experiments/chromium-command-line-switches/ for a list of options. Although these are command-line options, conversion from a Ruby hash works as you would expect. E.g. headless: true works out to --headless, window_size: [1024,768] works out to --window-size=1024,768, etc.
Working with Downloaded Files
The Capybara session is extended with a single #downloads method that provides access to files downloaded during the session.
page.click_link ""Download Report""
page.downloads.filenames # => [""report.csv""]
page.downloads[""report.csv""] # => #<File:report.csv>
Note that the #[] method is wrapped with Capybara's synchronize, so it will keep trying to find the file for up to Capybara.default_max_wait_time seconds.
Be sure to run page.downloads.reset at the beginning of every test run to empty the downloaded files list.
If you're using Cucumber, you can require ""capybara/headless_chrome/cucumber"" somewhere in your cucumber configuration to set this up for you.
Installation
Add this to your application's Gemfile:
group :test do
  gem 'capybara-headless_chrome'
end
And then execute:
$ bundle

Or install it yourself as:
$ gem install capybara-headless_chrome

Development
After checking out the repo, run bin/setup to install dependencies. Then, run rake spec to run the tests. You can also run bin/console for an interactive prompt that will allow you to experiment.
To install this gem onto your local machine, run bundle exec rake install. To release a new version, update the version number in version.rb, and then run bundle exec rake release, which will create a git tag for the version, push git commits and tags, and push the .gem file to rubygems.org.
Contributing
Bug reports and pull requests are welcome on GitHub at https://github.com/botandrose/capybara-headless_chrome.
License
The gem is available as open source under the terms of the MIT License.
",30
Lombiq/Lombiq-Orchard-Visual-Studio-Extension,C#,"Lombiq Orchard Visual Studio Extension readme

Visual Studio extension with many features and templates frequently used by  Lombiq developers. Contains Orchard-related as well as generic goodies.
Check out the extension's Readme for more info (it's there and not in the root of the repository so it's also accessible from inside VS).
The project's logo was created by Ulises TJ.
",6
Azure/azure-iot-sdk-c,C,"Azure IoT C SDKs and Libraries

This repository contains the following:

Azure IoT Hub Device C SDK to connect devices running C code to Azure IoT Hub.
Azure IoT Hub Device Provisioning Service Client SDK for enrolling devices with Azure IoT Device Provisioning Services and managing enrollments lists.
Azure IoT Hub Service C SDK to interface with an Azure IoT Hub service instance from a server-side C application.
Serializer Library for C to help serialize and deserialize data on your device.

Packages and Libraries
The simplest way to get started with the Azure IoT SDKs is to use the following packages and libraries:

Linux: Device SDK on apt-get
mbed:                                      Device SDK library on MBED
Arduino:                                   Device SDK library in the Arduino IDE
Windows:                                   Device SDK on NuGet
iOS:                                       Device SDK on CocoaPod

Samples
Here are a set of simple samples that will help you get started:

Device SDK Samples
Service SDK Samples
Serializer Library Samples

Compile the SDK
When no package or library is available for your platform or if you want to modify the SDK code, or port the SDK to a new platform, then you can leverage the build environment provided in the repository.

Device SDK
Service SDK

SDK API Reference Documentation
The API reference documentation for the C SDKs can be found here.
Other Azure IoT SDKs
To find Azure IoT SDKs in other languages, please refer to the azure-iot-sdks repository.
Developing Azure IoT Applications
To learn more about building Azure IoT Applications, you can visit the Azure IoT Dev Center.
Key Features and Roadmap
Device Client SDK
✔️ feature available  ✖️ feature planned but not supported  ➖ no support planned



Features
mqtt
mqtt-ws
amqp
amqp-ws
https
Description




Authentication
✔️
✔️*
✔️
✔️*
✔️*
Connect your device to IoT Hub securely with supported authentication, including private key, SASToken, X-509 Self Signed and Certificate Authority (CA) Signed.  *IoT Hub only supports X-509 CA Signed over AMQP and MQTT at the moment.


Send device-to-cloud message
✔️*
✔️*
✔️*
✔️*
✔️*
Send device-to-cloud messages (max 256KB) to IoT Hub with the option to add custom properties.  IoT Hub only supports batch send over AMQP and HTTPS only at the moment.  This SDK supports batch send over HTTP.  * Batch send over AMQP and AMQP-WS, and add system properties on D2C messages are in progress.


Receive cloud-to-device messages
✔️*
✔️*
✔️
✔️
✔️
Receive cloud-to-device messages and read associated custom and system properties from IoT Hub, with the option to complete/reject/abandon C2D messages.  *IoT Hub supports the option to complete/reject/abandon C2D messages over HTTPS and AMQP only at the moment.


Device Twins
✔️*
✔️*
✔️*
✔️*
➖
IoT Hub persists a device twin for each device that you connect to IoT Hub.  The device can perform operations like get twin tags, subscribe to desired properties.  *Send reported properties version and desired properties version are in progress.


Direct Methods
✔️
✔️
✔️
✔️
➖
IoT Hub gives you the ability to invoke direct methods on devices from the cloud.  The SDK supports handler for method specific and generic operation.


Upload file to Blob
➖
➖
➖
➖
✔️
A device can initiate a file upload and notifies IoT Hub when the upload is complete.   File upload requires HTTPS connection, but can be initiated from client using any protocol for other operations.


Connection Status and Error reporting
✔️*
✔️*
✔️*
✔️*
✖️
Error reporting for IoT Hub supported error code.  *This SDK supports error reporting on authentication and Device Not Found.


Retry policies
✔️*
✔️*
✔️*
✔️*
✖️
Retry policy for unsuccessful device-to-cloud messages have two options: no try, exponential backoff with jitter (default).   *Custom retry policy is in progress.


Devices multiplexing over single connection
➖
➖
✔️
✔️
✔️



Connection Pooling - Specifying number of connections
➖
➖
✖️
✖️
✖️




This SDK also contains options you can set and platform specific features.  You can find detail list in this document.
Service Client SDK
✔️ feature available  ✖️ feature planned but not supported  ➖ no support planned



Features
C
Description




Identity registry (CRUD)
✔️*
Use your backend app to perform CRUD operation for individual device or in bulk.  This SDK supports CRUD operation on individual device with create device from ID/Key pair only.


Cloud-to-device messaging
✔️
Use your backend app to send cloud-to-device messages in AMQP and AMQP-WS, and set up cloud-to-device message receivers.


Direct Methods operations
✔️
Use your backend app to invoke direct method on device.


Device Twins operations
✔️*
Use your backend app to perform device twin operations.  *Twin reported property update callback and replace twin are in progress.


Query
✖️
Use your backend app to perform query for information.


Jobs
✖️
Use your backend app to perform job operation.


File Upload
✖️
Set up your backend app to send file upload notification receiver.



Provisioning client SDK
This repository contains provisioning client SDK for the Device Provisioning Service.
✔️ feature available  ✖️ feature planned but not supported  ➖ no support planned



Features
mqtt
mqtt-ws
amqp
amqp-ws
https
Description




TPM Individual Enrollment
➖
➖
✔️
✔️
✔️
This SDK supports connecting your device to the Device Provisioning Service via individual enrollment using Trusted Platform Module.  This quickstart reviews how to create a simulated device for individual enrollment with TPM. TPM over MQTT is currently not supported by the Device Provisioning Service.


X.509 Individual Enrollment
✔️
✔️
✔️
✔️
✔️
This SDK supports connecting your device to the Device Provisioning Service via individual enrollment using X.509 leaf certificate.  This quickstart reviews how to create a simulated device for individual enrollment with X.509.


X.509 Enrollment Group
✔️
✔️
✔️
✔️
✔️
This SDK supports connecting your device to the Device Provisioning Service via enrollment group using X.509 root certificate.



Provisioniong Service SDK
This repository contains provisioning service client SDK for the Device Provisioning Service to programmatically enroll devices.



Feature
Support
Description




CRUD Operation with TPM Individual Enrollment
✔️
Programmatically manage device enrollment using TPM with the service SDK.  Please visit the samples folder to learn more about this feature.


Bulk CRUD Operation with TPM Individual Enrollment
✔️
Programmatically bulk manage device enrollment using TPM with the service SDK.  Please visit the samples folder to learn more about this feature.


CRUD Operation with X.509 Individual Enrollment
✔️
Programmatically manage device enrollment using X.509 individual enrollment with the service SDK.  Please visit the samples folder to learn more about this feature.


CRUD Operation with X.509 Group Enrollment
✔️
Programmatically manage device enrollment using X.509 group enrollment with the service SDK.  Please visit the samples folder to learn more about this feature.


Query enrollments
✔️
Programmatically query registration states with the service SDK.  Please visit the samples folder to learn more about this feature.



OS Platforms and Hardware Compatibility
The IoT Hub device SDK for C can be used with a broad range of OS platforms and devices.
The minimum requirements are for the device platform to support the following:

Being capable of establishing an IP connection: only IP-capable devices can communicate directly with Azure IoT Hub.
Support TLS: required to establish a secure communication channel with Azure IoT Hub.
Support SHA-256 (optional): necessary to generate the secure token for authenticating the device with the service. Different authentication methods are available and not all require SHA-256.
Have a Real Time Clock or implement code to connect to an NTP server: necessary for both establishing the TLS connection and generating the secure token for authentication.
Having at least 64KB of RAM: the memory footprint of the SDK depends on the SDK and protocol used as well as the platform targeted. The smallest footprint is achieved targeting microcontrollers.
Having at least 4KB of RAM set for incoming SSL max content length buffer: For some TLS libraries, this may be a configurable option and default may have been set as 4KB for low memory footprint devices. During TLS handshake, IoT Hub service will send Server Hello which includes IoT Hub server side certificates as part of Server Hello payload.
During renewal of these IoT Hub server side certificates, check will be made on IoT Hub service side to prevent Server Hello exceeding 4KB limit so that existing devices which are set for 4KB limit continue to work as before after certificate renewals.

Platform support details can be found in this document.
You can find an exhaustive list of the OS platforms the various SDKs have been tested against in the Azure Certified for IoT device catalog. Note that you might still be able to use the SDKs on OS and hardware platforms that are not listed on this page: all the SDKs are open sourced and designed to be portable. If you have suggestions, feedback or issues to report, refer to the Contribution and Support sections below.
Porting the Azure IoT Device Client SDK for C to New Devices
The C SDKs and Libraries:

Are written in ANSI C (C99) and avoids compiler extensions to maximize code portability and broad platform compatibility.
Expose a platform abstraction layer to isolate OS dependencies (threading and mutual exclusion mechanisms, communications protocol e.g. HTTP). Refer to our porting guide for more information about our abstraction layer.

In the repository you will find instructions and build tools to compile and run the device client SDK for C on Linux, Windows and microcontroller platforms (refer to the links above for more information on compiling the device client for C).
If you are considering porting the device client SDK for C to a new platform, check out the porting guide document.
Contribution, Feedback and Issues
If you encounter any bugs, have suggestions for new features or if you would like to become an active contributor to this project please follow the instructions provided in the contribution guidelines.
Support

Have a feature request for SDKs? Please post it on User Voice to help us prioritize.
Have a technical question? Ask on Stack Overflow with tag ""azure-iot-hub"".
Need Support? Every customer with an active Azure subscription has access to support with guaranteed response time.  Consider submitting a ticket and get assistance from Microsoft support team
Found a bug? Please help us fix it by thoroughly documenting it and filing an issue on GitHub (C, Java, .NET, Node.js, Python).

Read more

Azure IoT Hub documentation
Prepare your development environment to use the Azure IoT device SDK for C
Setup IoT Hub
Azure IoT device SDK for C tutorial
How to port the C libraries to other OS platforms
Cross compilation example
C SDKs API reference

SDK Folder Structure
/c-utility, /uamqp, /umqtt, /parson
These are git submodules that contain code, such as adapters and protocol implementations, shared with other projects. Note that some of them contain nested submodules.
/blob
This folder contains client components that enable access to Azure blob storage.
/dps_client
This folder contains client library for device provisioning service.
/certs
Contains certificates needed to communicate with Azure IoT Hub.
/doc
This folder contains application development guides and device setup instructions.
/build_all
This folder contains platform-specific build scripts for the client libraries and dependent components.
/iothub_client
Contains Azure IoT Hub client components that provide the raw messaging capabilities of the library. Refer to the API documentation and samples for information on how to use it.

build: has one subfolder for each platform (e.g. Windows, Linux, Mbed). Contains makefiles, batch files, and solutions that are used to generate the library binaries.
devdoc: contains requirements, designs notes, manuals.
inc: public include files.
src: client libraries source files.
samples: contains simple samples.
tests: unit and end-to-end tests for source code.

/serializer
Contains libraries that provide modeling and JSON serialization capabilities on top of the raw messaging library. These libraries facilitate uploading structured data and command and control for use with Azure IoT services. Refer to the API documentation and samples for information on how to use it.

build: has one subfolder for each platform (e.g. Windows, Linux, Mbed). Contains makefiles, batch files, and solutions that are used to generate the library binaries.
devdoc: contains requirements, designs notes, manuals.
inc: public include files.
src: client libraries source files.
samples: contains simple samples.
tests: unit tests and end-to-end tests for source code.

/iothub_service_client
Contains libraries that enable interactions with the IoT Hub service to perform operations such as sending messages to devices and managing the device identity registry.
/testtools
Contains tools that are currently used in testing the client libraries: Mocking Framework (micromock), Generic Test Runner (CTest), Unit Test Project Template, etc.
/tools
Miscellaneous tools: compilembed, mbed_build, traceabilitytool (checks spec requirements vs code implementation).
Long Term Support
The project offers a Long Term Support (LTS) version to allow users that do not need the latest features to be shielded from unwanted changes.
A new LTS version will be created every 6 months. The lifetime of an LTS branch is currently planned for one year. LTS branches receive all bug fixes that fall in one of these categories:

security bugfixes
critical bugfixes

No new features will be picked up in an LTS branch.
LTS branches are named lts_mm_yyyy, where mm and yyyy are the month and year when the branch was created. An example of such a branch is lts_07_2017.
Schedule1
Below is a table showing the mapping of the LTS branches to the packages released



Package
Github Branch
LTS Status
LTS Start Date
Maintenance End Date
Removed Date




Vcpkg: 1.2.14-1 Xenial: 0.2.0.0-16xenial Trusty: 0.2.0-16trusty Bionic: 0.2.0.0-9bionic
lts_01_2019
Active
2019-01-31
2020-01-31
2020-01-31


Nuget: 1.2.10 Xenial: 0.2.0.0-12xenial Trusty: 0.2.0-12trusty Bionic: 0.2.0.0-5bionic
lts_10_2018
Active
2018-10-03
2019-10-03
2019-10-03




1 All scheduled dates are subject to change by the Azure IoT SDK team.

Planned Release Schedule


This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.
Microsoft collects performance and usage information which may be used to provide and improve Microsoft products and services and enhance your experience.  To learn more, review the privacy statement.
",248
dtag-dev-sec/listbot,Shell,"listbot
listbot
",10
mscharley/dotfiles,Shell,"Dotfiles
Matthew Scharley <matt@scharley.me>
Installation (Docker)
Want to take this repo for a test run in a project?
docker run -it --rm -v $(pwd):/home/ubuntu/$(basename `pwd`) mscharley/dotfiles

Installation (*nix/OS X)
git clone git://github.com/mscharley/dotfiles ~/.dotfiles
cd ~/.dotfiles
./install.pl

Installation (Windows)
Still not automated. For now, copy the PowerShell profile to $profile in PowerShell
and make sure your execution policy allows for running it.
",7
joshmarinacci/general-editor,JavaScript,"__
General Ed is not a visual authoring tool for general use. It's a platform for us to build
some of our own tools. It is a structured document editor. It should not leave Mozilla because
that would set unrealistic expectations.
currently implemented demo editors:

simple SVG editor
simple 2d hypercard
family tree builder
simple 3D hypercard
boxes and lines editor for simple WebAudio

Doc server
General Ed comes with a document server. This server is small and generic, providing
the bare minimum to save and load documents. You can post and get JSON documents from the
store by ID. There is no way to list documents. You must know the ID of the doc you are looking
for.  There are now accounts. Everything is public.  You can also store and load images. In this
case the filetype is preserved, but the filename is not.
DocumentProvider
The core of every editor is a document provider (currently named TreeItemProviderInterface).
This is an object which provides information about the current document to the rest of the
system. It contains the actual document model and exposes this model through methods like
findNodeById(), findParent(), hasChildren(), etc.   Virtually every standard component of
the editor will access this document provider.
Document providers should extend the TreeItemProviderInterface, but most of them will
extend the TreeItemProvider which contains default implementations for many functions, making
it easier to get started.  This default ipmlementation assumes that there is a root
node at this.root, that all children are stored in an array called node.children, that
every node has an id at node.id. It automatically tracks expanded/collapsed state, maintains
a lookup table of nodes by id, and manages event listeners. Every example application extends
this default TreeItemProvider so that the real provider can just worry about domain specific
tasks.
#Property Sheet
GeneralEd includes a sophisticated property sheet which can display and edit the properties of
any object. It includes built in editors for strings, numbers, booleans, colors, and enums.
A key responsibility of the DocumentProvider is giving the property sheet information about the
properties of the currently selected object. The provider must have a method called getProperties
which accepts an object. The provider should inspect this object for properties (often just calling
Object.keys()) then return an array of property definitions.  A property def is just a JSON
object with certain attributions describing the property.  For example, if the object had
two properties called name and age then getProperties would return a array like this:
[
    {
        key:'name',   //the actual property key
        name:'Name',  // a text description of the property, to display to the user
        type:'string', //indicates this is a string
    },
    {
        key:'age',
        name:""Item's Age"",
        type:'number',
    }
]

A property definition must include the key and name, but may optionally include many other
attributes of the property.  Common attributes include:

type:  one of number, string, boolean, enum or anything else you want.
locked: if true, then the property value will be displayed but not be editable.
custom: if true then the property sheet will ask for a custom editor for this property
live: normally properties are only updated on the actual object once the user does a commit action,
such as leaving the field or pressing enter. However, if live is true then it will update on
every change. For example, pressing the up arrow in a number editor will increment the
number but only update when the user leaves the field. If live is true then the object
will update on every up arrow press.
value: represents the current value of the property. this should be removed and obtained another way.
hints: an object with extra information about how the property should be edited. This is specific to each type.

Property types:
string The default type is 'string'. Strings will be edited with a one line text field.
If you want multiple lines (a textarea) then set propdef.hints.multiline to true.
number The type number is for both integers and floating point values.
It will use the standard HTML 5 number editor, which is usually a text
box with arrows to increment and decrement the value.  You can also use the
up and down arrow keys. Holding down the shift key will increment by larger values.
The default increment value is 1. To use a different increment use incrementValue
and incrementLargeValue hints.  To set a minimum or maximum value use min and max hints.
If the user types in a non-number value then the value will be ignored and the box
put into an error state.
boolean is edited with a checkbox.
color colors are edited with a popup color chooser
array  allows the user to edit a growing list of values with the type specified
by the propdef.valueDef property. This is really buggy and probably shouldn't be used yet. It could
be used for things like adding tags to a blog, where the tags are a set of values from the provider. ex:
propdef.valueDef = {type:'enum'}
enum  enums are a value with a set number of values.  If you set the type to enum then
the provider must also implement getValuesForEnum which returns an array of values for
the enum.  The provider may also optionally implement getRendererForEnum which
returns a React component to render the value. This is useful if your enum values are
integer code values but you want show nicely formatted titles to the user.
Here is an example of using an enum to select between three stroke styles in the SVG editor.
    const STROKE_STYLES = ['solid','dotted','dashed']
    ...
    getValuesForEnum(key) {
        if(key === 'strokeStyle') return STROKE_STYLES
        ...
    }

Here is an example of using an enum to choose the (human) parent of person
in the family tree. Renderers used for enums will be allocated by the PropSheet
as needed. The renderer will receive a props containing provider and value.
    getValuesForEnum(key,obj) {
        if(key === 'parents') return this.root.children.map((ch)=>ch.id)
    }
    getRendererForEnum(key,obj) {
        if(key === 'parents') return IdToNameRenderer;
    }

    ...

    const IdToNameRenderer = (props) => {
        let value = ""---""
        if(props.value && props.provider) value = props.provider.findPersonById(props.value).name
        return <b>{value}</b>
    }

Multiple Selections
You may select an object by calling SelectionManager.setSelection(object). This will always
replace the current selection. To instead add an object so that multiple objects are selected, use
Selectionmanager.addToSelection(object).  To retrieve the selected object use getSelection().
If multiple items are selected then getSelection() will only return the first object. To get the
full selection of all objects use getFullSelection().
The property sheet will use getFullSelection. If multiple items are selected then updating a property
will set that property for all objects in the selection. If the selected items have a different set of
properties then only properties available on all items will be editable (the intersection of the property sets).
",5
Jonty/uk_petitions_data,None,"UK Petitions Data
This is a complete dump of the UK Government and Parliament Petitions website data, updated every 30 minutes. It includes geographical voting data.
This also includes a complete machine-readable dump of the old petitions.number10.gov.uk website that started it all.
This data is published under the OGL.
Current State
Done:

A complete dump of the full petitions data for every public petition for every government
A complete dump of petitions.number10.gov.uk converted into the same format as the modern petitions website
Automatic hourly updates by a CircleCi job
Vote counts by country and constituency

Not done:

More petition sources
Automatically building an sqlite DB
A nice interface to search/browse the data

Pull requests to fix things welcome.
",4
SnailTowardThesun/rtmp_server,C++,"RTMP SERVER

compile
cd trunk
cmake . && make

",3
Azure/azure-iot-sdk-c,C,"Azure IoT C SDKs and Libraries

This repository contains the following:

Azure IoT Hub Device C SDK to connect devices running C code to Azure IoT Hub.
Azure IoT Hub Device Provisioning Service Client SDK for enrolling devices with Azure IoT Device Provisioning Services and managing enrollments lists.
Azure IoT Hub Service C SDK to interface with an Azure IoT Hub service instance from a server-side C application.
Serializer Library for C to help serialize and deserialize data on your device.

Packages and Libraries
The simplest way to get started with the Azure IoT SDKs is to use the following packages and libraries:

Linux: Device SDK on apt-get
mbed:                                      Device SDK library on MBED
Arduino:                                   Device SDK library in the Arduino IDE
Windows:                                   Device SDK on NuGet
iOS:                                       Device SDK on CocoaPod

Samples
Here are a set of simple samples that will help you get started:

Device SDK Samples
Service SDK Samples
Serializer Library Samples

Compile the SDK
When no package or library is available for your platform or if you want to modify the SDK code, or port the SDK to a new platform, then you can leverage the build environment provided in the repository.

Device SDK
Service SDK

SDK API Reference Documentation
The API reference documentation for the C SDKs can be found here.
Other Azure IoT SDKs
To find Azure IoT SDKs in other languages, please refer to the azure-iot-sdks repository.
Developing Azure IoT Applications
To learn more about building Azure IoT Applications, you can visit the Azure IoT Dev Center.
Key Features and Roadmap
Device Client SDK
✔️ feature available  ✖️ feature planned but not supported  ➖ no support planned



Features
mqtt
mqtt-ws
amqp
amqp-ws
https
Description




Authentication
✔️
✔️*
✔️
✔️*
✔️*
Connect your device to IoT Hub securely with supported authentication, including private key, SASToken, X-509 Self Signed and Certificate Authority (CA) Signed.  *IoT Hub only supports X-509 CA Signed over AMQP and MQTT at the moment.


Send device-to-cloud message
✔️*
✔️*
✔️*
✔️*
✔️*
Send device-to-cloud messages (max 256KB) to IoT Hub with the option to add custom properties.  IoT Hub only supports batch send over AMQP and HTTPS only at the moment.  This SDK supports batch send over HTTP.  * Batch send over AMQP and AMQP-WS, and add system properties on D2C messages are in progress.


Receive cloud-to-device messages
✔️*
✔️*
✔️
✔️
✔️
Receive cloud-to-device messages and read associated custom and system properties from IoT Hub, with the option to complete/reject/abandon C2D messages.  *IoT Hub supports the option to complete/reject/abandon C2D messages over HTTPS and AMQP only at the moment.


Device Twins
✔️*
✔️*
✔️*
✔️*
➖
IoT Hub persists a device twin for each device that you connect to IoT Hub.  The device can perform operations like get twin tags, subscribe to desired properties.  *Send reported properties version and desired properties version are in progress.


Direct Methods
✔️
✔️
✔️
✔️
➖
IoT Hub gives you the ability to invoke direct methods on devices from the cloud.  The SDK supports handler for method specific and generic operation.


Upload file to Blob
➖
➖
➖
➖
✔️
A device can initiate a file upload and notifies IoT Hub when the upload is complete.   File upload requires HTTPS connection, but can be initiated from client using any protocol for other operations.


Connection Status and Error reporting
✔️*
✔️*
✔️*
✔️*
✖️
Error reporting for IoT Hub supported error code.  *This SDK supports error reporting on authentication and Device Not Found.


Retry policies
✔️*
✔️*
✔️*
✔️*
✖️
Retry policy for unsuccessful device-to-cloud messages have two options: no try, exponential backoff with jitter (default).   *Custom retry policy is in progress.


Devices multiplexing over single connection
➖
➖
✔️
✔️
✔️



Connection Pooling - Specifying number of connections
➖
➖
✖️
✖️
✖️




This SDK also contains options you can set and platform specific features.  You can find detail list in this document.
Service Client SDK
✔️ feature available  ✖️ feature planned but not supported  ➖ no support planned



Features
C
Description




Identity registry (CRUD)
✔️*
Use your backend app to perform CRUD operation for individual device or in bulk.  This SDK supports CRUD operation on individual device with create device from ID/Key pair only.


Cloud-to-device messaging
✔️
Use your backend app to send cloud-to-device messages in AMQP and AMQP-WS, and set up cloud-to-device message receivers.


Direct Methods operations
✔️
Use your backend app to invoke direct method on device.


Device Twins operations
✔️*
Use your backend app to perform device twin operations.  *Twin reported property update callback and replace twin are in progress.


Query
✖️
Use your backend app to perform query for information.


Jobs
✖️
Use your backend app to perform job operation.


File Upload
✖️
Set up your backend app to send file upload notification receiver.



Provisioning client SDK
This repository contains provisioning client SDK for the Device Provisioning Service.
✔️ feature available  ✖️ feature planned but not supported  ➖ no support planned



Features
mqtt
mqtt-ws
amqp
amqp-ws
https
Description




TPM Individual Enrollment
➖
➖
✔️
✔️
✔️
This SDK supports connecting your device to the Device Provisioning Service via individual enrollment using Trusted Platform Module.  This quickstart reviews how to create a simulated device for individual enrollment with TPM. TPM over MQTT is currently not supported by the Device Provisioning Service.


X.509 Individual Enrollment
✔️
✔️
✔️
✔️
✔️
This SDK supports connecting your device to the Device Provisioning Service via individual enrollment using X.509 leaf certificate.  This quickstart reviews how to create a simulated device for individual enrollment with X.509.


X.509 Enrollment Group
✔️
✔️
✔️
✔️
✔️
This SDK supports connecting your device to the Device Provisioning Service via enrollment group using X.509 root certificate.



Provisioniong Service SDK
This repository contains provisioning service client SDK for the Device Provisioning Service to programmatically enroll devices.



Feature
Support
Description




CRUD Operation with TPM Individual Enrollment
✔️
Programmatically manage device enrollment using TPM with the service SDK.  Please visit the samples folder to learn more about this feature.


Bulk CRUD Operation with TPM Individual Enrollment
✔️
Programmatically bulk manage device enrollment using TPM with the service SDK.  Please visit the samples folder to learn more about this feature.


CRUD Operation with X.509 Individual Enrollment
✔️
Programmatically manage device enrollment using X.509 individual enrollment with the service SDK.  Please visit the samples folder to learn more about this feature.


CRUD Operation with X.509 Group Enrollment
✔️
Programmatically manage device enrollment using X.509 group enrollment with the service SDK.  Please visit the samples folder to learn more about this feature.


Query enrollments
✔️
Programmatically query registration states with the service SDK.  Please visit the samples folder to learn more about this feature.



OS Platforms and Hardware Compatibility
The IoT Hub device SDK for C can be used with a broad range of OS platforms and devices.
The minimum requirements are for the device platform to support the following:

Being capable of establishing an IP connection: only IP-capable devices can communicate directly with Azure IoT Hub.
Support TLS: required to establish a secure communication channel with Azure IoT Hub.
Support SHA-256 (optional): necessary to generate the secure token for authenticating the device with the service. Different authentication methods are available and not all require SHA-256.
Have a Real Time Clock or implement code to connect to an NTP server: necessary for both establishing the TLS connection and generating the secure token for authentication.
Having at least 64KB of RAM: the memory footprint of the SDK depends on the SDK and protocol used as well as the platform targeted. The smallest footprint is achieved targeting microcontrollers.
Having at least 4KB of RAM set for incoming SSL max content length buffer: For some TLS libraries, this may be a configurable option and default may have been set as 4KB for low memory footprint devices. During TLS handshake, IoT Hub service will send Server Hello which includes IoT Hub server side certificates as part of Server Hello payload.
During renewal of these IoT Hub server side certificates, check will be made on IoT Hub service side to prevent Server Hello exceeding 4KB limit so that existing devices which are set for 4KB limit continue to work as before after certificate renewals.

Platform support details can be found in this document.
You can find an exhaustive list of the OS platforms the various SDKs have been tested against in the Azure Certified for IoT device catalog. Note that you might still be able to use the SDKs on OS and hardware platforms that are not listed on this page: all the SDKs are open sourced and designed to be portable. If you have suggestions, feedback or issues to report, refer to the Contribution and Support sections below.
Porting the Azure IoT Device Client SDK for C to New Devices
The C SDKs and Libraries:

Are written in ANSI C (C99) and avoids compiler extensions to maximize code portability and broad platform compatibility.
Expose a platform abstraction layer to isolate OS dependencies (threading and mutual exclusion mechanisms, communications protocol e.g. HTTP). Refer to our porting guide for more information about our abstraction layer.

In the repository you will find instructions and build tools to compile and run the device client SDK for C on Linux, Windows and microcontroller platforms (refer to the links above for more information on compiling the device client for C).
If you are considering porting the device client SDK for C to a new platform, check out the porting guide document.
Contribution, Feedback and Issues
If you encounter any bugs, have suggestions for new features or if you would like to become an active contributor to this project please follow the instructions provided in the contribution guidelines.
Support

Have a feature request for SDKs? Please post it on User Voice to help us prioritize.
Have a technical question? Ask on Stack Overflow with tag ""azure-iot-hub"".
Need Support? Every customer with an active Azure subscription has access to support with guaranteed response time.  Consider submitting a ticket and get assistance from Microsoft support team
Found a bug? Please help us fix it by thoroughly documenting it and filing an issue on GitHub (C, Java, .NET, Node.js, Python).

Read more

Azure IoT Hub documentation
Prepare your development environment to use the Azure IoT device SDK for C
Setup IoT Hub
Azure IoT device SDK for C tutorial
How to port the C libraries to other OS platforms
Cross compilation example
C SDKs API reference

SDK Folder Structure
/c-utility, /uamqp, /umqtt, /parson
These are git submodules that contain code, such as adapters and protocol implementations, shared with other projects. Note that some of them contain nested submodules.
/blob
This folder contains client components that enable access to Azure blob storage.
/dps_client
This folder contains client library for device provisioning service.
/certs
Contains certificates needed to communicate with Azure IoT Hub.
/doc
This folder contains application development guides and device setup instructions.
/build_all
This folder contains platform-specific build scripts for the client libraries and dependent components.
/iothub_client
Contains Azure IoT Hub client components that provide the raw messaging capabilities of the library. Refer to the API documentation and samples for information on how to use it.

build: has one subfolder for each platform (e.g. Windows, Linux, Mbed). Contains makefiles, batch files, and solutions that are used to generate the library binaries.
devdoc: contains requirements, designs notes, manuals.
inc: public include files.
src: client libraries source files.
samples: contains simple samples.
tests: unit and end-to-end tests for source code.

/serializer
Contains libraries that provide modeling and JSON serialization capabilities on top of the raw messaging library. These libraries facilitate uploading structured data and command and control for use with Azure IoT services. Refer to the API documentation and samples for information on how to use it.

build: has one subfolder for each platform (e.g. Windows, Linux, Mbed). Contains makefiles, batch files, and solutions that are used to generate the library binaries.
devdoc: contains requirements, designs notes, manuals.
inc: public include files.
src: client libraries source files.
samples: contains simple samples.
tests: unit tests and end-to-end tests for source code.

/iothub_service_client
Contains libraries that enable interactions with the IoT Hub service to perform operations such as sending messages to devices and managing the device identity registry.
/testtools
Contains tools that are currently used in testing the client libraries: Mocking Framework (micromock), Generic Test Runner (CTest), Unit Test Project Template, etc.
/tools
Miscellaneous tools: compilembed, mbed_build, traceabilitytool (checks spec requirements vs code implementation).
Long Term Support
The project offers a Long Term Support (LTS) version to allow users that do not need the latest features to be shielded from unwanted changes.
A new LTS version will be created every 6 months. The lifetime of an LTS branch is currently planned for one year. LTS branches receive all bug fixes that fall in one of these categories:

security bugfixes
critical bugfixes

No new features will be picked up in an LTS branch.
LTS branches are named lts_mm_yyyy, where mm and yyyy are the month and year when the branch was created. An example of such a branch is lts_07_2017.
Schedule1
Below is a table showing the mapping of the LTS branches to the packages released



Package
Github Branch
LTS Status
LTS Start Date
Maintenance End Date
Removed Date




Vcpkg: 1.2.14-1 Xenial: 0.2.0.0-16xenial Trusty: 0.2.0-16trusty Bionic: 0.2.0.0-9bionic
lts_01_2019
Active
2019-01-31
2020-01-31
2020-01-31


Nuget: 1.2.10 Xenial: 0.2.0.0-12xenial Trusty: 0.2.0-12trusty Bionic: 0.2.0.0-5bionic
lts_10_2018
Active
2018-10-03
2019-10-03
2019-10-03




1 All scheduled dates are subject to change by the Azure IoT SDK team.

Planned Release Schedule


This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.
Microsoft collects performance and usage information which may be used to provide and improve Microsoft products and services and enhance your experience.  To learn more, review the privacy statement.
",248
dtag-dev-sec/listbot,Shell,"listbot
listbot
",10
mscharley/dotfiles,Shell,"Dotfiles
Matthew Scharley <matt@scharley.me>
Installation (Docker)
Want to take this repo for a test run in a project?
docker run -it --rm -v $(pwd):/home/ubuntu/$(basename `pwd`) mscharley/dotfiles

Installation (*nix/OS X)
git clone git://github.com/mscharley/dotfiles ~/.dotfiles
cd ~/.dotfiles
./install.pl

Installation (Windows)
Still not automated. For now, copy the PowerShell profile to $profile in PowerShell
and make sure your execution policy allows for running it.
",7
joshmarinacci/general-editor,JavaScript,"__
General Ed is not a visual authoring tool for general use. It's a platform for us to build
some of our own tools. It is a structured document editor. It should not leave Mozilla because
that would set unrealistic expectations.
currently implemented demo editors:

simple SVG editor
simple 2d hypercard
family tree builder
simple 3D hypercard
boxes and lines editor for simple WebAudio

Doc server
General Ed comes with a document server. This server is small and generic, providing
the bare minimum to save and load documents. You can post and get JSON documents from the
store by ID. There is no way to list documents. You must know the ID of the doc you are looking
for.  There are now accounts. Everything is public.  You can also store and load images. In this
case the filetype is preserved, but the filename is not.
DocumentProvider
The core of every editor is a document provider (currently named TreeItemProviderInterface).
This is an object which provides information about the current document to the rest of the
system. It contains the actual document model and exposes this model through methods like
findNodeById(), findParent(), hasChildren(), etc.   Virtually every standard component of
the editor will access this document provider.
Document providers should extend the TreeItemProviderInterface, but most of them will
extend the TreeItemProvider which contains default implementations for many functions, making
it easier to get started.  This default ipmlementation assumes that there is a root
node at this.root, that all children are stored in an array called node.children, that
every node has an id at node.id. It automatically tracks expanded/collapsed state, maintains
a lookup table of nodes by id, and manages event listeners. Every example application extends
this default TreeItemProvider so that the real provider can just worry about domain specific
tasks.
#Property Sheet
GeneralEd includes a sophisticated property sheet which can display and edit the properties of
any object. It includes built in editors for strings, numbers, booleans, colors, and enums.
A key responsibility of the DocumentProvider is giving the property sheet information about the
properties of the currently selected object. The provider must have a method called getProperties
which accepts an object. The provider should inspect this object for properties (often just calling
Object.keys()) then return an array of property definitions.  A property def is just a JSON
object with certain attributions describing the property.  For example, if the object had
two properties called name and age then getProperties would return a array like this:
[
    {
        key:'name',   //the actual property key
        name:'Name',  // a text description of the property, to display to the user
        type:'string', //indicates this is a string
    },
    {
        key:'age',
        name:""Item's Age"",
        type:'number',
    }
]

A property definition must include the key and name, but may optionally include many other
attributes of the property.  Common attributes include:

type:  one of number, string, boolean, enum or anything else you want.
locked: if true, then the property value will be displayed but not be editable.
custom: if true then the property sheet will ask for a custom editor for this property
live: normally properties are only updated on the actual object once the user does a commit action,
such as leaving the field or pressing enter. However, if live is true then it will update on
every change. For example, pressing the up arrow in a number editor will increment the
number but only update when the user leaves the field. If live is true then the object
will update on every up arrow press.
value: represents the current value of the property. this should be removed and obtained another way.
hints: an object with extra information about how the property should be edited. This is specific to each type.

Property types:
string The default type is 'string'. Strings will be edited with a one line text field.
If you want multiple lines (a textarea) then set propdef.hints.multiline to true.
number The type number is for both integers and floating point values.
It will use the standard HTML 5 number editor, which is usually a text
box with arrows to increment and decrement the value.  You can also use the
up and down arrow keys. Holding down the shift key will increment by larger values.
The default increment value is 1. To use a different increment use incrementValue
and incrementLargeValue hints.  To set a minimum or maximum value use min and max hints.
If the user types in a non-number value then the value will be ignored and the box
put into an error state.
boolean is edited with a checkbox.
color colors are edited with a popup color chooser
array  allows the user to edit a growing list of values with the type specified
by the propdef.valueDef property. This is really buggy and probably shouldn't be used yet. It could
be used for things like adding tags to a blog, where the tags are a set of values from the provider. ex:
propdef.valueDef = {type:'enum'}
enum  enums are a value with a set number of values.  If you set the type to enum then
the provider must also implement getValuesForEnum which returns an array of values for
the enum.  The provider may also optionally implement getRendererForEnum which
returns a React component to render the value. This is useful if your enum values are
integer code values but you want show nicely formatted titles to the user.
Here is an example of using an enum to select between three stroke styles in the SVG editor.
    const STROKE_STYLES = ['solid','dotted','dashed']
    ...
    getValuesForEnum(key) {
        if(key === 'strokeStyle') return STROKE_STYLES
        ...
    }

Here is an example of using an enum to choose the (human) parent of person
in the family tree. Renderers used for enums will be allocated by the PropSheet
as needed. The renderer will receive a props containing provider and value.
    getValuesForEnum(key,obj) {
        if(key === 'parents') return this.root.children.map((ch)=>ch.id)
    }
    getRendererForEnum(key,obj) {
        if(key === 'parents') return IdToNameRenderer;
    }

    ...

    const IdToNameRenderer = (props) => {
        let value = ""---""
        if(props.value && props.provider) value = props.provider.findPersonById(props.value).name
        return <b>{value}</b>
    }

Multiple Selections
You may select an object by calling SelectionManager.setSelection(object). This will always
replace the current selection. To instead add an object so that multiple objects are selected, use
Selectionmanager.addToSelection(object).  To retrieve the selected object use getSelection().
If multiple items are selected then getSelection() will only return the first object. To get the
full selection of all objects use getFullSelection().
The property sheet will use getFullSelection. If multiple items are selected then updating a property
will set that property for all objects in the selection. If the selected items have a different set of
properties then only properties available on all items will be editable (the intersection of the property sets).
",5
Jonty/uk_petitions_data,None,"UK Petitions Data
This is a complete dump of the UK Government and Parliament Petitions website data, updated every 30 minutes. It includes geographical voting data.
This also includes a complete machine-readable dump of the old petitions.number10.gov.uk website that started it all.
This data is published under the OGL.
Current State
Done:

A complete dump of the full petitions data for every public petition for every government
A complete dump of petitions.number10.gov.uk converted into the same format as the modern petitions website
Automatic hourly updates by a CircleCi job
Vote counts by country and constituency

Not done:

More petition sources
Automatically building an sqlite DB
A nice interface to search/browse the data

Pull requests to fix things welcome.
",4
SnailTowardThesun/rtmp_server,C++,"RTMP SERVER

compile
cd trunk
cmake . && make

",3
electron/apps,JavaScript,"electron-apps 
A collection of apps built on Electron. electronjs.org/apps.
Adding your app
If you have an Electron application you'd like to see added,
please read the contributing doc.
How it Works
See contributing.md#how-it-works
License
MIT
",837
uccser/cs-field-guide,JavaScript,"Computer Science Field Guide

The Computer Science Field Guide (CSFG) is an online interactive resource for high school students learning about computer science, developed at the University of Canterbury in New Zealand.
The latest HTML release of the Computer Science Field Guide can be viewed at www.csfieldguide.org.nz.
This repository aims to be the source for all data associated with the CSFG, and also allows users to suggest improvements or create their own version.
Project Philosophy
The CSFG aims to be an document used for teaching Computer Science all over the world in many different languages.
After using an internal system for creating the guide (from 2012 to 2015), we have moved to a custom open source system.
All areas of the project, from chapter text to website design, are now available for all.
We want this project to be as accessible as possible to our many user groups, which includes students, teachers and educators, and developers.
Documentation
Documentation for this project can be found on
ReadTheDocs,
and can also be built from the documentation source within the docs/ directory.
Contributing
We would love your help to make this guide the best it can be!
Please read our
contribution guide
to get started.
License
The content of this project itself is licensed under the
Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license
(LICENCE-CONTENT file).
This license applies to the following contents of this project:

Markdown files located within the csfieldguide/chapters/content/ directory.
Images located within the csfieldguide/static/img/ directory.

Third-party libraries used in this project have their license's
listed within the LICENCE-THIRD-PARTY file, with a full copy of the license
available within the third-party-licences directory.
If a source file of a third-party library or system is stored within this
repository, then a copyright notice should be present at the top of the file.
The rest of the project, which is the underlying source code used to manage
and display this content, is licensed under the
MIT license (LICENCE file).
Contact
You can contact us at csse-education-research@canterbury.ac.nz.
",227
rust-lang/rust-repos,Rust,"Rust repositories list
This repository contains a list of all the public GitHub repos with source code
written in the Rust programming language. This repositories contains
both the source code for the scraper and the scraped list.
Everything in this repository, unless otherwise specified, is released under
the MIT license.
Running the scraper
To run the scraper, execute the program with the GITHUB_TOKEN environment
variable (containing a valid GitHub API token -- no permissions are required)
and the data directory as the first argument:
$ GITHUB_TOKEN=foobar cargo run --release -- data

The scraper automatically saves its state to disk, so it can be interrupted and
it will resume where it left. This also allows incremental updates of the list.
Using the data
The data is available in the data/github.csv file, in CSV format. That file
contains the GitHub GraphQL ID of the repository, its name, and whether it
contains a Cargo.toml and Cargo.lock.
All the repositories contained in the dataset are marked as using the language
by GitHub. Some results might be inaccurate for this reason.
",12
lambdalisue/dotfiles,Vim script,"dotfiles
Requirements


rhysd/dotfiles
go get github.com/rhysd/dotfiles


Usage
$ cd ~
$ dotfiles clone lambdalisue

",7
rust-lang/rust-repos,Rust,"Rust repositories list
This repository contains a list of all the public GitHub repos with source code
written in the Rust programming language. This repositories contains
both the source code for the scraper and the scraped list.
Everything in this repository, unless otherwise specified, is released under
the MIT license.
Running the scraper
To run the scraper, execute the program with the GITHUB_TOKEN environment
variable (containing a valid GitHub API token -- no permissions are required)
and the data directory as the first argument:
$ GITHUB_TOKEN=foobar cargo run --release -- data

The scraper automatically saves its state to disk, so it can be interrupted and
it will resume where it left. This also allows incremental updates of the list.
Using the data
The data is available in the data/github.csv file, in CSV format. That file
contains the GitHub GraphQL ID of the repository, its name, and whether it
contains a Cargo.toml and Cargo.lock.
All the repositories contained in the dataset are marked as using the language
by GitHub. Some results might be inaccurate for this reason.
",12
lambdalisue/dotfiles,Vim script,"dotfiles
Requirements


rhysd/dotfiles
go get github.com/rhysd/dotfiles


Usage
$ cd ~
$ dotfiles clone lambdalisue

",7
haskell-works/hw-json,Haskell,"hw-json

hw-json is a succinct JSON parsing library.
It uses succinct data-structures to allow traversal of large JSON strings with minimal memory overhead.
For an example, see app/Main.hs
Prerequisites

cabal version 2.2 or later

Memory benchmark
Parsing large Json files in Scala with Argonaut
      S0U       EU           OU       MU     CCSU CMD
--------- --------- ----------- -------- -------- ---------------------------------------------------------------
      0.0  80,526.3    76,163.6 72,338.6 13,058.6 sbt console
      0.0 536,660.4    76,163.6 72,338.6 13,058.6 import java.io._, argonaut._, Argonaut._
      0.0 552,389.1    76,163.6 72,338.6 13,058.6 val file = new File(""/Users/jky/Downloads/78mbs.json""
      0.0 634,066.5    76,163.6 72,338.6 13,058.6 val array = new Array[Byte](file.length.asInstanceOf[Int])
      0.0 644,552.3    76,163.6 72,338.6 13,058.6 val is = new FileInputStream(""/Users/jky/Downloads/78mbs.json"")
      0.0 655,038.1    76,163.6 72,338.6 13,058.6 is.read(array)
294,976.0 160,159.7 1,100,365.0 79,310.8 13,748.1 val json = new String(array)
285,182.9 146,392.6 1,956,264.5 82,679.8 14,099.6 val data = Parse.parse(json)
                    ***********

Parsing large Json files in Haskell with Aeson
-- CMD                                                     -- Mem (MB)
---------------------------------------------------------- -- --------
import Control.DeepSeq                                     --       94
import Data.Aeson                                          --      100
import qualified Data.ByteString.Lazy as BSL               --      104
bs <- BSL.readFile ""../corpus/bench/hospitalisation.json""  --      105
let !x = deepseq bs bs                                     --      146
let !y = decode json78m :: Maybe Value                     --      669
Parsing large Json files in Haskell with hw-json
-- CMD                                                                -- Mem (MB)
--------------------------------------------------------------------- -- --------
import Foreign                                                        --       93
import Control.Monad                                                  --       95
import Data.Word                                                      --       96
import HaskellWorks.Data.BalancedParens.Simple                        --       97
import HaskellWorks.Data.Bits.BitShown                                --       98
import HaskellWorks.Data.FromForeignRegion                            --       99
import HaskellWorks.Data.Json.Backend.Standard.Cursor                 --      106
import System.IO.MMap                                                 --      109
import qualified Data.ByteString                              as BS   --      110
import qualified Data.Vector.Storable                         as DVS  --      111
import qualified HaskellWorks.Data.ByteString                 as BS   --      112
import qualified HaskellWorks.Data.Json.Backend.Standard.Fast as FAST --      114
bs <- BS.mmap ""../corpus/bench/hospitalisation.json""                  --      115
let !cursor = FAST.makeCursor bs                                      --      203
Examples
Navigation example
import Control.Monad
import Data.String
import Data.Word
import HaskellWorks.Data.BalancedParens.Simple
import HaskellWorks.Data.Bits.BitShow
import HaskellWorks.Data.Bits.BitShown
import HaskellWorks.Data.FromForeignRegion
import HaskellWorks.Data.Json.Backend.Standard.Cursor
import HaskellWorks.Data.Json.Internal.Token.Types
import HaskellWorks.Data.RankSelect.Base.Rank0
import HaskellWorks.Data.RankSelect.Base.Rank1
import HaskellWorks.Data.RankSelect.Base.Select1
import HaskellWorks.Data.RankSelect.Poppy512
import System.IO.MMap

import qualified Data.ByteString                                as BS
import qualified Data.Vector.Storable                           as DVS
import qualified HaskellWorks.Data.Json.Backend.Standard.Cursor as C
import qualified HaskellWorks.Data.Json.Backend.Standard.Fast   as FAST
import qualified HaskellWorks.Data.TreeCursor                   as TC

let fc = TC.firstChild
let ns = TC.nextSibling
let pn = TC.parent
let ss = TC.subtreeSize
let cursor = FAST.makeCursor ""[null, {\""field\"": 1}]""
cursor
fc cursor
(fc >=> ns) cursor
Querying example
import Control.Monad
import Data.Function
import Data.List
import HaskellWorks.Data.Json.Backend.Standard.Load.Cursor
import HaskellWorks.Data.Json.Backend.Standard.Load.Partial
import HaskellWorks.Data.Json.Backend.Standard.Load.Raw
import HaskellWorks.Data.Json.PartialValue
import HaskellWorks.Data.MQuery
import HaskellWorks.Data.MQuery.Micro
import HaskellWorks.Data.MQuery.Row

import qualified Data.DList as DL

!cursor <- loadPartial ""../corpus/bench/78mb.json""
!cursor <- loadCursorWithIndex ""../corpus/bench/78mb.json""
!cursor <- loadCursor ""../corpus/bench/78mb.json""
!cursor <- loadCursorWithCsPoppyIndex ""../corpus/bench/78mb.json""
let !json = jsonPartialJsonValueAt cursor
let q = MQuery (DL.singleton json)

putPretty $ q >>= item & limit 10
putPretty $ q >>= item & page 10 1
putPretty $ q >>= item >>= hasKV ""founded_year"" (JsonPartialNumber 2005) & limit 10
putPretty $ q >>= item >>= entry
putPretty $ q >>= item >>= entry >>= named ""name"" & limit 10
putPretty $ q >>= (item >=> entry >=> named ""acquisition"" >=> entry >=> named ""price_currency_code"")
putPretty $ q >>= (item >=> entry >=> named ""acquisition"" >=> entry >=> named ""price_currency_code"") & onList (uniq . sort)
putPretty $ q >>= (item >=> entry >=> named ""acquisition"" >=> entry >=> named ""price_currency_code"" >=> asString >=> valueOf ""USD"") & limit 10
putPretty $ q >>= (item >=> entry >=> named ""acquisition"" >=> having (entry >=> named ""price_currency_code"" >=> asString >=> valueOf ""USD"") >=> entry >=> named ""price_amount"") & limit 10
putPretty $ q >>= (item >=> entry >=> named ""acquisition"" >=> having (entry >=> named ""price_currency_code"" >=> asString >=> valueOf ""USD"") >=> entry >=> named ""price_amount"" >=> castAsInteger ) & limit 10
putPretty $ q >>= (item >=> entry >=> named ""acquisition"" >=> having (entry >=> named ""price_currency_code"" >=> asString >=> valueOf ""USD"") >=> entry >=> named ""price_amount"" >=> castAsInteger ) & aggregate sum

putPretty $ q >>= item & limit 10
putPretty $ q >>= item & page 10 1
putPretty $ q >>= item >>= entry
putPretty $ q >>= item >>= entry >>= named ""name"" & limit 10
putPretty $ q >>= (item >=> entry >=> named ""acquisition"" >=> entry >=> named ""price_currency_code"" >=> asString)
putPretty $ q >>= (item >=> entry >=> named ""acquisition"" >=> entry >=> named ""price_currency_code"" >=> asString) & onList (uniq . sort)
putPretty $ q >>= (item >=> entry >=> named ""acquisition"" >=> entry >=> named ""price_currency_code"" >=> asString >=> valueOf ""USD"") & limit 10
putPretty $ q >>= (item >=> entry >=> named ""acquisition"" >=> having (entry >=> named ""price_currency_code"" >=> asString >=> valueOf ""USD"") >=> entry >=> named ""price_amount"") & limit 10
putPretty $ q >>= (item >=> entry >=> named ""acquisition"" >=> having (entry >=> named ""price_currency_code"" >=> asString >=> valueOf ""USD"") >=> entry >=> named ""price_amount"" >=> castAsInteger ) & limit 10
putPretty $ q >>= (item >=> entry >=> named ""acquisition"" >=> having (entry >=> named ""price_currency_code"" >=> asString >=> valueOf ""USD"") >=> entry >=> named ""price_amount"" >=> castAsInteger ) & aggregate sum
References

Semi-Indexing Semi-Structured Data in Tiny Space
Succinct Data Structures talk by Edward Kmett
Typed Tagless Final Interpreters

Special mentions

Sydney Paper Club

",36
Lombiq/Orchard-Azure-Application-Insights,C#,"Hosting - Azure Application Insights Readme
This Orchard CMS module enables easy integration of Azure Application Insights telemetry into Orchard. Just install the module, configure the instrumentation key from the admin and collected data will start appearing in Application Insights. The module is tenant-aware, so in a multi-tenant setup you can configure different instrumentation keys to collect request tracking and client-side tracking data on different tenants. This is also available on all sites of DotNest, the Orchard CMS as a Service.
Warning: this module is only compatible with the Orchard 1.9+.
Note that the module depends on the Helpful Libraries module so you should have that installed as well.
Hosting - Azure Application Insights is part of the Hosting Suite, which is a complete Orchard DevOps technology suite for building and maintaining scalable Orchard applications.
The module was created by Lombiq, one of the core developers of Orchard itself.
Configuration
You can configure the module, including setting the AI instrumentation key from the admin site, for each tenant. You can also set an application-wide instrumentation key to be used by all tenants (if the module is enabled) in the static configuration (i.e. Web.config, Azure Portal) with the key shown in the Constants class.
To collect detailed dependency data and server resource information you'll need to install the AI Status Monitor (for VMs and local development) or the Application Insights site extension for Azure Web Apps. Be aware that both tools will add DLLs to the app and create an ApplicationInsights.config file, but neither are needed. To fix this remove the config file and re-deploy (or locally: delete Orchard.Web/bin and App_Data/Dependencies and re-build) the app.
Using the AI site extension is currently not supported, the extension needs to be modified.
Extending the module with custom telemetry data
You can send custom events (i.e. totally new events like a user action happening) through a TelemetryClient object (you can see examples for this in the official AI documentation). You can create such an object for the current configuration (i.e. what is also used to send request telemetry) through
ITelemetryClientFactory.CreateTelemetryClientFromCurrentConfiguration().
You can also hook into the default behaviour of the module and e.g. extend what is send during request tracking by implementing the module's event handlers, see the
Events folder/namespace. Particularly you can implement IRequestTrackingEventHandler to add custom data to the request telemetry e.g. by adding custom properties to the Properties dictionary. Furthermore you can implement ITelemetryConfigurationEventHandler to alter the configuration used by any telemetry-sending operation like adding your own ITelemetryInitializers to the TelemetryInitializers collection.
Note on assembly binding errors when using dynamic compilation
Note that when you modify this one or a dependent project in the Orchard solution and then refresh a page without doing a manual rebuild (i.e. letting Orchard's dynamic compilation do the job) you can get the following error:

Could not load file or assembly 'Microsoft.Diagnostics.Tracing.EventSource, Version=1.1.11.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a' or one of its dependencies. The located assembly's manifest definition does not match the assembly reference.

This is because on the fly assembly redirection (see below) doesn't work for some reason in such cases. To solve the issue simply restart the website (from IIS or by restarting IIS Express) after doing a manual build.
Updating AI libraries
When assembly binding redirects are changed make sure to also edit AssemblyRedirectSetupShellEventHandler that mimicks such redirects instead of relying on the Web.config.
Note that since there is a 260 characters limit on paths on Windows, all unused library folders and files should be removed and folders shortened.
After updating you can check for breaking changes between the old and new assembly versions with BitDiffer.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/hosting-azure-application-insights (Mercurial repository)
https://github.com/Lombiq/Orchard-Azure-Application-Insights (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",11
vshaxe/vshaxe,Haxe,"Haxe Support for Visual Studio Code
     
This is an extension for Visual Studio Code that adds support for the Haxe language,
leveraging the Haxe Language Server. It works best with Haxe 4.0.0-rc.2, but supports any Haxe version starting from 3.4.0.
For usage with...

... Lime / OpenFL, the Lime extension should be installed.
... Kha, the Kha Extension Pack should be used.

Click here for install instructions.

Features
This is just a brief overview of the supported features. For more details, check out our extensive documentation.

Syntax Highlighting
Tasks (Tasks -> Run Task...)
Debugging (F5)
Commands (F1 -> search ""Haxe"")
Dependency Explorer
Completion
Postfix Completion
Snippets
Code Generation
Signature Help
Hover Hints
Go to Definition (F12)
Go to Type Definition
Peek Definition (Alt+F12)
Find All References (Shift+Alt+F12)
Peek References (Shift+F12)
Rename Symbol (F2)
Document Symbols (Ctrl+Shift+O)
Workspace Symbols (Ctrl+T)
Outline
Folding
Diagnostics
Code Actions (Ctrl+. on light bulbs)
Code Lens (F1 -> Haxe: Toggle Code Lens)
Formatting (Shift+Alt+F)
Extension API

",187
kiteco/kite-api-js,JavaScript,"Kite API
 
.STATES
An object containing the constant values of the various states of kited



State
Value
Alias




UNSUPPORTED
0



UNINSTALLED
1



NOT_RUNNING
2
INSTALLED


NOT_REACHABLE
3
RUNNING


UNLOGGED
4
REACHABLE


NOT_WHITELISTED
5
AUTHENTICATED


WHITELISTED
6
 



.editorConfig
The editorConfig object is provided as a way for an editor to store information across instances and session. It can be setup with different concrete stores that implements the various solutions to store the data.
By default it is setup with a MemoryStore that just stores everything in memory (it thus won't persist across session nor instances).
The available stores are:

MemoryStore: Used as a placeholder, it is also useful in tests.
FileStore: Stores everything on a file at the path specified during creation
LocalStore: When running in a browser environment this store allows you to use the localStorage as a way to store the config data. The store takes a key which will be used to access the data in the local storage object.

The object exposes two main functions, get and set that both returns a promise when invoked.
Storage is done using JSON format, and the path argument in both methods resolves to a real object in that JSON structure. However, stores implementation only have to deal with strings, the whole serialization/parsing is handled by the editor config object.
KiteAPI.editorConfig.set('path.to.some.data', 'data')
.then(() => KiteAPI.editorConfig.get('path.to.some.data'))
.then(data => {
  console.log(data)
  // output: data
  // store data: { path: { to: { some: 'data' }} }
})
.toggleRequestDebug()
This function is both a proxy to the KiteConnector method of the same name as well as calling the corresponding function on the Account object.
.onDidDetectWhitelistedPath(listener)
Registers a listener to the did-detect-whitelisted-path event and will be notified when a request for an editor API responds with a 200. The listener will be invoked with the path that have been detected as whitelisted.
The function returns a disposable object to unregister the listener from this event.
const disposable = KiteAPI.onDidDetectWhitelistedPath(path => {
  // path is whitelisted
})

disposable.dispose(); // the listener will no longer receive events
.onDidDetectNonWhitelistedPath(listener)
Registers a listener to the did-detect-non-whitelisted-path event and will be notified when a request for an editor API responds with a 403. The listener will be invoked with the path that have been detected as not whitelisted.
The function returns a disposable object to unregister the listener from this event.
const disposable = KiteAPI.onDidDetectNonWhitelistedPath(path => {
  // path is not whitelisted
})

disposable.dispose(); // the listener will no longer receive events
.requestJSON(options, data, timeout)
Makes a request to Kite using KiteConnector.request and automatically parses the JSON response when the status code was 200.
KiteAPI.requestJSON({path}).then(data => {
  // data is the result of JSON.parse on the response text
})
.isKiteLocal()
Makes a GET request to Kite at the endpoint /clientapi/iskitelocal. Responds with a boolean resolving Promise to true in the case of a 200 status code, false otherwise
KiteAPI.isKiteLocal().then(isLocal => {
  // do stuff related to kite-locality
})
.setKiteSetting(key, value)
Makes a POST request to Kite at the endpoint /clientapi/settings/${key}, where the body is set to value. It automatically parses the JSON response when the status code is 200
KiteAPI.setKiteSetting('some_setting_name', 'a_value')
.getKiteSetting(key)
Makes a GET request to Kite at the endpoint /clientapi/settings/${key}. It automatically parses the JSON response when the status code is 200
KiteAPI.getKiteSetting('some_setting').then(settingValue => {
  // do something with settingValue
})
.canAuthenticateUser()
Returns a promise that resolves if a user can be authenticated. A user can be authenticated if Kite is reachable and if the user is not already logged into Kite.
KiteAPI.canAuthenticateUser().then(() => {
  // user can be authenticated
})
.authenticateUser(email, password)
Makes a request to authenticate the user through Kite and returns a promise that will resolve if the authentication succeeds.
KiteAPI.authenticateUser('john@doe.com', 'password')
.then(() => {
  // User is logged in
})
.catch(err => {
  // authentication failed
})
.authenticateSessionID(key)
Makes a request to authenticate a user using a session id and returns a promise that will resolve if the authentication succeeds. This function can be used to authenticate a user in Kite whose account have been created using kite.com API.
KiteAPI.authenticateSessionID('session-id')
.then(() => {
  // User is logged in
})
.catch(err => {
  // authentication failed
})
.isPathWhitelisted(path)
Returns a promise that resolves if the path is part of the whitelist, otherwise the promise will be rejected.
When calling that function, depending on the outcome, a did-detect-whitelisted-path or did-detect-non-whitelisted-path event will be dispatched.
KiteAPI.isPathWhitelisted(path)
.then(() => {
  // path is whitelisted
})
.catch(err => {
  // path is not whitelisted or an error occurred
  // err contains the details of the failure
})
.canWhitelistPath(path)
Returns a promise that resolves if the path can be whitelisted.
A path can be whitelisted if it's not already part of the whitelist and if the projectdir endpoint responds with a 200.
In case of success, the promise resolves with the path returned by the projectdir endpoint.
KiteAPI.canWhitelistPath(path).then(projectDir => {
  // projectDir can be sent to the whitelist endpoint
})
.whitelistPath(path)
Makes a request to the whitelist endpoint and returns a promise that resolves if the path have been successfully whitelisted.
KiteAPI.whitelistPath(path).then(() => {
  // path is now whitelisted
})
.blacklistPath(path, noAction)
Makes a request to the blacklist endpoint and returns a promise that resolves if the path have been successfully blacklisted. The noAction parameters will allow to set the closed_whitelist_notification_without_action value in the request, it defaults to false.
KiteAPI.blacklistPath(path).then(() => {
  // path is now blacklisted
})
.getSupportedLanguages()
Makes a request to the languages endpoint and returns a promise that resolves with an array of the supported languages.
KiteAPI.getSupportedLanguages().then(languages => {
  // do something with languages
})
.getOnboardingFilePath()
Makes a request to the onboarding_file endpoint and returns a promise that resolves with a filepath string.
KiteAPI.getOnboardingFilePath().then(path => {
  // do something with path
})
.getHoverDataAtPosition(filename, source, position, editor)
.getReportDataAtPosition(filename, source, position, editor)
.getSymbolReportDataForId(id)
.getValueReportDataForId(id)
.getMembersDataForId(id)
.getUsagesDataForValueId(id)
.getUsageDataForId(id)
.getExampleDataForId(id)
.getUserAccountInfo()
.isFileAuthorized(filename)
.shouldOfferWhitelist(filename)
.shouldNotify(filename)
.projectDirForFile(filename)
.getStatus(filename)
.getCompletionsAtPosition(filename, source, position, editor)
.getSignaturesAtPosition(filename, source, position, editor)
.getAutocorrectData(filename, source, editorMeta)
.getAutocorrectModelInfo(version, editorMeta)
.getOsName()
.postSaveValidationData(filename, source, editorMeta)
.postAutocorrectFeedbackData(response, feedback, editorMeta)
.postAutocorrectHashMismatchData(response, requestStartTime, editorMeta)
.sendFeatureMetric(name)
.featureRequested(name, editor)
.featureFulfilled(name, editor)
.featureApplied(name, editor)
.Account
.initClient(hostname, port)
.disposeClient()
.toggleRequestDebug()
.checkEmail(data)
.createAccount(data, callback)
.login(data, callback)
.resetPassword(data, callback)
Delegated methods
.arch()
A proxy to the KiteConnector.arch method.
.canInstallKite()
A proxy to the KiteConnector.canInstallKite method.
.canRunKite()
A proxy to the KiteConnector.canRunKite method.
.canRunKiteEnterprise()
A proxy to the KiteConnector.canRunKiteEnterprise. method
.checkHealth()
A proxy to the KiteConnector.checkHealth method.
.downloadKite(url, options)
A proxy to the KiteConnector.downloadKite method.
.downloadKiteRelease(options)
A proxy to the KiteConnector.downloadKiteRelease. method
.hasBothKiteInstalled()
A proxy to the KiteConnector.hasBothKiteInstalled. method
.hasManyKiteEnterpriseInstallation()
A proxy to the KiteConnector.hasManyKiteEnterpriseInstallation method.
.hasManyKiteInstallation()
A proxy to the KiteConnector.hasManyKiteInstallation method.
.installKite(options)
A proxy to the KiteConnector.installKite method.
.isAdmin()
A proxy to the KiteConnector.isAdmin method.
.isKiteEnterpriseInstalled()
A proxy to the KiteConnector.isKiteEnterpriseInstalled method.
.isKiteEnterpriseRunning()
A proxy to the KiteConnector.isKiteEnterpriseRunning method.
.isKiteInstalled()
A proxy to the KiteConnector.isKiteInstalled method.
.isKiteReachable()
A proxy to the KiteConnector.isKiteReachable method.
.isKiteRunning()
A proxy to the KiteConnector.isKiteRunning method.
.isKiteSupported()
A proxy to the KiteConnector.isKiteSupported method.
.isOSSupported()
A proxy to the KiteConnector.isOSSupported method.
.isOSVersionSupported()
A proxy to the KiteConnector.isOSVersionSupported. method
.isUserAuthenticated()
A proxy to the KiteConnector.isUserAuthenticated. method
.onDidFailRequest(listener)
A proxy to the KiteConnector.onDidFailRequest method.
.request(options, data, timeout)
A proxy to the KiteConnector.request method.
.runKite()
A proxy to the KiteConnector.runKite method.
.runKiteAndWait()
A proxy to the KiteConnector.runKiteAndWait method.
.runKiteEnterprise()
A proxy to the KiteConnector.runKiteEnterprise method.
.runKiteEnterpriseAndWait()
A proxy to the KiteConnector.runKiteEnterpriseAndWait method.
.toggleRequestDebug()
A proxy to the KiteConnector.toggleRequestDebug method.
.waitForKite(attempts, interval)
A proxy to the KiteConnector.waitForKite method.
",2
patrickhulce/third-party-web,JavaScript,"Third Party Web
Check out the shiny new web UI https://www.thirdpartyweb.today/
Data on third party entities and their impact on the web.
This document is a summary of which third party scripts are most responsible for excessive JavaScript execution on the web today.
Table of Contents

Goals
Methodology
NPM Module
Updates
Data

Summary
How to Interpret
Third Parties by Category

Advertising
Analytics
Social
Video
Developer Utilities
Hosting Platforms
Marketing
Customer Success
Content & Publishing
Libraries
Tag Management
Mixed / Other


Third Parties by Total Impact


Future Work
FAQs
Contributing

Goals

Quantify the impact of third party scripts on the web.
Identify the third party scripts on the web that have the greatest performance cost.
Give developers the information they need to make informed decisions about which third parties to include on their sites.
Incentivize responsible third party script behavior.
Make this information accessible and useful.

Methodology
HTTP Archive is an inititiave that tracks how the web is built. Twice a month, ~4 million sites are crawled with Lighthouse on mobile. Lighthouse breaks down the total script execution time of each page and attributes the execution to a URL. Using BigQuery, this project aggregates the script execution to the origin-level and assigns each origin to the responsible entity.
NPM Module
The entity classification data is available as an NPM module.
const {getEntity} = require('third-party-web')
const entity = getEntity('https://d36mpcpuzc4ztk.cloudfront.net/js/visitor.js')
console.log(entity)
//   {
//     ""name"": ""Freshdesk"",
//     ""homepage"": ""https://freshdesk.com/"",
//     ""categories"": [""customer-success""],
//     ""domains"": [""d36mpcpuzc4ztk.cloudfront.net""]
//   }
Updates
2019-02-01 dataset
Huge props to WordAds for reducing their impact from ~2.5s to ~200ms on average! A few entities are showing considerably less data this cycle (Media Math, Crazy Egg, DoubleVerify, Bootstrap CDN). Perhaps they've added new CDNs/hostnames that we haven't identified or the basket of sites in HTTPArchive has shifted away from their usage.
2019-03-01 dataset
Almost 2,000 entities tracked now across ~3,000+ domains! Huge props to @simonhearne for making this massive increase possible. Tag Managers have now been split out into their own category since they represented such a large percentage of the ""Mixed / Other"" category.
2019-05-06 dataset
Google Ads clarified that www.googletagservices.com serves more ad scripts than generic tag management, and it has been reclassified accordingly. This has dropped the overall Tag Management share considerably back down to its earlier position.
2019-05-13 dataset
A shortcoming of the attribution approach has been fixed. Total usage is now reported based on the number of pages in the dataset that use the third-party, not the number of scripts. Correspondingly, all average impact times are now reported per page rather than per script. Previously, a third party could appear to have a lower impact or be more popular simply by splitting their work across multiple files.
Third-parties that performed most of their work from a single script should see little to no impact from this change, but some entities have seen significant ranking movement. Hosting providers that host entire pages are, understandably, the most affected.
Some notable changes below:



Third-Party
Previously (per-script)
Now (per-page)




Beeketing
137 ms
465 ms


Sumo
263 ms
798 ms


Tumblr
324 ms
1499 ms


Yandex APIs
393 ms
1231 ms


Google Ads
402 ms
1285 ms


Wix
972 ms
5393 ms



Data
Summary
Across top ~4 million sites, ~2700 origins account for ~57% of all script execution time with the top 50 entities already accounting for ~47%. Third party script execution is the majority chunk of the web today, and it's important to make informed choices.
How to Interpret
Each entity has a number of data points available.

Usage (Total Number of Occurrences) - how many scripts from their origins were included on pages
Total Impact (Total Execution Time) - how many seconds were spent executing their scripts across the web
Average Impact (Average Execution Time) - on average, how many milliseconds were spent executing each script
Category - what type of script is this


Third Parties by Category
This section breaks down third parties by category. The third parties in each category are ranked from first to last based on the average impact of their scripts. Perhaps the most important comparisons lie here. You always need to pick an analytics provider, but at least you can pick the most well-behaved analytics provider.
Overall Breakdown
Unsurprisingly, ads account for the largest identifiable chunk of third party script execution.


Advertising
These scripts are part of advertising networks, either serving or measuring.



Rank
Name
Usage
Average Impact




1
ExoClick
2,486
40 ms


2
BlueKai
2,526
74 ms


3
Gemius
7,900
75 ms


4
Affiliate Window
1,043
77 ms


5
MailMunch
4,637
78 ms


6
Crowd Control
2,118
81 ms


7
Rakuten Marketing
1,817
85 ms


8
Tribal Fusion
1,022
88 ms


9
PubNation
2,511
100 ms


10
Scorecard Research
13,146
100 ms


11
PushCrew
3,460
104 ms


12
Constant Contact
1,324
108 ms


13
Outbrain
6,123
108 ms


14
OptiMonk
1,030
116 ms


15
Unbounce
2,102
117 ms


16
Adroll
3,188
119 ms


17
Popads
4,628
122 ms


18
TrafficStars
1,010
122 ms


19
Rubicon Project
4,624
127 ms


20
Amazon Ads
21,711
129 ms


21
DTSCOUT
8,256
131 ms


22
Adyoulike
1,119
132 ms


23
Skimbit
9,803
136 ms


24
fluct
6,482
137 ms


25
Refersion
1,021
144 ms


26
Digital ad Consortium
3,959
144 ms


27
Criteo
73,060
153 ms


28
SmartAdServer
1,996
155 ms


29
AudienceSearch
6,156
156 ms


30
Cxense
4,274
165 ms


31
AOL / Oath / Verizon Media
1,341
171 ms


32
Sharethrough
2,350
177 ms


33
Tynt
18,437
182 ms


34
Microad
2,015
183 ms


35
Adform
8,099
187 ms


36
JuicyAds
2,448
192 ms


37
Pubmatic
4,639
193 ms


38
Bing Ads
13,461
197 ms


39
Index Exchange
3,383
197 ms


40
Smart AdServer
2,965
219 ms


41
Adloox
1,996
226 ms


42
Yahoo!
2,421
227 ms


43
Klaviyo
6,646
257 ms


44
MGID
7,964
266 ms


45
Sortable
1,094
269 ms


46
LongTail Ad Solutions
2,749
271 ms


47
VigLink
7,925
283 ms


48
AppNexus
8,478
289 ms


49
Privy
11,207
305 ms


50
iBillboard
3,265
322 ms


51
Market GID
1,421
369 ms


52
Teads
5,502
384 ms


53
Sizmek
3,971
428 ms


54
Taboola
13,612
465 ms


55
Yandex Ads
23,195
501 ms


56
sovrn
3,202
503 ms


57
Infolinks
4,759
594 ms


58
GumGum
3,776
641 ms


59
Admixer for Publishers
1,319
686 ms


60
WordAds
5,559
687 ms


61
OpenX
7,275
821 ms


62
DoubleVerify
1,929
933 ms


63
Media.net
3,825
956 ms


64
MediaVine
4,296
961 ms


65
Vidible
1,264
987 ms


66
Integral Ad Science
8,575
1077 ms


67
Moat
11,906
1141 ms


68
Google/Doubleclick Ads
624,172
1285 ms


69
AdMatic
1,410
1329 ms


70
LKQD
1,115
1485 ms


71
StickyADS.tv
3,382
1666 ms


72
33 Across
5,937
1736 ms


73
fam
2,326
1783 ms




Analytics
These scripts measure or track users and their actions. There's a wide range in impact here depending on what's being tracked.



Rank
Name
Usage
Average Impact




1
Alexa
1,209
56 ms


2
StatCounter
5,047
63 ms


3
Amplitude Mobile Analytics
1,327
72 ms


4
etracker
1,907
78 ms


5
Roxr Software
2,185
78 ms


6
Net Reviews
1,462
79 ms


7
Heap
1,845
80 ms


8
Trust Pilot
3,102
84 ms


9
Mixpanel
5,305
85 ms


10
Google Analytics
1,124,001
86 ms


11
Searchanise
2,752
92 ms


12
Chartbeat
6,874
100 ms


13
Hotjar
111,933
104 ms


14
Baidu Analytics
9,153
107 ms


15
Quantcast
6,539
112 ms


16
Marchex
4,382
119 ms


17
CallRail
5,021
120 ms


18
Parse.ly
3,070
126 ms


19
Snowplow
5,357
126 ms


20
Crazy Egg
11,926
130 ms


21
Marketo
1,427
140 ms


22
Monetate
1,009
158 ms


23
Treasure Data
12,262
162 ms


24
Nielsen NetRatings SiteCensus
11,322
166 ms


25
Evidon
1,311
170 ms


26
Snapchat
6,233
186 ms


27
Gigya
1,994
192 ms


28
BounceX
1,459
194 ms


29
Nosto
1,901
197 ms


30
DigiTrust
4,583
197 ms


31
Segment
8,047
198 ms


32
VWO
3,280
211 ms


33
FullStory
5,563
227 ms


34
ForeSee
1,441
255 ms


35
Optimizely
12,417
267 ms


36
Bazaarvoice
2,546
271 ms


37
Ezoic
2,968
271 ms


38
mPulse
5,332
348 ms


39
Inspectlet
5,646
362 ms


40
Yandex Metrica
242,224
376 ms


41
Radar
4,886
383 ms


42
Keen
3,241
384 ms


43
SessionCam
1,564
385 ms


44
Histats
13,523
463 ms


45
Feefo.com
1,430
472 ms


46
AB Tasty
3,435
559 ms


47
Salesforce
20,689
577 ms


48
Mouseflow
1,545
595 ms


49
Lucky Orange
6,691
903 ms




Social
These scripts enable social features.



Rank
Name
Usage
Average Impact




1
VK
7,315
86 ms


2
Instagram
6,195
92 ms


3
Micropat
21,584
105 ms


4
Pinterest
16,532
112 ms


5
Kakao
8,325
117 ms


6
LinkedIn
14,000
122 ms


7
Facebook
1,025,971
144 ms


8
Twitter
259,833
172 ms


9
Yandex Share
22,930
173 ms


10
ShareThis
28,888
313 ms


11
Shareaholic
11,846
412 ms


12
AddThis
134,999
458 ms


13
SocialShopWave
1,250
472 ms


14
PIXNET
26,582
925 ms


15
Tumblr
9,015
1445 ms


16
LiveJournal
3,811
1464 ms




Video
These scripts enable video player and streaming functionality.



Rank
Name
Usage
Average Impact




1
YouTube
30,547
156 ms


2
Brightcove
5,100
721 ms


3
Wistia
10,643
761 ms




Developer Utilities
These scripts are developer utilities (API clients, site monitoring, fraud detection, etc).



Rank
Name
Usage
Average Impact




1
Trusted Shops
1,328
45 ms


2
Stripe
5,020
81 ms


3
New Relic
3,340
81 ms


4
OneSignal
12,191
84 ms


5
Siteimprove
1,855
89 ms


6
Cookiebot
9,516
93 ms


7
GetSiteControl
2,901
94 ms


8
iubenda
10,528
111 ms


9
Bold Commerce
11,282
145 ms


10
Po.st
1,677
146 ms


11
AppDynamics
1,496
149 ms


12
Sift Science
1,232
149 ms


13
Swiftype
1,519
170 ms


14
Other Google APIs/SDKs
279,510
195 ms


15
Seznam
1,854
246 ms


16
MaxCDN Enterprise
1,785
265 ms


17
Fastly
6,187
269 ms


18
Rambler
9,145
270 ms


19
Cloudflare
8,628
272 ms


20
Affirm
1,188
285 ms


21
Google Maps
120,242
312 ms


22
PayPal
9,816
341 ms


23
Secomapp
1,103
428 ms


24
Datacamp
12,603
434 ms


25
Sentry
15,661
462 ms


26
Distil Networks
10,893
486 ms


27
Yandex APIs
21,677
1231 ms


28
Mapbox
3,686
1384 ms




Hosting Platforms
These scripts are from web hosting platforms (WordPress, Wix, Squarespace, etc). Note that in this category, this can sometimes be the entirety of script on the page, and so the ""impact"" rank might be misleading. In the case of WordPress, this just indicates the libraries hosted and served by WordPress not all sites using self-hosted WordPress.



Rank
Name
Usage
Average Impact




1
Blogger
10,705
64 ms


2
WordPress
105,874
177 ms


3
Dealer
9,877
515 ms


4
Shopify
71,063
633 ms


5
CDK Dealer Management
4,210
1036 ms


6
Squarespace
36,919
1133 ms


7
Hatena Blog
19,231
1282 ms


8
Weebly
14,462
1305 ms


9
Wix
40,752
5393 ms




Marketing
These scripts are from marketing tools that add popups/newsletters/etc.



Rank
Name
Usage
Average Impact




1
RD Station
3,457
76 ms


2
Bronto Software
1,110
132 ms


3
Listrak
1,070
145 ms


4
Hubspot
24,759
156 ms


5
Drift
5,072
163 ms


6
Ve
3,307
168 ms


7
Mailchimp
17,626
223 ms


8
Yotpo
10,307
238 ms


9
OptinMonster
7,556
300 ms


10
Beeketing
20,117
449 ms


11
Bigcommerce
7,846
537 ms


12
Albacross
1,737
753 ms


13
Sumo
20,502
794 ms




Customer Success
These scripts are from customer support/marketing providers that offer chat and contact solutions. These scripts are generally heavier in weight.



Rank
Name
Usage
Average Impact




1
Foursixty
1,129
82 ms


2
iPerceptions
2,822
92 ms


3
LivePerson
4,405
129 ms


4
Comm100
1,643
141 ms


5
LiveChat
10,498
154 ms


6
Pure Chat
4,029
169 ms


7
iAdvize SAS
1,060
251 ms


8
Tawk.to
46,981
343 ms


9
Jivochat
28,194
360 ms


10
Tidio Live Chat
6,518
368 ms


11
Help Scout
1,626
372 ms


12
Dynamic Yield
1,658
459 ms


13
Intercom
13,452
500 ms


14
LiveTex
2,337
510 ms


15
Olark
7,513
626 ms


16
ZenDesk
68,198
697 ms




Content & Publishing
These scripts are from content providers or publishing-specific affiliate tracking.



Rank
Name
Usage
Average Impact




1
Accuweather
1,510
75 ms


2
OpenTable
2,179
134 ms


3
Embedly
3,082
212 ms


4
AMP
46,495
349 ms


5
Medium
1,329
376 ms


6
Hotmart
1,113
828 ms




Libraries
These are mostly open source libraries (e.g. jQuery) served over different public CDNs. This category is unique in that the origin may have no responsibility for the performance of what's being served. Note that rank here does not imply one CDN is better than the other. It simply indicates that the libraries being served from that origin are lighter/heavier than the ones served by another.



Rank
Name
Usage
Average Impact




1
Adobe TypeKit
10,620
91 ms


2
Yandex CDN
1,939
155 ms


3
FontAwesome CDN
19,326
160 ms


4
Microsoft Hosted Libs
4,876
185 ms


5
Monotype
4,640
200 ms


6
jQuery CDN
147,162
227 ms


7
Google CDN
749,555
247 ms


8
Unpkg
2,947
251 ms


9
Cloudflare CDN
90,131
260 ms


10
JSDelivr CDN
23,200
336 ms


11
CreateJS CDN
1,713
3617 ms




Tag Management
These scripts tend to load lots of other scripts and initiate many tasks.



Rank
Name
Usage
Average Impact




1
Google Tag Manager
481,506
115 ms


2
BrightTag / Signal
6,968
115 ms


3
Adobe Tag Manager
27,224
334 ms


4
Tealium
12,175
353 ms


5
Ensighten
5,840
390 ms




Mixed / Other
These are miscellaneous scripts delivered via a shared origin with no precise category or attribution. Help us out by identifying more origins!



Rank
Name
Usage
Average Impact




1
Amazon Web Services
47,656
181 ms


2
All Other 3rd Parties
913,172
332 ms


3
Pagely
1,018
346 ms


4
Parking Crew
5,762
484 ms


5
uLogin
2,451
1211 ms




Third Parties by Total Impact
This section highlights the entities responsible for the most script execution across the web. This helps inform which improvements would have the largest total impact.



Name
Popularity
Total Impact
Average Impact




Google/Doubleclick Ads
624,172
801,894 s
1285 ms


All Other 3rd Parties
913,172
302,947 s
332 ms


Wix
40,752
219,788 s
5393 ms


Google CDN
749,555
184,832 s
247 ms


Facebook
1,025,971
147,647 s
144 ms


Google Analytics
1,124,001
96,913 s
86 ms


Yandex Metrica
242,224
91,128 s
376 ms


AddThis
134,999
61,857 s
458 ms


Google Tag Manager
481,506
55,435 s
115 ms


Other Google APIs/SDKs
279,510
54,369 s
195 ms


ZenDesk
68,198
47,548 s
697 ms


Shopify
71,063
44,957 s
633 ms


Twitter
259,833
44,660 s
172 ms


Squarespace
36,919
41,812 s
1133 ms


Google Maps
120,242
37,497 s
312 ms


jQuery CDN
147,162
33,468 s
227 ms


Yandex APIs
21,677
26,687 s
1231 ms


Hatena Blog
19,231
24,654 s
1282 ms


PIXNET
26,582
24,586 s
925 ms


Cloudflare CDN
90,131
23,395 s
260 ms


Weebly
14,462
18,877 s
1305 ms


WordPress
105,874
18,740 s
177 ms


Sumo
20,502
16,288 s
794 ms


AMP
46,495
16,225 s
349 ms


Tawk.to
46,981
16,092 s
343 ms


Moat
11,906
13,589 s
1141 ms


Tumblr
9,015
13,030 s
1445 ms


Salesforce
20,689
11,934 s
577 ms


Hotjar
111,933
11,672 s
104 ms


Yandex Ads
23,195
11,612 s
501 ms


Criteo
73,060
11,206 s
153 ms


33 Across
5,937
10,306 s
1736 ms


Jivochat
28,194
10,150 s
360 ms


Integral Ad Science
8,575
9,236 s
1077 ms


Adobe Tag Manager
27,224
9,105 s
334 ms


ShareThis
28,888
9,040 s
313 ms


Beeketing
20,117
9,034 s
449 ms


Amazon Web Services
47,656
8,624 s
181 ms


Wistia
10,643
8,098 s
761 ms


JSDelivr CDN
23,200
7,801 s
336 ms


Sentry
15,661
7,237 s
462 ms


Intercom
13,452
6,729 s
500 ms


Taboola
13,612
6,328 s
465 ms


Histats
13,523
6,264 s
463 ms


CreateJS CDN
1,713
6,196 s
3617 ms


Lucky Orange
6,691
6,042 s
903 ms


OpenX
7,275
5,971 s
821 ms


StickyADS.tv
3,382
5,634 s
1666 ms


LiveJournal
3,811
5,578 s
1464 ms


Datacamp
12,603
5,465 s
434 ms


Distil Networks
10,893
5,291 s
486 ms


Mapbox
3,686
5,100 s
1384 ms


Dealer
9,877
5,087 s
515 ms


Shareaholic
11,846
4,876 s
412 ms


YouTube
30,547
4,769 s
156 ms


Olark
7,513
4,702 s
626 ms


CDK Dealer Management
4,210
4,362 s
1036 ms


Tealium
12,175
4,301 s
353 ms


Bigcommerce
7,846
4,217 s
537 ms


fam
2,326
4,146 s
1783 ms


MediaVine
4,296
4,127 s
961 ms


Yandex Share
22,930
3,965 s
173 ms


Mailchimp
17,626
3,924 s
223 ms


Hubspot
24,759
3,870 s
156 ms


WordAds
5,559
3,817 s
687 ms


Brightcove
5,100
3,677 s
721 ms


Media.net
3,825
3,658 s
956 ms


Privy
11,207
3,421 s
305 ms


Tynt
18,437
3,351 s
182 ms


PayPal
9,816
3,343 s
341 ms


Optimizely
12,417
3,314 s
267 ms


FontAwesome CDN
19,326
3,102 s
160 ms


uLogin
2,451
2,969 s
1211 ms


Infolinks
4,759
2,828 s
594 ms


Amazon Ads
21,711
2,793 s
129 ms


Parking Crew
5,762
2,788 s
484 ms


Bing Ads
13,461
2,657 s
197 ms


Rambler
9,145
2,466 s
270 ms


Yotpo
10,307
2,454 s
238 ms


AppNexus
8,478
2,448 s
289 ms


GumGum
3,776
2,421 s
641 ms


Tidio Live Chat
6,518
2,398 s
368 ms


Cloudflare
8,628
2,343 s
272 ms


Ensighten
5,840
2,280 s
390 ms


Micropat
21,584
2,270 s
105 ms


OptinMonster
7,556
2,264 s
300 ms


VigLink
7,925
2,243 s
283 ms


MGID
7,964
2,122 s
266 ms


Teads
5,502
2,113 s
384 ms


Inspectlet
5,646
2,042 s
362 ms


Treasure Data
12,262
1,987 s
162 ms


AB Tasty
3,435
1,920 s
559 ms


Nielsen NetRatings SiteCensus
11,322
1,885 s
166 ms


AdMatic
1,410
1,873 s
1329 ms


Radar
4,886
1,871 s
383 ms


Blindado
760
1,859 s
2446 ms


Pinterest
16,532
1,855 s
112 ms


mPulse
5,332
1,853 s
348 ms


DoubleVerify
1,929
1,800 s
933 ms


LinkedIn
14,000
1,710 s
122 ms


Klaviyo
6,646
1,708 s
257 ms


Sizmek
3,971
1,700 s
428 ms


Fastly
6,187
1,667 s
269 ms


LKQD
1,115
1,655 s
1485 ms


Bold Commerce
11,282
1,635 s
145 ms


LiveChat
10,498
1,613 s
154 ms


sovrn
3,202
1,610 s
503 ms


Segment
8,047
1,593 s
198 ms


Crazy Egg
11,926
1,555 s
130 ms


Adform
8,099
1,512 s
187 ms


Skimbit
9,803
1,335 s
136 ms


Scorecard Research
13,146
1,316 s
100 ms


Albacross
1,737
1,308 s
753 ms


FullStory
5,563
1,264 s
227 ms


Vidible
1,264
1,248 s
987 ms


Keen
3,241
1,245 s
384 ms


LiveTex
2,337
1,192 s
510 ms


iubenda
10,528
1,170 s
111 ms


Snapchat
6,233
1,162 s
186 ms


Esri ArcGIS
978
1,150 s
1176 ms


DTSCOUT
8,256
1,085 s
131 ms


iBillboard
3,265
1,050 s
322 ms


Between Digital
915
1,036 s
1132 ms


OneSignal
12,191
1,025 s
84 ms


Yieldmo
934
984 s
1053 ms


Baidu Analytics
9,153
978 s
107 ms


Kakao
8,325
971 s
117 ms


Adobe TypeKit
10,620
971 s
91 ms


AudienceSearch
6,156
958 s
156 ms


Monotype
4,640
930 s
200 ms


Hotmart
1,113
922 s
828 ms


Mouseflow
1,545
919 s
595 ms


Admixer for Publishers
1,319
905 s
686 ms


DigiTrust
4,583
903 s
197 ms


Microsoft Hosted Libs
4,876
901 s
185 ms


Pubmatic
4,639
894 s
193 ms


Cookiebot
9,516
886 s
93 ms


fluct
6,482
886 s
137 ms


Drift
5,072
827 s
163 ms


Sekindo
457
806 s
1763 ms


Ezoic
2,968
805 s
271 ms


BrightTag / Signal
6,968
804 s
115 ms


Dynamic Yield
1,658
762 s
459 ms


LongTail Ad Solutions
2,749
746 s
271 ms


Unpkg
2,947
740 s
251 ms


Quantcast
6,539
730 s
112 ms


Cxense
4,274
704 s
165 ms


VWO
3,280
690 s
211 ms


Blogger
10,705
690 s
64 ms


Chartbeat
6,874
689 s
100 ms


Bazaarvoice
2,546
689 s
271 ms


Pure Chat
4,029
682 s
169 ms


Snowplow
5,357
676 s
126 ms


Feefo.com
1,430
675 s
472 ms


Okas Concepts
598
672 s
1124 ms


Index Exchange
3,383
668 s
197 ms


Outbrain
6,123
662 s
108 ms


Embedly
3,082
653 s
212 ms


WebpageFX
376
648 s
1724 ms


Smart AdServer
2,965
648 s
219 ms


VK
7,315
629 s
86 ms


Help Scout
1,626
605 s
372 ms


SessionCam
1,564
603 s
385 ms


CallRail
5,021
602 s
120 ms


Gemius
7,900
596 s
75 ms


SocialShopWave
1,250
590 s
472 ms


Rubicon Project
4,624
589 s
127 ms


Instagram
6,195
572 s
92 ms


Digital ad Consortium
3,959
570 s
144 ms


LivePerson
4,405
566 s
129 ms


Popads
4,628
564 s
122 ms


IBM Digital Analytics
925
557 s
602 ms


Ve
3,307
555 s
168 ms


Yahoo!
2,421
549 s
227 ms


Market GID
1,421
524 s
369 ms


Marchex
4,382
521 s
119 ms


Kaltura Video Platform
551
503 s
913 ms


Medium
1,329
500 s
376 ms


Meetrics
774
499 s
645 ms


Secomapp
1,103
473 s
428 ms


MaxCDN Enterprise
1,785
472 s
265 ms


JuicyAds
2,448
471 s
192 ms


Seznam
1,854
456 s
246 ms


Adloox
1,996
451 s
226 ms


Mixpanel
5,305
451 s
85 ms


LeasdBoxer
116
440 s
3792 ms


Sharethrough
2,350
416 s
177 ms


Stripe
5,020
405 s
81 ms


Digioh
768
395 s
515 ms


PerimeterX Bot Defender
297
392 s
1320 ms


Parse.ly
3,070
385 s
126 ms


Gigya
1,994
384 s
192 ms


Adroll
3,188
381 s
119 ms


Nosto
1,901
374 s
197 ms


Microad
2,015
368 s
183 ms


ForeSee
1,441
367 s
255 ms


MailMunch
4,637
363 s
78 ms


PushCrew
3,460
359 s
104 ms


Hola Networks
175
357 s
2038 ms


Ecwid
703
354 s
504 ms


Audience 360
420
353 s
842 ms


Pagely
1,018
353 s
346 ms


Disqus
855
352 s
412 ms


Cedato
100
351 s
3513 ms


TrackJS
860
347 s
404 ms


Affirm
1,188
339 s
285 ms


Bugsnag
843
331 s
393 ms


Clicktale
936
328 s
351 ms


Underdog Media
359
324 s
904 ms


StatCounter
5,047
320 s
63 ms


Pixlee
371
313 s
843 ms


Zmags
182
309 s
1699 ms


SmartAdServer
1,996
309 s
155 ms


Perfect Market
889
303 s
341 ms


Best Of Media S.A.
237
301 s
1271 ms


Yandex CDN
1,939
300 s
155 ms


Vox Media
708
296 s
418 ms


Sortable
1,094
294 s
269 ms


OpenTable
2,179
291 s
134 ms


BounceX
1,459
283 s
194 ms


GetSiteControl
2,901
273 s
94 ms


New Relic
3,340
272 s
81 ms


iAdvize SAS
1,060
267 s
251 ms


Wishpond Technologies
518
265 s
512 ms


piano
856
264 s
309 ms


Adthink
486
264 s
543 ms


RD Station
3,457
262 s
76 ms


Trust Pilot
3,102
262 s
84 ms


iPerceptions
2,822
261 s
92 ms


Swiftype
1,519
258 s
170 ms


UserReport
884
255 s
288 ms


Yieldify
729
255 s
350 ms


Searchanise
2,752
252 s
92 ms


PubNation
2,511
251 s
100 ms


Media Management Technologies
596
249 s
417 ms


Ooyala
337
247 s
734 ms


Unbounce
2,102
246 s
117 ms


Decibel Insight
551
246 s
447 ms


Po.st
1,677
245 s
146 ms


Fort Awesome
740
244 s
330 ms


Fraudlogix
872
239 s
274 ms


Expedia
299
235 s
786 ms


Comm100
1,643
231 s
141 ms


AOL / Oath / Verizon Media
1,341
229 s
171 ms


Maxymiser
831
226 s
272 ms


PhotoBucket
737
226 s
306 ms


AppDynamics
1,496
224 s
149 ms


Evidon
1,311
223 s
170 ms


Trip Advisor
177
221 s
1250 ms


Opta
337
221 s
655 ms


Celtra
568
218 s
383 ms


Technorati
478
215 s
451 ms


ShopiMind
324
212 s
655 ms


Media Math
604
210 s
348 ms


Signyfyd
202
209 s
1033 ms


GitHub
818
208 s
254 ms


Adocean
900
207 s
230 ms


Marketo
1,427
200 s
140 ms


Opentag
976
197 s
202 ms


SearchSpring
299
195 s
653 ms


Booking.com
685
190 s
277 ms


Connatix
201
187 s
932 ms


BlueKai
2,526
186 s
74 ms


Sift Science
1,232
184 s
149 ms


Janrain
153
178 s
1167 ms


Crowd Control
2,118
172 s
81 ms


ThreatMetrix
165
171 s
1039 ms


FirstImpression
302
171 s
566 ms


Roxr Software
2,185
170 s
78 ms


Rackspace
561
168 s
299 ms


Adtech (AOL)
214
168 s
783 ms


Siteimprove
1,855
164 s
89 ms


Adverline Board
565
164 s
291 ms


smartclip
393
163 s
415 ms


LoopMe
487
160 s
329 ms


IPONWEB
899
160 s
178 ms


Qubit Deliver
354
159 s
450 ms


Monetate
1,009
159 s
158 ms


Listrak
1,070
155 s
145 ms


Rakuten Marketing
1,817
155 s
85 ms


[24]7
137
151 s
1100 ms


Chitika
711
150 s
211 ms


etracker
1,907
148 s
78 ms


Heap
1,845
147 s
80 ms


Adyoulike
1,119
147 s
132 ms


Flowplayer
418
147 s
352 ms


Refersion
1,021
147 s
144 ms


Bronto Software
1,110
146 s
132 ms


PERFORM
96
144 s
1505 ms


Constant Contact
1,324
142 s
108 ms


Freshdesk
898
141 s
157 ms


SpotXchange
464
139 s
300 ms


AvantLink
156
139 s
888 ms


TagCommander
962
132 s
138 ms


MonetizeMore
111
129 s
1165 ms


AddShoppers
841
128 s
153 ms


eBay
681
128 s
188 ms


Picreel
569
127 s
223 ms


One by AOL
592
125 s
212 ms


WisePops
501
125 s
250 ms


PowerReviews
612
125 s
205 ms


Clerk.io ApS
738
124 s
169 ms


TrafficStars
1,010
123 s
122 ms


Convert Insights
955
123 s
129 ms


Pardot
381
121 s
318 ms


OptiMonk
1,030
120 s
116 ms


Marketplace Web Service
211
119 s
562 ms


StreamRail
72
117 s
1631 ms


Curalate
392
116 s
297 ms


Smarter Click
430
116 s
269 ms


Skype
669
115 s
172 ms


Net Reviews
1,462
115 s
79 ms


Bizible
858
114 s
133 ms


Accuweather
1,510
113 s
75 ms


Interpublic Group
459
111 s
242 ms


Revcontent
961
111 s
116 ms


Mather Economics
554
110 s
199 ms


Global-e
228
108 s
473 ms


Cloudinary
379
107 s
283 ms


Forensiq
494
105 s
212 ms


plista
903
105 s
116 ms


Sparkflow
353
104 s
294 ms


Pictela (AOL)
209
103 s
495 ms


Adnium
333
102 s
307 ms


Snacktools
337
101 s
301 ms


Survicate
540
101 s
187 ms


LinkedIn Ads
633
100 s
158 ms


ExoClick
2,486
100 s
40 ms


Symantec
863
99 s
114 ms


Dailymotion
232
98 s
424 ms


Amplitude Mobile Analytics
1,327
96 s
72 ms


Mobify
222
96 s
431 ms


Polar Mobile Group
477
95 s
200 ms


ZEDO
302
95 s
316 ms


Playbuzz
343
95 s
278 ms


OwnerIQ
749
95 s
127 ms


Livefyre
258
94 s
365 ms


Kargo
77
94 s
1219 ms


Shopgate
395
93 s
235 ms


Foursixty
1,129
92 s
82 ms


ReTargeter
256
92 s
358 ms


Tail Target
877
91 s
104 ms


issuu
797
91 s
114 ms


WebEngage
744
91 s
122 ms


Sidecar
334
91 s
272 ms


Tribal Fusion
1,022
90 s
88 ms


iovation
949
89 s
94 ms


SpringServer
95
89 s
935 ms


Adkontekst
226
88 s
388 ms


The Trade Desk
308
87 s
281 ms


Touch Commerce
131
86 s
660 ms


Cross Pixel Media
449
85 s
190 ms


The Hut Group
290
83 s
287 ms


Geniee
884
82 s
93 ms


Republer
580
82 s
141 ms


Reevoo
380
81 s
214 ms


Affiliate Window
1,043
81 s
77 ms


Bootstrap Chinese network
280
81 s
289 ms


Permutive
587
81 s
137 ms


Gleam
423
79 s
187 ms


Fanplayr
107
79 s
735 ms


Lytics
593
78 s
132 ms


GetResponse
750
78 s
104 ms


Kameleoon
170
77 s
452 ms


Tradelab
807
75 s
92 ms


FoxyCart
321
74 s
232 ms


JustPremium Ads
499
74 s
148 ms


LoyaltyLion
188
73 s
390 ms


Time
252
73 s
290 ms


SnapEngage
985
72 s
73 ms


Profitshare
326
71 s
218 ms


WalkMe
113
71 s
627 ms


PlayAd Media Group
126
71 s
561 ms


Keywee
271
70 s
257 ms


rewardStyle.com
640
69 s
109 ms


Typepad
277
69 s
249 ms


Usabilla
850
69 s
81 ms


Alexa
1,209
68 s
56 ms


WebSpectator
152
68 s
447 ms


Stackla PTY
331
68 s
205 ms


Nativo
588
68 s
115 ms


Adobe Test & Target
51
67 s
1312 ms


Google Plus
567
66 s
117 ms


Yottaa
163
66 s
404 ms


Smart Insight Tracking
700
65 s
93 ms


RebelMouse
56
65 s
1157 ms


Effective Measure
661
65 s
98 ms


Forter
91
64 s
706 ms


Navegg
687
64 s
93 ms


Madison Logic
513
64 s
124 ms


reEmbed
185
63 s
340 ms


Pixalate
186
61 s
330 ms


Branch Metrics
825
61 s
74 ms


ClickDesk
589
61 s
103 ms


ShopRunner
178
60 s
339 ms


Trusted Shops
1,328
59 s
45 ms


Kampyle
438
59 s
135 ms


AdSniper
207
58 s
280 ms


Elastic Ad
612
58 s
94 ms


Simplicity Marketing
150
58 s
385 ms


Evergage
242
56 s
233 ms


Rocket Fuel
668
55 s
83 ms


bRealTime
245
55 s
225 ms


Vimeo
183
55 s
301 ms


Github
373
54 s
145 ms


TRUSTe
492
54 s
110 ms


Autopilot
582
53 s
91 ms


InSkin Media
73
53 s
720 ms


Tencent
409
52 s
128 ms


TruConversion
256
52 s
201 ms


SoundCloud
222
51 s
231 ms


Clicktripz
200
50 s
251 ms


BoldChat
518
49 s
95 ms


Adscale
534
49 s
91 ms


KISSmetrics
529
49 s
92 ms


Kaizen Platform
252
49 s
193 ms


Zanox
379
49 s
128 ms


The ADEX
564
48 s
85 ms


Bootstrap CDN
712
48 s
67 ms


Highcharts
304
48 s
157 ms


DMD Marketing
295
47 s
160 ms


News
158
45 s
288 ms


Omniconvert
472
45 s
94 ms


CNET Content Solutions
74
45 s
602 ms


Onet
107
44 s
411 ms


Weborama
485
44 s
90 ms


Key CDN
234
43 s
183 ms


unpkg
243
42 s
174 ms


Reflektion
135
42 s
313 ms


LightWidget
525
42 s
80 ms


Steelhouse
366
42 s
115 ms


SkyScanner
114
42 s
369 ms


Conversant
80
42 s
523 ms


BlueCava
79
41 s
517 ms


Conversant Tag Manager
189
40 s
210 ms


Pingdom RUM
202
40 s
196 ms


Socialphotos
223
40 s
178 ms


User Replay
76
39 s
517 ms


ReadSpeaker
432
39 s
91 ms


Proper Media
105
39 s
371 ms


fifty-five
254
39 s
152 ms


Snack Media
96
38 s
399 ms


Nend
892
38 s
43 ms


SaleCycle
441
38 s
86 ms


Fresh Relevance
384
38 s
98 ms


TripleLift
230
38 s
163 ms


Riskified
499
37 s
74 ms


Concert
297
37 s
123 ms


Adition
389
36 s
93 ms


Ghostery Enterprise
223
36 s
161 ms


Petametrics
189
36 s
189 ms


Neodata
318
34 s
107 ms


DemandBase
376
34 s
89 ms


DialogTech
321
33 s
103 ms


CDN.net
90
33 s
368 ms


Intercept Interactive
247
33 s
132 ms


CPEx
219
32 s
146 ms


LiveHelpNow
301
32 s
106 ms


Sooqr Search
417
32 s
76 ms


Exponea
312
31 s
101 ms


MLveda
78
31 s
399 ms


Advance Magazine Group
168
29 s
175 ms


Knight Lab
59
29 s
497 ms


Ambassador
209
29 s
140 ms


eXelate
309
29 s
94 ms


Appier
331
29 s
87 ms


Feedbackify
316
29 s
91 ms


Sajari Pty
213
28 s
133 ms


Browsealoud
468
27 s
59 ms


BannerFlow
255
27 s
108 ms


Viacom
138
27 s
198 ms


SublimeSkinz
378
27 s
71 ms


ResponseTap
337
27 s
80 ms


Postcode Anywhere (Holdings)
138
27 s
194 ms


Vee24
101
27 s
262 ms


ResponsiveVoice
341
26 s
77 ms


Cachefly
55
26 s
475 ms


linkpulse
336
26 s
77 ms


NetAffiliation
220
26 s
118 ms


Wow Analytics
123
25 s
206 ms


VidPulse
79
25 s
320 ms


Ipify
227
25 s
110 ms


VoiceFive
183
24 s
134 ms


Hupso Website Analyzer
328
24 s
74 ms


Accordant Media
250
24 s
95 ms


Aggregate Knowledge
314
24 s
75 ms


FLXone
154
23 s
152 ms


SnapWidget
624
23 s
37 ms


Transifex
121
23 s
192 ms


TechTarget
64
23 s
361 ms


Alliance for Audited Media
95
23 s
243 ms


Unruly Media
179
23 s
128 ms


Sourcepoint
104
23 s
217 ms


Sweet Tooth
217
22 s
100 ms


Dynamic Converter
65
21 s
326 ms


cloudIQ
146
21 s
144 ms


OCSP
110
21 s
191 ms


Zarget
201
21 s
103 ms


Simpli.fi
181
21 s
114 ms


Borderfree
66
21 s
311 ms


Moovweb
71
20 s
287 ms


Pusher
117
20 s
174 ms


Resonance Insights
112
20 s
180 ms


Customer.io
175
20 s
115 ms


Video Media Groep
177
20 s
112 ms


Silverpop
236
19 s
80 ms


Edge Web Fonts
243
19 s
78 ms


Sirv
195
19 s
97 ms


FreakOut
284
19 s
65 ms


Vibrant Media
155
18 s
118 ms


LoginRadius
79
18 s
227 ms


AWeber
182
18 s
98 ms


Infinity Tracking
197
18 s
90 ms


Byside
79
18 s
222 ms


Vertical Mass
81
18 s
217 ms


Webtrends
53
17 s
326 ms


FuelX
96
17 s
179 ms


Council ad Network
142
17 s
120 ms


Rakuten LinkShare
122
17 s
139 ms


Delta Projects AB
225
17 s
75 ms


Civic
213
17 s
80 ms


PebblePost
73
17 s
228 ms


AdvertServe
148
16 s
110 ms


Cookie-Script.com
155
16 s
104 ms


AdSpruce
54
16 s
299 ms


Talkable
198
16 s
81 ms


Revolver Maps
150
16 s
106 ms


Fonecall
67
16 s
236 ms


AdRiver
148
16 s
107 ms


Vergic AB
73
16 s
215 ms


StumbleUpon
103
16 s
152 ms


Impact Radius
183
15 s
83 ms


Exactag
138
15 s
109 ms


CleverDATA
142
15 s
105 ms


MaxMind
118
15 s
126 ms


SlimCut Media Outstream
139
15 s
106 ms


GoSquared
193
15 s
76 ms


Exponential Interactive
242
15 s
61 ms


Hull.js
112
14 s
128 ms


Adobe Marketing Cloud
154
14 s
93 ms


The Publisher Desk
62
14 s
226 ms


Woopra
198
14 s
71 ms


CleverTap
168
14 s
83 ms


Vergic Engage Platform
59
14 s
235 ms


Salesforce.com
193
14 s
72 ms


Optimove
84
14 s
164 ms


Polyfill service
91
14 s
150 ms


AnswerDash
90
13 s
145 ms


Extole
92
13 s
139 ms


Widespace
130
13 s
97 ms


AdTrue
113
12 s
110 ms


Auto Link Maker
134
12 s
91 ms


Apester
145
12 s
83 ms


Betgenius
120
12 s
100 ms


DialogTech SourceTrak
177
12 s
67 ms


Drip
225
12 s
52 ms


Storygize
81
12 s
143 ms


SecuredVisit
141
11 s
81 ms


Vindico
96
11 s
117 ms


Opinion Stage
127
11 s
88 ms


BuySellAds
127
11 s
88 ms


CyberSource (Visa)
155
11 s
69 ms


Triblio
68
11 s
157 ms


C3 Metrics
65
11 s
163 ms


Research Online
131
11 s
81 ms


epoq internet services
90
11 s
117 ms


Friendbuy
131
11 s
80 ms


Twitter Online Conversion Tracking
169
10 s
62 ms


AliveChat
157
10 s
66 ms


Flickr
137
10 s
76 ms


AIR.TV
119
10 s
87 ms


Swoop
131
10 s
78 ms


Covert Pics
142
10 s
72 ms


Ziff Davis Tech
121
10 s
84 ms


HotelsCombined
63
10 s
158 ms


Reactful
81
10 s
120 ms


Polldaddy
97
10 s
99 ms


OnScroll
95
10 s
101 ms


Ad6Media
55
9 s
171 ms


Eyeota
124
9 s
75 ms


Boomtrain
106
9 s
87 ms


Pagefair
124
9 s
73 ms


Tag Inspector
100
9 s
90 ms


Braintree Payments
73
9 s
123 ms


CANDDi
73
9 s
123 ms


Adunity
71
9 s
127 ms


Captify Media
99
9 s
87 ms


ContextWeb
117
8 s
73 ms


Mopinion
75
8 s
112 ms


NaviStone
81
8 s
103 ms


Freespee
96
8 s
83 ms


RichRelevance
54
8 s
147 ms


Flockler
74
8 s
104 ms


Qualtrics
51
8 s
151 ms


Attribution
74
7 s
100 ms


Klevu Search
101
7 s
73 ms


UPS i-parcel
52
7 s
140 ms


Nanorep
55
7 s
132 ms


Catchpoint
76
7 s
96 ms


Improve Digital
54
7 s
131 ms


Sailthru
101
7 s
69 ms


Bookatable
70
7 s
99 ms


ARM
53
7 s
129 ms


MathJax
56
7 s
121 ms


Netlify
77
7 s
87 ms


Oracle Recommendations On Demand
86
7 s
76 ms


content.ad
54
6 s
119 ms


Barilliance
74
6 s
86 ms


Cookie Reports
83
6 s
76 ms


YoYo
72
6 s
82 ms


Raygun
69
6 s
85 ms


UpSellit
56
6 s
104 ms


Conversio
76
6 s
75 ms


Site24x7 Real User Monitoring
72
5 s
74 ms


DistroScale
59
5 s
89 ms


Fastly Insights
86
5 s
60 ms


Browser-Update.org
73
5 s
70 ms


Adobe Scene7
63
5 s
80 ms


eGain
65
5 s
77 ms


Bluecore
55
5 s
90 ms


StackAdapt
63
5 s
78 ms


Sociomantic Labs
68
5 s
70 ms


AdCurve
59
5 s
77 ms


MailPlus
78
4 s
56 ms


Fastest Forward
54
4 s
78 ms


Soundest
55
4 s
76 ms


Click4Assistance
52
4 s
80 ms


Xaxis
52
4 s
79 ms


Realytics
52
4 s
78 ms


SurveyMonkey
77
4 s
51 ms


StackExchange
57
4 s
69 ms


PrintFriendly
76
4 s
49 ms


Ekm Systems
60
4 s
61 ms


ShopStorm
55
3 s
62 ms



Future Work

Introduce URL-level data for more fine-grained analysis, i.e. which libraries from Cloudflare/Google CDNs are most expensive.
Expand the scope, i.e. include more third parties and have greater entity/category coverage.

FAQs
I don't see entity X in the list. What's up with that?
This can be for one of several reasons:

The entity does not have references to their origin on at least 50 pages in the dataset.
The entity's origins have not yet been identified. See How can I contribute?

What is ""Total Occurences""?
Total Occurrences is the number of pages on which the entity is included.
How is the ""Average Impact"" determined?
The HTTP Archive dataset includes Lighthouse reports for each URL on mobile. Lighthouse has an audit called ""bootup-time"" that summarizes the amount of time that each script spent on the main thread. The ""Average Impact"" for an entity is the total execution time of scripts whose domain matches one of the entity's domains divided by the total number of pages that included the entity.
Average Impact = Total Execution Time / Total Occurrences

How does Lighthouse determine the execution time of each script?
Lighthouse's bootup time audit attempts to attribute all toplevel main-thread tasks to a URL. A main thread task is attributed to the first script URL found in the stack. If you're interested in helping us improve this logic, see Contributing for details.
The data for entity X seems wrong. How can it be corrected?
Verify that the origins in data/entities.json5 are correct. Most issues will simply be the result of mislabelling of shared origins. If everything checks out, there is likely no further action and the data is valid. If you still believe there's errors, file an issue to discuss futher.

How can I contribute?
Only about 90% of the third party script execution has been assigned to an entity. We could use your help identifying the rest! See Contributing for details.
Contributing
Thanks
A huge thanks to @simonhearne and @soulgalore for their assistance in classifying additional domains!
Updating the Entities
The domain->entity mapping can be found in data/entities.json5. Adding a new entity is as simple as adding a new array item with the following form.
{
    ""name"": ""Facebook"",
    ""homepage"": ""https://www.facebook.com"",
    ""categories"": [""social""],
    ""domains"": [
        ""www.facebook.com"",
        ""connect.facebook.net"",
        ""staticxx.facebook.com"",
        ""static.xx.fbcdn.net"",
        ""m.facebook.com""
    ]
}
Updating Attribution Logic
The logic for attribution to individual script URLs can be found in the Lighthouse repo. File an issue over there to discuss further.
Updating the Data
The query used to compute the origin-level data is in sql/origin-execution-time-query.sql, running this against the latest Lighthouse HTTP Archive should give you a JSON export of the latest data that can be checked in at data/YYYY-MM-DD-origin-scripting.json.
Updating this README
This README is auto-generated from the templates lib/ and the computed data. In order to update the charts, you'll need to make sure you have cairo installed locally in addition to yarn install.
# Install `cairo` and dependencies for node-canvas
brew install pkg-config cairo pango libpng jpeg giflib
Updating the website
The web code is located in www/ directory of this repository. Open a PR to make changes.
",741
MrMaxie/get-google-fonts,JavaScript,"Get Google Fonts
Script downloading CSS file with fonts and adapt it to working in closed environment/offline. Useful for example when project must to be ran in network without connection with internet or when you make application based on projects like Electron.
Getting Started
Script can be ""installed"" manually by cloning ./main.js file or with NPM:
npm install get-google-fonts

Example Result
For example CSS with fonts used like this:
<link href='https://fonts.googleapis.com/css?family=Roboto:400,700&amp;subset=cyrillic' rel='stylesheet'>
Can be replaced with:
<link href='fonts/fonts.css' rel='stylesheet'>
Using in command line
Using in command line it's possible when script is installed as global or you know full path to cli.js file. By default NPM will prefer to install script as global and after that scripts will share file in bin directory named get-google-fonts. Then you can just to use that command anywhere.
Usage:
  get-google-fonts [OPTIONS] [ARGS]

Options:
  -i, --input URL         Input URL of CSS with fonts
  -o, --output [STRING]   Output directory (Default is ./fonts)
  -p, --path [STRING]     Path placed before every source of font in CSS  (Default is ./)
  -c, --css [STRING]      Name of CSS file (Default is fonts.css)
  -t, --template [STRING] Template of font filename (Default is {_family}-{weight}-{comment}{i}.{ext})
  -u, --useragent STRING  User-agent used at every connection
  -q, --quiet             Don't displays a lot of useful information
  -b, --base64            Save fonts inside CSS file as base64 URIs
	  --non-strict-ssl    Force to accepts only valid SSL certificates; in some
						  cases,such proxy or self-signed certificates
						  should be turned off
  -w, --overwriting       Allows overwrite existing files
	  --print-options     Shows result options object without performing any
						  action
  -s, --simulate          Simulation; No file will be saved
  -h, --help              Display help and usage details

To get a result like in Example, just enter the command in the folder with the HTML file:
get-google-fonts -i ""https://fonts.googleapis.com/css?family=Roboto:400,700&subset=cyrillic""

Using in code
Get-google-fonts can be required as module.
const GetGoogleFonts = require('get-google-fonts');
To get result like in Example, just create object and run download method.
new GetGoogleFonts().download('https://fonts.googleapis.com/css?family=Roboto:400,700&subset=cyrillic')
// => Promise
There are three useful methods in all module.
Constructor
Parameters:

config Allows you to preconfigure all downloads done by this object. See more... [optional]

Example:
let ggf_ttf = new GetGoogleFonts({
	userAgent: 'Wget/1.18'
})
let ggf_defaults = new GetGoogleFonts()
download
Parameters:

url URL to CSS as fonts object or plain string. Can be an array of arguments witch will be passed through GetGoogleFonts.constructUrl().
config Allows you to configure this one downloads. See more... [optional]

Example:
ggf.download([
	{
		Roboto: [400, 700]
	},
	['cyrillic']
]).then(() => {
	console.log('Done!')
}).catch(() => {
	console.log('Whoops!')
})
// or
ggf.download('https://fonts.googleapis.com/css?family=Roboto:400,700&subset=cyrillic', {
	userAgent: 'Wget/1.18'
}).then(() => {
	console.log('Done!')
}).catch(() => {
	console.log('Whoops!')
})
constructUrl
Generate URL of Google Fonts using given parameters.
Parameters:

families Object of fonts names and weights
subsets Array of subsets

Example:
GetGoogleFonts.constructUrl([
	{
		Roboto: ['400', 700],
		'Roboto': [400, '700i'],
		'Alegreya Sans SC': [300]
	},
	['cyrillic']
])
// => https://fonts.googleapis.com/css?family=Roboto:400,700,700i|Alegreya+Sans+SC:300&subset=cyrillic
Config object
Objects will be considered as follows:
Download config > GetGoogleFonts object config > Default config
// Default config object
{
	// Output directory when where all files will be saved.
	// According to this path, relative paths will be resolved.
	outputDir:  './fonts',
	// Path placed before every source of font in CSS.
	// It's also can be URL of your website.
	path:       './',
	// Template of font filename.
	template:   '{_family}-{weight}-{comment}{i}.{ext}',
	// Name of CSS file. Like other files
	// will be placed relative to output directory
	cssFile:    'fonts.css',
	// User-agent used at every connection. Accordingly, Google Fonts will
	// send the appropriate fonts. For example, providing a wget's
	// user-agent will end with the download of .ttf fonts.
	// Default user-agent downloads .woff2 fonts.
	userAgent:  'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' +
				'(KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',
	// Save fonts inside CSS file as base64 URIs
	base64:      false,
	// Force to accepts only valid SSL certificates; in some cases,
	// such proxy or self-signed certificates should be turned off
	strictSSL:   true,
	// Allows overwrite existing files.
	overwriting: false,
	// Displays a lot of useful information.
	verbose:     false,
	// Simulation; No file will be saved.
	simulate:    false
}
Template
Following variables can be used in the template:

{comment} Text from comment placed before @font-face. Google place there name of subset e.g. latin
{family} Font-family e.g. Source Sans Pro
{_family} Font-family (whitespace will be replaced with underscore) e.g. Source_Sans_Pro
{weight} Font-weight e.g. 400
{filename} Name of original file e.g. ODelI1aHBYDBqgeIAH2zlC2Q8seG17bfDXYR_jUsrzg
{ext} Original extension e.g. woff2
{i} A number that is incremented one by one each time a font file is added. Useful to preserve the uniqueness of font names in case you are not sure if the previous variables are enough. It starts from 1.

TODO

 Some kind of .lock file for making changes based on the last known state
 CSS scanning ability for self-recognize by script what must to be downloaded (with or without watchers)

License
This project is licensed under the Apache-2.0 License - see the LICENSE.md file for details
",13
indjev99/Small-Projects,C++,"Small Projects
Some small and quick projects of mine.
",2
GoogleCloudPlatform/training-data-analyst,Jupyter Notebook,"training-data-analyst
Labs and demos for Google Cloud Platform courses (http://cloud.google.com/training).
Contributing to this repo

Small edits are welcome! Please submit a Pull-Request. See also CONTRIBUTING.md
For larger edits, please submit an issue, and we will create a branch for you. Then, get the code reviewed (in the branch) before submitting.

Organization of this repo
Try out the code on Google Cloud Platform

Courses
Code for the following courses is included in this repo:
Google Cloud Platform Big Data and Machine Learning Fundamentals
https://cloud.google.com/training/courses/data-ml-fundamentals
GCP Big Data & Machine Learning Fundamentals
Data Engineering on Google Cloud Platform
https://cloud.google.com/training/courses/data-engineering

Serverless Data Analysis
Leveraging unstructured data
Serverless Machine Learning
Resilient streaming systems

Machine Learning on Google Cloud Platform (& Advanced ML on GCP)
https://www.coursera.org/learn/google-machine-learning
https://www.coursera.org/specializations/advanced-machine-learning-tensorflow-gcp

How Google Does ML
Launching into ML
Introduction to TensorFlow
Feature Engineering
Art and Science of ML
End-to-end machine learning on Structured Data
Production ML models
Image Classification Models in TensorFlow
Sequence Models for Time-Series and Text problems
Recommendation Engines using TensorFlow

Blog posts
blogs/
",2225
hodgef/simple-keyboard,JavaScript,"


The easily customisable and responsive on-screen virtual keyboard for Javascript projects.







📦 Installation & Usage
You can use simple-keyboard as a <script> tag from a CDN, or install it from npm.
Check out the Getting Started docs to begin.
📖 Documentation
Check out the simple-keyboard documentation site.
Feel free to browse the Q&A / Use-cases page for advanced use-cases.
🚀 Demo
https://simple-keyboard.com/demo
To run demo on your own computer

Clone this repository
npm install
npm start
Visit http://localhost:3000/

Other versions

React.js
Angular
Vue.js

Questions?

✅ Contributing
PR's and issues are welcome. Feel free to submit any issues you have at:
https://github.com/hodgef/simple-keyboard/issues
",644
myrrlyn/bitvec,Rust,"BitVec – Managing memory bit by bit







Summary
This crate provides data structures which allow working with bool as if it
were truly one bit wide in memory, rather than a u8 with only two valid
values. Currently, it only provides [u1], Box<[u1]>, and Vec<u1>
structures.
In addition to compact memory representation, this crate also allows you to
specify the order in which individual bits are stored in Rust fundamentals, and
which fundamental element (u8, u16, u32, and on 64-bit systems, u64) is
used to store the bits.
The data structures provided by this crate track as closely as possible the APIs
and trait implementations of their proper types in the Rust standard library.
BitSlice corresponds to [bool], BitBox to Box<[bool]>, and BitVec to
Vec<bool>, and each of these types should be drop-in compatible replacements
for their standard library counterparts.
What Makes bitvec Different Than All The Other Bit Vector Crates
The most significant differences are that bitvec provides arbitrary bit
ordering through the Cursor trait, and provides a full-featured slice type.
Other crates have fixed orderings, and are often unable to produce slices that
begin at any arbitrary bit.
Additionally, the bitvec types implement the full extent of the standard
library APIs possible, in both inherent methods and trait implementations.
The bitvec types’ handles are exactly the size of their standard library
counterparts, while the other crates carry bit index information in separate
fields, making their handles wider. Depending on your needs, this may sway your
opinion in either direction. bitvec is more compact, but mangles the internal
pointer representation and requires more complex logic to use the bit region,
while other crates’ types are larger, but have more straightforward internal
logic.
Why Would You Use This

You need to directly control a bitstream’s representation in memory.
You need to do unpleasant things with communications protocols.
You need a list of bools that doesn’t waste 7 bits for every bit used.
You need to do set arithmetic, or numeric arithmetic, on those lists.
You are running a find/replace command on your repository from &[bool] to
&BitSlice, or Vec<bool> to BitVec, and expect minimal damage as a
result.

Why Wouldn’t You Use This
Your concern with the memory representation of bitsets includes compression.
BitSlice performs absolutely no compression, and maps bits directly into
memory. Compressed bit sets can be found in other crates, such as the
compacts crate, which uses the Roaring BitSet format.
Usage
Minimum Rust Version: 1.34.0
I wrote this crate because I was unhappy with the other bit-vector crates
available. I specifically need to manage raw memory in bit-level precision, and
this is not a behavior pattern the other bit-vector crates made easily available
to me. This served as the guiding star for my development process on this crate,
and remains the crate’s primary goal.
To this end, the default type parameters for the BitVec type use u8 as the
storage primitive and use big-endian ordering of bits: the forwards direction is
from MSb to LSb, and the backwards direction is from LSb to MSb.
To use this crate, you need to depend on it in Cargo.toml:
[dependencies]
bitvec = ""0.12""
and include it in your crate root src/main.rs or src/lib.rs:
//  Only if you’re in Rust 2015
#[macro_use]
extern crate bitvec;

use bitvec::prelude::*;
This imports the following symbols:


bitvec! – a macro similar to vec!, which allows the creation of BitVecs
of any desired endianness, storage type, and contents. The documentation page
has a detailed explanation of its syntax.


BitSlice<C: Cursor, T: Bits> – the actual bit-slice reference type. It is
generic over a cursor type (C) and storage type (T). Note that BitSlice
is unsized, and can never be held directly; it must always be behind a
reference such as &BitSlice or &mut BitSlice.
Furthermore, it is impossible to put BitSlice into any kind of intelligent
pointer such as a Box or Rc! Any work that involves managing the memory
behind a bitwise type must go through BitBox or BitVec instead. This may
change in the future as I learn how to better manage this library, but for now
this limitation stands.


BitBox<C: Cursor, T: Bits> – a fixed-size bit collection in owned memory.


BitVec<C: Cursor, T: Bits> – the actual bit-vector structure type. It is
generic over a cursor type (C) and storage type (T). This type is the main
worker of the crate. It supports the full Vec<T> API and trait
implementations, with the exception that (at this time) it is impossible to
take a mutable reference to a single bit. This means that everything except
for let elt: &mut bool = &mut bv[index]; and bv[index] = some_bool(); is
possible to express.


Cursor – an open trait that defines an ordering schema for BitVec to use.
Little and big endian orderings are provided by default. If you wish to
implement other ordering types, the Cursor trait requires one function:

fn at<T: Bits>(index: u8) -> u8 takes a semantic index and computes a bit
offset into the primitive T for it.



BigEndian – a marker type that implements Cursor by defining the forward
direction as towards LSb and the backward direction as towards MSb.


LittleEndian – a marker type that implements Cursor by defining the
forward direction as towards MSb and the backward direction as towards LSb.


Bits – a sealed trait that provides generic access to the four Rust
primitives usable as storage types: u8, u16, u32, and u64. usize
and the signed integers do not implement Bits and cannot be used as the
storage type. u128 also does not implement Bits, as I am not confident in
its memory representation.


BitVec has the same API as Vec, and should be easy to use.
The bitvec! macro can take type information in its first two arguments.
Because macros do not have access to the type checker, it currently only accepts
the literal tokens BigEndian or LittleEndian as the first argument, one of
the four unsigned integer primitives as the second argument, and then as many
values as you wish to insert into the collection. It accepts any integer value,
and maps them to bits by comparing against 0. 0 becomes false and any other
integer, whether it is odd or not, becomes true. While the syntax is loose,
you should only use 0 and 1 to fill the macro, for readability and lack of
surprise.
no_std
This crate can be used in #![no_std] libraries, by disabling the default
feature set. In your Cargo.toml, write:
[dependencies]
bitvec = { version = ""0.12"", default-features = false }
or
[dependencies.bitvec]
version = ""0.12""
default-features = false
This turns off the standard library imports and all usage of dynamic memory
allocation. Without an allocator, the bitvec! and bitbox! macros, and the
BitVec and BitBox types, are all disabled and removed from the library,
leaving only the BitSlice type.
To use bitvec in a #![no_std] environment that does have an allocator,
re-enable the alloc feature, like so:
[dependencies.bitvec]
version = ""0.12""
default-features = false
features = [""alloc""]
The alloc feature restores the owned-memory types and their macros. The only
difference between alloc and std is the presence of the standard library
façade and runtime support.
The std feature includes allocation, so using this crate without any feature
flags or by explicitly enabling the std feature will enable full
functionality.
Serde Support
The serde feature, by default, enables serialization for the BitSlice type.
Enabling the alloc or std features enables both serialization and
deserialization for the BitBox and BitVec types.
The serde feature is opt-in, and requires setting it in your Cargo.toml:
# Cargo.toml

[dependencies.bitvec]
version = ""0.12""
features = [
  ""serde"", # enables serialization
  ""std"", # enables deserialization
]
Example
extern crate bitvec;

use bitvec::prelude::*;

use std::iter::repeat;

fn main() {
    let mut bv = bitvec![BigEndian, u8; 0, 1, 0, 1];
    bv.reserve(8);
    bv.extend(repeat(false).take(4).chain(repeat(true).take(4)));

    //  Memory access
    assert_eq!(bv.as_slice(), &[0b0101_0000, 0b1111_0000]);
    //                   index 0 -^               ^- index 11
    assert_eq!(bv.len(), 12);
    assert!(bv.capacity() >= 16);

    //  Stack operations
    bv.push(true);
    bv.push(false);
    bv.push(true);

    assert!(bv[12]);
    assert!(!bv[13]);
    assert!(bv[14]);
    assert!(bv.get(15).is_none());

    bv.pop();
    bv.pop();
    bv.pop();

    //  Set operations
    bv &= repeat(true);
    bv = bv | repeat(false);
    bv ^= repeat(true);
    bv = !bv;

    //  Arithmetic operations
    let one = bitvec![1];
    bv += one.clone();
    assert_eq!(bv.as_slice(), &[0b0101_0001, 0b0000_0000]);
    bv -= one.clone();
    assert_eq!(bv.as_slice(), &[0b0101_0000, 0b1111_0000]);

    //  Borrowing iteration
    let mut iter = bv.iter();
    //  index 0
    assert_eq!(iter.next().unwrap(), false);
    //  index 11
    assert_eq!(iter.next_back().unwrap(), true);
    assert_eq!(iter.len(), 10);
}
Immutable and mutable access to the underlying memory is provided by the AsRef
and AsMut implementations, so the BitVec can be readily passed to transport
functions.
BitVec implements Borrow down to BitSlice, and BitSlice implements
ToOwned up to BitVec, so they can be used in a Cow or wherever this API
is desired. Any case where a Vec/[T] pair cannot be replaced with a
BitVec/BitSlice pair is a bug in this library, and a bug report is
appropriate.
BitVec can relinquish its owned memory with .into_vec() or
.into_boxed_slice(), and BitSlice can relinquish its borrow by going out
of scope.
Warnings
The BitSlice type is able to cause memory aliasing, as multiple independent
&mut BitSlice instances may use the same underlying memory. This crate takes
care to ensure that all observed behavior is exactly as expected, without any
side effects.
The BitSlice methods only use whole-element instructions when the slice spans
the full width of the element; when the slice has only partial use, the methods
crawl each bit individually. This is slower on most architectures, but
guarantees safety.
Race conditions are avoided through use of the atomic read/modify/write
instructions stabilized in 1.34.0.
Planned Features
Contributions of items in this list are absolutely welcome! Contributions of
other features are also welcome, but I’ll have to be sold on them.

Creation of specialized pointers Rc<BitSlice> and Arc<BitSlice>.
Procedural macros for bitvec! and bitbox!
An FFI module, and bindings from other languages.

",43
haskell-works/hw-json-simd,C,"hw-json-simd
This library/tool will generate semi-indexes on JSON files as per the paper:
Semi-Indexing Semi-Structured Data in Tiny Space.
The command line tool
For a given JSON file, hw-json-simd will generate two semi-index
files, which both together can be loaded into a single in-memory semi-index.
The semi-index files can be generated using two methods, which will be called
standard and simple, which correspond to sections 4 and 5 of the
Semi-Indexing Semi-Structured Data in Tiny Space
paper respectively.
Navigation of the JSON text using the standard index is suppored by the hw-json project for more information.  There is currently
no support for navigation of the JSON text using the simple index.
Using on the command line
cat test.json | pv -t -e -b -a | time hw-json-simd create-index \
  -i /dev/stdin
  --output-ib-file test.json.ib.idx
  --output-bp-file test.json.bp.idx
  --method standard
cat test.json | pv -t -e -b -a | time hw-json-simd create-index \
  -i /dev/stdin
  --output-ib-file test.json.ib.idx
  --output-bp-file test.json.bp.idx
  --method simple
Installation
From hackage
cabal new-install hw-json-simd
",4
AdguardTeam/FiltersRegistry,JavaScript,"AG Filters Registry 
This repository contains the known filters subscriptions available to AdGuard users. We re-host these filters on filters.adtidy.org. Also, these filters can be slightly modified in order to achieve better compatibility with AdGuard.
Filters metadata


template.txt
Template file is used by the filters compiler to prepare the final filter version.


exclude.txt
A list of regular expressions. Rules that match these exclusions will not be included in the resulting filter.


metadata.json
Filter metadata. Includes name, description, etc.

filterId - unique filter identifier (integer);
name - filter name. Can be localized (we'll cover it later);
description - filter description;
timeAdded - time when this filter was added to the registry. Milliseconds since January 1, 1970. You can exec new Date().getTime() in the browser console to get the current time;
homepage - filter website/homepage;
expires - filter's default expiration period;
displayNumber - this number is used when AdGuard sorts available filters (GUI);
groupId - group identifier (see groups description below);
subscriptionUrl - default filter subscription URL
tags - a list of tags (see tags description below)

Metadata example:
{
  ""filterId"": 2,
  ""name"": ""English Filter"",
  ""description"": ""EasyList + AdGuard English filter. This filter is necessary for quality ad blocking."",
  ""timeAdded"": 1404115015843,
  ""homepage"": ""https://kb.adguard.com/en/general/adguard-ad-filters#english"",
  ""expires"": ""2 days"",
  ""displayNumber"": 1,
  ""groupId"": 1,
  ""subscriptionUrl"": ""https://filters.adtidy.org/extension/chromium/filters/2.txt"",
  ""tags"": [
    ""purpose:ads"",
  ""reference:101"",
  ""recommended"",
  ""reference:2""
  ]
}


revision.json
Filter version metadata, automatically filled and overwritten on each build.


filter.txt
Resulting compiled filter.


diff.txt
Build log that contains excluded and converted rules with an explanation.


domains-blacklist.txt
A list of domains to be removed from url-blocking domain modified rules.
Tags
Every filter can be marked by a number of tags. /tags/metadata.json contains every tag metadata.
Example:
  {
    ""tagId"": 1,
    ""keyword"": ""purpose:ads""
  },



lang:* tags
Language-specific filters are marked with one or multiple lang: tags. For instance, AdGuard Russian filter is marked with the lang:ru tag.


purpose:* tags
Determines filters purposes. Please note, that a filter can have multiple purposes. For instance, List-KR is marked with both purpose:ads and purpose:privacy.


recommended tag
Filters that are recommended to use in their category. The category is determined by the pair of the lang: and purpose: tags.


Groups
/groups/metadata.json - filters groups metadata. Each filter should belong to one of the groups.
Filters localization
Translations are here: https://adguard.oneskyapp.com/collaboration/project?id=119172
See scripts/oneskyapp/README.md.
How to build
npm install

Run the following command:
  node index.js

Build with white/black lists:
  node index.js -i=1,2,3 -s=4,5,6

Validate filters.json and filters_i18n.json for platforms:
  node validate ./platforms

",15
mdsteele/tachyomancer,Rust,"Tachyomancer
Tachyomancer is a puzzle game about designing circuit boards to solve problems.
It is still a work in progress.
License
Tachyomancer is licensed under the GNU GPL, version 3.  Tachyomancer is
free software: you can redistribute it and/or modify it under the terms of the
GNU General Public License as published by the Free Software Foundation, either
version 3 of the License, or (at your option) any later version.
Tachyomancer is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
The complete license can be found in the LICENSE file.
",2
grpc/grpc-node,JavaScript,"
gRPC on Node.js
Implementations
For a comparison of the features available in these two libraries, see this document
C-based Client and Server
Directory: packages/grpc-native-core (see here for installation information)
npm package: grpc.
This is the existing, feature-rich implementation of gRPC using a C++ addon. It works on all LTS versions of Node.js on most platforms that Node.js runs on.
Pure JavaScript Client
Directory: packages/grpc-js
npm package: @grpc/grpc-js
This library is currently incomplete and experimental. It is built on the http2 Node module.
This library implements the core functionality of gRPC purely in JavaScript, without a C++ addon. It works on the latest version of Node.js on all platforms that Node.js runs on.
Other Packages
gRPC Protobuf Loader
Directory: packages/proto-loader
npm package: @grpc/proto-loader
This library loads .proto files into objects that can be passed to the gRPC libraries.
gRPC Tools
Directory: packages/grpc-tools
npm package: grpc-tools
Distribution of protoc and the gRPC Node protoc plugin for ease of installation with npm.
gRPC Health Check Service
Directory: packages/grpc-health-check
npm package: grpc-health-check
Health check service for gRPC servers.
",1420
busy-beaver-dev/busy-beaver,Python,"Busy Beaver

  
Chicago Python's Community Engagement Slack bot.
Introduction
With over four thousand members, the Chicago Python Users Group (ChiPy) is one of the largest Python communities in the world. Slack has become the primary method of communication amongst our members in-between events. We developed an open-source Slack bot, codename: Busy Beaver, to increase community engagement.
We released Busy-Beaver on January 10th at the ChiPy monthly meeting. Slides and video recording from the release announcement are available online.
Features
GitHub Activity
Busy-Beaver posts daily summaries of public GitHub activity for registered users in the #busy-beaver channel on the ChiPy Slack. The goal of this feature is to increase engagement by sparking conversations around GitHub activity.
Users sign up for an account by DMing the bot with the phrase: connect. The bot requires users to sign into GitHub to ensure only authorized activity is posted in the channel.
Retweeter
Busy-Beaver retweets posts made to the @ChicagoPython Twitter account in the #at-chicagopython channel on the ChiPy Slack.
Roadmap
We are currently working on additional features to improve ChiPy community engagement. Please join the conversation in #busy-beaver-meta on the ChiPy Slack.
Contributing
Busy-Beaver is always looking for new contributors! Previous open source experience is not required! Please see CONTRIBUTING.md.
Development Notes
Busy-Beaver is an open source project where all artificats (code, Docker image, etc) are online. We use the Twelve-Factor Application Methodology for building services to design the CICD process and to keep information secure.
Web Application Stack

traefik - reverse proxy and load balancer
PostgreSQL
SQLAlchemy
Flask
flask-sqlalchemy
flask-migrate
requests

Tests

pytest
vcr.py
pytest-vcr
pytest-freezegun

vcr.py records cassettes of requests and responses for new tests, and replays them for previously written tests. Make sure to filter credentials
DevOps

Docker
Docker-Compose is how we deploy to production
watchtower (monitors DockerHub, downloads and deploys latest image)
Ansible
DigitalOcean

Services
We are grateful to the following organizations for providing free services to open source projects:

GitHub (code repo, issues)
DockerHub (hosting Docker images)
Travis CI (continuous integration platform)

API Docs
Can make requests to REST endpoints to kick off processes. Currently we are using CRON to run repetitive tasks; this is managed by Ansible to avoid manual configuration.
GitHub Summary Endpoint

Start the process to run a summary by making a POST request to /poll-twitter with Authentication header set to token {token} and JSON body:

{
  ""channel"": ""busy-beaver""
}
Retweeter Endpoint

Check Twitter feed for new posts to share on Slack by making a POST request to /github-summary with Authentication header set to token {token} and JSON body:

{
  ""channel"": ""at-chicagopython""
}
Creating API Account and Token
admin = ApiUser(username=""admin"", token=""abc123!"")
db.session.add(admin)
db.session.commit()
",30
PineconePi/PineconePi_ONE,C,"PineconePi ONE（Click to enter the website，Support：support@pineconepi.cn)
Pinecone Pie ONE, open your geek heart: with Synwit SWM320VET7 Chip; low cost, small size, size only 48 mm X31 mm; pin full extraction; breadboard direct insertion; support for MicroPython; ARM-Cortex-M4 kernel; hardware one-cycle multiplication; 512KB FLASH, 128KB SRAM; peripheral cross-mapping; maximum resolution 1024*768 TFT-LCD driver; rich hardware Resources: 1 group of 32-bit watchdog timers, 6 groups of 32-bit universal timers, 1 group of 32-bit dedicated pulse width measurement timers, 12-channel 16-bit PWM generator, 2 8-channel 12-bit, successive approximation ADC module of IMSPS, onboard Ch330N. Rich chip information; multi-dock support.
List

Board:Extension board data
Chipbook:Chip Datesheet
Document:Teaching documents
ISP_Download:ISP download software
JLINK_Software:Software about J-LINK
Keil Pack:Keil Pack
Library：Library functions (long-term updates)
MicroPython：MicroPython of PineconePi ONE
RTOS：RTOS on the PineconePi ONE
Schematic diagram and PCB doc：Schematic diagram and PCB doc about PineconePi ONE
SynwitIAR：SynwitIAR Software

PineconePi ONE（点我进入官网，官方交流企鹅群：481227232)
Pinecone Pi ONE|松果派ONE开启你的极客之心：搭载华芯微特SWM320处理器；低成本，小体积，尺寸仅48 mm x31 mm；引脚全引出；面包板直插；支持MicroPython;ARM-Cortex-M4内核；硬件单周期乘法；512KB FLASH,128KB SRAM；外设交叉映射；最高可支持分辨率1024*768TFT-LCD驱动；丰富的硬件资源：1组32位看门狗定时器，6组32位通用定时器，1组32位专用脉冲宽度测量定时器，12通道16位PWM发生器，2个8通道12位，IMSPS的逐次逼近型ADC模块；板载Ch330N。丰富的芯片资料；多扩展坞支持。
目录结构

Board:配套扩展坞资料
Chipbook:芯片手册
Document:教学文档
ISP_Download:ISP下载软件
JLINK_Software:JLINK相关软件
Keil Pack:Keil器件包
Library：库函数（长期更新）
MicroPython：MicroPython源码及固件
RTOS：将各种RTOS移植到PineconePi ONE
Schematic diagram and PCB doc：原理图与封装库
SynwitIAR：华芯微特SynwitIAR软件

",12
dguenms/Dawn-of-Civilization,C++,"Dawn of Civilization
The Dawn of Civilization mod for Civilization IV: Beyond the Sword, based on the popular Rhye's and Fall of Civilization mod.
Overview
Rhye's and Fall of Civilization is one of the most popular mods for Civilization IV: Beyond the Sword.
Rhye's and Fall intends to turn Civilization into a history simulator: the game is played on a realistic map of the world, civilizations start at historically accurate dates, the AI is encouraged to behave historically, and players can win the games by completing their historical victory.
Dawn of Civilization started out as a RFC modmod making several changes suited to my personal preferences.
But it has since grown way beyond that.
With RFC not being in active development anymore, DoC has taken over to carry on its torch of providing historical BtS gameplay and now stands as its own full-fledged mod.
As the spiritual successor of Rhye's and Fall, Dawn of Civilization seeks to continue to improve upon its basic formula, and become an even better history simulator while also provide engaging and enjoyable gameplay.
For more information, you can also go to the main thread on the development forums.
Installation
If you just want to play, you only need to clone this repo into your mod folder (Beyond the Sword/Mods/) and rename it to ""RFC Dawn of Civilization"".
Alternate ways to get the mod:

Installer for DoC including Modules
Installer for DoC without Modules

Main Features
The main goals of DoC are:

Representing parts of the world not represented by RFC
Improving on historical accuracy without sacrificing gameplay
Offering new content in the form of civilizations and victory types
Improving and optimizing existing RFC features
Adjusting base game elements to better suit a historical mod

Rhye's and Fall was created more with the intention of taking the normal BtS content into a framework that makes it a historical simulator.
Dawn of Civilization looks beyond that and tries to make an even better history simulator by including content beyond what BtS offered.
This is most evident in the inclusion of additional civilizations, but also in many rule and mechanic changes.
In my opinion, a good historical simulator always also has to be a good game.
This is why perfect historical accuracy isn't the goal of DoC - playing this game is about making interesting decisions and seeing their impact on the world.
So instead of forced or scripted historical events, DoC tries to include events and mechanics that encourage developments along historical lines without making them a certainty.
DoC also isn't a kitchen sink mod - every feature included has to serve a clear purpose in improving historicity or gameplay experience.
All of this shows in the following main features of Dawn of Civilization:

Inclusion of Beyond the Sword civilizations that have been left out: Korea, Byzantium, Holy Rome
Many new, completely playable civilizations: Harappa, Polynesia, Tamils, Tibet, Indonesia, Moors, Poland, Italy, Mughals, Thailand, Congo, Argentina, Brazil and Canada
Certain civilizations can be reborn to create a completely new game experience: Persia returns as Iran, the Aztecs return as Mexico and the Maya return as Colombia
Completely new tech tree covering 141 technologies over 7 eras
Added 23 new and changed 17 existing buildings
Added 20 new and redesigned several existing units
Completely new selection of 42 civics from 6 categories
You can play from 1700 AD in a new scenario
Improved the interface in various ways by including the BUG mod
You can play on Epic or Marathon game speed thanks to embryodead's RFC Epic/Marathon modmod
Two new difficulty levels featuring stronger AI opponents
Almost every already existing RFC civilization will experience an entirely different game due to changes in their UP, new UHV goals or general starting situation and environment
Replaced Carthage with Phoenicia, starting in 1200 BC in the Levant
Different spawn dates: India (1500 BC), Japan (525 AD), Netherlands (1580 AD) and Germany (1700 AD)
New stability system with more transparent factors, gradual effects of instability, and improved performance
Diplomatic institutions overhaul: new rules and resolutions for the Apostolic Palace and United Nations, extended and optimized Congress mechanics
New religions: Zoroastrianism, Orthodoxy, Catholicism and Protestantism. Christianity is founded as the Orthodox faith, with Catholicism and Protestantism branching off through specific events (the Great Schism and the Reformation, respectively)
New religious spread rules, with religions gradually spreading around their holy city depending on historical spread in various regions on the map. Owner state religion can influence which religions can appear, and religions can disappear when the owner state religion changes
Improved colonization of Africa and Asia: building the Trading Company grants conquerors to certain civilizations, and Congresses can be used to settle empty territory
You can win through a new victory type, the Unique Religious Victory. Different victory conditions exist depending on your state religion. Without a state religion, you can still win the Polytheism (Pantheon civic) and Secularism (Secularism civic) victories
Various map changes to accomodate other additions. In particular, China, India, Scandinavia and Canada have been completely redone
Cultural control spreads gradually to individual tiles
Inclusion of the SuperSpies mod: Spies acquire experience and promotions, Great Spies are created from Spy experience in the same way Great Generals are
Dynamic soundtrack based on your location and state religion
New specialist and great person type: Statesman and Great Statesman
Female Great People with appropriate graphics
Many new leaderheads have been added for all civilizations to cover all eras
Additional requirements for wonders, such as civics, resources and religions, to enocurage their historical placement
The plague can't kill units anymore
The AI receives help in many ways so it can better create historical empires, especially for Greece, Persia, Rome, Arabia and Mongolia
Many small changes to improve balance and AI performance

",40
Atlantis-Ecosystem-Model/ReactiveAtlantis,R,"

ReactiveAtlantis
ReactiveAtlantis is a R-package builded using the Shiny
package as the main platform for the reactive programming approach.
ReactiveAtlantis has several tools that were created to help in the tuning,
parameterization and analysis of the processes and parameters most often modified
during the calibration of Atlantis (e.g. growth rate, predation, recruitment,
Audzijonyte et al. 2017. Among the processes performed by this
package are:

Visualization and analysis of the input, output and initial conditions of an Atlantis model.
Interactive modification of Atlantis configuration files.
Simulation of new parameters to help in the calibration on an Atlantis model.
Execution of a model skill assessment, to evaluate the performance of the model
to reflect the observed data.

Getting Started
These instructions will give you access to use the R-package ReactiveAtlantis. If
you have some problem for your installation, please let me know and I will try to
solve it as soon as possible.
Prerequisites and installation
What things you need to install To run ReactiveAtlantis on R.
# install packages
install.packages('devtools')   ## you need to do this step just once
# running
library(""devtools"")
install_github('Atlantis-Ecosystem-Model/ReactiveAtlantis', force=TRUE, dependencies=TRUE)
library(""ReactiveAtlantis"")
Running ReactiveAtlantis
Compare outputs and Biomass visualization
nc.current  <- 'your_current_output.nc'
nc.old      <- 'your_previous_output.nc'
grp.csv     <- 'your_groups_definition_file.csv'
bgm.file    <- 'your_spatial_configuration_file.bgm'
cum.depths  <- c(0, 20, 50, 150, 250, 400, 650, 1000, 4300) ## This should be the cummulative depth of your model
## individual file
compare(nc.current, nc.out.old = NULL, grp.csv, bgm.file, cum.depths)
## compare to previuos run
compare(nc.current, nc.old, grp.csv, bgm.file, cum.depths)
Predation analysis from the Atlantis output
biom        <- 'your_BiomIndx.txt'
diet.file   <- 'your_DietCheck.txt'
bio.age     <- 'your_AgeBiomIndx.txt' ## optional file. just if you want to check the predation by age
grp.csv     <- 'your_groups_definition_file.csv'
## Predation by Age
predation(biom, grp.csv, diet.file, bio.age)
## No predation by Age
predation(biom, grp.csv, diet.file, bio.age = NULL)

Exploring predator-prey interactions from the initial conditions
prm.file    <- 'your_prm_file.prm'
nc.initial  <- 'your_initial_conditions.nc'
grp.csv     <- 'your_groups_definition_file.csv'
bgm.file    <- 'your_spatial_configuration_file.bgm'
cum.depths  <- c(0, 20, 50, 150, 250, 400, 650, 1000, 4300) ## This should be the cummulative depth of your model
feeding.mat(prm.file, grp.file, nc.initial, bgm.file, cum.depths)
Atlantis food web and trophic level composition
grp.csv     <- 'your_groups_definition_file.csv'
prm.file    <- 'your_prm_file.prm'
diet.file   <- 'your_DietCheck.txt'
food.web(diet.file, grp.file)
## optional you can explore the food web by polygon
food.web(diet.file, grp.file, diet.file.bypol)
## diet.file.bypol Detailed diet check file, this can be obtained as an extra output from Atlantis ""DetailedDietCheck.txt"". To get this file from Atlantis turn on the option ""flagdietcheck"" on the Run.prm file.
Growth of primary producers and limiting factors
nc.initial  <- 'your_initial_conditions.nc'
nc.current  <- 'your_current_output.nc'
grp.csv     <- 'your_groups_definition_file.csv'
prm.file    <- 'your_prm_file.prm'
growth.pp(nc.initial, grp.csv, prm.file, nc.current)
Analysis of recruitment and primary production
nc.initial  <- 'your_initial_conditions.nc'
nc.current  <- 'your_current_output.nc'
yoy.file    <- 'your_yoy_file.txt'
grp.csv     <- 'your_groups_definition_file.csv'
prm.file    <- 'your_prm_file.prm'
recruitment.cal(nc.initial, nc.current, yoy.file, grp.file, prm.file)
Harvest outputs and model skill assessment
catch.nc    <- 'your_output_CATCH.nc'
ext.catch   <- 'external_catch_time_serie.csv'
cum.depths  <- c(0, 20, 50, 150, 250, 400, 650, 1000, 4300) ## This should be the cummulative depth of your model
fsh.csv     <- 'your_fisheries_definition_file.csv'
bgm.file    <- 'your_spatial_configuration_file.bgm'
grp.csv     <- 'your_groups_definition_file.csv'
catch(grp.csv, fsh.csv, catch.nc, ext.catch)
Authors

Javier Porobic

License
This project is licensed under GPL3
",2
haskell-works/bits-extra,Haskell,"bits-extra


Useful bitwise operations.
This library exposes support for some BMI2 CPU instructions on some x86 based
CPUs.
Currently support operations include:

pdep - Parallel Deposit
pext - Parallel Extract

These operations are useful for high performance bit manipulation for
applications such as succinct data structures.
In the case where the target CPU architectures do not support these
instruction set, a very slow emulated version of these operations is
provided instead.
Note ghc-8.4.1 is required to target the BMI2 CPU instructions.
Compilation
It is sufficient to build, test and benchmark the library as follows
for emulated behaviour:
stack build
stack test
stack bench

To target the BMI2 instruction set, add the bmi2 flag:
stack build --flag bits-extra:bmi2
stack test  --flag bits-extra:bmi2
stack bench --flag bits-extra:bmi2

Benchmark results
Benchmarks with bmi2 flag defined on ghc-8.4.1 on Intel Core i7 2.9 GHz
Benchmark bench: RUNNING...
Fast pdep enabled: True
Fast pext enabled: True
benchmarking popCount-single/popCount64
time                 6.698 ns   (6.634 ns .. 6.766 ns)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 6.750 ns   (6.692 ns .. 6.841 ns)
std dev              243.5 ps   (195.3 ps .. 347.1 ps)
variance introduced by outliers: 60% (severely inflated)

benchmarking popCount-single/popCount32
time                 5.543 ns   (5.511 ns .. 5.574 ns)
                     1.000 R²   (0.999 R² .. 1.000 R²)
mean                 5.596 ns   (5.546 ns .. 5.650 ns)
std dev              177.0 ps   (144.8 ps .. 215.8 ps)
variance introduced by outliers: 54% (severely inflated)

benchmarking pdep-vector/popCount64
time                 1.083 μs   (1.074 μs .. 1.097 μs)
                     0.998 R²   (0.997 R² .. 1.000 R²)
mean                 1.089 μs   (1.075 μs .. 1.104 μs)
std dev              50.20 ns   (37.87 ns .. 70.22 ns)
variance introduced by outliers: 62% (severely inflated)

benchmarking pdep-vector/popCount32
time                 1.074 μs   (1.065 μs .. 1.083 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 1.079 μs   (1.071 μs .. 1.090 μs)
std dev              31.57 ns   (23.09 ns .. 49.88 ns)
variance introduced by outliers: 40% (moderately inflated)

benchmarking pdep-single/pdep64
time                 4.920 ns   (4.856 ns .. 5.007 ns)
                     0.996 R²   (0.990 R² .. 1.000 R²)
mean                 4.945 ns   (4.874 ns .. 5.125 ns)
std dev              361.5 ps   (150.9 ps .. 703.7 ps)
variance introduced by outliers: 86% (severely inflated)

benchmarking pdep-single/pdep32
time                 5.203 ns   (5.163 ns .. 5.244 ns)
                     1.000 R²   (0.999 R² .. 1.000 R²)
mean                 5.229 ns   (5.193 ns .. 5.268 ns)
std dev              126.2 ps   (109.9 ps .. 148.6 ps)
variance introduced by outliers: 41% (moderately inflated)

benchmarking pdep-vector/pdep64
time                 1.627 μs   (1.617 μs .. 1.641 μs)
                     0.999 R²   (0.998 R² .. 0.999 R²)
mean                 1.679 μs   (1.652 μs .. 1.723 μs)
std dev              107.2 ns   (74.61 ns .. 155.4 ns)
variance introduced by outliers: 75% (severely inflated)

benchmarking pdep-vector/pdep32
time                 1.626 μs   (1.621 μs .. 1.634 μs)
                     1.000 R²   (0.999 R² .. 1.000 R²)
mean                 1.637 μs   (1.624 μs .. 1.651 μs)
std dev              44.27 ns   (35.00 ns .. 58.61 ns)
variance introduced by outliers: 35% (moderately inflated)

benchmarking pext-single/pext64
time                 5.182 ns   (5.133 ns .. 5.239 ns)
                     0.999 R²   (0.998 R² .. 0.999 R²)
mean                 5.255 ns   (5.181 ns .. 5.371 ns)
std dev              314.1 ps   (211.7 ps .. 441.7 ps)
variance introduced by outliers: 81% (severely inflated)

benchmarking pext-single/pext32
time                 5.269 ns   (5.222 ns .. 5.340 ns)
                     0.997 R²   (0.993 R² .. 1.000 R²)
mean                 5.315 ns   (5.248 ns .. 5.513 ns)
std dev              388.9 ps   (145.5 ps .. 713.4 ps)
variance introduced by outliers: 86% (severely inflated)

benchmarking pext-vector/pext64
time                 1.647 μs   (1.632 μs .. 1.667 μs)
                     0.998 R²   (0.996 R² .. 0.999 R²)
mean                 1.673 μs   (1.649 μs .. 1.719 μs)
std dev              107.5 ns   (67.19 ns .. 171.9 ns)
variance introduced by outliers: 76% (severely inflated)

benchmarking pext-vector/pext32
time                 1.626 μs   (1.616 μs .. 1.637 μs)
                     1.000 R²   (0.999 R² .. 1.000 R²)
mean                 1.630 μs   (1.617 μs .. 1.649 μs)
std dev              53.18 ns   (33.93 ns .. 80.86 ns)
variance introduced by outliers: 44% (moderately inflated)

benchmarking plus-vector/plus64
time                 1.641 μs   (1.628 μs .. 1.657 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 1.647 μs   (1.635 μs .. 1.666 μs)
std dev              50.67 ns   (38.54 ns .. 76.92 ns)
variance introduced by outliers: 41% (moderately inflated)

benchmarking plus-vector/plus32
time                 1.943 μs   (1.932 μs .. 1.953 μs)
                     1.000 R²   (0.999 R² .. 1.000 R²)
mean                 1.948 μs   (1.934 μs .. 1.961 μs)
std dev              45.86 ns   (37.10 ns .. 59.21 ns)
variance introduced by outliers: 29% (moderately inflated)

Benchmark bench: FINISH

Benchmarks with bmi2 flag NOT defined on ghc-8.4.1 on Intel Core i7 2.9 GHz
Benchmark bench: RUNNING...
Fast pdep enabled: False
Fast pext enabled: False
benchmarking popCount-single/popCount64
time                 6.572 ns   (6.433 ns .. 6.765 ns)
                     0.996 R²   (0.992 R² .. 0.999 R²)
mean                 6.637 ns   (6.551 ns .. 6.758 ns)
std dev              346.9 ps   (252.8 ps .. 542.9 ps)
variance introduced by outliers: 76% (severely inflated)

benchmarking popCount-single/popCount32
time                 5.196 ns   (5.152 ns .. 5.253 ns)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 5.232 ns   (5.186 ns .. 5.293 ns)
std dev              186.6 ps   (152.8 ps .. 237.0 ps)
variance introduced by outliers: 60% (severely inflated)

benchmarking pdep-vector/popCount64
time                 5.409 μs   (5.344 μs .. 5.476 μs)
                     0.998 R²   (0.997 R² .. 0.999 R²)
mean                 5.458 μs   (5.391 μs .. 5.549 μs)
std dev              258.1 ns   (193.1 ns .. 374.6 ns)
variance introduced by outliers: 60% (severely inflated)

benchmarking pdep-vector/popCount32
time                 3.849 μs   (3.814 μs .. 3.885 μs)
                     0.999 R²   (0.999 R² .. 0.999 R²)
mean                 3.860 μs   (3.818 μs .. 3.919 μs)
std dev              166.6 ns   (131.0 ns .. 236.7 ns)
variance introduced by outliers: 56% (severely inflated)

benchmarking pdep-single/pdep64
time                 10.46 ns   (10.35 ns .. 10.58 ns)
                     0.999 R²   (0.999 R² .. 0.999 R²)
mean                 10.53 ns   (10.42 ns .. 10.65 ns)
std dev              385.2 ps   (321.8 ps .. 480.9 ps)
variance introduced by outliers: 60% (severely inflated)

benchmarking pdep-single/pdep32
time                 10.57 ns   (10.49 ns .. 10.68 ns)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 10.69 ns   (10.57 ns .. 10.80 ns)
std dev              404.1 ps   (341.0 ps .. 501.8 ps)
variance introduced by outliers: 62% (severely inflated)

benchmarking pdep-vector/pdep64
time                 3.113 μs   (3.091 μs .. 3.140 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 3.130 μs   (3.106 μs .. 3.158 μs)
std dev              82.60 ns   (65.72 ns .. 103.1 ns)
variance introduced by outliers: 32% (moderately inflated)

benchmarking pdep-vector/pdep32
time                 3.096 μs   (3.070 μs .. 3.125 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 3.086 μs   (3.059 μs .. 3.114 μs)
std dev              91.30 ns   (77.41 ns .. 110.3 ns)
variance introduced by outliers: 37% (moderately inflated)

benchmarking pext-single/pext64
time                 73.06 ns   (72.35 ns .. 73.84 ns)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 73.65 ns   (73.02 ns .. 74.35 ns)
std dev              2.368 ns   (1.974 ns .. 2.918 ns)
variance introduced by outliers: 50% (severely inflated)

benchmarking pext-single/pext32
time                 74.07 ns   (73.46 ns .. 74.62 ns)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 74.18 ns   (73.41 ns .. 74.83 ns)
std dev              2.550 ns   (2.161 ns .. 3.134 ns)
variance introduced by outliers: 54% (severely inflated)

benchmarking pext-vector/pext64
time                 72.78 μs   (71.21 μs .. 74.87 μs)
                     0.996 R²   (0.993 R² .. 0.999 R²)
mean                 72.17 μs   (71.38 μs .. 73.41 μs)
std dev              3.105 μs   (2.302 μs .. 4.741 μs)
variance introduced by outliers: 46% (moderately inflated)

benchmarking pext-vector/pext32
time                 72.69 μs   (72.04 μs .. 73.28 μs)
                     0.999 R²   (0.999 R² .. 0.999 R²)
mean                 73.00 μs   (72.25 μs .. 73.85 μs)
std dev              2.634 μs   (2.237 μs .. 3.137 μs)
variance introduced by outliers: 37% (moderately inflated)

benchmarking plus-vector/plus64
time                 1.856 μs   (1.833 μs .. 1.876 μs)
                     0.999 R²   (0.999 R² .. 0.999 R²)
mean                 1.861 μs   (1.844 μs .. 1.880 μs)
std dev              63.39 ns   (54.74 ns .. 75.83 ns)
variance introduced by outliers: 46% (moderately inflated)

benchmarking plus-vector/plus32
time                 1.547 μs   (1.532 μs .. 1.562 μs)
                     0.999 R²   (0.998 R² .. 0.999 R²)
mean                 1.554 μs   (1.538 μs .. 1.578 μs)
std dev              62.25 ns   (48.22 ns .. 91.40 ns)
variance introduced by outliers: 54% (severely inflated)

Benchmark bench: FINISH

Reference

Bit Manipulation Instruction Sets (Wikipedia)
GHC differential D4236

Acknowledgement
A lot of people helped in the writing of this library and the underlying GHC patch:

Ben Gamari
Moritz Angermann
Alex Mason
Moritz Kiefer

",4
cul-it/blacklight-cornell,Ruby,"![Build Status](https://newjenkins.library.cornell.edu/buildStatus/icon?job=Continuous Test - blacklight-cornell)
==  Cornell University implementation of Blacklight
",5
krivenko/pomerol2triqs,Python,"pomerol2triqs

Quick and dirty TRIQS wrapper around the Pomerol exact diagonalization library.
To learn how to use this wrapper, see example subdir in the source directory.
Features

Diagonalization of finite fermionic models with Hamiltonians written in terms of second quantization operators.
Calculation of single-particle Green's functions: G(\tau), G(i\omega_n), G(\omega).
Calculation of two-particle Green's functions: G(\omega;\nu,\nu') and G(\omega;\ell,\ell').

Notation for the two-particle Green's functions is adopted from the
PhD thesis of Lewin Boehnke.
Installation

Install the latest version of Pomerol exact diagonalization library (master branch).
Install the TRIQS library version 2.1.0.
source <path_to_triqs_install_dir>/share/cpp2pyvars.sh
source <path_to_triqs_install_dir>/share/triqsvars.sh
git clone https://github.com/krivenko/pomerol2triqs.git pomerol2triqs.git
mkdir pomerol2triqs.build && cd pomerol2triqs.build
cmake ../pomerol2triqs.git -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=<path_to_install_dir> -DPOMEROL_PATH=<path_to_pomerol_install_dir>
make
make test
make install

License
Copyright (C) 2017-2019 Igor Krivenko <igor.s.krivenko @ gmail.com>
With contributions from Hugo U.R. Strand
This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.
This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.
You should have received a copy of the GNU General Public License
along with this program.  If not, see http://www.gnu.org/licenses/.
",4
renich/LeyesMexicanas,Shell,"
Leyes Mexicanas

Descripción
Compendio de leyes de los Estados Unidos Mexicanos; en texto. Ésta se actualiza, automáticamente, a diario; a las 00:00 GMT.
Chécate el Índice.

Contexto
El Gobierno Mexicano, actualmente, ofrece las leyes por medio del sitio oficial de los Diputados Federales:
http://www.diputados.gob.mx/.
Por lo pronto, solo se ofrece en formatos no-libres y, en algunos casos, en HTML.
Ésto, hace muy difícil saber qué cambios específicos se hicieron a las leyes; en qué día y en qué líneas de la misma ley; haciendo
difícil la comparación de los cambios que surgen con el tiempo.
En el mundo del Software Libre y de Código Abierto; o FOSS, existen herramientas para:

Control de versiones.
Conversión de formatos cerrados a formatos abiertos.

Github es una red social que permite publicar proyectos abiertos; sin costo alguno; por medio de la herramienta Git.

Procedimiento

Se hace un sondeo por la página de los diputados para capturar la lista de archivos que contiene.
Se descarga cada archivo de la lista.
Se convierten los archivos a formato de téxto; utilizando el programa pdftotext.
Se sincronizan los documentos con el repositorio de Git; haciendo un commit del día.
Se publican los cambios en la página de Github.


Autor:
Renich Bon Ciric <renich@woralelandia.com>
Licencia:
GPLv3 o >

",8
yyano/Qiita-News,None,"Qiita-News
",6
MariaDB/mariadb.org-tools,Python,"mariadb.org-tools
This project collects together various small scripts or projects that are used for MariaDB development, but for one reason or another do not make sense to keep inside the main MariaDB source tree.
",17
aragonopendata/local-data-aragopedia,Java,"local-data-aragopedia
Repositorio en el que se incluye el código fuente y los datos correspondientes al proyecto de extracción y transformación de informes y macro-datos del Instituto Aragonés de Estadística (IAEst) para el proyecto Open Data Aragón.
Para un correcto uso del código se necesita tener una credencial del https://console.developers.google.com
Pasos a realizar para la descarga masiva de datos:

Se deben utilizar los scripts que se encuentra en la carpeta src/01_extraction-scripts, en el orden que se indica. Ojo, que hay que tener cuidado con las cookies, que deben ser actualizadas cada vez que se haga un proceso de extracción masiva.

Pasos a realizar para el procesamiento de datos:

Primero se debe de configurar el archivo system.properties espeficicando los parámetros que pide del Google Drive. Una vez hecho esto, en Constants se pondrá la variable publicDrive a true.
Se ejecuta la clase GenerateConfig.
Si se ha configurado que se publique en Google Drive, se debe descargar la configuración en formato Excel a una nueva carpeta.
Se ejecuta la clase GenerateData.
Se suben los archivos procesados a la base de datos de tripletas
Se configura el linked data API

URLs de la API:
La documentación correspondiente usando Swagger se está subiendo a http://opendata.aragon.es/doc/

Todas las observaciones: http://opendata.aragon.es/recurso/iaest/observacion
Observación específica: http://opendata.aragon.es/recurso/iaest/observacion/{idCubo}/{idObservation}
Todos los dsd (Data Structure Definitions): http://opendata.aragon.es/recurso/iaest/dsd
Estructura de un cubo específico: http://opendata.aragon.es/recurso/iaest/dsd/{idCubo}
Todas los propiedades: http://opendata.aragon.es/recurso/iaest/property
Dimension específica: http://opendata.aragon.es/recurso/iaest/dimension/{idDimension}
Medida específica: http://opendata.aragon.es/recurso/iaest/medida/{idMedida}
Codelist: http://opendata.aragon.es/kos/iaest/{idCodelist}
Concepto específico de una codelist: http://opendata.aragon.es/kos/iaest/{idCodelist}/{valor}
Todos los cubos: http://opendata.aragon.es/recurso/iaest/dataset
Todas las observaciones del cubo: http://opendata.aragon.es/recurso/iaest/dataset/{idCubo}
Cubos que tienen un valor dado para una dimensión dada: http://opendata.aragon.es/recurso/iaest/cubosdimension/{dimension}/{valor}
Cubos que tienen un valor dado para una medida dada: http://opendata.aragon.es/recurso/iaest/cubosmedida/{medida}/{valor}
Cubos que tienen un valor dado para alguna dimensión: http://opendata.aragon.es/recurso/iaest/cubosdimensionvalor/{valor}
Cubos que tienen un valor dado para alguna medida: http://opendata.aragon.es/recurso/iaest/cubosmedidavalor/{valor}
Cubos que tienen una dimensión dada: http://opendata.aragon.es/recurso/iaest/cubosdimensionpropiedad/{dimension}
Cubos que tienen una medida dada: http://opendata.aragon.es/recurso/iaest/cubosmedidapropiedad/{medida}
Cubos que tienen una comcarca para una dimensión específica http://opendata.aragon.es/recurso/iaest/cubosComarca/{valor}
Cubos que tienen un municipio para una dimensión específica http://opendata.aragon.es/recurso/iaest/cubosMunicipio/{valor}
Cubos que tienen una provincia para una dimensión específica http://opendata.aragon.es/recurso/iaest/cubosProvincia/{valor}
Cubos que tienen una comunidad autónoma para una dimensión específica http://opendata.aragon.es/recurso/iaest/cubosComunidadAutonoma/{valor}

Consultas a realizar en el SPARQL endpoint
En Consultas se pueden analizar estas consultas
",3
dformoso/machine-learning-mindmap,None,"Machine Learning Mindmap / Cheatsheet
A Mindmap summarising Machine Learning concepts, from Data Analysis to Deep Learning.
Overview
Machine Learning is a subfield of computer science that gives computers the ability to learn without being explicitly programmed. It explores the study and construction of algorithms that can learn from and make predictions on data.
Machine Learning is as fascinating as it is broad in scope. It spans over multiple fields in Mathematics, Computer Science, and Neuroscience. This is an attempt to summarize this enormous field in one .PDF file.
Download
Download the PDF here:

https://github.com/dformoso/machine-learning-mindmap/blob/master/Machine%20Learning.pdf

Same, but with a white background:

https://github.com/dformoso/machine-learning-mindmap/blob/master/Machine%20Learning%20-%20White%20BG.pdf

I've built the mindmap with MindNode for Mac. https://mindnode.com
Companion Notebook
This Mindmap/Cheatsheet has a companion Jupyter Notebook that runs through most of the Data Science steps that can be found at the following link:

https://github.com/dformoso/sklearn-classification

Mindmap on Deep Learning
Here's another mindmap which focuses only on Deep Learning

https://github.com/dformoso/deeplearning-mindmap

1. Process
The Data Science it's not a set-and-forget effort, but a process that requires design, implementation and maintenance. The PDF contains a quick overview of what's involved. Here's a quick screenshot.

2. Data Processing
First, we'll need some data. We must find it, collect it, clean it, and about 5 other steps. Here's a sample of what's required.

3. Mathematics
Machine Learning is a house built on Math bricks. Browse through the most common components, and send your feedback if you see something missing.

4. Concepts
A partial list of the types, categories, approaches, libraries, and methodology.

5. Models
A sampling of the most popular models. Send your comments to add more.

References
I'm planning to build a more complete list of references in the future. For now, these are some of the sources I've used to create this Mindmap.
 Stanford and Oxford Lectures. CS20SI, CS224d.
> Books: 
  > Deep Learning - Goodfellow. 
  > Pattern Recognition and Machine Learning - Bishop. 
  > The Elements of Statistical Learning - Hastie.
- Colah's Blog. http://colah.github.io
- Kaggle Notebooks.
- Tensorflow Documentation pages.
- Google Cloud Data Engineer certification materials.
- Multiple Wikipedia articles.

About Me
Twitter:

https://twitter.com/danielmartinezf

Linkedin:

https://www.linkedin.com/in/danielmartinezformoso/

Email:

daniel.martinez.formoso@gmail.com

",4035
PDIS/web-jekyll,CSS,"建置環境 Development
windows

git clone --recurse-submodules https://github.com/PDIS/web-jekyll
install ruby
install RubyInstaller Devkit ridk install
gem install jekyll bundler
bundler install
run jekyll serve or jekyll serve --drafts
open browser to 127.0.0.1:4000

mac

git clone --recurse-submodules https://github.com/PDIS/web-jekyll
install jekyll by gem
run jekyll serve or jekyll serve --drafts
open browser to 127.0.0.1:4000

檔案結構 Structure
單篇文章 Text

/_posts/

日誌 (category: blog)
專案 (category: project)



站內資料 Data

/_data/

features.yml

首頁區塊資訊 圖示樣式可參考


nav.yml

導覽列項目


aboutus.json

關於我們


informations.yml

置底連結文字


tracks.json

工作紀錄頁面


tools.yml

工具頁面




/_config.yml

首頁大標資訊



頁面版型 Page

/_layouts/

版型可用之嵌入內容 色碼可參考


/_includes/

page 可用之版型


/pages/

各頁 page


/index.html

首頁 page



其他 Others

支援 i18n

",6
ufosc/MuddySwamp,Python,"MuddySwamp
Multi-user dungeons, or ""MUDs"" are text-based role-playing games, that naturally evolved from the text-based rpg and adventure games of the 1970s.
This project aims to introduce a new generation—one that never experienced a world without broadband internet—to this classic game genre.
While this code can be adapted for any setting, we intend to render our university in beautiful ASCII.
Note: this repo has the internal (less stable) branch develop set to default.
For a more stable version, please look at the master branch.
Requirements
Hosting
For hosting a server, Python 3 must be installed on the system (along with an appropriate internet connection.) For help with Python installation, visit https://www.python.org
Connecting
For connecting to an existing server, a simple telnet client is required. However, we recommend using a dedicated MUD client to avoid ugliness like this:

There are many solid MUD clients available.
We have been using Mudlet, a completely free and open source MUD client.
Here's the same scenario, in Mudlet:

Getting Started
Hosting
Download this repository, or one of the releases. In a terminal, navigate to the repository and run
./MuddySwamp.py

By default, the server uses port 1234. If you want to specify a different port, you can run
./MuddySwamp.py [port]

If you are hosting a server for other people to connect, you will need to port foward your router. When you port forward, select the TCP protocol and direct traffic towards whatever port the server is listening on.
Once the server begins running, you will see an administrator prompt:
2019-02-24 14:49:09,497 [MudServerThread] [INFO] Starting server.
2019-02-24 14:49:09,497 [MudServerThread] [INFO] Server started successfully.

You can type ""help"" to get a list of administrator commands.
help
2019-02-24 14:50:01,441 [MainThread] [INFO] Server commands are:
 broadcast [message] - Broadcasts a message to the entire server
 players - Prints a list of all players
 stop - Stops the server
 list [locations|items|chars] - list all available loaded locations/items/chars

Connecting
with a Raw Telnet Client
If you want to use an ugly, raw telent client, you can use the following terminal command on *nix systems:
telnet <ip address> 1234

On Windows, a telnet client is not provided by default. One option is to follow this guide
to enable the Windows telnet client.
Alternatively, you can install PuTTY, a free and open source telnet and ssh client.
with Mudlet

Run Mudlet. You will be prompted to ""Select a profile to connect with"".
You must enter a set of required fields:

For ""Profile name"", put whatever you prefer.
For ""Server address"", put the address of the server.
For ""Port"", put the port of the server.


Once all the fields are entered, simply press ""connect"".
Have fun!
When you exit Mudlet, you will be asked if you want to save the profile. Select ""Yes"", and simply load the profile next time you play.

Contributing
Please read CONTRIBUTING.md for how to work on the project.
License
This project is licensed under the MIT License - see the LICENSE.md file for details.
",4
