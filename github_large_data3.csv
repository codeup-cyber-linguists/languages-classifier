title,language,original,stars
Imaginary11/api-gateway,Java,"api-gateway
基于SpringCloudGateway 实现的网关，包含动态路由、IP黑名单、接口白名单、JWT权限认证等功能，拓展简单，易于上手。

快速上手开发

拉源码
修改配置文件中的redis 地址 和eureka 地址
启动 ApiGatewayApplication

组件介绍
1.IP黑名单检查
该组件永远会被执行! 用户请求时第一步先经过黑名单检查,会获取用户的IP地址,如果用户的IP地址在全局黑名单中,结束请求并响应状态码:403;反则继续执行处理...
存储形式 redis string

key : blacklist_ip:{ip}
ttl : one day

IP黑名单RESTFul 接口



接口名称
定义
协议




获取所有IP黑名单
/gateway/blacklist/ip
GET


新增IP黑名单
/gateway/blacklist/ip
POST


删除IP黑名单
/gateway/blacklist/ip/{ip}
DELETE



请求示例
{
            ""ip"": ""127.0.0.1"", # ip
}

2.接口白名单检查
支持如下格式

/nc/*
/nc/sms/*
/nc/sms/deliver
/nc/sms/{id}
/nc/sms/{id}/info

存储形式 redis hash

whitelist_api
whitelist_api_pattern
whitelist_service

接口白名单RESTFul 接口



接口名称
定义
协议




获取所有白名单
/gateway/whitelist/api
GET


新增白名单
/gateway/whitelist/api
POST


删除白名单
/gateway/whitelist/api
DELETE



请求示例
{
            ""api"": ""/nc/sms/delivery"", # api
}

3.权限认证
当创建API时开启了安全认证,该组件会被执行! 组件会将流程交给权限认证插件,权限认证插件负责做相关处理后决定将流程交给下一个组件处理或结束请求
默认全局开启 jwt 认证 ，接口白名单除外
4.动态路由
支持http/lb 协议路由。
动态路由接口



接口名称
定义
协议




获取所有路由
/gateway/routes
GET


根据路由ID查询路由详情
/gateway/routes/{id}
GET


新增路由
/gateway/routes
POST


修改路由信息
/gateway/routes/{id}
PUT


删除路由
/gateway/routes/{id}
DELETE



请求示例
{
            ""id"": ""nc"", # 服务id
            ""predicates"": [
                {
                    ""name"": ""Path"",
                    ""args"": {
                        ""pattern"": ""/nc/**""  # 匹配规则
                    }
                }
            ],
            ""filters"": [], # 过滤器
            ""uri"": ""http://127.0.0.1:8081"",   # 转发地址 , 如果服务注册到网关，可以使用 lb://nc  ，自动实现负载均衡
            ""order"": 0 # 排序
}

todo

参数检查
访问限制
协议转换
前置处理器
中心处理器(主处理器)
后置处理器
异常处理器
HTTP/HTTPS
自定义服务

",7
charles9n/bert-sklearn,Jupyter Notebook,"scikit-learn wrapper for BERT
A scikit-learn model for text classification/regression based on the huggingface pytorch port of Google's BERT(Bidirectional Encoder Representations from Transformers) model.

Added an MSE loss for regression tasks
Added configurable MLP as final classifier/regressor

basic operation
model.fit(X,y) where


X: list, pandas dataframe, or numpy array of text or text pairs


y : list, pandas dataframe, or numpy array of labels/targets


from bert_sklearn import BertClassifier
from bert_sklearn import BertRegressor
from bert_sklearn import load_model

# define model
model = BertClassifier()   # for classification 
# model = BertRegressor()  # for regression 
 
# fit model
model.fit(X_train, y_train)

# make predictions
y_pred = model.predict(X_test)

# make probabilty predictions
y_pred = model.predict_proba(X_test)

# score model on test data
model.score(X_test, y_test)

# save model to disk
savefile='/data/mymodel.bin'
model.save(savefile)

# load model from disk
new_model = load_model(savefile)

# do stuff with new model
new_model.score(X_test, y_test)
See demo notebook.
model options
# try different options...
model.bert_model = 'bert-large-uncased'
model.num_mlp_layers = 3
model.max_seq_length = 196
model.epochs = 4
model.learning_rate = 4e-5
model.gradient_accumulation_steps = 4

# fit model
model.fit(X_train, y_train)

# do stuff...
model.score(X_test, y_test)
See options
hyperparameter tuning
from sklearn.model_selection import GridSearchCV

params = {'epochs':[3, 4], 'learning_rate':[2e-5, 3e-5, 5e-5]}

# wrap classifier in GridSearchCV
clf = GridSearchCV(BertClassifier(validation_fraction=0), 
                    params,
                    scoring='accuracy',
                    verbose=True)

# fit gridsearch 
clf.fit(X_train ,y_train)
See demo_tuning_hyperparameters notebook.
GLUE datasets
The train and dev data sets from the GLUE(Generalized Language Understanding Evaluation)  benchmarks were used with bert-base-uncased model and compared against the reported results in the Google paper and GLUE leaderboard.




MNLI(m/mm)
QQP
QNLI
SST-2
CoLA
STS-B
MRPC
RTE




BERT base(leaderboard)
84.6/83.4
89.2
90.1
93.5
52.1
87.1
84.8
66.4


bert-sklearn
83.7/83.9
90.2
88.6
92.32
58.1
89.7
86.8
64.6



Individual task demos can be found here.
other examples
See IMDb for a demo on the Internet Movie Database review sentiment task.
installation
requires python >= 3.5 and pytorch >= 0.4.1
# install pytorch-pretrained-bert from PyPI
pip install pytorch-pretrained-bert==v0.6.1

# setup bert-sklearn locally
git clone -b master https://github.com/charles9n/bert-sklearn
cd bert-sklearn
pip install .
tests
Run tests with pytest:
python -m pytest -sv tests/
references


Google's original tf code  and paper


huggingface pytorch port


",24
CodeDarigan/WAT,GDScript,"WAT Alpha

What is WAT?
WAT (Waiting and Testing) is an automated testing Plugin for the Godot Game Engine to help you test your code in a quick, comprehensive and isolated enviroment. It is compatiable with Godot 3.1 (and hopefully further). It is not compatiable with older versions of Godot due to its use of typed code.
Why WAT when GUT exists?
First and foremost because it was a learning experience about testing in general for me.
Secondly because I wanted a testing tool that would act as if it were part of the editor itself instead of a scene that runs them. This
presents its own issues with the usage of tool but so far so good.
For those unfamiller with GUT, you can check it out here
How do I use WAT?
You can check out the WAT Guide on the WAT Wiki
Who created WAT?
WAT was created by myself (if we ignore GUT as WAT's predecessor and all the people who put up with my frustrations in the Godot Community Discord). You can send me a message here, on twitter @CodeDarigan or on reddit /u/CodeDariganIE
",11
Ogg-Technologies/warframe-database,None,"warframe-database
Contains data about warframe
",2
dddjava/Jig,Java,"JIG
JIGの紹介
コンセプト
三層＋ドメインモデルのアーキテクチャで実装されたコードから、以下の分析・設計情報を生成します。

ドメインモデルのクラスに記述された業務の概念とビジネスルール
アプリケーション層に記述された業務機能

想定するアーキテクチャ
三層＋ドメインモデルのアーキテクチャでの使用を想定しています。


JIGドキュメント
JIGの生成する分析・設計情報をJIGドキュメントと呼びます。
種類は JigDocument を参照してください。
使い方
JIGは二種類の実行方法を提供しています。使い方や設定はそれぞれのREADMEを参照してください。

コマンドライン
Gradleプラグイン

実行環境

Java8以降。
ダイアグラム出力には Graphviz が必要です。

LICENSE
Apache License 2.0
",55
intel/intel-graphics-compiler,C++,"Intel(R) Graphics Compiler for OpenCL(TM)
Introduction
The Intel(R) Graphics Compiler for OpenCL(TM) is an llvm based compiler for
OpenCL(TM) targeting Intel Gen graphics hardware architecture.
Please refer to http://01.org/compute-runtime for additional details regarding
Intel's motivation and intentions wrt OpenCL support in the open source.
License
The Intel(R) Graphics Compute Runtime for OpenCL(TM) is distributed under the MIT.
You may obtain a copy of the License at:
https://opensource.org/licenses/MIT
Dependencies

Common Clang - https://github.com/intel/opencl-clang
Clang Source - https://github.com/llvm-mirror/clang
Khronos OpenCL Headers - https://github.com/KhronosGroup/OpenCL-Headers
LLVM Source -  https://github.com/llvm-mirror/llvm

Supported Linux versions
IGC is supported on the following 32/64 bits Linux operating systems:

Ubuntu 14.04, 16.04, 17.04, 18.04

Building

Install prerequisites

Building IGC needs flex, bison, cmake version later than 3.4.3 and
libz.  You can install required packages on ubuntu 18.04 like below:
$ sudo apt-get install flex bison libz-dev cmake


Download all dependencies and create workspace folder as below:

<workspace>
      |- igc                          https://github.com/intel/intel-graphics-compiler
      |- llvm_patches                 https://github.com/intel/llvm-patches
      |- llvm_source                  https://github.com/llvm-mirror/llvm
            |- projects/opencl-clang  https://github.com/intel/opencl-clang
            |- projects/llvm-spirv    https://github.com/KhronosGroup/SPIRV-LLVM-Translator
            |- tools/clang            https://github.com/llvm-mirror/clang

This can be done using the following commands:
$ cd <workspace>
$ git clone -b release_70 https://github.com/llvm-mirror/llvm llvm_source
$ git clone -b release_70 https://github.com/llvm-mirror/clang llvm_source/tools/clang
$ git clone -b ocl-open-70 https://github.com/intel/opencl-clang llvm_source/projects/opencl-clang
$ git clone -b llvm_release_70 https://github.com/KhronosGroup/SPIRV-LLVM-Translator llvm_source/projects/llvm-spirv
$ git clone https://github.com/intel/llvm-patches llvm_patches
$ git clone https://github.com/intel/intel-graphics-compiler igc
  [If using specific release]
$ cd igc && git checkout -b tag igc_release_2019-01-15


Under workspace create a build folder.  For example:

$ cd <workspace>
$ mkdir build


Build IGC using commands:

$ cd build
$ cmake ../igc/IGC
$ make -j`nproc`


Install IGC

$ sudo make install

Supported Platforms

Intel Core Processors supporting Gen8 graphics devices
Intel Core Processors supporting Gen9 graphics devices
Intel Core Processors supporting Gen10 graphics devices
Intel Atom Processors supporting Gen9 graphics devices

How to provide feedback
Please submit an issue using native github.com interface: https://github.com/intel/intel-graphics-compiler/issues.
How to contribute
Create a pull request on github.com with your patch. Make sure your change is
cleanly building. A maintainer will contact you if there are questions or concerns.
",208
spring-io/dataflow.spring.io,JavaScript,"




Introduction
This is the Spring Cloud Data Flow Website, located at https://dataflow.spring.io.
Building
You'll need NodeJS and Yarn installed globally. Note that, for Node, you need version 10, not the latest version.
# Init
yarn install        # Install dependencies

# Linter / Prettier
yarn run lint       # Linter
yarn run fix        # Fix linting errors

# Dev
yarn start          # Run dev

# Prod
yarn build          # Run dev
yarn serve          # Serve the prod build
Configure
Algolia
export ALGOLIA_ADMIN_KEY=<KEY>
Documentation
Features:

Versioning
Markdown syntax
Advanced markdown syntax: download external files, embed template/code/video, tabs component ...
Search on a documentation version

",2
moov-io/wire,Go,"moov-io/wire





Package github.com/moov-io/wire implements a reader and writer written in Go  for creating, parsing and validating FED Wire Messages (FEDWire)
Docs: docs.moov.io | api docs
Project Status
Moov WIRE is under active development and should not currently be used for production. Please star the project if you are interested in its progress.
Usage
Go library
github.com/moov-io/wire offers a Go based ACH file reader and writer. To get started checkout a specific example:

Supported Standard Entry Class (SEC) codes
| Business Function Code | Name                                  | Example |
|----------|---------------------------------------|-----------------------------------|------------------------------------|
| BTR      | BankTransfer                          | Link |
| CKS      | CheckSameDaySettlement                | Link |
| CTP      | CustomerTransferPlus                  | Link) |
| CTR      | CustomerTransfer                      | Link |
| DEP      | DepositSendersAccount                 | Link |
| DRB      | BankDrawdownRequest                   | Link) |
| DRC      | CustomerCorporateDrawdownRequest      | Link |
| DRW      | DrawdownRequest                       | Link) |
| FFR      | FEDFundsReturned                      | Link) |
| FFS      | FEDFundsSold                          | Link) |
| SVC      | ServiceMessage                        | Link |

From Source
This project uses Go Modules and thus requires Go 1.11+. You can download the source code and we offer tagged and released versions as well. We highly recommend you use a tagged release for production.
$ git@github.com:moov-io/wire.git

# Pull down into the Go Module cache
$ go get -u github.com/moov-io/wire

$ go doc github.com/moov-io/wire BatchHeader

Getting Help



channel
info




Project Documentation
Our project documentation available online.


Google Group moov-users
The Moov users Google group is for contributors other people contributing to the Moov project. You can join them without a google account by sending an email to moov-users+subscribe@googlegroups.com. After receiving the join-request message, you can simply reply to that to confirm the subscription.


Twitter @moov_io
You can follow Moov.IO's Twitter feed to get updates on our project(s). You can also tweet us questions or just share blogs or stories.


GitHub Issue
If you are able to reproduce an problem please open a GitHub Issue under the specific project that caused the error.


moov-io slack
Join our slack channel to have an interactive discussion about the development of the project. Request an invite to the slack channel



Supported and Tested Platforms

64-bit Linux (Ubuntu, Debian), macOS, and Windows

Note: 32-bit platforms have known issues and are not supported.
Contributing
Yes please! Please review our Contributing guide and Code of Conduct to get started!
Note: This project uses Go Modules, which requires Go 1.11 or higher.
Releasing
To make a release of wire simply open a pull request with CHANGELOG.md and version.go updated with the next version number and details. You'll also need to push the tag (i.e. git push origin v1.0.0) to origin in order for CI to make the release.
License
Apache License 2.0 See LICENSE for details.
",3
intel/intel-graphics-compiler,C++,"Intel(R) Graphics Compiler for OpenCL(TM)
Introduction
The Intel(R) Graphics Compiler for OpenCL(TM) is an llvm based compiler for
OpenCL(TM) targeting Intel Gen graphics hardware architecture.
Please refer to http://01.org/compute-runtime for additional details regarding
Intel's motivation and intentions wrt OpenCL support in the open source.
License
The Intel(R) Graphics Compute Runtime for OpenCL(TM) is distributed under the MIT.
You may obtain a copy of the License at:
https://opensource.org/licenses/MIT
Dependencies

Common Clang - https://github.com/intel/opencl-clang
Clang Source - https://github.com/llvm-mirror/clang
Khronos OpenCL Headers - https://github.com/KhronosGroup/OpenCL-Headers
LLVM Source -  https://github.com/llvm-mirror/llvm

Supported Linux versions
IGC is supported on the following 32/64 bits Linux operating systems:

Ubuntu 14.04, 16.04, 17.04, 18.04

Building

Install prerequisites

Building IGC needs flex, bison, cmake version later than 3.4.3 and
libz.  You can install required packages on ubuntu 18.04 like below:
$ sudo apt-get install flex bison libz-dev cmake


Download all dependencies and create workspace folder as below:

<workspace>
      |- igc                          https://github.com/intel/intel-graphics-compiler
      |- llvm_patches                 https://github.com/intel/llvm-patches
      |- llvm_source                  https://github.com/llvm-mirror/llvm
            |- projects/opencl-clang  https://github.com/intel/opencl-clang
            |- projects/llvm-spirv    https://github.com/KhronosGroup/SPIRV-LLVM-Translator
            |- tools/clang            https://github.com/llvm-mirror/clang

This can be done using the following commands:
$ cd <workspace>
$ git clone -b release_70 https://github.com/llvm-mirror/llvm llvm_source
$ git clone -b release_70 https://github.com/llvm-mirror/clang llvm_source/tools/clang
$ git clone -b ocl-open-70 https://github.com/intel/opencl-clang llvm_source/projects/opencl-clang
$ git clone -b llvm_release_70 https://github.com/KhronosGroup/SPIRV-LLVM-Translator llvm_source/projects/llvm-spirv
$ git clone https://github.com/intel/llvm-patches llvm_patches
$ git clone https://github.com/intel/intel-graphics-compiler igc
  [If using specific release]
$ cd igc && git checkout -b tag igc_release_2019-01-15


Under workspace create a build folder.  For example:

$ cd <workspace>
$ mkdir build


Build IGC using commands:

$ cd build
$ cmake ../igc/IGC
$ make -j`nproc`


Install IGC

$ sudo make install

Supported Platforms

Intel Core Processors supporting Gen8 graphics devices
Intel Core Processors supporting Gen9 graphics devices
Intel Core Processors supporting Gen10 graphics devices
Intel Atom Processors supporting Gen9 graphics devices

How to provide feedback
Please submit an issue using native github.com interface: https://github.com/intel/intel-graphics-compiler/issues.
How to contribute
Create a pull request on github.com with your patch. Make sure your change is
cleanly building. A maintainer will contact you if there are questions or concerns.
",208
spring-io/dataflow.spring.io,JavaScript,"




Introduction
This is the Spring Cloud Data Flow Website, located at https://dataflow.spring.io.
Building
You'll need NodeJS and Yarn installed globally. Note that, for Node, you need version 10, not the latest version.
# Init
yarn install        # Install dependencies

# Linter / Prettier
yarn run lint       # Linter
yarn run fix        # Fix linting errors

# Dev
yarn start          # Run dev

# Prod
yarn build          # Run dev
yarn serve          # Serve the prod build
Configure
Algolia
export ALGOLIA_ADMIN_KEY=<KEY>
Documentation
Features:

Versioning
Markdown syntax
Advanced markdown syntax: download external files, embed template/code/video, tabs component ...
Search on a documentation version

",2
moov-io/wire,Go,"moov-io/wire





Package github.com/moov-io/wire implements a reader and writer written in Go  for creating, parsing and validating FED Wire Messages (FEDWire)
Docs: docs.moov.io | api docs
Project Status
Moov WIRE is under active development and should not currently be used for production. Please star the project if you are interested in its progress.
Usage
Go library
github.com/moov-io/wire offers a Go based ACH file reader and writer. To get started checkout a specific example:

Supported Standard Entry Class (SEC) codes
| Business Function Code | Name                                  | Example |
|----------|---------------------------------------|-----------------------------------|------------------------------------|
| BTR      | BankTransfer                          | Link |
| CKS      | CheckSameDaySettlement                | Link |
| CTP      | CustomerTransferPlus                  | Link) |
| CTR      | CustomerTransfer                      | Link |
| DEP      | DepositSendersAccount                 | Link |
| DRB      | BankDrawdownRequest                   | Link) |
| DRC      | CustomerCorporateDrawdownRequest      | Link |
| DRW      | DrawdownRequest                       | Link) |
| FFR      | FEDFundsReturned                      | Link) |
| FFS      | FEDFundsSold                          | Link) |
| SVC      | ServiceMessage                        | Link |

From Source
This project uses Go Modules and thus requires Go 1.11+. You can download the source code and we offer tagged and released versions as well. We highly recommend you use a tagged release for production.
$ git@github.com:moov-io/wire.git

# Pull down into the Go Module cache
$ go get -u github.com/moov-io/wire

$ go doc github.com/moov-io/wire BatchHeader

Getting Help



channel
info




Project Documentation
Our project documentation available online.


Google Group moov-users
The Moov users Google group is for contributors other people contributing to the Moov project. You can join them without a google account by sending an email to moov-users+subscribe@googlegroups.com. After receiving the join-request message, you can simply reply to that to confirm the subscription.


Twitter @moov_io
You can follow Moov.IO's Twitter feed to get updates on our project(s). You can also tweet us questions or just share blogs or stories.


GitHub Issue
If you are able to reproduce an problem please open a GitHub Issue under the specific project that caused the error.


moov-io slack
Join our slack channel to have an interactive discussion about the development of the project. Request an invite to the slack channel



Supported and Tested Platforms

64-bit Linux (Ubuntu, Debian), macOS, and Windows

Note: 32-bit platforms have known issues and are not supported.
Contributing
Yes please! Please review our Contributing guide and Code of Conduct to get started!
Note: This project uses Go Modules, which requires Go 1.11 or higher.
Releasing
To make a release of wire simply open a pull request with CHANGELOG.md and version.go updated with the next version number and details. You'll also need to push the tag (i.e. git push origin v1.0.0) to origin in order for CI to make the release.
License
Apache License 2.0 See LICENSE for details.
",3
apmckinlay/gsuneido,Go,"gsuneido
In progress Go implementation of Suneido (http://suneido.com)

Todo


built-in methods and functions


client end of client-server


database


server end of client-server


Win32 interface ?


",4
oboat9/Generic-Dungeon-Game,Python,"Generic Dungeon Game
A Generic Dungeon Game Made For Computer Science 10
###Instructions
To start the game if you have python & pygame, run the main.py file.
If you do not have python and or pygame, you open the build.zip and run main.exe
Important Note

In order to run this you Must have Python 3.7+ https://www.python.org/ and Pygame https://www.pygame.org/ installed.

Controls

Movement is either the arrow keys or WASD
Shooting is Left Mouse Click

Project Criteria

6 Levels
Key and exit for each level
Original Sprites

Game Description
This is a game that should just be a fairly generic dungeon escape game.
There will be a key and an exit for each level, of which there will be at least six.
Features

Original Level Design
Original Sound Design
Original Sprites
Enemies
Projectiles / Weapons

Planned Features (Deadline Dependant)

Basic Lighting System (Black shadow around a certain radius of the player to increase difficulty) (Not on boss levels)

Tasks
Design & Planning
Levels

 Design Level 1
 Design Level 2 (Needs To Add Bonus)
 Design Level 3
 Design Level 4 (Needs To Add Bonus)
 Design Level 5 (Needs To Add Bonus)
 Design Level 6

Sprites

 Design Character
 Design Regular Enemy
 Design Enemy With Projectile
 Design Mid-Boss
 Design Final-Boss
 Design ""Key""

Sound

 Design Sound Effects (In Progress)
####### Music
 Design Menu Music
 Design Game Music

Programming
Game Fundamentals

 Basic Movement
 Obstacles For Player
 Scrolling Camera

Game Look

 Player Sprite & Animation (In Progress)
 Enemy Sprite & Animation (In Progress)

Game Mechanics

 Impliment Enemies
 Impliment Weapons (In Progress)
 Impliment Level Finish (Start next level when done last one)

",2
aodn/chef,Ruby,"Overview
This chef repository contains all cookbooks used for the IMOS infrastructure.
Getting Started
Prerequisites



software
version
install notes




git

apt-get


ruby
>= 1.9.3
apt-get / rbenv


bundler

gem


vagrant
>= 1.8.6
Download package from website.  On Ubuntu 18.04 >=2.1.1 is required to support vagrant-berkshelf


chef_dk
= 1.3.40
Install for your own distribution.


vagrant-berkshelf
>= 4.0.1
vagrant plugin install vagrant-berkshelf


VirtualBox
>= 5.1.6
Download package from website



See websites or documentation for more detailed installation instructions
Optional Vagrant Plugins
Note: in addition to the vagrant plugins listed above, the following are worth a look:

vagrant-vbguest
vagrant-cachier

Private Repository
You may have noticed that data_bags, roles and nodes are referencing
private/SOMETHING. We'll need to plug a chef private repository under
private/. private-sample contains a sample tree structure with a few basic
definitions so you can get started. In order to use the sample definitions,
link it as shown below:
$ ln -s private-sample private

The symbolic links should look lively now.
In case you plan on using real infrastructure, it is advised to create a new
private repository just with the private definitions and link private to it.
Private Sample Repository (Project Officer box etc)
Click here for Project Officer box documentation
Read more here for more examples of nodes and how to
use them.
Run up a node with Vagrant
Running up a VM should be as simple as:
$ vagrant up <node_name>

You can now ssh to the new VM:
$ vagrant ssh <node_name>

or check the status:
$ vagrant status <node_name>

The Vagrantfile contains some other overridable options - refer to it to see what they are.
Packing Your Own Vagrant Basebox (with Packer)
To build a basebox using packer, you'll need packer
version 0.8.0 or greater. From the chef directory you can then run:
$ berks vendor private/packer/cookbooks1
$ cp -a cookbooks private/packer/cookbooks2
$ cd private/packer && packer build basebox.json

Workflow
If you wish to contribute to this repository, you will have to fork it.
Before doing any new work, you should create a new branch:
$ git co -b new_feature

Work on the branch as needed, then push it:
$ git push origin new_feature

Submit a PR and see if we accept it.
Testing
Unit Testing
chefspec unit tests can be run either from the root:
$ chef exec rake spec

or from a particular cookbook directory, e.g.:
$ cd cookbooks/imos_po
$ chef exec rspec spec

",2
kaiwahour/home,JavaScript,"Kaiwa Hour Website
Public facing website for Kaiwa Hour built in React. This website is current
under construction. View the latest (pre-)deployment here.

Photo credit: Bernard Hermant on Unsplash
Getting started
To build the website, first git clone this repository. Then from the command
line, run:
yarn install

Start the development server with
yarn start
Problems with yarn?
First, try deleting yarn.lock and the /node_modules directory (if it exists). Then run yarn, and if that succeeds, run yarn update.
Getting involved
Want to get involved? Check out the open
Issues for this
project. More detailed guidelines and instructions on contributing will soon
be added.
",4
su37josephxia/kaikeba-code,JavaScript,"课程代码分享 （仅供参考）
基础知识 - 前端大班车
示例代码运行

NodeJS 10.0 need https://nodejs.org/en/
Clone or download this repository
Enter your local directory, and
install dependencies:

npm install
",3
MikaPham/AwesomePhotos,Swift,"AWESOME PHOTOS
An iOS applicationn that allows users to take photo/media and store them on the Cloud (Firebase specifically). Photos/Medias can also be shared to other users.
Getting Started
These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.
Main Features


User can register and login to our application using their email account.


Once logged in, a user can: Change their password; take a picture or a video, and other core features.


After capturing a media, the media can be uploaded to Firebase Storage. The upload/save to local process can be done manually by pressing the upload/save button or it can be set automatically in the Settings screen.


Each user can add owner or share their medias to other users. By adding an owner, the original copy of the photo is given to the owner(s) and by sharing, a watermarked copy will be shared.


A user can also edit the permission of their photos or videos.


Prerequisites
What things you need to install the software and how to install them
Swift Version 5
Xcode 9.3 or Higher

Installing
open Xcode
pod install
change bundle identifier to a unique identifier
Type Cmd + R or press the build button

Built With

XCode

Database and Services used

Firebase Authentication
Cloud Firestore
Cloud Storage
Cloud Functions

Contributing
Please read CONTRIBUTING.md for details on our code of conduct, and the process for submitting pull requests to us.
Authors

Pham Minh Quang - Initial work
Minh Kha Pham - Initial work
Pham Minh Quang - Initial work
Nguyen Duc Thien Hieu - Initial work
Rasmus Bak Petersen - Initial work
Duong Viet Trung - Initial work

License
This project is licensed under the MIT License - see the LICENSE.md file for details
Acknowledgments
Libraries used:

https://github.com/danielgindi/Charts - Pie Chart
https://github.com/rubygarage/media-watermark - Watermark
https://firebase.google.com/docs/auth - Firebase Authentication
https://firebase.google.com/docs/firestore - Firebase Firestore
https://firebase.google.com/docs/storage - Firebase Storage
https://github.com/SnapKit/SnapKit - Snapkit
https://github.com/raulriera/TextFieldEffects - TextFieldEffects
https://github.com/ReactiveX/RxSwift - RxSwift
https://github.com/ReactiveX/RxSwift/tree/master/RxCocoa - RxCocoa

References:

http://khou22.com/ios/2016/08/10/swift-navigation-basics-how-to-setup-a-simple-tab-bar-app.html
https://www.youtube.com/watch?v=2-nxXXQyVuE
https://www.youtube.com/channel/UCuP2vJ6kRutQBfRmdcI92mA

",2
aodn/chef,Ruby,"Overview
This chef repository contains all cookbooks used for the IMOS infrastructure.
Getting Started
Prerequisites



software
version
install notes




git

apt-get


ruby
>= 1.9.3
apt-get / rbenv


bundler

gem


vagrant
>= 1.8.6
Download package from website.  On Ubuntu 18.04 >=2.1.1 is required to support vagrant-berkshelf


chef_dk
= 1.3.40
Install for your own distribution.


vagrant-berkshelf
>= 4.0.1
vagrant plugin install vagrant-berkshelf


VirtualBox
>= 5.1.6
Download package from website



See websites or documentation for more detailed installation instructions
Optional Vagrant Plugins
Note: in addition to the vagrant plugins listed above, the following are worth a look:

vagrant-vbguest
vagrant-cachier

Private Repository
You may have noticed that data_bags, roles and nodes are referencing
private/SOMETHING. We'll need to plug a chef private repository under
private/. private-sample contains a sample tree structure with a few basic
definitions so you can get started. In order to use the sample definitions,
link it as shown below:
$ ln -s private-sample private

The symbolic links should look lively now.
In case you plan on using real infrastructure, it is advised to create a new
private repository just with the private definitions and link private to it.
Private Sample Repository (Project Officer box etc)
Click here for Project Officer box documentation
Read more here for more examples of nodes and how to
use them.
Run up a node with Vagrant
Running up a VM should be as simple as:
$ vagrant up <node_name>

You can now ssh to the new VM:
$ vagrant ssh <node_name>

or check the status:
$ vagrant status <node_name>

The Vagrantfile contains some other overridable options - refer to it to see what they are.
Packing Your Own Vagrant Basebox (with Packer)
To build a basebox using packer, you'll need packer
version 0.8.0 or greater. From the chef directory you can then run:
$ berks vendor private/packer/cookbooks1
$ cp -a cookbooks private/packer/cookbooks2
$ cd private/packer && packer build basebox.json

Workflow
If you wish to contribute to this repository, you will have to fork it.
Before doing any new work, you should create a new branch:
$ git co -b new_feature

Work on the branch as needed, then push it:
$ git push origin new_feature

Submit a PR and see if we accept it.
Testing
Unit Testing
chefspec unit tests can be run either from the root:
$ chef exec rake spec

or from a particular cookbook directory, e.g.:
$ cd cookbooks/imos_po
$ chef exec rspec spec

",2
kaiwahour/home,JavaScript,"Kaiwa Hour Website
Public facing website for Kaiwa Hour built in React. This website is current
under construction. View the latest (pre-)deployment here.

Photo credit: Bernard Hermant on Unsplash
Getting started
To build the website, first git clone this repository. Then from the command
line, run:
yarn install

Start the development server with
yarn start
Problems with yarn?
First, try deleting yarn.lock and the /node_modules directory (if it exists). Then run yarn, and if that succeeds, run yarn update.
Getting involved
Want to get involved? Check out the open
Issues for this
project. More detailed guidelines and instructions on contributing will soon
be added.
",4
su37josephxia/kaikeba-code,JavaScript,"课程代码分享 （仅供参考）
基础知识 - 前端大班车
示例代码运行

NodeJS 10.0 need https://nodejs.org/en/
Clone or download this repository
Enter your local directory, and
install dependencies:

npm install
",3
MikaPham/AwesomePhotos,Swift,"AWESOME PHOTOS
An iOS applicationn that allows users to take photo/media and store them on the Cloud (Firebase specifically). Photos/Medias can also be shared to other users.
Getting Started
These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.
Main Features


User can register and login to our application using their email account.


Once logged in, a user can: Change their password; take a picture or a video, and other core features.


After capturing a media, the media can be uploaded to Firebase Storage. The upload/save to local process can be done manually by pressing the upload/save button or it can be set automatically in the Settings screen.


Each user can add owner or share their medias to other users. By adding an owner, the original copy of the photo is given to the owner(s) and by sharing, a watermarked copy will be shared.


A user can also edit the permission of their photos or videos.


Prerequisites
What things you need to install the software and how to install them
Swift Version 5
Xcode 9.3 or Higher

Installing
open Xcode
pod install
change bundle identifier to a unique identifier
Type Cmd + R or press the build button

Built With

XCode

Database and Services used

Firebase Authentication
Cloud Firestore
Cloud Storage
Cloud Functions

Contributing
Please read CONTRIBUTING.md for details on our code of conduct, and the process for submitting pull requests to us.
Authors

Pham Minh Quang - Initial work
Minh Kha Pham - Initial work
Pham Minh Quang - Initial work
Nguyen Duc Thien Hieu - Initial work
Rasmus Bak Petersen - Initial work
Duong Viet Trung - Initial work

License
This project is licensed under the MIT License - see the LICENSE.md file for details
Acknowledgments
Libraries used:

https://github.com/danielgindi/Charts - Pie Chart
https://github.com/rubygarage/media-watermark - Watermark
https://firebase.google.com/docs/auth - Firebase Authentication
https://firebase.google.com/docs/firestore - Firebase Firestore
https://firebase.google.com/docs/storage - Firebase Storage
https://github.com/SnapKit/SnapKit - Snapkit
https://github.com/raulriera/TextFieldEffects - TextFieldEffects
https://github.com/ReactiveX/RxSwift - RxSwift
https://github.com/ReactiveX/RxSwift/tree/master/RxCocoa - RxCocoa

References:

http://khou22.com/ios/2016/08/10/swift-navigation-basics-how-to-setup-a-simple-tab-bar-app.html
https://www.youtube.com/watch?v=2-nxXXQyVuE
https://www.youtube.com/channel/UCuP2vJ6kRutQBfRmdcI92mA

",2
RobertCNelson/ti-linux-kernel-dev,Shell,"This is just a set of scripts to rebuild a known working kernel for ARM devices.
Script Bugs: ""bugs@rcn-ee.com""
Note, for older git tag's please use: https://github.com/RobertCNelson/yakbuild
Dependencies: GCC ARM Cross ToolChain
Linaro:
http://www.linaro.org/downloads/
Dependencies: Linux Kernel Source
This git repo contains just scripts/patches to build a specific kernel for some
ARM devices. The kernel source will be downloaded when you run any of the build
scripts.
By default this script will clone the linux-stable tree:
https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git
to: ${DIR}/ignore/linux-src:
If you've already cloned torvalds tree and would like to save some hard drive
space, just modify the LINUX_GIT variable in system.sh to point to your current
git clone directory.
Build Kernel Image:
./build_kernel.sh

Optional: Build Debian Package:
./build_deb.sh

Development/Hacking:
first run (to setup baseline tree): ./build_kernel.sh
then modify files under KERNEL directory
then run (to rebuild with your changes): ./tools/rebuild.sh
",45
expo/expo,Objective-C,"Expo Client  
Expo is a set of tools, libraries, and services that let you build native iOS and Android apps by writing JavaScript. This repository is where the Expo client software is developed, and includes the client apps, modules, apps, and more.
Click here to view our documentation for developing on Expo. If you're new to the Expo community, click here to view the Expo Community Guidelines. Thank you for helping keep the Expo community open and welcoming!
Introduction
This is the source code for the Expo client app used to view experiences published to the Expo service. If you want to build and install the Expo client directly onto a device, you're in the right place. Note that if you just want to install the Expo client app on a simulator, you do not need to build it from source. Instead, you should follow the instructions here.
To build the Expo client app, follow the instructions in the Set Up section below. Use the expo-cli command line to use Expo's infrastructure to build your app.
Please ask us on the forums if you get stuck.
Disclaimers:
If you want to build a standalone app that has a custom icon and name, see our documentation here. You're in the wrong place, you shouldn't need to build the Expo clients from source.
If you need to make native code changes to your Expo project, such as adding custom native modules, we can generate a native project for you. You're in the wrong place, you shouldn't need to build the Expo clients from source.
Set Up
Note: We support building the clients only on macOS.

Install nix (currently curl https://nixos.org/nix/install | sh)
Install direnv (to do this with nix, run nix-env -iA nixpkgs.direnv)
Clone this repo; we recommend cloning it to a directory whose full path does not include any spaces (you should clone all the submodules with git clone --recurse-submodules)
Run yarn in the tools-public directory.

iOS

Make sure you have latest non-beta Xcode installed.
Run git lfs pull.
Run ./generate-files-ios.js in the tools-public directory.
Open and run ios/Exponent.xcworkspace in Xcode.

Android

Make sure you have Android Studio 3 installed
Run android/install-ndk-17c.sh to get the required version of the Android NDK.
See ""Running on a Device""

Running on a Device
iOS

In Xcode's menu bar, open the Xcode drop-down menu, and select Preferences.  Then in the Accounts tab of the preferences menu, add your personal or team apple developer account.
Connect your test device to your computer with a USB cable.
In Xcode's menu bar, open the Product drop-down menu, select Destination, then in the Device grouping select your device.
In the project navigator, select the Exponent project to bring up the project's settings, and then:

In the General tab, in the Identity section, put in a unique Bundle Identifier.
Also in the General tab, in the Signing section, select your personal or team apple developer account as your Team, and create a new signing certificate by clicking Fix Issue.


Finally, run the build

Android

If the Play Store version of the Expo Client App is installed on your test device, uninstall it.
Connect your test device to your computer with a USB cable.
Run fastlane android start, or alternately open the android directory in Android Studio, start it, and in the Select Deployment Target dialog, select your device.

Standalone Apps
If you don't need custom native code outside of the Expo SDK, head over to our documentation on building standalone apps without needing Android Studio and Xcode.
If you're still here, make sure to follow the Configure app.json section of the docs before continuing. You'll need to add the appropriate fields to your app.json before the standalone app scripts can run. Once that's done, continue on to the platform-specific instructions.
Android
The Android standalone app script creates a new directory android-shell-app with the modified Android project in it. It then compiles that new directory giving you a signed or unsigned .apk depending on whether you provide a keystore and the necessary passwords. If there are issues with the app you can open the android-shell-app project in Android Studio to debug.
Here are the steps to build a standalone Android app:

Publish your experience with Expo CLI. Note the published URL.
cd tools-public.
If you want a signed .apk, run gulp android-shell-app --url [the published experience url] --sdkVersion [sdk version of your experience] --keystore [path to keystore] --alias [keystore alias] --keystorePassword [keystore password] --keyPassword [key password] --workingDir=../.
If you don't want a signed .apk, run gulp android-shell-app --url [the published experience url] --sdkVersion [sdk version of your experience] --workingDir=../.
The .apk file will be at /tmp/shell-signed.apk for a signed .apk or at /tmp/shell-debug.apk for an unsigned .apk.
adb install the .apk file to test it.
Upload to the Play Store!

iOS
The iOS standalone app script has two actions, build and configure. build creates an archive or a simulator build of the Expo iOS workspace. configure accepts a path to an existing archive and modifies all its configuration files so that it will run as a standalone Expo experience rather than as the Expo client app.
Here are the steps to build a standalone iOS app:

Publish your experience with Expo CLI. Note the published URL.
cd tools-public.
gulp ios-shell-app --action build --type [simulator or archive] --configuration [Debug or Release]
The resulting archive will be created at ../shellAppBase-[type].
gulp ios-shell-app --url [the published experience url] --action configure --type [simulator or archive] --archivePath [path to ExpoKitApp.app] --sdkVersion [sdk version of your experience] --output your-app.tar.gz
This bundle is not signed and cannot be submitted to iTunes Connect as-is; you'll need to manually sign it if you'd like to submit it to Apple. Fastlane is a good option for this. Also, Expo will do this for you if you don't need to build this project from source.
If you created a simulator build in the first step, unpack the tar.gz using tar -xvzf your-app.tar.gz. Then you can run this on iPhone Simulator using xcrun simctl install booted <app path> and xcrun simctl launch booted <app identifier>. Another alternative which some people prefer is to install the ios-sim tool and then use ios-sim launch <app path>.
There are a few more optional flags you can pass to this script. They are all documented in the block comments inside xdl/src/detach/IosShellApp.js.

Modifying JS Code
The Expo client apps run a root Expo project in addition to native code. By default this will use a published version of the project, so any changes made in the home directory will not show up without some extra work.
Serve this project locally by running expo start from the home directory. On iOS, you'll additionally need to set DEV_KERNEL_SOURCE to LOCAL in EXBuildConstants.plist (the default is PUBLISHED).
The native Android Studio and XCode projects have a build hook which will find this if expo start is running. Keep this running and rebuild the app on each platform.
Project Layout

android contains the Android project.
home contains the JavaScript source code of the app.
ios contains the iOS project.
ios/Exponent.xcworkspace is the Xcode workspace. Always open this instead of Exponent.xcodeproj because the workspace also loads the CocoaPods dependencies.
tools-public contains build and configuration tools.
template-files contains templates for files that require private keys. They are populated using the keys in template-files/keys.json.
template-files/ios/dependencies.json specifies the CocoaPods dependencies of the app.

Tests
iOS
For native XCTest unit tests:

Press Command+U in XCode to build and test the Tests unit test target.
Alternatively, run fastlane ios test from the parent directory of ios.

For JS integration tests, test the ExponentIntegrationTests target (not included in the default test scheme). This target requires you to configure EXTestEnvironment.plist with a key testSuiteUrl whose value is the URL to load some version of Expo's test-suite app. This will run a bunch of Jasmine tests against the Expo SDK.
Contributing
Foundation Unimodules
The Foundation Unimodules by Expo are under packages, along with other JS packages. Each Unimodule has its own tests in its package (yarn test) and under apps/test-suite (run a development build of the Expo client, run expo start in test-suite, and load it on a device). We recommend reading the source for several Unimodules to get a sense of the code conventions and taste.

Guide to Unimodule Development
Contributing to Expo in General
Expo JS Style Guide (also mostly applies to TypeScript)

Expo client
Please check with us before putting work into a Pull Request! We don't yet have a good guide available that covers the nuances of how to work with the Expo client so you will want a direct line of communication with someone on the team to ask us questions. The best place to talk to us is either on Slack at https://slack.expo.io or the forums at https://forums.expo.io.
License
The Expo source code is made available under the MIT license. Some of the dependencies are licensed differently, with the BSD license, for example.
",6177
SiddheshNan/Acer-A515-51G-Hackintosh,Rich Text Format,"Acer-A515-51G-Hackintosh
Supports MacOS 10.13.x and 10.14.x

What Works

 Audio w/ headphone jack
 Backlight
 WebCam
 CPU Speedstep (XCPM)
 Ethernet
 HDMI
 Battery Management
 Sleep + Wake
 Smart Touchpad + Gestures
 Usb 3.0 + Type C
 Sleep From (Lid)
 iGPU with disabled dGPU
 WiFi (2.4 + 5GHz) + BT by using BCM94352z
 Native hotkey support w/ Fn keys

What Doesn't Works

 SD Card Reader


Installation

Create a Bootable USB for OSx by using the guide by RehabMan [Guide] Booting the OS X installer on LAPTOPS with Clover.

Audio

The Sound Card is Realtek ALC255, which is drived by AppleALC on layout-id 3.
If headphones are not working, please see ALCPlugFix.

Bluetooth

The Native Atheros BT will work out-of-the Box, But you can't turn off the BT becouse Power Management is not supported;
To Fix this, you'll have to get a MacOS compatible wifi+BT card. The best choice will be BCM94352Z which has WiFi+BT.

Ethernet

The Ethernet is 8411B, and you'll need RealtekRTL8111.kext to Enable Gigabit Ethernet.

Graphics

The iGPU is Intel UHD Graphics 620, and its enabled using Ig-Platform-id=0x191E0000
The discrete graphics' name is NVIDIA GeForce MX150, and its disabled becuase macOS doesn't support Optimus technology. Plus Battery Life is Improved to 6-7 hours of Backup.
Native brightness hotkey support; using DSDT.aml patched from RehabMan's [Guide] Patching DSDT/SSDT for LAPTOP backlight control.

Touchpad

The touchpad works After Installing VoodooPs2Controller.kext, After that Install ApplePS2SmartTouchpad.kext in order to recognise the touchpad as native one, And after that all gestures will work fine.

USB

To Raise the Port Limit, We're using USBInjectAll.kext from RehabMan. You can also Find Patched SSDT-UIAC under ACPI/Patched Folder.

Wi-Fi

The Native Wi-Fi Card this Laptop uses is Atheros QCA9377. Since it is not supported natively, and there are none kexts to enable this card on macOS, We can't use this card and I don't think it'll get supported in the future.
The Best Choice will be to Replace current Card with BCM94352Z which has WiFI+BT, or BCM943602BAED Which Supports MacOS natively.
I Have Already Changed My Current WiFi card with BCM94352Z, you can find it on AliExpress for like $20-30.
I've Patched Config.plist for BCM94352Z and BCM943602BAED and also added Kexts for BT as well. Keep in mind, this laptop uses M.2(NGFF) Socket with A+E Key. Half size Card Won't Work.

Credits

Special Thanks to Acidanthera for most of the Kexts.
Special Thanks to RehabMan.
Thanks to Clover Bootloader.
Thanks to goodwin for ALCPlugfix.
Thanks to EMlyDinEsH for ApplePS2SmartTouchPad.kext.
Thanks to daliansky for Some Patches which I used here from XiaoMi-Pro.

",4
MUME/MMapper,C++,"MMapper




Download the latest version of MMapper
Features

Automatic room creation during mapping
Automatic connection of new rooms
Terrain detection (forest, road, mountain, etc)
Exits detections
Fast OpenGL rendering
Pseudo 3D layers and drag and drop mouse operations
Multi platform support
Group manager support to see other people on your map
Integrated mud client
Remote editing support

Usage
Set up your client according to the wiki instructions.
Build
Check out the wiki for Linux, Windows and Mac build instructions.
",14
betterscientificsoftware/betterscientificsoftware.github.io,Python,"betterscientificsoftware.github.io
Who We Are
Better Scientific Software is an organization dedicated to improving developer productivity and improving software sustainability for computational science and engineering (CSE).
This repository provides source material for the Better Scientific Software BSSw.io web portal. Better Scientific Software (BSSw) community members can contribute content using standard GitHub tools and processes. Contributions can be made via:

Web browser editing:  For many people (even BSSw project members), this is probably the preferred way.  GitHub provides a nice web editor for Markdown.
Cloning: If you have push access, you can clone and commit to this repository.  This approach could be best for remote editing and activities that span across multiple source files.
Forking: This option is like cloning, but works for anyone.  You can make edits to your own forked copy of the repo, either in a browser or from a local repository.  Contributions are submitted to BSSw by using a pull request.
Submit article via Google Form: For those unfamiliar with GitHub tools and processes, or who simply want to make a quick contribution, we offer a Google Form for submissions.

For details see our What To Contribute and How To Contribute pages.
",55
xndcn/smzdm.com,None,"smzdm.com
",10
SiddheshNan/Acer-A515-51G-Hackintosh,Rich Text Format,"Acer-A515-51G-Hackintosh
Supports MacOS 10.13.x and 10.14.x

What Works

 Audio w/ headphone jack
 Backlight
 WebCam
 CPU Speedstep (XCPM)
 Ethernet
 HDMI
 Battery Management
 Sleep + Wake
 Smart Touchpad + Gestures
 Usb 3.0 + Type C
 Sleep From (Lid)
 iGPU with disabled dGPU
 WiFi (2.4 + 5GHz) + BT by using BCM94352z
 Native hotkey support w/ Fn keys

What Doesn't Works

 SD Card Reader


Installation

Create a Bootable USB for OSx by using the guide by RehabMan [Guide] Booting the OS X installer on LAPTOPS with Clover.

Audio

The Sound Card is Realtek ALC255, which is drived by AppleALC on layout-id 3.
If headphones are not working, please see ALCPlugFix.

Bluetooth

The Native Atheros BT will work out-of-the Box, But you can't turn off the BT becouse Power Management is not supported;
To Fix this, you'll have to get a MacOS compatible wifi+BT card. The best choice will be BCM94352Z which has WiFi+BT.

Ethernet

The Ethernet is 8411B, and you'll need RealtekRTL8111.kext to Enable Gigabit Ethernet.

Graphics

The iGPU is Intel UHD Graphics 620, and its enabled using Ig-Platform-id=0x191E0000
The discrete graphics' name is NVIDIA GeForce MX150, and its disabled becuase macOS doesn't support Optimus technology. Plus Battery Life is Improved to 6-7 hours of Backup.
Native brightness hotkey support; using DSDT.aml patched from RehabMan's [Guide] Patching DSDT/SSDT for LAPTOP backlight control.

Touchpad

The touchpad works After Installing VoodooPs2Controller.kext, After that Install ApplePS2SmartTouchpad.kext in order to recognise the touchpad as native one, And after that all gestures will work fine.

USB

To Raise the Port Limit, We're using USBInjectAll.kext from RehabMan. You can also Find Patched SSDT-UIAC under ACPI/Patched Folder.

Wi-Fi

The Native Wi-Fi Card this Laptop uses is Atheros QCA9377. Since it is not supported natively, and there are none kexts to enable this card on macOS, We can't use this card and I don't think it'll get supported in the future.
The Best Choice will be to Replace current Card with BCM94352Z which has WiFI+BT, or BCM943602BAED Which Supports MacOS natively.
I Have Already Changed My Current WiFi card with BCM94352Z, you can find it on AliExpress for like $20-30.
I've Patched Config.plist for BCM94352Z and BCM943602BAED and also added Kexts for BT as well. Keep in mind, this laptop uses M.2(NGFF) Socket with A+E Key. Half size Card Won't Work.

Credits

Special Thanks to Acidanthera for most of the Kexts.
Special Thanks to RehabMan.
Thanks to Clover Bootloader.
Thanks to goodwin for ALCPlugfix.
Thanks to EMlyDinEsH for ApplePS2SmartTouchPad.kext.
Thanks to daliansky for Some Patches which I used here from XiaoMi-Pro.

",4
MUME/MMapper,C++,"MMapper




Download the latest version of MMapper
Features

Automatic room creation during mapping
Automatic connection of new rooms
Terrain detection (forest, road, mountain, etc)
Exits detections
Fast OpenGL rendering
Pseudo 3D layers and drag and drop mouse operations
Multi platform support
Group manager support to see other people on your map
Integrated mud client
Remote editing support

Usage
Set up your client according to the wiki instructions.
Build
Check out the wiki for Linux, Windows and Mac build instructions.
",14
betterscientificsoftware/betterscientificsoftware.github.io,Python,"betterscientificsoftware.github.io
Who We Are
Better Scientific Software is an organization dedicated to improving developer productivity and improving software sustainability for computational science and engineering (CSE).
This repository provides source material for the Better Scientific Software BSSw.io web portal. Better Scientific Software (BSSw) community members can contribute content using standard GitHub tools and processes. Contributions can be made via:

Web browser editing:  For many people (even BSSw project members), this is probably the preferred way.  GitHub provides a nice web editor for Markdown.
Cloning: If you have push access, you can clone and commit to this repository.  This approach could be best for remote editing and activities that span across multiple source files.
Forking: This option is like cloning, but works for anyone.  You can make edits to your own forked copy of the repo, either in a browser or from a local repository.  Contributions are submitted to BSSw by using a pull request.
Submit article via Google Form: For those unfamiliar with GitHub tools and processes, or who simply want to make a quick contribution, we offer a Google Form for submissions.

For details see our What To Contribute and How To Contribute pages.
",55
xndcn/smzdm.com,None,"smzdm.com
",10
lxsgdsgg/microservices-framework,Java,"microservice-framework 微服务框架
项目说明
1.registry 服务注册中心
netflix-eureka
微服务的注册、发现与管理

2.config-center 配置中心
spring-config
所有其他微服务模块都需要从配置中心读取配置文件，存放地址为classpath:/config/dev/{module-name}

3.common 通用模块
通用模块中一般放置一些通用的model和utils工具类

4.gateway 网关模块
netflix-zuul
微服务中的网关层

5 monitor 监控模块
spring-cloud-admin Hystrix-dashboard
负责监控整个微服务架构的运行情况

6 auth-center 认证模块
spring-security spring-security-oauth2
负责整个微服务的授权与鉴权，从而保证微服务的安全性。

7 user-center 用户模块
spring-boot netflix-feign
负责全局的用户、角色、权限

8 zipkin
spring-cloud-sleuth zipkin
微服务的数据收集以及链路追踪

9 log-center 日志中心 （待完成）
elasticSearch logstash kibana
日志收集、分析、处理

10 service A 和 service b
两个示例模块。微服务B模块通过feignClient声明式的调用service A中的服务

操作说明
1.首先运行个项目sql文件夹中的sql并启动redis
2.然后修改config-center中的各模块的相关数据库地址以及redis地址(注意各模块内还有相应的配置文件，主要包括为模块指定config-server和eureka-server的位置以及确定各模块自己的端口号)
3.按顺序启动服务
  registry--->config-center--->gateway--->auth-center--->其他微服务
4.通过地址可以访问serviceA中暴露的接口。 地址示例：localhost:{serviceA-port}/getJsonData/{name}
5.通过ServiceB的Feign接口可以声明式访问serviceA中的暴露的的接口。地址示例：localhost:{serviceB-port}/courseApi/{name}
6.通过密码模式访问受保护接口。
    a.auth-center获取访问受保护接口的token 地址示例：localhost:8762/uaa/oauth/token?grant_type=password&client_id=system&client_secret=system&scope=app&username=admin&password=admin
    b.根据获取的token访问受保护的接口 地址示例:localhost:8001/users/current?access_token=ad630d4d-24ab-4496-8294-49ae03e1ce2f
7.通过授权码模式访问受保护的接口
    a.通过浏览器访问http://localhost:8766/oauth/authorize?client_id=system&redirect_uri=http://localhost:9001/callback&response_type=code&scope=app
    b.输入admin admin
    c.点击确认授权，获取到授权码
    d.根据授权码获取access_token:localhost:8766/oauth/token?code=MXqYZb&grant_type=authorization_code&redirect_uri=http://localhost:9001/callback&scope=app(注意要添加header属性 [{""key"":""Content-Type"",""value"":""application/x-www-form-urlencoded"",""description"":""""}])
    e.根据token范围接口
    ps:若未说明由浏览器访问则均通过PostMan发送相关请求

Q&A
1.运行时报错：The server time zone value 'ÖÐ¹ú±ê×¼Ê±¼ä' is unrecognized or represents more than one time zone
解决方案：在mysql中执行：set GLOBAL time_zone='+8:00'
",4
daily-co/daily-js,JavaScript,"daily-js
The official front-end library for the Daily.co video calling API.

Manage call lifecycle and participant state
Respond to in-call events
Customize call layout and UI

API docs for creating and managing rooms, permission tokens,
recordings, and other resources are here:
https://docs.daily.co/reference
See a demo here:
https://www.daily.co/api/demo
Getting started
You can use this library from a <script> tag, as a CommonJS-style
module with require, or as an ES6-style module with import
(including within a <script type=""module""> context).
The easiest way to get started is to clone and build this repo, use
dist/daily-iframe.js in a script tag, then in your application code
call the window.DailyIframe.wrap() factory method.
git clone https://github.com/daily-co/daily-js.git
cd daily-js
npm install
npm run build

To explore the capabilities of this front-end API, see see demo/
and demo/README.md for running a local demo.
Sample html/js:
<html>
<head><title>basic video call events demo</title>
</head>
<body>

<iframe id=""call-frame""
        width=350 height=425
        allow=""camera; microphone; autoplay""
        style=""position: absolute;
               right: 1em;
               bottom: 3em;""></iframe>

<script>
function showEvent(e) {
  console.log('VIDEO CALL EVENT -->', e);
}

function run() {
  window.callFrame = window.DailyIframe
                        .wrap(document.getElementById('call-frame'));
  callFrame.on('loading', showEvent)
           .on('loaded', showEvent)
           .on('camera-error', showEvent)
           .on('started-camera', showEvent)
           .on('joining-meeting', showEvent)
           .on('joined-meeting', showEvent)
           .on('left-meeting', showEvent)
           .on('participant-joined', showEvent)
           .on('participant-updated', showEvent)
           .on('participant-left', showEvent)
           .on('recording-started', showEvent)
           .on('recording-stopped', showEvent)
           .on('recording-stats', showEvent)
           .on('recording-error', showEvent)
           .on('recording-upload-completed', showEvent)
           .on('message', showEvent)
           .on('error', showEvent);

  console.log('VIDEO CALL WRAPPER -->', callFrame);

  callFrame.join({ url: YOUR_DAILY_CO_MEETING_URL });
}
</script>
<script crossorigin src=""https://unpkg.com/@daily-co/daily-js@0.2.3"" onload=""run()""></script>

</body>
</html>

Of course, you can also use a bundler like webpack or rollup.
npm install @daily-co/daily-js


Then in your application code:
// webpack/node-style require
//
const DailyIframe = require('@daily-co/daily-js');
let callFrame = DailyIframe().wrap(MY_IFRAME);

// or, cutting-edge, super-whizzy import
//
import DailyIframe from '@daily-co/daily-js';
let callFrame = DailyIframe().wrap(MY_IFRAME);

The DailyIframe class
This main entry point for this library's functionality is the
DailyIframe class.
The class exposes methods and events for managing the call lifecycle,
managing participant state, and customizing video element
layout and styling.
Methods

factory methods

wrap(iframe, properties)
createTransparentFrame(properties)


instance methods

join(properties)
leave()
startScreenShare()
stopScreenShare()
iframe()
meetingState()
participants()
updateParticipant(sessionId, properties)
loadCss({ bodyClass, cssText, cssFile })
updateParticipants(propertiesObject)
localAudio()
localVideo()
setLocalAudio()
setLocalVideo()
setBandwidth({ kbs, trackConstraints })
on(eventName, callback)
once(eventName, callback)
off(eventName, callback)



Factory methods and top-level configuration
You don't ever need to call the DailyIframe constructor
directly. Instead, use one of the factory methods, wrap() or
createTransparentFrame().
Both factory methods accept a properties object. (You can also set
these properties when you call the join() method.)
// top-level configuration properties. can be passed to the factory
// method that creates the DailyIframe object, or to the join()
// method.
{
  url: <required: url of the meeting to join>
  token: <optional: meeting join token>
  cssFile: <optional: an external css stylesheet to load>
  cssText: <optional: inline css style text to load>
  bodyClass: <optional: class attributes to apply to the iframe body element>
}

wrap(iframe, properties)
Use this factory method to wrap an iframe DOM element that you've
already defined.
The first argument is the iframe you want to wrap. The second argument
is the properties object defined above. A properties argument is
optional. You can also set these properties when you call the join()
method.
createTransparentFrame(properties)
Use this factory method when you want to implement the call user
interface yourself. This method creates a full-width, full-height,
transparent iframe that ignores all pointer events. The iframe is
appended to the document.body.
Instance methods reference
join(properties)
Joins a meeting.
Takes the same properties object that the factory methods take. The
properties argument is optional, but the meeting url must be set
either here or previously.
Returns a promise, which resolves when the join completes. The promise
resolves with a participants object. This is the same participants
object that is passed to the joined-meeting event. You will often
want to do some call setup or UI updating as soon as a meeting is
joined. You can do that when the join() promise resolves, or by
installing a joined-meeting event listener. The two approaches are
pretty much equivalent.
async function joinExample() {
  let participants;
  try {
    participants await callFrame.join();
  } catch (e) {
    console.error('ERROR while joining meeting', e);
    return;
  }
  console.log('local mic is', participants.local.audio ? 'on': 'off');
}

leave()
Leaves the meeting. If there is no meeting, this method does
nothing. Returns null;
startScreenShare()
Starts a screen share from the local participant. If there is no
meeting, or this is not a browser that supports screen sharing, or
enable_screenshare is set to false for either the room or the
meeting token, this method does nothing.
There's no way to know if the user ignores or cancels the browser's
screen share confirmation dialog.
To confirm that screen sharing started, listen for
update-participant events and check the local user's screen
property.
Returns null.
stopScreenShare()
Stops a current screen share, if there is one.
Returns null.
iframe()
Returns the iframe DOM element that this object wraps.
meetingState()
Returns the current meeting state.

new
joining-meeting
joined-meeting
left-meeting
error

If an error is thrown, the meeting state will transition to 'error',
not 'left-meeting', even though the meeting connection will also be
terminated by the error.
participants()
Returns the current meeting participants. The participants information
is an object that looks like this:
{
  local: {
    user_id: ""user_123"",
    audio: true,
    cam_info: {
      height: 180,
      left: 286,
      top: 16,
      video_height: 720,
      video_width: 1280,
      width: 320,
    },
    video: true,
    screen: false,
    screen_info: {},
    joinedAt: Date(2019-04-30T00:06:16.011Z),
    local: true,
    owner: true,
    session_id: ""3c9ba1ea-baab-4876-d501-21a1d49c0902"",
    user_name: ""A. User Name""
  },
  ""e20b7ead-54c3-459e-800a-ca4f21882f2f"": {
    user_id: ""e20b7ead-54c3-459e-800a-ca4f21882f2f"",
    audio: true,
    cam_info: {}
    video: false,
    screen: false,
    screen_info: {}.
    joinedAt: Date(2019-04-30T00:06:32.485Z),
    local: false,
    owner: false,
    session_id: ""e20b7ead-54c3-459e-800a-ca4f21882f2f"",
    user_name: """"
  }
}

The object keys are 'local' for the local participant and the
participant's session_id for remote participants.
Participant properties are as follows:

session_id - a unique id generated each time a participant joins a meeting
user_id - the user's id if set by a meeting token, otherwise the session_id
user_name - the user's name if set by a meeting token or set from the account if the user is logged into a Daily.co account
local - true for the local user
owner - true if set by a meeting token or the user is logged into a Daily.co account and is a member of the room's team
joined_at - js Date object, the time that the user joined the room
audio - true if the user's mic is active
video - true if the user's camera is active
screen - true if the user is screen sharing
cam_info - properties of the participant's video element. top, left, width and height are the video element's global position as returned by the getBoundingClientRect() DOM method. video_width and video_height are the current width and height of the live video stream. video_width and video_height can change as network conditions change. If there is no current camera stream, this will be an empty object.
screen_info - properties of the participant's screen video element. This has the same properties as cam_info.

updateParticipant(sessionId, config)
Modify a participant, either by sending a message to change its state,
or by changing the local view.
Returns this.
The first argument is the participant's session_id, or 'local' for
the local participant.
The second argument is a set of actions to take.
Actions:

setAudio: true | false,
setVideo: true | false,
eject: true
styles: custom layout (see below)

setAudio, setVideo, and eject on remote participants require
meeting owner permission. If an action is not possible (or if there is
no current meeting) the action will be silently ignored.
Please note that remotely controlling a user's microphone and
camera is a potential privacy issue. This functionality is important
for some use cases, but should not be a general feature of video call
user interfaces. Think carefully before you enable remote control of
cameras and microphones. And be aware that browsers will require that
a user explicitly allow mic/camera device access at least once. Chrome
will prompt the first time a user joins a call on a specific
subdomain. Safari will prompt once each meeting session.
The styles action is only used if you are implementing your own
custom in-call video layout. The format of the styles property is:
styles: {
  cam: {
    div: { ...css properties }
    overlay: { ...css properties }
    video: { ...css properties }
  },
  screen: {
    div: { ...css properties }
    overlay: { ...css properties }
    video: { ...css properties }
  }
}

The styles.cam.div style css properties are applied to the container
div for the participant's camera stream. The style.cam.overlay style
css properties are applied to the overlay element for the
participant's camera stream. The styles.cam.video css properties are
applied to the video element for the participant's camera stream. The
styles.screen.div and styles.screen.video are applied to the
container and video element for the participant's screen share feed.
For example, to position the local camera feed and make it visible,
you only need to set a few css properties of the local participant's
styles.cam.div. Here's how you might ""shadow"" the position and size
of a placeholder div you've created:
let bounds = localVidPositioningEl.getBoundingClientRect();
callFrame.updateParticipant('local', {
  styles: {
    cam: {
      div: {
        visibility: 'visible',
        top: bounds.top,
        left: bounds.left,
        width: bounds.width,
        height: bounds.height
      }
    }
  }
});

See the next section, about the loadCss() method, for more
information about implementing custom layouts.
loadCss({ bodyClass, cssFile, cssText })
You can call this function any time to (re-)set the body classes and
CSS that you've passed into the iframe.
These three styling properties are used to implement completely custom
layouts. They are ignored unless you have constructed the DailyIframe
object using the createTransparentFrame() factory function.
The three styling properties can be passed to the factory function, to
the join() method, or to this loadCss() method.
See the demo/layout-css.html and demo/layout-css.css files for an
example of a custom layout with several dynamic options, implemented
entirely in css.
The bodyClass property is a string. The class attribute of the
body element inside the call iframe will be set to this string. You
can include multiple class names in the string. (Just separate the
class names with spaces.)
callFrame.loadCss({ bodyClass: 'theme-bubbles minimized-view' });


The cssFile property is the url of a stylesheet to fetch
externally. The url can be an absolute url, or a relative url. If it's
relative, the url will be resolved relative to the parent iframe.
Each call to loadCss() will replace the previous cssFile
stylesheet, if a cssFile property is passed to the method. (It can
sometimes be useful to switch stylesheets in the middle of a call.) To
remove the previous stylesheet, pass an empty string ('') as the
cssFile property.
 callFrame.loadCss({ cssFile: '/static/call-theme-bubbles.css' });

The cssText property is a string of css to load into the iframe
inside a <style> element.
Each call to loadCss() will replace the previous cssText
style element, if a cssText property is passed to the method.
// a very simple custom layout:
// this css will display every participant's
// video streams in a column down the right side
// of the window
//
callFrame.loadCss({ cssText: `
  .daily-video-div {
    position: relative;
    visibility: visible;
    width: 320;
    height: 180;
    margin: 1em;
    margin-left: auto;
  }
`});

The loadCss() method returns this.
CSS for custom layouts
Each available video stream in the video call iframe is wrapped in a
div, and has a sibling element that is a div you can use as an
overlay. You can style the video container, the overlay, and the video
element. You can also style a separate top level div, a div that
wraps all of the video elements, and an info div.
Here is the DOM structure of the elements in a call that you can
style.
<body class="" (bodyClass classes...) "">
  <div class=""daily-video-toplevel-div (toplevel classes...)"">

    <div class=""daily-videos-wapper (call-state classes)"">

      <div class=""daily-video-div (video classes...)""
           data-user-name="" (user_name) "">
        <div class=""daily-video-overlay (video classes...)""
             data-user-name="" (user_name) ""></div>
        <video class=""daily-video-element (video classes...)"">
               data-user-name="" (user_name) ""></video>
      </div>
      ... additional video elements

    </div>

    <div class=""info-div""></div>
  </div>
</body>

Here are the default styles for the container and video element classes.
.daily-video-toplevel-div {
   position: fixed;
   top: 0;
   left: 0;
   width: 100%;
   height: 100%;
}
.daily-videos-wrapper {
   position: fixed;
   top: 0;
   left: 0;
   width: 100%;
   height: 100%;
}
.daily-video-div {
  position: fixed;
  visibility: hidden;
}
.daily-video-overlay {
  position: absolute;
  width: 100%;
  height: 100%;
  z-index: 1;
}
.daily-video-element {
  position: absolute;
  width: 100%;
  overflow: hidden;
  height: 100%
}
.daily-video-element.local.cam {
  transform: scale(-1,1);
}

As you can see above, the visibility of the .daily-video-div
container elements is set to hidden initially. This means that until
you override the default styles, no video streams are displayed.
Note that all available audio is always played. Even when a
participant's video stream is hidden, that participant's audio is
audible.
Lists of classes that depend on call and participant state are
attached to the various elements listed above.
Additional classes of .daily-video-toplevel-div:

recording: the call is being recorded
recording-uploading: the recording is being done locally and saved to the cloud, and uploading to the cloud is in progress. This should always be true during a local cloud recording, and will stay true until the upload completes, even after the recording is stopped.

Additional classes of .daily-videos-wrapper:

local-cam-on: the local camera is turned on
local-cam-muted: the local camera is unavailable, blocked, or muted
local-screen: a local screen share is in progress
remote-cams-N: there are N remote video participants in the
call. This counts all video participants, even those that have no
camera or have muted their camera. It does not count dial-in
participants.
remote-cams-on-N: there are N remote cameras turned on
remote-cams-muted-N: there are N remote cameras unavailable or
muted
remote-screens-N: there are N remote screen shares in progress

Additional classes of .daily-video-div, .daily-video-overlay, and
.daily-video-element (the same set of additional classes is set for
each ""bundle"" of these elements):

local: this is video from the local participant
remote: this is video from a remote participant
cam: this is camera video
screen: this is screen sharing video
cam-on: the camera for this participant is turned on and streaming
cam-muted: the camera for this participant is unavailable, blocked, or muted
mic-on: the mic for this participant is turned on and streaming
mic-muted: the mic for this participant is unavailable, blocked, or muted

For example CSS-driven layouts, see the /demo directory.
updateParticipants(propertiesObject)
Syntactic sugar for updating multiple participants with a single
call. The propertiesObject's keys are participant session ids and
values are the properties objects described above. Internally, this
method just loops over the keys and calls updateParticipant()
multiple times.
Returns this.
localAudio()
Returns the local mic state or null if not in a call. Syntactic sugar for this.participants.local.audio.
localVideo()
Returns the local camera state or null if not in a call. Syntactic sugar for this.participants.local.video.
setLocalAudio(bool)
Updates the local mic state. Does nothing if not in a call. Syntactic sugar for this.updateParticipant('local', { audio: bool }).
Returns this.
setLocalVideo(bool)
Updates the local camera state. Does nothing if not in a call. Syntactic sugar for this.updateParticipant('local', { video: bool }).
Returns this.
setBandwidth({ kbs, trackConstraints })
Experimental method: Sets a cap on the upstream video bandwidth
used for each WebRTC peer connection. This API may change in the future.
In general we try to hide all the complexity of WebRTC so that you can
focus on your own application rather than the details of audio and
video network streams! We do bandwidth management, for example, that
is ""the right thing"" for most use cases (based on lots of empirical
call data, plus experience working around cross-platform quirks).
But sometimes you might need to reach down through the abstraction
boundary to do specialized things. We want to make that possible,
too. This method is an experiment in that direction. Please let us
know if you're using it, and whether it helps you, and what other
functionality you need for your applications.
The kbs property is a soft cap on the upstream video bandwidth used
for each peer connection. Currently this is implemented by setting
b=AS for each local
participant SDP m=video section. This mechanic may change in the
future, though, as browsers evolve.
Note that the kbs cap does not take into account audio bandwidth. We
don't currently support customizing audio bandwidth settings.
The trackConstraints property is a MediaTrackConstraints dictionary that will be applied to the local video track, if possible. Browser support for the MediaStreamTrack.applyConstraints() is still a work in progress. But support is improving rapidly.
Here's an example of using setBandwidth() to transmit 64x64 images
at a target video bandwidth cap of 32 kilobits per second.
callFrame.setBandwidth({
  kbs: 32,
  trackConstraints: { width: 64, height: 64 }
});

Returns this.
on(eventName, callback)  once(eventName, callback)  off(eventName, callback)
Adds and removes event callbacks. See documentation for EventEmitter.
Events
DailyIframe implements the EventEmitter interface.
You can install callbacks for the following events:

joining-meeting
joined-meeting
left-meeting
participant-joined
participant-updated
participant-left
error

The on(), once(), and off() methods add and remove
callbacks. All of these methods return the this object, so that it's
easy to chain calls.
// example of using on() to add event callbacks
//
callFrame.on('joining-meeting', (evt) => { 
               console.log('joining-meeting event', evt);
               showSpinner();
            })
         .on('joined-meeting', (evt) => {
               console.lg('joined-meeting event', evt);
               callFrame.iframe().style.visibility = 'visible';
            });

The event object passed to the callbacks always includes an action
property with the event's name, so that your callback functions can
handle multiple event types.
events reference
joining-meeting
Emitted when the join() method is called, while the call is loading
and connecting.
// example event object
{ action: 'joining-meeting' }

joined-meeting
Emitted when the call has connected. The participants property lists
the current participants in the call. See the participants() method
above for a description of the participant object.
// example event object
{
  action: 'joined-meeting',
  participants: {
    local: {
      audio: true,
      cam_info: {
        height: 180,
        left: 286,
        top: 16,
        video_height: 720,
        video_width: 1280,
        width: 320,
      },
      joinedAt: Date(Mon Apr 29 2019 15:18:20 GMT-0700),
      local: true,
      owner: true,
      screen: false,
      screen_info: {},
      session_id: '42fb115a-6d42-4155-ae4f-c96629f5217d',
      user_id: 'f26added-7821-49fc-9cb1-f9e22924b2c4',
      user_name: ""kwindla-desktop"",
      video: true
    }
  }
}

left-meeting
Emitted when the call disconnects.
// example event object
{ action: 'left-meeting' }

participant-joined
Emitted when a new participant joins the call. The event's
participant property contains all available information about the participant.
Please note that this
event may arrive for a remote participant before the
participant-joined event, because remote participant data can become
available before audio and video streams are ready.
// example event object
{
  action: 'participant-joined',
  participant: {
    audio: false,
    cam_info: {},
    joinedAt: Date(Mon Apr 29 2019 15:29:20 GMT-0700),
    local: false,
    owner: false,
    screen: false,
    screen_info: {},
    session_id: '049ebba2-523b-4e6c-9a9f-1f8bb956670d',
    user_id: '049ebba2-523b-4e6c-9a9f-1f8bb956670d',
    user_name: '',
    video: false,
  }
}

participant-updated
Emitted when participant state changes. This event is fired for both
local and remote participant state changes. The event's participant
property contains all available information about the participant.
// example event object
{
  action: 'participant-updated'
  participant: { ... }
}

participant-left
Emitted when a remote participant state leaves the call. The event's
participant property contains all of the available information about
the participant just before the participant disconnected.
// example event object
{
  action: 'participant-left'
  participant: { ... }
}

error
Emitted when an unrecoverable call error is encountered. The event's
errorMsg property will contain a string with additional information.
If a call is in progress when the error is thrown, a left-meeting
event should be emitted immediately after the error event.
// example event object
{
  action: 'error',
  errorMsg: 'network unreachable'
}

",3
paultyng/go-newrelic,Go,"go-newrelic-api

",9
jalyResource/GrowTextView,Objective-C,"iOS 一个比较完美的 Growing TextView（高度自适应输入框）
<img src=""https://upload-images.jianshu.io/upload_images/2591472-754d34bb3c0c126c.gif?imageMogr2/auto-orient/strip"" width=""250/>



GrowTextView




缘由
现在都 2019 年了，App 中使用自动增高的输入框已经很常见了，即时通讯的 Chat 界面、社交类 App 的评论功能都可以看到自增高输入框。但写出一个自增高输入框容易，写好难。现在市面上一些主流 App 的输入框依然会有一些瑕疵，例如：文字挡住一部分、粘贴大量文字时出现偏移，具体问题下面详细分析。
现在 iOS 开发已经过了搭建 UI 和普通业务功能的初级阶段，App 要想赢得用户的青睐，除了 App 的功能、UI 设计，交互体验的细节处理至关重要。一般用户只要使用输入框能正常输入即可，90% 的用户不会察觉输入框的一些细节，但作为开发人员应该知道这些细节(bug)并做出处理。
主流 App 的输入框之痛
粘贴文本出现文字偏移
这个问题严格来说算 bug，毕竟粘贴还是一个很常见的操作。

挡住部分文字
这个问题对 App 功能没有任何影响，但这么多 App 竟然都有这个问题而且非常普遍，是我始料未及的。测试了多个 App 后，只有QQ的输入框做的最好，粘贴、遮挡文字等问题根本不存在。





比较完美的输入框
写出一个自增高的输入框还是比较容易的，大致过程就是给 textView 添加左、右、下/上、高度四个约束，然后监听文字变化的通知，进而修改输入框的高度。我接下来主要说一下大家可能遇到的一些细节问题。
1、粘贴文本，文字偏移
我的做法是继承 UITextView 重写 setBounds 方法，重新调整contentOffset
- (void)setBounds:(CGRect)bounds
{
    [super setBounds:bounds];
    //    NSLog(@""bounds:%@"", NSStringFromCGRect(bounds));
    if (self.contentSize.height <= self.bounds.size.height + 1){
        self.contentOffset = CGPointZero; // Fix wrong contentOfset
    } else if (!self.tracking) {
        CGPoint offset = self.contentOffset;
        if (offset.y  > self.contentSize.height - bounds.size.height) {
            offset.y = self.contentSize.height - bounds.size.height;
            if (!self.decelerating && !self.tracking && !self.dragging) {
                self.contentOffset = offset;
            }
            // Fix wrong contentOfset when paster huge text
        }
    }
}

2、文字离输入框顶部间隙时大时小
正常情况下滚动输入框的文字，文字可以滚动到控件顶部。而 QQ 的输入框，不管怎么滑动文字，文字和输入框顶部都有一段固定间隔。

先了解输入框的一个属性textContainerInset，这个值默认是 (8, 0, 8, 0)，就是说默认情况下文字和输入框顶部有 8pt 的偏移，所以当文字输入较多的时候文字向上滚动，那么文字和控件顶部间隙会减小到 0。
实现QQ输入框的效果，我能想到的方案是把textContainerInset全设置为 0（或者top/bottom为0），这样文字就紧挨着输入框，文字和输入框顶部的固定间距就是 0 了。接着要给UITextView 添加一个 containerView ，containerView 竖向比 UITextView  高出一部分，从而实现 顶部/底部 的固定间距。
3、挡住部分文字
这个问题是因为 单行文字高度 * 最大行数  != 输入框的最大高度，输入框的最大高度可不是随便设置的，先确定输入框的font和最大行数，font.lineHeight * 行数就是输入框的最大高度。这样就不会出现文字挡住一部分的问题了
GrowTextView
接下来就是我自己写的自增高输入框了，目前没发现什么问题，至少没有上面涉及的问题。

GrowTextView

[Reference]

JSQMessagesViewController
袁峥-textView自适应文字高度

Requirements

ARC

Installation
Manual
Add JLTextContentView.h/m and JLTextView.h/m to your project.
Cocoapods. [Recommand]
Base is available through CocoaPods. To install
it, simply add the following line to your Podfile:
pod 'GrowTextView', '~> 1.0'
Author
wuzhenli, zhenli@6.cn
License
GrowTextView is available under the MIT license. See the LICENSE file for more info.
",90
bradzacher/mysqldump,TypeScript,"Mysql Dump
 
Create a backup of a MySQL database.
Installation
yarn add mysqldump
// or
npm install mysqldump

Usage
import mysqldump from 'mysqldump'
// or const mysqldump = require('mysqldump')

// dump the result straight to a file
mysqldump({
    connection: {
        host: 'localhost',
        user: 'root',
        password: '123456',
        database: 'my_database',
    },
    dumpToFile: './dump.sql',
})

// return the dump from the function and not to a file
const result = await mysqldump({
    connection: {
        host: 'localhost',
        user: 'root',
        password: '123456',
        database: 'my_database',
    },
})
Result
The returned result contains the dump property, which is split into schema and data.
export default interface DumpReturn {
    /**
     * The result of the dump
     */
    dump : {
        /**
         * The concatenated SQL schema dump for the entire database.
         * Null if configured not to dump.
         */
        schema : string | null
        /**
         * The concatenated SQL data dump for the entire database.
         * Null if configured not to dump.
         */
        data : string | null
        /**
         * The concatenated SQL trigger dump for the entire database.
         * Null if configured not to dump.
         */
        trigger : string | null
    }
    tables : Table[]
}

Options
All the below options are documented in the typescript declaration file:
export interface ConnectionOptions {
	/**
	 * The database host to connect to.
	 * Defaults to 'localhost'.
	 */
	host?: string;
	/**
	 * The port on the host to connect to.
	 * Defaults to 3306.
	 */
	port?: number;
	/**
	 * The database to dump.
	 */
	database: string;
	/**
	 * The DB username to use to connect.
	 */
	user: string;
	/**
	 * The password to use to connect.
	 */
	password: string;
	/**
	 * The charset to use for the connection.
	 * Defaults to 'UTF8_GENERAL_CI'.
	 */
	charset?: string;
}
export interface SchemaDumpOptions {
	/**
	 * True to include autoincrement values in schema, false otherwise.
	 * Defaults to true.
	 */
	autoIncrement?: boolean;
	/**
	 * True to include engine values in schema, false otherwise.
	 * Defaults to true.
	 */
	engine?: boolean;
	/**
	 * True to run a sql formatter over the output, false otherwise.
	 * Defaults to true.
	 */
	format?: boolean;
	/**
	 * Options for table dumps
	 */
	table?: {
		/**
		 * Guard create table calls with an ""IF NOT EXIST""
		 * Defaults to true.
		 */
		ifNotExist?: boolean;
		/**
		 * Drop tables before creation (overrides `ifNotExist`).
		 * Defaults to false.
		 */
		dropIfExist?: boolean;
		/**
		 * Include the `DEFAULT CHARSET = x` at the end of the table definition
		 * Set to true to include the value form the DB.
		 * Set to false to exclude it altogether.
		 * Set to a string to explicitly set the charset.
		 * Defaults to true.
		 */
		charset?: boolean | string;
	};
	view?: {
		/**
		 * Uses `CREATE OR REPLACE` to define views.
		 * Defaults to true.
		 */
		createOrReplace?: boolean;
		/**
		 * Include the `DEFINER = {\`user\`@\`host\` | CURRENT_USER}` in the view definition or not
		 * Defaults to false.
		 */
		definer?: boolean;
		/**
		 * Include the `ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}` in the view definition or not
		 * Defaults to false.
		 */
		algorithm?: boolean;
		/**
		 * Incldue the `SQL SECURITY {DEFINER | INVOKER}` in the view definition or not
		 * Defaults to false.
		 */
		sqlSecurity?: boolean;
	};
}
export interface TriggerDumpOptions {
	/**
	 * The temporary delimiter to use between statements.
	 * Set to false to not use delmiters
	 * Defaults to ';;'.
	 */
	delimiter?: string | false;
	/**
	 * Drop triggers before creation.
	 * Defaults to false.
	 */
	dropIfExist?: boolean;
	/**
	 * Include the `DEFINER = {\`user\`@\`host\` | CURRENT_USER}` in the view definition or not
	 * Defaults to false.
	 */
	definer?: boolean;
}
export interface DataDumpOptions {
	/**
	 * True to run a sql formatter over the output, false otherwise.
	 * Defaults to true.
	 */
	format?: boolean;
	/**
	 * Include file headers in output
	 * Defaults to true.
	 */
	verbose ?: boolean
	/**
	 * Use a read lock during the data dump (see: https://dev.mysql.com/doc/refman/5.7/en/replication-solutions-backups-read-only.html)
	 * Defaults to false.
	 */
	lockTables ?: boolean
	/**
	 * Dump data from views.
	 * Defaults to false.
	 */
	includeViewData?: boolean;
	/**
	 * Maximum number of rows to include in each multi-line insert statement
	 * Defaults to 1 (i.e. new statement per row).
	 */
	maxRowsPerInsertStatement?: number;
	/**
	 * True to return the data in a function, false to not.
	 * This is useful in databases with a lot of data.
	 *
	 * We stream data from the DB to reduce the memory footprint.
	 * However note that if you want the result returned from the function,
	 * this will result in a larger memory footprint as the string has to be stored in memory.
	 *
	 * Defaults to false if dumpToFile is truthy, or true if not dumpToFile is falsey.
	 */
	returnFromFunction?: boolean;
	/**
	 * A map of tables to additional where strings to add.
	 * Use this to limit the number of data that is dumped.
	 * Defaults to no limits
	 */
	where?: {
		[k: string]: string;
	};
}
export interface DumpOptions {
	/**
	 * The list of tables that you want to dump.
	 * Defaults to all tables (signalled by passing an empty array).
	 */
	tables?: string[];
	/**
	 * True to use the `tables` options as a blacklist, false to use it as a whitelist.
	 * Defaults to false.
	 */
	excludeTables?: boolean;
	/**
	 * Explicitly set to false to not include the schema in the dump.
	 * Defaults to including the schema.
	 */
	schema?: false | SchemaDumpOptions;
	/**
	 * Explicitly set to false to not include data in the dump.
	 * Defaults to including the data.
	 */
	data?: false | DataDumpOptions;
	/**
	 * Explicitly set to false to not include triggers in the dump.
	 * Defaults to including the triggers.
	 */
	trigger?: false | TriggerDumpOptions;
}
export interface Options {
	/**
	 * Database connection options
	 */
	connection: ConnectionOptions;
	/**
	 * Dump configuration options
	 */
	dump?: DumpOptions;
	/**
	 * Set to a path to dump to a file.
	 * Exclude to just return the string.
	 */
	dumpToFile?: string;
}
export interface ColumnList {
	/**
	 * Key is the name of the column
	 */
	[k: string]: {
		/**
		 * The type of the column as reported by the underlying DB.
		 */
		type: string;
		/**
		 * True if the column is nullable, false otherwise.
		 */
		nullable: boolean;
	};
}
export interface Table {
	/**
	 * The name of the table.
	 */
	name: string;
	/**
	 * The raw SQL schema dump for the table.
	 * Null if configured to not dump.
	 */
	schema: string | null;
	/**
	 * The raw SQL data dump for the table.
	 * Null if configured to not dump.
	 */
	data: string | null;
	/**
	 * The list of column definitions for the table.
	 */
	columns: ColumnList;
	/**
	 * An ordered list of columns (for consistently outputing as per the DB definition)
	 */
	columnsOrdered: string[];
	/**
	 * True if the table is actually a view, false otherwise.
	 */
	isView: boolean;
	/**
	 * A list of triggers attached to the table
	 */
	triggers: string[];
}
export interface DumpReturn {
	/**
	 * The result of the dump
	 */
	dump: {
		/**
		 * The concatenated SQL schema dump for the entire database.
		 * Null if configured not to dump.
		 */
		schema: string | null;
		/**
		 * The concatenated SQL data dump for the entire database.
		 * Null if configured not to dump.
		 */
		data: string | null;
		/**
		 * The concatenated SQL trigger dump for the entire database.
		 * Null if configured not to dump.
		 */
		trigger: string | null;
	};
	tables: Table[];
}
export default function main(inputOptions: Options): Promise<DumpReturn>;

export as namespace mysqldump;

The MIT License
Contributing
Installation
Make sure to first install all the required development dependencies:
yarn
// or
npm install .

Linting
We use eslint in conjunction with typescript-eslint-parser for code linting.
PRs are required to pass the linting with no errors and preferrably no warnings.
Testing
Tests can be run via the test script - yarn test / npm test.
Additionally it's required that you do a build and run your test against the public package to ensure the build doesn't cause regressions - yarn run test-prod / npm run test-prod.
PRs are required to maintain the 100% test coverage, and all tests must pass successfully.
",68
bradzacher/mysqldump,TypeScript,"Mysql Dump
 
Create a backup of a MySQL database.
Installation
yarn add mysqldump
// or
npm install mysqldump

Usage
import mysqldump from 'mysqldump'
// or const mysqldump = require('mysqldump')

// dump the result straight to a file
mysqldump({
    connection: {
        host: 'localhost',
        user: 'root',
        password: '123456',
        database: 'my_database',
    },
    dumpToFile: './dump.sql',
})

// return the dump from the function and not to a file
const result = await mysqldump({
    connection: {
        host: 'localhost',
        user: 'root',
        password: '123456',
        database: 'my_database',
    },
})
Result
The returned result contains the dump property, which is split into schema and data.
export default interface DumpReturn {
    /**
     * The result of the dump
     */
    dump : {
        /**
         * The concatenated SQL schema dump for the entire database.
         * Null if configured not to dump.
         */
        schema : string | null
        /**
         * The concatenated SQL data dump for the entire database.
         * Null if configured not to dump.
         */
        data : string | null
        /**
         * The concatenated SQL trigger dump for the entire database.
         * Null if configured not to dump.
         */
        trigger : string | null
    }
    tables : Table[]
}

Options
All the below options are documented in the typescript declaration file:
export interface ConnectionOptions {
	/**
	 * The database host to connect to.
	 * Defaults to 'localhost'.
	 */
	host?: string;
	/**
	 * The port on the host to connect to.
	 * Defaults to 3306.
	 */
	port?: number;
	/**
	 * The database to dump.
	 */
	database: string;
	/**
	 * The DB username to use to connect.
	 */
	user: string;
	/**
	 * The password to use to connect.
	 */
	password: string;
	/**
	 * The charset to use for the connection.
	 * Defaults to 'UTF8_GENERAL_CI'.
	 */
	charset?: string;
}
export interface SchemaDumpOptions {
	/**
	 * True to include autoincrement values in schema, false otherwise.
	 * Defaults to true.
	 */
	autoIncrement?: boolean;
	/**
	 * True to include engine values in schema, false otherwise.
	 * Defaults to true.
	 */
	engine?: boolean;
	/**
	 * True to run a sql formatter over the output, false otherwise.
	 * Defaults to true.
	 */
	format?: boolean;
	/**
	 * Options for table dumps
	 */
	table?: {
		/**
		 * Guard create table calls with an ""IF NOT EXIST""
		 * Defaults to true.
		 */
		ifNotExist?: boolean;
		/**
		 * Drop tables before creation (overrides `ifNotExist`).
		 * Defaults to false.
		 */
		dropIfExist?: boolean;
		/**
		 * Include the `DEFAULT CHARSET = x` at the end of the table definition
		 * Set to true to include the value form the DB.
		 * Set to false to exclude it altogether.
		 * Set to a string to explicitly set the charset.
		 * Defaults to true.
		 */
		charset?: boolean | string;
	};
	view?: {
		/**
		 * Uses `CREATE OR REPLACE` to define views.
		 * Defaults to true.
		 */
		createOrReplace?: boolean;
		/**
		 * Include the `DEFINER = {\`user\`@\`host\` | CURRENT_USER}` in the view definition or not
		 * Defaults to false.
		 */
		definer?: boolean;
		/**
		 * Include the `ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}` in the view definition or not
		 * Defaults to false.
		 */
		algorithm?: boolean;
		/**
		 * Incldue the `SQL SECURITY {DEFINER | INVOKER}` in the view definition or not
		 * Defaults to false.
		 */
		sqlSecurity?: boolean;
	};
}
export interface TriggerDumpOptions {
	/**
	 * The temporary delimiter to use between statements.
	 * Set to false to not use delmiters
	 * Defaults to ';;'.
	 */
	delimiter?: string | false;
	/**
	 * Drop triggers before creation.
	 * Defaults to false.
	 */
	dropIfExist?: boolean;
	/**
	 * Include the `DEFINER = {\`user\`@\`host\` | CURRENT_USER}` in the view definition or not
	 * Defaults to false.
	 */
	definer?: boolean;
}
export interface DataDumpOptions {
	/**
	 * True to run a sql formatter over the output, false otherwise.
	 * Defaults to true.
	 */
	format?: boolean;
	/**
	 * Include file headers in output
	 * Defaults to true.
	 */
	verbose ?: boolean
	/**
	 * Use a read lock during the data dump (see: https://dev.mysql.com/doc/refman/5.7/en/replication-solutions-backups-read-only.html)
	 * Defaults to false.
	 */
	lockTables ?: boolean
	/**
	 * Dump data from views.
	 * Defaults to false.
	 */
	includeViewData?: boolean;
	/**
	 * Maximum number of rows to include in each multi-line insert statement
	 * Defaults to 1 (i.e. new statement per row).
	 */
	maxRowsPerInsertStatement?: number;
	/**
	 * True to return the data in a function, false to not.
	 * This is useful in databases with a lot of data.
	 *
	 * We stream data from the DB to reduce the memory footprint.
	 * However note that if you want the result returned from the function,
	 * this will result in a larger memory footprint as the string has to be stored in memory.
	 *
	 * Defaults to false if dumpToFile is truthy, or true if not dumpToFile is falsey.
	 */
	returnFromFunction?: boolean;
	/**
	 * A map of tables to additional where strings to add.
	 * Use this to limit the number of data that is dumped.
	 * Defaults to no limits
	 */
	where?: {
		[k: string]: string;
	};
}
export interface DumpOptions {
	/**
	 * The list of tables that you want to dump.
	 * Defaults to all tables (signalled by passing an empty array).
	 */
	tables?: string[];
	/**
	 * True to use the `tables` options as a blacklist, false to use it as a whitelist.
	 * Defaults to false.
	 */
	excludeTables?: boolean;
	/**
	 * Explicitly set to false to not include the schema in the dump.
	 * Defaults to including the schema.
	 */
	schema?: false | SchemaDumpOptions;
	/**
	 * Explicitly set to false to not include data in the dump.
	 * Defaults to including the data.
	 */
	data?: false | DataDumpOptions;
	/**
	 * Explicitly set to false to not include triggers in the dump.
	 * Defaults to including the triggers.
	 */
	trigger?: false | TriggerDumpOptions;
}
export interface Options {
	/**
	 * Database connection options
	 */
	connection: ConnectionOptions;
	/**
	 * Dump configuration options
	 */
	dump?: DumpOptions;
	/**
	 * Set to a path to dump to a file.
	 * Exclude to just return the string.
	 */
	dumpToFile?: string;
}
export interface ColumnList {
	/**
	 * Key is the name of the column
	 */
	[k: string]: {
		/**
		 * The type of the column as reported by the underlying DB.
		 */
		type: string;
		/**
		 * True if the column is nullable, false otherwise.
		 */
		nullable: boolean;
	};
}
export interface Table {
	/**
	 * The name of the table.
	 */
	name: string;
	/**
	 * The raw SQL schema dump for the table.
	 * Null if configured to not dump.
	 */
	schema: string | null;
	/**
	 * The raw SQL data dump for the table.
	 * Null if configured to not dump.
	 */
	data: string | null;
	/**
	 * The list of column definitions for the table.
	 */
	columns: ColumnList;
	/**
	 * An ordered list of columns (for consistently outputing as per the DB definition)
	 */
	columnsOrdered: string[];
	/**
	 * True if the table is actually a view, false otherwise.
	 */
	isView: boolean;
	/**
	 * A list of triggers attached to the table
	 */
	triggers: string[];
}
export interface DumpReturn {
	/**
	 * The result of the dump
	 */
	dump: {
		/**
		 * The concatenated SQL schema dump for the entire database.
		 * Null if configured not to dump.
		 */
		schema: string | null;
		/**
		 * The concatenated SQL data dump for the entire database.
		 * Null if configured not to dump.
		 */
		data: string | null;
		/**
		 * The concatenated SQL trigger dump for the entire database.
		 * Null if configured not to dump.
		 */
		trigger: string | null;
	};
	tables: Table[];
}
export default function main(inputOptions: Options): Promise<DumpReturn>;

export as namespace mysqldump;

The MIT License
Contributing
Installation
Make sure to first install all the required development dependencies:
yarn
// or
npm install .

Linting
We use eslint in conjunction with typescript-eslint-parser for code linting.
PRs are required to pass the linting with no errors and preferrably no warnings.
Testing
Tests can be run via the test script - yarn test / npm test.
Additionally it's required that you do a build and run your test against the public package to ensure the build doesn't cause regressions - yarn run test-prod / npm run test-prod.
PRs are required to maintain the 100% test coverage, and all tests must pass successfully.
",68
Lombiq/Arithmetics,C#,"Unum - Proof of concept readme
This project was developed as part of Hastlayer, the .NET HLS tool that converts .NET programs into equivalent logic hardware implementations.
Its goal is to implement a Unum proof of concept: the number type and an example using it, all transformable with Hastlayer.
The project's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/arithmetics (Mercurial repository)
https://github.com/Lombiq/Arithmetics (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub. Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
About Unum
Unum is a new number format invented by Dr. John L. Gustafson that can be used to store any number with exact precision (or known error). It can be used to achieve better range and accuracy than IEEE floating point formats while eliminating the algebraic errors that the IEEE floats are prone to.
For more about its advantages see: http://ubiquity.acm.org/article.cfm?id=2913029.
",6
damng/hackernews-rss-with-inlined-content,Python,"hackernews-rss-inlined-content
Loads the hackerness rss and inlines the contents of the pages. Chrome with Selenium loads the page, dom-distiller makes the contents like they're in firefox's reader mode, and the resulting html is served as the entry description. The 300 or so entries become about 5mb. PDFs and things that yield no usible text preview will remain as they were on the old feed.
I invoke it as:
  xvfb-run python main.py

I used xvfb-run instead of headless mode because extensions are not supported in headless. One directory up, in ""../dat"", is a chrome user profile data directory that is copied for each instance of the browser run and deleted when finished. You can initialize it and add whatever extensions you want. The resulting rss file is then commited/pushed on here and served via gitpages.
This is hack level code.
The feed is available at https://damng.github.io/hackernews-rss-with-inlined-content/output.rss
If you find this useful, don't give me anything. Instead, go to http://templeos.org, donate to the TempleOS project and become a Templar. Dontate to the Brain and Behavior Research Foundation (https://www.bbrfoundation.org/).
",10
wbthomason/dotfiles,Emacs Lisp,".dotfiles
My configuration files. Things are mostly organized nicely into directories; I use GNU stow to
install the configuration via symlinks. Feel free to copy, modify, etc.
",2
QuantaPay/QWallet,JavaScript,"
Q-Wallet | A Wallet for Everyday
BTC, ETH, QPY and 50+ other cryptocurrencies in wallet available as an Android, iOS and Web app.
Using our click-to-pay and scan-to-pay payment system, customers experience fast and easy shopping from any device. Our responsive checkout system perfectly fits to any screen from smartphones and supports all major languages to accommodate global customers.
More info : https://qpay.group
",5
ademakov/Evenk,C++,"Evenk
A C++14 library for concurrent programming.
The primary target platform for now is Linux x86-64. Additionally it might
be used on Mac OS/X x86-64 but for the lack of the futex system call some
features do not function there.
",65
CloneTrooper1019/Roblox-Client-Tracker,Lua,"



What is this?
The Roblox Client Tracker is an unofficial repository that aims to provide detailed diff logs for changes to Roblox's engine.
It generates information by data mining files retrieved from Roblox's deployment servers. The information that is mined and presented in this repository is retrieved using publicly disclosed end-points.
Comparison Links
Use these links to compare versions of Roblox's client.

Compare roblox->gametest1
Compare roblox->gametest2
Compare gametest1->gametest2

Info Specification
API-Dump.json
This is a JSON version of Roblox's API Dump. It contains more data than the original API Dump and can be read into a data tree by most programming languages using a JSON parser.
This file is extracted using RobloxStudioBeta.exe -API API-Dump.json
API-Dump.txt
This is a readable version of Roblox's JSON API Dump. Its style is derived from the Legacy API Dump, but with a few alterations and improvements to the sorting of data. This file is generated from the Roblox API Dump Tool.
CppTree.txt
A very-rough hierarchical dump of the C++ class/enum type names that could be extracted from the symbol data of Roblox Studio's exe.
DeepStrings.txt
This is a sorted list of dumped strings from Roblox Studio's exe. There is some garbage data dumped into this file, but most of it should be legible.
FVariables.txt
This is a sorted list of fast variables, which are used by Roblox to toggle changes to the engine remotely on multiple platforms without having to redeploy the client.
Legacy-API-Dump.txt
This is the legacy version of Roblox's API Dump. It is a feature bundled with their game client that exports a readable version of Roblox's usable Lua API. The file is extracted using RobloxPlayerBeta.exe --API Legacy-API-Dump.txt
RobloxShaderData.csv
This CSV maps all of Roblox's known shaders, and which graphics APIs use them. Each mapped shader has a mapped name and shader-type.
rbxManifest.txt
A file that describes (almost) every file that is expected to be extracted from the zip files specified in rbxPkgManifest.txt
Every two lines of this file corresponds to a local file path, and the MD5 signature expected of the file extracted to that path.
Note that although most paths are relative to the root directory, some of them aren't (specifically, the files in Plugins/ and Qml/)
rbxPkgManifest.txt
A file that describes the zip files that should be fetched from Roblox's setup servers when assembling Roblox Studio.
The file starts with a line describing the version for the package manifest schema.
After the version, information about each file is listed sequentually as such:
FileName.ext
MD5 Signature
Compressed Size
Decompressed Size

These files are fetched from Roblox's servers via:
http://setup.{roblox}.com/{version-guid}-{FileName.ext}
(or https://s3.amazonaws.com/setup.{roblox}.com/{version-guid}-{FileName.ext})
Note that {roblox} switches out with either roblox, gametest1.robloxlabs or gametest2.robloxlabs.
rbxManifest.csv
A CSV version of rbxManifest.txt, made to be easier to read from GitHub.
rbxPkgManifest.csv
A CSV version of rbxPkgManifest.txt, made to be easier to read from GitHub.
version.txt
Describes the current version of Roblox Studio. Formatted as: (MajorRevision).(Version).(Patch).(Commit)
version-guid.txt
Describes the current GUID version of Roblox Studio.
",20
moelattma/Clubster,JavaScript,"Clubster
CMPS 115 Project
Clubster is a club-managing application for both iOS and Android.
The frontend is built using React Native, while the backend is built using Node, Express, and mongoose to communicate with MongoDB.
",2
ppy/osu-notification-server,TypeScript,"osu!notification server
Requirements

node 10+

Setup
Copy (or symlink) following files from osu!web project to this project's root directory:

oauth-public.key (located in storage in osu!web)
.env

Configurations:

Only through environment variable:

WEBSOCKET_BASEDIR: set if you need to explicitly specify path to the files above.
APP_ENV: defaults to development. The .env.${APP_ENV} file will be loaded first before .env.


Either environment variable or in .env files:

Refer to src/config.ts.



Lastly, osu!web needs to be configured to use redis-based session.
Run

yarn
yarn build
yarn serve

Development
Watch and rebuild automatically with yarn watch. The server doesn't auto-restart though.
Code linting (style check)
Run yarn lint --fix and additionally do manual fixes if needed.
Testing
To be added.
",11
libreoffice-ja/conf-libreoffice-jp,HTML,"LibreOffice Asia Conference 2019 site page
It uses middleman
How to test it on your own computer

install ruby 2.4.3

using rbenv or any alternatives is strongly recommended


install bundler gem

gem install bundler --no-doc

use bundler to install whole middleman-related gems

bundle install

now you can generate the site like this:

middleman server

and you can see your own instance via http://localhost:4567
enjoy!

",3
cms-sw/cms-sw.github.io,JavaScript,"CMSSW work pages
They include:

Actual documentation.
Various scripts to import log files sparse in /afs to the git repository.

Importing log files to the repository.
A reasonable amount of processed log files, usually in json format, can be
stored in this git repository and not cause scalability issues, since git
is extremely good at compressing similar files.
This allows us to serve integration builds results via Github
Pages
In order to populate the data directory:
git clone cms-sw.github.com
cd cms-sw.github.com
./process-logs --logdir <path-to-your-toplevel-log-directory>
make -j 20
git commit data -m'Results updated'
git push origin master

Contributing to repository.
This repository contains two branches - master and code. All user submitted changes should go to code branch which will then be merged into master branch. Auto-generated data such as JSON files submitted by Cms Bot should go directly in to master. This should solve PR issues like this.
",6
openSUSE/open-build-service,Ruby,"



Open Build Service
The Open Build Service (OBS) is a generic system to build and distribute binary packages from sources in an automatic, consistent and reproducible way. You can release packages as well as updates, add-ons, appliances and entire distributions for a wide range of operating systems and hardware architectures. More information can be found on openbuildservice.org.
The OBS consists of a backend and a frontend. The backend implements all the core functionality (i.e. building packages). The frontend provides a web application and XML API for interacting with the backend. Additionally there is a command line client (osc) for the API which is developed in a separate repository.
Licensing
The Open Build Service is Free Software and is released under the terms of the GPL, except where noted. Additionally, 3rd-party content (like, but not exclusively, the webui icon theme) may be released under a different license. Please check the respective files for details.
Community
You can discuss with the OBS Team via IRC on the channel #opensuse-buildservice. Or you can use our mailing list opensuse-buildservice@opensuse.org. Please refer to the openSUSE Mailing Lists page to learn about our mailing list subscription and additional information.
Development / Contribution
If you want to contribute to the OBS please checkout our contribution readme:-)
Source Code Repository Layout
The OBS source code repository is hosted on Github and organized like this:
    dist          Files relevant for our distribution packages
    docs          Documentation, examples and schema files
    src/api       Rails app (Ruby on Rails)
    src/backend   Backend code (Perl)

Installation
To run the OBS in production we recommend using our appliance which is the whole package: A recent and stable Linux Operating System (openSUSE) bundled and pre-configured with all the server and OBS components you need to get going.
If that is not for you because you have some special needs for your setup (e.g. different partition schema, SLES as base system, etc.) you can also install our packages and run a setup wizard.
After finishing the installation of your base system, follow these steps:


Add the OBS software repository with zypper. Please be aware, that the needed URL differs, depending on your Base Operating System. We use openSUSE Leap 42.1 in this example.
zypper ar -f http://download.opensuse.org/repositories/OBS:/Server:/2.7/openSUSE_42.1/OBS:Server:2.7.repo


Install the package
zypper in -t pattern OBS_Server


Run our setup wizard
/usr/lib/obs/server/setup-appliance.sh


Advanced Setup
If you have a more complex setup (e.g. a distributed backend) we recommend to read the Administration
chapter in our reference manual.
",458
unosquare/ffmediaelement,C#,"FFME: The Advanced WPF MediaElement Alternative






⭐️ Please star this project if you like it and show your appreciation via PayPal.Me

Current NuGet Release Status

If you would like to support this project, you can show your appreciation via PayPal.Me
Current Status: (2019-04-19) - Release 4.1.300 is now available, (see the Releases)
NuGet Package available here: https://www.nuget.org/packages/FFME.Windows/
FFmpeg Version: 4.1.1 32-bit or 64-bit

Please note the current NuGet realease might require a different version of the FFmpeg binaries than the ones of the current state of the source code.
Quick Usage Guide for WPF Apps
Here is a quick guide on how to get started.

Open Visual Studio (v2019 recommended), and create a new WPF Application. Target Framework must be 4.6.1 or above.
Install the NuGet Package from your Package Manager Console: PM> Install-Package FFME.Windows
You need FFmpeg shared binaries (64 or 32 bit, depending on your app's target architecture). Build your own or download a compatible build from Zeranoe FFmpeg Builds site.
Your FFmpeg build should have a bin folder with 3 exe files and some dll files. Copy all those files to a folder such as c:\ffmpeg
Within you application's startup code (Main method), set Unosquare.FFME.Library.FFmpegDirectory = @""c:\ffmpeg"";.
Use the FFME MediaElement control as any other WPF control.
For example: In your MainForm.xaml, add the namespace: xmlns:ffme=""clr-namespace:Unosquare.FFME;assembly=ffme.win"" and then add the FFME control your window's XAML: <ffme:MediaElement x:Name=""Media"" Background=""Gray"" LoadedBehavior=""Play"" UnloadedBehavior=""Manual"" />
To play files or streams, simply set the Source property: Media.Source = new Uri(@""c:\your-file-here"");. Since Source is a dependency property, it need to be set from the GUI thread.

Note: To build your own FFmpeg binaries, I recommend the Media Autobuild Suite but please don't ask for help on it here.
Additional Usage Notes

Remember: The Unosquare.FFME.Windows.Sample provides usage examples for plenty of features. Use it as your main reference.
The generated API documentation is available here

Features Overview
FFME is an advanced and close drop-in replacement for Microsoft's WPF MediaElement Control. While the standard MediaElement uses DirectX (DirectShow) for media playback, FFME uses FFmpeg to read and decode audio and video. This means that for those of you who want to support stuff like HLS playback, or just don't want to go through the hassle of installing codecs on client machines, using FFME might just be the answer.
FFME provides multiple improvements over the standard MediaElement such as:

Fast media seeking and frame-by-frame seeking
Properties such as Position, Balance, SpeedRatio, IsMuted, and Volume are all Dependency Properties.
Additional and extended media events. Extracting (and modifying) video, audio and subtitle frames is very easy.
Easily apply FFmpeg video and audio filtergraphs.
Extract media metadata and tech specs of a media stream (title, album, bit rate, codecs, FPS, etc).
Apply volume, balance and speed ratio to media playback.
MediaState actually works on this control. The standard WPF MediaElement severely lacks in this area.
Ability to pick media streams contained in a file or a URL.
Specify input and codec parameters.
Opt-in hardware decoding acceleration via devices or via codecs.
Capture stream packets, audio, video and subtitle frames
Perform custom stream reading and stream recording

... all in a single MediaElement control
FFME also supports opening capture devices. See example Source URLs below and issue #48
device://dshow/?audio=Microphone (Vengeance 2100):video=MS Webcam 4000
device://gdigrab?title=Command Prompt
device://gdigrab?desktop

If you'd like audio to not change pitch while changing the SpeedRatio property, you'll need the SoundTouch.dll library v2.1.1 available on the same directory as the FFmpeg binaries. You can get the SoundTouch library here.
About how it works
First off, let's review a few concepts. A packet is a group of bytes read from the input. All packets are of a specific MediaType (Audio, Video, Subtitle, Data), and contain some timing information and most importantly compressed data. Packets are sent to a Codec and in turn, the codec produces Frames. Please note that producing 1 frame does not always take exactly 1 packet. A packet may contain many frames but also a frame may require several packets for the decoder to build it. Frames will contain timing informattion and the raw, uncompressed data. Now, you may think you can use frames and show pixels on the screen or send samples to the sound card. We are close, but we still need to do some additional processing. Turns out different Codecs will produce different uncompressed data formats. For example, some video codecs will output pixel data in ARGB, some others in RGB, and some other in YUV420. Therefore, we will need to Convert these frames into something all hardware can use natively. I call these converted frames, MediaBlocks. These MediaBlocks will contain uncompressed data in standard Audio and Video formats that all hardware is able to receive.
The process described above is implemented in 3 different layers:

The MediaContainer wraps an input stream. This layer keeps track of a MediaComponentSet which is nothing more than a collecttion of MediaComponent objects. Each MediaComponent holds packet caching, frame decoding, and block conversion logic. It provides the following important functionality:

We call Open to open the input stream and detect the different stream components. This also determines the codecs to use.
We call Read to read the next available packet and store it in its corresponding component (audio, video, subtitle, data, etc)
We call Decode to read the following packet from the queue that each of the components hold, and return a set of frames.
Finally, we call Convert to turn a given frame into a MediaBlock.


The MediaEngine wraps a MediaContainer and it is responsible for executing commands to control the input stream (Play, Pause, Stop, Seek, etc.) while keeping keeping 3 background workers.

The PacketReadingWroker is designed to continuously read packets from the MediaContainer. It will read packets when it needs them and it will pause if it does not need them. This is determined by how much data is in the cache. It will try to keep approximately 1 second of media packets at all times.
The FrameDecodingWroker gets the packets that the PacketReadingWorker writes and decodes them into frames. It then converts those frames into blocks and writes them to a MediaBlockBuffer. This block buffer can then be read by something else (the following worker described here) so its contents can be rendered.
Finally, the BlockRenderingWorker reads blocks form the MediaBlockBuffers and sends those blocks to a plat-from specific IMediaRenderer.


At the highest level, we have a MediaElement. It wraps a MediaEngine and it contains platform-specific implementation of methods to perform stuff like audio rendering, video rendering, subtitle rendering, and property synchronization between the MediaEngine and itself.

A high-level diagram is provided as additional reference below.

Some Work In Progress
Your help is welcome!

I am planning the next version of this control, Floyd. See the Issues section.

Windows: Compiling, Running and Testing
Please note that I am unable to distribute FFmpeg's binaries because I don't know if I am allowed to do so. Follow the instructions below to compile, run and test FFME.

Clone this repository.
Download the FFmpeg shared binaries for your target architecture: 32-bit or 64-bit.
Extract the contents of the zip file you just downloaded and go to the bin folder that got extracted. You should see 3 exe files and multiple dll files. Select and copy all of them.
Now paste all files from the prior step onto a well-known folder. Take note of the full path. (I used c:\ffmpeg\)
Open the solution and set the Unosquare.FFME.Windows.Sample project as the startup project. You can do this by right clicking on the project and selecting Set as startup project. Please note that you will need Visual Studio 2019 with dotnet Core 3.0 SDK for your target architecture installed.
Under the Unosquare.FFME.Windows.Sample project, find the file App.xaml.cs and under the constructor, locate the line Library.FFmpegDirectory = @""c:\ffmpeg""; and replace the path so that it points to the folder where you extracted your FFmpeg binaries (dll files).
Click on Start to run the project.
You should see a sample media player. Click on the Open icon located at the bottom right and enter a URL or path to a media file.
The file or URL should play immediately, and all the properties should display to the right of the media display by clicking on the Info icon.
You can use the resulting compiled assemblies in your project without further dependencies. Look for ffme.win.dll.

Thanks
In no particular order

To the FFmpeg team for making the Swiss Army Knife of media. I encourage you to donate to them.
To Kyle Schwarz for creating and making Zeranoe FFmpeg builds available to everyone.
To the NAudio team for making the best audio library out there for .NET -- one day I will contribute some improvements I have noticed they need.
To Ruslan Balanukhin for his FFmpeg interop bindings generator tool: FFmpeg.AutoGen.
To Martin Bohme for his tutorial on creating a video player with FFmpeg.
To Barry Mieny for his beautiful FFmpeg logo

Similar Projects

Meta Vlc
Microsoft FFmpeg Interop
WPF-MediaKit
LibVLC.NET
Microsoft Player Framework

License

Please refer to the LICENSE file for more information.

",416
dotnet/jitutils,C#,"Dotnet JIT code gen utilities - jitutils
This repo holds a collection of utilities used by RyuJIT developers to
automate tasks when working on CoreCLR.
This project has adopted the Microsoft Open Source Code of Conduct.
For more information see the Code of Conduct FAQ
or contact opencode@microsoft.com with any additional questions or comments.
Summary
Current tools include:

Assembly diffs: jit-diff, jit-dasm, jit-dasm-pmi, jit-analyze.
CI jobs information: cijobs.
JIT source code formatting: jit-format.
General tools: pmi

Getting started

Clone the jitutils repo:

    git clone https://github.com/dotnet/jitutils



Install the 2.1 .NET Core SDK (including the dotnet command-line interface, or CLI) from here.


Build the tools:


    cd jitutils
    bootstrap.cmd

(on non-Windows, run bootstrap.sh. NOTE: On Mac, you need to first use ulimit -n 2048 or the dotnet restore part of the build will fail.)

Optionally, add the built tools directory to your path, e.g.:

    set PATH=%PATH%;<root>\jitutils\bin

",55
Ultimate-Hosts-Blacklist/Spam404,Python,"About Spam404

This filter protects you from online scams. This filter is regularly updated with data collected by Spam404.com.


About Ultimate-Hosts-Blacklist
Ultimate-Hosts-Blacklist serve a place to test and keep a track on each input sources that are present into Ultimate Hosts Blacklist.
As Ultimate Hosts Blacklist grew up it became impossible to test the whole repository, as it takes weeks to finish. That's why we use the GitHub organization system in order to create different repository for each list that are present into Ultimate Hosts Blacklist.

About PyFunceble
PyFunceble like Funceble is A tool to check domains or IP availability by returning 3 possible status: ACTIVE, INACTIVE or INVALID.
It also has been described by one of its most active user as:

[An] excellent script for checking ACTIVE, INACTIVE and EXPIRED domain names.

If you need further informations about PyFunceble or Funceble please report to our Wiki page and/or if you don't find any answer feel free to create an issue into one of the Dead Hosts's or Py-Funceble's repositories.
About the status returned by PyFunceble
For an up to date version of this part please report to the Status section of our Wiki.
ACTIVE
This status is returned when one of the following cases is met:


We can extract the expiration date from Lookup().whois().

Please note that we don't check if the date is in the past.



Lookup().nslookup() don't return server can't find domain-name.me: NXDOMAIN.


HTTOCode().get() return one the following code [100, 101, 200, 201, 202, 203, 204, 205, 206].


INACTIVE
This status is returned when all the following cases are met:

We can't extract the expiration date from Lookup().whois().
Lookup().nslookup() return server can't find domain-name.me: NXDOMAIN.

INVALID
This status is returned when the following case is met:


Domain extension has an invalid format or is unregistered in IANA Root Zone Database.

Understand by this that the extension is not present into the iana-domains-db.json file.



",3
mojaie/MolecularGraph.jl,Jupyter Notebook,"MolecularGraph.jl




MolecularGraph.jl is a graph-based molecule modeling and chemoinformatics analysis toolkit which is fully implemented in Julia.
Installation
 (v1.0) pkg> add MolecularGraph

Usage

Try examples and tutorials in the notebook directory
Documentation and API Reference

Features


Chemical structure file I/O

Structure image drawing and export to SVG
SDFile import/export (.sdf, .mol)
SMILES/SMARTS parser



Basic descriptors

Molecular weight, composition and formula
H-bond donor/acceptor
rotatable bonds
Aromaticity
Wildman-Crippen logP



Molecular graph topology

Ring, scaffold, connectivity
Graph traversal



Sub(super)structure

Library search by using SMARTS query
Subgraph isomorphism detection with VF2 algorithm
Node-induced and edge-induced
Constraints (mandatory/forbidden mapping)



Ontology-based functional group detection/analysis


Maximum common substructure (MCS)

By clique detection algorithm
Node-induced (MCIS) and edge-induced (MCES)
Connected and disconnected
Topological constraint (known as tdMCS)
Diameter restriction (MCS-DR) and graph-based local similarity (GLS)



License
MIT license
Copyright
(C) 2018-2019 Seiji Matsuoka
",11
m4rcu5nl/docker-pdns-recursor-alpine,Shell,"Lightweight pdns-recursor Docker image
   
Docker image for PowerDNS Recursor 4.1.3. Super lightweight thanks to the Alpine Linux 3.8 base image. Total image size for the current build is 27.2MB

Getting the image
You can either clone this repo and build the image yourself or pull it from Docker Hub. The :latest tag on Docker Hub is a daily automated build.
# Example command to build an image
sudo docker image build -t pdns-recursor .

# Example command to pull an image
sudo docker image pull m4rcu5/pdns-recursor:latest

Using the image
Default configuration
The recursor will work out of the box as long as you query it from a private network. To create a container without any additional configuration simply run:
docker container run \
    --detach \
    --hostname resolver.local \
    --name pdns-recursor \
    --mount type=bind,src=/etc/localtime,dst=/etc/localtime,readonly=true \
    m4rcu5/pdns-recursor:latest
Let's assume the container can be reached on 172.17.0.4. You can now query it with dig for example:
dig +tcp @172.17.0.4 google.com

; <<>> DiG 9.10.3-P4-Ubuntu <<>> +tcp google.com @172.17.0.4
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 55033
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;google.com.            IN  A

;; ANSWER SECTION:
google.com.     300 IN  A   216.58.211.110

;; Query time: 37 msec
;; SERVER: 172.17.0.4#53(172.17.0.4)
;; WHEN: Thu Aug 31 23:35:44 CEST 2017
;; MSG SIZE  rcvd: 55

Custom configuration
It is possible to overwrite default settings by defining them in custom .conf files in /data/recursor-conf.d/. As an example this repo contains a zone file for the localhost zone and a configuration file to tell the recursor it has authority for that zone. Let's assume this repo has been cloned to /opt of the container host resulting in something similar to this:
/opt
└── pdns-recursor
    └── data
        ├── recursor-conf.d
        │   └── auth-zones.conf
        ├── scripts
        └── zones
            └── localhost

Now this local folder can be mounted as a volume inside the container:
docker container run \
    --detach \
    --hostname resolver.local \
    --name pdns-recursor \
    --mount type=bind,src=/etc/localtime,dst=/etc/localtime,readonly=true \
    --mount type=bind,src=/opt/pdns-recursor/data,dst=/data,readonly=true \
    m4rcu5/pdns-recursor:latest
Now the recursor will look in /data/zones/localhost whenever it receives a query for that zone.
Let's say we also want the recursor to validate DNSSEC queries. This can be accomplished by creating a .conf file for it and restarting the container:
# Create the config file
echo 'dnssec=validate' > /opt/pdns-recursor/data/recursor-conf.d/dnssec.conf

# Restart the container
docker container restart pdns-recursor
A complete list of settings for the recursor can be found on https://doc.powerdns.com/md/recursor/settings/
",2
hchiam/in-browser-style-linter,JavaScript,"in-browser-style-linter
Do quick style checks on your page after it renders, and without leaving your browser:

Add settings.
Run in-browser. (Chrome/Firefox/IE)
See red buttons.

Or follow this demo: https://youtu.be/eK5jMvivitQ
STEP 1: To add a setting, edit this part of the JS code:
// Enter your desired settings here:
var settings = [
    {
        selector:'a', // CSS selector
        property:'color', // CSS property to check
        expectedValues:['red','rgb(88, 96, 105)'], // acceptable expected values of property
        // contains:true // OPTIONAL: boolean to say actual value can at least contain the expected value
        // innerHTML:'Some innerHTML text.' // OPTIONAL: you can be more specific than CSS selectors
    }
];
to something like this (try it on https://www.google.com):
// Enter your desired settings here:
var settings = [
{
    s:'a',
    p:'font-family',
    v:'avenir',
    i:'About'
},
{
    s:'a.gb_d',
    p:'font-family',
    v:'avenir',
    c:true
},
{
    s:'.gLFyf.gsfi',
    p:'color',
    v:['#eee','rgb(88, 96, 105)','#3e3e3e'],
},
];
(For details, see more info.)
STEP 2: Set up your browser to run the JS code:

Chrome extension: In-Browser Style Linter
Chrome: snippets
Firefox: Scratchpad
Internet Explorer: F12 > Console > paste the whole code into the terminal (paste after the "">"" symbol on the bottom) > Ctrl+Enter (or hit run)

STEP 3: You'll see ugly red buttons, like this:

Hovering over the error button shows the expected and actual values. See this demo: https://youtu.be/eK5jMvivitQ
More info: (click to expand)

Basics (""SVP"")
Minimal required info:
var settings = [
    {
        selector:'a', // a CSS selector like 'div span a:hover'
        property:'color', // a CSS property
        value:'red' // the expected value after page render
    }
];
All parameters have short forms to let you save on keystrokes. Here's an equivalent to the example above:
var settings = [
    {
        s:'a', // s is for selector
        p:'color', // p is for property
        v:'red' // v (or ev) is for expected value
    }
];


c is for Contains Value
To relax the matching of the property value to simply ""contain"" the expected value, set the optional parameter to true:
var settings = [
    {
        selector:'a',
        property:'background',
        value:'#333',
        contains:true // would not flag '#333 url(""img_tree.gif"") no-repeat fixed center' as error
    }
];
Alternatively:
var settings = [
    {
        s:'a',
        p:'background',
        v:'#333',
        c:true // would not flag '#333 url(""img_tree.gif"") no-repeat fixed center' as error
    }
];


Multiple Allowable Values = []
To specify several allowable expected values, use an array:
var settings = [
    {
        selector:'a',
        property:'color',
        value:['red', 'rgb(88, 96, 105)']
    }
];
This is also compatible with the ""contains"" option (see above).


i Can Be More Specific with innerHTML
To specify elements that have a specific innerHTML (in addition to the CSS selector), set the optional parameter value:
var settings = [
    {
        selector:'a',
        property:'color',
        innerHTML:'Some innerHTML text.', // check the color of <a> tags with this innerHTML
        value:'rgb(88, 96, 105)'
    }
];
Alternatively:
var settings = [
    {
        p:'a',
        p:'color',
        i:'Some innerHTML text.', // check the color of <a> tags with this innerHTML
        v:'rgb(88, 96, 105)'
    }
];

Like this project?
Another Chrome extension in the works: https://github.com/hchiam/in-browser-test-automator
",2
xlsdg/umi-dva-antd-mobile-starter,JavaScript,"umi-dva-antd-mobile-starter

Get started with Umi.js and Ant Design Mobile.

Getting Started
Install dependencies.
yarn
Start dev server.
yarn start
Build for production with minification.
yarn build
Build for development with minification.
yarn dev
License
MIT
",2
mojaie/MolecularGraph.jl,Jupyter Notebook,"MolecularGraph.jl




MolecularGraph.jl is a graph-based molecule modeling and chemoinformatics analysis toolkit which is fully implemented in Julia.
Installation
 (v1.0) pkg> add MolecularGraph

Usage

Try examples and tutorials in the notebook directory
Documentation and API Reference

Features


Chemical structure file I/O

Structure image drawing and export to SVG
SDFile import/export (.sdf, .mol)
SMILES/SMARTS parser



Basic descriptors

Molecular weight, composition and formula
H-bond donor/acceptor
rotatable bonds
Aromaticity
Wildman-Crippen logP



Molecular graph topology

Ring, scaffold, connectivity
Graph traversal



Sub(super)structure

Library search by using SMARTS query
Subgraph isomorphism detection with VF2 algorithm
Node-induced and edge-induced
Constraints (mandatory/forbidden mapping)



Ontology-based functional group detection/analysis


Maximum common substructure (MCS)

By clique detection algorithm
Node-induced (MCIS) and edge-induced (MCES)
Connected and disconnected
Topological constraint (known as tdMCS)
Diameter restriction (MCS-DR) and graph-based local similarity (GLS)



License
MIT license
Copyright
(C) 2018-2019 Seiji Matsuoka
",11
m4rcu5nl/docker-pdns-recursor-alpine,Shell,"Lightweight pdns-recursor Docker image
   
Docker image for PowerDNS Recursor 4.1.3. Super lightweight thanks to the Alpine Linux 3.8 base image. Total image size for the current build is 27.2MB

Getting the image
You can either clone this repo and build the image yourself or pull it from Docker Hub. The :latest tag on Docker Hub is a daily automated build.
# Example command to build an image
sudo docker image build -t pdns-recursor .

# Example command to pull an image
sudo docker image pull m4rcu5/pdns-recursor:latest

Using the image
Default configuration
The recursor will work out of the box as long as you query it from a private network. To create a container without any additional configuration simply run:
docker container run \
    --detach \
    --hostname resolver.local \
    --name pdns-recursor \
    --mount type=bind,src=/etc/localtime,dst=/etc/localtime,readonly=true \
    m4rcu5/pdns-recursor:latest
Let's assume the container can be reached on 172.17.0.4. You can now query it with dig for example:
dig +tcp @172.17.0.4 google.com

; <<>> DiG 9.10.3-P4-Ubuntu <<>> +tcp google.com @172.17.0.4
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 55033
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;google.com.            IN  A

;; ANSWER SECTION:
google.com.     300 IN  A   216.58.211.110

;; Query time: 37 msec
;; SERVER: 172.17.0.4#53(172.17.0.4)
;; WHEN: Thu Aug 31 23:35:44 CEST 2017
;; MSG SIZE  rcvd: 55

Custom configuration
It is possible to overwrite default settings by defining them in custom .conf files in /data/recursor-conf.d/. As an example this repo contains a zone file for the localhost zone and a configuration file to tell the recursor it has authority for that zone. Let's assume this repo has been cloned to /opt of the container host resulting in something similar to this:
/opt
└── pdns-recursor
    └── data
        ├── recursor-conf.d
        │   └── auth-zones.conf
        ├── scripts
        └── zones
            └── localhost

Now this local folder can be mounted as a volume inside the container:
docker container run \
    --detach \
    --hostname resolver.local \
    --name pdns-recursor \
    --mount type=bind,src=/etc/localtime,dst=/etc/localtime,readonly=true \
    --mount type=bind,src=/opt/pdns-recursor/data,dst=/data,readonly=true \
    m4rcu5/pdns-recursor:latest
Now the recursor will look in /data/zones/localhost whenever it receives a query for that zone.
Let's say we also want the recursor to validate DNSSEC queries. This can be accomplished by creating a .conf file for it and restarting the container:
# Create the config file
echo 'dnssec=validate' > /opt/pdns-recursor/data/recursor-conf.d/dnssec.conf

# Restart the container
docker container restart pdns-recursor
A complete list of settings for the recursor can be found on https://doc.powerdns.com/md/recursor/settings/
",2
hchiam/in-browser-style-linter,JavaScript,"in-browser-style-linter
Do quick style checks on your page after it renders, and without leaving your browser:

Add settings.
Run in-browser. (Chrome/Firefox/IE)
See red buttons.

Or follow this demo: https://youtu.be/eK5jMvivitQ
STEP 1: To add a setting, edit this part of the JS code:
// Enter your desired settings here:
var settings = [
    {
        selector:'a', // CSS selector
        property:'color', // CSS property to check
        expectedValues:['red','rgb(88, 96, 105)'], // acceptable expected values of property
        // contains:true // OPTIONAL: boolean to say actual value can at least contain the expected value
        // innerHTML:'Some innerHTML text.' // OPTIONAL: you can be more specific than CSS selectors
    }
];
to something like this (try it on https://www.google.com):
// Enter your desired settings here:
var settings = [
{
    s:'a',
    p:'font-family',
    v:'avenir',
    i:'About'
},
{
    s:'a.gb_d',
    p:'font-family',
    v:'avenir',
    c:true
},
{
    s:'.gLFyf.gsfi',
    p:'color',
    v:['#eee','rgb(88, 96, 105)','#3e3e3e'],
},
];
(For details, see more info.)
STEP 2: Set up your browser to run the JS code:

Chrome extension: In-Browser Style Linter
Chrome: snippets
Firefox: Scratchpad
Internet Explorer: F12 > Console > paste the whole code into the terminal (paste after the "">"" symbol on the bottom) > Ctrl+Enter (or hit run)

STEP 3: You'll see ugly red buttons, like this:

Hovering over the error button shows the expected and actual values. See this demo: https://youtu.be/eK5jMvivitQ
More info: (click to expand)

Basics (""SVP"")
Minimal required info:
var settings = [
    {
        selector:'a', // a CSS selector like 'div span a:hover'
        property:'color', // a CSS property
        value:'red' // the expected value after page render
    }
];
All parameters have short forms to let you save on keystrokes. Here's an equivalent to the example above:
var settings = [
    {
        s:'a', // s is for selector
        p:'color', // p is for property
        v:'red' // v (or ev) is for expected value
    }
];


c is for Contains Value
To relax the matching of the property value to simply ""contain"" the expected value, set the optional parameter to true:
var settings = [
    {
        selector:'a',
        property:'background',
        value:'#333',
        contains:true // would not flag '#333 url(""img_tree.gif"") no-repeat fixed center' as error
    }
];
Alternatively:
var settings = [
    {
        s:'a',
        p:'background',
        v:'#333',
        c:true // would not flag '#333 url(""img_tree.gif"") no-repeat fixed center' as error
    }
];


Multiple Allowable Values = []
To specify several allowable expected values, use an array:
var settings = [
    {
        selector:'a',
        property:'color',
        value:['red', 'rgb(88, 96, 105)']
    }
];
This is also compatible with the ""contains"" option (see above).


i Can Be More Specific with innerHTML
To specify elements that have a specific innerHTML (in addition to the CSS selector), set the optional parameter value:
var settings = [
    {
        selector:'a',
        property:'color',
        innerHTML:'Some innerHTML text.', // check the color of <a> tags with this innerHTML
        value:'rgb(88, 96, 105)'
    }
];
Alternatively:
var settings = [
    {
        p:'a',
        p:'color',
        i:'Some innerHTML text.', // check the color of <a> tags with this innerHTML
        v:'rgb(88, 96, 105)'
    }
];

Like this project?
Another Chrome extension in the works: https://github.com/hchiam/in-browser-test-automator
",2
xlsdg/umi-dva-antd-mobile-starter,JavaScript,"umi-dva-antd-mobile-starter

Get started with Umi.js and Ant Design Mobile.

Getting Started
Install dependencies.
yarn
Start dev server.
yarn start
Build for production with minification.
yarn build
Build for development with minification.
yarn dev
License
MIT
",2
cienciadedatos/descripcion-y-orientaciones,None,"Proyecto de traducción colaborativa de R4DS

Este proyecto tiene por objetivo desarrollar una traducción colaborativa del libro R for Data Science para la comunidad hispanoparlante. Se puede acceder a la versión en español en proceso y sin editar acá.
Código de conducta
El proyecto de traducción de R4DS es un espacio abierto que tiene por objetivo la generación de recursos para la comunidad hispanohablante a través de una dinámica de trabajo colaborativa. Durante el proceso de traducción y revisión, se espera que las personas puedan participar libremente, compartiendo ideas, haciendo sugerencias, proponiendo cambios, analizando puntos de vista, etc., en un ambiente de respeto y colaboración. Asimismo, se espera que quienes participen hagan críticas justas, constructivas y propositivas, no juicios de valor.
En todo momento se cautelará que este sea un ambiente libre de acoso y hostigamiento, independiente del sexo, identidad, género, edad, orientación sexual, discapacidad, apariencia física, tamaño corporal, raza, etnia, religión (o la falta de ella), ideología, nacionalidad, variante lingüística u opciones tecnológicas. No se tolerará el acoso ni el hostigamiento en ninguna de sus formas. Quienes incurran en este tipo de conductas serán marginados del proceso de trabajo de forma permanente. El lenguaje sexual y las imágenes de ese tipo no son apropiados en ningún espacio de este proyecto.
Materiales de trabajo
En este respositorio se encuentran los materiales con los lineamientos para el proceso de traducción y revisión:

En el archivo orientaciones para la traducción se describen los aspectos para tener en cuenta a la hora de traducir el texto y el código utilizado en cada capítulo.
En el archivo flujo de trabajo se señala la secuencia de acciones a seguir por parte de l*s participantes a lo largo del proceso de traducción y revisión de los capitulos.
En el archivo distribución de roles se explicitan las responsabilidades de las personas que participan en este proyecto y la asignación de capítulos para traducir y revisar.
En el archivo orientaciones para la revisión se entregan recomendaciones sobre los aspectos en los que es necesario fijarse al momento de revisar una traducción y el tipo de lenguaje a utilizar para hacer recomendaciones.

¿Cómo participar?
Este es un proyecto abierto a la comunidad en el que todas las personas pueden participar siempre y cuando adscriban nuestro código de conducta. Si tienes interés en sumarte al equipo, revisa los documentos de este repositorio para conocer cómo estamos organizando el trabajo y luego contáctanos a través del issue que hemos abierto para ese propósito.
",17
crspybits/SharedImages,Swift,"SharedImages
Screen grabs















Main Features
Private & self-owned social media
Users store their images in their own cloud storage (Dropbox or Google Drive), and safely share those images with invited others. Facebook users can be invited.
Private discussion threads per image
Conversations about the images are also stored in user cloud storage. And users that can access the images can access the discussion threads.
Development status
On the Apple App Store as Neebla. This app uses the SyncServerII backend.
Building this project
Straight out of the box, this project will not build cleanly. See these instructions.
",7
BenzLeung/benz-amr-recorder,JavaScript,"AMR 录音机
(README in English)
纯前端解码、播放、录音、编码 AMR 音频，无须服务器支持，基于 amr.js 和 RecorderJs。
注意：由于使用了 amr.js 做编码和解码，因此 js 文件（压缩后，未 gzip）接近 500 KB，使用前请考虑。
特性

方便的 API 实现解码、播放、录音、编码 AMR 文件。
支持 url 和 blob （即<input type=""file"">）方式获取 AMR。
支持将浏览器 <audio> 所支持的音频格式（例如 MP3 或 OGG 音频）转换成 AMR 音频。
编码后的 AMR 文件可下载，无须服务器。

Demo
demo.html
浏览器兼容性
最新的浏览器兼容性请参阅 Can I Use 。

仅播放：https://caniuse.com/#feat=audio-api
播放+录音：https://caniuse.com/#feat=stream

安装
方法一：引入 js 文件
<script type=""text/javascript"" src=""./BenzAMRRecorder.min.js""></script>
方法二：使用 npm
npm install benz-amr-recorder

var BenzAMRRecorder = require('benz-amr-recorder');
用法
注意： 建议把 initWithXXX() 或 play() 方法绑定到一个用户事件中（例如 click、touchstart）。因为几乎所有移动设备（以及桌面版 Chrome 70+）都禁止页面自动播放音频。参考：

https://webkit.org/blog/6784/new-video-policies-for-ios/
https://developers.google.com/web/updates/2017/09/autoplay-policy-changes

播放 AMR：
var amr = new BenzAMRRecorder();
amr.initWithUrl('path/to/voice.amr').then(function() {
  amr.play();
});
amr.onEnded(function() {
  alert('播放完毕');
})
播放本地文件：
<input type=""file"" id=""amr-file"" accept="".amr"">
var amr = new BenzAMRRecorder();
var amrFileObj = document.getElementById('amr-file');
amrFileObj.onchange = function() {
  amr.initWithBlob(this.files[0]).then(function() {
    amr.play();
  });
}
录制 AMR：
var amrRec = new BenzAMRRecorder();
amrRec.initWithRecord().then(function() {
  amrRec.startRecord();
});
下载 AMR：
window.location.href = window.URL.createObjectURL(amr.getBlob());
把 MP3 转换成 AMR （需要浏览器原生支持 MP3）：
var amrFromMp3 = new BenzAMRRecorder();
amrFromMp3.initWithUrl('path/to/file.mp3').then(function() {
  // 下载 amr 文件
  window.location.href = window.URL.createObjectURL(amrFromMp3.getBlob());
})
API
初始化对象
/**
 * 是否已经初始化
 * @return {boolean}
 */
amr.isInit();
/**
 * 使用浮点数据初始化
 * @param {Float32Array} array
 * @return {Promise}
 */
amr.initWithArrayBuffer(array);
/**
 * 使用 Blob 对象初始化（ <input type=""file"">）
 * @param {Blob} blob
 * @return {Promise}
 */
amr.initWithBlob(blob);
/**
 * 使用 url 初始化
 * @param {string} url
 * @return {Promise}
 */
amr.initWithUrl(url);
/**
 * 初始化录音
 * @return {Promise}
 */
amr.initWithRecord();
事件
注意：事件不会叠加，也就是说，新注册的事件将覆盖掉旧的事件。
/**
 * 播放
 * @param {Function} fn
 */
amr.onPlay(function() {
  console.log('开始播放');
});
/**
 * 停止（包括播放结束）
 * @param {Function} fn
 */
amr.onStop(function() {
  console.log('停止播放');
});
/**
 * 播放结束
 * @param {Function} fn
 */
amr.onEnded(function() {
  console.log('播放结束');
});
/**
 * 播放到结尾自动结束
 * @param {Function} fn
 */
amr.onAutoEnded(function() {
  console.log('播放自动结束');
});
/**
 * 开始录音
 * @param {Function} fn
 */
amr.onStartRecord(function() {
  console.log('开始录音');
});
/**
 * 结束录音
 * @param {Function} fn
 */
amr.onFinishRecord(function() {
  console.log('结束录音');
});
播放控制
/**
 * 播放
 */
amr.play();
/**
 * 停止
 */
amr.stop();
/**
 * 是否正在播放
 * @return {boolean}
 */
amr.isPlaying();
录音控制
/**
 * 开始录音
 */
amr.startRecord();
/**
 * 结束录音，并把录制的音频转换成 AMR
 * @return {Promise}
 */
amr.finishRecord();
/**
 * 放弃录音
 */
amr.cancelRecord();
/**
 * 是否正在录音
 * @return {boolean}
 */
amr.isRecording();
其他
/**
 * 获取音频的时间长度（单位：秒）
 * @return {Number}
 */
amr.getDuration();
/**
 * 获取 AMR 文件的 Blob 对象（用于下载文件）
 * @return {Blob}
 */
amr.getBlob();
尚未完成的特性

 使用 Worker 编码解码 AMR。
 暂停功能。
 播放进度控制。
 浏览器兼容性检查（#9 #11）。

许可
MIT.
",75
llvm/llvm-project,C++,"The LLVM Compiler Infrastructure
This directory and its subdirectories contain source code for LLVM,
a toolkit for the construction of highly optimized compilers,
optimizers, and runtime environments.
",1116
michaellee/michaelsoolee.com,HTML,"michaelsoolee.com
",7
xlsdg/umi-dva-antd-typescript-starter,TypeScript,"umi-dva-antd-typescript-starter

Get started with Umi.js and Ant Design.

Getting Started
Install dependencies.
yarn
Start dev server.
yarn start
Build for production with minification.
yarn build
Build for development with minification.
yarn dev
License
MIT
",3
crspybits/SharedImages,Swift,"SharedImages
Screen grabs















Main Features
Private & self-owned social media
Users store their images in their own cloud storage (Dropbox or Google Drive), and safely share those images with invited others. Facebook users can be invited.
Private discussion threads per image
Conversations about the images are also stored in user cloud storage. And users that can access the images can access the discussion threads.
Development status
On the Apple App Store as Neebla. This app uses the SyncServerII backend.
Building this project
Straight out of the box, this project will not build cleanly. See these instructions.
",7
BenzLeung/benz-amr-recorder,JavaScript,"AMR 录音机
(README in English)
纯前端解码、播放、录音、编码 AMR 音频，无须服务器支持，基于 amr.js 和 RecorderJs。
注意：由于使用了 amr.js 做编码和解码，因此 js 文件（压缩后，未 gzip）接近 500 KB，使用前请考虑。
特性

方便的 API 实现解码、播放、录音、编码 AMR 文件。
支持 url 和 blob （即<input type=""file"">）方式获取 AMR。
支持将浏览器 <audio> 所支持的音频格式（例如 MP3 或 OGG 音频）转换成 AMR 音频。
编码后的 AMR 文件可下载，无须服务器。

Demo
demo.html
浏览器兼容性
最新的浏览器兼容性请参阅 Can I Use 。

仅播放：https://caniuse.com/#feat=audio-api
播放+录音：https://caniuse.com/#feat=stream

安装
方法一：引入 js 文件
<script type=""text/javascript"" src=""./BenzAMRRecorder.min.js""></script>
方法二：使用 npm
npm install benz-amr-recorder

var BenzAMRRecorder = require('benz-amr-recorder');
用法
注意： 建议把 initWithXXX() 或 play() 方法绑定到一个用户事件中（例如 click、touchstart）。因为几乎所有移动设备（以及桌面版 Chrome 70+）都禁止页面自动播放音频。参考：

https://webkit.org/blog/6784/new-video-policies-for-ios/
https://developers.google.com/web/updates/2017/09/autoplay-policy-changes

播放 AMR：
var amr = new BenzAMRRecorder();
amr.initWithUrl('path/to/voice.amr').then(function() {
  amr.play();
});
amr.onEnded(function() {
  alert('播放完毕');
})
播放本地文件：
<input type=""file"" id=""amr-file"" accept="".amr"">
var amr = new BenzAMRRecorder();
var amrFileObj = document.getElementById('amr-file');
amrFileObj.onchange = function() {
  amr.initWithBlob(this.files[0]).then(function() {
    amr.play();
  });
}
录制 AMR：
var amrRec = new BenzAMRRecorder();
amrRec.initWithRecord().then(function() {
  amrRec.startRecord();
});
下载 AMR：
window.location.href = window.URL.createObjectURL(amr.getBlob());
把 MP3 转换成 AMR （需要浏览器原生支持 MP3）：
var amrFromMp3 = new BenzAMRRecorder();
amrFromMp3.initWithUrl('path/to/file.mp3').then(function() {
  // 下载 amr 文件
  window.location.href = window.URL.createObjectURL(amrFromMp3.getBlob());
})
API
初始化对象
/**
 * 是否已经初始化
 * @return {boolean}
 */
amr.isInit();
/**
 * 使用浮点数据初始化
 * @param {Float32Array} array
 * @return {Promise}
 */
amr.initWithArrayBuffer(array);
/**
 * 使用 Blob 对象初始化（ <input type=""file"">）
 * @param {Blob} blob
 * @return {Promise}
 */
amr.initWithBlob(blob);
/**
 * 使用 url 初始化
 * @param {string} url
 * @return {Promise}
 */
amr.initWithUrl(url);
/**
 * 初始化录音
 * @return {Promise}
 */
amr.initWithRecord();
事件
注意：事件不会叠加，也就是说，新注册的事件将覆盖掉旧的事件。
/**
 * 播放
 * @param {Function} fn
 */
amr.onPlay(function() {
  console.log('开始播放');
});
/**
 * 停止（包括播放结束）
 * @param {Function} fn
 */
amr.onStop(function() {
  console.log('停止播放');
});
/**
 * 播放结束
 * @param {Function} fn
 */
amr.onEnded(function() {
  console.log('播放结束');
});
/**
 * 播放到结尾自动结束
 * @param {Function} fn
 */
amr.onAutoEnded(function() {
  console.log('播放自动结束');
});
/**
 * 开始录音
 * @param {Function} fn
 */
amr.onStartRecord(function() {
  console.log('开始录音');
});
/**
 * 结束录音
 * @param {Function} fn
 */
amr.onFinishRecord(function() {
  console.log('结束录音');
});
播放控制
/**
 * 播放
 */
amr.play();
/**
 * 停止
 */
amr.stop();
/**
 * 是否正在播放
 * @return {boolean}
 */
amr.isPlaying();
录音控制
/**
 * 开始录音
 */
amr.startRecord();
/**
 * 结束录音，并把录制的音频转换成 AMR
 * @return {Promise}
 */
amr.finishRecord();
/**
 * 放弃录音
 */
amr.cancelRecord();
/**
 * 是否正在录音
 * @return {boolean}
 */
amr.isRecording();
其他
/**
 * 获取音频的时间长度（单位：秒）
 * @return {Number}
 */
amr.getDuration();
/**
 * 获取 AMR 文件的 Blob 对象（用于下载文件）
 * @return {Blob}
 */
amr.getBlob();
尚未完成的特性

 使用 Worker 编码解码 AMR。
 暂停功能。
 播放进度控制。
 浏览器兼容性检查（#9 #11）。

许可
MIT.
",75
llvm/llvm-project,C++,"The LLVM Compiler Infrastructure
This directory and its subdirectories contain source code for LLVM,
a toolkit for the construction of highly optimized compilers,
optimizers, and runtime environments.
",1116
michaellee/michaelsoolee.com,HTML,"michaelsoolee.com
",7
xlsdg/umi-dva-antd-typescript-starter,TypeScript,"umi-dva-antd-typescript-starter

Get started with Umi.js and Ant Design.

Getting Started
Install dependencies.
yarn
Start dev server.
yarn start
Build for production with minification.
yarn build
Build for development with minification.
yarn dev
License
MIT
",3
Happyxianyueveryday/myds,C++,"myds
(myds = my data-structure)
5月1日开始更新，目标不迟于5月20日更新完毕。
数据结构与算法复习，基于C++或者java重新实现基本的数据结构与算法。
因为时间有限，单元测试写的不够完善，若发现实现上的错误，欢迎在issue区指正，非常感谢。
目录（随时间更新）
基础部分
1. Linked-list: 单向链表 （5月1日已完成）
2. Louble-list: 双向链表
3. Tree: 二叉搜索树 （5月2日已完成）
4. Stack: 栈 （5月2日已完成）
5. Queue: 队列 （5月10日已完成）
6. Graph: 图 （5月4日已完成，readme文档也已完成）
7. Sort: 排序 （5月7日已完成，预定于5月16日更新readme文档）
8. Search: 查找 （5月11日已完成）
扩展部分
1. ThreadTrees: 线索树（5月15日已完成，readme文档也已完成）
2. BPlusTree: B+树 （预定于5月16日完成）
3. AVLTree: AVL树
4. BRTree: 红黑树
5. Hash: 哈希表
6. StrAlgorithm: 字符串算法（KMP,Manacher等）
数据结构leetcode习题精选
该部分统一见Application文件夹下的相关专题。
1. Application: 数据结构相关的leetcode习题精选
附件：Java版本实现
1. Java: 数据结构的Java实现版本
",4
osrf/homebrew-simulation,Ruby,"


Build
Status




macOS 10.12, 10.14 travis-ci



macOS 10.13 azure pipeline




homebrew-simulation
Homebrew tap for osrf simulation software
To use:
brew tap osrf/simulation
",24
duboya/CTR-Prediction,Jupyter Notebook,"CTR Prediction 论文、个人学习笔记分享
动态更新学习中实现或者阅读过的计算广告相关论文、学习资料和业界分享，作为自己学习的总结，也希望能为计算广告相关行业的同学带来便利。
同时欢迎对CTR Prediction感兴趣的同学与我(杜博亚)讨论相关问题，我的联系方式如下：

Email: duboyabz@163.com
知乎私信: 杜博亚的知乎
个人博客: https://blog.csdn.net/dby_freedom?t=1

其中，个人博客收录了本人关于CTR的理论、实践总结，欢迎访问、关注~
目录
Optimization Method
Online Optimization，Parallel SGD，FTRL等优化方法，实用并且能够给出直观解释的文章

Google Vizier A Service for Black-Box Optimization.pdf
Google的深度学习自动调参框架Vizier
在线最优化求解(Online Optimization)-冯扬.pdf
非常推荐冯扬的这个教程，把在线优化问题讲的非常透
Hogwild A Lock-Free Approach to Parallelizing Stochastic Gradient Descent.pdf
Parallelized Stochastic Gradient Descent.pdf
A Survey on Algorithms of the Regularized Convex Optimization Problem.pptx
Follow-the-Regularized-Leader and Mirror Descent- Equivalence Theorems and L1 Regularization.pdf
A Review of Bayesian Optimization.pdf
Taking the Human Out of the Loop- A Review of Bayesian Optimization.pdf
非线性规划.doc

CTR Prediction
作为计算广告的核心，CTR预估永远是研究的热点，下面每一篇都是非常流行的文章，推荐逐一精读

Deep Crossing- Web-Scale Modeling without Manually Crafted Combinatorial Features.pdf 
Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction.pdf 
阿里提出的Large Scale Piece-wise Linear Model (LS-PLM) CTR预估模型
[GBDT+LR]Practical Lessons from Predicting Clicks on Ads at Facebook.pdf 
[FNN]Deep Learning over Multi-field Categorical Data.pdf 
Entire Space Multi-Task Model_ An Effective Approach for Estimating Post-Click Conversion Rate.pdf 
Deep Interest Network for Click-Through Rate Prediction.pdf 
Bid-aware Gradient Descent for Unbiased Learning with Censored Data in Display Advertising.pdf 
RTB 中训练 CTR 模型数据集是赢得出价的广告，预测时的样本却是所有候选的广告，也就是训练集和测试集的分布不一致，这篇文章就是要消除这样的 bias
[Multi-Task]An Overview of Multi-Task Learning in Deep Neural Networks.pdf 
Ad Click Prediction a View from the Trenches.pdf 
Google大名鼎鼎的用FTRL解决CTR在线预估的工程文章，非常经典。
[PNN]Product-based Neural Networks for User Response Prediction.pdf 
Image Matters- Visually modeling user behaviors using Advanced Model Server.pdf 
阿里提出引入商品图像特征的（Deep Image CTR Model）CTR预估模型，并介绍其分布式机器学习框架 Advanced Model Server (AMS)
[Wide & Deep]Wide & Deep Learning for Recommender Systems.pdf 
[DeepFM]- A Factorization-Machine based Neural Network for CTR Prediction.pdf 
Logistic Regression in Rare Events Data.pdf 
样本稀少情况下的LR模型训练，讲的比较细
Deep & Cross Network for Ad Click Predictions.pdf 
Google 在17年发表的 Deep&Cross 网络，类似于 Wide&Deep, 比起 PNN 只做了特征二阶交叉，Deep&Cross 理论上能够做任意高阶的特征交叉
Learning Deep Structured Semantic Models for Web Search using Clickthrough Data.pdf 
Adaptive Targeting for Online Advertisement.pdf 
一篇比较简单但是全面的CTR预估的文章，有一定实用性

参考文献
[1] https://github.com/wzhe06/Ad-papers/blob/master/README.md
[2] https://github.com/duboya/ML_CIA
",12
xlsdg/umi-mobx-antd-mobile-starter,JavaScript,"umi-mobx-antd-mobile-starter

Get started with Umi.js and Ant Design Mobile.

Getting Started
Install dependencies.
yarn
Start dev server.
yarn start
Build for production with minification.
yarn build
Build for development with minification.
yarn dev
License
MIT
",2
SplendidStrontium/splendidstrontium.github.io,CSS,"splendidstrontium.github.io
@TODO

sitemap.html

",2
mytardis/mydata,Python,"MyData
    
Desktop application for uploading data to MyTardis
See:

MyData Documentation @ Read the Docs

",6
bluecats/bluecats-android-sdk,None,"BlueCats SDK for Android
  
The BlueCats' SDKs have been developed for easy integration and to offer flexibility across different use cases. Please email us at developers@bluecats.com if you have any questions!
See the BlueCats Developer Portal for SDK documentation and getting started guides.
Need some beacons? Check out our online store for a StarterPack or email our sales team.
Change Logs
v2.1.4-r2

Added additional security features.

v2.1.4

Fixed an issue with the Local Storage Manager being null when the SDK is stopped and restarted in the background on Android 8.

v2.1.3

Added an embedded BCBeaconManagerCallback instance to enable background scan for the situations when the BlueCatsSDK is running but no app compoments are able to receive beacons list, especially for Android 8+. See Sample Code here in background-scan branch

v2.1.2

Support Android O+ for the background execution limits.
Stability Update.

v2.0.7

Stability Update. Fix for ANR scenario during SDK shutdown after stopPurring request.

v2.0.5

Stability Update. Fix for potential crash scenario during SDK shutdown after stopPurring request.

v2.0.4

Stability Update. Fixes sqlite warnings. Adds support for StrictMode during debug. Fixes for potential Local Storage (sqlite) crashes.

v2.0.3

Optimised Beacon sync.
Optimised stability.

v2.0.2

A new BCLogManager is introduced to manage SDK logs in scope of App for debug purpose.See example
Optimised beacon scanner for Android 7.

v2.0.1

Eddystone formats are supported.
A new BCBeaconManager is available to filter BlueCats Beacon, iBeacon, Eddystone, etc.
Google Play Service dependency is removed.

See Release Notes and the migration guide for more details.
Android SDK Installation
Step 1.
a) Using Android Studio
Installing the SDK into your project by adding the following to your build.gradle:
implementation 'com.bluecats:bluecats-android-sdk:2.1.4'
It will automatically add the extra dependencies for you:
compile 'com.google.code.gson:gson:2.4'
compile 'com.android.support:support-compat:24+'
compile 'com.android.support:support-core-utils:24+'
If you want to use your own dependencies on Gson and Support with another version, exclude the transitive dependencies like this:
compile('com.bluecats:bluecats-android-sdk:2.1.4', {
        exclude group: 'com.google.code.gson', module: 'gson'
        exclude group: 'com.android.support', module: 'support-compat'
        exclude group: 'com.android.support', module: 'support-core-utils'
    })
Please NOTE that missing Gson or Support dependencies will cause a runtime crash due to ClassNotFoundException, so only exclude them from BlueCats SDK if you are sure you've included the correct dependencies in build.gradle.
b) Using Eclipse
Follow the instructions detailed here.
Step 2.
Add the following to your ProGuard rules:
-renamesourcefileattribute SourceFile
-keepattributes LineNumberTable, SourceFile

-keep class com.bluecats.sdk.** {*;}

Step 3.
Get an app token from app.bluecats.com/apps by clicking ""Create New App"" and giving your new app a name. Once created, your new app should appear in the list and you'll be able to access your app token by clicking the ""Show App Token / Secret"" button below it.
NOTE: If you don't already have an account on app.bluecats.com, you will need to create one in order to progress past this point. If you're unsure of what to do, send an email to sales@bluecats.com and we'll gladly help you out!
Requirements
AndroidManifest.xml
While the following XML isn't vital to add manually (the SDK adds it automatically), it's kept here for the sake of ensuring you understand what is required of the SDK. The permissions and service is the absolute minimum required to allow the SDK to run.
<uses-permission android:name=""android.permission.BLUETOOTH"" />
<uses-permission android:name=""android.permission.BLUETOOTH_ADMIN"" />
<uses-permission android:name=""android.permission.INTERNET"" />
<uses-permission android:name=""android.permission.ACCESS_FINE_LOCATION"" />
<uses-permission android:name=""android.permission.ACCESS_NETWORK_STATE"" />
<uses-permission android:name=""android.permission.WAKE_LOCK"" />

<application ...>
  ...
  <service android:enabled=""true"" android:name=""com.bluecats.sdk.BlueCatsSDKService"" android:permission=""android.permission.BIND_JOB_SERVICE""/>
</application>
If you want the SDK to start when the device starts, you may add this to your xml:
<uses-permission android:name=""android.permission.RECEIVE_BOOT_COMPLETED"" />

<application ...>
    ...

    <receiver android:name=""com.bluecats.sdk.BlueCatsSDKServiceReceiver"" >
        <intent-filter>
            <action android:name=""android.intent.action.BOOT_COMPLETED"" />
        </intent-filter>
    </receiver>
</application>
Android 8.0+ (Oreo) Background Execution Limits
Android 8 imposes limitations on what apps can do while running in the background. If there are no foreground componants running(include Activity or Foreground Service), the system will then shutdowns the app's background services as if the app had had called the services' Service.stopSelf() methods. And calling startService() method with no foreground componants running will cause an IllegalStateException to be thrown. See more
Android 6.0+ (Marshmallow) Location Permissions
Android 6.0+ introduces a new permission model which changes the way Location Services permissions are managed. Before scanning for beacons the app is required to request location permissions from the user via a system dialogue. Detailed information on implementing location permission support in Android 6.0 can be found here.
Android SDK 4.3+ (API Level 18+)
The SDK will still work on lower versions of Android, but Bluetooth Low Energy Scanning will be disabled. You will still be able to make use of the Sites Nearby functionality as this does not involve scanning for beacons.
Sample Code
See the sample BlueCats Scratching Post app for usage and integration instructions.
Building Environment of Release v2.1.4-r2

Android Studio 3.4.1
Android Gradle Plugin 3.1.2
Gradle Version 4.4
Android NDK r17
Android SDK Tools 26.1.1
Android SDK Platform-Tools 28.0.3
Android SDK Build Tools 28.0.3
Java Version: 8
OS: MAC OSX 10.13.6

Have a Question?

If you've found a bug, please open an issue.
If you have a general question, see our support center for articles on our platform and beacons.
If you want a particular feature, please open an issue.
If taking a look at our SDK for the first time, please see our Android SDK documentation.

",15
Gerry-Lee/Study,C,"该项目记录所有学习的心得，包括网站、工具、新的领域知识等等，使用分层架构，每个目录下都有相应的readme文件，用于描述该目录的内容。
",10
MXET/SCUTTLE,Python,"S.C.U.T.T.L.E.
Sensing, Connected, Utility Transport Taxi for Level Environments (SCUTTLE).
This repository contains Python and Matlab Code for Texas A&M's open-source Scuttle Mobile Robot.
Computer Vision Demo: https://youtu.be/9t1XHcomlIs
90lbs Payload Demo: https://youtu.be/xK2SHM6fj18
Animated assembly (shows all parts) https://youtu.be/1VYjcl6etOM

Info for Assembly:
Bill of Materials is found in CAD folder
Hardware Parts instructions - https://youtu.be/wpSIqTLZpCg
3D Printing instructions - https://youtu.be/PXD6mWnY9d0
Wiring the battery pack - https://youtu.be/JS_9AhtAyLg
Battery pack, soldering the bottom - https://youtu.be/A2vBMLaxQs0
Battery pack, heat-set inserts - https://youtu.be/Fhat7w075Js
Battery pack, crimping insluated terminals  - https://youtu.be/GCT9hjeIvX8
Battery Cells, proper use - https://youtu.be/fC4sgXplA3k
Wiring, power wires overview - https://youtu.be/E7_NHTZwens
Wiring, crimping signal wires - https://youtu.be/388paGI_ecE
Wiring, I2C bus board - https://youtu.be/THejHu2klQM
Wiring, I2C board soldering - https://youtu.be/_GyacXFINLY
Wiring, I2C bracket - https://youtu.be/_ZNiIEPJr7c
Wiring, anatomy of duPont crimp - https://youtu.be/388paGI_ecE
Motors, soldering wires - https://youtu.be/d_CcTHs64qQ
Wheels, gluing the pulley - https://youtu.be/daPXxpQAJaQ
Wheels, assembling drivetrain - https://youtu.be/BN1E99_LWlo
Encoders, mounting - https://youtu.be/DEkEQ5825po
Project Writeup:
https://www.hackster.io/112547/scuttle-mobile-utility-robot-from-texas-a-m-d6c2b7
Info for Programming & Testing
Connect by SSH and test motors - https://youtu.be/gOEy8NsPlZY
Change your debian password - https://youtu.be/Scn5cUx_RIU
Connect to WiFi (WPA Enterprise) - https://youtu.be/6MDQHE0V518
Encoders, accuracy to expect - https://youtu.be/WdWj5KSYGlc
Battery, voltage checking - https://youtu.be/yBV0TCLIw5Y
Testing Beaglebone with RC_test - https://youtu.be/QZSLSp3yt1U
Using Matlab GUI v1.1 - https://youtu.be/fwuoglO3J0k
Running color_tracking_v1.py - https://youtu.be/PZ6i2W_9lJE
Check devices on i2c bus - https://youtu.be/L2ZTV-0t43Y
Lessons / Failures
Epoxy does not stick to ABS - https://youtu.be/M9cL-bfdHPk
Slightly oversized print - failed belt mesh - https://youtu.be/F0HP_MXiXHo
Slipping with unlevel chassis - https://youtu.be/JfYrRua7tiw
Screws in PLA issue (student share) - https://youtu.be/qfHjSpBnqg8
Dupont Crimps quality/testing (student share) - https://youtu.be/p01EBFcK8_E
How to cut a PCB  - https://youtu.be/4vjoToIsxR4
Info for Design Modifications
Modifying the wheel pulley - https://youtu.be/8o-XcZ3_teM
",8
calvinchankf/AlgoDaily,Python,"AlgoDaily
I believe that practising algorithms every day is a long-term investment in my life.



Languages

golang, python, js

Target topics

Binary Search
Binary Search Tree
Binary Tree
N-aray Tree(Trie)
Graph(Dijkstra, Union Find, Kruskal, Prim's, Minimum Spanning Tree, Topological Ordering...etc)
Stack
Queue
Array
Sorting
Hash Table
Heap
Linked list
Bit Operation
Dynamic programming(Kadan's)
Backtracking(Permutations & Combinations & Subsets...etc)
Math
and more...

Questions from

Leetcode
Project Euler
Google Code Jam
Google Kick Start
Glassdoor
Interviews
...

My other related repos

GoogleCodeJam
GoogleKickStart

Started since September 1st 2018



Day
Task
Type
From
remarks




1
binary search
binary search
leetcode 704
📌


2
sqrt(x)
binary search
leetcode 69



3
search in rotated sorted array
binary search
leetcode 33
📌


4
Find Peak Element
binary search
leetcode 162



5
Find Minimum in Rotated Sorted Array
binary search
leetcode 153
📌


6
Find Minimum in Rotated Sorted Array
binary search
leetcode 153
📌2 extra solutions for day5


7
Search for a Range
binary search
leetcode 34
📌lower bound + upper bound binary search


8
Find K Closest Elements
binary search
leetcode 658



9
Closest Binary Search Tree Value
binary search, tree
leetcode 270
1st attemp 10sep2018, 2nd 20jan2019


10
Closest Binary Search Tree Value
binary search, tree
leetcode 270
revise day9 problems with legit recursive & iterative dfs


11
Breadth First Search
tree

basics: revise breadth first search


12
Search in a Sorted Array of Unknown Size
binary search
leetcode 702
golang unsupported on leetcode, do it in python instead


13
Pow(x, n)
recursion, dynamic programming
leetcode 50
📌2nd dynamic programming. 3rd optimized


14
Valid Perfect Square
binary search
leetcode 50



15
Find Smallest Letter Greater Than Target
binary search
leetcode 50



16
Lower & Upper Bound Binary Searches
binary search

my medium article


17
Intersection of Two Arrays
binary search, hashtable
leetcode 349



18
Intersection of Two Arrays II
binary search, hashtable
leetcode 350



19
Two Sum II
binary search, hashtable
leetcode 167
1st O(n)Q(n). 2nd O(n)Q(1)


20
Find the Duplicate Number
sort, hashtable, 2 pointers
leetcode 287
📌for the 2 pointers approach


21
Find Minimum in Rotated Sorted Array II
binary search
leetcode 154
📌 very hard classic question


22
Validate Binary Search Tree
binary tree
leetcode 98
📌 1st 25sep2018 2nd 18mar2019


23
Inorder Successor in BST
binary tree
leetcode 285
📌recursion, iteration O(logn). followup: predecessor


24
Depth First Search
binary tree




25
Binary Search Tree Iterator
BST
leetcode 173
my first solution was super slow tho it passes the OJ. The suggested solution is awesome


26
Search in a Binary Search Tree
binary tree
leetcode 700
📌1st recursion, 2nd iteration


27
Iterative inorder traversal on BST
binary tree

📌


28
Pre & post order traversal on BST
binary tree

📌


29
Insert into a Binary Search Tree
BST
leetcode 701
recursive & iterative


30
Delete a node in a BST
BST
leetcode 450
my 1st attempt was super discursive LOL


31
Delete a node in a BST
BST
leetcode 450
📌 classic approach, bottom up recursion to replace the target node with its successor


32
Pre & In & Post Order Depth First Search
tree

revision on tree traversals


33
Kth Largest Element in a Stream
BST, binary search
leetcode 703
📌


34
Kth Largest Element in a Stream
BST, binary search
leetcode 703
learned what is heap and did a heap solution


35
Lowest Common Ancestor of a Binary Search Tree
BST
leetcode 235
⭐


36
Convert Sorted Array to Binary Search Tree
BST
leetcode 108



37
Balanced Binary Tree
Tree
leetcode 110



38
Contains Duplicate III
array, BST, bucket
leetcode 220
🧐 I don't understand the suggested solutions in BST and Bucket


39
Lowest Common Ancestor of a Binary Tree
tree
leetcode 226
⭐ even O(2n) leads to Time Limit Exceeded, what a draconian challenge. checkout suggestion


40
Recursive Binary Search
binary tree

to understand the logic of recursion


41
Binary Tree Preorder Traversal
binary tree




42
N-ary Tree Preorder Traversal
N-ary tree
leetcode 589



43
Binary Tree Inorder Traversal
binary tree
leetcode 94
the iterative solution is mind blowing


44
Binary Tree Postorder Traversal
binary tree
leetcode 145



45
Binary Tree Postorder Traversal
binary tree
leetcode 145
to understand the trick of the iterative solution


46
N-ary Tree Postorder Traversal
N-ary tree
leetcode 590



47
Binary Tree Level Order Traversal
binary tree
leetcode 102
📌 bfs, followup: zig zag level order


48
N-ary Tree Level Order Traversal
N-ary tree
leetcode 429



49
Maximum Depth of Binary Tree
binary tree
leetcode 104



50
Maximum Depth of N-ary Tree
N-ary tree
leetcode 559



51
Symmetric Tree
binary tree
leetcode 101



52
Path Sum
binary tree
leetcode 112
iterative, recursive


53
Populating Next Right Pointers in Each Node I & II
binary tree
leetcode 116
actaully my initial solution is O(n) in complexity and use only O(1) in space, it is good enough to apply on II as well


54
Construct String from Binary Tree
binary tree
leetcode 606
followup leetcode 536


55
Binary Tree Right Side View
binary tree
leetcode 199
2nd & 3rd attempt beat 100%


56
Construct Binary Tree from Inorder and Postorder Traversal
binary tree
leetcode 106



57
Construct Binary Tree from Preorder and Inorder Traversal
binary tree
leetcode 105



58
Construct Binary Tree from Preorder and Postorder Traversal
binary tree
leetcode 889
similar to day 56,57


59
Serialize and Deserilize Binary Tree





tree
leetcode 889
serialize, 📌1st, 2nd, 3rd O(n)




60
Serialize and Deserilize Binary Tree
binary tree
leetcode 297
deserialize, the suggested concept is good📌1st, 2nd, 3rd O(n)


61
Serialize and Deserilize BST
binary tree
leetcode 449
😎 actually the concept of day60 is applicable here in BST


62
Serialize and Deserilize N-ary Tree
binary tree
leetcode 428
My naive approach was to use json, it actaully beats 42.29%...not bad 😂 The suggested approach is astonishing 😳


63
Encode N-ary Tree to Binary Tree
binary tree
leetcode 431
encode ✅ decode 🤔


64
Encode N-ary Tree to Binary Tree
binary tree
leetcode 431
encode ✅ decode ✅ This question is pretty hard, it spent me 2 nights but it was worth doing.


65
Implement Trie (Prefix Tree)
trie
leetcode 208
📌1st hashtable


66
Implement Trie (Prefix Tree)
trie
leetcode 208
📌2nd array, it only works for a-z instead of all characters


67
Map Sum Pairs
trie
leetcode 677
i like this


68
Replace Words
string
leetcode 648
my 1st hasty solution got passed, but i think it is incorrect so i made another one


69
Add and Search Word Data Structure
string
leetcode 211
AddWord


70
Add and Search Word Data Structure
string
leetcode 211
Search


71
Implement Trie (Prefix Tree)
trie
leetcode 208
another approach: array instead of hashtable and it is faster


72
Design Search Autocomplete System
trie
leetcode 642
constructor and insert


73
Design Search Autocomplete System
trie
leetcode 642
search(incomplete)


73
Design Search Autocomplete System
trie
leetcode 642
WTF with leetcode


74
Implement Trie (Prefix Tree)
trie
leetcode 208
just do it again with python cos im gonna try autocomplete with python


75
Design Search Autocomplete System
trie
leetcode 642
do the same thing in python and i got accepted, fuck leetcode


76
Design Search Autocomplete System
trie
leetcode 642
i finally made it in golang and I BEAT 100% submissions, 2nd more concise


77
Word Search
trie
leetcode 79
my 1st attempt got TLE, it is weird cos the notion is correct


78
Word Search II
trie
leetcode 212
its actually a follow-up of day77 problem, tho my snippet can only beats 14% 😭


79
Moving Average from Data Stream
queue
leetcode 346
2 solutions


80
Design Circular Queue
queue, linked list
leetcode 622
1st array. 2nd linked list 📌3rd classic 2 pointers approach using a fixed length array


81
Walls and Gates
queue/tree
leetcode 286
⭐️1st dfs, 2nd bfs


82
Number of Islands
queue/tree
leetcode 200
1st attempt:dfs beats 9.4%, 2nd attempt:dfs beats 100%, 3rd attempt:bfs beats 50.43%


83
Open the Lock
queue/tree
leetcode 752
very interesting question, 1st attempt beats 5%, 2nd attempt beats 85%


84
Perfect Squares
queue/tree
leetcode 279
bfs: 204ms beats 36%


85
Count Primes
dynamic programming
leetcode 204
1st attempt TLE, 2nd attempt: learned from the others, 3rd: optimization


86
Min Stack
stack
leetcode 155
📌


87
Valid Parentheses
stack
leetcode 20



88
Daily Temperatures
stack
leetcode 739
kinda dynamic programming approach


89
Evaluate Reverse Polish Notation
stack
leetcode 739
very direct, beats 100%


90
Clone Graph
graph
leetcode 133
⭐️ 1st graph question: iterative bfs for now, will try different approaches


91
Clone Graph
graph, stack, queue
leetcode 133
⭐️ 2nd attempt: iterative dfs, 3rd attempt: recursive dfs


92
Target Sum
graph, stack
leetcode 494
1st attempt: iterative dfs beats 57%. 2nd attempt: recursive dfs beats 27%


93
Implement Queue using Stacks
queue, stack
leetcode 232
📌 classic question. my classic solution also beats 100%


94
Implement Stack using Queues
queue, stack
leetcode 225
📌 classic question. my classic solution also beats 100%


95
Decode String
stack
leetcode 394
⭐️1st 1 stack O(n). 2nd 2 stack O(n) more concise


96
Flood Fill
recursion, queue
leetcode 733
actually the question is quite similar to the one on day82. recursion beats 92%, bfs beats 11%


97
Design HashSet
hashtable
leetcode 705
very good question. 1st navie approach beats 6.9%, 2nd another naive beats 51%, 📌3rd linked list chaining LTE but important


98
Design HashMap
hashtable
leetcode 706
very good question. 1st navie approach beats 3.28%, 📌3rd linked list chaining LTE but important


99
Contains Duplicate
hashtable
leetcode 217



100
Single Number
hashtable, math, beat operation
leetcode 136
✌🏻🤓 day 100 with a 📌 classic question, 1st, 2nd beats 51%, 3rd beats 100%


101
Happy Number
hashtable
leetcode 202
very interesting question


102
Ugly Number
math, recursion
leetcode 263
1st beats 5%, 2nd beats 5%, optimal beats 100%


103
Two Sum
hashtable
leetcode 1
O(n) beats 100%, O(2n) beats 55%, O(n^2) beats 14%


104
Find the Unique Mistyped Item In an Unsorted Array
hashtable

⭐️ Basic but not easy. I was asked by a friend who works at a top startup


105
Isomorphic Strings
hashtable
leetcode 205
1st beats 1%, 2nd beats 5.88%


106
Minimum Index Sum of Two Lists
hashtable
leetcode 599
1st beats 93.33%


107
Largest product in a grid
array
euler 11



108
First Unique Character in a String
hashtable
leetcode 387
1st beats 10.32%, 2nd beats 100%


109
Contains Duplicate II
hashtable
leetcode 219
1st beats 87.34%


109
Amicable numbers
array
euler 21
2nd question today


110
Logger Rate Limiter
design, hashtable
leetcode 359
i think there is only one common solution


110
Group Anagrams
hashtable
leetcode 49
1st beats 96.55%, 2nd beats 89.66%


111
Coin Change
hashtable, dynamic programming
leetcode 322
⭐️ 2 years ago, I gave up. today i nailed it 💪🏻 1st beats 13.75%, 2nd beats 25.32%, 3rd beats 96.2%


112
Combination Sum IV
hashtable, tree, dynamic programming
leetcode 377
⭐️ classic DP, classic solution


112
Coin Sums
hashtable, tree, dynamic programming, backtracking
euler 31
⭐️ this question is similar to the previous one but the tricky thing is it requires DIFFERENT combinations


113
Coin Change II
hashtable, tree, dynamic programming, backtracking
leetcode 518
⭐️ this question is exactly the same with the two on day112 but leetcode is draconian in the Time Limit


114
Design Linked List
linked list
leetcode 707
📌 classic implementation of linked list operation


115
Intersection of Two Linked Lists
linked list
leetcode 160
📌 tricky linked list question, 3 approaches but only one is considered as Linked List related


115
Reverse Linked List
linked list
leetcode 206
📌 2nd approach is more linked list related


115
FizzBuzz
array
leetcode 206
1st beats 82.49%, 2nd beats 95.48%


115
Multiples of 3 and 5
array
euler 1
did it in a row becos it is similar to day115 q3 FizzBuzz


116
Linked List Cycle
linked list
leetcode 141
📌 classic question


116
Linked List Cycle II
linked list
leetcode 142
📌 classic question


117
Remove Nth Node From End of List
linked list
leetcode 19
📌 classic question, 1 classic + 2 other approaches


117
Remove Linked List Elements
linked list
leetcode 19
1st naive approach 📌 2nd classic approach


118
Odd Even Linked List
linked list
leetcode 118
both naive and classic approach still beats 100% LOL


118
Palindrome Linked List
linked list
leetcode 234
⭐️1st naive O(n)Q(n). 2nd O(2n)Q(1)


118
Valid Sudoku
hashtable
leetcode 36
very straight forward O(3n) solution beats 31.25%. But the 2 one-pass solutions also beats 31.25% 🤔


118
Permutations
backtracking, recursion

📌 basic


118
Permutations
backtracking, recursion
leetcode 46
📌 recursive beats 77.21%, iterative beats 77.14%


119
Permutations II
backtracking, recursion
leetcode 47
📌 1st attempt 7.41%


120
Permutations II
backtracking, recursion
leetcode 47
2nd day on the same question for understanding 📌 2nd attempt use hashtable which beats 100%(46.47% for python)


121
Median of Two Sorted Arrays
array
leetcode 4
1st O(nlogn), 2nd O(m+n), 📌3rd O(log(m+n))


121
Find the K th Element in 2 Sorted Arrays
array
asked by a friend
1st O(nlogn), 2nd O(m+n), suggested solution is hard to understand(see main.py)


122
Next Permutation
backtracking, recursion
leetcode 31
1st O(n) beats 100%, 2nd O(n) is terse and it beats 100%. Next Lexicographical Permutation Algorithm


122
Combinations
backtracking, recursion
leetcode 77
📌 1st beats 1.22%, 2nd beats 9.76%, 3rd beats 12.20%


123
Subsets
backtracking, recursion
leetcode 78
📌 both 1st, 2nd & 3rd beat 100 but python vers beat 35.29%, 3.67% and 35.29% only 🤔


123
Subsets II
backtracking, recursion
leetcode 90
📌 1st beats 100% but python ver beat 41.22% only 🤔


124
Subsets II
backtracking, recursion
leetcode 90
📌 spent more time to understand the iterative approach, although it eventaully beats merely 29.41%


125
Letter Combinations of a Phone Number
recursion
interview
asked by the facebook interviewer


126
Recursion
recursion

📌 a little intro of recursion of Standford CS106B


127
Combination Sum
hashtable, dynamic programming, backtracking
leetcode 39
📌 similar to day113, Coin Change II. 1st beats 23.85%, 2nd beats 63.08%


127
Combination Sum II
recursion, backtracking
leetcode 40
very similer to day119, Permutations II. 1st beats 38.89%, 2nd beats 100%. 📌 Be careful of the nuance between it and Combination Sum


127
Combination Sum III
recursion, backtracking
leetcode 216
📌 very similer to day112, Combination Sum IV and day124, Subsets II. 1st beats 100%


128
Fibonacci
recursion, dynamic programming
leetcode 509
📌 classic DP. top-down recursion beats 100%. bottom-up iteration beats 100%


128
Climbing Staris
recursion, dynamic programming
leetcode 70
📌 classic DP. top-down recursion beats 100%. bottom-up iteration beats 100%


128
Min Cost Climbing Staris
dynamic programming
leetcode 746
📌 classic DP. bruce forece recursion TLE. bottom-up iteration beats 100%


128
First Missing Positive
array
leetcode 41
1 in go beats 100%, 2 in python


129
Meeting Rooms
sort, greedy
leetcode 252
⭐️interval, 1st beats 100% 🎉takeaway: sort.Slice()


129
Meeting Rooms II
sort, greedy
leetcode 253
⭐️interval, 1st beats 0%, 2nd beats 96.97%


130
Reverse Integer
array
leetcode 7
📌 1st & 2nd approach 4ms. i gave up 2 years ago, now i finally know how to do it


130
Merge Sorted Array
sort, array
leetcode 88
📌 1st naive approach 0ms. 2nd is learned from others


131
Longest Common Substring
array, dynamic programming

📌 1st is naive. 2nd approach is classic


131
Search a 2D Matrix
binary search
leetcode 74
revise binary search, both 1st and 2nd is 8ms


131
Search a 2D Matrix II
binary search
leetcode 240
revise binary search, 1st and 2nd are 32ms beats 100%


132
Longest Common Subsequence
array, dynamic programming

📌 1st dynamic programming. 2nd recursive, 3rd recursive memorization


132
Range Sum Query - Immutable
array, dynamic programming

1st brute force beats 46%, 2nd cache beats 100%


132
Range Sum Query 2D - Immutable
array, dynamic programming

1st beats 80%


133
Bubble Sort
sort

revise sorting


133
Selection Sort
sort

revise sorting


133
Insertion Sort
sort

revise sorting


133
Merge Sort
sort

revise sorting


133
Quick Sort
sort

revise sorting. spaced & in-place


134
Rectangle Overlap
math
leetode 836
i hate this kind of questions


134
Rectangle Area
math
leetode 223
i hate this kind of questions


134
Minimum Absolute Difference in BST
BST
leetode 530



134
Find Bottom Left Tree Value
tree
leetode 513
both bfs & dfs run 12ms and beat 100%


134
Path Sum III
tree
leetode 437
1st LTE. 2nd recursive dfs. 3rd iterative dfs


135
Find Pivot Index
array, math
leetode 724
i failed to come up with a correct approach, learned from others


135
Largest Number At Least Twice of Others
array, sort
leetode 747
1st O(nlogn), 2nd O(n)


135
Plus One
array
leetode 66
should be just one solution


135
Find the Invalid Node in a BST
array
glassdoor



135
Minimum Distance Between BST Nodes
BST
leetode 783
this question is exactly the same wih leetcode 530


135
Unique Email Addresses
string
leetode 929



135
Continuous Subarray Sum
array
leetode 523
1st 104ms beats 0% LOL, 2nd 92ms beats 33.33%


135
String to Integer (atoi)
array
leetode 8
1st & 2nd 4ms beats 100%, fuck the corner cases


136
Diagonal Traverse
array
leetode 498
1st beats 22.22%


136
Spiral Matrix
array
leetode 54
1st, 2nd beats 100% ⭐️ 3rd is stunning


136
Pascals Triangle
array
leetode 118
1st beats 100%


136
Pascals Triangle II
array
leetode 119
1st, 2nd beats 100%


136
Minimum Path Sum
array, dynamic programming
leetode 64
1st 220ms beats 0%, 📌 2nd beats 100%


137
Jewels and Stones
hashtable
leetode 771
1st hashtable 0ms beats 100%


137
Number of 1 Bits
bit op
leetode 191
📌 the key here is to practice bit operation, i ignore any attempts using strings


137
Reverse Bits
bit op
leetode 190
📌 the key here is to practice bit operation, i ignore any attempts using strings


138
LRU Cache
hashtable, linked list
leetode 146
📌📌📌


138
Correct Flights Route
array
interview
🤔 interesting...1st O(n^2), 2nd O(n)


138
Length of Last Word
array
leetcode 58
not a good question


139
Count and Say
array
leetcode 38
⭐️


139
LFU Cache
hashtable, linked list
leetode 460
📌📌📌 1st attempt learned from others, 2nd attempt is my own and new way to implement it(although it is not efficient)


140
Count Complete Tree Nodes
binary tree
leetode 222
1st 24ms beats 92%, 2nd 20ms beats 100%


141
Container With Most Water
array
leetode 11
1st 368ms beats 34%, 2nd 12ms beats 100%


141
Add Binary
array, string
leetode 67
1st 0ms beats 100%


141
Implement strStr()
array, string
leetode 28
1st 544ms beats 2.04%, 2nd 0ms beats 100%


141
Longest Common Prefix
array, string
leetode 14
1st 4ms beats 12.90%, 2nd 0ms beats 100%


141
Remove Duplicates from Sorted Array
array, string
leetode 14
1st 220ms beats 2.04%(.py 19% .js 47%), 2nd 60ms beats 100%


141
Remove Duplicates from Sorted Array II
array, string
leetode 80
1st, 2nd 8ms beats 100%


142
Spiral Matrix II
array, string
leetode 59
1st 0ms beats 100%


142
Merge Two Binary Trees
tree
leetode 617
1st 40ms beats 59.15%, 2nd 36ms beats 100%


143
Reverse String
string
leetode 344



143
Array Partition I
array
leetode 561
1st beats 5.48%, 2nd 128ms beats 12.33%


143
Remove Element
array
leetode 27
1st beats 100%. i also implement it in JS with 2 ways


143
Minimum Size Subarray Sum
array
leetode 209
️⭐️ 1st beats 5.8%, 2nd beats 20%, 3rd learned from others


143
Maximum Product of Three Numbers
array
leetode 628
⭐️ 1st beats 28.57%, 2nd 44ms beats 100%


143
Reverse Only Letters
array
leetode 917



143
Flipping an Image
array
leetode 832



144
Most Common Word
string
leetode 819
takeaway: python=>re.split(r'\W+', paragraph). go=>regexp.MustCompile([!?',;. ]). js=>paragraph.split(/[!?',;. ]/)


144
Subtree of Another Tree
tree
leetode 572
1st O(n^2). 2d O(n)


144
Subtree with Maximum Average
tree
1point3acres



144
Divide Two Integers
math
leetcode 29
📌 classic question in hk interviews


144
Product of Array Except Self
math
leetcode 238
using / got beting 60%. naive got TLE, proper approach 2836ms beats 5.92%


145
K Closest Points to Origin
sort, heap
leetcode 973
📌 1st beats 72.46%. 2nd beats 98.95% in python


145
Gray Code
array, bit op
leetcode 89
🤔 interesting


145
Longest Substring with At Most Two Distinct Characters
array, hashtable
leetcode 159
1st, 2nd beat 17.65%


145
Maximize Distance to Closest Person
array
leetcode 849
1st beats 100%


146
Middle of the Linked List
linked list
leetcode 876
📌


146
Insert into a Cyclic Sorted List
linked list
leetcode 708
📌


146
Merge Two Sorted Lists
linked list
leetcode 21
📌


146
Merge k Sorted Lists
linked list
leetcode 23
📌 naive approach runs faster than the decent approach 😂


146
Remove Duplicates from Sorted List II
linked list
leetcode 82
📌 1st, 2nd beats 100%


147
Network Delay Time
graph
leetcode 743
📌 Dijkstra's Algorithm. Altenative: DFS


147
Cheapest Flights Within K Stops
graph
leetcode 787
📌 1st Dijkstra's variation. 2nd BFS


148
Longest Palindromic Substring
string
leetcode 5
⭐️


148
Maximum Size Subarray Sum Equals k
array
leetcode 325
1st appraoach LTE, 2nd approach learned from others which beats 100%


148
Subarray Sum Equals K
array
leetcode 560
1st appraoach refers to leetcode 325, therefore i came up with the idea immediately which beats 100%


149
Maze
array
glassdoor
️⭐️


149
2 Sum Closest
array
glassdoor
⭐️


149
Reverse 2nd Half of a Linked List
linked list
glassdoor
📌


149
Reverse Linked List II
linked list
leetcode 92
️📌


149
Window Sum
array
glassdoor
⭐️


149
To Lower Case
array
leetcode 709
️


149
Sort Array By Parity
array
leetcode 905
️


149
Sort Array By Parity II
array
leetcode 922
️


149
Robot Return to Origin
string
leetcode 657
️1st beats 100%, 2nd beats 0% 😂


149
Find Anagram Mappings
array
leetcode 760
️


149
Uncommon Words from Two Sentences
hashtable
leetcode 884
️


149
Rotate String
string
leetcode 796
️


150
Find Leaves of Binary Tree
tree
leetcode 366
️1st O(h*n) beats 100%


150
Second Minimum Node In a Binary Tree
tree
leetcode 671
️1st, 2nd O(n) beats 100%


150
Two Sum IV - Input is a BST
tree
leetcode 653
️1st, 2nd O(n) beats 50% only 🤔


150
Increasing Order Search Tree
tree
leetcode 897
️1st O(n) beats 100%


150
Reverse Words in a String II
array
leetcode 186
️1st O(n) beats 100%


150
Compare Version Numbers
array
leetcode 165
️1st O(n) beats 100%


150
Sum Root to Leaf Numbers
tree
leetcode 129
️1st, 2nd O(n) beats 100%


150
Number of Connected Components in an Undirected Graph
hashtable, graph
leetcode 323
️1st beats 100%


150
Maximum Binary Tree
tree
leetcode 654
️1st beats 0%. 2nd beats 100%


151
Rotate Function
tree
leetcode 396
️1st O(n^2) beats 100%, 2nd O(n) beats 100%


151
Rotate List
linked list
leetcode 61
️1st O(n) beats 100%


151
Arithmetic Slices
array
leetcode 413
️1st O(n) beats 100%


152
Round Robin
queue
glassdoor
️📌📌📌


152
Sliding Window Median
array, binary search
leetcode 480
1st TLE 📌2nd binary search O(n*2logk)


152
Permutation in String
hashtable, array, string
leetcode 480
⭐️ 1st O(n^2) hashtable beats  0%, 2nd O(n^2) [26]int{} beats 40%


152
Set Matrix Zeroes
hashtable, array
leetcode 73
1st O(n) beats 52%


152
Top K Frequent Elements
hashtable, bucket sort
leetcode 347
1st O(nlogn) beats 64%. 2nd O(n) beats 64%. takeaway: use bucket sort for frequency questions, 3rd ~O(n) quick select


153
Top K Frequent Words
hashtable, bucket sort
leetcode 692
1st O(2nlogn) beats 25%. 2nd O(nlogn) beats 100%. takeaway: sort things with priority queue using priority WITH other params


153
Sort Characters By Frequency
hashtable, bucket sort
leetcode 451
1st O(n) beats 66%


153
Zigzag Iterator
array
leetcode 281
1st O(k) beats 61%. leetcode doesn't support golang, did it in python


153
Triangle
dfs, dynamic programming
leetcode 120
📌 1st TLE, 2nd MLE. 3rd dp in O(n) beats 100%


154
Products' Average Rating
array
glassdoor
⭐️


154
Greatest Common Divisor
math
glassdoor
⭐️ takeaway: Euclidian Algorithm


154
Least Common Multiple
math
glassdoor
⭐️ takeaway: A * B = LCM * GCD


154
Cells Mutation
array
glassdoor
⭐️


154
Amplitude of a Tree
tree
glassdoor
⭐️


155
Union Find
graph, union find
Study Union Find
📌 Quick Find 👎🏻 -> Quick Union 🤔 -> Union Find 🎉


155
Minimum Spanning Tree
graph, union find
Study MST
📌 heap + list of hashtables 🤔 -> Kruskal(Union Find) 🎉 or Prim(heap + hashtable)


156
Go Through all the Warehouses with Minimum Cost
graph, union find
glassdoor
📌 real life problem <- Kruskal, Prim


156
Minimum Window Substring
2pointers, hashtable
leetcode 76
1st O(n) beats 20%


157
LRU Cache Miss Count
linked list, hashtable
glassdoor
1st O(n^2), 2nd O(n)


157
Four Integers
array
glassdoor
O(nlogn)


157
Rotate a Matrix
array
glassdoor
1st space O(n), 📌 i2nd space O(1) in-place


157
Rotate Image
array
leetcode 48
📌 inplace


157
Copy List with Random Pointer
linked list
leetcode 138
⭐️


158
Path Sum IV
linked list
leetcode 138
1st O(4n)


158
Binary Tree Vertical Order Traversal
tree
leetcode 314
1st recursive, 2nd iterative, both O(n)


158
Course Schedule
graph
leetcode 207
1st LTE, 📌 2nd, 3rd Classic Approach: Topological Ordering


159
Topological Ordering
graph
study
📌 1st: Topological Ordering with DFS 📌 2nd: Topological Ordering with BFS/Khan's


159
Course Schedule II
graph
leetcode 210
📌 DFS, BFS Topological Ordering


160
Order Dependency
graph
glassdoor
📌 DFS, BFS


160
Graph Valid Tree
graph, union find
leetcode 261
📌 1st union find O(nlogn) beats 100%


160
Merge Strings
array
glassdoor
⭐️ very similar to ZigZag iterator


160
Find Distinct Substrings with Exactly K Distinct Characters
array

📌1st O(n^3), 2nd O(n^2)


161
Reorder Log Files
array
leetcode 937
1st O(nlogn)


161
Dijkstra's Algorithm on a Directed Graph
graph
study
📌


161
Dijkstra's Algorithm on an Undirected Graph
graph
study
📌


161
Find All Anagrams in a String
string
leetcode 438
1st, 2nd O(kn)


162
Optimal Flights
binary search
interview
📌1st O(n^2), 2nd O(nlogn)


162
Max Area of Island
array
leetcode 695
1st dfs O(n). 2nd bfs(n)


162
Find K Pairs with Smallest Sums
array
leetcode 373
1st 2nd O(nlogn)


163
Number of Distinct Islands
array
leetcode 694
1st O(nk)


163
Kth Largest Element in an Array
array, heap
leetcode 215
1st, 2nd O(nlogn)


163
Number of Islands II
array
leetcode 305
1st O(k * m^2 * n^2) beats 6% LOL. 📌 2nd O(mn + k * log(mn)) beats 28%


163
Redundant Connection
union find
leetcode 684
O(nlogn)


164
Friends Circles
union find, graph
leetcode 547
1st union find O(n^2logn -> n^3). 2nd hashtable + bfs O(n^2)


164
Sentence Similarity II
union find
leetcode 737
O(nlogn)


165
Sentence Similarity
hashtable
leetcode 734
⭐O(n)


166
Heap
heap
study
📌 implement a heap with push, pop and heapify


166
Heap Sort
heap, sort
study
📌 implement heapsort using a heap


166
Generate Parentheses
backtracking
study
📌1st O(2^2n) beats 5%


166
Count Univalue Subtrees
tree
leetcode 250
📌1st O(n) 100%


167
Decode Ways
dynamic programing
leetcode 91
⭐️1st brute force TLE, 2nd memorized brute force O(n) beats 100%


167
Unique Paths
dynamic programing
leetcode 62
📌 dp recursion O(mn)  📌 dp matrix O(mn)


167
Merge Intervals
sort, greedy
leetcode 56
⭐️interval


168
Non-overlapping Intervals
sort, greedy
leetcode 435
⭐️interval


168
Minimum Number of Arrows to Burst Balloons
sort, greedy
leetcode 452
⭐️interval


168
Largest BST Subtree
BST
leetcode 333
1st O(n^2)


168
Range Sum of BST
BST
leetcode 938
1st recursive, 2nd iterative, both O(n) beat 98%


169
Delete a node in a BST
BST
leetcode 450
📌[revise day30] 2 classic approaches, bottom up recursion to replace the target node with its predecessor or successor


169
Inorder Successor in BST
BST
leetcode 285
📌 [revise day23] iterative and recursive O(logn)


169
Inorder Successor in BST II
BST
leetcode 510
⭐️ interesting, would be a potential follow-up of leetcode 285


169
Closest Binary Search Tree Value II
BST
leetcode 272
⭐ 1st O(nlogn)


170
Split BST
BST
leetcode 776
📌📌📌 super hard recursion question, must revise again


170
Unique Word Abbreviation
hashtable
leetcode 288



170
Longest Substring Without Repeating Characters
hashtable
leetcode 3
📌 1st O(n^2), 2nd O(n)


170
Repeated DNA Sequences
hashtable
leetcode 187
1st O(n)


170
Find Duplicate Subtrees
hashtable, tree
leetcode 652
1st O(n)


170
Group Shifted Strings
hashtable
leetcode 249
O(n)


171
K-th Symbol in Grammar
hashtable
leetcode 779
1st LTE. 📌2nd tricky recursion. see ./idea.jpeg


171
3Sum
hashtable, 2pointers
leetcode 779
📌1st hashtable O(n^2) beats 18%. 📌2nd hashtable+2pointers O(n^2) beats 27%, 📌3rd hashtable+2pointers O(n^2) beats 54%


171
Two Sum III - Data structure design
hashtable
leetcode 170
📌1st hashtable O(n) beats 100%


171
4Sum
hashtable
leetcode 18
📌1st hashtable O(n^3) beats 15%, 2nd hashtable+2pointer O(n^3) beats 95%


171
4Sum II
hashtable
leetcode 454
📌1st O(n^3), 2nd O(n^2)


172
Insert Delete GetRandom O(1)
hashtable
leetcode 380
1st random O(n) beats 6%, 📌2nd random O(1) beats 100%


172
Insert Delete GetRandom O(1) - Duplicates allowed
hashtable
leetcode 381
1st random O(n) beats 6%


173
Find Median from Data Stream
heap, bst
leetcode 295
1st heap, 2nd BST both LTE, 📌3rd: 2 heaps


173
Lonely Pixel I
hashtable
leetcode 531
1st, 2nd beats 100%


173
Minimum Add/Delete to Make Parentheses Valid
stack
leetcode 921
1st O(n) beats 100%


173
Largest Permutation
permutation
geeksforgeeks



174
Integer to English Words
array
leetcode 273
⭐️tedious but frequently asked


174
Remove duplicates from a string in O(1) space
hashtable, bucket
geeksforgeeks
⭐ 1st O(n) space, 2nd O(1) space


175
Design Hit Counter
hashtable, array
leetcode 362
⭐ 1st python OrderedDict O(1) for all operations beats 100%, 2nd hashtable+array beats 100%


175
Design Tic-Tac-Toe
array
leetcode 348
1st O(n^2), 😱2nd O(1)


175
Valid Tic-Tac-Toe State
array
leetcode 794
1st stupid. 2nd decent


176
Repeated Substring Pattern
array
leetcode 459
1st O(n^2), 2nd O(n)


176
Repeated String Match
array
leetcode 686
1st O(kn)


176
License Key Formatting
array, stack
leetcode 482
1st O(n)


176
Serialize and Deserialize array of string
array
geeksforgeeks
O(n)


177
Letter Case Permutation
permutation
leetcode 784
1st permutation+hashset O(2^n), 2nd permutation O(2^n)


177
Best Time to Buy and Sell Stock
dynamic programming
leetcode 121
📌📌📌1st dp O(n), 2nd heap O(nlogn)


177
Best Time to Buy and Sell Stock II
dynamic programming
leetcode 122
📌📌📌1st, 2nd dp O(n)


178
Best Time to Buy and Sell Stock III
dynamic programming
leetcode 122
📌📌📌1st O(n^2) LTE, 2nd O(2n) beats 32%


178
Minimum Time Difference
sort
leetcode 539
1st O(nlogn) beats 73%, 2nd O(n) beats 93%


178
Next Closest Time
string
leetcode 681
⭐️ so many corner cases, it was the worst attempt i ever made


179
Game of Life
array, bit op
leetcode 289
1st space O(mn), 2nd space O(1)


179
Add Two Numbers
linked list
leetcode 2
📌1st, 2nd O(n)


179
Add Two Numbers II
linked list
leetcode 445
📌1st O(n)


179
Topological Ordering Of IDs
graph
revise
implement topological ordering of a list of IDs


179
Alien Dictionary
array, graph
leetcode 269
📌1st BFS, 2nd DFS


180
Letter Combinations of a Phone Number
recursion
leetcode 17
📌


180
Multiply Strings
array
leetcode 43
📌


180
Add Strings
array
leetcode 415
📌


181
Valid Word Square
array
leetcode 422
1st 3 cases. 2nd try catch


181
Maximum Subarray
array
leetcode 53
1st O(n^2) 📌📌📌2nd, 3rd O(n) Kadan's Algorithm. followups: 1)print subarray 2) min subarray


181
Maximum Product Subarray
array
leetcode 152
1st O(n^2) 📌📌📌2nd O(n) Kadan's Algorithm. followups: 1)print subarray 2) min subarray


181
Subarray Product Less Than K
array
leetcode 713
⭐️sliding window


182
Word Ladder
bfs, hashtable
leetcode 127
1st LTE, ⭐️2nd O(n^26*l)


182
Word Ladder II
bfs, hashtable
leetcode 127
LTE, revise later


182
Minimum Genetic Mutation
bfs, hashtable
leetcode 433
1st O(MN)


183
Bold Words in String
hashtable, sort
leetcode 758
️️⭐️1st O(nlogn): interval problem


183
Add Bold Tag in String
hashtable, sort
leetcode 616
⭐️1st O(nlogn): interval problem


183
Number of Segments in a String
string
leetcode 616
takeawaystrings.Fields(s)


183
Sort Colors
bucket
leetcode 75
0th merge sort O(nlogn), 1st bucket sort O(2n), 📌2nd moving zeros swap O(2n), 📌3rd partitioning swap O(n)


183
Move Zeroes
array
leetcode 75
⭐️swap


183
Missing Number
bucket, math
leetcode 268
1st math O(n), 2nd bucket O(2n)


184
Swap Nodes in Pairs
linked list
leetcode 24
📌


184
Reverse Nodes in k-Group
linked list
leetcode 25
📌1st O(n) but wordy. 2nd O(n) concise


184
Sort List
linked list, sort
leetcode 148
📌merge sort


184
Peeking Iterator
design
leetcode 284
leetcode doesnt support golang


184
One Edit Distance
array
leetcode 161
1st O(3n), 2nd O(n)


184
Zero/K Sum Subarray(s)
array
glassdoor
📌similar to leetcode 325 and leetcode 560


185
Edit Distance
dynamic programming
leetcode 72
📌📌📌 similar to LCS


185
Shortest Unsorted Continuous Subarray
sort, array
leetcode 581
1st O(nlogn), 2nd O(n)


185
Maximum Average Subarray
dynamic programming
glassdoor
📌1st O(nlogn), 2nd O(n)


185
Maximum Average Subarray I
array
leetcode 643
sliding window O(n)


185
Subdomain Visit Count
string
leetcode 811
1st O(n)


185
Trapping Rain Water
dynamic programming
leetcode 42
1st O(3n)


186
Unique Binary Search Trees
dynamic programming
leetcode 96
⭐️Catalan Number Sequence very similar to the Fibonacci Sequence


187
Valid Palindrome
2pointers
leetcode 125
📌


187
Valid Palindrome II
2pointers
leetcode 680
📌1st O(n), 2nd O(n) too but more readable


188
Cut Off Trees for Golf Event
heap, sort
leetcode 675
📌1st heap, 2nd sort


188
Lowest Common Ancestor of a N-ary Tree
tree
glassdoor
⭐️


188
Distance Between 2 Values in a BST
tree
glassdoor
⭐️build BST => find LCA => ans = depth(a) + depth(b) - 2*depth(lca)


189
Reverse Words in a String
string
leetcode 151
1st time=space=O(n)


189
Max Consecutive Ones
array
leetcode 485
O(n)


189
Max Consecutive Ones II
array
leetcode 487
O(2n)


189
Max Consecutive Ones III
array
leetcode 1004
1st O(n^2), 2nd O(2n)


189
Pacific Atlantic Water Flow
graph
leetcode 417
1st O(4^mn) LTE. 📌2nd O(5mn)


190
Design Compressed String Iterator
string, queue
leetcode 604
1st O(n) unicode.IsDigit()


190
Basic Calculator II
string, stack
leetcode 227
1st O(2n), takeaway: isdigit() in python, unicode.IsDigit() in golang, and isinstance(a, int) in python for type checking, 📌2nd O(2n) too but more concise but be careful of float division


191
Array Partition by Occurence
hashtable
interview
⭐️ O(n^2)


192
Partition Labels
hashtable
leetcode 763
️️️⭐️WTF, I was asked about it yestarday during an interview. 1st O(n^2). 2nd O(n)


192
Basic Calculator
string, stack
leetcode 224
1st O(2n) tedious operation beats 25%, 2nd concise O(2n) beats 90%


193
Basic Calculator III
string, stack, recursion
leetcode 772
1st O(n^2) recursion, 2nd O(n) recursion


193
Sort Transformed Array
2pointers
leetcode 360
1st O(n)


194
Quick Select
sort
revision
revise Quick Sort and use it to find the k-th smallest element in O(n) time


195
01 Matrix
bfs, dynamic programming
leetcode 542
1st BFS O(n^2) LTE, 2nd DP O(2n) beats 96%


195
Recover Binary Search Tree
tree
leetcode 99
1st inroder+sort O(nlogn), 2nd inorder O(n)


195
Baseball Game
stack
leetcode 682
1st O(n)


195
Sort a Stack using a Stack
stack
Sort Stacks
⭐️


196
N-Queens
backtracking
leetcode 51
📌1st classic backtracking approach


196
N-Queens II
backtracking
leetcode 52
📌1st classic backtracking approach


196
Search in Rotated Sorted Array II
binary search
leetcode 81
📌classic binary search question similar to leetcode 33


196
Kth Smallest Element in a Sorted Matrix
heap, binary search
leetcode 378
⭐️ 1st O(nlogn) sort, ⭐️2nd O(nlogn) max heap, 3rd O(logn*logn) binary search


197
Maximal Square
dynamic programming
leetcode 221
📌dynamic programming in linear time O(n)


197
Binary Search Variations
binary search
revision
📌common, lower & upper bound in both iterative and recursive implementation


197
Factorial Trailing Zeroes
math
leetcode 172
⭐


198
Quick Select
sort
revision
wrote the quick select again in both go & python for better understanding


198
Sliding Puzzle
graph
leetcode 773
⭐ with follow-up


198
Largest Rectangle in Histogram
array, stack
leetcode 84
1st O(n^2) brute force, 📌 2nd O(2n) stack


198
Maximal Rectangle
dynamic programming, stack
leetcode 85
📌


198
Down to One
dfs
glassdoor
⭐️


199
Strobogrammatic Number
2pointers, hashtable
leetcode 246
1st O(n) 2pointers, 2nd O(n) hashtable+2pointers


199
Strobogrammatic Number II
recursion, hashtable
leetcode 247
recursion


199
Strobogrammatic Number III
recursion, hashtable
leetcode 248
recursion: reuse the approach in ii)


200
Maximum Sum Rectangle/Submatrix
dynamic programming
youtube
O(rc^2)kadan's algo approach


200
Roman to Integer
hashtable
leetcode 13
⭐️


200
Integer to Roman
hashtable
leetcode 12
⭐️


200
Invert Binary Tree
tree
leetcode 226
📌️1st recursion, 2nd bfs


200
Binary Tree Level Order Traversal II
binary tree
leetcode 107
similar to leetcode 102 1st bfs


200
Average of Levels in Binary Tree
binary tree
leetcode 637
bfs


200
Same Tree
binary tree
leetcode 100
1st recursive dfs, 2nd iterative dfs


200
Smallest String Starting From Leaf
binary tree
leetcode 988
1st recursive dfs, 2nd iterative dfs


201
Trim a Binary Search Tree
binary tree, recursion
leetcode 669
1st recursion, 2nd better recursion


201
Shortest Word Distance
2pointers, binary search
leetcode 243
⭐️O(n), O(nlogn)


201
Shortest Word Distance II
2pointers
leetcode 244
⭐️O(n)


201
Shortest Word Distance III
2pointers, binary search
leetcode 245
⭐️O(n), O(nlogn)


201
Next Greater Element I
array
leetcode 496
1st O(n^2)


202
Next Greater Element II
stack
leetcode 503
1st O(2n), 2nd O(n)


202
Next Greater Element III
stack
leetcode 556
📌Next Lexicographical Permutation Algorithm O(n). Similar to leetcode 31


202
Binary Tree Paths
tree
leetcode 257
1st recursive dfs O(n), 2nd iterative dfs O(n)


202
Diameter of Binary Tree
tree, recursion
leetcode 543
O(n) recursive dfs


202
Construct Binary Tree from String
tree, recursion
leetcode 536
O(n) recursive dfs, follow-up of leetcode 606


202
Equal Tree Partition
tree, recursion, hashtable
leetcode 663
⭐️1st O(2n) recursive dfs+hashtable. 2nd O(n) recursive dfs+hashset


203
Check Completeness of a Binary Tree
tree, recursion
leetcode 958
1st tedious dfs O(2n), 📌 2nd bfs O(n)


203
Check Fullness of a Binary Tree
tree
glassdoor
1st dfs O(n)


203
Word Break
dynamic programming, recursion
leetcode 139
📌1st recursion+hashtable O(n^2)


203
Word Break II
dynamic programming, recursion
leetcode 140
1st top-down TLE, 2nd bottom-up TLE, 📌3rd bottom-up+hashtable O(n^3) beats 47%


204
Exam Room
array, heap
leetcode 855
1st O(n), 2nd O(logn)


205
Evaluate Division
graph
leetcode 399
📌1st O(ENlogN) unionfind beats 70%


205
Heap
heap
revise
📌rectify implementation in python and js. operations: push, push, heapify, shift up & down.


206
Heaters
binary search
leetcode 475
1st binary search beats 100%


206
Binary Search Nearest
binary search
study
📌 binary search the index of nearest item


206
Paint House
dynamic programming
leetcode 256
1st O(2^n) TLE.📌2nd O(n)


206
Paint House II
dynamic programming
leetcode 265
📌2nd O(n) follow-up of I), very similar


206
House Robber
dynamic programming
leetcode 198
📌 1st O(n) space. 2nd O(1) space. 3rd concise. 4rd follow-up: print path


206
House Robber II
dynamic programming
leetcode 213
📌 follow-up of I)


207
House Robber III
recusion
leetcode 337
📌traversal: tree, recursion, concept: dynamic programming


207
Random Pick Index
binary search
leetcode 398
⭐️lower bound binary search


207
Random Pick with Weight
binary search
leetcode 528
⭐️equal or smaller than binary search


207
Search Insert Position
binary search
leetcode 35
⭐️lower bound binary search


207
Linked List Random Node
linked list
leetcode 382
1st naive O(n)O(1), 2nd O(1)O(n), 📌 3rd reservoir sampling O(1)O(n)


207
Partition List
linked list
leetcode 86
1st O(n)O(n) ⭐ 2nd O(n)O(n) be careful of the next pointer


207
Flatten Binary Tree to Linked List
tree, recursion
leetcode 114
1st O(2n), 📌2nd O(n) mind-blowing suggested solution


208
Sum of Left Leaves
tree, recursion
leetcode 404
1st recursive dfs, 2nd iterative dfs, both O(n)


208
Binary Tree Zigzag Level Order Traversal
tree
leetcode 103
1st bfs O(n)


208
3Sum Closest
binary search, 2pointers
leetcode 16
1st binary search O(n^2). 2nd 2pointers O(n^2)


208
3Sum Smaller
binary search, 2pointers
leetcode 259
1st binary search O(n^2). 2nd 2pointers O(n^2)


209
Design Twitter
hashtable
leetcode 355
1st beats 12%, 2nd beat 31%


209
Path Sum II
tree
leetcode 113
1st recursive dfs, 2nd iterative dfs


209
Sliding Window Maximum
binary search
leetcode 239
1st O(n^2) LTE, 2nd binary search O(nlogk)


210
Surrounded Regions
graph
leetcode 130
1st O(2n)


210
Island Perimeter
graph
leetcode 463
1st dfs. 2nd bfs. 2 followups


210
Vertical Order Traversal of a Binary Tree
tree, hashtable
leetcode 463
1st bfs, 2nd bfs. takeaway: python sorted(nums,cmp=f), go sort.Slice


210
Reorganize String
heap, hashtable
leetcode 767
1st hashtable + heap


210
Max Stack
binary search, linked list
leetcode 716
1st O(n) 📌2nd O(logn)


211
Mean Stack
binary search, linked list
glassdoor
📌O(logn)


211
Middle Stack
linked list
glassdoor
📌O(1) for all push, pop, top, peekMiddle, popMiddle


211
Gas Station
greedy
leetcode 134
1st O(n^2). ⭐️2nd O(n)


211
Reorder List
stack, linked list
leetcode 143
1st stack O(2n) ⭐️ 2nd O(3n) linked list reversal


211
Plus One Linked List
stack, linked list
leetcode 143
1st stack O(2n) ⭐️ 2nd O(3n) linked list reversal


212
Robot Room Cleaner
graph
leetcode 489
⭐️dfs


212
Fruit Into Baskets
2pointers
leetcode 489
⭐️ sliding window


212
Backspace String Compare
stack
leetcode 489



212
Design Circular Deque
queue, linked list
leetcode 641
1st array. 2nd linked list. 📌3rd classic 2 pointers approach using a fixed length array


213
Keys and Rooms
graph
leetcode 841
1st iterative dfs. 2nd recursive dfs


213
Flatten 2D Vector
2pointers
leetcode 251
1st unfolding. 2nd 2pointers


213
Flatten Nested List Iterator
recursion
leetcode 251
1st recursion


214
Foregone Solution
math
codejam
qualification round Q1


214
Your Can Go Your Own Way
graph
codejam
qualification round Q2


214
Crytopangrams
math
codejam
qualification round Q3. only works on sample testcases...😢


215
Crytopangrams
math, bsearch, hash
codejam
qualification round Q3. I finally made it works ✌🏻


216
Nested List Weight Sum
graph
leetcode 339
1st recursive dfs, 2nd iterative dfs


216
Calculate Maximum Call Stack's Size
recursion
study



217
Most Frequent Subtree Sum
tree, hashtable
leetcode 508
1st bottom up recursive dfs + hashtable O(n)


217
Employee Importance
graph, hashtable
leetcode 690
1st bottom up recursive dfs + hashtable O(n)


218
Sum of Root To Leaf Binary Numbers
tree
leetcode 1022
1st recursive dfs, 2nd iterative dfs


218
First Bad Version
binary search
leetcode 278
⭐️


219
2Sum combinations
binary search, recursion
interview
⭐️ the followup is hard


220
Palindrome Permutation
hashtable
leetcode 266



220
Palindrome Permutation II
hashtable, recursion
leetcode 267
⭐️


221
Pylons
recursion
codejam
round 1A q1


221
Longest Valid Parentheses
dynamic programming
leetcode 32
1st O(n^3) LTE 📌2nd O(n)


221
Remove Invalid Parentheses
recursion
leetcode 301
1st O(2^n) LTE 📌2nd < O(2^n)


222
Shortest Palindrome
string
leetcode 214
1st O(n^2) python LTE, golang beats 5% 📌2nd learned from others O(n^2) but less heavy string operation


222
Longest Palindrome
hashtable
leetcode 409
1st O(n)


223
Palindrome Pairs
hashtable
leetcode 336
1st O(k * n^2) LTE. 📌2nd O(n * k^2) learned from others


223
Binary Tree Maximum Path Sum
recursion
leetcode 124
1st O(n)


224
Maximum Difference Between Node and Ancestor
recursion
leetcode 1026
1st O(n)


224
Maximum Difference Between Node and Ancestor
recursion
leetcode 1026
1st O(n^2) LTE. 📌 2nd O(nlogn) learned from others


224
Longest Substring with At Most K Distinct Characters
hashtable, sliding window
leetcode 340
📌1st O(2n)


225
Maximum Width of Binary Tree
tree
leetcode 662
1st O(n)


225
Longest Substring with At Least K Repeating Characters
tree
leetcode 395
1st O(n^2), 2nd O(nlogn)


225
Single Number II
bit op
leetcode 137
1st Time O(n) Space O(n). 📌2nd Time O(32n) Space O(1)


225
Single Number III
bit op
leetcode 137
1st Time O(n) Space O(n). 📌2nd Time O(n) Space O(1)


226
Hamming Distance
bit op
leetcode 461
📌


226
Total Hamming Distance
bit op
leetcode 477
1st O(n^2logm) LTE. 📌2nd O(nlogm)


226
Binary Number with Alternating Bits
bit op
leetcode 477
1st O(logn)


226
Integer Replacement
dyanmic programming, bit op
leetcode 397
⭐️1st O(n), 2nd bit op O(nlogm)


226
Image Smoother
array
leetcode 661
1st O(n)


227
Partition Array Into Three Parts With Equal Sum
array
leetcode 1013
1st O(n)


227
Non-decreasing Array
array
leetcode 665
1st O(4n)


227
Monotonic Array
array
leetcode 665
1st O(n), 2nd O(n) more concise


227
Best Time to Buy and Sell Stock with Transaction Fee
dynamic programming
leetcode 714
📌1st O(n)


227
Best Time to Buy and Sell Stock with Cooldown
dynamic programming
leetcode 309
📌1st O(n)


228
Array Nesting
hashtable
leetcode 565
1st O(n^2) 📌2nd O(n)


228
Nested List Weight Sum II
recursion, hashtable
leetcode 364
1st O(3n), 2nd O(2n)


228
Maximum Length of Repeated Subarray
dynamic programming
leetcode 718
📌longest common substring


228
Interval List Intersections
greedy
leetcode 986
📌 similar to merge intervals


228
Employee Free Time
heap, greedy
leetcode 759
1st O(nlogn)


228
Insert Interval
binary search
leetcode 57
📌1st O(n)


228
Delete Operation for Two Strings
dynamic programming
leetcode 583
📌1st O(n^2) similar to leetcode 72


229
Building Palindromes
hashtable
kickstart
round B q1: passed small dataset only


229
Diverse Subarray
hashtable
kickstart
round B q3: passed small dataset only


229
Find Common Characters
hashtable
leetcode 1002
1st O(MNM). 2nd O(MN)


230
Min Queue
binary search, linked list
glassdoor
O(logn)


230
XOR Between Range
bit op
glassdoor
📌O(1)


230
Maximum Frequency Stack
hashtable
leetcode 895
📌1st, 2nd O(1)


230
Split Linked List in Parts
math, linked list
leetcode 725
O(2n)


231
Shuffle an Array
array
leetcode 384
1st bultin random.shuffle(). 📌2nd O(n) Fisher-Yates Algorithm


231
Flip Equivalent Binary Trees
recursion
leetcode 951
⭐️1st O(n)


232
Toeplitz Matrix
array
leetcode 766
1st O(rc)Q(r). 2nd O(rc)Q(1)


232
Concatenated Words
dynamic programming
leetcode 472
📌1st time hard dp without extra help


233
Longest Word in Dictionary
array
leetcode 720
1st O(kn)


233
Closest Leaf in a Binary Tree
tree, graph
leetcode 742
⭐️1st O(N+E)


234
Word Squares
trie, recursion
leetcode 425
📌


234
Degree of an Array
hashtable
leetcode 697



235
Lonely Pixel II
hashtable
leetcode 533
tricky question ⭐️ O(2n)


235
Group Isomorphic Strings
hashtable
glassdoor
📌1st O(n^3). 2nd O(nk)


235
Longest Increasing Subsequence
dynamic programming
leetcode 300
📌1st O(n^2) dp


235
Number of Longest Increasing Subsequence
dynamic programming
leetcode 673
📌1st O(n^2) dp


236
My Calendar I
binary search
leetcode 729
⭐️


237
Manhattan Crepe Cart
arrary
code jam



237
Design Log Storage System
design
leetcode 635
⭐️design


237
Time Based Key-Value Store
hashtbale, binary search
leetcode 981
⭐️design


237
Transpose Matrix
array
leetcode 867



238
Minesweeper
graph, hashtable
leetcode 529
⭐️ looks difficult but its easy. 1st bfs and 2nd dfs O(rc)


238
Number Of Corner Rectangles
hashtable
leetcode 750
📌tricky dp questions + 1 followup


239
Ugly Number II
recursion, hashtable
leetcode 264
⭐️i guess its a dynamic programming question


239
Minimum Area Rectangle
hashtable
leetcode 939
📌1st O(RCC) similar to lc750 but LTE. 2nd and 3rd OrderedDict+set O(RRCC) beat 24.04% and 79.11%


239
Bulls and Cows
hashtable
leetcode 299
1st O(n^2). 2nd O(n)


239
Most Stones Removed with Same Row or Column
union find
leetcode 947
1st O(n^2logn)


239
Reverse Vowels of a String
2pointers
leetcode 345
1st O(n)


239
Contiguous Array
hashtable
leetcode 525
⭐️1st O(n) zero sum subarray


239
Binary Tree Longest Consecutive Sequence
tree
leetcode 525
⭐️1st O(n)


239
Excel Sheet Column Number
math
leetcode 171



239
Excel Sheet Column Title
math
leetcode 168



240
Second Maximum Number
array
interview
📌 O(n)


241
Wiggle Sort
sort, swap
leetcode 280
⭐️1st O(nlogn) 2nd O(n)


241
Wiggle Sort II
sort
leetcode 324
1st O(nlogn)


242
Palindromic Substrings
2pointers
leetcode 647
📌very similar to leetcode 5


242
Find Duplicate File in System
hashtable
leetcode 609
1st O(nk)


242
Third Maximum Number
array
leetcode 414
📌 O(n)


243
Count Numbers with Unique Digits
math
leetcode 357
⭐️ O(n)


243
Prison Cells After N Days
array
leetcode 957
⭐️


244
The Maze
graph
leetcode 490
📌 1st BFS O(RC) 2nd DFS O(RC)


244
The Maze II
graph
leetcode 505
📌 1st BFS O(RC) 2nd DFS TLE WTF!?!


244
Asteroid Collision
stack
leetcode 735
📌1st O(n)


244
Minimum Cost For Tickets
stack
leetcode 983
📌1st O(n), 2nd O(n)


245
Snakes and Ladders
graph, hashtable
leetcode 909
⭐️1st O(mnlogn), 2nd O(mn)


245
Cousins in Binary Tree
tree, hashtable
leetcode 993
1st O(n)


245
Permutation Sequence
permutation
leetcode 60
📌1st O((n-1)!)


246
Can Place Flowers
array
leetcode 605
1st, 2nd O(n)


246
All Nodes Distance K in Binary Tree
tree, graph, hashtable
leetcode 863
1st O(n)


247
Find the Town Judge
graph, hashtable
leetcode 997
1st O(T+N^2), 2nd O(T+2n)


247
Restore IP Addresses
graph, hashtable
leetcode 997
📌O(3^n) worst


247
Convert Sorted List to Binary Search Tree
linked list, BST
leetcode 109
📌O(2n)


248
Rotting Oranges
graph, hashtable
leetcode 994
⭐️O(kRC)


248
Valid Parenthesis String
dynamic programming
leetcode 678
⭐️1st O(3^n) brute force. 2nd O(n)


249
Validate IP Address
string
leetcode 468
bad question description


249
Design Snake Game
hashtbale, linked list
leetcode 353
⭐️


250
Car Fleet
sort, math
leetcode 853
O(nlogn) learned from others


250
Longest Word in Dictionary through Deleting
string, array
leetcode 524
1st O(nk)


251
Word Pattern
hashtable
leetcode 290
📌1st, 2nd O(n)


251
Longest Repeating Character Replacement
hashtable, 2pointers
leetcode 424
📌1st O(n), 2nd O(nk)


251
Next Greater Element II
linked list, stack
leetcode 1019
📌1st O(4n), 2nd O(2n)


252
Convert Binary Search Tree to Sorted Doubly Linked List
BST, linked list
leetcode 426
1st O(2n)


252
Sorted an Array
sort
leetcode 912
merge sort, quick sort


253
Add One Row to Tree
recursion
leetcode 623
⭐️recursive dfs O(n)


253
Boundary of Binary Tree
tree
leetcode 545
📌preorder + inorder + postorder


254
Rotate Array
array
leetcode 189
1st O(2n)Q(n) 📌 2nd O(3n)Q(1)


254
Valid Anagram
hashtable
leetcode 242
1st O(S+T)


254
Kth Smallest Element in a BST
tree
leetcode 230
1st O(n)


254
Kill Process
graph
leetcode 582
1st bfs O(n) 2nd dfs O(n)



",9
vesoft-inc/nebula,C++,"Nebula
Nebula is a distributed, scalable, lighting-fast graph database. It is the only solution in the world capable to host graphs with dozens of billions of vertices (nodes) and trillions of edges, while still provides millisecond latency.
Nebula's goal is to provide reading, writing, and computing with high concurrency, low latency for super large scale graphs. Nebula is an open source project and we are looking forward to working with the community to popularize and promote the graph database.
As a graph database, Nebula has these features

Symmetrically distributed
Highly scalable
Fault tolerant
Strong data consistency
SQL-like query language

How can I get Nebula
Nebula source code is available here on GitHub. The currently release is version 0.1. Please refer the Release Notes here for details. You can also download docker image to try it.
More details on how to get Nebula image click Get Started.
How can I contribute
As the team behind Nebula, we fully commit to the community and all-in to the open source project. All the core features are and will be implemented in the open source repository.
We also encourage the community to involve the project. There are a few ways you can contribute:

You can download and try Nebula, and provide us feedbacks
You can submit your feature requirements and bug reports
You can perfectionate documentations
You can fix bugs or implement features. More details on how to build the project and submit the Pull Requests click how-to-contribute.

Licensing
Nebula is under Apache 2.0 license, so you can freely download, modify, deploy the source code to meet your needs. You can also freely deploy Nebula as a back-end service to support your SAAS deployment.
In order to prevent cloud providers monetizing from the project without contributing back, we added Common Clause 1.0 to the project. As mentioned above, we fully commit to the open source community. We would love to hear your thoughts on the licensing model and are willing to make it more suitable for the community.
Contact

Please use GitHub issue tracker for filling bugs or feature requests.
Join .

",117
cccnqu/wd107b,HTML,"網頁設計課程 -- 筆記、習題與專案



欄位
內容




學生
xxx


學號末兩碼
xx


教師
陳鍾誠


學校
金門大學


科系
資訊工程系


我的專案
說明



",2
DangerOnTheRanger/maniwani,Python,"Maniwani - an anonymous imageboard for the 21st century
Maniwani is a work-in-progress imageboard implementation using Flask.
Come visit the project IRC channel #maniwani on rizon.net.
Where does the name come from? I could tell you, but by that point
you'd have been torn to pieces.
Features

Real-time content updates - watch as new posts roll into a thread you're viewing.
Fully-featured REST API - don't like the web frontend? Submit a PR Write your own client.
Full Markdown support - add any kind of formatting you like to your posts.
Theme support - use a night theme for late night browsing sessions, or set your own custom colors.
Excellent attachment support - attach text, every kind of video under the sun, and most image formats
you can think of, WebP included. One day soon, you'll be able to attach and view rich text documents and
3D models inside the browser, too. Don't want people posting certain kinds of files to your site? Admins
can specify allowed MIME types on a per-board basis.
Will SQLite and flat files suffice for your deployment? Done. Or will nothing less than Postgres, S3,
and Redis do? Turn Maniwani on and it scales right up.
Turn-key setup and installation, thanks to Docker. Edit a couple plaintext config files, and
you're set - no more messing with system libraries, or manually clicking through a setup page after you
finally got everything booting. Updating to a new version of Maniwani is equally easy; no manual migrations required.
CDN support - because nobody's application server or object store deserves the pain of having to
serve up static files. CDN not included.
No Javascript? No problem - obviously you'll miss out on some stuff like real-time updates, though.

Planned

Randomized anime-styled avatars for everyone - no more keeping track of who is who in a thread with
only hard-to-differentiate hex IDs!

Installation
With Docker - standalone development image
In this directory, run the following to build a development Docker image:
docker build -t maniwani-dev --target dev .

To run your new instance, then type:
docker run -p 5000:5000 maniwani-dev

Point your web browser at http://127.0.0.1:5000 to view your new installation. Note
that running Maniwani in this way will not save any data after the container is closed.
This Docker method is intended to easily see what Maniwani is capable of, as well as
serve as a quick and easily-replicated testbed.
With Docker - production image and environment
It is also possible through docker-compose to spin up an environment very similar
to what one might use in production for Maniwani (uWSGI in addition to Postgres, Redis,
Minio, and captchouli), though for the time being this setup is Linux-only and
requires docker-compose. In this directory, type:
docker-compose build
docker-compose run captchouli bootstrap
docker-compose up -d
docker-compose run maniwani bootstrap

The last command will only need to be run once per clean installation of the production
environment. If you ever want to remove all database and storage data, remove the
compose-data and compose-captchouli directories, though you'll likely need root
permissions to do so since some subdirectories are created by other users. At this point,
you can use the normal docker-compose start and docker-compose stop to start and stop the production
environment, navigating to http://127.0.0.1:5000 as per usual to view Maniwani. If you
want additional info on deploying Maniwani in production, see doc/deploying.md for more.
As a final sidenote, this method will run all of your computer's traffic through
a local DNS proxy while active, as otherwise it would not be possible to view
attachments, since the local S3 server would be unreachable via hostname. If
you want to audit the DNS proxy code (which is an open-source 3rd-party container),
feel free to do so at https://github.com/mageddo/dns-proxy-server .
Without Docker
Note that building without Docker takes a lot less time but is less straightforward and does
not simulate a production environment. If you're interested in working on Maniwani, this is
a good setup option, but otherwise you're probably better off using Docker to test or deploy
a Maniwani instance.
Python 3.4+ in addition to Pipenv and npm are required for installation, but I currently use 3.6
for developing Maniwani; if you're having problems with 3.4, file a bug report, but also
try 3.6 if you can. To install everything save for ffmpeg (see the following ""Notes on ffmpeg""
section for more), run the following commands in this directory:
pipenv install
npm install
npm run gulp

You'll also want to initialize a database with some initial options; so run:
pipenv run python bootstrap.py

Next, to run the development server, type pipenv run python storestub.py & followed by pipenv run flask run,
and point your web browser at http://127.0.0.1:5000 to view your new Maniwani installation. If you ever want
to wipe the database clean, that's currently handled by removing test.db (and the uploads directory if
you uploaded anything) and re-running the bootstrap.py script.
Notes on ffmpeg
Installing ffmpeg can either be done with your system's package manager if you
do not already have it installed, or you can use the ffmpeg_bootstrap.py script
to grab a static build of ffmpeg like so, assuming you are in the same directory
as the script itself:
python3 ffmpeg_bootstrap.py
cp ffmpeg-stub-config.cfg ../maniwani.cfg
echo MANIWANI_CFG=maniwani.cfg > ../.env

Screenshots
Front page aggregating all boards:

Viewing a thread:

Board index (images are pulled from the most recent OP in each board):

Thread gallery mode:

Board catalog view, also showing off responsive mode:

",31
PCSX2/pcsx2,C++,"PCSX2
  
PCSX2 is a free and open-source PlayStation 2 (PS2) emulator. Its purpose is to emulate the PS2's hardware, using a combination of MIPS CPU Interpreters, Recompilers and a Virtual Machine which manages hardware states and PS2 system memory. This allows you to play PS2 games on your PC, with many additional features and benefits.
Project Details
The PCSX2 project has been running for more than ten years. Past versions could only run a few public domain game demos, but newer versions can run many games at full speed, including popular titles such as Final Fantasy X and Devil May Cry 3. Visit the PCSX2 homepage to check the latest compatibility status of games (with more than 2000 titles tested), or ask for help in the official forums.
The latest officially released stable version is version 1.4.0.
Installers and binaries for both Windows and Linux are available from our website.
Development builds are also available from our website.
System Requirements
Minimum

OS: Windows Vista SP2 or newer or GNU/Linux (32-bit or 64-bit)
CPU: Any that supports SSE2 (Pentium 4 and up, Athlon64 and up) @ 1600 STR or better
GPU: DirectX 10 GPU or better
RAM: 2GB or more

Recommended

OS: Windows 7/8/8.1/10 (64-bit) or GNU/Linux (64-bit)
CPU: Intel Haswell (or AMD equivalent) @ 2000 STR or better
GPU: DirectX 11 GPU or greater
RAM: 4GB or more

Notes


You need the Visual C++ 2015 x86 Redistributables for this version to work.
Note: Visual C++ 2017 is directly compatible with Visual C++ 2015. While the project is built with Visual C++ 2015, either version will work.


PCSX2 1.4.0 is the last stable version to support Windows XP and Direct3D9. Windows XP is no longer getting updates (including security-related updates), and graphics drivers for Windows XP are older and no longer maintained.


Make sure to update your operating system, drivers, and DirectX (if applicable) to ensure you have the best experience possible. Having a newer GPU is also recommended so you have the latest supported drivers.


Because of copyright issues, and the complexity of trying to work around it, you need a BIOS dump extracted from a legitimately-owned PS2 console to use the emulator. For more information about the BIOS and how to get it from your console, visit this page.


PCSX2 mainly takes advantage of 2 CPU cores. As of this commit PCSX2 can now take advantage of more than 2 cores using the MTVU speedhack. This can be a significant speedup on CPUs with 3+ cores, but it may be a slowdown on GS-limited games (or on CPUs with fewer than 2 cores).


Requirements benchmarks are based on a statistic from the Passmark CPU bench marking software. When we say ""STR"", we are referring to Passmark's ""Single Thread Rating"" statistic. You can look up your CPU on https://cpubenchmark.net to see how it compares to PCSX2's requirements.


Screenshots








",3233
williamfiset/Algorithms,Java," 
Dynamic Programming

Coin change problem - O(nW)
Edit distance - O(nm)
🎥Knapsack 0/1 - O(nW)
Knapsack unbounded (0/∞) - O(nW)
Maximum contiguous subarray - O(n)
Longest Common Subsequence (LCS) - O(nm)
Longest Increasing Subsequence (LIS) - O(n2)
Longest Palindrome Subsequence (LPS) - O(n2)
🎥Traveling Salesman Problem (dynamic programming, iterative) - O(n22n)
Traveling Salesman Problem (dynamic programming, recursive) - O(n22n)
Minimum Weight Perfect Matching (iterative, complete graph) - O(n22n)

Geometry

Angle between 2D vectors - O(1)
Angle between 3D vectors - O(1)
Circle-circle intersection point(s) - O(1)
Circle-line intersection point(s) - O(1)
Circle-line segment intersection point(s) - O(1)
Circle-point tangent line(s) - O(1)
Closest pair of points (line sweeping algorithm) - O(nlog(n))
Collinear points test (are three 2D points on the same line) - O(1)
Convex hull (Graham Scan algorithm) - O(nlog(n))
Convex hull (Monotone chain algorithm) - O(nlog(n))
Convex polygon area - O(n)
Convex polygon cut - O(n)
Convex polygon contains points - O(log(n))
Coplanar points test (are four 3D points on the same plane) - O(1)
Line class (handy infinite line class) - O(1)
Line-circle intersection point(s) - O(1)
Line segment-circle intersection point(s) - O(1)
Line segment to general form (ax + by = c) - O(1)
Line segment-line segment intersection - O(1)
Longitude-Latitude geographic distance - O(1)
Point is inside triangle check - O(1)
Point rotation about point - O(1)
Triangle area algorithms - O(1)
[UNTESTED] Circle-circle intersection area - O(1)
[UNTESTED] Circular segment area - O(1)

Graph theory
Tree algorithms

Tree canonical form (tree isomorphism, tree encoding) - O(Elog(E))
Tree center(s) - O(V+E)
Tree diameter - O(V+E)

Network flow

Bipartite graph verification (adjacency list) - O(V+E)
🎥Max flow & Min cut (Ford-Fulkerson with DFS, adjacency list) - O(fE)
Max flow & Min cut (Ford-Fulkerson with DFS, adjacency matrix) - O(fV2)
🎥Max flow & Min cut (Edmonds-Karp, adjacency list) - O(VE2)
🎥Max flow & Min cut (Capacity scaling, adjacency list) - O(E2log2(U))
🎥Max flow & Min cut (Dinic's, adjacency list) - O(EV2) or O(E√V) for bipartite graphs
Maximum Cardinality Bipartite Matching (augmenting path algorithm, adjacency list) - O(VE)
Min Cost Max Flow (Bellman-Ford, adjacency list) - O(E2V2)
Min Cost Max Flow (Johnson's algorithm, adjacency list) - O(E2Vlog(V))

Other graph theory

🎥Articulation points/cut vertices (adjacency list) - O(V+E)
Bellman-Ford (edge list, negative cycles, fast & optimized) - O(VE)
🎥Bellman-Ford (adjacency list, negative cycles) - O(VE)
Bellman-Ford (adjacency matrix, negative cycles) - O(V3)
🎥Breadth first search (adjacency list) - O(V+E)
Breadth first search (adjacency list, fast queue) - O(V+E)
🎥Bridges/cut edges (adjacency list) - O(V+E)
Find connected components (adjacency list, union find) - O(Elog(E))
Find connected components (adjacency list, DFS) - O(V+E)
Depth first search (adjacency list, iterative) - O(V+E)
Depth first search (adjacency list, iterative, fast stack) - O(V+E)
🎥Depth first search (adjacency list, recursive) - O(V+E)
🎥Dijkstra's shortest path (adjacency list, lazy implementation) - O(Elog(V))
🎥Dijkstra's shortest path (adjacency list, eager implementation + D-ary heap) - O(ElogE/V(V))
🎥Eulerian Path (directed edges) - O(E+V)
🎥Floyd Warshall algorithm (adjacency matrix, negative cycle check) - O(V3)
Graph diameter (adjacency list) - O(VE)
Kruskal's min spanning tree algorithm (edge list, union find) - O(Elog(E))
🎥Kruskal's min spanning tree algorithm (edge list, union find, lazy sorting) - O(Elog(E))
Prim's min spanning tree algorithm (lazy version, adjacency list) - O(Elog(E))
Prim's min spanning tree algorithm (lazy version, adjacency matrix) - O(V2)
Prim's min spanning tree algorithm (eager version, adjacency list) - O(Elog(V))
Steiner tree (minimum spanning tree generalization) - O(V3 + V2 * 2T + V * 3T)
🎥Tarjan's strongly connected components algorithm (adjacency list)  - O(V+E)
Tarjan's strongly connected components algorithm (adjacency matrix)  - O(V2)
🎥Topological sort (acyclic graph, adjacency list) - O(V+E)
Topological sort (acyclic graph, adjacency matrix) - O(V2)
Traveling Salesman Problem (brute force) - O(n!)
🎥Traveling Salesman Problem (dynamic programming, iterative) - O(n22n)
Traveling Salesman Problem (dynamic programming, recursive) - O(n22n)

Linear algebra

Freivald's algorithm (matrix multiplication verification) - O(kn2)
Gaussian elimination (solve system of linear equations) - O(cr2)
Gaussian elimination (modular version, prime finite field) - O(cr2)
Linear recurrence solver (finds nth term in a recurrence relation) - O(m3log(n))
Matrix determinant (Laplace/cofactor expansion) - O((n+2)!)
Matrix inverse - O(n3)
Matrix multiplication - O(n3)
Matrix power - O(n3log(p))
Square matrix rotation - O(n2)

Mathematics

[UNTESTED] Chinese remainder theorem
Prime number sieve (sieve of Eratosthenes) - O(nlog(log(n)))
Prime number sieve (sieve of Eratosthenes, compressed) - O(nlog(log(n)))
Totient function (phi function, relatively prime number count) - O(n1/4)
Totient function using sieve (phi function, relatively prime number count) - O(nlog(log(n)))
Extended euclidean algorithm - ~O(log(a + b))
Greatest Common Divisor (GCD) - ~O(log(a + b))
Fast Fourier transform (quick polynomial multiplication) - O(nlog(n))
Fast Fourier transform (quick polynomial multiplication, complex numbers) - O(nlog(n))
Primality check - O(√n)
Primality check (Rabin-Miller) - O(k)
Least Common Multiple (LCM) - ~O(log(a + b))
Modular inverse - ~O(log(a + b))
Prime factorization (pollard rho) - O(n1/4)
Relatively prime check (coprimality check) - ~O(log(a + b))

Other

Bit manipulations - O(1)
List permutations - O(n!)
🎥Power set (set of all subsets) - O(2n)
Set combinations - O(n choose r)
Set combinations with repetition - O((n+r-1) choose r)
Sliding Window Minimum/Maximum - O(1)
Square Root Decomposition - O(1) point updates, O(√n) range queries
Unique set combinations - O(n choose r)

Search algorithms

Binary search (real numbers) - O(log(n))
Interpolation search (discrete discrete) - O(n) or O(log(log(n))) with uniform input
Ternary search (real numbers) - O(log(n))
Ternary search (discrete numbers) - O(log(n))

Sorting algorithms

Bubble sort - O(n2)
Bucket sort - Θ(n + k)
Counting sort - O(n + k)
Heapsort - O(nlog(n))
Insertion sort - O(n2)
Mergesort - O(nlog(n))
Quicksort (in-place, Hoare partitioning) - Θ(nlog(n))
Selection sort - O(n2)

String algorithms

Booth's algorithm (finds lexicographically smallest string rotation) - O(n)
Knuth-Morris-Pratt algorithm (finds pattern matches in text) - O(n+m)
Longest Common Prefix (LCP) array - O(nlog(n)) bounded by SA construction, otherwise O(n)
🎥Longest Common Substring (LCS) - O(nlog(n)) bounded by SA construction, otherwise O(n)
🎥Longest Repeated Substring (LRS) - O(nlog(n))
Manacher's algorithm (finds all palindromes in text) - O(n)
Rabin-Karp algorithm (finds pattern matches in text) - O(n+m)
Substring verification with suffix array - O(nlog(n)) SA construction and O(mlog(n)) per query

Contributing
This repository is contribution friendly 😃. If you're an algorithms enthusiast (like me!) and want to add or improve an algorithm your contribution is welcome! Please be sure to include tests 😘.
For developers
This project uses Gradle as a build system (and testing). Make sure you have Java 8+ SDK installed and the gradle command-line tool. Run the build command to make sure you don't get any errors:
Algorithms$ gradle build
Adding a new algorithm
The procedure to add a new algorithm named Foo is the following:

Identify the category folder your algorithm belongs to. For example a matrix multiplication snippet would belong to the com/williamfiset/algorithms/linearalgebra folder. You may also create a new category folder if appropriate.
Add the algorithm implementation to com/williamfiset/algorithms/category/ as com/williamfiset/algorithms/category/Foo.java
Add tests for Foo in javatests/com/williamfiset/algorithms/category/FooTest.java
Edit the build.gradle file if you added a new category to the project.
Test your algorithm thoroughly (see testing section below)
Send pull request for review 😮

Testing
This repository places a large emphasis on good testing practice to ensure that published algorithms are bug free and high quality. Testing is done using a combinations of frameworks including: JUnit, Mockito and the Google Truth framework. Currently very few algorithms have tests because they were (informally) tested against problems on Kattis in a competitive programming setting, but we are slowly migrating to formally testing these algorithms for robustness. To run all the tests execute:
Algorithms$ gradle test
When developing you likely do not want to run all tests but only a subset of them. For example, if you want to run the FloydWarshallSolverTest.java file under javatests/com/williamfiset/algorithms/graphtheory/FloydWarshallSolverTest.java you can execute:
Algorithms$ gradle test --tests ""javatests.com.williamfiset.algorithms.graphtheory.FloydWarshallSolverTest""
License
This repository is released under the MIT license. In short, this means you are free to use this software in any personal, open-source or commercial projects. Attribution is optional but appreciated.
",527
williamfiset/Algorithms,Java," 
Dynamic Programming

Coin change problem - O(nW)
Edit distance - O(nm)
🎥Knapsack 0/1 - O(nW)
Knapsack unbounded (0/∞) - O(nW)
Maximum contiguous subarray - O(n)
Longest Common Subsequence (LCS) - O(nm)
Longest Increasing Subsequence (LIS) - O(n2)
Longest Palindrome Subsequence (LPS) - O(n2)
🎥Traveling Salesman Problem (dynamic programming, iterative) - O(n22n)
Traveling Salesman Problem (dynamic programming, recursive) - O(n22n)
Minimum Weight Perfect Matching (iterative, complete graph) - O(n22n)

Geometry

Angle between 2D vectors - O(1)
Angle between 3D vectors - O(1)
Circle-circle intersection point(s) - O(1)
Circle-line intersection point(s) - O(1)
Circle-line segment intersection point(s) - O(1)
Circle-point tangent line(s) - O(1)
Closest pair of points (line sweeping algorithm) - O(nlog(n))
Collinear points test (are three 2D points on the same line) - O(1)
Convex hull (Graham Scan algorithm) - O(nlog(n))
Convex hull (Monotone chain algorithm) - O(nlog(n))
Convex polygon area - O(n)
Convex polygon cut - O(n)
Convex polygon contains points - O(log(n))
Coplanar points test (are four 3D points on the same plane) - O(1)
Line class (handy infinite line class) - O(1)
Line-circle intersection point(s) - O(1)
Line segment-circle intersection point(s) - O(1)
Line segment to general form (ax + by = c) - O(1)
Line segment-line segment intersection - O(1)
Longitude-Latitude geographic distance - O(1)
Point is inside triangle check - O(1)
Point rotation about point - O(1)
Triangle area algorithms - O(1)
[UNTESTED] Circle-circle intersection area - O(1)
[UNTESTED] Circular segment area - O(1)

Graph theory
Tree algorithms

Tree canonical form (tree isomorphism, tree encoding) - O(Elog(E))
Tree center(s) - O(V+E)
Tree diameter - O(V+E)

Network flow

Bipartite graph verification (adjacency list) - O(V+E)
🎥Max flow & Min cut (Ford-Fulkerson with DFS, adjacency list) - O(fE)
Max flow & Min cut (Ford-Fulkerson with DFS, adjacency matrix) - O(fV2)
🎥Max flow & Min cut (Edmonds-Karp, adjacency list) - O(VE2)
🎥Max flow & Min cut (Capacity scaling, adjacency list) - O(E2log2(U))
🎥Max flow & Min cut (Dinic's, adjacency list) - O(EV2) or O(E√V) for bipartite graphs
Maximum Cardinality Bipartite Matching (augmenting path algorithm, adjacency list) - O(VE)
Min Cost Max Flow (Bellman-Ford, adjacency list) - O(E2V2)
Min Cost Max Flow (Johnson's algorithm, adjacency list) - O(E2Vlog(V))

Other graph theory

🎥Articulation points/cut vertices (adjacency list) - O(V+E)
Bellman-Ford (edge list, negative cycles, fast & optimized) - O(VE)
🎥Bellman-Ford (adjacency list, negative cycles) - O(VE)
Bellman-Ford (adjacency matrix, negative cycles) - O(V3)
🎥Breadth first search (adjacency list) - O(V+E)
Breadth first search (adjacency list, fast queue) - O(V+E)
🎥Bridges/cut edges (adjacency list) - O(V+E)
Find connected components (adjacency list, union find) - O(Elog(E))
Find connected components (adjacency list, DFS) - O(V+E)
Depth first search (adjacency list, iterative) - O(V+E)
Depth first search (adjacency list, iterative, fast stack) - O(V+E)
🎥Depth first search (adjacency list, recursive) - O(V+E)
🎥Dijkstra's shortest path (adjacency list, lazy implementation) - O(Elog(V))
🎥Dijkstra's shortest path (adjacency list, eager implementation + D-ary heap) - O(ElogE/V(V))
🎥Eulerian Path (directed edges) - O(E+V)
🎥Floyd Warshall algorithm (adjacency matrix, negative cycle check) - O(V3)
Graph diameter (adjacency list) - O(VE)
Kruskal's min spanning tree algorithm (edge list, union find) - O(Elog(E))
🎥Kruskal's min spanning tree algorithm (edge list, union find, lazy sorting) - O(Elog(E))
Prim's min spanning tree algorithm (lazy version, adjacency list) - O(Elog(E))
Prim's min spanning tree algorithm (lazy version, adjacency matrix) - O(V2)
Prim's min spanning tree algorithm (eager version, adjacency list) - O(Elog(V))
Steiner tree (minimum spanning tree generalization) - O(V3 + V2 * 2T + V * 3T)
🎥Tarjan's strongly connected components algorithm (adjacency list)  - O(V+E)
Tarjan's strongly connected components algorithm (adjacency matrix)  - O(V2)
🎥Topological sort (acyclic graph, adjacency list) - O(V+E)
Topological sort (acyclic graph, adjacency matrix) - O(V2)
Traveling Salesman Problem (brute force) - O(n!)
🎥Traveling Salesman Problem (dynamic programming, iterative) - O(n22n)
Traveling Salesman Problem (dynamic programming, recursive) - O(n22n)

Linear algebra

Freivald's algorithm (matrix multiplication verification) - O(kn2)
Gaussian elimination (solve system of linear equations) - O(cr2)
Gaussian elimination (modular version, prime finite field) - O(cr2)
Linear recurrence solver (finds nth term in a recurrence relation) - O(m3log(n))
Matrix determinant (Laplace/cofactor expansion) - O((n+2)!)
Matrix inverse - O(n3)
Matrix multiplication - O(n3)
Matrix power - O(n3log(p))
Square matrix rotation - O(n2)

Mathematics

[UNTESTED] Chinese remainder theorem
Prime number sieve (sieve of Eratosthenes) - O(nlog(log(n)))
Prime number sieve (sieve of Eratosthenes, compressed) - O(nlog(log(n)))
Totient function (phi function, relatively prime number count) - O(n1/4)
Totient function using sieve (phi function, relatively prime number count) - O(nlog(log(n)))
Extended euclidean algorithm - ~O(log(a + b))
Greatest Common Divisor (GCD) - ~O(log(a + b))
Fast Fourier transform (quick polynomial multiplication) - O(nlog(n))
Fast Fourier transform (quick polynomial multiplication, complex numbers) - O(nlog(n))
Primality check - O(√n)
Primality check (Rabin-Miller) - O(k)
Least Common Multiple (LCM) - ~O(log(a + b))
Modular inverse - ~O(log(a + b))
Prime factorization (pollard rho) - O(n1/4)
Relatively prime check (coprimality check) - ~O(log(a + b))

Other

Bit manipulations - O(1)
List permutations - O(n!)
🎥Power set (set of all subsets) - O(2n)
Set combinations - O(n choose r)
Set combinations with repetition - O((n+r-1) choose r)
Sliding Window Minimum/Maximum - O(1)
Square Root Decomposition - O(1) point updates, O(√n) range queries
Unique set combinations - O(n choose r)

Search algorithms

Binary search (real numbers) - O(log(n))
Interpolation search (discrete discrete) - O(n) or O(log(log(n))) with uniform input
Ternary search (real numbers) - O(log(n))
Ternary search (discrete numbers) - O(log(n))

Sorting algorithms

Bubble sort - O(n2)
Bucket sort - Θ(n + k)
Counting sort - O(n + k)
Heapsort - O(nlog(n))
Insertion sort - O(n2)
Mergesort - O(nlog(n))
Quicksort (in-place, Hoare partitioning) - Θ(nlog(n))
Selection sort - O(n2)

String algorithms

Booth's algorithm (finds lexicographically smallest string rotation) - O(n)
Knuth-Morris-Pratt algorithm (finds pattern matches in text) - O(n+m)
Longest Common Prefix (LCP) array - O(nlog(n)) bounded by SA construction, otherwise O(n)
🎥Longest Common Substring (LCS) - O(nlog(n)) bounded by SA construction, otherwise O(n)
🎥Longest Repeated Substring (LRS) - O(nlog(n))
Manacher's algorithm (finds all palindromes in text) - O(n)
Rabin-Karp algorithm (finds pattern matches in text) - O(n+m)
Substring verification with suffix array - O(nlog(n)) SA construction and O(mlog(n)) per query

Contributing
This repository is contribution friendly 😃. If you're an algorithms enthusiast (like me!) and want to add or improve an algorithm your contribution is welcome! Please be sure to include tests 😘.
For developers
This project uses Gradle as a build system (and testing). Make sure you have Java 8+ SDK installed and the gradle command-line tool. Run the build command to make sure you don't get any errors:
Algorithms$ gradle build
Adding a new algorithm
The procedure to add a new algorithm named Foo is the following:

Identify the category folder your algorithm belongs to. For example a matrix multiplication snippet would belong to the com/williamfiset/algorithms/linearalgebra folder. You may also create a new category folder if appropriate.
Add the algorithm implementation to com/williamfiset/algorithms/category/ as com/williamfiset/algorithms/category/Foo.java
Add tests for Foo in javatests/com/williamfiset/algorithms/category/FooTest.java
Edit the build.gradle file if you added a new category to the project.
Test your algorithm thoroughly (see testing section below)
Send pull request for review 😮

Testing
This repository places a large emphasis on good testing practice to ensure that published algorithms are bug free and high quality. Testing is done using a combinations of frameworks including: JUnit, Mockito and the Google Truth framework. Currently very few algorithms have tests because they were (informally) tested against problems on Kattis in a competitive programming setting, but we are slowly migrating to formally testing these algorithms for robustness. To run all the tests execute:
Algorithms$ gradle test
When developing you likely do not want to run all tests but only a subset of them. For example, if you want to run the FloydWarshallSolverTest.java file under javatests/com/williamfiset/algorithms/graphtheory/FloydWarshallSolverTest.java you can execute:
Algorithms$ gradle test --tests ""javatests.com.williamfiset.algorithms.graphtheory.FloydWarshallSolverTest""
License
This repository is released under the MIT license. In short, this means you are free to use this software in any personal, open-source or commercial projects. Attribution is optional but appreciated.
",527
spencer-hann/Text_to_Image_Synthesis,Jupyter Notebook,"Text to Image Synthesis
Using NLP feature extraction, and Generative Adversarial Networks to generate images from text descriptions.
Partial replication of the paper Generative Adversarial Text to Image Synthesis.
Authors: Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee
Data sets
General Use
From Caltech and Oxford websites:

Caltech-UCSD Birds-200-2011
Oxford-102 Flowers

Preprocessed w/Text descriptions
From primary author's github repo:

Birds from github.com/reedscot/cvpr2016
Flowers from github.com/reedscot/cvpr2016

",2
baiczsy/spring-elasticsearch-client,Java,"spring-elasticsearch-client
基于ES的elasticsearch-rest-high-level-client 7.0.0与spring整合客户端。
使用说明
clone项目并install到本地库(暂未发布到center仓库)。
补充：如需使用中文检索，请在Elasticsearch中安装ik中文分词插件，注意对应版本。
ik安装：
$ ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.0.0/elasticsearch-analysis-ik-7.0.0.zip

依赖
<dependency>
    <groupId>org.springframework</groupId>
    <artifactId>spring-context</artifactId>
    <version>5.1.6.RELEASE</version>
</dependency>
<dependency>
    <groupId>com.github.baiczsy</groupId>
    <artifactId>spring-elasticsearch-client</artifactId>
    <version>1.0.0</version>
</dependency>
配置
@Configuration
public class RestClientConfigure {

    /**
     * 配置RestClient连接池，基于commons-pool2
     */
    @Bean
    public RestClientPoolConfig poolConfig(){
        RestClientPoolConfig poolConfig = new RestClientPoolConfig();
        poolConfig.setMinIdle(5);
        poolConfig.setMaxWaitMillis(2000);
        //other...
        return poolConfig;
    }

    @Bean
    public RestClientConfiguration restClientConfiguration(){
        RestClientStandaloneConfiguration configuration = new RestClientStandaloneConfiguration(""localhost"",9200);
        configuration.setConnectTimeout(1000);
        configuration.setConnectionRequestTimeout(500);
        configuration.setSocketTimeout(20000);
        configuration.setMaxConnTotal(100);
        configuration.setMaxConnPerRoute(100);
        return configuration;
    }

    /**
     * 装配RestClient配置类(集群)
     */
    /*@Bean
    public RestClientConfiguration clusterClientConfiguration(){
        List<ElasticsearchNode> hosts = new ArrayList<>();
        hosts.add(new ElasticsearchNode(""localhost"", 9200));
        hosts.add(new ElasticsearchNode(""localhost"", 9201));
        RestClientClusterConfiguration configuration = new RestClientClusterConfiguration(hosts);
        configuration.setConnectTimeout(1000);
        configuration.setConnectionRequestTimeout(500);
        configuration.setSocketTimeout(20000);
        configuration.setMaxConnTotal(100);
        configuration.setMaxConnPerRoute(100);
        return configuration;
    }*/

    /**
     * 装配ElasticsearchClientFactory
     */
    @Bean
    public ElasticsearchClientFactory elasticsearchClientFactory(RestClientConfiguration configuration, RestClientPoolConfig poolConfig){
        ElasticsearchClientFactory factory = new ElasticsearchClientFactory(configuration, poolConfig);
        //如果需要，可以设置默认的请求头信息
        //Map<String, String> headers = new HashMap<>();
        //headers.put(""key"", ""value"");
        //factory.setDefaultHeaders(headers);
        return factory;
    }
    
    /**
     * 装配RestClientTemplate
     */
    @Bean
    public RestClientTemplate restClientTemplate(ElasticsearchClientFactory factory){
        return new RestClientTemplate(factory);
    }
}
注入RestClientTemplate
@Autowired
private RestClientTemplate template;
创建索引
//构建mapping
Map<String, Object> name = new HashMap<>();
name.put(""type"", ""text"");
name.put(""analyzer"", ""ik_max_word"");

Map<String, Object> age = new HashMap<>();
age.put(""type"", ""integer"");

Map<String, Object> address = new HashMap<>();
address.put(""type"", ""text"");
address.put(""analyzer"", ""ik_max_word"");

Map<String, Object> properties = new HashMap<>();
properties.put(""name"", name);
properties.put(""age"", age);
properties.put(""address"", address);

Map<String, Object> mapping = new HashMap<>();
mapping.put(""properties"", properties);

//users为index名称
CreateIndexRequest request = new CreateIndexRequest(""users"").mapping(mapping);
template.opsForIndices().create(request);
删除索引
DeleteIndexRequest request = new DeleteIndexRequest(""users"");
template.opsForIndices().delete(request);
添加数据
//用户数据
Map<String, Object> map = new HashMap<>();
map.put(""name"", ""zing"");
map.put(""age"", 26);
map.put(""address"",""global village"");

IndexRequest request = new IndexRequest(""users"").id(""1"").source(map);
template.persist(request);
根据id查找数据
GetRequest request = new GetRequest(""users"").id(""1"");
Map<String, Object> map = template.get(request);
更新数据
//用户数据
Map<String, Object> map = new HashMap<>();
map.put(""name"", ""bobo"");
map.put(""age"", 28);

UpdateRequest request = new UpdateRequest(""users"", ""1"").doc(map);
template.update(index, id, map);
删除数据
DeleteRequest request = new DeleteRequest(""users"").id(""1"");
template.delete(index, id);
检索数据
//fields为需要检索的字段名(如：name，address)，param为查询条件
public List<Map<String, Object>> search(String index, String param, String[] fields) {
    return template.opsForQuery().multiMatch(index, param, fields);
}
分页检索
template.opsForQuery()
            .from(0)
            .size(10)
            .multiMatch(index, param, fields);
异步操作
template.opsForQuery()
            .from(0)
            .size(10)
            .multiMatch(index, param, fields, new ActionListener<SearchResponse>() {
                @Override
                public void onResponse(SearchResponse searchResponse) {
                    for (SearchHit hit : searchResponse.getHits().getHits()) {
                        log.info(hit.getSourceAsString());
                    }
                }

                @Override
                public void onFailure(Exception e) {
                }
});
使用execute方法操作RestHighLevelClient
//例如添加一条记录
template.execute((highLevelClient) -> {
    IndexRequest request = new IndexRequest(index).id(id).source(map);
    request.timeout(TimeValue.timeValueNanos(timeout));
    highLevelClient.index(request, RequestOptions.DEFAULT);
});
说明：其他API参阅RestClienTemplate类，且所有API都支持同步和异步操作。
",2
mozilla/blurts-server,FreeMarker,"Firefox Monitor Server
Summary
Firefox Monitor notifies users when their credentials have been compromised in a data breach.
This code is for the monitor.firefox.com service & website.
Breach data is powered by haveibeenpwned.com.
See the Have I Been Pwned about page for
the ""what"" and ""why"" of data breach alerts.
Development
Requirements

Node 8+ (with npm)
Postgres

Install


Clone and change to the directory:
git clone https://github.com/mozilla/blurts-server.git
cd blurts-server


Install dependencies:
npm install


Copy the .env-dist file to .env:
cp .env-dist .env


Run


Run the server:
npm start


Note: npm start uses onchange and nodemon to automatically detect file
changes, re-compile static assets, and restart the express process. If you want
more control, see the scripts section of package.json for more commands.

Navigate to localhost:6060/

Database
To create the database tables ...


Create the blurts database:
createdb blurts


Update the DATABASE_URL value in your .env file with your local db
credentials:
DATABASE_URL=""postgres://<username>@localhost:<port>/blurts""



Run the migrations:
npm run db:migrate



Emails
The included .env-dist sets DEBUG_DUMMY_SMTP=1 which disables emails.
To send emails, you'll need to unset DEBUG_DUMMY_SMTP and supply real SMTP
config values for sending email.
You can set and source these via the .env file, or set them directly:
export DEBUG_DUMMY_SMTP=
export SMTP_HOST=<your-smtp-host>
export SMTP_PORT=<your-smtp-port>
export SMTP_USERNAME=<your-username>
export SMTP_PASSWORD=<your-password>
Firefox Accounts
Subscribe with a Firefox Account is controlled via the FXA_ENABLED
environment variable. (See .env-dist)
The repo comes with a development FxA oauth app pre-configured in .env, which
should work fine running the app on http://localhost:6060
To use a different Firefox Accounts oauth relying party,
you'll need to create an FxA Oauth Client and then set some OAUTH config values.
You can set and source these via the .env file:
OAUTH_CLIENT_ID=<your-fxa-oauth-client-id>
OAUTH_CLIENT_SECRET=<your-fxa-oauth-client-secret>
OAUTH_AUTHORIZATION_URI=""https://oauth-stable.dev.lcip.org/v1/authorization""
OAUTH_PROFILE_URI=""https://stable.dev.lcip.org/profile/v1/profile""
OAUTH_TOKEN_URI=""https://oauth-stable.dev.lcip.org/v1/token""
Testing
The full test suite can be run via npm test.
Individual tests
To run individual tests, use NODE_ENV=tests and jest:
NODE_ENV=tests jest --runInBand tests/home.test.js

To run tests with interactive debugger lines enabled:
NODE_ENV=tests node inspect --harmony ./node_modules/.bin/jest tests/home.test.js

Lint
After installing the dependencies, you can lint the code by calling:
npm run lint
Deployment
Firefox Monitor Breach Alerts is designed with 12-factor methodology.
Deploy on Heroku
You will need to set some required environment variables on Heroku.
heroku config:set COOKIE_SECRET=unsafe-cookie-secret-for-heroku
heroku config:set DEBUG_DUMMY_SMTP=1
And any others, depending on the features you're running on Heroku - e.g.,
Email or Firefox Accounts.
",193
gatsbyjs/store.gatsbyjs.org,JavaScript,"





  Gatsby Swag Store

This is the Gatsby store, where we make swag, stickers, and other Gatsby goodies available to contributors and Gatsby enthusiasts. 💪💜


  Photo credit: doraforscale

See it live: store.gatsbyjs.org
Technical Overview
This store is built with data from:

Shopify
The Shopify JavaScript Buy SDK
Auth0

We’re using Gatsby V2 and Emotion to get the data on screen.
The store is statically rendered using the Shopify source plugin, and the maintainer dashboard is a dynamic app (e.g. client-only routes) protected by Auth0.
Free Swag for Contributors
If you're a contributor to Gatsby, that means you can get one free item from our store! Log in using your GitHub account and we'll give you a discount code good for a t-shirt or socks. With five or more contributions, You can claim your level two swag!
See the docs for claiming contributor swag for additional details.
Cheap Swag for All
We sell our swag at pretty close to cost (we round for easy math) and we don’t charge shipping fees. Grab a t-shirt or socks and show everyone your favorite blazing fast framework!
Frequently Asked Questions

Why does it say I'm not eligible when I enter the discount code?
  
Try opening the store in an incognito window and then proceed to check out. When checking out, make sure you're using the same email that's listed on your GitHub account.


Why can't I get the store to authenticate?
  
We think this is a local storage issue, and it only seems to happen in Safari-based browsers. This includes all iOS browsers. Please see this issue for details (or to help us fix it).


Why won't my credit card work?
  
Please make sure the card isn't frozen or otherwise blocked by your financial institution. If it's not that, please send us an email to team@gatsbyjs.com if you're unable to pay with your credit card.


I've been waiting for my package, but it hasn't arrived yet.
  
International shipments can take up to 6 weeks to be delivered. 😱 Tracking updates may not always show up in real time on your tracking link. If you still have not received your order at the end of 6 weeks, please let us know by sending an email to team@gatsbyjs.com, and we'll see how we can help!


I wanted to order something but it's out. How long will it take before it's back in stock?
  
Some of the swag has been selling like hotcakes (only less tasty and a providing a bit more coverage). Once an item is out, it takes us about 3 weeks for it to get back in stock.


The delivery service is telling me I need to pay additional fees to get my order. I thought Gatsby covered shipping costs. What gives?
  
On some international orders, customs will add additional taxes, duties, and other fees. This is unpredictable, and we have no way of knowing if or when it will happen, or how much it will be. If this happens to you, there is, unfortunately, nothing else Gatsby can do. You are responsible for paying any additional fees imposed as part of the customs process. Thanks for your understanding!

",234
Dfinitski/N7DDC-ATU-100-mini-and-extended-boards,C,"The latest wirmware version from me - 2.9. Project is closed.
Sourscode is available.
Added the ability to turn off the display backlight by timer and indication of the power delivered to the antenna and the transmitter efficiency.
This sells was added:  (for mini board, see the manual)
30 – sell for setting the time of glow dysplay or its backlite, in seconds .
The backlite is glowing whilw press any buttons and if RF power comes to input.П
By default it is disabled, value 00.
31 — cell for setting of an addidional indication mode,
value 00 — for indicating L and C only.
Value 01 — for indicating the power delivered to the antenna and efficiency of fider and transmitter пwhen input power is enough for correct SWR meassuring. By default is enable, value 01.
Warning!!! The device does not take into account  its own efficiency.
32 — cell for setting a feeder power loss ratio, the first number — integer part of decibell, second number — ten’s parts of decibell. Velue by default — 1.2 (12 writen in the cell). this value uses for counting the power delivered to antenna. The loss value can be found in the reference data for the used cable or you can measure the exact value yourself.
If it is not necessary to take into account feeder losses, the value 00 should be written into the cell, then the calculations will correspond only to the mismatch losses.
N7DDC ATU-100 mini board
The easy DIY 5x5 elements 100 Watt automatic antenna tuner


N7DDC ATU-100 extended board

The flexible tunable firmware for ATU from 5x5 to 7x7 elements, up to 1500 Watt measured power
The autor's 7x7 elements 100 Watt version


3D printed housing for 0.91"" OLED display, 3 buttons and TNC RF connectors



AS example 1000 Watt ATU designed by EU2AV (VIDEO)

ATU-100 7x7 demo with M0NKA transceiver in Auto mode, LZ2GX (VIDEO)

ATU-100 5x5 Auto mode with FTdx-1200 (VIDEO)

The first video from N7DDC, 5x5 elements, ENGLISH subtitles (VIDEO)

",7
Green-Wood/hands-on-statistical-learning,Python,"hands-on-statistical-learning
从零开始的纯手工python机器学习方法实现（numpy & pandas）
已有的部分

线性回归
对数几率回归
多分类（OVO，OVR）
降维处理
决策树
感知机（pytorch神经网络）
支持向量机（smo算法）
naive贝叶斯、em算法
集成学习

将来会有的部分

聚类

应该会有的部分（咕咕咕）

数据可视化
训练可视化
分类界面可视化

",2
kubernetes-sigs/kind,Go,"


  
View The Documentation
kind is a tool for running local Kubernetes clusters using Docker container ""nodes"".
kind is primarily designed for testing Kubernetes 1.11+, initially targeting the conformance tests.
If you have go and docker installed GO111MODULE=""on"" go get -u sigs.k8s.io/kind@master && kind create cluster is all you need!

kind consists of:

Go packages implementing cluster creation, image build, etc.
A command line interface (kind) built on these packages.
Docker image(s) written to run systemd, Kubernetes, etc.
kubetest integration also built on these packages (WIP)

kind bootstraps each ""node"" with kubeadm. For more details see the design documentation.
NOTE: kind is still a work in progress, see the 1.0 roadmap.
Installation and usage
You can install the latest bleeding edge kind code with GO111MODULE=""on"" go get -u sigs.k8s.io/kind@master.
NOTE: please use the latest go to do this, ideally go 1.12.5 or greater.
This will put kind in $(go env GOPATH)/bin. If you encounter the error
kind: command not found after installation then you may need to either add that directory to your $PATH as
shown here or do manual installation by cloning the repo and run
make build from the repository.
Without installing go, kind can be built reproducibly with docker using make build.
Stable binaries are also available on the releases page. Stable releases are
generally recommended for CI usage in particular.
To install, download the binary for your platform from ""Assets"" and place this
into your $PATH. E.G. for macOS:
wget https://github.com/kubernetes-sigs/kind/releases/download/0.2.1/kind-darwin-amd64
chmod +x kind-darwin-amd64
mv kind-darwin-amd64 /some-dir-in-your-PATH/kind
To use kind, you will need to install docker.
Once you have docker running you can create a cluster with kind create cluster
To delete your cluster use kind delete cluster
To create a cluster from Kubernetes source:

ensure that Kubernetes is cloned in $(go env GOPATH)/src/k8s.io/kubernetes
build a node image and create a cluster with kind build node-image && kind create cluster --image kindest/node:latest

Multi-node clusters and other advanced features may be configured with a config
file, for more usage see the docs or run kind [command] --help
Community, discussion, contribution, and support
Please reach out for bugs, feature requests, and other issues!
The maintainers of this project are reachable via:

Kubernetes Slack in the #kind channel
filing an issue against this repo
The Kubernetes SIG-Testing Mailing List

Current maintainers are @BenTheElder and @munnerz - feel free to
reach out if you have any questions!
Pull Requests are very welcome!
See the issue tracker if you're unsure where to start, or feel free to reach out to discuss.
See also: our own contributor guide and the Kubernetes community page.
Why kind?

kind supports multi-node (including HA) clusters
kind supports building Kubernetes release builds from source

support for make / bash / docker, bazel, or installing from apt, in addition to pre-published builds.


kind is written in go, and can be used as a library, has stable releases
kind supports Windows in addition to MacOS and Linux
kind is a CNCF certified conformant Kubernetes installer

Alternatives
Some other open source projects with slightly different but overlapping use cases, features etc.

https://github.com/bsycorp/kind
https://github.com/ubuntu/microk8s
https://github.com/kinvolk/kube-spawn
https://github.com/kubernetes/minikube
https://github.com/danderson/virtuakube
https://github.com/kubernetes-sigs/kubeadm-dind-cluster

Code of conduct
Participation in the Kubernetes community is governed by the Kubernetes Code of Conduct.
",1806
zaguiini/react-native-pure-jwt,Java,"react-native-pure-jwt
A React Native library that uses native modules to work with JWTs!
react-native-pure-jwt is a library that implements the power of JWTs inside React Native!
It's goal is to sign, verify and decode JSON web tokens in order to provide a secure way to transmit authentic messages between two parties.
The difference to another libraries is that react-native-pure-jwt relies on the native realm in order to do JWT-related operations instead of the Javascript realm, so it's more stable (and works without hacks!).
Supported algorithms: HS256, HS384, HS512
React Native version required: >= 0.46.0
What's a JSON Web Token?
Don't know what a JSON Web Token is? Read on. Otherwise, jump down to the Installation section.
JWT is a means of transmitting information between two parties in a compact, verifiable form.
The bits of information encoded in the body of a JWT are called claims. The expanded form of the JWT is in a JSON format, so each claim is a key in the JSON object.
The compacted representation of a signed JWT is a string that has three parts, each separated by a .:
eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJKb2UifQ.ipevRNuRP6HflG8cFKnmUPtypruRC4fb1DWtoLL62SY

Each section is base 64 encoded. The first section is the header, which at a minimum needs to specify the algorithm used to sign the JWT. The second section is the body. This section has all the claims of this JWT encoded in it. The final section is the signature. It's computed by passing a combination of the header and body through the algorithm specified in the header.
If you pass the first two sections through a base 64 decoder, you'll get the following (formatting added for clarity):
header
{
  ""alg"": ""HS256""
}

body
{
  ""sub"": ""Joe""
}

In this case, the information we have is that the HMAC using SHA-256 algorithm was used to sign the JWT. And, the body has a single claim, sub with value Joe.
There are a number of standard claims, called Registered Claims, in the specification and sub (for subject) is one of them.
To compute the signature, you must know the secret that was used to sign it. In this case, it was the word secret. You can see the signature creation is action here (Note: Trailing = are lopped off the signature for the JWT).
Now you know (just about) all you need to know about JWTs. (Credits: jwtk/jjwt)
Installation
Install the package with:
yarn add react-native-pure-jwt
And then run:
react-native link react-native-pure-jwt
The linking process on the iOS version works with Cocoapods
Manual Android linking

in android/app/build.gradle:

dependencies {
    ...
    compile ""com.facebook.react:react-native:+""  // From node_modules
+   compile project(':react-native-pure-jwt')
}

in android/settings.gradle:

...
include ':app'
+ include ':react-native-pure-jwt'
+ project(':react-native-pure-jwt').projectDir = new File(rootProject.projectDir, '../node_modules/react-native-pure-jwt/android')

in MainApplication.java:

+ import com.zaguiini.RNPureJwt.RNPureJwtPackage;

  public class MainApplication extends Application implements ReactApplication {
    //......

    @Override
    protected List<ReactPackage> getPackages() {
      return Arrays.<ReactPackage>asList(
+         new RNPureJwtPackage(),
          new MainReactPackage()
      );
    }

    ......
  }
Manual iOS linking
You need to use Cocoapods at the moment. Open your Podfile and insert the following line in your main target:
pod 'RNPureJwt', :podspec => '../node_modules/react-native-pure-jwt/RNPureJwt.podspec'
Then run pod install and open your .xcworkspace
Usage
import jwt from ""react-native-pure-jwt"";

sign:

jwt
  .sign(
    {
      iss: ""luisfelipez@live.com"",
      exp: new Date().getTime() + 3600 * 1000, // expiration date, required, in ms, absolute to 1/1/1970
      additional: ""payload""
    }, // body
    ""my-secret"", // secret
    {
      alg: ""HS256""
    }
  )
  .then(console.log) // token as the only argument
  .catch(console.error); // possible errors

decode:

jwt
  .decode(
    token, // the token
    secret, // the secret
    {
      skipValidation: true // to skip signature and exp verification
    }
  )
  .then(console.log) // already an object. read below, exp key note
  .catch(console.error);

/*
  response example:
    {
      headers: {
        alg: 'HS256'
      },
      payload: {
        iss: 'luisfelipez@live.com',
        exp: 'some date', // IN SECONDS
      }
    }
*/

Feel free to colaborate with the project!
",29
ChrisPritchard/Xelmish,F#,"Xelmish - XNA + Elmish!
Xelmish is a small project that creates an XNA Game loop (via Mono Game) and connects it to the Elmish MVU architecure, via a custom setState method in its own version of the classic Elmish.Program module (Xelmish.Program).
In this way, you can develop games using the excellent Elmish architecture, with all the power of an XNA renderer! You can also convert existing Elmish applications to use Xelmish by rewriting their view functions.
To use Xelmish, the Elmish program must provide a view function that returns a list of 'viewables', functions which take an XNA SpriteBatch. A set of common such functions like colour, image and text are provided in the Xelmish Viewables helper module.
Xelmish is for 2D games (the SpriteBatch object is for drawing textures, not rendering vertices). Hopefully it allows users to develop such games rapidly using the Elm architecture and F#!
Update: Available on Nuget here
Simple Example of Usage
The simplest usage of Xelmish is shown in the first sample, xelmish-first. This sample renders a square to the screen, and allows you to move and resize it with key presses. It doesnt have any loaded assets like textures, fonts or sound, and therefore also doesn't require the monogame content pipeline. Nice and simple.
Once you have processed that, see the samples section below for a guide on the other, progressively more involved samples in the project.
Development Info
Xelmish was developed first with Visual Studio Community 2017, then later with Visual Studio Community 2019, on various Windows 10 machines. A Visual Studio solution file is in the root of the project if you wish to build using these IDEs. However, it should be fully compilable from the command line and other IDEs if that is your preference.
It has been built with pure dotnet core 2.2, and you will need to have this installed to compile it. Xelmish and its samples have been tested on Windows 10, Mac OSX and Ubuntu 18.
A note for Linux builders
On Linux the Monogame Content Pipeline may not work by default. If you get mono failure errors, try installing mono-complete, e.g. sudo apt install mono-complete. I was able to compile and run the samples on Ubuntu 18.04 after this without issue.
Note you also need the dotnet core 2.2 SDK to be installed on Linux in order to compile Xelmish and the samples.
Samples description
Under /samples, there are numerous projects that use Elmish and Xelmish. These are described below, in their order of complexity.
0. Xelmish-first
The most basic sample, described above. Just a coloured rectangle on the screen with move/resize commands.
1. Simple-Counter
The 'hello world' of Elmish, this sample should be almost identical (except for the Xelmish view) to other counters in other Elmish-* projects
2. Sub-Model
An app with two sub components, each containing a counter and a clock. Pretty similar to other samples in Elmish projects, but with Xelmish views
3. Tetris-Clone
The game tetris, implemented using several elmish components for screens, with a relatively simple Xelmish view. Much more involved than prior samples, but still simple enough to follow easily I hope.
4. Space-Invaders-Clone
A clone of 1979's space invaders, though not a hundred percent accurate to the old version. Compared to Tetris, Space Invaders requires a great deal more events, animations and individual entities, so it serves as a good demonstration of how the bulky (compared to direct imperative style) Elmish eventing model performs in such a context.
This is also the first sample that uses audio, with retro beeps and explosions based on game events. Sounds and music are a little complex to handle in the Elmish/Monogame structure, due to their temporal differences from textures, which makes it worth seeing a real world example.
History and Reasoning
Xelmish has been built for the 2019 F# Applied Competition, but also as a replacement architecture for my prior fsharp-gamecore experimental engine.
While I have successfully built several small games with gamecore, I was finding that as my games increased in complexity the very simplistic model/view architecture in gamecore started to get stretched and warp. Things which were view-specific started to leak into model, and vice versa.
In contrast the battle-tested Elmish model has, so far, proved a pleasure to work with. Much more elegant, and it has also achieved in a far better way my goal of having games being purely functional (where performance permits) and agnostic of engine. The MVU architecture, and parent-child relationships that the Elm architecture handles so well, mean that a game can be designed and theorised without having the engine get in the way, which is (in my opinion) ideal.
License
Xelmish is provided under the MIT license. PLease contact me if you have issue with this. In addition, many if not all of the sample projects use fonts that are provided under the SIL Open Font License, a copy of which is in the root of the solution.
",25
Green-Wood/hands-on-statistical-learning,Python,"hands-on-statistical-learning
从零开始的纯手工python机器学习方法实现（numpy & pandas）
已有的部分

线性回归
对数几率回归
多分类（OVO，OVR）
降维处理
决策树
感知机（pytorch神经网络）
支持向量机（smo算法）
naive贝叶斯、em算法
集成学习

将来会有的部分

聚类

应该会有的部分（咕咕咕）

数据可视化
训练可视化
分类界面可视化

",2
kubernetes-sigs/kind,Go,"


  
View The Documentation
kind is a tool for running local Kubernetes clusters using Docker container ""nodes"".
kind is primarily designed for testing Kubernetes 1.11+, initially targeting the conformance tests.
If you have go and docker installed GO111MODULE=""on"" go get -u sigs.k8s.io/kind@master && kind create cluster is all you need!

kind consists of:

Go packages implementing cluster creation, image build, etc.
A command line interface (kind) built on these packages.
Docker image(s) written to run systemd, Kubernetes, etc.
kubetest integration also built on these packages (WIP)

kind bootstraps each ""node"" with kubeadm. For more details see the design documentation.
NOTE: kind is still a work in progress, see the 1.0 roadmap.
Installation and usage
You can install the latest bleeding edge kind code with GO111MODULE=""on"" go get -u sigs.k8s.io/kind@master.
NOTE: please use the latest go to do this, ideally go 1.12.5 or greater.
This will put kind in $(go env GOPATH)/bin. If you encounter the error
kind: command not found after installation then you may need to either add that directory to your $PATH as
shown here or do manual installation by cloning the repo and run
make build from the repository.
Without installing go, kind can be built reproducibly with docker using make build.
Stable binaries are also available on the releases page. Stable releases are
generally recommended for CI usage in particular.
To install, download the binary for your platform from ""Assets"" and place this
into your $PATH. E.G. for macOS:
wget https://github.com/kubernetes-sigs/kind/releases/download/0.2.1/kind-darwin-amd64
chmod +x kind-darwin-amd64
mv kind-darwin-amd64 /some-dir-in-your-PATH/kind
To use kind, you will need to install docker.
Once you have docker running you can create a cluster with kind create cluster
To delete your cluster use kind delete cluster
To create a cluster from Kubernetes source:

ensure that Kubernetes is cloned in $(go env GOPATH)/src/k8s.io/kubernetes
build a node image and create a cluster with kind build node-image && kind create cluster --image kindest/node:latest

Multi-node clusters and other advanced features may be configured with a config
file, for more usage see the docs or run kind [command] --help
Community, discussion, contribution, and support
Please reach out for bugs, feature requests, and other issues!
The maintainers of this project are reachable via:

Kubernetes Slack in the #kind channel
filing an issue against this repo
The Kubernetes SIG-Testing Mailing List

Current maintainers are @BenTheElder and @munnerz - feel free to
reach out if you have any questions!
Pull Requests are very welcome!
See the issue tracker if you're unsure where to start, or feel free to reach out to discuss.
See also: our own contributor guide and the Kubernetes community page.
Why kind?

kind supports multi-node (including HA) clusters
kind supports building Kubernetes release builds from source

support for make / bash / docker, bazel, or installing from apt, in addition to pre-published builds.


kind is written in go, and can be used as a library, has stable releases
kind supports Windows in addition to MacOS and Linux
kind is a CNCF certified conformant Kubernetes installer

Alternatives
Some other open source projects with slightly different but overlapping use cases, features etc.

https://github.com/bsycorp/kind
https://github.com/ubuntu/microk8s
https://github.com/kinvolk/kube-spawn
https://github.com/kubernetes/minikube
https://github.com/danderson/virtuakube
https://github.com/kubernetes-sigs/kubeadm-dind-cluster

Code of conduct
Participation in the Kubernetes community is governed by the Kubernetes Code of Conduct.
",1806
zaguiini/react-native-pure-jwt,Java,"react-native-pure-jwt
A React Native library that uses native modules to work with JWTs!
react-native-pure-jwt is a library that implements the power of JWTs inside React Native!
It's goal is to sign, verify and decode JSON web tokens in order to provide a secure way to transmit authentic messages between two parties.
The difference to another libraries is that react-native-pure-jwt relies on the native realm in order to do JWT-related operations instead of the Javascript realm, so it's more stable (and works without hacks!).
Supported algorithms: HS256, HS384, HS512
React Native version required: >= 0.46.0
What's a JSON Web Token?
Don't know what a JSON Web Token is? Read on. Otherwise, jump down to the Installation section.
JWT is a means of transmitting information between two parties in a compact, verifiable form.
The bits of information encoded in the body of a JWT are called claims. The expanded form of the JWT is in a JSON format, so each claim is a key in the JSON object.
The compacted representation of a signed JWT is a string that has three parts, each separated by a .:
eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJKb2UifQ.ipevRNuRP6HflG8cFKnmUPtypruRC4fb1DWtoLL62SY

Each section is base 64 encoded. The first section is the header, which at a minimum needs to specify the algorithm used to sign the JWT. The second section is the body. This section has all the claims of this JWT encoded in it. The final section is the signature. It's computed by passing a combination of the header and body through the algorithm specified in the header.
If you pass the first two sections through a base 64 decoder, you'll get the following (formatting added for clarity):
header
{
  ""alg"": ""HS256""
}

body
{
  ""sub"": ""Joe""
}

In this case, the information we have is that the HMAC using SHA-256 algorithm was used to sign the JWT. And, the body has a single claim, sub with value Joe.
There are a number of standard claims, called Registered Claims, in the specification and sub (for subject) is one of them.
To compute the signature, you must know the secret that was used to sign it. In this case, it was the word secret. You can see the signature creation is action here (Note: Trailing = are lopped off the signature for the JWT).
Now you know (just about) all you need to know about JWTs. (Credits: jwtk/jjwt)
Installation
Install the package with:
yarn add react-native-pure-jwt
And then run:
react-native link react-native-pure-jwt
The linking process on the iOS version works with Cocoapods
Manual Android linking

in android/app/build.gradle:

dependencies {
    ...
    compile ""com.facebook.react:react-native:+""  // From node_modules
+   compile project(':react-native-pure-jwt')
}

in android/settings.gradle:

...
include ':app'
+ include ':react-native-pure-jwt'
+ project(':react-native-pure-jwt').projectDir = new File(rootProject.projectDir, '../node_modules/react-native-pure-jwt/android')

in MainApplication.java:

+ import com.zaguiini.RNPureJwt.RNPureJwtPackage;

  public class MainApplication extends Application implements ReactApplication {
    //......

    @Override
    protected List<ReactPackage> getPackages() {
      return Arrays.<ReactPackage>asList(
+         new RNPureJwtPackage(),
          new MainReactPackage()
      );
    }

    ......
  }
Manual iOS linking
You need to use Cocoapods at the moment. Open your Podfile and insert the following line in your main target:
pod 'RNPureJwt', :podspec => '../node_modules/react-native-pure-jwt/RNPureJwt.podspec'
Then run pod install and open your .xcworkspace
Usage
import jwt from ""react-native-pure-jwt"";

sign:

jwt
  .sign(
    {
      iss: ""luisfelipez@live.com"",
      exp: new Date().getTime() + 3600 * 1000, // expiration date, required, in ms, absolute to 1/1/1970
      additional: ""payload""
    }, // body
    ""my-secret"", // secret
    {
      alg: ""HS256""
    }
  )
  .then(console.log) // token as the only argument
  .catch(console.error); // possible errors

decode:

jwt
  .decode(
    token, // the token
    secret, // the secret
    {
      skipValidation: true // to skip signature and exp verification
    }
  )
  .then(console.log) // already an object. read below, exp key note
  .catch(console.error);

/*
  response example:
    {
      headers: {
        alg: 'HS256'
      },
      payload: {
        iss: 'luisfelipez@live.com',
        exp: 'some date', // IN SECONDS
      }
    }
*/

Feel free to colaborate with the project!
",29
ChrisPritchard/Xelmish,F#,"Xelmish - XNA + Elmish!
Xelmish is a small project that creates an XNA Game loop (via Mono Game) and connects it to the Elmish MVU architecure, via a custom setState method in its own version of the classic Elmish.Program module (Xelmish.Program).
In this way, you can develop games using the excellent Elmish architecture, with all the power of an XNA renderer! You can also convert existing Elmish applications to use Xelmish by rewriting their view functions.
To use Xelmish, the Elmish program must provide a view function that returns a list of 'viewables', functions which take an XNA SpriteBatch. A set of common such functions like colour, image and text are provided in the Xelmish Viewables helper module.
Xelmish is for 2D games (the SpriteBatch object is for drawing textures, not rendering vertices). Hopefully it allows users to develop such games rapidly using the Elm architecture and F#!
Update: Available on Nuget here
Simple Example of Usage
The simplest usage of Xelmish is shown in the first sample, xelmish-first. This sample renders a square to the screen, and allows you to move and resize it with key presses. It doesnt have any loaded assets like textures, fonts or sound, and therefore also doesn't require the monogame content pipeline. Nice and simple.
Once you have processed that, see the samples section below for a guide on the other, progressively more involved samples in the project.
Development Info
Xelmish was developed first with Visual Studio Community 2017, then later with Visual Studio Community 2019, on various Windows 10 machines. A Visual Studio solution file is in the root of the project if you wish to build using these IDEs. However, it should be fully compilable from the command line and other IDEs if that is your preference.
It has been built with pure dotnet core 2.2, and you will need to have this installed to compile it. Xelmish and its samples have been tested on Windows 10, Mac OSX and Ubuntu 18.
A note for Linux builders
On Linux the Monogame Content Pipeline may not work by default. If you get mono failure errors, try installing mono-complete, e.g. sudo apt install mono-complete. I was able to compile and run the samples on Ubuntu 18.04 after this without issue.
Note you also need the dotnet core 2.2 SDK to be installed on Linux in order to compile Xelmish and the samples.
Samples description
Under /samples, there are numerous projects that use Elmish and Xelmish. These are described below, in their order of complexity.
0. Xelmish-first
The most basic sample, described above. Just a coloured rectangle on the screen with move/resize commands.
1. Simple-Counter
The 'hello world' of Elmish, this sample should be almost identical (except for the Xelmish view) to other counters in other Elmish-* projects
2. Sub-Model
An app with two sub components, each containing a counter and a clock. Pretty similar to other samples in Elmish projects, but with Xelmish views
3. Tetris-Clone
The game tetris, implemented using several elmish components for screens, with a relatively simple Xelmish view. Much more involved than prior samples, but still simple enough to follow easily I hope.
4. Space-Invaders-Clone
A clone of 1979's space invaders, though not a hundred percent accurate to the old version. Compared to Tetris, Space Invaders requires a great deal more events, animations and individual entities, so it serves as a good demonstration of how the bulky (compared to direct imperative style) Elmish eventing model performs in such a context.
This is also the first sample that uses audio, with retro beeps and explosions based on game events. Sounds and music are a little complex to handle in the Elmish/Monogame structure, due to their temporal differences from textures, which makes it worth seeing a real world example.
History and Reasoning
Xelmish has been built for the 2019 F# Applied Competition, but also as a replacement architecture for my prior fsharp-gamecore experimental engine.
While I have successfully built several small games with gamecore, I was finding that as my games increased in complexity the very simplistic model/view architecture in gamecore started to get stretched and warp. Things which were view-specific started to leak into model, and vice versa.
In contrast the battle-tested Elmish model has, so far, proved a pleasure to work with. Much more elegant, and it has also achieved in a far better way my goal of having games being purely functional (where performance permits) and agnostic of engine. The MVU architecture, and parent-child relationships that the Elm architecture handles so well, mean that a game can be designed and theorised without having the engine get in the way, which is (in my opinion) ideal.
License
Xelmish is provided under the MIT license. PLease contact me if you have issue with this. In addition, many if not all of the sample projects use fonts that are provided under the SIL Open Font License, a copy of which is in the root of the solution.
",25
ryanfleury/data_desk,C,"Data Desk
Description
Data Desk is a project utility that parses a simple C-like data description format. Input files in this data description format are parsed to create corresponding abstract syntax trees which represent the information extracted from the files. These abstract syntax trees are then sent to project-specific custom code that is written by the user. This custom code is simply a dynamic library with a few exported functions that are used as callbacks for the parser. Below is a list of the callbacks.

DataDeskCustomInitCallback(void) is called when the parser starts.
DataDeskCustomFileCallback(DataDeskASTNode *root, char *filename) is called when the parser finishes parsing a file.
DataDeskCustomConstantCallback(DataDeskConstant constant_info, char *filename) is called for every constant definition that is parsed.
DataDeskCustomStructCallback(DataDeskStruct struct_info, char *filename) is called for every structure that is parsed.
DataDeskCustomDeclarationCallback(DataDeskDeclaration declaration, char *filename) is called for every declaration that is parsed.
DataDeskCustomCleanUpCallback(void) is called before the parser shuts down.

The abstract syntax tree is formed completely by DataDeskASTNode structures. This structure can be found in the data_desk.h file.
Data Desk also offers a number of utility functions for introspecting on abstract syntax trees it passes to your custom code. A list of these is in the data_desk.h file, which can be included into your custom layer.
Usage
To use Data Desk, you'll need to do a few things:

Get Data Desk
Make or get some Data Desk format files (.ds)
Make a project-specific custom layer

Step 1: Get Data Desk

Run the command git clone https://github.com/ryanfleury/data_desk
cd data_desk
build on Windows or ./build.sh on Mac/Linux

NOTE: The build command on Windows expects to find cl (MSVC). Your environment should know about this. The easiest way to do this is to call vcvarsall.bat in your terminal environment, which is packaged with Visual Studio.
Step 2: Make or get Data Desk format files (.ds)
Grab an example or make your own.
Step 3: Make a project-specific custom layer


An easy way to write the code for this is to check out the custom layer template, located here. Fill out the functions in your custom layer code however you want to. There are some helper functions available in data_desk.h that might be useful for you here. This can be dropped into your code and used.


To build a custom layer, you just need to build a DLL with the function callbacks you've written as the appropriate exported symbols. data_desk.h outlines what symbols are used for each callback.


Step 4: Run Data Desk
To run Data Desk with your custom layer, you can use the following command template:
data_desk --custom /path/to/custom/layer /file/to/parse/1 /file/to/parse/2 ...
Data Desk (.ds) File Documentation
A valid Data Desk file is defined as a set of zero or more Declarations, Structs, Enums, Flagss, Comments, or Consts. Each of the following sections defines these (and what they are comprised of).

Identifiers
Numeric Constants
String Constants
Character Constants
Binary Operators
Expressions
Types
Declarations
Structs
Enums
Flags
Constant Expressions
Comments
Tags

Identifiers
Identifiers are defined as a sequence of characters that begin with either an underscore or an alphabetic character, and contain numeric characters, alphabetic characters, or underscores (similar to C).
Numeric Constants
Numeric constants (Numbers) are defined as a sequence of characters that begin with a numeric character, and contain only numeric characters, periods, or alphabetic characters.
NOTE: Data Desk does not guarantee the correctness as defined by programming languages of your numeric constants. For example, the following will be interpreted by Data Desk as a numeric constant: 1.2.3.a.b.c. Because Data Desk does not do any evaluation of numeric constants, it will not enforce validity of numeric constants.
String Constants
String constants (Strings) can be single-line or multi-line.
A single-line string constant is defined similarly to those in C. It begins with a double-quote character, and ends with a non-escaped double-quote character. Double-quote characters can be escaped with a backslash.
A multi-line string constant is defined as beginning with three double-quote characters (""""""), and ending with three double-quote characters ("""""").
Character Constants
Character constants (Chars) are defined almost identically to single-line string constants, but with single-quote beginning and ending characters instead of double-quote characters.
Binary Operators
Data Desk defines a subset of the binary operators found in C. It does not define shorthand assignment operators, like += or >>=. The following binary operators are defined (in order of ascending precedence):

+: Addition
-: Subtraction
*: Multiplication
/: Division
%: Modulus
<<: Left Bitshift
>>: Right Bitshift
&: Bitwise And
|: Bitwise Or
&&: Boolean And
||: Boolean Or

Expressions
An expression (Expr) in Data Desk is defined as being one of the following:

Identifier
Number
String
Char
Expr Binary Operator Expr

Types
Types are used in declarations. They are defined as being the following:

A group of 0 or more * characters, representing the number of layers of indirection
A type name, which can be:

Some Identifier referring to a type name
A Struct definition (refer to next section)


A group of 0 or more array size specifiers, being defined as a [ character, some Expression, and a ] character

Declarations
Declarations are defined as follows:
Identifier : Type ;
Structs
Structs are groups of zero or more declarations. They are defined as:
struct Identifier
{
Zero or more Declarations
}
Enums
Enums are groups of one or more identifiers. They are defined as:
enum Identifier
{
One or more Identifiers, each followed by , characters.
}
When transpiled to C, these will be defined as a normal C enum; that is, the first one will be defined as a constant that evaluates to 0, the next to 1, and so on.
Flags
Flagss are groups of one or more identifiers. They are defined as:
flags Identifier
{
One or more Identifiers, each followed by , characters.
}
When transpiled to C, these will be defined as several C preprocessor macros that evaluate to unique bits inside of an integral value. These are similar to Enums, but their purpose is to define unique bits instead of unique integral values for a set of constants.
Constant Expressions
Constant expressions (Consts) are defined as:
Identifier :: Expression ;
Comments
Comments are ignored by the parser. They can be single-line or multi-line.
Single-line comments can be defined with two / characters. They are terminated by a newline character.
Multi-line comments can be defined with a /* pattern. They are terminated by a */ pattern. They can also be nested. For example, if there exists the pattern /*/*, it will require */*/ to terminate.
Tags
Structs, Declarations (including those within Structs), or Consts can be preceded with one or more Tags. A Tag is defined as beginning with a @ character, and ending with whitespace. These are used to annotate meta-information about various things. They will be passed to custom-layer code.
",8
chenBright/leetcode,C++,"leetcode
LeetCode Problems' Solutions

001-两数之和*
003-无重复字符的最长子串*
005-最长回文子串*
007-整数反转*
009-回文数*
011-盛最多水的容器
013-罗马数字转整数
014-最长公共前缀*（TODO：字典树）
015-三数之和*
016-最接近的三数之和
018-四数之和*
020-有效的括号
021-合并两个有序链表
026-删除排序数组中的重复项
027-移除元素
028-实现strStr()*（TODO：KMP）
031-下一个排列*
033-搜索旋转排序数组*
034-在排序数组中查找元素的第一个和最后一个位置
035-搜索插入位置
038-报数
039-组合总和
040-组合总和 II*
048-旋转图像*
053-最大子序和
054-螺旋矩阵
055-跳跃游戏*
056-合并区间*
058-最后一个单词的长度
059-螺旋矩阵 II
061-旋转链表
062-不同路径*
063-不同路径 II*
064-最小路径和*（最短路径算法）
066-加一
067-二进制求和*
069-x的平方根*（牛顿迭代法）
070-爬楼梯*
071-简化路径*
073-矩阵置零
074-搜索二维矩阵*
075-颜色分类*
078-子集*（迭代、位运算）
079-单词搜索
080-删除排序数组中的重复项 II
081-搜索旋转排序数组 II*
083-删除排序链表中的重复元素
088-合并两个有序数组
145-二叉树的后序遍历*
091-解码方法*
094-二叉树的中序遍历
095- 不同的二叉搜索树 II*
096-不同的二叉搜索树*
098-验证二叉搜索树
100-相同的树
100-相同的树*（迭代）
102-二叉树的层次遍历
103-二叉树的锯齿形层次遍历
104-二叉树的最大深度
107-二叉树的层次遍历 II
108-将有序数组转换为二叉搜索树
110-平衡二叉树
111-二叉树的最小深度
118-杨辉三角
119-杨辉三角 II
120-三角形最小路径和*
121-买卖股票的最佳时机*（动态规划）
122-买卖股票的最佳时机 II*（贪心算法）
123- 买卖股票的最佳时机 III*
125-验证回文串
136-只出现一次的数字
139-单词拆分*
141-环形链表
144-二叉树的前序遍历
145-二叉树的后序遍历*
147-对链表进行插入排序
148-排序链表*
150-逆波兰表达式求值
152-乘积最大子序列*
153-寻找旋转排序数组中的最小值*
155-最小栈
160-相交链表*（栈、哈希表、其他）
162-寻找峰值*（简介的实现）
167-两数之和 II - 输入有序数组
168-Excel表列名称*
169-求众数*
171-Excel表列序号
173-二叉搜索树迭代器
172-阶乘后的零*
179-最大数*
188-买卖股票的最佳时机 IV*
189-旋转数组*
190-颠倒二进制位*
191-位1的个数*
198-打家劫舍*
202-快乐数
209-长度最小的子数组*
213-打家劫舍 II*
215-数组中的第K个最大元素*
216-组合总和 III*
221-最大正方形*
263-丑数
264-丑数 II*
279-完全平方数*（不会做）
303-区域和检索 - 数组不可变*
764-使用最小花费爬楼梯*

",2
zapret-info/z-i,None,"z-i
Register of Internet Addresses filtered in Russian Federation
",929
changSic/Task,Swift,"동료들 깃허브 주소


이봉원 강사님


창근님


현철님


커리큘럼


과제 기록하기 google drive


changsic.github.io


유용한 정보


구글 검색 특징 - 검색에 [keyword]로 제약조건 걸기.


Swift 로 작성된 iOS Resources IOS Example


앱 아이콘 만들기 makeappicon


주니어 개발자 채용정보



FastCampus iOS SCHOOL
week 5 ( 4월 2일 합류 )

week 5 - 3 (4.2 화)

Assignment


week 5 - 4 (4.4 목)

Assignment



week 6

AutoLayout (4.10 화)
Initializer
ScrollView
TableView
ARC
Instagram 만들기 프로젝트

Week7

Higher Ordering Function
Calculator 만들기 프로젝트

Week8

2차 테스트
이론 , 오토레이아웃, 도미노 피자 앱
Notification Center
Error handling
Data Structure
Algorithms

Week9

ImagePicker
Gesture
touch
Gesture Recognizer

",2
pytorch/pytorch,C++,"

PyTorch is a Python package that provides two high-level features:

Tensor computation (like NumPy) with strong GPU acceleration
Deep neural networks built on a tape-based autograd system

You can reuse your favorite Python packages such as NumPy, SciPy and Cython to extend PyTorch when needed.

More about PyTorch
Installation

Binaries
From Source
Docker Image
Building the Documentation
Previous Versions


Getting Started
Communication
Releases and Contributing
The Team




System
2.7
3.5
3.6




Linux CPU


—


Linux GPU


—


Windows CPU / GPU
—

—


Linux (ppc64le) CPU

—



Linux (ppc64le) GPU

—




See also the ci.pytorch.org HUD.
More About PyTorch
At a granular level, PyTorch is a library that consists of the following components:



Component
Description




torch
a Tensor library like NumPy, with strong GPU support


torch.autograd
a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch


torch.jit
a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code


torch.nn
a neural networks library deeply integrated with autograd designed for maximum flexibility


torch.multiprocessing
Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training


torch.utils
DataLoader and other utility functions for convenience



Usually one uses PyTorch either as:

a replacement for NumPy to use the power of GPUs.
a deep learning research platform that provides maximum flexibility and speed.

Elaborating further:
A GPU-Ready Tensor Library
If you use NumPy, then you have used Tensors (a.k.a ndarray).

PyTorch provides Tensors that can live either on the CPU or the GPU, and accelerates the
computation by a huge amount.
We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs
such as slicing, indexing, math operations, linear algebra, reductions.
And they are fast!
Dynamic Neural Networks: Tape-Based Autograd
PyTorch has a unique way of building neural networks: using and replaying a tape recorder.
Most frameworks such as TensorFlow, Theano, Caffe and CNTK have a static view of the world.
One has to build a neural network, and reuse the same structure again and again.
Changing the way the network behaves means that one has to start from scratch.
With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to
change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes
from several research papers on this topic, as well as current and past work such as
torch-autograd,
autograd,
Chainer, etc.
While this technique is not unique to PyTorch, it's one of the fastest implementations of it to date.
You get the best of speed and flexibility for your crazy research.

Python First
PyTorch is not a Python binding into a monolithic C++ framework.
It is built to be deeply integrated into Python.
You can use it naturally like you would use NumPy / SciPy / scikit-learn etc.
You can write your new neural network layers in Python itself, using your favorite libraries
and use packages such as Cython and Numba.
Our goal is to not reinvent the wheel where appropriate.
Imperative Experiences
PyTorch is designed to be intuitive, linear in thought and easy to use.
When you execute a line of code, it gets executed. There isn't an asynchronous view of the world.
When you drop into a debugger, or receive error messages and stack traces, understanding them is straightforward.
The stack trace points to exactly where your code was defined.
We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.
Fast and Lean
PyTorch has minimal framework overhead. We integrate acceleration libraries
such as Intel MKL and NVIDIA (cuDNN, NCCL) to maximize speed.
At the core, its CPU and GPU Tensor and neural network backends
(TH, THC, THNN, THCUNN) are mature and have been tested for years.
Hence, PyTorch is quite fast – whether you run small or large neural networks.
The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.
We've written custom memory allocators for the GPU to make sure that
your deep learning models are maximally memory efficient.
This enables you to train bigger deep learning models than before.
Extensions Without Pain
Writing new neural network modules, or interfacing with PyTorch's Tensor API was designed to be straightforward
and with minimal abstractions.
You can write new neural network layers in Python using the torch API
or your favorite NumPy-based libraries such as SciPy.
If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.
There is no wrapper code that needs to be written. You can see a tutorial here and an example here.
Installation
Binaries
Commands to install from binaries via Conda or pip wheels are on our website:
https://pytorch.org
NVIDIA Jetson platforms
Python wheels for NVIDIA's Jetson Nano, Jetson TX2, and Jetson AGX Xavier are available via the following URLs:

Stable binaries:

Python 2.7: https://nvidia.box.com/v/torch-stable-cp27-jetson-jp42
Python 3.6: https://nvidia.box.com/v/torch-stable-cp36-jetson-jp42


Rolling weekly binaries:

Python 2.7: https://nvidia.box.com/v/torch-weekly-cp27-jetson-jp42
Python 3.6: https://nvidia.box.com/v/torch-weekly-cp36-jetson-jp42



They requires JetPack 4.2 and above and are maintained by @dusty-nv
From Source
If you are installing from source, we highly recommend installing an Anaconda environment.
You will get a high-quality BLAS library (MKL) and you get a controlled compiler version regardless of your Linux distro.
Once you have Anaconda installed, here are the instructions.
If you want to compile with CUDA support, install

NVIDIA CUDA 7.5 or above
NVIDIA cuDNN v6.x or above

If you want to disable CUDA support, export environment variable NO_CUDA=1.
Other potentially useful environment variables may be found in setup.py.
If you are building for NVIDIA's Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to are available here
Install Dependencies
Common
conda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing

On Linux
# Add LAPACK support for the GPU if needed
conda install -c pytorch magma-cuda90 # or [magma-cuda80 | magma-cuda92 | magma-cuda100 ] depending on your cuda version
Get the PyTorch Source
git clone --recursive https://github.com/pytorch/pytorch
cd pytorch
# if you are updating an existing checkout
git submodule sync 
git submodule update --init --recursive
Install PyTorch
On Linux
export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
python setup.py install
On macOS
export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install
On Windows
At least Visual Studio 2017 Update 3 (version 15.3.3 with the toolset 14.11) and NVTX are needed.
If the version of Visual Studio 2017 is higher than 15.4.5, installing of ""VC++ 2017 version 15.4 v14.11 toolset"" is strongly recommended.
 If the version of Visual Studio 2017 is lesser than 15.3.3, please update Visual Studio 2017 to the latest version along with installing ""VC++ 2017 version 15.4 v14.11 toolset"".
 There is no guarantee of the correct building with VC++ 2017 toolsets, others than version 15.4 v14.11.
 ""VC++ 2017 version 15.4 v14.11 toolset"" might be installed onto already installed Visual Studio 2017 by running its installation once again and checking the corresponding checkbox under ""Individual components""/""Compilers, build tools, and runtimes"".
For building against CUDA 8.0 Visual Studio 2015 Update 3 (version 14.0), and the patch are needed to be installed too.
The details of the patch can be found here.
NVTX is a part of CUDA distributive, where it is called ""Nsight Compute"". For installing it onto already installed CUDA run CUDA installation once again and check the corresponding checkbox.
Be sure that CUDA with Nsight Compute is installed after Visual Studio 2017.
cmd
REM [Optional] The following two lines are needed for Python 2.7, but the support for it is very experimental.
set MSSdk=1
set FORCE_PY27_BUILD=1

REM [Optional] As for CUDA 8, VS2015 Update 3 is required; use the following line.
set ""CUDAHOSTCXX=%VS140COMNTOOLS%..\..\VC\bin\amd64\cl.exe""

set CMAKE_GENERATOR=Visual Studio 15 2017 Win64
set DISTUTILS_USE_SDK=1

REM Run ""Visual Studio 2017 Developer Command Prompt""
for /f ""usebackq tokens=*"" %i in (`""%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe"" -version [15^,16^) -products * -latest -property installationPath`) do call ""%i\VC\Auxiliary\Build\vcvarsall.bat"" x64 -vcvars_ver=14.11

python setup.py install

Docker Image
Dockerfile is supplied to build images with cuda support and cudnn v7. You can pass -e PYTHON_VERSION=x.y flag to specify which Python version is to be used by Miniconda, or leave it unset to use the default. Build from pytorch repo directory as docker needs to copy git repo into docker filesystem while building the image.
docker build -t pytorch -f docker/pytorch/Dockerfile .

You can also pull a pre-built docker image from Docker Hub and run with nvidia-docker,
but this is not currently maintained and will pull PyTorch 0.2.
nvidia-docker run --rm -ti --ipc=host pytorch/pytorch:latest

Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.
for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you
should increase shared memory size either with --ipc=host or --shm-size command line options to nvidia-docker run.
Building the Documentation
To build documentation in various formats, you will need Sphinx and the
readthedocs theme.
cd docs/
pip install -r requirements.txt

You can then build the documentation by running make <format> from the
docs/ folder. Run make to get a list of all available output formats.
Previous Versions
Installation instructions and binaries for previous PyTorch versions may be found
on our website.
Getting Started
Three pointers to get you started:

Tutorials: get you started with understanding and using PyTorch
Examples: easy to understand pytorch code across all domains
The API Reference

Communication

forums: discuss implementations, research, etc. https://discuss.pytorch.org
GitHub issues: bug reports, feature requests, install issues, RFCs, thoughts, etc.
Slack: The PyTorch Slack hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration etc. If you are a beginner looking for help, the primary medium is PyTorch Forums. If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1
newsletter: no-noise, one-way email newsletter with important announcements about pytorch. You can sign-up here: https://eepurl.com/cbG0rv

Releases and Contributing
PyTorch has a 90 day release cycle (major releases). Please let us know if you encounter a bug by filing an issue.
We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.
If you plan to contribute new features, utility functions or extensions to the core, please first open an issue and discuss the feature with us.
Sending a PR without discussion might end up resulting in a rejected PR, because we might be taking the core in a different direction than you might be aware of.
The Team
PyTorch is a community driven project with several skillful engineers and researchers contributing to it.
PyTorch is currently maintained by Adam Paszke, Sam Gross, Soumith Chintala and Gregory Chanan with major contributions coming from hundreds of talented individuals in various forms and means.
A non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Kopf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito.
Note: this project is unrelated to hughperkins/pytorch with the same name. Hugh is a valuable contributor in the Torch community and has helped with many things Torch and PyTorch.
License
PyTorch is BSD-style licensed, as found in the LICENSE file.
",28097
jspm/jspm-cli,TypeScript,"jspm 2.0 CLI
   
ES module package manager.
See https://jspm.org for a project overview and guide.
For previous releases, see the jspm 0.16 and jspm 0.17 branches.
For support, join the Gitter room or Google Group.
Use jspm help to see the full up-to-date list of commands.
If you are interested in contributing to the project, please read the contributors' guide.
For a list of community projects and tools, see the Integrations Page.
Documentation
Full API documentation for the 2.0 release is still in progress.
License
Apache 2.0
",3701
pytorch/pytorch,C++,"

PyTorch is a Python package that provides two high-level features:

Tensor computation (like NumPy) with strong GPU acceleration
Deep neural networks built on a tape-based autograd system

You can reuse your favorite Python packages such as NumPy, SciPy and Cython to extend PyTorch when needed.

More about PyTorch
Installation

Binaries
From Source
Docker Image
Building the Documentation
Previous Versions


Getting Started
Communication
Releases and Contributing
The Team




System
2.7
3.5
3.6




Linux CPU


—


Linux GPU


—


Windows CPU / GPU
—

—


Linux (ppc64le) CPU

—



Linux (ppc64le) GPU

—




See also the ci.pytorch.org HUD.
More About PyTorch
At a granular level, PyTorch is a library that consists of the following components:



Component
Description




torch
a Tensor library like NumPy, with strong GPU support


torch.autograd
a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch


torch.jit
a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code


torch.nn
a neural networks library deeply integrated with autograd designed for maximum flexibility


torch.multiprocessing
Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training


torch.utils
DataLoader and other utility functions for convenience



Usually one uses PyTorch either as:

a replacement for NumPy to use the power of GPUs.
a deep learning research platform that provides maximum flexibility and speed.

Elaborating further:
A GPU-Ready Tensor Library
If you use NumPy, then you have used Tensors (a.k.a ndarray).

PyTorch provides Tensors that can live either on the CPU or the GPU, and accelerates the
computation by a huge amount.
We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs
such as slicing, indexing, math operations, linear algebra, reductions.
And they are fast!
Dynamic Neural Networks: Tape-Based Autograd
PyTorch has a unique way of building neural networks: using and replaying a tape recorder.
Most frameworks such as TensorFlow, Theano, Caffe and CNTK have a static view of the world.
One has to build a neural network, and reuse the same structure again and again.
Changing the way the network behaves means that one has to start from scratch.
With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to
change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes
from several research papers on this topic, as well as current and past work such as
torch-autograd,
autograd,
Chainer, etc.
While this technique is not unique to PyTorch, it's one of the fastest implementations of it to date.
You get the best of speed and flexibility for your crazy research.

Python First
PyTorch is not a Python binding into a monolithic C++ framework.
It is built to be deeply integrated into Python.
You can use it naturally like you would use NumPy / SciPy / scikit-learn etc.
You can write your new neural network layers in Python itself, using your favorite libraries
and use packages such as Cython and Numba.
Our goal is to not reinvent the wheel where appropriate.
Imperative Experiences
PyTorch is designed to be intuitive, linear in thought and easy to use.
When you execute a line of code, it gets executed. There isn't an asynchronous view of the world.
When you drop into a debugger, or receive error messages and stack traces, understanding them is straightforward.
The stack trace points to exactly where your code was defined.
We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.
Fast and Lean
PyTorch has minimal framework overhead. We integrate acceleration libraries
such as Intel MKL and NVIDIA (cuDNN, NCCL) to maximize speed.
At the core, its CPU and GPU Tensor and neural network backends
(TH, THC, THNN, THCUNN) are mature and have been tested for years.
Hence, PyTorch is quite fast – whether you run small or large neural networks.
The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.
We've written custom memory allocators for the GPU to make sure that
your deep learning models are maximally memory efficient.
This enables you to train bigger deep learning models than before.
Extensions Without Pain
Writing new neural network modules, or interfacing with PyTorch's Tensor API was designed to be straightforward
and with minimal abstractions.
You can write new neural network layers in Python using the torch API
or your favorite NumPy-based libraries such as SciPy.
If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.
There is no wrapper code that needs to be written. You can see a tutorial here and an example here.
Installation
Binaries
Commands to install from binaries via Conda or pip wheels are on our website:
https://pytorch.org
NVIDIA Jetson platforms
Python wheels for NVIDIA's Jetson Nano, Jetson TX2, and Jetson AGX Xavier are available via the following URLs:

Stable binaries:

Python 2.7: https://nvidia.box.com/v/torch-stable-cp27-jetson-jp42
Python 3.6: https://nvidia.box.com/v/torch-stable-cp36-jetson-jp42


Rolling weekly binaries:

Python 2.7: https://nvidia.box.com/v/torch-weekly-cp27-jetson-jp42
Python 3.6: https://nvidia.box.com/v/torch-weekly-cp36-jetson-jp42



They requires JetPack 4.2 and above and are maintained by @dusty-nv
From Source
If you are installing from source, we highly recommend installing an Anaconda environment.
You will get a high-quality BLAS library (MKL) and you get a controlled compiler version regardless of your Linux distro.
Once you have Anaconda installed, here are the instructions.
If you want to compile with CUDA support, install

NVIDIA CUDA 7.5 or above
NVIDIA cuDNN v6.x or above

If you want to disable CUDA support, export environment variable NO_CUDA=1.
Other potentially useful environment variables may be found in setup.py.
If you are building for NVIDIA's Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to are available here
Install Dependencies
Common
conda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing

On Linux
# Add LAPACK support for the GPU if needed
conda install -c pytorch magma-cuda90 # or [magma-cuda80 | magma-cuda92 | magma-cuda100 ] depending on your cuda version
Get the PyTorch Source
git clone --recursive https://github.com/pytorch/pytorch
cd pytorch
# if you are updating an existing checkout
git submodule sync 
git submodule update --init --recursive
Install PyTorch
On Linux
export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
python setup.py install
On macOS
export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install
On Windows
At least Visual Studio 2017 Update 3 (version 15.3.3 with the toolset 14.11) and NVTX are needed.
If the version of Visual Studio 2017 is higher than 15.4.5, installing of ""VC++ 2017 version 15.4 v14.11 toolset"" is strongly recommended.
 If the version of Visual Studio 2017 is lesser than 15.3.3, please update Visual Studio 2017 to the latest version along with installing ""VC++ 2017 version 15.4 v14.11 toolset"".
 There is no guarantee of the correct building with VC++ 2017 toolsets, others than version 15.4 v14.11.
 ""VC++ 2017 version 15.4 v14.11 toolset"" might be installed onto already installed Visual Studio 2017 by running its installation once again and checking the corresponding checkbox under ""Individual components""/""Compilers, build tools, and runtimes"".
For building against CUDA 8.0 Visual Studio 2015 Update 3 (version 14.0), and the patch are needed to be installed too.
The details of the patch can be found here.
NVTX is a part of CUDA distributive, where it is called ""Nsight Compute"". For installing it onto already installed CUDA run CUDA installation once again and check the corresponding checkbox.
Be sure that CUDA with Nsight Compute is installed after Visual Studio 2017.
cmd
REM [Optional] The following two lines are needed for Python 2.7, but the support for it is very experimental.
set MSSdk=1
set FORCE_PY27_BUILD=1

REM [Optional] As for CUDA 8, VS2015 Update 3 is required; use the following line.
set ""CUDAHOSTCXX=%VS140COMNTOOLS%..\..\VC\bin\amd64\cl.exe""

set CMAKE_GENERATOR=Visual Studio 15 2017 Win64
set DISTUTILS_USE_SDK=1

REM Run ""Visual Studio 2017 Developer Command Prompt""
for /f ""usebackq tokens=*"" %i in (`""%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe"" -version [15^,16^) -products * -latest -property installationPath`) do call ""%i\VC\Auxiliary\Build\vcvarsall.bat"" x64 -vcvars_ver=14.11

python setup.py install

Docker Image
Dockerfile is supplied to build images with cuda support and cudnn v7. You can pass -e PYTHON_VERSION=x.y flag to specify which Python version is to be used by Miniconda, or leave it unset to use the default. Build from pytorch repo directory as docker needs to copy git repo into docker filesystem while building the image.
docker build -t pytorch -f docker/pytorch/Dockerfile .

You can also pull a pre-built docker image from Docker Hub and run with nvidia-docker,
but this is not currently maintained and will pull PyTorch 0.2.
nvidia-docker run --rm -ti --ipc=host pytorch/pytorch:latest

Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.
for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you
should increase shared memory size either with --ipc=host or --shm-size command line options to nvidia-docker run.
Building the Documentation
To build documentation in various formats, you will need Sphinx and the
readthedocs theme.
cd docs/
pip install -r requirements.txt

You can then build the documentation by running make <format> from the
docs/ folder. Run make to get a list of all available output formats.
Previous Versions
Installation instructions and binaries for previous PyTorch versions may be found
on our website.
Getting Started
Three pointers to get you started:

Tutorials: get you started with understanding and using PyTorch
Examples: easy to understand pytorch code across all domains
The API Reference

Communication

forums: discuss implementations, research, etc. https://discuss.pytorch.org
GitHub issues: bug reports, feature requests, install issues, RFCs, thoughts, etc.
Slack: The PyTorch Slack hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration etc. If you are a beginner looking for help, the primary medium is PyTorch Forums. If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1
newsletter: no-noise, one-way email newsletter with important announcements about pytorch. You can sign-up here: https://eepurl.com/cbG0rv

Releases and Contributing
PyTorch has a 90 day release cycle (major releases). Please let us know if you encounter a bug by filing an issue.
We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.
If you plan to contribute new features, utility functions or extensions to the core, please first open an issue and discuss the feature with us.
Sending a PR without discussion might end up resulting in a rejected PR, because we might be taking the core in a different direction than you might be aware of.
The Team
PyTorch is a community driven project with several skillful engineers and researchers contributing to it.
PyTorch is currently maintained by Adam Paszke, Sam Gross, Soumith Chintala and Gregory Chanan with major contributions coming from hundreds of talented individuals in various forms and means.
A non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Kopf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito.
Note: this project is unrelated to hughperkins/pytorch with the same name. Hugh is a valuable contributor in the Torch community and has helped with many things Torch and PyTorch.
License
PyTorch is BSD-style licensed, as found in the LICENSE file.
",28097
jspm/jspm-cli,TypeScript,"jspm 2.0 CLI
   
ES module package manager.
See https://jspm.org for a project overview and guide.
For previous releases, see the jspm 0.16 and jspm 0.17 branches.
For support, join the Gitter room or Google Group.
Use jspm help to see the full up-to-date list of commands.
If you are interested in contributing to the project, please read the contributors' guide.
For a list of community projects and tools, see the Integrations Page.
Documentation
Full API documentation for the 2.0 release is still in progress.
License
Apache 2.0
",3701
edavis10/redmine,Ruby,"
Redmine
Redmine is a flexible project management web application written using Ruby
on Rails framework.
More details can be found in the doc directory or on the official website
www.redmine.org
",2427
dpc10ster/rjafroc-master,R,"RJafroc-master



What is this repository for?

Modeling, Analysis, Validation and Visualization of ROC/FROC studies
Extends and replaces obsolete Windows version of software (JAFROC: http://www.devchakraborty.com)

Preliminary documentation (vignettes) available at https://dpc10ster.github.io/rjafroc-master/
RJafroc is longer distributed via CRAN - please ignore version 1.1.0 at https://github.com/cran/RJafroc

The C++ code in the RJafroc package was written by a student (Xuetong Zhai) who is no longer working for me. Unfortunately, the code does not pass the Solaris compiler. I do not have a Solaris machine to test my fixes, none of my users employ Solaris, even books on compiled code in R packages stay away from Solaris, and I am not proficient in C++. Long story short, the CRAN version was removed and there is not much I can do about it (keeping up with rarely used compilers is not a good use of my time; replacing the C++ code with R code would considerably slow down the algorithms, rendering them almost useless). Hence the decision to distribute the code directly using GitHub.
Version 1.1.0 at https://github.com/cran/RJafroc is obsolete

Those already familiar with installing R packages from GitHub can ignore the following directions
How do I get set up?
Short version: install directly from GitHub using package devtools

Install R and RStudio.
Create an empty directory, e.g., myProject. In my computer it is /Users/Dev/Downloads/myProject.
Open RStudio.
Starting from RStudio > File > New Project > Existing Directory > Select myProject > Create Project.
Oila! You should see myProject.RProj in the Files menu.
Install the devtools package as shown below:
Starting from RStudio > Packages > Install > devtools.
Load devtools as shown below:

library(devtools)


Install RJafroc directly from GitHub (this is where devtools is used):

install_github(""dpc10ster/rjafroc-master"")


Hit Enter on any prompts...
Lots of activity and compilation of C++ code ....
Load RJafroc as shown below:

library(RJafroc)


Test the installation:

cbmPlot <- PlotCbmFit(c(1, 2), c(0.5, 0.5))
print(cbmPlot)


You should see two ROC plots in the Plots window.
Preliminary documentation (vignettes) is available at https://dpc10ster.github.io/rjafroc-master/.
Be sure to study these examples and make full use of the online documentation.
Put your data and other files, if any, in myProject.
TBA

Long version: download the RJafroc package and install from the downloaded files

Clone this repository to a directory anywhere on your computer. On my computer it is in /Users/Dev/Downloads/rjafroc. Rename the folder if necessary to match my example.
I find the GitHub desktop app useful in mananging my downloads/uploads from Git.
Install R and RStudio.
Navigate to the rjafroc directory.
Open RJafroc.Rproj. This will open RStudio.
Navigate to File menu (lower-right window) and click on DESCRIPTION file.
Install all packages listed under Imports, e.g.,
openxlsx,
ggplot2,
stringr,
tools,
utils,
stats,
bbmle,
binom,
mvtnorm,
dplyr,
numDeriv,
Rcpp
For example, to install the first two above-listed packages, use the following command at the Console prompt:

install.packages(c(""openxlsx"", ""ggplot2""))


Click on Build > Install and Restart (upper right panel). If errors result from missing packages, install those packages.
A successful Install and Restart will result in the following line in the Console window:

library(RJafroc)


Thats it! RJafroc has been installed to your computer and is visible to any other R project in any directory.
You will not need to access the RJafroc folder again (unless you reinstall a new version of the software).
All necessary files of the installation are in a hidden directory that you do not normally need to worry about.
Create an empty directory, e.g., myProject. In my computer it is /Users/Dev/Downloads/myProject.
Starting from RStudio > File > New Project > Existing Directory > myProject > Create Project.
Oila! You should see myProject.RProj in the Files menu.
Click on Packages and scroll down to find RJafroc, and check the box next to it. This results in RJafroc being loaded to the current workspace. The following line appears in the Console window (this is the hidden directory referrred to above).

  library(""RJafroc"", lib.loc=""/Library/Frameworks/R.framework/Versions/3.5/Resources/library"")


Click on RJafroc in the packages window. A help window opens up. I find it convenient to put this in its own window by clicking the ""out"" arrow button (hover message: Show in new window). You can access all documentation from here.
Test the installation:

cbmPlot <- PlotCbmFit(c(1, 2), c(0.5, 0.5))
print(cbmPlot)


You should see two ROC plots in the Plots window.
Preliminary documentation (vignettes) is available at https://dpc10ster.github.io/rjafroc-master/.
Put your data and other files, if any, in myProject.
TBA

Contribution guidelines

Writing tests
TBA
Code review
TBA
Other guidelines
TBA

Who do I talk to?
dpc10ster@gmail.com
",2
redmine/redmine,Ruby,"
Redmine
Redmine is a flexible project management web application written using Ruby
on Rails framework.
More details can be found in the doc directory or on the official website
www.redmine.org
",3349
kubesphere/kubesphere,Go,"KubeSphere



What is KubeSphere
KubeSphere is an enterprise-grade multi-tenant container management platform that built on Kubernetes. It provides an easy-to-use UI enables creation of computing resources with a few clicks and one-click deployment, which reduces the learning curve and empower the DevOps teams. It greatly reduces the complexity of the daily work of development, testing, operation and maintenance, aiming to solve the pain spots of Kubernetes' storage, network, security and ease of use, etc.

See this document that describes the KubeSphere landscape and details.

Features
KubeSphere provides an easy-to-use console with the awesome user experience that allows you to quickly get started with a container management platform. KubeSphere provides and integrates workload management, DevOps Delivery, multi-tenant management, multi-dimensional monitoring, service and network management, application scheduling, infrastructure management, image registry management, etc. It also supports multiple open source storage and high-performance cloud storage as the persistent storage services.

See this document that elaborates on the KubeSphere features and services from a professional point of view.


Installation
KubeSphere installation supports following 2 kinds of installation:

All-in-One: For those who are new to KubeSphere and looking for the fastest way to install and experience the dashboard, the all-in-one installation must be your best choice since it supports one-click installation.
Multi-Node: Multi-node is used for installing KubeSphere on multiple instances, supports for installing a highly available master and etcd cluster which is able to use in a formal environment.
For Chinese version, see KubeSphere Installation Guide (安装指南) .

Minimum Requirements

Operating Systems

CentOS 7.5 (64 bit)
Ubuntu 16.04/18.04 LTS (64 bit)
Red Hat Enterprise Linux Server 7.4 (64 bit)
Debian Stretch 9.5 (64 bit)


Hardware

CPU：4 Core,  Memory：8 G, Disk Space：100 G



Quick Start
The Quick Start Guide provides 7 quick-start examples to walk you through the process and common manipulation in KubeSphere, with a quick overview of the basic features of KubeSphere that helps you to get familiar with it.
Latest Release
KubeSphere Advanced Edition 1.0.1 was released on January 28th, 2019. See the Release Notes For 1.0.1 to preview the updates.
RoadMap
Currently, KubeSphere has released the following three major editions. Advanced Edition 2.0.0 will be released on April 18, 2019. The 2.0.0 release will include microservice governance, log query and collection, alerting, S2i, code continuous inspection (SonarQube), quota management for workspace, improve security performance, GPU support, as well as providing Porter, which is a load balancer for bare metal Kubernetes clusters.
Community Edition => Express Edition => Advanced Edition

Documentation

KubeSphere Documentation (En/中) 
KubeSphere Docementation (PDF)

Support, Discussion, and Community
If you need any help with KubeSphere, please join us at Slack channel where most of our team hangs out at.
Please submit any KubeSphere bugs, issues, and feature requests to KubeSphere GitHub Issue.
Contributing to the project
All members of the KubeSphere community must abide by Code of Conduct. Only by respecting each other can we develop a productive, collaborative community.
How to submit a pull request to KubeSphere? See Pull Request Instruction.
You can then find out more detail here.
",324
Lombiq/Orchard-Security,C#,"Lombiq Security Orchard module Readme
Project Description
An Orchard module to enhance security.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/orchard-security (Mercurial repository)
https://github.com/Lombiq/Orchard-Security (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",2
Z3Prover/bin,None,"bin
Auxiliary repository used to store pre-compiled binary distribution packages of Z3.
The 'releases' directory contains official releases (stable) up to 2017.
Releases after this time can be found directly in https://github.com/z3prover/z3/releases.
The 'nightly' directory contains nightly automatic builds of the latest source code (may be unstable).
",16
HsiangHo/work-is-impossible,Shell,"
然而，作为Github(侗狌姣友网)资深老哥，我并不觉得有多骄傲，正因为是另一位格瓦纳先生(Qie Guevara)让我明白：
打工, 是不可能打工的!
做生意又不会做，就是在Github上撸点代码这种东西，才能维持得了生活这样子。在Github的感觉就像回家一样，里面的老哥个个都是人才，说话又好听，我超喜欢在里面。
银行卡余额： ￥807
理想要有，哪天开始不打工了不也美滋滋！
为什么在Jun 27 2018 这天没有GitHub commit绿点呢？ 答：跨越日界线，从洛杉矶飞回成都（结束美加之旅）啦！
2016年07月20日:　　开始打工！
2016年07月21日:　　继续打工！　　已打工1天
2016年07月22日:　　继续打工！　　已打工2天
2016年07月23日:　　继续打工！　　已打工3天
2016年07月24日:　　继续打工！　　已打工4天
2016年07月25日:　　继续打工！　　已打工5天
2016年07月26日:　　继续打工！　　已打工6天
2016年07月27日:　　继续打工！　　已打工7天
2016年07月28日:　　继续打工！　　已打工8天
2016年07月29日:　　继续打工！　　已打工9天
2016年07月30日:　　继续打工！　　已打工10天
2016年07月31日:　　继续打工！　　已打工11天
2016年08月01日:　　继续打工！　　已打工12天
2016年08月02日:　　继续打工！　　已打工13天
2016年08月03日:　　继续打工！　　已打工14天
2016年08月04日:　　继续打工！　　已打工15天
2016年08月05日:　　继续打工！　　已打工16天
2016年08月06日:　　继续打工！　　已打工17天
2016年08月07日:　　继续打工！　　已打工18天
2016年08月08日:　　继续打工！　　已打工19天
2016年08月09日:　　继续打工！　　已打工20天
2016年08月10日:　　继续打工！　　已打工21天
2016年08月11日:　　继续打工！　　已打工22天
2016年08月12日:　　继续打工！　　已打工23天
2016年08月13日:　　继续打工！　　已打工24天
2016年08月14日:　　继续打工！　　已打工25天
2016年08月15日:　　继续打工！　　已打工26天
2016年08月16日:　　继续打工！　　已打工27天
2016年08月17日:　　继续打工！　　已打工28天
2016年08月18日:　　继续打工！　　已打工29天
2016年08月19日:　　继续打工！　　已打工30天
2016年08月20日:　　继续打工！　　已打工31天
2016年08月21日:　　继续打工！　　已打工32天
2016年08月22日:　　继续打工！　　已打工33天
2016年08月23日:　　继续打工！　　已打工34天
2016年08月24日:　　继续打工！　　已打工35天
2016年08月25日:　　继续打工！　　已打工36天
2016年08月26日:　　继续打工！　　已打工37天
2016年08月27日:　　继续打工！　　已打工38天
2016年08月28日:　　继续打工！　　已打工39天
2016年08月29日:　　继续打工！　　已打工40天
2016年08月30日:　　继续打工！　　已打工41天
2016年08月31日:　　继续打工！　　已打工42天
2016年09月01日:　　继续打工！　　已打工43天
2016年09月02日:　　继续打工！　　已打工44天
2016年09月03日:　　继续打工！　　已打工45天
2016年09月04日:　　继续打工！　　已打工46天
2016年09月05日:　　继续打工！　　已打工47天
2016年09月06日:　　继续打工！　　已打工48天
2016年09月07日:　　继续打工！　　已打工49天
2016年09月08日:　　继续打工！　　已打工50天
2016年09月09日:　　继续打工！　　已打工51天
2016年09月10日:　　继续打工！　　已打工52天
2016年09月11日:　　继续打工！　　已打工53天
2016年09月12日:　　继续打工！　　已打工54天
2016年09月13日:　　继续打工！　　已打工55天
2016年09月14日:　　继续打工！　　已打工56天
2016年09月15日:　　继续打工！　　已打工57天
2016年09月16日:　　继续打工！　　已打工58天
2016年09月17日:　　继续打工！　　已打工59天
2016年09月18日:　　继续打工！　　已打工60天
2016年09月19日:　　继续打工！　　已打工61天
2016年09月20日:　　继续打工！　　已打工62天
2016年09月21日:　　继续打工！　　已打工63天
2016年09月22日:　　继续打工！　　已打工64天
2016年09月23日:　　继续打工！　　已打工65天
2016年09月24日:　　继续打工！　　已打工66天
2016年09月25日:　　继续打工！　　已打工67天
2016年09月26日:　　继续打工！　　已打工68天
2016年09月27日:　　继续打工！　　已打工69天
2016年09月28日:　　继续打工！　　已打工70天
2016年09月29日:　　继续打工！　　已打工71天
2016年09月30日:　　继续打工！　　已打工72天
2016年10月01日:　　继续打工！　　已打工73天
2016年10月02日:　　继续打工！　　已打工74天
2016年10月03日:　　继续打工！　　已打工75天
2016年10月04日:　　继续打工！　　已打工76天
2016年10月05日:　　继续打工！　　已打工77天
2016年10月06日:　　继续打工！　　已打工78天
2016年10月07日:　　继续打工！　　已打工79天
2016年10月08日:　　继续打工！　　已打工80天
2016年10月09日:　　继续打工！　　已打工81天
2016年10月10日:　　继续打工！　　已打工82天
2016年10月11日:　　继续打工！　　已打工83天
2016年10月12日:　　继续打工！　　已打工84天
2016年10月13日:　　继续打工！　　已打工85天
2016年10月14日:　　继续打工！　　已打工86天
2016年10月15日:　　继续打工！　　已打工87天
2016年10月16日:　　继续打工！　　已打工88天
2016年10月17日:　　继续打工！　　已打工89天
2016年10月18日:　　继续打工！　　已打工90天
2016年10月19日:　　继续打工！　　已打工91天
2016年10月20日:　　继续打工！　　已打工92天
2016年10月21日:　　继续打工！　　已打工93天
2016年10月22日:　　继续打工！　　已打工94天
2016年10月23日:　　继续打工！　　已打工95天
2016年10月24日:　　继续打工！　　已打工96天
2016年10月25日:　　继续打工！　　已打工97天
2016年10月26日:　　继续打工！　　已打工98天
2016年10月27日:　　继续打工！　　已打工99天
2016年10月28日:　　继续打工！　　已打工100天
2016年10月29日:　　继续打工！　　已打工101天
2016年10月30日:　　继续打工！　　已打工102天
2016年10月31日:　　继续打工！　　已打工103天
2016年11月01日:　　继续打工！　　已打工104天
2016年11月02日:　　继续打工！　　已打工105天
2016年11月03日:　　继续打工！　　已打工106天
2016年11月04日:　　继续打工！　　已打工107天
2016年11月05日:　　继续打工！　　已打工108天
2016年11月06日:　　继续打工！　　已打工109天
2016年11月07日:　　继续打工！　　已打工110天
2016年11月08日:　　继续打工！　　已打工111天
2016年11月09日:　　继续打工！　　已打工112天
2016年11月10日:　　继续打工！　　已打工113天
2016年11月11日:　　继续打工！　　已打工114天
2016年11月12日:　　继续打工！　　已打工115天
2016年11月13日:　　继续打工！　　已打工116天
2016年11月14日:　　继续打工！　　已打工117天
2016年11月15日:　　继续打工！　　已打工118天
2016年11月16日:　　继续打工！　　已打工119天
2016年11月17日:　　继续打工！　　已打工120天
2016年11月18日:　　继续打工！　　已打工121天
2016年11月19日:　　继续打工！　　已打工122天
2016年11月20日:　　继续打工！　　已打工123天
2016年11月21日:　　继续打工！　　已打工124天
2016年11月22日:　　继续打工！　　已打工125天
2016年11月23日:　　继续打工！　　已打工126天
2016年11月24日:　　继续打工！　　已打工127天
2016年11月25日:　　继续打工！　　已打工128天
2016年11月26日:　　继续打工！　　已打工129天
2016年11月27日:　　继续打工！　　已打工130天
2016年11月28日:　　继续打工！　　已打工131天
2016年11月29日:　　继续打工！　　已打工132天
2016年11月30日:　　继续打工！　　已打工133天
2016年12月01日:　　继续打工！　　已打工134天
2016年12月02日:　　继续打工！　　已打工135天
2016年12月03日:　　继续打工！　　已打工136天
2016年12月04日:　　继续打工！　　已打工137天
2016年12月05日:　　继续打工！　　已打工138天
2016年12月06日:　　继续打工！　　已打工139天
2016年12月07日:　　继续打工！　　已打工140天
2016年12月08日:　　继续打工！　　已打工141天
2016年12月09日:　　继续打工！　　已打工142天
2016年12月10日:　　继续打工！　　已打工143天
2016年12月11日:　　继续打工！　　已打工144天
2016年12月12日:　　继续打工！　　已打工145天
2016年12月13日:　　继续打工！　　已打工146天
2016年12月14日:　　继续打工！　　已打工147天
2016年12月15日:　　继续打工！　　已打工148天
2016年12月16日:　　继续打工！　　已打工149天
2016年12月17日:　　继续打工！　　已打工150天
2016年12月18日:　　继续打工！　　已打工151天
2016年12月19日:　　继续打工！　　已打工152天
2016年12月20日:　　继续打工！　　已打工153天
2016年12月21日:　　继续打工！　　已打工154天
2016年12月22日:　　继续打工！　　已打工155天
2016年12月23日:　　继续打工！　　已打工156天
2016年12月24日:　　继续打工！　　已打工157天
2016年12月25日:　　继续打工！　　已打工158天
2016年12月26日:　　继续打工！　　已打工159天
2016年12月27日:　　继续打工！　　已打工160天
2016年12月28日:　　继续打工！　　已打工161天
2016年12月29日:　　继续打工！　　已打工162天
2016年12月30日:　　继续打工！　　已打工163天
2016年12月31日:　　继续打工！　　已打工164天
2017年01月01日:　　继续打工！　　已打工165天
2017年01月02日:　　继续打工！　　已打工166天
2017年01月03日:　　继续打工！　　已打工167天
2017年01月04日:　　继续打工！　　已打工168天
2017年01月05日:　　继续打工！　　已打工169天
2017年01月06日:　　继续打工！　　已打工170天
2017年01月07日:　　继续打工！　　已打工171天
2017年01月08日:　　继续打工！　　已打工172天
2017年01月09日:　　继续打工！　　已打工173天
2017年01月10日:　　继续打工！　　已打工174天
2017年01月11日:　　继续打工！　　已打工175天
2017年01月12日:　　继续打工！　　已打工176天
2017年01月13日:　　继续打工！　　已打工177天
2017年01月14日:　　继续打工！　　已打工178天
2017年01月15日:　　继续打工！　　已打工179天
2017年01月16日:　　继续打工！　　已打工180天
2017年01月17日:　　继续打工！　　已打工181天
2017年01月18日:　　继续打工！　　已打工182天
2017年01月19日:　　继续打工！　　已打工183天
2017年01月20日:　　继续打工！　　已打工184天
2017年01月21日:　　继续打工！　　已打工185天
2017年01月22日:　　继续打工！　　已打工186天
2017年01月23日:　　继续打工！　　已打工187天
2017年01月24日:　　继续打工！　　已打工188天
2017年01月25日:　　继续打工！　　已打工189天
2017年01月26日:　　继续打工！　　已打工190天
2017年01月27日:　　继续打工！　　已打工191天
2017年01月28日:　　继续打工！　　已打工192天
2017年01月29日:　　继续打工！　　已打工193天
2017年01月30日:　　继续打工！　　已打工194天
2017年01月31日:　　继续打工！　　已打工195天
2017年02月01日:　　继续打工！　　已打工196天
2017年02月02日:　　继续打工！　　已打工197天
2017年02月03日:　　继续打工！　　已打工198天
2017年02月04日:　　继续打工！　　已打工199天
2017年02月05日:　　继续打工！　　已打工200天
2017年02月06日:　　继续打工！　　已打工201天
2017年02月07日:　　继续打工！　　已打工202天
2017年02月08日:　　继续打工！　　已打工203天
2017年02月09日:　　继续打工！　　已打工204天
2017年02月10日:　　继续打工！　　已打工205天
2017年02月11日:　　继续打工！　　已打工206天
2017年02月12日:　　继续打工！　　已打工207天
2017年02月13日:　　继续打工！　　已打工208天
2017年02月14日:　　继续打工！　　已打工209天
2017年02月15日:　　继续打工！　　已打工210天
2017年02月16日:　　继续打工！　　已打工211天
2017年02月17日:　　继续打工！　　已打工212天
2017年02月18日:　　继续打工！　　已打工213天
2017年02月19日:　　继续打工！　　已打工214天
2017年02月20日:　　继续打工！　　已打工215天
2017年02月21日:　　继续打工！　　已打工216天
2017年02月22日:　　继续打工！　　已打工217天
2017年02月23日:　　继续打工！　　已打工218天
2017年02月24日:　　继续打工！　　已打工219天
2017年02月25日:　　继续打工！　　已打工220天
2017年02月26日:　　继续打工！　　已打工221天
2017年02月27日:　　继续打工！　　已打工222天
2017年02月28日:　　继续打工！　　已打工223天
2017年03月01日:　　继续打工！　　已打工224天
2017年03月02日:　　继续打工！　　已打工225天
2017年03月03日:　　继续打工！　　已打工226天
2017年03月04日:　　继续打工！　　已打工227天
2017年03月05日:　　继续打工！　　已打工228天
2017年03月06日:　　继续打工！　　已打工229天
2017年03月07日:　　继续打工！　　已打工230天
2017年03月08日:　　继续打工！　　已打工231天
2017年03月09日:　　继续打工！　　已打工232天
2017年03月10日:　　继续打工！　　已打工233天
2017年03月11日:　　继续打工！　　已打工234天
2017年03月12日:　　继续打工！　　已打工235天
2017年03月13日:　　继续打工！　　已打工236天
2017年03月14日:　　继续打工！　　已打工237天
2017年03月15日:　　继续打工！　　已打工238天
2017年03月16日:　　继续打工！　　已打工239天
2017年03月17日:　　继续打工！　　已打工240天
2017年03月18日:　　继续打工！　　已打工241天
2017年03月19日:　　继续打工！　　已打工242天
2017年03月20日:　　继续打工！　　已打工243天
2017年03月21日:　　继续打工！　　已打工244天
2017年03月22日:　　继续打工！　　已打工245天
2017年03月23日:　　继续打工！　　已打工246天
2017年03月24日:　　继续打工！　　已打工247天
2017年03月25日:　　继续打工！　　已打工248天
2017年03月26日:　　继续打工！　　已打工249天
2017年03月27日:　　继续打工！　　已打工250天
2017年03月28日:　　继续打工！　　已打工251天
2017年03月29日:　　继续打工！　　已打工252天
2017年03月30日:　　继续打工！　　已打工253天
2017年03月31日:　　继续打工！　　已打工254天
2017年04月01日:　　继续打工！　　已打工255天
2017年04月02日:　　继续打工！　　已打工256天
2017年04月03日:　　继续打工！　　已打工257天
2017年04月04日:　　继续打工！　　已打工258天
2017年04月05日:　　继续打工！　　已打工259天
2017年04月06日:　　继续打工！　　已打工260天
2017年04月07日:　　继续打工！　　已打工261天
2017年04月08日:　　继续打工！　　已打工262天
2017年04月09日:　　继续打工！　　已打工263天
2017年04月10日:　　继续打工！　　已打工264天
2017年04月11日:　　继续打工！　　已打工265天
2017年04月12日:　　继续打工！　　已打工266天
2017年04月13日:　　继续打工！　　已打工267天
2017年04月14日:　　继续打工！　　已打工268天
2017年04月15日:　　继续打工！　　已打工269天
2017年04月16日:　　继续打工！　　已打工270天
2017年04月17日:　　继续打工！　　已打工271天
2017年04月18日:　　继续打工！　　已打工272天
2017年04月19日:　　继续打工！　　已打工273天
2017年04月20日:　　继续打工！　　已打工274天
2017年04月21日:　　继续打工！　　已打工275天
2017年04月22日:　　继续打工！　　已打工276天
2017年04月23日:　　继续打工！　　已打工277天
2017年04月24日:　　继续打工！　　已打工278天
2017年04月25日:　　继续打工！　　已打工279天
2017年04月26日:　　继续打工！　　已打工280天
2017年04月27日:　　继续打工！　　已打工281天
2017年04月28日:　　继续打工！　　已打工282天
2017年04月29日:　　继续打工！　　已打工283天
2017年04月30日:　　继续打工！　　已打工284天
2017年05月01日:　　继续打工！　　已打工285天
2017年05月02日:　　继续打工！　　已打工286天
2017年05月03日:　　继续打工！　　已打工287天
2017年05月04日:　　继续打工！　　已打工288天
2017年05月05日:　　继续打工！　　已打工289天
2017年05月06日:　　继续打工！　　已打工290天
2017年05月07日:　　继续打工！　　已打工291天
2017年05月08日:　　继续打工！　　已打工292天
2017年05月09日:　　继续打工！　　已打工293天
2017年05月10日:　　继续打工！　　已打工294天
2017年05月11日:　　继续打工！　　已打工295天
2017年05月12日:　　继续打工！　　已打工296天
2017年05月13日:　　继续打工！　　已打工297天
2017年05月14日:　　继续打工！　　已打工298天
2017年05月15日:　　继续打工！　　已打工299天
2017年05月16日:　　继续打工！　　已打工300天
2017年05月17日:　　继续打工！　　已打工301天
2017年05月18日:　　继续打工！　　已打工302天
2017年05月19日:　　继续打工！　　已打工303天
2017年05月20日:　　继续打工！　　已打工304天
2017年05月21日:　　继续打工！　　已打工305天
2017年05月22日:　　继续打工！　　已打工306天
2017年05月23日:　　继续打工！　　已打工307天
2017年05月24日:　　继续打工！　　已打工308天
2017年05月25日:　　继续打工！　　已打工309天
2017年05月26日:　　继续打工！　　已打工310天
2017年05月27日:　　继续打工！　　已打工311天
2017年05月28日:　　继续打工！　　已打工312天
2017年05月29日:　　继续打工！　　已打工313天
2017年05月30日:　　继续打工！　　已打工314天
2017年05月31日:　　继续打工！　　已打工315天
2017年06月01日:　　继续打工！　　已打工316天
2017年06月02日:　　继续打工！　　已打工317天
2017年06月03日:　　继续打工！　　已打工318天
2017年06月04日:　　继续打工！　　已打工319天
2017年06月05日:　　继续打工！　　已打工320天
2017年06月06日:　　继续打工！　　已打工321天
2017年06月07日:　　继续打工！　　已打工322天
2017年06月08日:　　继续打工！　　已打工323天
2017年06月09日:　　继续打工！　　已打工324天
2017年06月10日:　　继续打工！　　已打工325天
2017年06月11日:　　继续打工！　　已打工326天
2017年06月12日:　　继续打工！　　已打工327天
2017年06月13日:　　继续打工！　　已打工328天
2017年06月14日:　　继续打工！　　已打工329天
2017年06月15日:　　继续打工！　　已打工330天
2017年06月16日:　　继续打工！　　已打工331天
2017年06月17日:　　继续打工！　　已打工332天
2017年06月18日:　　继续打工！　　已打工333天
2017年06月19日:　　继续打工！　　已打工334天
2017年06月20日:　　继续打工！　　已打工335天
2017年06月21日:　　继续打工！　　已打工336天
2017年06月22日:　　继续打工！　　已打工337天
2017年06月23日:　　继续打工！　　已打工338天
2017年06月24日:　　继续打工！　　已打工339天
2017年06月25日:　　继续打工！　　已打工340天
2017年06月26日:　　继续打工！　　已打工341天
2017年06月27日:　　继续打工！　　已打工342天
2017年06月28日:　　继续打工！　　已打工343天
2017年06月29日:　　继续打工！　　已打工344天
2017年06月30日:　　继续打工！　　已打工345天
2017年07月01日:　　继续打工！　　已打工346天
2017年07月02日:　　继续打工！　　已打工347天
2017年07月03日:　　继续打工！　　已打工348天
2017年07月04日:　　继续打工！　　已打工349天
2017年07月05日:　　继续打工！　　已打工350天
2017年07月06日:　　继续打工！　　已打工351天
2017年07月07日:　　继续打工！　　已打工352天
2017年07月08日:　　继续打工！　　已打工353天
2017年07月09日:　　继续打工！　　已打工354天
2017年07月10日:　　继续打工！　　已打工355天
2017年07月11日:　　继续打工！　　已打工356天
2017年07月12日:　　继续打工！　　已打工357天
2017年07月13日:　　继续打工！　　已打工358天
2017年07月14日:　　继续打工！　　已打工359天
2017年07月15日:　　继续打工！　　已打工360天
2017年07月16日:　　继续打工！　　已打工361天
2017年07月17日:　　继续打工！　　已打工362天
2017年07月18日:　　继续打工！　　已打工363天
2017年07月19日:　　继续打工！　　已打工364天
2017年07月20日:　　继续打工！　　已打工365天
2017年07月21日:　　继续打工！　　已打工366天
2017年07月22日:　　继续打工！　　已打工367天
2017年07月23日:　　继续打工！　　已打工368天
2017年07月24日:　　继续打工！　　已打工369天
2017年07月25日:　　继续打工！　　已打工370天
2017年07月26日:　　继续打工！　　已打工371天
2017年07月27日:　　继续打工！　　已打工372天
2017年07月28日:　　继续打工！　　已打工373天
2017年07月29日:　　继续打工！　　已打工374天
2017年07月30日:　　继续打工！　　已打工375天
2017年07月31日:　　继续打工！　　已打工376天
2017年08月01日:　　继续打工！　　已打工377天
2017年08月02日:　　继续打工！　　已打工378天
2017年08月03日:　　继续打工！　　已打工379天
2017年08月04日:　　继续打工！　　已打工380天
2017年08月05日:　　继续打工！　　已打工381天
2017年08月06日:　　继续打工！　　已打工382天
2017年08月07日:　　继续打工！　　已打工383天
2017年08月08日:　　继续打工！　　已打工384天
2017年08月09日:　　继续打工！　　已打工385天
2017年08月10日:　　继续打工！　　已打工386天
2017年08月11日:　　继续打工！　　已打工387天
2017年08月12日:　　继续打工！　　已打工388天
2017年08月13日:　　继续打工！　　已打工389天
2017年08月14日:　　继续打工！　　已打工390天
2017年08月15日:　　继续打工！　　已打工391天
2017年08月16日:　　继续打工！　　已打工392天
2017年08月17日:　　继续打工！　　已打工393天
2017年08月18日:　　继续打工！　　已打工394天
2017年08月19日:　　继续打工！　　已打工395天
2017年08月20日:　　继续打工！　　已打工396天
2017年08月21日:　　继续打工！　　已打工397天
2017年08月22日:　　继续打工！　　已打工398天
2017年08月23日:　　继续打工！　　已打工399天
2017年08月24日:　　继续打工！　　已打工400天
2017年08月25日:　　继续打工！　　已打工401天
2017年08月26日:　　继续打工！　　已打工402天
2017年08月27日:　　继续打工！　　已打工403天
2017年08月28日:　　继续打工！　　已打工404天
2017年08月29日:　　继续打工！　　已打工405天
2017年08月30日:　　继续打工！　　已打工406天
2017年08月31日:　　继续打工！　　已打工407天
2017年09月01日:　　继续打工！　　已打工408天
2017年09月02日:　　继续打工！　　已打工409天
2017年09月03日:　　继续打工！　　已打工410天
2017年09月04日:　　继续打工！　　已打工411天
2017年09月05日:　　继续打工！　　已打工412天
2017年09月06日:　　继续打工！　　已打工413天
2017年09月07日:　　继续打工！　　已打工414天
2017年09月08日:　　继续打工！　　已打工415天
2017年09月09日:　　继续打工！　　已打工416天
2017年09月10日:　　继续打工！　　已打工417天
2017年09月11日:　　继续打工！　　已打工418天
2017年09月12日:　　继续打工！　　已打工419天
2017年09月13日:　　继续打工！　　已打工420天
2017年09月14日:　　继续打工！　　已打工421天
2017年09月15日:　　继续打工！　　已打工422天
2017年09月16日:　　继续打工！　　已打工423天
2017年09月17日:　　继续打工！　　已打工424天
2017年09月18日:　　继续打工！　　已打工425天
2017年09月19日:　　继续打工！　　已打工426天
2017年09月20日:　　继续打工！　　已打工427天
2017年09月21日:　　继续打工！　　已打工428天
2017年09月22日:　　继续打工！　　已打工429天
2017年09月23日:　　继续打工！　　已打工430天
2017年09月24日:　　继续打工！　　已打工431天
2017年09月25日:　　继续打工！　　已打工432天
2017年09月26日:　　继续打工！　　已打工433天
2017年09月27日:　　继续打工！　　已打工434天
2017年09月28日:　　继续打工！　　已打工435天
2017年09月29日:　　继续打工！　　已打工436天
2017年09月30日:　　继续打工！　　已打工437天
2017年10月01日:　　继续打工！　　已打工438天
2017年10月02日:　　继续打工！　　已打工439天
2017年10月03日:　　继续打工！　　已打工440天
2017年10月04日:　　继续打工！　　已打工441天
2017年10月05日:　　继续打工！　　已打工442天
2017年10月06日:　　继续打工！　　已打工443天
2017年10月07日:　　继续打工！　　已打工444天
2017年10月08日:　　继续打工！　　已打工445天
2017年10月09日:　　继续打工！　　已打工446天
2017年10月10日:　　继续打工！　　已打工447天
2017年10月11日:　　继续打工！　　已打工448天
2017年10月12日:　　继续打工！　　已打工449天
2017年10月13日:　　继续打工！　　已打工450天
2017年10月14日:　　继续打工！　　已打工451天
2017年10月15日:　　继续打工！　　已打工452天
2017年10月16日:　　继续打工！　　已打工453天
2017年10月17日:　　继续打工！　　已打工454天
2017年10月18日:　　继续打工！　　已打工455天
2017年10月19日:　　继续打工！　　已打工456天
2017年10月20日:　　继续打工！　　已打工457天
2017年10月21日:　　继续打工！　　已打工458天
2017年10月22日:　　继续打工！　　已打工459天
2017年10月23日:　　继续打工！　　已打工460天
2017年10月24日:　　继续打工！　　已打工461天
2017年10月25日:　　继续打工！　　已打工462天
2017年10月26日:　　继续打工！　　已打工463天
2017年10月27日:　　继续打工！　　已打工464天
2017年10月28日:　　继续打工！　　已打工465天
2017年10月29日:　　继续打工！　　已打工466天
2017年10月30日:　　继续打工！　　已打工467天
2017年10月31日:　　继续打工！　　已打工468天
2017年11月01日:　　继续打工！　　已打工469天
2017年11月02日:　　继续打工！　　已打工470天
2017年11月03日:　　继续打工！　　已打工471天
2017年11月04日:　　继续打工！　　已打工472天
2017年11月05日:　　继续打工！　　已打工473天
2017年11月06日:　　继续打工！　　已打工474天
2017年11月07日:　　继续打工！　　已打工475天
2017年11月08日:　　继续打工！　　已打工476天
2017年11月09日:　　继续打工！　　已打工477天
2017年11月10日:　　继续打工！　　已打工478天
2017年11月11日:　　继续打工！　　已打工479天
2017年11月12日:　　继续打工！　　已打工480天
2017年11月13日:　　继续打工！　　已打工481天
2017年11月14日:　　继续打工！　　已打工482天
2017年11月15日:　　继续打工！　　已打工483天
2017年11月16日:　　继续打工！　　已打工484天
2017年11月17日:　　继续打工！　　已打工485天
2017年11月18日:　　继续打工！　　已打工486天
2017年11月19日:　　继续打工！　　已打工487天
2017年11月20日:　　继续打工！　　已打工488天
2017年11月21日:　　继续打工！　　已打工489天
2017年11月22日:　　继续打工！　　已打工490天
2017年11月23日:　　继续打工！　　已打工491天
2017年11月24日:　　继续打工！　　已打工492天
2017年11月25日:　　继续打工！　　已打工493天
2017年11月26日:　　继续打工！　　已打工494天
2017年11月27日:　　继续打工！　　已打工495天
2017年11月28日:　　继续打工！　　已打工496天
2017年11月29日:　　继续打工！　　已打工497天
2017年11月30日:　　继续打工！　　已打工498天
2017年12月01日:　　继续打工！　　已打工499天
2017年12月02日:　　继续打工！　　已打工500天
2017年12月03日:　　继续打工！　　已打工501天
2017年12月04日:　　继续打工！　　已打工502天
2017年12月05日:　　继续打工！　　已打工503天
2017年12月06日:　　继续打工！　　已打工504天
2017年12月07日:　　继续打工！　　已打工505天
2017年12月08日:　　继续打工！　　已打工506天
2017年12月09日:　　继续打工！　　已打工507天
2017年12月10日:　　继续打工！　　已打工508天
2017年12月11日:　　继续打工！　　已打工509天
2017年12月12日:　　继续打工！　　已打工510天
2017年12月13日:　　继续打工！　　已打工511天
2017年12月14日:　　继续打工！　　已打工512天
2017年12月15日:　　继续打工！　　已打工513天
2017年12月16日:　　继续打工！　　已打工514天
2017年12月17日:　　继续打工！　　已打工515天
2017年12月18日:　　继续打工！　　已打工516天
2017年12月19日:　　继续打工！　　已打工517天
2017年12月20日:　　继续打工！　　已打工518天
2017年12月21日:　　继续打工！　　已打工519天
2017年12月22日:　　继续打工！　　已打工520天
2017年12月23日:　　继续打工！　　已打工521天
2017年12月24日:　　继续打工！　　已打工522天
2017年12月25日:　　继续打工！　　已打工523天
2017年12月26日:　　继续打工！　　已打工524天
2017年12月27日:　　继续打工！　　已打工525天
2017年12月28日:　　继续打工！　　已打工526天
2017年12月29日:　　继续打工！　　已打工527天
2017年12月30日:　　继续打工！　　已打工528天
2017年12月31日:　　继续打工！　　已打工529天
2018年01月01日:　　继续打工！　　已打工530天
2018年01月02日:　　继续打工！　　已打工531天
2018年01月03日:　　继续打工！　　已打工532天
2018年01月04日:　　继续打工！　　已打工533天
2018年01月05日:　　继续打工！　　已打工534天
2018年01月06日:　　继续打工！　　已打工535天
2018年01月07日:　　继续打工！　　已打工536天
2018年01月08日:　　继续打工！　　已打工537天
2018年01月09日:　　继续打工！　　已打工538天
2018年01月10日:　　继续打工！　　已打工539天
2018年01月11日:　　继续打工！　　已打工540天
2018年01月12日:　　继续打工！　　已打工541天
2018年01月13日:　　继续打工！　　已打工542天
2018年01月14日:　　继续打工！　　已打工543天
2018年01月15日:　　继续打工！　　已打工544天
2018年01月16日:　　继续打工！　　已打工545天
2018年01月17日:　　继续打工！　　已打工546天
2018年01月18日:　　继续打工！　　已打工547天
2018年01月19日:　　继续打工！　　已打工548天
2018年01月20日:　　继续打工！　　已打工549天
2018年01月21日:　　继续打工！　　已打工550天
2018年01月22日:　　继续打工！　　已打工551天
2018年01月23日:　　继续打工！　　已打工552天
2018年01月24日:　　继续打工！　　已打工553天
2018年01月25日:　　继续打工！　　已打工554天
2018年01月26日:　　继续打工！　　已打工555天
2018年01月27日:　　继续打工！　　已打工556天
2018年01月28日:　　继续打工！　　已打工557天
2018年01月29日:　　继续打工！　　已打工558天
2018年01月30日:　　继续打工！　　已打工559天
2018年01月31日:　　继续打工！　　已打工560天
2018年02月01日:　　继续打工！　　已打工561天
2018年02月02日:　　继续打工！　　已打工562天
2018年02月03日:　　继续打工！　　已打工563天
2018年02月04日:　　继续打工！　　已打工564天
2018年02月05日:　　继续打工！　　已打工565天
2018年02月06日:　　继续打工！　　已打工566天
2018年02月07日:　　继续打工！　　已打工567天
2018年02月08日:　　继续打工！　　已打工568天
2018年02月09日:　　继续打工！　　已打工569天
2018年02月10日:　　继续打工！　　已打工570天
2018年02月11日:　　继续打工！　　已打工571天
2018年02月12日:　　继续打工！　　已打工572天
2018年02月13日:　　继续打工！　　已打工573天
2018年02月14日:　　继续打工！　　已打工574天
2018年02月15日:　　继续打工！　　已打工575天
2018年02月16日:　　继续打工！　　已打工576天
2018年02月17日:　　继续打工！　　已打工577天
2018年02月18日:　　继续打工！　　已打工578天
2018年02月19日:　　继续打工！　　已打工579天
2018年02月20日:　　继续打工！　　已打工580天
2018年02月21日:　　继续打工！　　已打工581天
2018年02月22日:　　继续打工！　　已打工582天
2018年02月23日:　　继续打工！　　已打工583天
2018年02月24日:　　继续打工！　　已打工584天
2018年02月25日:　　继续打工！　　已打工585天
2018年02月26日:　　继续打工！　　已打工586天
2018年02月27日:　　继续打工！　　已打工587天
2018年02月28日:　　继续打工！　　已打工588天
2018年03月01日:　　继续打工！　　已打工589天
2018年03月02日:　　继续打工！　　已打工590天
2018年03月03日:　　继续打工！　　已打工591天
2018年03月04日:　　继续打工！　　已打工592天
2018年03月05日:　　继续打工！　　已打工593天
2018年03月06日:　　继续打工！　　已打工594天
2018年03月07日:　　继续打工！　　已打工595天
2018年03月08日:　　继续打工！　　已打工596天
2018年03月09日:　　继续打工！　　已打工597天
2018年03月10日:　　继续打工！　　已打工598天
2018年03月11日:　　继续打工！　　已打工599天
2018年03月12日:　　继续打工！　　已打工600天
2018年03月13日:　　继续打工！　　已打工601天
2018年03月14日:　　继续打工！　　已打工602天
2018年03月15日:　　继续打工！　　已打工603天
2018年03月16日:　　继续打工！　　已打工604天
2018年03月17日:　　继续打工！　　已打工605天
2018年03月18日:　　继续打工！　　已打工606天
2018年03月19日:　　继续打工！　　已打工607天
2018年03月20日:　　继续打工！　　已打工608天
2018年03月21日:　　继续打工！　　已打工609天
2018年03月22日:　　继续打工！　　已打工610天
2018年03月23日:　　继续打工！　　已打工611天
2018年03月24日:　　继续打工！　　已打工612天
2018年03月25日:　　继续打工！　　已打工613天
2018年03月26日:　　继续打工！　　已打工614天
2018年03月27日:　　继续打工！　　已打工615天
2018年03月28日:　　继续打工！　　已打工616天
2018年03月29日:　　继续打工！　　已打工617天
2018年03月30日:　　继续打工！　　已打工618天
2018年03月31日:　　继续打工！　　已打工619天
2018年04月01日:　　继续打工！　　已打工620天
2018年04月02日:　　继续打工！　　已打工621天
2018年04月03日:　　继续打工！　　已打工622天
2018年04月04日:　　继续打工！　　已打工623天
2018年04月05日:　　继续打工！　　已打工624天
2018年04月06日:　　继续打工！　　已打工625天
2018年04月07日:　　继续打工！　　已打工626天
2018年04月08日:　　继续打工！　　已打工627天
2018年04月09日:　　继续打工！　　已打工628天
2018年04月10日:　　继续打工！　　已打工629天
2018年04月11日:　　继续打工！　　已打工630天
2018年04月12日:　　继续打工！　　已打工631天
2018年04月13日:　　继续打工！　　已打工632天
2018年04月14日:　　继续打工！　　已打工633天
2018年04月15日:　　继续打工！　　已打工634天
2018年04月16日:　　继续打工！　　已打工635天
2018年04月17日:　　继续打工！　　已打工636天
2018年04月18日:　　继续打工！　　已打工637天
2018年04月19日:　　继续打工！　　已打工638天
2018年04月20日:　　继续打工！　　已打工639天
2018年04月21日:　　继续打工！　　已打工640天
2018年04月22日:　　继续打工！　　已打工641天
2018年04月23日:　　继续打工！　　已打工642天
2018年04月24日:　　继续打工！　　已打工643天
2018年04月25日:　　继续打工！　　已打工644天
2018年04月26日:　　继续打工！　　已打工645天
2018年04月27日:　　继续打工！　　已打工646天
2018年04月28日:　　继续打工！　　已打工647天
2018年04月29日:　　继续打工！　　已打工648天
2018年04月30日:　　继续打工！　　已打工649天
2018年05月01日:　　继续打工！　　已打工650天
2018年05月02日:　　继续打工！　　已打工651天
2018年05月03日:　　继续打工！　　已打工652天
2018年05月04日:　　继续打工！　　已打工653天
2018年05月05日:　　继续打工！　　已打工654天
2018年05月06日:　　继续打工！　　已打工655天
2018年05月07日:　　继续打工！　　已打工656天
2018年05月08日:　　继续打工！　　已打工657天
2018年05月09日:　　继续打工！　　已打工658天
2018年05月10日:　　继续打工！　　已打工659天
2018年05月11日:　　继续打工！　　已打工660天
2018年05月12日:　　继续打工！　　已打工661天
2018年05月13日:　　继续打工！　　已打工662天
2018年05月14日:　　继续打工！　　已打工663天
2018年05月15日:　　继续打工！　　已打工664天
2018年05月16日:　　继续打工！　　已打工665天
2018年05月17日:　　继续打工！　　已打工666天
2018年05月18日:　　继续打工！　　已打工667天
2018年05月19日:　　继续打工！　　已打工668天
2018年05月20日:　　继续打工！　　已打工669天
2018年05月21日:　　继续打工！　　已打工670天
2018年05月22日:　　继续打工！　　已打工671天
2018年05月23日:　　继续打工！　　已打工672天
2018年05月24日:　　继续打工！　　已打工673天
2018年05月25日:　　继续打工！　　已打工674天
2018年05月26日:　　继续打工！　　已打工675天
2018年05月27日:　　继续打工！　　已打工676天
2018年05月28日:　　继续打工！　　已打工677天
2018年05月28日:　　继续打工！　　已打工678天
2018年05月29日:　　继续打工！　　已打工679天
2018年05月30日:　　继续打工！　　已打工680天
2018年05月31日:　　继续打工！　　已打工681天
2018年06月01日:　　继续打工！　　已打工682天
2018年06月02日:　　继续打工！　　已打工683天
2018年06月03日:　　继续打工！　　已打工684天
2018年06月04日:　　继续打工！　　已打工685天
2018年06月05日:　　继续打工！　　已打工686天
2018年06月06日:　　继续打工！　　已打工687天
2018年06月07日:　　继续打工！　　已打工688天
2018年06月08日:　　继续打工！　　已打工689天
2018年06月09日:　　继续打工！　　已打工690天
2018年06月10日:　　继续打工！　　已打工691天
2018年06月11日:　　继续打工！　　已打工692天
2018年06月12日:　　继续打工！　　已打工693天
2018年06月13日:　　继续打工！　　已打工694天
2018年06月14日:　　继续打工！　　已打工695天
2018年06月15日:　　继续打工！　　已打工696天
2018年06月16日:　　继续打工！　　已打工697天
2018年06月17日:　　继续打工！　　已打工698天
2018年06月18日:　　继续打工！　　已打工699天
2018年06月19日:　　继续打工！　　已打工700天
2018年06月20日:　　继续打工！　　已打工701天
2018年06月21日:　　继续打工！　　已打工702天
2018年06月22日:　　继续打工！　　已打工703天
2018年06月23日:　　继续打工！　　已打工704天
2018年06月24日:　　继续打工！　　已打工705天
2018年06月25日:　　继续打工！　　已打工706天
2018年06月26日:　　继续打工！　　已打工707天
2018年06月28日:　　继续打工！　　已打工708天
2018年06月29日:　　继续打工！　　已打工709天
2018年06月30日:　　继续打工！　　已打工710天
2018年07月01日:　　继续打工！　　已打工711天
2018年07月02日:　　继续打工！　　已打工712天
2018年07月03日:　　继续打工！　　已打工713天
2018年07月04日:　　继续打工！　　已打工714天
2018年07月05日:　　继续打工！　　已打工715天
2018年07月06日:　　继续打工！　　已打工716天
2018年07月07日:　　继续打工！　　已打工717天
2018年07月08日:　　继续打工！　　已打工718天
2018年07月09日:　　继续打工！　　已打工719天
2018年07月10日:　　继续打工！　　已打工720天
2018年07月11日:　　继续打工！　　已打工721天
2018年07月12日:　　继续打工！　　已打工722天
2018年07月13日:　　继续打工！　　已打工723天
2018年07月14日:　　继续打工！　　已打工724天
2018年07月15日:　　继续打工！　　已打工725天
2018年07月16日:　　继续打工！　　已打工726天
2018年07月17日:　　继续打工！　　已打工727天
2018年07月18日:　　继续打工！　　已打工728天
2018年07月19日:　　继续打工！　　已打工729天
2018年07月20日:　　继续打工！　　已打工730天
2018年07月21日:　　继续打工！　　已打工731天
2018年07月22日:　　继续打工！　　已打工732天
2018年07月23日:　　继续打工！　　已打工733天
2018年07月24日:　　继续打工！　　已打工734天
2018年07月25日:　　继续打工！　　已打工735天
2018年07月26日:　　继续打工！　　已打工736天
2018年07月27日:　　继续打工！　　已打工737天
2018年07月28日:　　继续打工！　　已打工738天
2018年07月29日:　　继续打工！　　已打工739天
2018年07月30日:　　继续打工！　　已打工740天
2018年07月31日:　　继续打工！　　已打工741天
2018年08月01日:　　继续打工！　　已打工742天
2018年08月02日:　　继续打工！　　已打工743天
2018年08月03日:　　继续打工！　　已打工744天
2018年08月04日:　　继续打工！　　已打工745天
2018年08月05日:　　继续打工！　　已打工746天
2018年08月06日:　　继续打工！　　已打工747天
2018年08月07日:　　继续打工！　　已打工748天
2018年08月08日:　　继续打工！　　已打工749天
2018年08月09日:　　继续打工！　　已打工750天
2018年08月10日:　　继续打工！　　已打工751天
2018年08月11日:　　继续打工！　　已打工752天
2018年08月12日:　　继续打工！　　已打工753天
2018年08月13日:　　继续打工！　　已打工754天
2018年08月14日:　　继续打工！　　已打工755天
2018年08月15日:　　继续打工！　　已打工756天
2018年08月16日:　　继续打工！　　已打工757天
2018年08月17日:　　继续打工！　　已打工758天
2018年08月18日:　　继续打工！　　已打工759天
2018年08月19日:　　继续打工！　　已打工760天
2018年08月20日:　　继续打工！　　已打工761天
2018年08月21日:　　继续打工！　　已打工762天
2018年08月22日:　　继续打工！　　已打工763天
2018年08月23日:　　继续打工！　　已打工764天
2018年08月24日:　　继续打工！　　已打工765天
2018年08月25日:　　继续打工！　　已打工766天
2018年08月26日:　　继续打工！　　已打工767天
2018年08月27日:　　继续打工！　　已打工768天
2018年08月28日:　　继续打工！　　已打工769天
2018年08月29日:　　继续打工！　　已打工770天
2018年08月30日:　　继续打工！　　已打工771天
2018年08月31日:　　继续打工！　　已打工772天
2018年09月01日:　　继续打工！　　已打工773天
2018年09月02日:　　继续打工！　　已打工774天
2018年09月03日:　　继续打工！　　已打工775天
2018年09月04日:　　继续打工！　　已打工776天
2018年09月05日:　　继续打工！　　已打工777天
2018年09月06日:　　继续打工！　　已打工778天
2018年09月07日:　　继续打工！　　已打工779天
2018年09月08日:　　继续打工！　　已打工780天
2018年09月09日:　　继续打工！　　已打工781天
2018年09月10日:　　继续打工！　　已打工782天
2018年09月11日:　　继续打工！　　已打工783天
2018年09月12日:　　继续打工！　　已打工784天
2018年09月13日:　　继续打工！　　已打工785天
2018年09月14日:　　继续打工！　　已打工786天
2018年09月15日:　　继续打工！　　已打工787天
2018年09月16日:　　继续打工！　　已打工788天
2018年09月17日:　　继续打工！　　已打工789天
2018年09月18日:　　继续打工！　　已打工790天
2018年09月19日:　　继续打工！　　已打工791天
2018年09月20日:　　继续打工！　　已打工792天
2018年09月21日:　　继续打工！　　已打工793天
2018年09月22日:　　继续打工！　　已打工794天
2018年09月23日:　　继续打工！　　已打工795天
2018年09月24日:　　继续打工！　　已打工796天
2018年09月25日:　　继续打工！　　已打工797天
2018年09月26日:　　继续打工！　　已打工798天
2018年09月27日:　　继续打工！　　已打工799天
2018年09月28日:　　继续打工！　　已打工800天
2018年09月29日:　　继续打工！　　已打工801天
2018年09月30日:　　继续打工！　　已打工802天
2018年10月01日:　　继续打工！　　已打工803天
2018年10月02日:　　继续打工！　　已打工804天
2018年10月03日:　　继续打工！　　已打工805天
2018年10月04日:　　继续打工！　　已打工806天
2018年10月05日:　　继续打工！　　已打工807天
2018年10月06日:　　继续打工！　　已打工808天
2018年10月07日:　　继续打工！　　已打工809天
2018年10月08日:　　继续打工！　　已打工810天
2018年10月09日:　　继续打工！　　已打工811天
2018年10月10日:　　继续打工！　　已打工812天
2018年10月11日:　　继续打工！　　已打工813天
2018年10月12日:　　继续打工！　　已打工814天
2018年10月13日:　　继续打工！　　已打工815天
2018年10月14日:　　继续打工！　　已打工816天
2018年10月15日:　　继续打工！　　已打工817天
2018年10月16日:　　继续打工！　　已打工818天
2018年10月17日:　　继续打工！　　已打工819天
2018年10月18日:　　继续打工！　　已打工820天
2018年10月19日:　　继续打工！　　已打工821天
2018年10月20日:　　继续打工！　　已打工822天
2018年10月21日:　　继续打工！　　已打工823天
2018年10月22日:　　继续打工！　　已打工824天
2018年10月23日:　　继续打工！　　已打工825天
2018年10月24日:　　继续打工！　　已打工826天
2018年10月25日:　　继续打工！　　已打工827天
2018年10月26日:　　继续打工！　　已打工828天
2018年10月27日:　　继续打工！　　已打工829天
2018年10月28日:　　继续打工！　　已打工830天
2018年10月29日:　　继续打工！　　已打工831天
2018年10月30日:　　继续打工！　　已打工832天
2018年10月31日:　　继续打工！　　已打工833天
2018年11月01日:　　继续打工！　　已打工834天
2018年11月02日:　　继续打工！　　已打工835天
2018年11月03日:　　继续打工！　　已打工836天
2018年11月04日:　　继续打工！　　已打工837天
2018年11月05日:　　继续打工！　　已打工838天
2018年11月06日:　　继续打工！　　已打工839天
2018年11月07日:　　继续打工！　　已打工840天
2018年11月08日:　　继续打工！　　已打工841天
2018年11月09日:　　继续打工！　　已打工842天
2018年11月10日:　　继续打工！　　已打工843天
2018年11月11日:　　继续打工！　　已打工844天
2018年11月12日:　　继续打工！　　已打工845天
2018年11月13日:　　继续打工！　　已打工846天
2018年11月14日:　　继续打工！　　已打工847天
2018年11月15日:　　继续打工！　　已打工848天
2018年11月16日:　　继续打工！　　已打工849天
2018年11月17日:　　继续打工！　　已打工850天
2018年11月18日:　　继续打工！　　已打工851天
2018年11月19日:　　继续打工！　　已打工852天
2018年11月20日:　　继续打工！　　已打工853天
2018年11月21日:　　继续打工！　　已打工854天
2018年11月22日:　　继续打工！　　已打工855天
2018年11月23日:　　继续打工！　　已打工856天
2018年11月24日:　　继续打工！　　已打工857天
2018年11月25日:　　继续打工！　　已打工858天
2018年11月26日:　　继续打工！　　已打工859天
2018年11月27日:　　继续打工！　　已打工860天
2018年11月28日:　　继续打工！　　已打工861天
2018年11月29日:　　继续打工！　　已打工862天
2018年11月30日:　　继续打工！　　已打工863天
2018年12月01日:　　继续打工！　　已打工864天
2018年12月02日:　　继续打工！　　已打工865天
2018年12月03日:　　继续打工！　　已打工866天
2018年12月04日:　　继续打工！　　已打工867天
2018年12月05日:　　继续打工！　　已打工868天
2018年12月06日:　　继续打工！　　已打工869天
2018年12月07日:　　继续打工！　　已打工870天
2018年12月08日:　　继续打工！　　已打工871天
2018年12月09日:　　继续打工！　　已打工872天
2018年12月10日:　　继续打工！　　已打工873天
2018年12月11日:　　继续打工！　　已打工874天
2018年12月12日:　　继续打工！　　已打工875天
2018年12月13日:　　继续打工！　　已打工876天
2018年12月14日:　　继续打工！　　已打工877天
2018年12月15日:　　继续打工！　　已打工878天
2018年12月16日:　　继续打工！　　已打工879天
2018年12月17日:　　继续打工！　　已打工880天
2018年12月18日:　　继续打工！　　已打工881天
2018年12月19日:　　继续打工！　　已打工882天
2018年12月20日:　　继续打工！　　已打工883天
2018年12月21日:　　继续打工！　　已打工884天
2018年12月22日:　　继续打工！　　已打工885天
2018年12月23日:　　继续打工！　　已打工886天
2018年12月24日:　　继续打工！　　已打工887天
2018年12月25日:　　继续打工！　　已打工888天
2018年12月26日:　　继续打工！　　已打工889天
2018年12月27日:　　继续打工！　　已打工890天
2018年12月28日:　　继续打工！　　已打工891天
2018年12月29日:　　继续打工！　　已打工892天
2018年12月30日:　　继续打工！　　已打工893天
2018年12月31日:　　继续打工！　　已打工894天
2019年01月01日:　　继续打工！　　已打工895天
2019年01月02日:　　继续打工！　　已打工896天
2019年01月03日:　　继续打工！　　已打工897天
2019年01月04日:　　继续打工！　　已打工898天
2019年01月05日:　　继续打工！　　已打工899天
2019年01月06日:　　继续打工！　　已打工900天
2019年01月07日:　　继续打工！　　已打工901天
2019年01月08日:　　继续打工！　　已打工902天
2019年01月09日:　　继续打工！　　已打工903天
2019年01月10日:　　继续打工！　　已打工904天
2019年01月11日:　　继续打工！　　已打工905天
2019年01月12日:　　继续打工！　　已打工906天
2019年01月13日:　　继续打工！　　已打工907天
2019年01月14日:　　继续打工！　　已打工908天
2019年01月15日:　　继续打工！　　已打工909天
2019年01月16日:　　继续打工！　　已打工910天
2019年01月17日:　　继续打工！　　已打工911天
2019年01月18日:　　继续打工！　　已打工912天
2019年01月19日:　　继续打工！　　已打工913天
2019年01月20日:　　继续打工！　　已打工914天
2019年01月21日:　　继续打工！　　已打工915天
2019年01月22日:　　继续打工！　　已打工916天
2019年01月23日:　　继续打工！　　已打工917天
2019年01月24日:　　继续打工！　　已打工918天
2019年01月25日:　　继续打工！　　已打工919天
2019年01月26日:　　继续打工！　　已打工920天
2019年01月27日:　　继续打工！　　已打工921天
2019年01月28日:　　继续打工！　　已打工922天
2019年01月29日:　　继续打工！　　已打工923天
2019年01月30日:　　继续打工！　　已打工924天
2019年01月31日:　　继续打工！　　已打工925天
2019年02月01日:　　继续打工！　　已打工926天
2019年02月02日:　　继续打工！　　已打工927天
2019年02月03日:　　继续打工！　　已打工928天
2019年02月04日:　　继续打工！　　已打工929天
2019年02月05日:　　继续打工！　　已打工930天
2019年02月06日:　　继续打工！　　已打工931天
2019年02月07日:　　继续打工！　　已打工932天
2019年02月08日:　　继续打工！　　已打工933天
2019年02月09日:　　继续打工！　　已打工934天
2019年02月10日:　　继续打工！　　已打工935天
2019年02月11日:　　继续打工！　　已打工936天
2019年02月12日:　　继续打工！　　已打工937天
2019年02月13日:　　继续打工！　　已打工938天
2019年02月14日:　　继续打工！　　已打工939天
2019年02月15日:　　继续打工！　　已打工940天
2019年02月16日:　　继续打工！　　已打工941天
2019年02月17日:　　继续打工！　　已打工942天
2019年02月18日:　　继续打工！　　已打工943天
2019年02月19日:　　继续打工！　　已打工944天
2019年02月20日:　　继续打工！　　已打工945天
2019年02月21日:　　继续打工！　　已打工946天
2019年02月22日:　　继续打工！　　已打工947天
2019年02月23日:　　继续打工！　　已打工948天
2019年02月24日:　　继续打工！　　已打工949天
2019年02月25日:　　继续打工！　　已打工950天
2019年02月26日:　　继续打工！　　已打工951天
2019年02月27日:　　继续打工！　　已打工952天
2019年02月28日:　　继续打工！　　已打工953天
2019年03月01日:　　继续打工！　　已打工954天
2019年03月02日:　　继续打工！　　已打工955天
2019年03月03日:　　继续打工！　　已打工956天
2019年03月04日:　　继续打工！　　已打工957天
2019年03月05日:　　继续打工！　　已打工958天
2019年03月06日:　　继续打工！　　已打工959天
2019年03月07日:　　继续打工！　　已打工960天
2019年03月08日:　　继续打工！　　已打工961天
2019年03月09日:　　继续打工！　　已打工962天
2019年03月10日:　　继续打工！　　已打工963天
2019年03月11日:　　继续打工！　　已打工964天
2019年03月12日:　　继续打工！　　已打工965天
2019年03月13日:　　继续打工！　　已打工966天
2019年03月14日:　　继续打工！　　已打工967天
2019年03月15日:　　继续打工！　　已打工968天
2019年03月16日:　　继续打工！　　已打工969天
2019年03月17日:　　继续打工！　　已打工970天
2019年03月18日:　　继续打工！　　已打工971天
2019年03月19日:　　继续打工！　　已打工972天
2019年03月20日:　　继续打工！　　已打工973天
2019年03月21日:　　继续打工！　　已打工974天
2019年03月22日:　　继续打工！　　已打工975天
2019年03月23日:　　继续打工！　　已打工976天
2019年03月24日:　　继续打工！　　已打工977天
2019年03月25日:　　继续打工！　　已打工978天
2019年03月26日:　　继续打工！　　已打工979天
2019年03月27日:　　继续打工！　　已打工980天
2019年03月28日:　　继续打工！　　已打工981天
2019年03月29日:　　继续打工！　　已打工982天
2019年03月30日:　　继续打工！　　已打工983天
2019年03月31日:　　继续打工！　　已打工984天
2019年04月01日:　　继续打工！　　已打工985天
2019年04月02日:　　继续打工！　　已打工986天
2019年04月03日:　　继续打工！　　已打工987天
2019年04月04日:　　继续打工！　　已打工988天
2019年04月05日:　　继续打工！　　已打工989天
2019年04月06日:　　继续打工！　　已打工990天
2019年04月07日:　　继续打工！　　已打工991天
2019年04月08日:　　继续打工！　　已打工992天
2019年04月09日:　　继续打工！　　已打工993天
2019年04月10日:　　继续打工！　　已打工994天
2019年04月11日:　　继续打工！　　已打工995天
2019年04月12日:　　继续打工！　　已打工996天
2019年04月13日:　　继续打工！　　已打工997天
2019年04月14日:　　继续打工！　　已打工998天
2019年04月15日:　　继续打工！　　已打工999天
2019年04月16日:　　继续打工！　　已打工1000天
2019年04月17日:　　继续打工！　　已打工1001天
2019年04月18日:　　继续打工！　　已打工1002天
2019年04月19日:　　继续打工！　　已打工1003天
2019年04月20日:　　继续打工！　　已打工1004天
2019年04月21日:　　继续打工！　　已打工1005天
2019年04月22日:　　继续打工！　　已打工1006天
2019年04月23日:　　继续打工！　　已打工1007天
2019年04月24日:　　继续打工！　　已打工1008天
2019年04月25日:　　继续打工！　　已打工1009天
2019年04月26日:　　继续打工！　　已打工1010天
2019年04月27日:　　继续打工！　　已打工1011天
2019年04月28日:　　继续打工！　　已打工1012天
2019年04月29日:　　继续打工！　　已打工1013天
2019年04月30日:　　继续打工！　　已打工1014天
2019年05月01日:　　继续打工！　　已打工1015天
2019年05月02日:　　继续打工！　　已打工1016天
2019年05月03日:　　继续打工！　　已打工1017天
2019年05月04日:　　继续打工！　　已打工1018天
2019年05月05日:　　继续打工！　　已打工1019天
2019年05月06日:　　继续打工！　　已打工1020天
2019年05月07日:　　继续打工！　　已打工1021天
2019年05月08日:　　继续打工！　　已打工1022天
2019年05月09日:　　继续打工！　　已打工1023天
2019年05月10日:　　继续打工！　　已打工1024天
2019年05月11日:　　继续打工！　　已打工1025天
2019年05月12日:　　继续打工！　　已打工1026天
2019年05月13日:　　继续打工！　　已打工1027天
2019年05月14日:　　继续打工！　　已打工1028天
2019年05月15日:　　继续打工！　　已打工1029天
2019年05月16日:　　继续打工！　　已打工1030天
2019年05月17日:　　继续打工！　　已打工1031天
",4
ErickJMenezes/scc,JavaScript,"scc
Sistema de controle de chamados para Projeto Integralizador 5
integrantes
Erick Johnson Menezes
Vinicius Santana
",2
HsiangHo/work-is-impossible,Shell,"
然而，作为Github(侗狌姣友网)资深老哥，我并不觉得有多骄傲，正因为是另一位格瓦纳先生(Qie Guevara)让我明白：
打工, 是不可能打工的!
做生意又不会做，就是在Github上撸点代码这种东西，才能维持得了生活这样子。在Github的感觉就像回家一样，里面的老哥个个都是人才，说话又好听，我超喜欢在里面。
银行卡余额： ￥807
理想要有，哪天开始不打工了不也美滋滋！
为什么在Jun 27 2018 这天没有GitHub commit绿点呢？ 答：跨越日界线，从洛杉矶飞回成都（结束美加之旅）啦！
2016年07月20日:　　开始打工！
2016年07月21日:　　继续打工！　　已打工1天
2016年07月22日:　　继续打工！　　已打工2天
2016年07月23日:　　继续打工！　　已打工3天
2016年07月24日:　　继续打工！　　已打工4天
2016年07月25日:　　继续打工！　　已打工5天
2016年07月26日:　　继续打工！　　已打工6天
2016年07月27日:　　继续打工！　　已打工7天
2016年07月28日:　　继续打工！　　已打工8天
2016年07月29日:　　继续打工！　　已打工9天
2016年07月30日:　　继续打工！　　已打工10天
2016年07月31日:　　继续打工！　　已打工11天
2016年08月01日:　　继续打工！　　已打工12天
2016年08月02日:　　继续打工！　　已打工13天
2016年08月03日:　　继续打工！　　已打工14天
2016年08月04日:　　继续打工！　　已打工15天
2016年08月05日:　　继续打工！　　已打工16天
2016年08月06日:　　继续打工！　　已打工17天
2016年08月07日:　　继续打工！　　已打工18天
2016年08月08日:　　继续打工！　　已打工19天
2016年08月09日:　　继续打工！　　已打工20天
2016年08月10日:　　继续打工！　　已打工21天
2016年08月11日:　　继续打工！　　已打工22天
2016年08月12日:　　继续打工！　　已打工23天
2016年08月13日:　　继续打工！　　已打工24天
2016年08月14日:　　继续打工！　　已打工25天
2016年08月15日:　　继续打工！　　已打工26天
2016年08月16日:　　继续打工！　　已打工27天
2016年08月17日:　　继续打工！　　已打工28天
2016年08月18日:　　继续打工！　　已打工29天
2016年08月19日:　　继续打工！　　已打工30天
2016年08月20日:　　继续打工！　　已打工31天
2016年08月21日:　　继续打工！　　已打工32天
2016年08月22日:　　继续打工！　　已打工33天
2016年08月23日:　　继续打工！　　已打工34天
2016年08月24日:　　继续打工！　　已打工35天
2016年08月25日:　　继续打工！　　已打工36天
2016年08月26日:　　继续打工！　　已打工37天
2016年08月27日:　　继续打工！　　已打工38天
2016年08月28日:　　继续打工！　　已打工39天
2016年08月29日:　　继续打工！　　已打工40天
2016年08月30日:　　继续打工！　　已打工41天
2016年08月31日:　　继续打工！　　已打工42天
2016年09月01日:　　继续打工！　　已打工43天
2016年09月02日:　　继续打工！　　已打工44天
2016年09月03日:　　继续打工！　　已打工45天
2016年09月04日:　　继续打工！　　已打工46天
2016年09月05日:　　继续打工！　　已打工47天
2016年09月06日:　　继续打工！　　已打工48天
2016年09月07日:　　继续打工！　　已打工49天
2016年09月08日:　　继续打工！　　已打工50天
2016年09月09日:　　继续打工！　　已打工51天
2016年09月10日:　　继续打工！　　已打工52天
2016年09月11日:　　继续打工！　　已打工53天
2016年09月12日:　　继续打工！　　已打工54天
2016年09月13日:　　继续打工！　　已打工55天
2016年09月14日:　　继续打工！　　已打工56天
2016年09月15日:　　继续打工！　　已打工57天
2016年09月16日:　　继续打工！　　已打工58天
2016年09月17日:　　继续打工！　　已打工59天
2016年09月18日:　　继续打工！　　已打工60天
2016年09月19日:　　继续打工！　　已打工61天
2016年09月20日:　　继续打工！　　已打工62天
2016年09月21日:　　继续打工！　　已打工63天
2016年09月22日:　　继续打工！　　已打工64天
2016年09月23日:　　继续打工！　　已打工65天
2016年09月24日:　　继续打工！　　已打工66天
2016年09月25日:　　继续打工！　　已打工67天
2016年09月26日:　　继续打工！　　已打工68天
2016年09月27日:　　继续打工！　　已打工69天
2016年09月28日:　　继续打工！　　已打工70天
2016年09月29日:　　继续打工！　　已打工71天
2016年09月30日:　　继续打工！　　已打工72天
2016年10月01日:　　继续打工！　　已打工73天
2016年10月02日:　　继续打工！　　已打工74天
2016年10月03日:　　继续打工！　　已打工75天
2016年10月04日:　　继续打工！　　已打工76天
2016年10月05日:　　继续打工！　　已打工77天
2016年10月06日:　　继续打工！　　已打工78天
2016年10月07日:　　继续打工！　　已打工79天
2016年10月08日:　　继续打工！　　已打工80天
2016年10月09日:　　继续打工！　　已打工81天
2016年10月10日:　　继续打工！　　已打工82天
2016年10月11日:　　继续打工！　　已打工83天
2016年10月12日:　　继续打工！　　已打工84天
2016年10月13日:　　继续打工！　　已打工85天
2016年10月14日:　　继续打工！　　已打工86天
2016年10月15日:　　继续打工！　　已打工87天
2016年10月16日:　　继续打工！　　已打工88天
2016年10月17日:　　继续打工！　　已打工89天
2016年10月18日:　　继续打工！　　已打工90天
2016年10月19日:　　继续打工！　　已打工91天
2016年10月20日:　　继续打工！　　已打工92天
2016年10月21日:　　继续打工！　　已打工93天
2016年10月22日:　　继续打工！　　已打工94天
2016年10月23日:　　继续打工！　　已打工95天
2016年10月24日:　　继续打工！　　已打工96天
2016年10月25日:　　继续打工！　　已打工97天
2016年10月26日:　　继续打工！　　已打工98天
2016年10月27日:　　继续打工！　　已打工99天
2016年10月28日:　　继续打工！　　已打工100天
2016年10月29日:　　继续打工！　　已打工101天
2016年10月30日:　　继续打工！　　已打工102天
2016年10月31日:　　继续打工！　　已打工103天
2016年11月01日:　　继续打工！　　已打工104天
2016年11月02日:　　继续打工！　　已打工105天
2016年11月03日:　　继续打工！　　已打工106天
2016年11月04日:　　继续打工！　　已打工107天
2016年11月05日:　　继续打工！　　已打工108天
2016年11月06日:　　继续打工！　　已打工109天
2016年11月07日:　　继续打工！　　已打工110天
2016年11月08日:　　继续打工！　　已打工111天
2016年11月09日:　　继续打工！　　已打工112天
2016年11月10日:　　继续打工！　　已打工113天
2016年11月11日:　　继续打工！　　已打工114天
2016年11月12日:　　继续打工！　　已打工115天
2016年11月13日:　　继续打工！　　已打工116天
2016年11月14日:　　继续打工！　　已打工117天
2016年11月15日:　　继续打工！　　已打工118天
2016年11月16日:　　继续打工！　　已打工119天
2016年11月17日:　　继续打工！　　已打工120天
2016年11月18日:　　继续打工！　　已打工121天
2016年11月19日:　　继续打工！　　已打工122天
2016年11月20日:　　继续打工！　　已打工123天
2016年11月21日:　　继续打工！　　已打工124天
2016年11月22日:　　继续打工！　　已打工125天
2016年11月23日:　　继续打工！　　已打工126天
2016年11月24日:　　继续打工！　　已打工127天
2016年11月25日:　　继续打工！　　已打工128天
2016年11月26日:　　继续打工！　　已打工129天
2016年11月27日:　　继续打工！　　已打工130天
2016年11月28日:　　继续打工！　　已打工131天
2016年11月29日:　　继续打工！　　已打工132天
2016年11月30日:　　继续打工！　　已打工133天
2016年12月01日:　　继续打工！　　已打工134天
2016年12月02日:　　继续打工！　　已打工135天
2016年12月03日:　　继续打工！　　已打工136天
2016年12月04日:　　继续打工！　　已打工137天
2016年12月05日:　　继续打工！　　已打工138天
2016年12月06日:　　继续打工！　　已打工139天
2016年12月07日:　　继续打工！　　已打工140天
2016年12月08日:　　继续打工！　　已打工141天
2016年12月09日:　　继续打工！　　已打工142天
2016年12月10日:　　继续打工！　　已打工143天
2016年12月11日:　　继续打工！　　已打工144天
2016年12月12日:　　继续打工！　　已打工145天
2016年12月13日:　　继续打工！　　已打工146天
2016年12月14日:　　继续打工！　　已打工147天
2016年12月15日:　　继续打工！　　已打工148天
2016年12月16日:　　继续打工！　　已打工149天
2016年12月17日:　　继续打工！　　已打工150天
2016年12月18日:　　继续打工！　　已打工151天
2016年12月19日:　　继续打工！　　已打工152天
2016年12月20日:　　继续打工！　　已打工153天
2016年12月21日:　　继续打工！　　已打工154天
2016年12月22日:　　继续打工！　　已打工155天
2016年12月23日:　　继续打工！　　已打工156天
2016年12月24日:　　继续打工！　　已打工157天
2016年12月25日:　　继续打工！　　已打工158天
2016年12月26日:　　继续打工！　　已打工159天
2016年12月27日:　　继续打工！　　已打工160天
2016年12月28日:　　继续打工！　　已打工161天
2016年12月29日:　　继续打工！　　已打工162天
2016年12月30日:　　继续打工！　　已打工163天
2016年12月31日:　　继续打工！　　已打工164天
2017年01月01日:　　继续打工！　　已打工165天
2017年01月02日:　　继续打工！　　已打工166天
2017年01月03日:　　继续打工！　　已打工167天
2017年01月04日:　　继续打工！　　已打工168天
2017年01月05日:　　继续打工！　　已打工169天
2017年01月06日:　　继续打工！　　已打工170天
2017年01月07日:　　继续打工！　　已打工171天
2017年01月08日:　　继续打工！　　已打工172天
2017年01月09日:　　继续打工！　　已打工173天
2017年01月10日:　　继续打工！　　已打工174天
2017年01月11日:　　继续打工！　　已打工175天
2017年01月12日:　　继续打工！　　已打工176天
2017年01月13日:　　继续打工！　　已打工177天
2017年01月14日:　　继续打工！　　已打工178天
2017年01月15日:　　继续打工！　　已打工179天
2017年01月16日:　　继续打工！　　已打工180天
2017年01月17日:　　继续打工！　　已打工181天
2017年01月18日:　　继续打工！　　已打工182天
2017年01月19日:　　继续打工！　　已打工183天
2017年01月20日:　　继续打工！　　已打工184天
2017年01月21日:　　继续打工！　　已打工185天
2017年01月22日:　　继续打工！　　已打工186天
2017年01月23日:　　继续打工！　　已打工187天
2017年01月24日:　　继续打工！　　已打工188天
2017年01月25日:　　继续打工！　　已打工189天
2017年01月26日:　　继续打工！　　已打工190天
2017年01月27日:　　继续打工！　　已打工191天
2017年01月28日:　　继续打工！　　已打工192天
2017年01月29日:　　继续打工！　　已打工193天
2017年01月30日:　　继续打工！　　已打工194天
2017年01月31日:　　继续打工！　　已打工195天
2017年02月01日:　　继续打工！　　已打工196天
2017年02月02日:　　继续打工！　　已打工197天
2017年02月03日:　　继续打工！　　已打工198天
2017年02月04日:　　继续打工！　　已打工199天
2017年02月05日:　　继续打工！　　已打工200天
2017年02月06日:　　继续打工！　　已打工201天
2017年02月07日:　　继续打工！　　已打工202天
2017年02月08日:　　继续打工！　　已打工203天
2017年02月09日:　　继续打工！　　已打工204天
2017年02月10日:　　继续打工！　　已打工205天
2017年02月11日:　　继续打工！　　已打工206天
2017年02月12日:　　继续打工！　　已打工207天
2017年02月13日:　　继续打工！　　已打工208天
2017年02月14日:　　继续打工！　　已打工209天
2017年02月15日:　　继续打工！　　已打工210天
2017年02月16日:　　继续打工！　　已打工211天
2017年02月17日:　　继续打工！　　已打工212天
2017年02月18日:　　继续打工！　　已打工213天
2017年02月19日:　　继续打工！　　已打工214天
2017年02月20日:　　继续打工！　　已打工215天
2017年02月21日:　　继续打工！　　已打工216天
2017年02月22日:　　继续打工！　　已打工217天
2017年02月23日:　　继续打工！　　已打工218天
2017年02月24日:　　继续打工！　　已打工219天
2017年02月25日:　　继续打工！　　已打工220天
2017年02月26日:　　继续打工！　　已打工221天
2017年02月27日:　　继续打工！　　已打工222天
2017年02月28日:　　继续打工！　　已打工223天
2017年03月01日:　　继续打工！　　已打工224天
2017年03月02日:　　继续打工！　　已打工225天
2017年03月03日:　　继续打工！　　已打工226天
2017年03月04日:　　继续打工！　　已打工227天
2017年03月05日:　　继续打工！　　已打工228天
2017年03月06日:　　继续打工！　　已打工229天
2017年03月07日:　　继续打工！　　已打工230天
2017年03月08日:　　继续打工！　　已打工231天
2017年03月09日:　　继续打工！　　已打工232天
2017年03月10日:　　继续打工！　　已打工233天
2017年03月11日:　　继续打工！　　已打工234天
2017年03月12日:　　继续打工！　　已打工235天
2017年03月13日:　　继续打工！　　已打工236天
2017年03月14日:　　继续打工！　　已打工237天
2017年03月15日:　　继续打工！　　已打工238天
2017年03月16日:　　继续打工！　　已打工239天
2017年03月17日:　　继续打工！　　已打工240天
2017年03月18日:　　继续打工！　　已打工241天
2017年03月19日:　　继续打工！　　已打工242天
2017年03月20日:　　继续打工！　　已打工243天
2017年03月21日:　　继续打工！　　已打工244天
2017年03月22日:　　继续打工！　　已打工245天
2017年03月23日:　　继续打工！　　已打工246天
2017年03月24日:　　继续打工！　　已打工247天
2017年03月25日:　　继续打工！　　已打工248天
2017年03月26日:　　继续打工！　　已打工249天
2017年03月27日:　　继续打工！　　已打工250天
2017年03月28日:　　继续打工！　　已打工251天
2017年03月29日:　　继续打工！　　已打工252天
2017年03月30日:　　继续打工！　　已打工253天
2017年03月31日:　　继续打工！　　已打工254天
2017年04月01日:　　继续打工！　　已打工255天
2017年04月02日:　　继续打工！　　已打工256天
2017年04月03日:　　继续打工！　　已打工257天
2017年04月04日:　　继续打工！　　已打工258天
2017年04月05日:　　继续打工！　　已打工259天
2017年04月06日:　　继续打工！　　已打工260天
2017年04月07日:　　继续打工！　　已打工261天
2017年04月08日:　　继续打工！　　已打工262天
2017年04月09日:　　继续打工！　　已打工263天
2017年04月10日:　　继续打工！　　已打工264天
2017年04月11日:　　继续打工！　　已打工265天
2017年04月12日:　　继续打工！　　已打工266天
2017年04月13日:　　继续打工！　　已打工267天
2017年04月14日:　　继续打工！　　已打工268天
2017年04月15日:　　继续打工！　　已打工269天
2017年04月16日:　　继续打工！　　已打工270天
2017年04月17日:　　继续打工！　　已打工271天
2017年04月18日:　　继续打工！　　已打工272天
2017年04月19日:　　继续打工！　　已打工273天
2017年04月20日:　　继续打工！　　已打工274天
2017年04月21日:　　继续打工！　　已打工275天
2017年04月22日:　　继续打工！　　已打工276天
2017年04月23日:　　继续打工！　　已打工277天
2017年04月24日:　　继续打工！　　已打工278天
2017年04月25日:　　继续打工！　　已打工279天
2017年04月26日:　　继续打工！　　已打工280天
2017年04月27日:　　继续打工！　　已打工281天
2017年04月28日:　　继续打工！　　已打工282天
2017年04月29日:　　继续打工！　　已打工283天
2017年04月30日:　　继续打工！　　已打工284天
2017年05月01日:　　继续打工！　　已打工285天
2017年05月02日:　　继续打工！　　已打工286天
2017年05月03日:　　继续打工！　　已打工287天
2017年05月04日:　　继续打工！　　已打工288天
2017年05月05日:　　继续打工！　　已打工289天
2017年05月06日:　　继续打工！　　已打工290天
2017年05月07日:　　继续打工！　　已打工291天
2017年05月08日:　　继续打工！　　已打工292天
2017年05月09日:　　继续打工！　　已打工293天
2017年05月10日:　　继续打工！　　已打工294天
2017年05月11日:　　继续打工！　　已打工295天
2017年05月12日:　　继续打工！　　已打工296天
2017年05月13日:　　继续打工！　　已打工297天
2017年05月14日:　　继续打工！　　已打工298天
2017年05月15日:　　继续打工！　　已打工299天
2017年05月16日:　　继续打工！　　已打工300天
2017年05月17日:　　继续打工！　　已打工301天
2017年05月18日:　　继续打工！　　已打工302天
2017年05月19日:　　继续打工！　　已打工303天
2017年05月20日:　　继续打工！　　已打工304天
2017年05月21日:　　继续打工！　　已打工305天
2017年05月22日:　　继续打工！　　已打工306天
2017年05月23日:　　继续打工！　　已打工307天
2017年05月24日:　　继续打工！　　已打工308天
2017年05月25日:　　继续打工！　　已打工309天
2017年05月26日:　　继续打工！　　已打工310天
2017年05月27日:　　继续打工！　　已打工311天
2017年05月28日:　　继续打工！　　已打工312天
2017年05月29日:　　继续打工！　　已打工313天
2017年05月30日:　　继续打工！　　已打工314天
2017年05月31日:　　继续打工！　　已打工315天
2017年06月01日:　　继续打工！　　已打工316天
2017年06月02日:　　继续打工！　　已打工317天
2017年06月03日:　　继续打工！　　已打工318天
2017年06月04日:　　继续打工！　　已打工319天
2017年06月05日:　　继续打工！　　已打工320天
2017年06月06日:　　继续打工！　　已打工321天
2017年06月07日:　　继续打工！　　已打工322天
2017年06月08日:　　继续打工！　　已打工323天
2017年06月09日:　　继续打工！　　已打工324天
2017年06月10日:　　继续打工！　　已打工325天
2017年06月11日:　　继续打工！　　已打工326天
2017年06月12日:　　继续打工！　　已打工327天
2017年06月13日:　　继续打工！　　已打工328天
2017年06月14日:　　继续打工！　　已打工329天
2017年06月15日:　　继续打工！　　已打工330天
2017年06月16日:　　继续打工！　　已打工331天
2017年06月17日:　　继续打工！　　已打工332天
2017年06月18日:　　继续打工！　　已打工333天
2017年06月19日:　　继续打工！　　已打工334天
2017年06月20日:　　继续打工！　　已打工335天
2017年06月21日:　　继续打工！　　已打工336天
2017年06月22日:　　继续打工！　　已打工337天
2017年06月23日:　　继续打工！　　已打工338天
2017年06月24日:　　继续打工！　　已打工339天
2017年06月25日:　　继续打工！　　已打工340天
2017年06月26日:　　继续打工！　　已打工341天
2017年06月27日:　　继续打工！　　已打工342天
2017年06月28日:　　继续打工！　　已打工343天
2017年06月29日:　　继续打工！　　已打工344天
2017年06月30日:　　继续打工！　　已打工345天
2017年07月01日:　　继续打工！　　已打工346天
2017年07月02日:　　继续打工！　　已打工347天
2017年07月03日:　　继续打工！　　已打工348天
2017年07月04日:　　继续打工！　　已打工349天
2017年07月05日:　　继续打工！　　已打工350天
2017年07月06日:　　继续打工！　　已打工351天
2017年07月07日:　　继续打工！　　已打工352天
2017年07月08日:　　继续打工！　　已打工353天
2017年07月09日:　　继续打工！　　已打工354天
2017年07月10日:　　继续打工！　　已打工355天
2017年07月11日:　　继续打工！　　已打工356天
2017年07月12日:　　继续打工！　　已打工357天
2017年07月13日:　　继续打工！　　已打工358天
2017年07月14日:　　继续打工！　　已打工359天
2017年07月15日:　　继续打工！　　已打工360天
2017年07月16日:　　继续打工！　　已打工361天
2017年07月17日:　　继续打工！　　已打工362天
2017年07月18日:　　继续打工！　　已打工363天
2017年07月19日:　　继续打工！　　已打工364天
2017年07月20日:　　继续打工！　　已打工365天
2017年07月21日:　　继续打工！　　已打工366天
2017年07月22日:　　继续打工！　　已打工367天
2017年07月23日:　　继续打工！　　已打工368天
2017年07月24日:　　继续打工！　　已打工369天
2017年07月25日:　　继续打工！　　已打工370天
2017年07月26日:　　继续打工！　　已打工371天
2017年07月27日:　　继续打工！　　已打工372天
2017年07月28日:　　继续打工！　　已打工373天
2017年07月29日:　　继续打工！　　已打工374天
2017年07月30日:　　继续打工！　　已打工375天
2017年07月31日:　　继续打工！　　已打工376天
2017年08月01日:　　继续打工！　　已打工377天
2017年08月02日:　　继续打工！　　已打工378天
2017年08月03日:　　继续打工！　　已打工379天
2017年08月04日:　　继续打工！　　已打工380天
2017年08月05日:　　继续打工！　　已打工381天
2017年08月06日:　　继续打工！　　已打工382天
2017年08月07日:　　继续打工！　　已打工383天
2017年08月08日:　　继续打工！　　已打工384天
2017年08月09日:　　继续打工！　　已打工385天
2017年08月10日:　　继续打工！　　已打工386天
2017年08月11日:　　继续打工！　　已打工387天
2017年08月12日:　　继续打工！　　已打工388天
2017年08月13日:　　继续打工！　　已打工389天
2017年08月14日:　　继续打工！　　已打工390天
2017年08月15日:　　继续打工！　　已打工391天
2017年08月16日:　　继续打工！　　已打工392天
2017年08月17日:　　继续打工！　　已打工393天
2017年08月18日:　　继续打工！　　已打工394天
2017年08月19日:　　继续打工！　　已打工395天
2017年08月20日:　　继续打工！　　已打工396天
2017年08月21日:　　继续打工！　　已打工397天
2017年08月22日:　　继续打工！　　已打工398天
2017年08月23日:　　继续打工！　　已打工399天
2017年08月24日:　　继续打工！　　已打工400天
2017年08月25日:　　继续打工！　　已打工401天
2017年08月26日:　　继续打工！　　已打工402天
2017年08月27日:　　继续打工！　　已打工403天
2017年08月28日:　　继续打工！　　已打工404天
2017年08月29日:　　继续打工！　　已打工405天
2017年08月30日:　　继续打工！　　已打工406天
2017年08月31日:　　继续打工！　　已打工407天
2017年09月01日:　　继续打工！　　已打工408天
2017年09月02日:　　继续打工！　　已打工409天
2017年09月03日:　　继续打工！　　已打工410天
2017年09月04日:　　继续打工！　　已打工411天
2017年09月05日:　　继续打工！　　已打工412天
2017年09月06日:　　继续打工！　　已打工413天
2017年09月07日:　　继续打工！　　已打工414天
2017年09月08日:　　继续打工！　　已打工415天
2017年09月09日:　　继续打工！　　已打工416天
2017年09月10日:　　继续打工！　　已打工417天
2017年09月11日:　　继续打工！　　已打工418天
2017年09月12日:　　继续打工！　　已打工419天
2017年09月13日:　　继续打工！　　已打工420天
2017年09月14日:　　继续打工！　　已打工421天
2017年09月15日:　　继续打工！　　已打工422天
2017年09月16日:　　继续打工！　　已打工423天
2017年09月17日:　　继续打工！　　已打工424天
2017年09月18日:　　继续打工！　　已打工425天
2017年09月19日:　　继续打工！　　已打工426天
2017年09月20日:　　继续打工！　　已打工427天
2017年09月21日:　　继续打工！　　已打工428天
2017年09月22日:　　继续打工！　　已打工429天
2017年09月23日:　　继续打工！　　已打工430天
2017年09月24日:　　继续打工！　　已打工431天
2017年09月25日:　　继续打工！　　已打工432天
2017年09月26日:　　继续打工！　　已打工433天
2017年09月27日:　　继续打工！　　已打工434天
2017年09月28日:　　继续打工！　　已打工435天
2017年09月29日:　　继续打工！　　已打工436天
2017年09月30日:　　继续打工！　　已打工437天
2017年10月01日:　　继续打工！　　已打工438天
2017年10月02日:　　继续打工！　　已打工439天
2017年10月03日:　　继续打工！　　已打工440天
2017年10月04日:　　继续打工！　　已打工441天
2017年10月05日:　　继续打工！　　已打工442天
2017年10月06日:　　继续打工！　　已打工443天
2017年10月07日:　　继续打工！　　已打工444天
2017年10月08日:　　继续打工！　　已打工445天
2017年10月09日:　　继续打工！　　已打工446天
2017年10月10日:　　继续打工！　　已打工447天
2017年10月11日:　　继续打工！　　已打工448天
2017年10月12日:　　继续打工！　　已打工449天
2017年10月13日:　　继续打工！　　已打工450天
2017年10月14日:　　继续打工！　　已打工451天
2017年10月15日:　　继续打工！　　已打工452天
2017年10月16日:　　继续打工！　　已打工453天
2017年10月17日:　　继续打工！　　已打工454天
2017年10月18日:　　继续打工！　　已打工455天
2017年10月19日:　　继续打工！　　已打工456天
2017年10月20日:　　继续打工！　　已打工457天
2017年10月21日:　　继续打工！　　已打工458天
2017年10月22日:　　继续打工！　　已打工459天
2017年10月23日:　　继续打工！　　已打工460天
2017年10月24日:　　继续打工！　　已打工461天
2017年10月25日:　　继续打工！　　已打工462天
2017年10月26日:　　继续打工！　　已打工463天
2017年10月27日:　　继续打工！　　已打工464天
2017年10月28日:　　继续打工！　　已打工465天
2017年10月29日:　　继续打工！　　已打工466天
2017年10月30日:　　继续打工！　　已打工467天
2017年10月31日:　　继续打工！　　已打工468天
2017年11月01日:　　继续打工！　　已打工469天
2017年11月02日:　　继续打工！　　已打工470天
2017年11月03日:　　继续打工！　　已打工471天
2017年11月04日:　　继续打工！　　已打工472天
2017年11月05日:　　继续打工！　　已打工473天
2017年11月06日:　　继续打工！　　已打工474天
2017年11月07日:　　继续打工！　　已打工475天
2017年11月08日:　　继续打工！　　已打工476天
2017年11月09日:　　继续打工！　　已打工477天
2017年11月10日:　　继续打工！　　已打工478天
2017年11月11日:　　继续打工！　　已打工479天
2017年11月12日:　　继续打工！　　已打工480天
2017年11月13日:　　继续打工！　　已打工481天
2017年11月14日:　　继续打工！　　已打工482天
2017年11月15日:　　继续打工！　　已打工483天
2017年11月16日:　　继续打工！　　已打工484天
2017年11月17日:　　继续打工！　　已打工485天
2017年11月18日:　　继续打工！　　已打工486天
2017年11月19日:　　继续打工！　　已打工487天
2017年11月20日:　　继续打工！　　已打工488天
2017年11月21日:　　继续打工！　　已打工489天
2017年11月22日:　　继续打工！　　已打工490天
2017年11月23日:　　继续打工！　　已打工491天
2017年11月24日:　　继续打工！　　已打工492天
2017年11月25日:　　继续打工！　　已打工493天
2017年11月26日:　　继续打工！　　已打工494天
2017年11月27日:　　继续打工！　　已打工495天
2017年11月28日:　　继续打工！　　已打工496天
2017年11月29日:　　继续打工！　　已打工497天
2017年11月30日:　　继续打工！　　已打工498天
2017年12月01日:　　继续打工！　　已打工499天
2017年12月02日:　　继续打工！　　已打工500天
2017年12月03日:　　继续打工！　　已打工501天
2017年12月04日:　　继续打工！　　已打工502天
2017年12月05日:　　继续打工！　　已打工503天
2017年12月06日:　　继续打工！　　已打工504天
2017年12月07日:　　继续打工！　　已打工505天
2017年12月08日:　　继续打工！　　已打工506天
2017年12月09日:　　继续打工！　　已打工507天
2017年12月10日:　　继续打工！　　已打工508天
2017年12月11日:　　继续打工！　　已打工509天
2017年12月12日:　　继续打工！　　已打工510天
2017年12月13日:　　继续打工！　　已打工511天
2017年12月14日:　　继续打工！　　已打工512天
2017年12月15日:　　继续打工！　　已打工513天
2017年12月16日:　　继续打工！　　已打工514天
2017年12月17日:　　继续打工！　　已打工515天
2017年12月18日:　　继续打工！　　已打工516天
2017年12月19日:　　继续打工！　　已打工517天
2017年12月20日:　　继续打工！　　已打工518天
2017年12月21日:　　继续打工！　　已打工519天
2017年12月22日:　　继续打工！　　已打工520天
2017年12月23日:　　继续打工！　　已打工521天
2017年12月24日:　　继续打工！　　已打工522天
2017年12月25日:　　继续打工！　　已打工523天
2017年12月26日:　　继续打工！　　已打工524天
2017年12月27日:　　继续打工！　　已打工525天
2017年12月28日:　　继续打工！　　已打工526天
2017年12月29日:　　继续打工！　　已打工527天
2017年12月30日:　　继续打工！　　已打工528天
2017年12月31日:　　继续打工！　　已打工529天
2018年01月01日:　　继续打工！　　已打工530天
2018年01月02日:　　继续打工！　　已打工531天
2018年01月03日:　　继续打工！　　已打工532天
2018年01月04日:　　继续打工！　　已打工533天
2018年01月05日:　　继续打工！　　已打工534天
2018年01月06日:　　继续打工！　　已打工535天
2018年01月07日:　　继续打工！　　已打工536天
2018年01月08日:　　继续打工！　　已打工537天
2018年01月09日:　　继续打工！　　已打工538天
2018年01月10日:　　继续打工！　　已打工539天
2018年01月11日:　　继续打工！　　已打工540天
2018年01月12日:　　继续打工！　　已打工541天
2018年01月13日:　　继续打工！　　已打工542天
2018年01月14日:　　继续打工！　　已打工543天
2018年01月15日:　　继续打工！　　已打工544天
2018年01月16日:　　继续打工！　　已打工545天
2018年01月17日:　　继续打工！　　已打工546天
2018年01月18日:　　继续打工！　　已打工547天
2018年01月19日:　　继续打工！　　已打工548天
2018年01月20日:　　继续打工！　　已打工549天
2018年01月21日:　　继续打工！　　已打工550天
2018年01月22日:　　继续打工！　　已打工551天
2018年01月23日:　　继续打工！　　已打工552天
2018年01月24日:　　继续打工！　　已打工553天
2018年01月25日:　　继续打工！　　已打工554天
2018年01月26日:　　继续打工！　　已打工555天
2018年01月27日:　　继续打工！　　已打工556天
2018年01月28日:　　继续打工！　　已打工557天
2018年01月29日:　　继续打工！　　已打工558天
2018年01月30日:　　继续打工！　　已打工559天
2018年01月31日:　　继续打工！　　已打工560天
2018年02月01日:　　继续打工！　　已打工561天
2018年02月02日:　　继续打工！　　已打工562天
2018年02月03日:　　继续打工！　　已打工563天
2018年02月04日:　　继续打工！　　已打工564天
2018年02月05日:　　继续打工！　　已打工565天
2018年02月06日:　　继续打工！　　已打工566天
2018年02月07日:　　继续打工！　　已打工567天
2018年02月08日:　　继续打工！　　已打工568天
2018年02月09日:　　继续打工！　　已打工569天
2018年02月10日:　　继续打工！　　已打工570天
2018年02月11日:　　继续打工！　　已打工571天
2018年02月12日:　　继续打工！　　已打工572天
2018年02月13日:　　继续打工！　　已打工573天
2018年02月14日:　　继续打工！　　已打工574天
2018年02月15日:　　继续打工！　　已打工575天
2018年02月16日:　　继续打工！　　已打工576天
2018年02月17日:　　继续打工！　　已打工577天
2018年02月18日:　　继续打工！　　已打工578天
2018年02月19日:　　继续打工！　　已打工579天
2018年02月20日:　　继续打工！　　已打工580天
2018年02月21日:　　继续打工！　　已打工581天
2018年02月22日:　　继续打工！　　已打工582天
2018年02月23日:　　继续打工！　　已打工583天
2018年02月24日:　　继续打工！　　已打工584天
2018年02月25日:　　继续打工！　　已打工585天
2018年02月26日:　　继续打工！　　已打工586天
2018年02月27日:　　继续打工！　　已打工587天
2018年02月28日:　　继续打工！　　已打工588天
2018年03月01日:　　继续打工！　　已打工589天
2018年03月02日:　　继续打工！　　已打工590天
2018年03月03日:　　继续打工！　　已打工591天
2018年03月04日:　　继续打工！　　已打工592天
2018年03月05日:　　继续打工！　　已打工593天
2018年03月06日:　　继续打工！　　已打工594天
2018年03月07日:　　继续打工！　　已打工595天
2018年03月08日:　　继续打工！　　已打工596天
2018年03月09日:　　继续打工！　　已打工597天
2018年03月10日:　　继续打工！　　已打工598天
2018年03月11日:　　继续打工！　　已打工599天
2018年03月12日:　　继续打工！　　已打工600天
2018年03月13日:　　继续打工！　　已打工601天
2018年03月14日:　　继续打工！　　已打工602天
2018年03月15日:　　继续打工！　　已打工603天
2018年03月16日:　　继续打工！　　已打工604天
2018年03月17日:　　继续打工！　　已打工605天
2018年03月18日:　　继续打工！　　已打工606天
2018年03月19日:　　继续打工！　　已打工607天
2018年03月20日:　　继续打工！　　已打工608天
2018年03月21日:　　继续打工！　　已打工609天
2018年03月22日:　　继续打工！　　已打工610天
2018年03月23日:　　继续打工！　　已打工611天
2018年03月24日:　　继续打工！　　已打工612天
2018年03月25日:　　继续打工！　　已打工613天
2018年03月26日:　　继续打工！　　已打工614天
2018年03月27日:　　继续打工！　　已打工615天
2018年03月28日:　　继续打工！　　已打工616天
2018年03月29日:　　继续打工！　　已打工617天
2018年03月30日:　　继续打工！　　已打工618天
2018年03月31日:　　继续打工！　　已打工619天
2018年04月01日:　　继续打工！　　已打工620天
2018年04月02日:　　继续打工！　　已打工621天
2018年04月03日:　　继续打工！　　已打工622天
2018年04月04日:　　继续打工！　　已打工623天
2018年04月05日:　　继续打工！　　已打工624天
2018年04月06日:　　继续打工！　　已打工625天
2018年04月07日:　　继续打工！　　已打工626天
2018年04月08日:　　继续打工！　　已打工627天
2018年04月09日:　　继续打工！　　已打工628天
2018年04月10日:　　继续打工！　　已打工629天
2018年04月11日:　　继续打工！　　已打工630天
2018年04月12日:　　继续打工！　　已打工631天
2018年04月13日:　　继续打工！　　已打工632天
2018年04月14日:　　继续打工！　　已打工633天
2018年04月15日:　　继续打工！　　已打工634天
2018年04月16日:　　继续打工！　　已打工635天
2018年04月17日:　　继续打工！　　已打工636天
2018年04月18日:　　继续打工！　　已打工637天
2018年04月19日:　　继续打工！　　已打工638天
2018年04月20日:　　继续打工！　　已打工639天
2018年04月21日:　　继续打工！　　已打工640天
2018年04月22日:　　继续打工！　　已打工641天
2018年04月23日:　　继续打工！　　已打工642天
2018年04月24日:　　继续打工！　　已打工643天
2018年04月25日:　　继续打工！　　已打工644天
2018年04月26日:　　继续打工！　　已打工645天
2018年04月27日:　　继续打工！　　已打工646天
2018年04月28日:　　继续打工！　　已打工647天
2018年04月29日:　　继续打工！　　已打工648天
2018年04月30日:　　继续打工！　　已打工649天
2018年05月01日:　　继续打工！　　已打工650天
2018年05月02日:　　继续打工！　　已打工651天
2018年05月03日:　　继续打工！　　已打工652天
2018年05月04日:　　继续打工！　　已打工653天
2018年05月05日:　　继续打工！　　已打工654天
2018年05月06日:　　继续打工！　　已打工655天
2018年05月07日:　　继续打工！　　已打工656天
2018年05月08日:　　继续打工！　　已打工657天
2018年05月09日:　　继续打工！　　已打工658天
2018年05月10日:　　继续打工！　　已打工659天
2018年05月11日:　　继续打工！　　已打工660天
2018年05月12日:　　继续打工！　　已打工661天
2018年05月13日:　　继续打工！　　已打工662天
2018年05月14日:　　继续打工！　　已打工663天
2018年05月15日:　　继续打工！　　已打工664天
2018年05月16日:　　继续打工！　　已打工665天
2018年05月17日:　　继续打工！　　已打工666天
2018年05月18日:　　继续打工！　　已打工667天
2018年05月19日:　　继续打工！　　已打工668天
2018年05月20日:　　继续打工！　　已打工669天
2018年05月21日:　　继续打工！　　已打工670天
2018年05月22日:　　继续打工！　　已打工671天
2018年05月23日:　　继续打工！　　已打工672天
2018年05月24日:　　继续打工！　　已打工673天
2018年05月25日:　　继续打工！　　已打工674天
2018年05月26日:　　继续打工！　　已打工675天
2018年05月27日:　　继续打工！　　已打工676天
2018年05月28日:　　继续打工！　　已打工677天
2018年05月28日:　　继续打工！　　已打工678天
2018年05月29日:　　继续打工！　　已打工679天
2018年05月30日:　　继续打工！　　已打工680天
2018年05月31日:　　继续打工！　　已打工681天
2018年06月01日:　　继续打工！　　已打工682天
2018年06月02日:　　继续打工！　　已打工683天
2018年06月03日:　　继续打工！　　已打工684天
2018年06月04日:　　继续打工！　　已打工685天
2018年06月05日:　　继续打工！　　已打工686天
2018年06月06日:　　继续打工！　　已打工687天
2018年06月07日:　　继续打工！　　已打工688天
2018年06月08日:　　继续打工！　　已打工689天
2018年06月09日:　　继续打工！　　已打工690天
2018年06月10日:　　继续打工！　　已打工691天
2018年06月11日:　　继续打工！　　已打工692天
2018年06月12日:　　继续打工！　　已打工693天
2018年06月13日:　　继续打工！　　已打工694天
2018年06月14日:　　继续打工！　　已打工695天
2018年06月15日:　　继续打工！　　已打工696天
2018年06月16日:　　继续打工！　　已打工697天
2018年06月17日:　　继续打工！　　已打工698天
2018年06月18日:　　继续打工！　　已打工699天
2018年06月19日:　　继续打工！　　已打工700天
2018年06月20日:　　继续打工！　　已打工701天
2018年06月21日:　　继续打工！　　已打工702天
2018年06月22日:　　继续打工！　　已打工703天
2018年06月23日:　　继续打工！　　已打工704天
2018年06月24日:　　继续打工！　　已打工705天
2018年06月25日:　　继续打工！　　已打工706天
2018年06月26日:　　继续打工！　　已打工707天
2018年06月28日:　　继续打工！　　已打工708天
2018年06月29日:　　继续打工！　　已打工709天
2018年06月30日:　　继续打工！　　已打工710天
2018年07月01日:　　继续打工！　　已打工711天
2018年07月02日:　　继续打工！　　已打工712天
2018年07月03日:　　继续打工！　　已打工713天
2018年07月04日:　　继续打工！　　已打工714天
2018年07月05日:　　继续打工！　　已打工715天
2018年07月06日:　　继续打工！　　已打工716天
2018年07月07日:　　继续打工！　　已打工717天
2018年07月08日:　　继续打工！　　已打工718天
2018年07月09日:　　继续打工！　　已打工719天
2018年07月10日:　　继续打工！　　已打工720天
2018年07月11日:　　继续打工！　　已打工721天
2018年07月12日:　　继续打工！　　已打工722天
2018年07月13日:　　继续打工！　　已打工723天
2018年07月14日:　　继续打工！　　已打工724天
2018年07月15日:　　继续打工！　　已打工725天
2018年07月16日:　　继续打工！　　已打工726天
2018年07月17日:　　继续打工！　　已打工727天
2018年07月18日:　　继续打工！　　已打工728天
2018年07月19日:　　继续打工！　　已打工729天
2018年07月20日:　　继续打工！　　已打工730天
2018年07月21日:　　继续打工！　　已打工731天
2018年07月22日:　　继续打工！　　已打工732天
2018年07月23日:　　继续打工！　　已打工733天
2018年07月24日:　　继续打工！　　已打工734天
2018年07月25日:　　继续打工！　　已打工735天
2018年07月26日:　　继续打工！　　已打工736天
2018年07月27日:　　继续打工！　　已打工737天
2018年07月28日:　　继续打工！　　已打工738天
2018年07月29日:　　继续打工！　　已打工739天
2018年07月30日:　　继续打工！　　已打工740天
2018年07月31日:　　继续打工！　　已打工741天
2018年08月01日:　　继续打工！　　已打工742天
2018年08月02日:　　继续打工！　　已打工743天
2018年08月03日:　　继续打工！　　已打工744天
2018年08月04日:　　继续打工！　　已打工745天
2018年08月05日:　　继续打工！　　已打工746天
2018年08月06日:　　继续打工！　　已打工747天
2018年08月07日:　　继续打工！　　已打工748天
2018年08月08日:　　继续打工！　　已打工749天
2018年08月09日:　　继续打工！　　已打工750天
2018年08月10日:　　继续打工！　　已打工751天
2018年08月11日:　　继续打工！　　已打工752天
2018年08月12日:　　继续打工！　　已打工753天
2018年08月13日:　　继续打工！　　已打工754天
2018年08月14日:　　继续打工！　　已打工755天
2018年08月15日:　　继续打工！　　已打工756天
2018年08月16日:　　继续打工！　　已打工757天
2018年08月17日:　　继续打工！　　已打工758天
2018年08月18日:　　继续打工！　　已打工759天
2018年08月19日:　　继续打工！　　已打工760天
2018年08月20日:　　继续打工！　　已打工761天
2018年08月21日:　　继续打工！　　已打工762天
2018年08月22日:　　继续打工！　　已打工763天
2018年08月23日:　　继续打工！　　已打工764天
2018年08月24日:　　继续打工！　　已打工765天
2018年08月25日:　　继续打工！　　已打工766天
2018年08月26日:　　继续打工！　　已打工767天
2018年08月27日:　　继续打工！　　已打工768天
2018年08月28日:　　继续打工！　　已打工769天
2018年08月29日:　　继续打工！　　已打工770天
2018年08月30日:　　继续打工！　　已打工771天
2018年08月31日:　　继续打工！　　已打工772天
2018年09月01日:　　继续打工！　　已打工773天
2018年09月02日:　　继续打工！　　已打工774天
2018年09月03日:　　继续打工！　　已打工775天
2018年09月04日:　　继续打工！　　已打工776天
2018年09月05日:　　继续打工！　　已打工777天
2018年09月06日:　　继续打工！　　已打工778天
2018年09月07日:　　继续打工！　　已打工779天
2018年09月08日:　　继续打工！　　已打工780天
2018年09月09日:　　继续打工！　　已打工781天
2018年09月10日:　　继续打工！　　已打工782天
2018年09月11日:　　继续打工！　　已打工783天
2018年09月12日:　　继续打工！　　已打工784天
2018年09月13日:　　继续打工！　　已打工785天
2018年09月14日:　　继续打工！　　已打工786天
2018年09月15日:　　继续打工！　　已打工787天
2018年09月16日:　　继续打工！　　已打工788天
2018年09月17日:　　继续打工！　　已打工789天
2018年09月18日:　　继续打工！　　已打工790天
2018年09月19日:　　继续打工！　　已打工791天
2018年09月20日:　　继续打工！　　已打工792天
2018年09月21日:　　继续打工！　　已打工793天
2018年09月22日:　　继续打工！　　已打工794天
2018年09月23日:　　继续打工！　　已打工795天
2018年09月24日:　　继续打工！　　已打工796天
2018年09月25日:　　继续打工！　　已打工797天
2018年09月26日:　　继续打工！　　已打工798天
2018年09月27日:　　继续打工！　　已打工799天
2018年09月28日:　　继续打工！　　已打工800天
2018年09月29日:　　继续打工！　　已打工801天
2018年09月30日:　　继续打工！　　已打工802天
2018年10月01日:　　继续打工！　　已打工803天
2018年10月02日:　　继续打工！　　已打工804天
2018年10月03日:　　继续打工！　　已打工805天
2018年10月04日:　　继续打工！　　已打工806天
2018年10月05日:　　继续打工！　　已打工807天
2018年10月06日:　　继续打工！　　已打工808天
2018年10月07日:　　继续打工！　　已打工809天
2018年10月08日:　　继续打工！　　已打工810天
2018年10月09日:　　继续打工！　　已打工811天
2018年10月10日:　　继续打工！　　已打工812天
2018年10月11日:　　继续打工！　　已打工813天
2018年10月12日:　　继续打工！　　已打工814天
2018年10月13日:　　继续打工！　　已打工815天
2018年10月14日:　　继续打工！　　已打工816天
2018年10月15日:　　继续打工！　　已打工817天
2018年10月16日:　　继续打工！　　已打工818天
2018年10月17日:　　继续打工！　　已打工819天
2018年10月18日:　　继续打工！　　已打工820天
2018年10月19日:　　继续打工！　　已打工821天
2018年10月20日:　　继续打工！　　已打工822天
2018年10月21日:　　继续打工！　　已打工823天
2018年10月22日:　　继续打工！　　已打工824天
2018年10月23日:　　继续打工！　　已打工825天
2018年10月24日:　　继续打工！　　已打工826天
2018年10月25日:　　继续打工！　　已打工827天
2018年10月26日:　　继续打工！　　已打工828天
2018年10月27日:　　继续打工！　　已打工829天
2018年10月28日:　　继续打工！　　已打工830天
2018年10月29日:　　继续打工！　　已打工831天
2018年10月30日:　　继续打工！　　已打工832天
2018年10月31日:　　继续打工！　　已打工833天
2018年11月01日:　　继续打工！　　已打工834天
2018年11月02日:　　继续打工！　　已打工835天
2018年11月03日:　　继续打工！　　已打工836天
2018年11月04日:　　继续打工！　　已打工837天
2018年11月05日:　　继续打工！　　已打工838天
2018年11月06日:　　继续打工！　　已打工839天
2018年11月07日:　　继续打工！　　已打工840天
2018年11月08日:　　继续打工！　　已打工841天
2018年11月09日:　　继续打工！　　已打工842天
2018年11月10日:　　继续打工！　　已打工843天
2018年11月11日:　　继续打工！　　已打工844天
2018年11月12日:　　继续打工！　　已打工845天
2018年11月13日:　　继续打工！　　已打工846天
2018年11月14日:　　继续打工！　　已打工847天
2018年11月15日:　　继续打工！　　已打工848天
2018年11月16日:　　继续打工！　　已打工849天
2018年11月17日:　　继续打工！　　已打工850天
2018年11月18日:　　继续打工！　　已打工851天
2018年11月19日:　　继续打工！　　已打工852天
2018年11月20日:　　继续打工！　　已打工853天
2018年11月21日:　　继续打工！　　已打工854天
2018年11月22日:　　继续打工！　　已打工855天
2018年11月23日:　　继续打工！　　已打工856天
2018年11月24日:　　继续打工！　　已打工857天
2018年11月25日:　　继续打工！　　已打工858天
2018年11月26日:　　继续打工！　　已打工859天
2018年11月27日:　　继续打工！　　已打工860天
2018年11月28日:　　继续打工！　　已打工861天
2018年11月29日:　　继续打工！　　已打工862天
2018年11月30日:　　继续打工！　　已打工863天
2018年12月01日:　　继续打工！　　已打工864天
2018年12月02日:　　继续打工！　　已打工865天
2018年12月03日:　　继续打工！　　已打工866天
2018年12月04日:　　继续打工！　　已打工867天
2018年12月05日:　　继续打工！　　已打工868天
2018年12月06日:　　继续打工！　　已打工869天
2018年12月07日:　　继续打工！　　已打工870天
2018年12月08日:　　继续打工！　　已打工871天
2018年12月09日:　　继续打工！　　已打工872天
2018年12月10日:　　继续打工！　　已打工873天
2018年12月11日:　　继续打工！　　已打工874天
2018年12月12日:　　继续打工！　　已打工875天
2018年12月13日:　　继续打工！　　已打工876天
2018年12月14日:　　继续打工！　　已打工877天
2018年12月15日:　　继续打工！　　已打工878天
2018年12月16日:　　继续打工！　　已打工879天
2018年12月17日:　　继续打工！　　已打工880天
2018年12月18日:　　继续打工！　　已打工881天
2018年12月19日:　　继续打工！　　已打工882天
2018年12月20日:　　继续打工！　　已打工883天
2018年12月21日:　　继续打工！　　已打工884天
2018年12月22日:　　继续打工！　　已打工885天
2018年12月23日:　　继续打工！　　已打工886天
2018年12月24日:　　继续打工！　　已打工887天
2018年12月25日:　　继续打工！　　已打工888天
2018年12月26日:　　继续打工！　　已打工889天
2018年12月27日:　　继续打工！　　已打工890天
2018年12月28日:　　继续打工！　　已打工891天
2018年12月29日:　　继续打工！　　已打工892天
2018年12月30日:　　继续打工！　　已打工893天
2018年12月31日:　　继续打工！　　已打工894天
2019年01月01日:　　继续打工！　　已打工895天
2019年01月02日:　　继续打工！　　已打工896天
2019年01月03日:　　继续打工！　　已打工897天
2019年01月04日:　　继续打工！　　已打工898天
2019年01月05日:　　继续打工！　　已打工899天
2019年01月06日:　　继续打工！　　已打工900天
2019年01月07日:　　继续打工！　　已打工901天
2019年01月08日:　　继续打工！　　已打工902天
2019年01月09日:　　继续打工！　　已打工903天
2019年01月10日:　　继续打工！　　已打工904天
2019年01月11日:　　继续打工！　　已打工905天
2019年01月12日:　　继续打工！　　已打工906天
2019年01月13日:　　继续打工！　　已打工907天
2019年01月14日:　　继续打工！　　已打工908天
2019年01月15日:　　继续打工！　　已打工909天
2019年01月16日:　　继续打工！　　已打工910天
2019年01月17日:　　继续打工！　　已打工911天
2019年01月18日:　　继续打工！　　已打工912天
2019年01月19日:　　继续打工！　　已打工913天
2019年01月20日:　　继续打工！　　已打工914天
2019年01月21日:　　继续打工！　　已打工915天
2019年01月22日:　　继续打工！　　已打工916天
2019年01月23日:　　继续打工！　　已打工917天
2019年01月24日:　　继续打工！　　已打工918天
2019年01月25日:　　继续打工！　　已打工919天
2019年01月26日:　　继续打工！　　已打工920天
2019年01月27日:　　继续打工！　　已打工921天
2019年01月28日:　　继续打工！　　已打工922天
2019年01月29日:　　继续打工！　　已打工923天
2019年01月30日:　　继续打工！　　已打工924天
2019年01月31日:　　继续打工！　　已打工925天
2019年02月01日:　　继续打工！　　已打工926天
2019年02月02日:　　继续打工！　　已打工927天
2019年02月03日:　　继续打工！　　已打工928天
2019年02月04日:　　继续打工！　　已打工929天
2019年02月05日:　　继续打工！　　已打工930天
2019年02月06日:　　继续打工！　　已打工931天
2019年02月07日:　　继续打工！　　已打工932天
2019年02月08日:　　继续打工！　　已打工933天
2019年02月09日:　　继续打工！　　已打工934天
2019年02月10日:　　继续打工！　　已打工935天
2019年02月11日:　　继续打工！　　已打工936天
2019年02月12日:　　继续打工！　　已打工937天
2019年02月13日:　　继续打工！　　已打工938天
2019年02月14日:　　继续打工！　　已打工939天
2019年02月15日:　　继续打工！　　已打工940天
2019年02月16日:　　继续打工！　　已打工941天
2019年02月17日:　　继续打工！　　已打工942天
2019年02月18日:　　继续打工！　　已打工943天
2019年02月19日:　　继续打工！　　已打工944天
2019年02月20日:　　继续打工！　　已打工945天
2019年02月21日:　　继续打工！　　已打工946天
2019年02月22日:　　继续打工！　　已打工947天
2019年02月23日:　　继续打工！　　已打工948天
2019年02月24日:　　继续打工！　　已打工949天
2019年02月25日:　　继续打工！　　已打工950天
2019年02月26日:　　继续打工！　　已打工951天
2019年02月27日:　　继续打工！　　已打工952天
2019年02月28日:　　继续打工！　　已打工953天
2019年03月01日:　　继续打工！　　已打工954天
2019年03月02日:　　继续打工！　　已打工955天
2019年03月03日:　　继续打工！　　已打工956天
2019年03月04日:　　继续打工！　　已打工957天
2019年03月05日:　　继续打工！　　已打工958天
2019年03月06日:　　继续打工！　　已打工959天
2019年03月07日:　　继续打工！　　已打工960天
2019年03月08日:　　继续打工！　　已打工961天
2019年03月09日:　　继续打工！　　已打工962天
2019年03月10日:　　继续打工！　　已打工963天
2019年03月11日:　　继续打工！　　已打工964天
2019年03月12日:　　继续打工！　　已打工965天
2019年03月13日:　　继续打工！　　已打工966天
2019年03月14日:　　继续打工！　　已打工967天
2019年03月15日:　　继续打工！　　已打工968天
2019年03月16日:　　继续打工！　　已打工969天
2019年03月17日:　　继续打工！　　已打工970天
2019年03月18日:　　继续打工！　　已打工971天
2019年03月19日:　　继续打工！　　已打工972天
2019年03月20日:　　继续打工！　　已打工973天
2019年03月21日:　　继续打工！　　已打工974天
2019年03月22日:　　继续打工！　　已打工975天
2019年03月23日:　　继续打工！　　已打工976天
2019年03月24日:　　继续打工！　　已打工977天
2019年03月25日:　　继续打工！　　已打工978天
2019年03月26日:　　继续打工！　　已打工979天
2019年03月27日:　　继续打工！　　已打工980天
2019年03月28日:　　继续打工！　　已打工981天
2019年03月29日:　　继续打工！　　已打工982天
2019年03月30日:　　继续打工！　　已打工983天
2019年03月31日:　　继续打工！　　已打工984天
2019年04月01日:　　继续打工！　　已打工985天
2019年04月02日:　　继续打工！　　已打工986天
2019年04月03日:　　继续打工！　　已打工987天
2019年04月04日:　　继续打工！　　已打工988天
2019年04月05日:　　继续打工！　　已打工989天
2019年04月06日:　　继续打工！　　已打工990天
2019年04月07日:　　继续打工！　　已打工991天
2019年04月08日:　　继续打工！　　已打工992天
2019年04月09日:　　继续打工！　　已打工993天
2019年04月10日:　　继续打工！　　已打工994天
2019年04月11日:　　继续打工！　　已打工995天
2019年04月12日:　　继续打工！　　已打工996天
2019年04月13日:　　继续打工！　　已打工997天
2019年04月14日:　　继续打工！　　已打工998天
2019年04月15日:　　继续打工！　　已打工999天
2019年04月16日:　　继续打工！　　已打工1000天
2019年04月17日:　　继续打工！　　已打工1001天
2019年04月18日:　　继续打工！　　已打工1002天
2019年04月19日:　　继续打工！　　已打工1003天
2019年04月20日:　　继续打工！　　已打工1004天
2019年04月21日:　　继续打工！　　已打工1005天
2019年04月22日:　　继续打工！　　已打工1006天
2019年04月23日:　　继续打工！　　已打工1007天
2019年04月24日:　　继续打工！　　已打工1008天
2019年04月25日:　　继续打工！　　已打工1009天
2019年04月26日:　　继续打工！　　已打工1010天
2019年04月27日:　　继续打工！　　已打工1011天
2019年04月28日:　　继续打工！　　已打工1012天
2019年04月29日:　　继续打工！　　已打工1013天
2019年04月30日:　　继续打工！　　已打工1014天
2019年05月01日:　　继续打工！　　已打工1015天
2019年05月02日:　　继续打工！　　已打工1016天
2019年05月03日:　　继续打工！　　已打工1017天
2019年05月04日:　　继续打工！　　已打工1018天
2019年05月05日:　　继续打工！　　已打工1019天
2019年05月06日:　　继续打工！　　已打工1020天
2019年05月07日:　　继续打工！　　已打工1021天
2019年05月08日:　　继续打工！　　已打工1022天
2019年05月09日:　　继续打工！　　已打工1023天
2019年05月10日:　　继续打工！　　已打工1024天
2019年05月11日:　　继续打工！　　已打工1025天
2019年05月12日:　　继续打工！　　已打工1026天
2019年05月13日:　　继续打工！　　已打工1027天
2019年05月14日:　　继续打工！　　已打工1028天
2019年05月15日:　　继续打工！　　已打工1029天
2019年05月16日:　　继续打工！　　已打工1030天
2019年05月17日:　　继续打工！　　已打工1031天
",4
ErickJMenezes/scc,JavaScript,"scc
Sistema de controle de chamados para Projeto Integralizador 5
integrantes
Erick Johnson Menezes
Vinicius Santana
",2
Lombiq/Lombiq-Orchard-Visual-Studio-Extension,C#,"Lombiq Orchard Visual Studio Extension readme

Visual Studio extension with many features and templates frequently used by  Lombiq developers. Contains Orchard-related as well as generic goodies.
Check out the extension's Readme for more info (it's there and not in the root of the repository so it's also accessible from inside VS).
The project's logo was created by Ulises TJ.
",6
metabase/metabase,Clojure,"Metabase
Metabase is the easy, open source way for everyone in your company to ask questions and learn from data.





Features

5 minute setup (We're not kidding)
Let anyone on your team ask questions without knowing SQL
Rich beautiful dashboards with auto refresh and fullscreen
SQL Mode for analysts and data pros
Create canonical segments and metrics for your team to use
Send data to Slack or email on a schedule with Pulses
View data in Slack anytime with MetaBot
Humanize data for your team by renaming, annotating and hiding fields

For more information check out metabase.com
Supported databases

Postgres
MySQL
Druid
SQL Server
Redshift
MongoDB
Google BigQuery
SQLite
H2
Oracle
Vertica
Presto
Snowflake

Don't see your favorite database? File an issue to let us know.
Installation
Metabase can be run just about anywhere so checkout our Installation Guides for detailed instructions for various deployments.  Here's the TLDR:
Docker
To run Metabase via Docker, just type
docker run -d -p 3000:3000 --name metabase metabase/metabase
JVM Jar
To run the jar you will need to have a Java Runtime installed. As a quick check to see if you system already has one, try
java -version
If you see something like
java version ""1.8.0_51""
Java(TM) SE Runtime Environment (build 1.8.0_51-b16)
Java HotSpot(TM) 64-Bit Server VM (build 25.51-b03, mixed mode)
you are good to go. Otherwise, download the Java Runtime Environment at http://java.com/
Go to the Metabase Download Page and download the current build. Place the downloaded jar into a newly created directory (as it will create some files when it is run), and run it on the command line:
java -jar metabase.jar
Now, open a browser and go to http://localhost:3000 , and you will be asked a set of questions that will set up a user account, and then you can add a database connection. For this to work you will need to get some information about which database you want to connect to, such as the Host Name and Port that it is running on, the Database Name and the User and Password that you will be using.
Once you have added this connection, you will be taken into the app and you'll be ready to ask your first question.
For a more detailed walkthrough, check out our Getting Started guide.
Frequently Asked Questions
Some questions come up over and over again. Check here first:
FAQ
Security Disclosure
Security is very important to us. If you discover any issue regarding security, please disclose the information responsibly by sending an email to security@metabase.com and not by creating a GitHub issue.
Contributing
To get started with a development installation of the Metabase, follow the instructions at our Developers Guide.
Then take a look at our Contribution Guide for information about our process and where you can fit in!
Talk to other contributors in our Gitter room.
Internationalization
We want Metabase to be avaliable in as many languages as possible. See what translations are avaliable and help contribute to internationalization using our project over at POEditor
Extending and Deep Integrations
Metabase also allows you to hit our Query API directly from Javascript to integrate the simple analytics we provide with your own application or third party services to do things like:

Build moderation interfaces
Export subsets of your users to third party marketing automation software
Provide a specialized customer lookup application for the people in your company

Danger zone
The button below will deploy the branch where this README.md lives onto Heroku. Metabase developers use it to deploy branches of Metabase to test our PRs, etc. We DO NOT recommend you using this for production. Instead, please use a stable build.

License
Unless otherwise noted, all Metabase source files are made available under the terms of the GNU Affero General Public License (AGPL).
See LICENSE.txt for details and exceptions.
Unless otherwise noted, all files © 2019 Metabase, Inc.
",14943
StevenWeathers/thunderdome-planning-poker,Go,"Thunderdome Planning Poker
Thunderdome is an open source agile planning poker tool in the theme of Battling for points that helps teams estimate stories.

Planning Sessions are Battles
Users are Warriors
Stories are Plans
Points are in fibonacci sequence of 1, 2, 3, 5, 8, 13 and ? because how often do you point higher than 13 really?

Uses WebSockets and Svelte frontend framework for a truly Reactive UI experience

Building and running with docker-compose (easiest solution)
Prefered way of building and running the application with Postgres DB
docker-compose up --build

Building
To run without docker you will need to first build, then setup the postgres DB,
and pass the user, pass, name, host, and port to the application as environment variables
DB_HOST=
DB_PORT=
DB_USER=
DB_PASS=
DB_NAME=

Install dependencies
go get -d -v
go get -u github.com/gobuffalo/packr/packr
npm install

Build with Make
make build

OR manual steps
Build static assets
npm run build

bundle up static assets
packr

Build for current OS
go build

Let the Pointing Battles begin!
Run the server and visit http://localhost:8080
",4
cosmos/cosmos-sdk,Go,"Cosmos SDK










The Cosmos-SDK is a framework for building blockchain applications in Golang.
It is being used to build Gaia, the first implementation of the Cosmos Hub.
WARNING: The SDK has mostly stabilized, but we are still making some
breaking changes.
Note: Requires Go 1.12+
Cosmos Hub Mainnet
To run a full-node for the mainnet of the Cosmos Hub, first install gaia, then follow the guide.
For status updates and genesis file, see the
launch repo.
Quick Start
To learn how the SDK works from a high-level perspective, go to the SDK Intro.
If you want to get started quickly and learn how to build on top of the SDK, please follow the SDK Application Tutorial. You can also fork the tutorial's repo to get started building your own Cosmos SDK application.
For more, please go to the Cosmos SDK Docs
Disambiguation
This Cosmos-SDK project is not related to the React-Cosmos project (yet). Many thanks to Evan Coury and Ovidiu (@skidding) for this Github organization name. As per our agreement, this disambiguation notice will stay here.
",1055
jalvesaq/vimcmdline,Vim script,"vimcmdline: Send lines to interpreter
This plugin sends lines from either Vim or Neovim to a command line
interpreter (REPL application). There is support for
Clojure, Golang, Haskell, JavaScript, Julia, Jupyter, Lisp, Macaulay2, Matlab,
Prolog, Python, Ruby, Sage, Scala, Shell script, and Swift
(see Nvim-R for R support on
Vim/Neovim). The interpreter may run in a Neovim built-in terminal (Neovim
buffer), an external terminal emulator or in a tmux pane. The main advantage
of running the interpreter in a Neovim terminal is that the output is
colorized, as in the screenshot below, where we have different colors for
general output, positive and negative numbers, and the prompt line:

If running in either a Neovim built-in terminal or an external terminal, the
plugin runs one instance of the REPL application for each file type. If
running in a tmux pane, it runs one REPL application for Vim instance.
How to install
Either use a plugin manager such as Vim-Plug or copy the directories
ftplugin, plugin and syntax and their files to your ~/.vim or
~/.config/nvim directory.
Usage
If you are editing one of the supported file types, in Normal mode do:


<LocalLeader>s to start the interpreter.


<Space> to send the current line to the interpreter.


<LocalLeader><Space> to send the current line to the interpreter and keep the cursor on the current line.


<LocalLeader>q to send the quit command to the interpreter.


For languages that can source chunks of code:


In Visual mode, press:

<Space> to send a selection of text to the interpreter.



And, in Normal mode, press:


<LocalLeader>p to send from the line to the end of paragraph.


<LocalLeader>b to send block of code between the two closest marks.


<LocalLeader>f to send the entire file to the interpreter.




Options
Below are examples of how to set the options in your vimrc:
"" vimcmdline mappings
let cmdline_map_start          = '<LocalLeader>s'
let cmdline_map_send           = '<Space>'
let cmdline_map_send_and_stay  = '<LocalLeader><Space>'
let cmdline_map_source_fun     = '<LocalLeader>f'
let cmdline_map_send_paragraph = '<LocalLeader>p'
let cmdline_map_send_block     = '<LocalLeader>b'
let cmdline_map_quit           = '<LocalLeader>q'

"" vimcmdline options
let cmdline_vsplit      = 1      "" Split the window vertically
let cmdline_esc_term    = 1      "" Remap <Esc> to :stopinsert in Neovim's terminal
let cmdline_in_buffer   = 1      "" Start the interpreter in a Neovim's terminal
let cmdline_term_height = 15     "" Initial height of interpreter window or pane
let cmdline_term_width  = 80     "" Initial width of interpreter window or pane
let cmdline_tmp_dir     = '/tmp' "" Temporary directory to save files
let cmdline_outhl       = 1      "" Syntax highlight the output
let cmdline_auto_scroll = 1      "" Keep the cursor at the end of terminal (nvim)
You can also define what application will be run as the interpreter for each
supported file type. If you want to do this, create a dictionary called
cmdline_app, and add items with the 'filetype' as key and the interpreter as
value, as in the example below:
let cmdline_app           = {}
let cmdline_app['python'] = 'ptipython3'
let cmdline_app['ruby']   = 'pry'
let cmdline_app['sh']     = 'bash'
If you are using Neovim, you can use its syntax highlight capabilities to
colorize the interpreter output, and you can customize the colors in your
vimrc in three different ways:


The hex code of the foreground color.


The ANSI number of the foreground color.


The complete highlighting specification.


The example of customization below will work if either your editor supports
true colors or if it supports 256 colors (see in Neovim :h tui-colors):
if has('gui_running') || &termguicolors
    let cmdline_color_input    = '#9e9e9e'
    let cmdline_color_normal   = '#00afff'
    let cmdline_color_number   = '#00ffff'
    let cmdline_color_integer  = '#00ffff'
    let cmdline_color_float    = '#00ffff'
    let cmdline_color_complex  = '#00ffff'
    let cmdline_color_negnum   = '#d7afff'
    let cmdline_color_negfloat = '#d7afff'
    let cmdline_color_date     = '#00d7af'
    let cmdline_color_true     = '#5fd787'
    let cmdline_color_false    = '#ff5f5f'
    let cmdline_color_inf      = '#00afff'
    let cmdline_color_constant = '#5fafff'
    let cmdline_color_string   = '#5fd7af'
    let cmdline_color_stderr   = '#0087ff'
    let cmdline_color_error    = '#ff0000'
    let cmdline_color_warn     = '#c0ffff'
    let cmdline_color_index    = '#d7d787'
elseif &t_Co == 256
    let cmdline_color_input    = 247
    let cmdline_color_normal   =  39
    let cmdline_color_number   =  51
    let cmdline_color_integer  =  51
    let cmdline_color_float    =  51
    let cmdline_color_complex  =  51
    let cmdline_color_negnum   = 183
    let cmdline_color_negfloat = 183
    let cmdline_color_date     =  43
    let cmdline_color_true     =  78
    let cmdline_color_false    = 203
    let cmdline_color_inf      =  39
    let cmdline_color_constant =  75
    let cmdline_color_string   =  79
    let cmdline_color_stderr   =  33
    let cmdline_color_error    =  15
    let cmdline_color_warn     =   1
    let cmdline_color_index    = 186
endif
And the next example sets the value of an option as the complete highlighting
specification.
let cmdline_color_error = 'ctermfg=1 ctermbg=15 guifg=#c00000 guibg=#ffffff gui=underline term=underline'
If you prefer that the output is highlighted using your current colorscheme,
put in your vimrc:
let cmdline_follow_colorscheme = 1
Finally, if you want to run the interpreter in an external terminal emulator,
you have to define the command to run it, as in the examples below:
let cmdline_external_term_cmd = ""gnome-terminal -e '%s'""
let cmdline_external_term_cmd = ""xterm -e '%s' &""
where %s will be replaced with the terminal command required to run the REPL
application in a tmux session. Note that gnome-terminal does not require an
& at the end of the command because it forks immediately after startup.
Your ~/.inputrc should not include set keymap vi because it would cause
some applications to start in vi's edit mode. Then, you would always have to
press either a or i in the interpreter console before using it.
How to add support for a new language


Look at the Vim scripts in the ftplugin directory and make a copy of
the script supporting the language closer to the language that you want
to support.


Save the new script with the name ""filetype_cmdline.vim"" where
""filetype"" is the output of echo &filetype when you are editing a
script of the language that you want to support.


Edit the new script and change the values of its variables as necessary.


Test your new file-type script by running your application in either Vim
or Neovim and using either the built-in terminal or a Tmux split pane.


Look at the Vim scripts in the syntax directory and make a copy of the
script supporting the language whose output is closer to the output of
the language that you want to support.


Save the new script with the name ""cmdlineoutput_app.vim"" where ""app"" is
the name of the interpreter. For example, for the ""matlab"" file-type, the
interpreter is ""octave"".


Edit the new script and change both the pattern used to recognize the
input line and the pattern used to recognize errors.


Test your new syntax highlighting script by running your application in a
Neovim built-in terminal.


See also
Plugins with similar functionality are neoterm, vim-slime and repl.nvim.
",130
simplezhli/Saber,Java,"Saber
  
本项目帮助你快速使用LiveData与ViewModel


已适配AndroidX。


支持Kotlin。


支持 ViewModel、AndroidViewModel。（默认为 ViewModel）


支持 observe、observeForever 两种观察模式。（默认为 observe）


支持 SingleLiveEvent、MediatorLiveData、MutableLiveData。（默认为 MutableLiveData）


支持自定义LiveData类型。


支持事件总线的操作。


Forever模式自动取消订阅。


详细介绍

感受LiveData 与 ViewModel结合之美

使用方式
添加依赖
    implementation 'com.github.simplezhli.saber:saber-api:0.2.4'
    //AndroidX使用
    implementation 'com.github.simplezhli.saber:saberx-api:0.2.4'

    annotationProcessor 'com.github.simplezhli.saber:saber-compiler:0.2.4'
首先创建一个类，使用@LiveData注解标记你要保存的数据。注意这里的参数名称value，下面会用到。
public class SeekBar {

    @LiveData
    Integer value;
}
当然也可以直接标记你的JavaBean，来直接保存此类。那么参数名为类名的首字母小写：seekBar
@LiveData
public class SeekBar {

    Integer value;
}
使用@LiveData(classType = LiveDataClassType.LIST)可以指定对应的数据集合类型
Build -- > Make Project 会生成代码如下：
public class SeekBarViewModel extends ViewModel {
  private MutableLiveData<Integer> mValue;

  public MutableLiveData<Integer> getValue() {
    if (mValue == null) {
      mValue = new MutableLiveData<>();
    }
    return mValue;
  }

  public Integer getValueValue() {
    return getValue().getValue();
  }

  public void setValue(Integer mValue) {
    if (this.mValue == null) {
      return;
    }
    this.mValue.setValue(mValue);
  }

  public void postValue(Integer mValue) {
    if (this.mValue == null) {
      return;
    }
    this.mValue.postValue(mValue);
  }
}

如果想使用AndroidViewModel的话，可以添加@AndroidViewModel注解
@AndroidViewModel
public class SeekBar {

    @LiveData
    Integer value;
}
自定义LiveData类型
public class Single {

    @LiveData(type = LiveDataType.OTHER, liveDataType = XXXLiveData.class)
    Integer value;
}

生成代码提供了LiveData的常用操作。


setXXX()要在主线程中调用。


postXXX()既可在主线程也可在子线程中调用。


getXXX()用于获取观察者。


getXXXValue()可以获取保存的数据。


addSource()用于监听LiveData。(MediatorLiveData专用)


removeSource()移除监听的LiveData。(MediatorLiveData专用)


1. 普通使用方法
一般情况下可以直接使用它。比如：
public class TestFragment extends Fragment {

    private SeekBar mSeekBar;

    @BindViewModel(isShare = true) //<--标记需要绑定的ViewModel
    SeekBarViewModel mSeekBarViewModel;
    
    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState) {
        // Inflate the layout for this fragment
        View root = inflater.inflate(R.layout.fragment_test, container, false);
        mSeekBar = root.findViewById(R.id.seekBar);
        Saber.bind(this); // <--这里绑定ViewModel
        subscribeSeekBar();
        return root;
    }

    private void subscribeSeekBar() {

        mSeekBar.setOnSeekBarChangeListener(new SeekBar.OnSeekBarChangeListener() {
            @Override
            public void onProgressChanged(SeekBar seekBar, int progress, boolean fromUser) {
                if (fromUser) {
                    mSeekBarViewModel.setValue(progress);
                }
            }
			......
        });
    }

    @OnChange(model = ""mSeekBarViewModel"") //<--接收变化的ViewModel变量名
    void setData(Integer value){ //注意这里使用 @LiveData 标记的参数名
        if (value != null) {
            mSeekBar.setProgress(value);
        }
    }
}
@BindViewModel用于绑定ViewModel。
@OnChange(model = ""xxx"")用于接收指定ViewModel的数据变化，可以不设置，默认model名称为mViewModel。
如果需要Fragment之间数据共享，需要@BindViewModel(isShare = true)，当然也要保证传入相同的key值。默认key值是类的规范名称，也就是包名加类名。

所以一旦需要互通的Fragment类名或包名不一致，就无法数据共享。这时可以指定key值：@BindViewModel(key = ""value"")
2. 事件总线使用方法，详细用法参看LiveEventBus
    @OnChange(model = ""key_name"", isBus = true)
    void liveDataBus(String value){
        
    }
发送：
    LiveEventBus.get().with(""key_name"").postValue(""value"");
更多的使用方法可以参看本项目demo。
3.Kotlin环境使用注意事项
1.将以下代码添加到 build.gradle 文件中，保证生成代码的正确性。
    kapt {
        correctErrorTypes = true
    }

2.Kotlin默认会生成set/get方法，并把属性设置为private 所以只要保证Kotlin中字段可见性不是private即可，简单解决可以在字段上添加 @JvmField，也可以使用lateinit.
    @BindViewModel
    lateinit var mViewModel: TestViewModel
    
    //或
    
    @JvmField
    @BindViewModel
    var mViewModel: TestViewModel? = null
TODO
1.因为现有的@OnChange注解承载的功能过多，不易使用。后面会将EventBus功能从中提出，添加一个新的注解（或许叫做@LiveEventBus）。
2.有什么好的建议或者功能欢迎提Issues。
版本变化

点击查看

Thanks For


butterknife


在 SnackBar，Navigation 和其他事件中使用 LiveData


LiveEventBus


License
Copyright 2018 simplezhli

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

",63
conda-forge/staged-recipes,Python,"About
This repo is a holding area for recipes destined for a conda-forge feedstock repo. To find out more about conda-forge, see https://github.com/conda-forge/conda-smithy.

Build status
  
Getting started

Fork this repository.
Make a new folder in recipes for your package. Look at the example recipe, our documentation and the FAQ  for help.
Open a pull request. Building of your package will be tested on Windows, Mac and Linux.
When your pull request is merged a new repository, called a feedstock, will be created in the github conda-forge organization, and build/upload of your package will automatically be triggered. Once complete, the package is available on conda-forge.

FAQ
1. How do I start editing the recipe?
Look at one of these examples
in this repository and modify it as necessary.
Your final recipe should have no comments and follow the order in the example.
If there are details you are not sure about please open a pull request. The conda-forge team will be happy to answer your questions.
2. How do I populate the hash field?
If your package is on PyPI, you can get the sha256 hash from your package's page on PyPI; look for the SHA256 link next to the download link for your package.
You can also generate a hash from the command line on Linux (and Mac if you install the necessary tools below). If you go this route, the sha256 hash is preferable to the md5 hash.
To generate the md5 hash: md5 your_sdist.tar.gz
To generate the sha256 hash: openssl sha256 your_sdist.tar.gz
You may need the openssl package, available on conda-forge:
conda install openssl -c conda-forge
3. How do I exclude a platform?
Use the skip key in the build section along with a selector:
build:
    skip: true  # [win]
A full description of selectors is in the conda docs.
Additionally, when pushing commits for a recipe that excludes Windows, put [skip appveyor] in the commit message to prevent CI tests
on Windows from even starting.
4. What does the build: 0 entry mean?
The build number is used when the source code for the package has not changed but you need to make a new
build. For example, if one of the dependencies of the package was not properly specified the first time
you build a package, then when you fix the dependency and rebuild the package you should increase the build
number.
When the package version changes you should reset the build number to 0.
5. Do I have to import all of my unit tests into the recipe's test field?
No, you do not.
6. Do all of my package's dependencies have to be in conda(-forge) already?
Short answer: yes. Long answer: In principle, as long as your dependencies are in at least one of
your user's conda channels they will be able to install your package. In practice, that is difficult
to manage, and we strive to get all dependencies built in conda-forge.
7. When or why do I need to use {{ PYTHON }} -m pip install --no-deps . -vv?
This should be the default install line for most Python packages. This is preferable to python setup.py because it handles metadata in a conda-friendlier way. We also want to make sure dependencies are handled through conda, and --no-deps means most Python dependencies are needed only at run time, not build.
8. Do I need bld.bat and/or build.sh?
In many cases, no. Python packages almost never need it. If the build can be done with one line you can put it in the script line of the build section.
9. What does being a conda-forge feedstock maintainer entail?
The maintainers ""job"" is to:

keep the feedstock updated by merging eventual maintenance PRs from conda-forge's bots;
keep the package updated by bumping the version whenever there is a new release;
answer eventual question about the package on the feedstock issue tracker.

10. Why are there recipes already in the recipes directory? Should I do something about it?
When a PR of recipe(s) is ready to go, it is merged into master. This will trigger a CI build specially designed to convert the recipe(s). However, for any number of reasons the recipe(s) may not be converted right away. In the interim, the recipe(s) will remain in master until they can be converted. There is no action required on the part of recipe contributors to resolve this. Also it should have no impact on any other PRs being proposed. If these recipe(s) pending conversion do cause issues for your submission, please ping conda-forge/core for help.
11. Some checks failed, but it wasn't my recipe! How do I trigger a rebuild?
Sometimes, some of the CI tools' builds fail due to no error within your recipe. If that happens, you can trigger a rebuild by re-creating the last commit and force pushing it to your branch:
# edit your last commit, giving it a new time stamp and hash
# (you can just leave the message as it is)
git commit --amend
# push to github, overwriting your branch
git push -f
If the problem was due to scripts in the staged-recipes repository, you may be asked to ""rebase"" once these are fixed. To do so, run:
# If you didn't add a remote for conda-forge/staged-recipes yet, also run
# these lines:
# git remote add upstream https://github.com/conda-forge/staged-recipes.git
# git fetch --all
git rebase upstream/master
git push -f
12. My pull request passes all checks, but hasn't received any attention. How do I call attention to my PR?  What is the customary amount of time to wait?
If your PR is passing all checks, but has not been acted on by the staged recipes
maintainers, you can ping @conda-forge/staged-recipes to request action. You do
not need to wait any specific amount of time once the recipe is ready to go.
If your recipe still does not recieve any attention after a few days, you may
attempt to re-ping @conda-forge/staged-recipes. You may also attempt to bring
the PR up in our Gitter chat room at https://gitter.im/conda-forge/conda-forge.github.io
All apologies in advance if your recipe PR does not recieve prompt attention.
This is a high volume repository and issues can easily be missed. We are always
looking for more staged-recipe reviewers. If you are interested in volunteering,
please contact a member of @conda-forge/core. We'd love to have the help!
13. How to build with old compilers (GCC v4) on staged-recipes?
First, don't. Second, please don't.
Add a conda_build_config.yaml file inside the recipe folder with the contents
channel_sources:
- conda-forge/label/cf201901,defaults   # [unix]
- conda-forge,defaults                  # [win]
channel_targets:
- conda-forge cf201901                  # [unix]
- conda-forge main                      # [win]
c_compiler:                             # [unix]
- gcc                                   # [linux]
- clang                                 # [osx]
cxx_compiler:                           # [unix]
- gxx                                   # [linux]
- clangxx                               # [osx]
fortran_compiler:                       # [unix]
- gfortran                              # [unix]
",244
1185430411/ds-leetcode,C++,"ds-leetcode
leetcode的数据结构算法题刷题笔记
",2
jhonatadam/cacador-de-particulas,C#,"cacador-de-particulas
",2
guanguans/favorite-link,Python,"favorite link
收集喜欢的网址
License
GNU General Public License v3.0
May 17, 2019

CLI PHP的PHP版本管理器
React的高度可定制和多功能的GraphQL客户端
🌊灵活有趣的JavaScript文件上传库

May 16, 2019

一键部署Shadowsocks服务；免费Shadowsocks账号分享；免费SS账号分享; 翻墙 
PHP安全SDK及编码规范
.net反序列化课程中提到的漏洞检测工具
K8工具合集(内网渗透/提权工具/远程溢出/漏洞利用/扫描工具/密码破解/免杀工具
大型内网渗透自定义插件化扫描器（附C#/VC/Delphi/Python插件Demo源码) 程序采用多线程批量扫描大型内网多个IP段C段主机，目前插件包含: C段旁注扫描、子域名扫描、Ftp密码爆破、Mysql密码爆、系统密码爆破、存活主机扫描、端口扫描、Web信息探测、操作系统版本探测、Cisco思科设备扫描等,支持调用任意外部程序或脚本
RediSearch的PHP客户端库。
在Kubernetes建立容器图像
用Go语言实现PHP内置函数 
音乐搜索器 - 多站合一音乐搜索解决方案
劳动仲裁和劳动诉讼的攻略

May 15, 2019

Lua 简明教程
为Electron实施Chrome扩展API
国内第一套 Clojure 视频课程，Let Lisp Rocks ! 
一个逗比写的各种逗比脚本~
Consul是一种分布式，高可用性和数据中心感知解决方案，用于跨动态分布式基础架构连接和配置应用程序。
《机器学习训练秘籍》
Gin Weibo App
编译为Lua的语言

May 14, 2019

将Laravel框架进行扩展配置处理，让其在开发API时更得心应手。
基于Yii2制作的电子商城
列出长期996后容易遭遇的职业病，为职业病防治法提供立法依据
用于保存支付宝账单的Xposed库
Alfred 3工作流程，可以非常精确地搜索和终止进程
BookStack，基于MinDoc，使用Beego开发的在线文档管理系统，功能类似Gitbook和看云。
vue.js(element框架)+golang(beego框架)开发的运维发布系统,支持git,jenkins版本发布,go ssh,BT两种文件传输方式选择,支持部署前准备任务和部署后任务钩子函数
Tpay是微信和支付宝最强的个人免签全自动回调支付系统
PHP的现代任务运行器
纯PHP实现的ECDH库
基于Docker的LAMP开发环境
扩展为PHP 7提供高效的数据结构
web指纹识别
TideFinger——指纹识别小工具，汲取整合了多个web指纹库，结合了多种指纹检测方法，让指纹检测更快捷、准确。
即使不用饿了么订餐，也请务必收藏好该库！🔥 一行代码即可接入，二级联动订餐列表
mysql 启发式规则建议
PHP GIF编码器和解码器
一个很棒的Flutter学习资源，官方教程，插件，工具，文章，App，视频教程等的资源列表 
PHP源码加密模块
Yay是一个高级PHP预处理器
Minime \ Annotations是第一个KISS PHP注释库。

May 13, 2019

拥有 218 种格式支持和丰富的 API 接口
GitHub秘籍
Windows To Go
一种可自定义的GraphQL样式查询语言，用于与JavaScript对象进行交互。
SQL浏览器（DB4S）项目的官方主页。以前称为“SQLite数据库浏览器”和“SQLite数据库浏览器”。网站：sqlitebrowser.org
一个用Rust编写的业余爱好者微内核，具有类似于seL4的基于功能的系统。
一系列开源和商业工具，有助于红队运作。
一本专注于Go语法/语义的在线书籍。
MSSQL注入提权,bypass的一些总结
Plates是一个本机PHP模板系统，它快速，易于使用且易于扩展。
使用google translate api将翻译文件翻译成其他语言
免费的Google Translate API PHP包。翻译完全免费。
PHP类免费使用Google Translator API。
一个更好的`npm发布'
CLI中基于Redis文本的UI客户端
Cobalt Strike 渗透神器
灵活而强大的Python数据分析/操作库，提供类似于R data.frame对象，统计函数等标记数据结构
PHP中的高性能Web框架和应用程序服务器。
awesome-go-China
哔哩哔哩开放接口第三方文档（仅提供官方曾公开接口）
一个PHP端口的Gibberish AES javascript加密库
一个基于React的Web工具包
使用OpenSSL扩展的PKI公共/私有RSA密钥加密

May 12, 2019

显示和控制您的Android设备
Tpay_Svr是微信和支付宝的个人免签 24小时全自动回调支付系统的php服务端程序 
微信/支付宝二维码支付监听器 - 免签接口 指母付/钉钉支付/个人二维码收款系统/微信支付支付宝支付二维码监听自动发货/个人免签系统/个人免签支付 微信.支付宝 个人支付监控 
欢迎使用 composer 免签约支付宝与微信 带监听
暗网导航
通过SSH在任何服务器上运行VS Code。
FTP，SFTP，WebDAV，Git，S3，Minio，LDAP，Caldav，Carddav，Mysql，Backblaze的Web客户端

May 11, 2019

BeEF是The Browser Exploitation Framework的缩写。它是一种专注于Web浏览器的渗透测试工具。
Content Security Policy 入门教程
详细介绍如何使用 CSP 防止 XSS 攻击。
非常简单的点击复制CSS效果
英特尔公司
下一代对象关系数据库。一个@MagicStack项目。
PowerToys是一组用于高级用户调整和简化其Windows体验以提高工作效率的实用程序。
Overlord是哔哩哔哩基于Go语言编写的memcache和redis&cluster的代理及集群管理功能，致力于提供自动化高可用的缓存服务解决方案。 bilibili.com
Thruway是用于PHP的WAMP（Web应用程序消息传递协议）的开源客户端和路由器实现。Thruway使用事件驱动的非阻塞I / O模型（reactphp），非常适合现代实时应用。
适用于移动腾讯分析HTML5的PHP SDK
Symfony编码标准的开发存储库
使用php开发实现webdav协议的项目
🛁 PHP版的代码整洁之道 中文翻译
爆破字典
TensorFlow的Rust语言绑定
Yii2 Audit记录并显示web / cli请求，数据库更改，php / js错误和相关数据。
基于Docker快速搭建Gitlab与Gitlab CI/CD服务
Laravel Envoy提供了一种简洁的语法，用于定义您在远程服务器上运行的常见任务。使用Blade样式语法，您可以轻松设置部署任务，Artisan命令等。
V8 JavaScript引擎的PHP扩展
RESTful API 设计参考文献列表，可帮助你更加彻底的了解REST风格的接口设计。
JWT Framework
带有解析器的HTML5视频播放器，可以节省流量
字节跳动
Chrome运行时性能瓶颈分析
Vue.js剪贴板库（无依赖关系，小于2kb）
在浏览器中使用同类最佳的编解码器缩小图像。
K8s本机管道资源。
go程序热编译工具，提升开发效率
用React、Redux、Immutable做俄罗斯方块
用Vue、Vuex 做俄罗斯方块
PhpStorm Plugins-排序 use 包名称
使用Docker和docker-compose运行Symfony应用程序

May 10, 2019

OpenMV相机模块
在本笔记本中，我将创建一个预测股票价格变动的完整流程。 跟着我们，我们将取得一些非常好的结果。
HQ开放数据集的以主题为中心的列表。
✍️Fusuma轻松制作带有Markdown的幻灯片。
李笑来-挤挤都会有的 - 写给女生的性高潮指南
一个简单，轻量级的JavaScript API，用于处理浏览器cookie
将您的ascii图表涂鸦转换为快乐的小SVG
远程管理工具
本文原文由知名Hacker Eric S. Raymond 所撰写，教你如何成为一名黑客。
🖍终端字符串样式正确完成
使用盲水印保护创作者的知识产权
Golang的惯用HTTP中间件
社区Bash框架。
用于格式化和操作数字的JavaScript库。
这是在Node.js中构建CLI的框架。 此框架是使用Heroku CLI构建的，但通用于构建任何自定义CLI。
cvpr2019 papers，极市团队整理 
使用GitHub和Slack集成将您的代码带到您关心的对话中
Vim 从入门到精通
IPFS的桌面客户端。
PHP中IPFS API的客户端库
Generatedata是一个免费、开放源码的脚本，主要由javascript ， PHP和MySQL构成，它可以让您可以迅速生成大量各种格式的客户数据，用于测试软件，把数据输入数据库等。
创建使用虚假数据填充的自定义测试数据库
8个不错的随机生成数据库测试数据的利器 - 我就是snail啦 - OSCHINA
一个开源工具，用于从MySql数据库生成完整的后端。
Docker打包版本的benkeen / generatedata项目
😍Web前端助手--FeHelper

May 9, 2019

Cloud-Admin是国内首个基于Spring Cloud微服务化开发平台，具有统一授权、认证后台管理系统，其中包含具备用户管理、资源权限管理、网关API管理等多个模块，支持多业务系统并行开发，可以作为后端服务的开发脚手架。
使用Headless Chrome将javascript呈现的页面呈现为HTML的节点服务器。与prerender中间件一起使用。
一份网络安全入门的资料。
ARP+DNS欺骗工具，网络安全第三次实验，课堂演示用，严禁非法用途。ARPSpoof，wifi hijack，dns spoof
Kubernetes中文指南/云原生应用架构实践手册 
Vuido是一个基于Vue.js创建本机桌面应用程序的框架。使用Vuido的应用程序可以在Windows，OS X和Linux上运行，使用本机GUI组件，不需要Electron。
React in patterns 中文版 
🔍 让我帮你百度一下？
《Mastering GO》中文译本，暂时命名为《玩转 GO》。
Docker Machine的VMWare Workstation驱动程序
功能齐全的BitTorrent客户端软件包和实用程序
GitLab API客户端，使Go程序能够以简单统一的方式与GitLab交互
Golang的HTTP模拟
《GO专家编程》
将您的Flutter代码带到Web浏览器
一个真正可怕的异步网络聊天，在前端没有使用任何JS
使用React构建本机Windows应用程序的框架。
桌面自动化框架
m4b-tool是一个命令行实用程序，用于合并，拆分和分章有声读物文件，如mp3，ogg，flac，m4a或m4b
Laravel项目的自动代码格式
10 分钟掌握 MySQL 的索引查询优化技巧
我们发布了一个新的项目—Crypko。在Crypko中，你可以从以太坊区块链上获取并交易AI生成的二次元角色。
使用MakeGirlsMoe创建动画角色
MyFlash是由美团点评公司技术工程部开发维护的一个回滚DML操作的工具。该工具通过解析v4版本的binlog，完成回滚操作。相对已有的回滚工具，其增加了更多的过滤选项，让回滚更加容易。 该工具已经在美团点评内部使用
Laravel best practices
用于学习算法并在任何编程语言中实现它们

May 8, 2019

Github用户排名
CockroachDB  - 开源的云原生SQL数据库。
go.rice是一个Go包，它可以很容易地处理html，js，css，图像，模板等资源。
将本地服务器公开给外部网络
Laravel电子商务套餐，适用于专业，超快的在线商店，复杂的B2B应用程序和#gigacommerce
Rust通道的多进程直接替换
用于从Google Analytics检索综合浏览量和其他数据的Laravel程序包
微信官方 js-sdk CommonJS 版本
华丽的应用程序，纠正您以前的控制台命令。
免费购物车系统。OpenCart是一个基于PHP的开源在线电子商务解决方案。
史上最全的PyTorch学习资源汇总
分布式服务框架Zookeeper -- 管理分布式环境中的数据 
🚇 即刻 Ⓙ SDK 
优化即刻网页版体验的 Chrome 插件 
简单的错误处理原语
为方便WAF入库的项目 | 分享PHP免杀大马 | 菜是原罪 | 多姿势(假的就一个)
干将是一个自动化插件式网络漏洞检测系统
ThinkCMF电商版，基于ThinkCMF5.1开发
Honukai主题和颜色哦我的ZSH和iTerm
cos-php-sdk-v5
又拍云 SDK for PHPer

May 7, 2019

分布式知识图存储
phpEnv一款优雅强大的php集成环境
Symfony与Swoole集成以加速您的应用程序。
Php-webdriver库是Selenium WebDriver的PHP语言绑定，它允许您从PHP控制Web浏览器。
Golang优雅的刮板和抓取器框架
子豪兄的零基础树莓派教程，代码存放地及更新勘误
vue.js(element框架)+golang(beego框架)开发的运维发布系统,支持git,jenkins版本发布,go ssh,BT两种文件传输方式选择,支持部署前准备任务和部署后任务钩子函数
Wizard是基于Laravel开发框架开发的一款开源项目（API）文档管理工具。
提供在Linux上运行最新版腾讯QQ与TIM的解决方案
该存储库包含我的O'Reilly书籍Flask Web Development第二版的源代码示例。
AnyBar是您的菜单栏的一个小指示器，可以做一件简单的事情：它显示一个彩色圆点。点的含义以及何时更改它取决于您。
编写和优化Go代码
SQLFlow是连接SQL引擎的桥梁，例如MySQL，Hive，SparkSQL或SQL Server，带有TensorFlow和其他机器学习工具包。SQLFlow扩展了SQL语言，以支持模型训练，预测和推理。
工匠菜单 - 通过优雅的控制台GUI使用Artisan
ShopXO商城系统、国内领先企业级B2C免费开源电商系统，包含PC/wap/小程序
中国首个开源学校教务管理系统、网站布局自动化、学生/成绩/教师、成绩查询 
从供应商目录中删除测试和文档并缩小所有php文件
BladeOne是刀片模板引擎的独立版本，它使用单个PHP文件，可以移植并在不同的项目中使用。它允许您在Laravel外部使用刀片模板。
docker-slim：自动缩减 docker 镜像的体积的工具。大幅度缩减 docker 镜像的体积，方便分发，使用命令 docker-slim build --http-probe your-name/your-app。比如 Node.js 镜像缩减后的对比： from ubuntu:14.04 - 432MB => 14MB (缩减了 30.85 倍)
小霸王，其乐无穷 。红白机，FC在线游戏
简悦 ( SimpRead ) - 让你瞬间进入沉浸式阅读的扩展
gh-ost是MySQL的无触发在线模式迁移解决方案。
利用HTTP协议 远程加解密数据包，实现Burp一条龙服务。
api-standards
API设计指南
Standard Readme
中华人民共和国密码行业标准(GM/T)文本
支持国密SM2/SM3/SM4/SM9/ZUC/SSL的OpenSSL分支
pt-query-digest
README文件语法解读，即Github Flavored Markdown语法介绍

May 6, 2019

ntp包是基于RFC5905的Simple NTP（SNTP）客户端的实现。它允许您连接到远程NTP服务器并请求有关当前时间的信息。
将Word文档（.docx文件）转换为HTML
kunpeng是一个Golang编写的开源POC框架/库，以动态链接库的形式提供各种语言调用，通过此项目可快速开发漏洞检测类的系统。
vfsStream是虚拟文件系统的流包装器，可能有助于单元测试模拟真实文件系统。它可以与任何单元测试框架一起使用，如PHPUnit或SimpleTest。
一个PHP快速CGI客户端，用于同步向PHP-FPM发送请求（a）
不同行业的应用机器学习和数据科学笔记本和图书馆的精选列表。
即刻黄历macOS Saver
福利直通车
集合多家 API 的新一代图床 
UrlHum是一个使用PHP和Laravel Framework构建的现代，隐私和快速URL Shortener
laravel telescope 是Laravel框架的优雅调试助手。Telescope可深入了解进入应用程序的请求，异常，日志条目，数据库查询，排队作业，邮件，通知，缓存操作，计划任务，变量转储等。望远镜是您当地Laravel开发环境的绝佳伴侣。
commit 提交指南

May 5, 2019

各种xlsx的分析器和编写器。
使用Wordpress和WooCommerce构建高度可定制的电子商务网站，销售shadowsocks服务
FydeOS - 面向未来的云驱动操作系统 | 为中国用户打造的 Chrome OS
Quicker-window自动化软件
Quicker IOS 客户端
CC助手 - 超越剪贴板

May 4, 2019

GraphQL 开发原则
通过思维导图整理redis的重要知识点
用于从Google Analytics检索综合浏览量和其他数据的Laravel程序包
slimdump是一个小工具，可以帮助您创建大型MySQL数据库的可配置转储
首款微信 macOS 客户端撤回拦截与多开 
上最棒的开源 Win10 数字权利（数字许可证）激活工具！ 

May 3, 2019

易于上手的样品参考微服务和基于容器的应用。 Linux和Windows Docker容器上的跨平台，由.NET Core 2.2，Docker引擎以及Azure，Kubernetes或Service Fabric提供支持。
Surface Pro 6 黑苹果全球首发
一个静态网站生成器，用于使用Vue.js构建超快速的网站
供 Dash 使用的中文文档
Dash中文文档
go 语言框架 gin 的中文文档
快速开发macOS PHP开发环境
快速，评论，节点的极简主义Web框架。
深入理解Node.js：核心思想与源码分析
Vue.docset dash 文档 Vue 离线文档中文版
leetcode题解，记录自己的leetcode解题之路。
一个基本的游戏模拟器，支持终端“云游戏”

May 2, 2019

LeanCloud 发布的 Git Commit 日志风格指南。
动漫场景按图搜索
类似unix的逆向工程框架和命令行工具
测试可能是拖累。 AVA可以帮助您完成任务。 AVA是Node.js的测试运行器，具有简洁的API，详细的错误输出，新语言功能的拥抱和进程隔离，可让您更有效地编写测试。
FreeRDP是一个免费的远程桌面协议库和客户端
管理hosts的更好方法。
mysql注入,bypass的一些心得

May 1, 2019

精通以太坊 （中文版）
Cloud-Native API网关
从VS Code调试在Google Chrome中运行的JavaScript代码。
when当我键入kubectl run时会发生什么？
用于收集和报告指标的插件驱动的服务器代理。
一个经典的XSS渗透管理平台
Mac版微信的功能拓展
如何用GitHub Actions编译Golang项目

April 30, 2019

备忘单
200+ Mac菜单栏应用程序
一个简单的macOS应用程序，用于监控云服务的状态
Python中的简单区块链
Sortable是一个用于可重新排序的拖放列表的JavaScript库。
💬使用Google Translate，Bing Translator，Yandex.Translate等的命令行翻译器
引导您的用户浏览您的应用
Traefik是一个现代HTTP反向代理和负载均衡器，可以轻松部署微服务。Traefik与您现有的基础架构组件（Docker，Swarm模式，Kubernetes，Marathon，Consul，Etcd，Rancher，Amazon ECS ......）集成，并自动动态配置。在您的协调器上指向Traefik应该是您需要的唯一配置步骤。
SeaweedFS是一个简单且高度可扩展的分布式文件系统。有两个目标：存储数十亿个文件！快速提供文件！SeaweedFS实现了一个带有O（1）磁盘搜索的对象存储和一个带有POSIX接口的可选Filer，支持S3 API，FUSE挂载，兼容Hadoop。
Uber Technologies公开发布的分布式跟踪系统。 它可用于监视基于微服务的分布式系统
🎉全面的PHP客户端软件包，用于使用Hubtel Payment API
现代HTTP基准测试工具
Java资源大全中文版，包括开发库、开发工具、网站、博客、微信、微博等，由伯乐在线持续更新。
所有算法都在Python中实现
Cyber​​netically增强的网络应用程序
一个高级web目录扫描工具，功能将会强于DirBuster、Dirsearch、cansina、御剑。
一个外观漂亮且易于使用的照片管理系统，您可以在服务器上运行，管理和共享照片。
url2pdf基于中文支持wkhtmltopdf的docker镜像。
此程序包允许您管理数据库中的用户权限和组。
☁️LskyPro，您在云端的相册。
✨另一个OneDrive目录索引
Scoop bucket 用于开源/免费软件游戏和游戏相关工具
scoop-extras

April 29, 2019

PHP MySQL类的包装器，它使用MySQLi和预处理语句。
收集&推荐优秀的 Apps/硬件/技巧/周边等

April 28, 2019

一种查看和导航目录树的新方法
基于workerman的分布式实时消息传递框架。
Web安全学习笔记

April 27, 2019

微信支付宝个人免签收款Api系统，有了它对接再也不用担心我的业务不能支付了。
基于swoft-cloud的微服务架构，最小化拆分粒度，PHP7、多进程、协程、异步任务、mysql连接池、redi连接池、rpc连接池、服务治理、服务注册与发现、Aop切面、全注解
yq是一个便携式命令行YAML处理器
《深入解析Go》
这本书是来自walu的phpbook升级版
适用于macOS的更好的Cmd-Tab（OSX）
适用于您可爱网站的CSS唯一工具提示库。
在非Laravel应用程序中使用每个Illuminate组件的示例
🚀Chrome/ Firefox Extension可以从WebSite中搜索RSS源URL，支持RSSHub

April 26, 2019

文档和源代码PopClip扩展。
用C编写的通用数据结构和算法库。
编译器工具包。对于PHP（是的，我在命名事物上很有创意）......
如何搜索和阅读一篇论文 
qq群和soyun社工库的查询sql和索引存储过程
一个小型的较为简陋的社工库查询系统,使用Vue.js+Flask+MongoDB.可用52g, 本人不提供任何社工库资源. 
搜呼社工库
社工库查询系统
PHP的现代任务运行器
在 2019 成为一名 Go 开发者的路线图。为学习 Go 的人而准备。
💗在chrome扩展中写博客并推送到github pages

April 25, 2019

用于个人用途的部署机器人。
《Koa2进阶学习笔记》已完结🎄🎄🎄
小白入门学习vue和vue实例，vue总结
HTTP负载生成器，ApacheBench（ab）替换，以前称为rakyll / boom
基于Golang解决的长连接并发服务器框架
Go中快速，结构化，水平的日志记录。
electron builder 
切勿再次使用打印进行调试
用于辅助安全工程师漏洞挖掘、测试、复现，集合了mock、httplog、dns tools、xss，可用于测试各类无回显、无法直观判断或特定场景下的漏洞。
从生产中不需要的文件中清除PHP Composer供应商文件夹
tj/node-prune: Remove unnecessary files from node_modules (.md, .ts, ...)从node_modules（.md，.ts，...）中删除不必要的文件
Github 项目活跃度分析工具
lanproxy是一个将局域网个人电脑、服务器代理到公网的内网穿透工具，目前仅支持tcp流量转发，可支持任何tcp上层协议（访问内网网站、本地支付接口调试、ssh访问、远程桌面...）。
Windows 10的macOS Mojave Dynamic Desktop功能端口

April 24, 2019

最小的Markdown编辑器桌面应用程序
tshark的终端UI，灵感来自Wireshark
基于GitHub问题的评论系统。
分享一些黑苹果clover配置文件
黑客神器，谁用谁知道！
⛓区块链应用框架✨
Vue.js驱动的现代网站生成器
写代码时，持续思考这段代码能不能很容易的写出单元测试，能够大幅度提高最终产出的代码质量(即使实际上没写单元测试...)
用JavaScript编写的HTML to Markdown转换器
HackMD
Github味道Markdown编辑器。
创建基于Composer的Phar of PHP应用程序
bin依赖项没有冲突
Brook是一个跨平台（Linux / MacOS / Windows / Android / iOS）代理/ VPN软件

April 23, 2019

php导出导入excel
A LotServer KeyGen
Bluecherry监控系统（服务器应用）
Kratos是bilibili开源的一套Go微服务框架，包含大量微服务相关框架及工具。
swoole source reading 源码分析

April 22, 2019

wtfpython的中文翻译/施工结束/ 能力有限，欢迎帮我改进翻译
🖌docsify cli工具 - 一个神奇的文档生成器。
使用node.js的OAuth2.0的一个非常简单的演示
FastAPI框架，高性能，易学，快速编码，随时可用于制作
基于纯PHP实现的DH库，用于服务器端
dogeTV for macOS
Commit messages guide
一个专注于隐私，可扩展和美丽的Web浏览器
2019年成为Go开发者的路线图
一个带有调试工具的python vm注入器，基于gdb。
mathdroid (Odi)
将您的Python和Javascript代码转换为DOT流程图
轻松地将图像组合在一起，而不会弄乱画布

April 21, 2019

nicedoc.io是一个表示层，用于美化github.com上托管的任何doc文件。
利用思维导图整理的docker知识点
开发效率提升：Mac生产力工具链推荐
中国5级行政区域mysql库
msgpack.org [PHP]
Übersicht
一个漂亮的终端仿真器，模仿旧的阴极显示器......
Go设计模式的精选列表
 Full-featured code intelligence and smart autocomplete for Sublime Text
对象存储PHP简易代码，支持简单上传，删除，列表功能。支持网易，腾讯，七牛，UCloud四家API

April 20, 2019

Chrome扩展: 告别跨域，定制HTTP请求响应头
使用相同的Laravel安装运行多个网站，同时保持租户特定数据分离，以实现完全独立的多域设置

April 19, 2019

PHP中的BDD
wordpress theme rizhuti 日主题 不要钱 随意用

April 18, 2019

这是GitHub Actions main.workflow文件的语言规范和官方解析器。它作为GitHub Actions的一部分在生产中运行。它是用Go编写的。
使每个人都能够构建异步软件
z  - 跳来跳去
与GitHub API v3键入的交互
laravel-admin使用iframe-tab打开多页面
一个新鲜和轻量级的JavaScript游戏引擎
Laravel idcard验证器
ripgrep递归地在目录中搜索正则表达式模式
👊 Java Android 近几年最全面的技术点以及面试题 供自己学习使用
Laravel Eloquent模型的特性，可以让您克隆模型及其关系，包括文件。甚至到另一个数据库。
验证表单异步
一个全栈增长工程师的练手项目集. 
[包]用于包开发的Laravel Testing Helper
为Laravel开发测试助手
Frps 一键安装脚本，Frpc Windows 便捷脚本！Frp 远程桌面！
Linux 平台下基于 Rust + GTK 开发的网易云音乐播放器
Siler是一组通用的高级抽象，旨在用于PHP中的声明性编程API。
又一个 swoole
清理不值得信任的HTML用户输入
🐳 WeChat Playground - 开源微信调试工具
PHP设计模式的使用
PHP安全配置检查器

April 17, 2019

从混乱的网络中提取内容。
跨平台，GPU加速的终端仿真器
前端性能监控系统,消息队列,高可用,集群等相关架构
xhprof: PHP7 support
高效的基于令牌桶的速率限制器包。
用于构建Kubernetes应用程序的SDK。提供高级API，有用的抽象和项目脚手架。
Go包用于轻松呈现JSON，XML，二进制数据和HTML模板响应。
Beats是用Go编写的轻量级数据发送器，可以安装在服务器上以捕获各种操作数据（考虑日志，指标或网络数据包数据）。Beats直接或通过Logstash将操作数据发送到Elasticsearch，因此可以使用Kibana进行可视化。
Go编程语言的自动完成守护程序
适用于Laravel 5.x的SweetAlert2
基于Web的文件管理器在单个PHP文件中，使用Tiny File Manager高效，轻松地管理文件
以前所未有的速度构建Laravel Web应用程序，易于安装，无需定制。
姬长信API一个开源免费不限制提供生活常用,出行服务,开发工具,金融服务,通讯服务和公益大数据的平台 
《Chrome插件开发全攻略》配套完整Demo
一个轻量的工具集合
Python - 100天从新手到大师
SOLID原则 - 简单易懂的解释
使用Docker运行Laravel项目并使用Kubernetes进行部署
接口和抽象类 - 简单易懂的解释
✨用于macOS的Finder工具栏应用程序，用于打开Terminal，iTerm或Hyper中的当前目录。

April 16, 2019

PHP-FPM状态页面CLI
DH算法的API端，DH是一种利用非对称协商对称密钥的交换算法，他避免了对称密钥于公网来回传递的问题。
一个主要用于信息搜集的工具集，主要是用于对网站子域名、开放端口、端口指纹、c段地址、敏感目录等信息进行批量搜集。
用户友好的命令行shell。
💻 计算机速成课 | Crash Course 字幕组 (全40集 2018-5-1 精校完成)
自动完成软件发布的繁琐任务。愉快地发布和发布您的Git存储库，npm包，GitHub和GitLab版本，更改日志等等！
🙈实现过滤敏感词汇🔞，基于确定有穷自动机(DFA)算法，支持composer安装扩展
Golang的实战项目，学习笔记，代码例程汇总。
适用于Yii2应用程序的OAuth2包装器
Yii2 Webpack2资产管理
一组匹配中国大陆手机号码的正则表达式。
zipline 是开源量化平台，但是当前zipline 并不支持A股的测试，很多在线平台如优矿，聚宽等都是基于zipline，本项目改进zipline，使得zipline支持A股测试

April 15, 2019

一个极简内存池实现
 迅雷快鸟 Linux 版 
Squeezer Framework  - 构建无服务器的dApp
具有AutoCompletion和语法突出显示的MySQL终端客户端。
Swoole 开发的MySQL数据库连接池
基于爬虫的web漏洞扫描器
统计GitHub上文件的代码行数
saber / http库提供了处理http请求和响应的实用程序。
Laravel Repositories是Laravel 5的一个包，用于抽象数据库层。这使应用程序更容易维护。
依赖注入容器
用于InnoDB文件格式的解析器，在Ruby中

April 14, 2019

用于更改Rust的RFC
🌸命令行模糊查找器
Rust协议缓冲区的Rust实现
用于跟踪应用程序日志的artisan命令
💾用于Qiniu存储的Flysystem适配器。

April 13, 2019

微信支付宝个人免签收款系统
基于令牌桶算法和漏桶算法来实现的限速限流，Golang实现。
Parsedown的Markdown额外扩展
PHP DataMapper and ORM
Passbolt CE后端，一个用Cakephp编写的JSON API

April 12, 2019

关于视觉，文本，强化学习等中的pytorch的一组示例
人性化的API设计
ManaPHP Framework
用于使用CSS绘制图案的Web组件。
Google日志记录模块的C ++实现
DokuWiki开源Wiki引擎
基于MDX的演示文稿
通往计算机科学的免费自学教育之路！
Markdown 语法简体中文版
Rust每日新闻
此组件提供PHP 7.3之前的版本中不可用的功能。
PHP的FastCGI（FCGI）协议实现
🌈 Mix CLI — 让 PHP 像 Golang 一样开发命令行程序 (单执行文件)
【新】微信服务号+微信小程序+微信支付+支付宝支付
Git源代码镜像
Docker Desktop for Windows的错误报告

April 11, 2019

HTML5桌面应用程序开发通用工具
有关如何在keybase.io上创建GPG密钥的分步指南，将其添加到本地GPG设置并与Git和GitHub一起使用。
Keybase Go Library, Client, Service, OS X, iOS, Android, Electron
由前googlers，为前googlers  - 类似技术和服务的查找表
一个想帮你总结所有类型的上传漏洞的靶场
Laravel核心代码学习
连接，保护，控制和观察服务。
一个连接，管理和保护微服务的开放平台。
百宝门跨域单点登录(SSO)教程对应的示例完整代码
docker 教程 t.me/dockertutorial
Go语言实战: 编写可维护Go语言代码建议
996.ICU chrome 插件
Vanilla是一个功能强大的简单论坛，您可以轻松自定义，使其与您的社区一样独特。

April 10, 2019

轻松下载markdown文件中的所有图像
令人愉快且以性能为中心的纯CSS加载动画。
Crypto 101，关于密码学的入门书。
JavaScript 算法与数据结构
可以放在文档头部的所有内容的列表
一个基于Composer的简单PHP框架，看起来像一个小小的Laravel。
🔍基于一个唯一字符串生成数据库查询
人类的Python开发工作流程。
PHP中的一个简单的Podcast RSS编辑器
一个简单的PHP GitHub API客户端
go的现代文本索引库
✨强大、优雅的小程序异步库🚀 小程序promise
Apache Airflow
使用Vue CLI 3和Laravel的示例项目
饥人谷出品：一个会动的简历。
整理了一些在开发或学习过程中写的代码片段，并进行简单分类。
整理了一些在开发或学习过程中写的代码片段，并进行简单分类。
文件上传包含验证和存储策略
最初基于Carbon的独立DateTime库
允许轻松地将Ws Security标头添加到SOAP请求中
AutoRoute: 自动将HTTP请求映射到PHP操作类。
Query Translator是一个具有AST表示的搜索查询转换器
Jeff的算法手册，笔记等的错误跟踪
在GitHub上自动从标签，问题，标签和拉取请求生成更改日志。
如何维护更新日志
如果您构建软件，请保留更改日志。
PHP安全配置检查器
TinyLara/framework
JavaScript动画引擎

April 9, 2019

YApi 是一个可本地部署的、打通前后端及QA的、可视化的接口管理平台
YDoc 是一个更懂你的文档站构建工具，基于 markdown 轻松生成完整静态站点
WebAsssembly的独立JIT样式运行时，使用Cranelift
WebAsssembly的独立JIT样式运行时，使用Cranelift
构建Active Directory域并进行黑客攻击
Traefik是一个现代HTTP反向代理和负载均衡器，可以轻松部署微服务。Traefik与您现有的基础架构组件（Docker，Swarm模式，Kubernetes，Marathon，Consul，Etcd，Rancher，Amazon ECS ......）集成，并自动动态配置。在您的协调器上指向Traefik应该是您需要的唯一配置步骤。
Curl client for PHP HTTP
ThinkSNS的H5客户端文档。
自由门最新7.67版-无界19.02正式版下载
nginx源码中文注释版
机器学习》（西瓜书）公式推导解析
可能是让你受益匪浅的英语进阶指南
链家二手房租房在线数据，存量房交易服务平台数据，详细数据分析教程

April 8, 2019

《神经网络与深度学习》
GitLab API client for PHP 
印刷品般的漢字排版框架
Markdown 的 100 个骚操作（更新 100 年）
使用Biham和Kocher已知的明文攻击破解传统的zip加密。
自动更新正在运行的Docker容器
MySQL Log Analysis
爬取secwiki和xuanwu.github.io/sec.today,分析安全信息站点、安全趋势、提取安全工作者账号(twitter,weixin,github等)
用于WebShell Log Analysis的webshel​​l示例
v2ray的模板们
📄 中文排版 Composer 包
该软件包允许您使用固定窗口算法轻松创建和验证速率限制。
一个just-add-css集合的样式，使简单的网站更好一点
Fracker是一套工具，可以轻松跟踪和分析PHP函数调用，其目标是在PHP应用程序的手动安全评估期间协助研究人员。
Cacti是一个完整的网络图形解决方案，
Cacti是一个完整的网络图形解决方案
Kanboard是专注于看板方法的项目管理软件。
TypiCMS是一个使用Laravel 5.8构建的模块化多语言内容管理系统。开箱即用，您可以管理页面，事件，新闻，地点，菜单，翻译等。
laravel-source-analysis/
swoole source reading 源码分析
PHP中的M3U8解析器/转储器。

April 7, 2019

通过扫描MX记录，每日更新，清理和验证的一次性电子邮件域列表。
PHP扩展，以了解内存使用情况
Synology Audio Station / DS Audio的歌词插件
Yii 2扩展库，用于显示与Leaflet的交互式地图。
Laragon是一个可移植，隔离，快速且功能强大的通用开发环境，适用于PHP，Node.js，Python，Java，Go，Ruby。它快速，重量轻......
网络信息安全从业者面试指南
记录2019年社招面试过程中的一些问题，供大家参考，可以补充和指正，一起成长～
Editor.js的PHP后端
全栈Vue + Laravel + Axios CRUD示例

April 6, 2019

BootstrapVue为Vue.js提供了最全面的Bootstrap 4组件和网格系统实现之一，并且具有广泛的自动...
Laravel-Vue SPA入门项目模板。
一个高度自以为是的入门套件，用于使用Laravel和Vue.js构建单页应用程序
从PHP控制Chrome
PHP5.3 +路由类。轻巧但极其灵活。支持REST，动态和反向路由。
快速灵活的路由器
🎭Lava中的Laravel Translator课程！
🎩具有Lua，Markdown，HTTP / 2，QUIC，Redis和PostgreSQL支持的小型自包含pure-Go Web服务器
Iris是（THIS）地球上最快的社区驱动的网络框架。HTTP / 2，MVC等。为所有人提供无与伦比的免费支持。您的老式Web框架可以做到吗？
Iris 是一款超快、简洁高效的 Go 语言 Web开发框架。 Iris 功能强大、使用简单，它将会是你下一个网站、API 服务或者分布式应用基础框架的不二之选。
Standard Notes是一个简单的私人笔记应用程序，可在大多数平台上使用，包括Web，Mac，Windows，Linux，iOS和Android。
✨这包100多个gopher图片和元素将帮助您构建几乎任何与Go Programming Language相关的设计：演示文稿，博客或社交媒体中的帖子，课程，视频等等。
👾快速，简单，干净的视频下载器

April 5, 2019

FastDFS是一个开源高性能分布式文件系统（DFS）。它的主要功能包括：文件存储，文件同步和文件访问，以及高容量和负载平衡的设计。
qBittorrent BitTorrent客户端
完整而强大的PSR-14 EventDispatcher规范实现。
一个基于浏览器端 JS 实现的在线代理
一个用于图像中人脸检测的开源库。人脸检测速度可达1500FPS。
Shopify GraphQL设计教程
用于将标记化的PHP源代码转换为XML（以及可能的其他格式）的小型库
适合您项目的漂亮文档工具。

April 4, 2019

《React 学习之道》The Road to learn React (简体中文版)
用于PHP和Symfony的浏览器测试和Web爬网库
PHP项目/库的代码质量见解
通用第三方登录SDK，支持微信，微信扫码，QQ，微博登录，支付宝登录，Facebook，Line，Twitter，Google
用于与Ubiquiti的UniFi Controller API交互的PHP API客户端类
提供有关PHP对象图的有用操作
💱货币汇率库
Python脚本。模拟登录知乎， 爬虫，操作excel，微信公众号，远程开机
记录各种学习笔记(算法、Java、数据库、并发......)
用于运行WebAssembly二进制文件的PHP扩展。
应用程序仪表板和启动器
最先进的前端拖放页面构建器。以创纪录的速度创建高端，像素完美的网站。任何主题，任何页面，任何设计。
python爬虫教程，带你从零到一，包含js逆向，selenium, tesseract OCR识别,mongodb的使用，以及scrapy框架
用于生成Google站点地图XML文件的库
OOP代理包装器实用程序 - 生成和管理对象的代理
li 3是PHP的快速，灵活和最大的RAD开发框架。
 Go! AOP PHP - 面向切面编程的框架，用于新的软件开发水平
Parser Reflection API  - 提供源代码分析，无需将类加载到PHP内存中
允许反映对象属性，包括继承属性和非公共属性
微信开发者工具(微信小程序)linux完美支持
小米路由器Shell工具箱，本人自用，主要参考了小米的Misstar Tools制作，仅学习之用！
phar部署的例子
一个结构清晰的，易于维护的，现代的PHP Markdown解析器
GitHub 自动部署机器人
用于从PHP存档（PHAR）读取phar.io清单信息的组件
建立和管理Phars的申请。
Lisp到PHP编译器
PHP 依赖注入，从此不再考虑加载顺序

April 3, 2019

在触控栏中显示macOS Dock
聚合音乐Api，支持 node / android / ios / electron-render 调用
electron跨平台音乐播放器；可搜网易云、QQ音乐、虾米音乐；支持QQ、微博登录，云歌单; 支持一键导入音乐平台歌单
自己动手打造一个属于自己的直播间（视频直播、聊天室、弹幕、多端适配）
自己动手打造一个属于自己的直播间（视频直播、聊天室、弹幕、多端适配）
全栈Web开发笔记。
纯 Go 写的直播服务器
此项目用来提取收集以往泄露的密码中符合条件的强弱密码
C/C++面试基础知识总结 
下一代ls命令
大学生知识共享池
awesome window manager
TP生成静态站点类
基于Redis实现的延时队列服务，提供队列创建、删除及消息发送、接收、删除的操作。
phpstorm插件,用于thinkphp5框架的视图,配置,路由,数据库,模型智能提示和跳转
Web版中国菜刀
一天 30 秒 ⏱ 一段代码 ✍️ 一个场景 🖼
一个简洁优雅的图像识别转换文字的php类库, 须安装tesseract-ocr
Tesseract开源OCR引擎（主存储库）
数据字典自动生成文档 
📚 现代 Web 开发，现代 Web 开发导论 | 基础篇 | 进阶篇 | 架构优化篇 | React 篇 | Vue 篇
CrawlerDetect是一个PHP类，用于通过用户代理检测机器人/爬虫/蜘蛛

April 2, 2019

猫抓 chrome媒体嗅探插件
一个强大的Javascript库，用于捕获键盘输入。它没有依赖关系。
生成真正的随机用户代理
Mercure Component允许使用Mercure协议轻松地将更新推送到Web浏览器和其他HTTP客户端。
测试技术资源
Rust的Actor框架
运行维基百科的协作编辑软件。这是gerrit.wikimedia.org的一面镜子。
Flash OS映像到SD卡和USB驱动器，安全，轻松。
PHP中的Mustache实现。
Nmap扫描、漏洞利用脚本 
弱口令,敏感目录,敏感文件等渗透测试常用攻击字典
一个开源的网址导航网站项目，您可以拿来制作自己的网址导航。
根据关键字与 hosts 生成的关键词，利用 github 提供的 api，监控 git 泄漏。
暗网中文网监控爬虫
适用于PHP的腾讯云API 3.0 SDK
PHP中异步编程的资源列表
生成identicon头像,头像图片生成
PHP开发人员的一些工具。
 客户管理的前沿创新-悟空CRM
phpseclib  -  PHP安全通信库
爱百应，百度云网盘搜索引擎，爬虫+网站 

April 1, 2019

Markdown 简体中文与西文混排要点
Beanbun 是用 PHP 编写的多进程网络爬虫框架，具有良好的开放性、高可扩展性，基于 Workerman。
基于借方和贷方原则的余额会计（簿记）制度
swoole php多进程管理
缺少它的网站的RSS源
RSS-Bridge是一个PHP项目，能够为没有网站的网站生成RSS和Atom提要。
使用REST API包装自定义SQL数据库
十一月二十二日 题：在哈尔滨（一） 在这每一个寒冷又寂寞的夜晚 我窝藏在温暖沁人的被子里 那诞生的每一个轻浮的梦境中 我都能梦见一个美丽的花园 我梦见我的至亲们 我梦见我的挚友们 我梦见我的那个她 大家在花园里 都在听我的至亲们谈论着一个人 讲着他小时候的可爱 讲着他中学的稚嫩 到高中的叛逆以及大学的成熟 大家默默地听着 而她只是默默又含蓄地笑着 亲人们面向我 欢笑地讲述着过往 朋友们有说有笑地拍拍 我不是太宽大的肩膀 她伸手轻柔地捋捋 我被微风吹拂开的发丝 我一面微笑着 一手自然地靠在她的肩膀上 贴着她的脸颊 触碰着她的耳畔与发梢 我们愉悦地沐浴在晚霞的余晖之中 沉浸在多姿多彩的霞光下 只有我望着天空呀在发着呆 我看到远处红色与褐色相接的地方 有一轮红色的太阳 身后蓝的发黑的天际上 …
百度云/百度网盘Python客户端
各种脚本 -- 关于 虾米 xiami.com, 百度网盘 pan.baidu.com, 115网盘 115.com, 网易音乐 music.163.com, 百度音乐 music.baidu.com, 360网盘/云盘 yunpan.cn, 视频解析 flvxz.com, bt torrent ↔ magnet, ed2k 搜索, tumblr 图片下载, unzip
Babun  - 你会喜爱的Windows shell
restful-api风格接口 APP接口 APP接口权限 oauth2.0 接口版本管理 接口鉴权

March 31, 2019

Linux命令大全搜索工具，内容包含Linux命令手册、详解、学习、搜集。
当···时发生了什么？
learning-rust
一款轻量级、功能强大的内网穿透代理服务器。支持tcp、udp流量转发，支持内网http代理、内网socks5代理，同时支持snappy压缩、站点保护、加密传输、多路复用、header修改等。支持web图形化管理，集成多用户模式。
Wizard是基于Laravel开发框架开发的一款开源项目（API）文档管理工具。
一些常用的 docker 镜像 
一个PHPer的升级之路 

March 30, 2019

Linters Runner for Go。比gometalinter快5倍。漂亮的彩色输出。只能报告新问题。误报率较低。Yaml / toml配置。
将html转换为图像，pdf或字符串
php 资源文件管理
🔑 🔓免费开源的科学上网工具
用于reCAPTCHA的PHP客户端库，这是一项免费服务，可以保护您的网站免受垃圾邮件和滥用。
阿里巴巴nacos配置中心-PHP客户端
北京市预约挂号统一平台挂号小助手
Yasumi是一个简单的PHP库，用于计算国定假日
一个超级简单的PHP超全局变量管理扩展
shell代码部署系统
用C编写的简单，高性能的PHP框架

March 29, 2019

一款简单易用、功能强大的混沌实验注入工具
RedisLock for PHP是一种同步机制，用于在有许多exe线程的环境中强制限制对资源的访问...
RedisLock for PHP是一种同步机制，用于在存在许多执行线程的环境中强制限制对资源的访问。锁旨在实施互斥并发控制策略。
kubernetes1.13集群部署文档，包括kubernetes、dashboard、coredns、ingress、metrics、ceph rbd、helm、harbor、jenkins等相关组件部署文档
黑箱应用故障注入和资源发现的攻击模式和原语词典。
《把时间当作朋友》
《我也有话要说》—— 普通人的当众讲话技能
基于swoole开发的通信引擎，在线聊天平台，前端集成layerim框架，swoole基于eayswoole框架，异步连接池，多进程，异步任务，独立httperserver api，websocket推送，重构使用swoft-cloud 进行微服务架构
php实现的aes, des, 3des加密解密类
GitHub 上一些有趣的 HTML 小游戏进行了汉化
通过python脚本修改本机id破解teamviewer(tv版本需要是14以下)
黑苹果长期维护机型EFI及安装教程整理
Google Chrome扩展程序，用于修改Google Chrome 55+的页面默认编码。
轻松多个美丽的简历，创造你有史以来最好的简历！用Vue和LESS制作。
科学上网插件的离线安装包储存在这里
Goodby CSV是一种高内存高效，灵活且可扩展的开源CSV导入/导出库。

March 28, 2019

go 项目设计文件
JsonMapper  - 将嵌套的JSON结构映射到PHP类
一个还不错的图床工具，支持Mac和Win、支持压缩后上传、添加图片或文字水印、多张同时上传、同时上传到多个云、右击图片文件上传、快捷键上传剪贴板截图、提供Mweb接口，目前支持的云有：七牛、阿里、腾讯、网易、京东、百度、又拍、青云、Ucloud、sm.ms、Imgur！
帮助发现和安装工具
检测PHP代码中的设计模式
具有令牌桶算法的PHP速率限制库
本项目致力于收集网上公开来源的威胁情报，主要关注信誉类威胁情报（如IP/域名等），以及事件类威胁情报。
基于SQLite构建的轻量级分布式关系数据库。
Mojito 是一个基于 Laravel, Vue, Element构建的后台管理系统。
Node.js 应用线上/线下故障、压测问题和性能调优指南手册（更新中...）
异步PHP
一个开源的二次元向的社区程序
在blade模板中轻松使用过滤器。
网站分析应用程序
一个简化版的 man 手册。
code cola是一个chrome扩展，用于直观地编辑在线页面的CSS样式。
tldr alfred workflow
PHP Client for tldr
the only cheat sheet you need cheat.sh
955 不加班的公司名单
中文版 Awesome VS Code
PHP Excel Helper  - 基于PhpSpreadsheet以简单的方式编写和读取电子表格

March 27, 2019

通过与GitHub和GitLab的webhook集成增强Composer Satis
用于golang的socket.io库，一个实时应用程序框架。
Golang gRPC中间件：拦截器链接，身份验证，日志记录，重试等。
《Flutter实战》电子书
一本侧重于Go语言语法和语义的编程解释和指导书 
“996”工作制，即每天早9点到岗，一直工作到晚上9点。每周工作6天。

March 26, 2019

Pornhub模式标志生成器
本文介绍的是利用学生身份可以享受到的相关学生优惠权益，但也希望各位享受权利的同时不要忘记自己的义务，不要售卖、转手自己的学生优惠资格，使得其他同学无法受益。
CrazyCodes's blog
用于消费来自任何Broker的消息的lib
Twill是Laravel的开源CMS工具包，可帮助开发人员快速创建直观，强大且灵活的自定义管理控制台。在Spectrum上与我们和其他人聊天！
用于swarrot集成的symfony包
Debian，Ubuntu和CentOS的OpenVPN road warrior安装程序。
用于管理Kong网关的仪表板
脚本集，关于CSP（内容安全策略）的想法
Collections Abstraction Library
谷歌reCaptcha模块形成Magento2。
基于Flexbox的现代CSS框架
微小的WebSockets
php富文本过滤类，XSS Filter 
各种安全相关思维导图整理收集
laravel 即插即用的b2c商城扩展。
这是PHP CodeSniffer的一组嗅探，用于检查PHP跨版本兼容性。它将允许您分析代码以与PHP的更高版本和更低版本兼容。
带有可插拔后端的JWT登录微服务，如OAuth2，Google，Github，htpasswd，osiam，..
Netflix Eureka服务器的PHP客户端。支持所有Eureka REST操作。
（Spring Cloud）Netflix Eureka服务注册和发现的PHP客户端。http://hamid.work
图说设计模式
一个好玩的Web安全-漏洞测试平台
PHP7扩展开发 系列教程
当你ssh时带上你的.bashrc，.vimrc等
每周为你提供高质量的关于小程序、h5等前端领域的文章和项目
北京大学课程资料整理
北京邮电大学计算机考研信息汇总

March 25, 2019

phan是一个静态语法兼容性工具，它可以分析语法是否符合指定php版本，并将结果输出到指定文件。
一款离线，高颜值的🍅工作软件，二十五分钟专注做一件事⭐️。
用NodeJS解析纯真IP库(QQwry.dat) 支持IP段查询
前端小测答题收集专用
GoAccess是一个实时网络日志分析器和交互式查看器，可以在* nix系统中的终端或通过浏览器运行。
将Nginx log_format转换为goaccess配置文件
自动SQL注入和数据库接管工具
用于（反）序列化任何复杂数据的库（支持XML，JSON，YAML）
人人都能学会的 WordPress 实战课
在几秒钟内获得一个干净的，随时可用的Linux盒子。
记录自己学习TensorFlow的参考资料、笔记和代码
以图搜图
微信域名拦截检测、QQ域名拦截检测、360域名拦截检测、域名Whois查询
PhpBoot 是为快速开发微服务/RESTful API 设计的PHP框架。它可以帮助开发者更聚焦在业务本身, 而将原来开发中不得不做, 但又重复枯燥的事情丢给框架, 比如编写接口文档、参数校验和远程调用代码等。

March 24, 2019

Ludwig是一个基于TensorFlow构建的工具箱，可以训练和测试深度学习模型，而无需编写代码。
Web版中国菜刀
一个轻量级的包消息
微信mac/ipad协议，webapi封装好的实现方案，免IIS一键部署。 可实现微信80%功能；支持62数据登录、扫码登录、收发朋友圈、查看朋友圈、微信建群、微信拉人进群、微信公众号阅读、微信消息收发、微信附近的人定位、微信添加好友、微信红包接收、微信防撤回、分享小程序、微信加粉、微信收藏、微信标签等
轻松地异步运行代码
该项目的目的是在PHP中创建DNS记录的抽象对象表示。该项目包含代表DNS对象的各种类（如Zone，ResourceRecord和各种RData类型），用于将BIND样式文本文件转换为PHP对象的解析器，以及用于创建美观BIND记录的构建器。
此组件为Intl扩展（IDN功能）提供部分本机PHP实现。

March 23, 2019

mac 极简的开发环境 valet
OBS Studio  - 用于直播和屏幕录制的免费开源软件
适用于Linux，BSD和OSX的快速轻量级日志处理器和转发器
🦔快速，轻量级和无架构的搜索后端。Elasticsearch的替代方案，可在几MB的RAM上运行。
PHP徽章，使用包装信息为您的自述文件提供一些徽章。
收集&推荐优秀的 Apps/硬件/技巧/周边等
PHP的调试栏

March 22, 2019

Go 语言中文网 | Golang中文社区 | Go语言学习园地 源码
GCTT Go中文网翻译组。
阿里云容器服务持续交付
记录成长的过程
rust 协程库
从markdown文件创建书籍。像Gitbook一样，但在Rust中实现
这可能是yii2中最好用的微信SDK
在渗透测试中快速检测常见中间件、组件的高危漏洞。
在线子域名信息收集工具
在线子域名信息收集工具
Laravel GraphQL Server
这是ZipArchive方法的一个简单包装器，带有一些方便的功能
📦MacApp Store命令行界面
Hexo的管理界面
Beanstalk队列服务器的管理控制台
从Laravel应用程序创建静态站点包
您的基础架构作为GraphQL服务
根据Mix清单添加预加载和预取链接
XHProf是PHP的功能级分层分析器，具有简单的基于HTML的用户界面。
PHP非侵入式监控平台- 优化性能，定位Bug的神器，别再让你的PHP程序裸奔。
在MongoDB上构建的XHProf数据的图形界面
适用于PHP 7的现代XHProf兼容PHP Profiler
用于macOS上的v2ray-core的GUI
V2rayU,基于v2ray核心的mac版客户端,

March 21, 2019

下一代前端统一框架 - 支持桌面Web、移动H5和小程序
慢雾安全团队知识库
一种用于企业自托管的开源文档管理工具。
Python中的张量和动态神经网络，具有强大的GPU加速功能
中文近义词工具包
使用PHP Annotations声明GraphQL API
懒人博客，前后端分离，Vue+Beego Restful api 开箱即用，部署简单，后台管理系统简洁美观。
Golang实现的基于beego框架的接口在线文档管理系统
根据Mix清单添加预加载和预取链接
准确率99.9%的ip地址定位库
Sublime Text颜色方案已准备好用于下一代JavaScript语法
中文开源字体集 
轻松存储一些值
中国远程工作资料大全
PHP 5.3+的高度自以为是的模拟框架
Y分钟学习X种语言

March 20, 2019

PHP阿里巴巴云客户端的官方存储库
Go 每日阅读和 Go 夜读 
JIKEFM - 即刻电台📻
php开源商城系统，基于swoole、easyswoole框架开发
phalcon-oauth2-server
开源Spotify客户端库
easywechat for thinkphp支持
一个免费且符合道德标准的照片共享平台，由ActivityPub联合提供支持。
使用相同的Laravel安装运行多个网站，同时保持租户特定数据分离，以实现完全独立的多域设置。
建立在laravel for all上的电子商务框架，用于构建和扩展您的业务。

March 19, 2019

Caffe：深度学习的快速开放框架。
用户友好的命令行shell。
一个完整的PHP操作工具
*nix系统管理员测试问题和答案的集合。
微信公众号排版编辑器，转换 Markdown 到微信特制的 HTML
为比特币社区提供的一组资源。
📆使用HTML模板的jQuery日历插件
LaraCMS 后台管理系统
LaraCMS Framework。—— LaraCMS 核心基础框架，配合 LaraCMS 使用。
PHP高级工程师面试题汇总(2018.05)
基于swoole实现的微信机器人，依赖vbot和微信网页版的功能，帮助管理微信群/聊天/踢人等
在Composer运行时合并一个或多个其他composer.json文件
🍀本地git统计信息，包括类似GitHub的贡献日历。
油猴脚本 直接下载百度网盘和百度网盘分享的文件,直链下载超级加速
创建包含个人数据的zip文件
超级速查表 - 编程语言、框架和开发工具的速查表，单个文件包含一切你需要知道的东西 ⚡️
超赞的 Linux 软件
tree-ql是一个laravel扩展,通过简单的配置构建出一套极具描述性,可读性,且没有任何冗余的高性能API.
具有AutoCompletion和语法突出显示的MySQL终端客户端。
带有详细注释的 yii2 2.0.3 代码。喜欢的话请点star，欢迎大家一起来补充
构建自己的PHP框架
A simple PHP framework 构建自己的PHP 框架代码示例
NideShop：基于Node.js+MySQL开发的开源微信小程序商城（微信小程序） nideshop.com
企业仓库管理系统
php仓库进销存

March 18, 2019

ClickHouse是一个用于大数据的免费分析DBMS。
在浏览器中播放生成音乐的平台。
Godot Engine  - 多平台2D和3D游戏引擎
🚰用于检测代码和测试中的内存泄漏的PHPUnit插件
roave / dont是一个小型的PHP软件包，旨在实现设计防御性代码时的良好实践
Github信息泄漏监控系统
PHP7的简洁并行并发API
JSON-Schema +假数据生成器
多框架编写器库安装程序 
Composer安装程序扩展程序
支持（Laravel / Lumen / PSR-15 / Swoft / Slim / ThinkPHP） -  PHP CORS（跨源资源共享）中间件。
由libsodium提供支持的高级加密接口
apache/logging-log4php
用于检测用户浏览器的PHP类
一个轻量级的PHP分页器
通过生成包含所有自动加载文件的单个PHP文件来优化类加载性能。

March 17, 2019

输入检查JSX for Rust
小练习让你习惯阅读和编写Rust代码！
最近对Go语言很感兴趣，所以整理相关资料，呵呵
适合所有人的PHP数据库迁移

March 16, 2019

OPNsense GUI，API和系统后端
基于Lua的跨平台构建实用程序
Element UI 中国省市区级联数据
linux内核网络协议栈源码阅读分析注释--带详尽中文分析注释以及相关流程分析调用注释，对理解分析内核协议栈源码很有帮助
开源GraphQL CMS
拼多多API SDK【拼多多开放平台】
腾讯AI开放平台 【Tencent AI open platform】
该库提供了围绕数学运算的工具。
php的公式解释器
PHP Redis Cache缓存策略技术
A web framework for Rust.
draw.io是一个在线图表网站，提供此项目中的源代码。
一个用于图像中人脸检测的开源库。人脸检测速度可达1500FPS。
代码信息的统计程序 

March 15, 2019

自学是门手艺
WebSVN  - 在线subversion存储库浏览器
用于定义状态的轻量级库
使用Vega，您可以使用JSON格式描述数据可视化，并使用HTML5 Canvas或SVG生成交互式视图。
用于与Pusher Channels HTTP API交互的PHP库
💦 微博系统实现
基于 Laravel 可灵活自定义的的私人微信机器人，能够实现如：拜年群发自动回复、消息转发、防撤回、暗号加好友、甚至留言统计等功能
简单的闪光通知
将PHP数据转换为JavaScript。
Lua http restful api框架
MySQL入门教程（MySQL tutorial book）
PHP cron表达式解析器可以解析CRON表达式，确定它是否应该运行，计算表达式的下一个运行日期，以及计算表达式的上一个运行日期。您可以跳过n个匹配日期来计算远期或过去的日期。

March 14, 2019

终端的系统监控仪表板
Redis多线程叉
PHP 7+付款处理库。它提供了处理付款所需的一切：信用卡和异地购买，订阅，支付等 - 由Forma-Pro提供
用于PHP的HTML to Markdown转换器
一个关于人工智能渗透测试分析系列
weblogic 漏洞扫描工具
上传漏洞fuzz字典生成脚本
OpenCV的PHP扩展
OS X的全局鼠标手势
连接所有Kubernetes集群，无论它们在世界的哪个位置。
Markdown解析器，做得对。100％CommonMark支持，扩展，语法插件和高速
将任何使用STDIN / STDOUT的程序转换为WebSocket服务器。像inetd一样，但对于WebSockets。
Overlord是哔哩哔哩基于Go语言编写的memcache和redis&cluster的代理及集群管理功能，致力于提供自动化高可用的缓存服务解决方案。
github泄露扫描系统
为CentOS / Debian / Ubuntu自动安装Shadowsocks Server
关于交叉编译Rust程序需要了解的一切！
莲米粒是一个基于PHP+MySQL+微信小程序技术栈的、拥有用户登入、发布、修改、删除和转发信息、以及私信聊天模块的信息流应用。
Laravel 5 系列入门教程 https://github.com/johnlui/Learn-Lara…
基于php和bash的离线下载神器 http://goxz.gq
EleTeam开源项目-电商全套解决方案之PHP版-Shop-for-PHP-Yii2。一个类似京东/天猫/淘宝的商城，有对应的APP支持，由EleTeam团队维护！
覃健祥的学习笔记，各种几十分钟入门的文档
回到阅读的乐趣 -  Arc90的可读性PHP端口
基于GIT的应用程序的现代笔记，不需要本地GIT环境。gitnoteapp.com
《大话设计模式》php版本
纯PHP实现GraphQL协议
阿波罗11号制导计算机（AGC）中指令模块（Comanche055）和登月模块（Luminary099）原始代码。

March 13, 2019

SQL优化器和重写器
一个简单的腾讯企业邮箱SDK
QueryPHP 是一款现代化的高性能 PHP 7 常驻框架
rust 网络代理和隧道（VPN）
ngx_php7  - 用于nginx模块的嵌入式php7脚本语言。ngx_php的主线开发版本。
旨在通过分析企业信息安全建设过程中的心路历程
PSR-7和PSR-15 OpenAPI验证中间件
轻松安全地管理crontab文件
下一代ShadowsocksX
开源运维平台：帮助中小型企业完成主机、任务、发布部署、配置文件、监控、报警等管理(
Jamlee/coroutine: php nonpreemptive multipletasking
rust 一个模块化工具包，用于使用Rust和Wasm构建快速，可靠的Web应用程序和库
rust 从命令行轻松安全地共享文件。功能齐全的Firefox发送客户端。
TypeScript 入门教程
精通比特币（第二版）-- 区块链编程
使用Nginx+Lua实现自定义WAF

March 12, 2019

使用Yii 2的嵌套集的高级树管理模块。
保持应用程序设置同步（OS X / Linux）
shell十三问--shell教程（markdown 版本）
SecurityManageFramwork是一款适用于企业内网安全管理平台，包含资产管理，漏洞管理，账号管理，知识库管、安全扫描自动化功能模块，可用于企业内部的安全管理。 本平台旨在帮助安全人员少，业务线繁杂，周期巡检困难，自动化程度低的甲方，更好的实现企业内部的安全管理
rust style and philosophy
PHP的轻量级HTTP客户端
通过SSH擦除并重新安装正在运行的Linux系统，而无需重新启动。你知道你想。
大家一起被捕吧计划
编译器。对于PHP
PHP中的PHP VM实现
用于生成随机数和字符串的库
Easily manage git hooks in your composer config
在您的composer配置中轻松管理git钩子。此命令行工具可以轻松实现git挂钩的一致项目范围使用。
用于PHP的AST可视化工具
Medis是一款美观，易用的Redis数据库管理应用程序。

March 11, 2019

Google Chrome（和Chromium）的扩展程序，可以反转网站的亮度。
在本地运行Kubernetes
一个简单的服务器，用于每个Web套接字实时发送和接收消息。（包括时尚的web-ui）
精选的前端常见面试问题集
Box应用程序简化了PHAR构建过程。
CentOS 7 安装 LNMP 环境

March 10, 2019

将Figma帧转换为Google幻灯片演示文稿🍭
一个简单的包解析PHP中的Github Flavored Markdown
Lepton是一个基于GitHub Gist的精简代码片段管理器
具有多字节支持的PHP字符串操作库
3y原创技术文章导航
Acme PHP是一个简单但非常可扩展的CLI加密器CLI客户端，可帮助您获取和续订免费的HTTPS证书。
PHP多版本安装和管理
超级微信电脑客户端，支持多开、防消息撤销、语音消息备份...开放WeChatSDK
YOURLS是一组PHP脚本，允许您运行自己的URL Shortener。您可以完全控制数据，详细统计信息，分析，插件等。它是免费和开源的。
官方GitHub API的Ruby客户端。
GitHub Desktop
SketchI18N是Sketch.app的多语言插件

March 9, 2019

📒《Node.js实战：使用 Egg.js + Vue.js + Docker 构建渐进式、可持续集成与交付应用》 源码
第三方Jike app chrome扩展。
2018 JDDC对话大赛亚军解决方案
欢迎来到以太坊Wiki！
Opulence是一个PHP Web应用程序框架，它简化了创建和维护安全，可扩展的网站的难度部分。
简单快速的HTML解析器
网页版 PS
一个简单的静态http服务器
使用电子和vue.js制作的简单RSS阅读器应用程序
FEX 面试问题
GLPI是一个免费资产和IT管理软件包，ITIL服务台，许可跟踪和软件审计。
基于社区的GPL许可网络监控系统
Spala（SPA + Lalavel）。适用于Laravel和Vue开发人员的现代轻量级CMS（开源项目）。
The most awesome Powerline theme for ZSH around!

March 8, 2019

该软件包为Laravel 5.8提供了与FFmpeg的集成。文件的存储由Laravel的Filesystem处理。
最适合入门的laravel初级教程
用于动态编辑.env文件的Laravel包。
用于动态编辑.env文件的Laravel包。
python各大网站登陆方式与一些简单的爬虫
NGiИX config generator on steroids
分析正在运行的容器的资源使用情况和性能特征。
HttpClient组件提供了同步或异步获取HTTP资源的强大方法。
高仿书旗小说 Flutter版，支持iOS、Android
skl api，企业级后台API开发平台。使用beego语言架构。开发平台内嵌了用户、用户组、机构、角色、权限、多语言、枚举、OA引擎等功能模块。
Welcome！Hello YCY
MailEclipse⚡️轻松玩你的Laravel Mailables！
✏️了解如何用C编写哈希表
BetterAndBetter 是一款包含很多功能的 macOS 软件

March 7, 2019

纯bash脚本，用于测试和等待TCP主机和端口的可用性
各种滑动验证码识别 [腾讯云] [易盾] [Vaptcha] [Geetest] [极验] 各种网站破解 lengyue.me
一种开源可信云本机注册表项目，用于存储，签名和扫描内容。
CentOS7服务器的一些配置
维护的ctags实现
具有GPL许可证的高性能MySQL代理。
一个极简的基于swoole常驻内存框架，支持在fpm下运行
学习强国 懒人刷分工具 自动学习
一个视频播放器，开源版 potplayer ，用于学习和交流音视频技术
为通过Composer管理的每个PHP项目创建简单的phar
一个基于ThinkPHP5.1+和AmazeUI的快速后台开发框架
Googletest  -  Google测试和模拟框架
Lua + libUV + jIT = pure awesomesauce

March 6, 2019

正在搭架子 作为公共分享资料
README的艺术
PHP Protobuf  -  Google的PHP协议缓冲区
PHP 7的异步协程。
功能丰富的跨平台传输BitTorrent客户端。 比内置Web GUI更快，功能更多。
一个易于使用的库，用于使用InfluxDB和PHP。
一种向类动态添加方法的特性
htrace.sh是用于http / https故障排除和分析的shell脚本。它也是围绕几个开源安全工具的简单包装脚本。
记录一下SS的前世今生，以及一个简单的教程总结
记录一下SS的前世今生，以及一个简单的教程总结
swover 一个基于Swoole扩展的服务器框架
远程运行VS代码。coder.com
以开发人员为中心的HTTP客户端，针对大多数常见用例进行了优化。

March 5, 2019

🔥 让阅读变成一件有意义的事。Golang好文推荐；收录平时阅读到的一些Go相关写的比较好、质量较高的干货文章.
为互联网IT人打造的中文版awesome-go
适用于npm和GitHub的免费，快速，可靠的开源CDN
HTTP，HTTPS，WebSocket调试代理
机器学习100天
🚄用于PHP的快速生成对象Hydrator
为PHP实现XDG基本目录规范
根据网易云音乐的歌单, 下载flac无损音乐到本地.。
Go client for Redis
HTML5视频速度控制器（适用于谷歌浏览器）
Oracle数据库的应用程序和工具使用示例
PPGo_Job是一款可视化的、多人多权限的定时任务管理系统，采用golang开发，安装方便，资源消耗少，支持大并发，可同时管理多台服务器上的定时任务。
CamelCasePlugin for IDEA IDEs
EvaOAuth 1.0：统一接口的 OAuth 登录 PHP 类库
EvaOAuth 1.0：统一接口的 OAuth 登录 PHP 类库
EvaThumber : 基于URL的图片处理库 (可实现缩略图 | 二维码 | 水印 | 面部识别等,基于PHP
Vita：简单快速的VPN网关
队列论：软件开发简介
开源数字标牌解决方案
微信调试、API调试和AJAX的调试的工具，能将日志通过WebSocket输出到Chrome浏览器的console中

March 4, 2019

提供多款 Shadowrocket 规则，带广告过滤功能。用于 iOS 未越狱设备选择性地自动翻墙。
《大话设计模式》php版本
一个简单的脚本，用于在PHP中缓存第三方API调用
轻量级，简单但功能强大的PHP5缓存类，它使用文件系统进行缓存。
程序员技能图谱 
jSearch(聚搜) 是一款专注内容的chrome搜索扩展，一次搜索聚合多平台内容。
100行Python代码快速获得一个代理池，两分钟获得数千个有效代理

March 3, 2019

Webpack的演示与课程4
Go中的DNS客户端，通过HTTPS支持Google DNS
🥐一个Lua REPL和调试器
search and download music 从网易云音乐、QQ音乐、酷狗音乐、百度音乐、虾米音乐等搜索和下载歌曲
An official read-only mirror of http://hg.nginx.org/unit
Live2D 看板娘插件 (fghrsh.net/post/123.html) 上使用的后端 API live2d.fghrsh.net
Live2D 看板娘插件 (fghrsh.net/post/123.html) 的前端 HTML 源码 live2d.fghrsh.net
在Go中发送电子邮件的最佳方式。
库用于清理和清理网页以创建大量数据集。
用Lua编写的Nginx的Prometheus度量库
📦📦安装直接在浏览器中运行的npm依赖项。无需Browserify，Webpack或导入地图。
分布式系统讲座系列的课程材料
分布式系统讲座系列的课程材料
用于构建高效数据科学工作流的Python库
关于python的面试题
预制工厂采用faker方法建议，以提高生产力
The Hoa\Fastcgi library. 
该死的易受攻击的Web应用程序（DVWA）
mailcow: dockerized 搭建一个满分的自有邮箱服务

March 2, 2019

用Lua编写的Nginx的Prometheus度量库
PHP 最优秀资源的整理汇集
网络资产发现引擎
PHP的图像处理库
如果将markdown视作一门编程语言可以做哪些有趣的事情呢?
鲜亮的高饱和色彩，专注视觉的小程序组件库 
个人准备渗透测试和安全面试的经验，和部分厂商的面试题
个人准备渗透测试和安全面试的经验，和部分厂商的面试题
NexT 是一个高质量并且优雅的Hexo 主题。这是精心制作做出来的 hexo 主题。
这是一个webshel​​l开源项目
这是一个webshel​​l开源项目
中国运营商IP地址库-每日更新
Keybase Go Library, Client, Service, OS X, iOS, Android, Electron
打造优质前端博客，欢迎关注我的公众号：前端工匠
Delve是Go编程语言的调试器。
Elasticsearch 可视化DashBoard, 支持Es监控、实时搜索，Index template快捷替换修改，索引列表信息查看， SQL converts to DSL等
Frp配置面板 
HTPC / Homelab服务管理器 - 用PHP编写
报告PHPUnit测试套件中运行缓慢的测试
devdocs.io的Alfred工作流程
适用于DevDocs.io的全功能桌面应用

March 1, 2019

用于JavaScript的GitHub REST API客户端
📗[WIP] Laravel 应用部署上线课程系列。
程序猿成长计划
用于GraphQL订阅的WebSocket客户端+服务器
  redis-full-check是阿里云Redis&MongoDB团队开源的用于校验2个redis数据是否一致的工具，通常用于redis数据迁移（redis-shake）后正确性的校验。
HTTPlug，PHP的HTTP客户端抽象
Go中的快速键值DB。
Gitter for GitHub - 可能是目前颜值最高的GitHub小程序客户端
一款手势驱动的裁图插件
EventEmitter3是一个高性能的EventEmitter。它已针对各种代码路径进行了微优化，使其成为Node.js和浏览器中最快的EventEmitter之一。
certbot'renewing letencrypt证书插件 - 自动验证aliyun / tencentyun / godaddy dns
防止通过表单提交的垃圾邮件
用于配置组装的Composer插件
阿里妈妈前端团队出品的开源接口管理工具RAP第二代
这是一个用于检测和解码QR码的PHP库。这是第一个也是唯一一个无需扩展即可工作的QR码阅读器。
Swoft从入门到微服务课程代码
Symfony之上的开源电子商务框架
利用curlmulti内置的IO事件循环实现，具备高性能、高通用性、高扩展性，尤其适合复杂业务逻辑大批量请求的应用场景。
The best php curl library.

February 28, 2019

一款屏幕保护软件
Node.js REST开发的未来
一个广泛的JavaScript和Node.js数学库
定制kubernetes YAML配置
mageMagick 7
在不到30秒的时间内获得零编码的完整虚假REST API（严重）
Goji是一个简约的Web框架，它重视可组合性和简单性。
⚡️由Spray-can，Netty，underow，jetty，Vert.x，Grizzly，node.js和Go实现的高性能websocket服务器。它支持1,200,000个有效的websocket连接
Laravel插入重复键并插入忽略
gRPC PHP客户端库
直观的查找和替换CLI（sed替代）
Package Repository Website 
UPX  -  eXecutables的终极包装工具
Go服务器的平滑重启和零停机时间部署。
用Rust写的Javascript引擎
PHP到JavaScript转换器和用JavaScript编写的VM
iOS上的Native App over HTTP
使用kubeadm在AWS上真正便宜的Kubernetes集群
互联网广告的黑洞
轻量级Kubernetes。易于安装，内存的一半，所有二进制文件都小于40mb。
所有PHP函数，重写为抛出异常而不是返回false
PHPGGC是一个unserialize（）有效负载库，以及一个从命令行或以编程方式生成它们的工具。
用于商业用途的开源电子商务laravel框架
用于商业用途的开源电子商务laravel框架
imagecolormatch（）OOB堆写入漏洞
Laravel的文件管理器
vue-laravel-file-manager
缺少的Spotlight插件系统
Php JWT example.
PHP的一个小而强大的REPL。

February 27, 2019

用于富文本编辑的世界上最流行的JavaScript库。可用于React，Vue和Angular
程序员如何优雅的挣零花钱
DDos/DoS工具集
Zero是一个简化Web开发的Web服务器。
存储性能开发套件
Lumen和Laravel 5的发电机集合。
在本地运行Kubernetes
基于 Flutter & scoped_model 实现的视频类App客户端
✨轻量级依赖注入
让你自定义的方法也可以使用依赖注入.
由WebAssembly提供支持的通用二进制文件
通过Tensorflow JS在客户端进行NSFW检测

February 26, 2019

libp2p网络堆栈libp2p.io的技术规范
每个软件开发人员应该知道的（大多数）技术事项的集合
timeago.js 是一个非常简洁、轻量级、不到 1kb 的很简洁的 Javascript 库，用来将 datetime 时间转化成类似于*** 时间前的描述字符串，例如：“3 小时前”。
📅 中国农历（阴历）与阳历（公历）转换与查询工具
多框架编写器库安装程序http://composer.github.com/installers
OSS Browser 提供类似windows资源管理器功能。用户可以很方便的浏览文件，上传下载文件，支持断点续传等。
hexo主题的hub样式复制
ShowDoc是一个非常适合IT团队在线共享文档的工具
Rust箱提供有关电池的跨平台信息。
针对seniverse api的Api使用演示http://www.seniverse.com/doc
极客时间：nginx核心知识100讲配置文件与代码分享
Swoole 远程调试器
120个常见数据科学面试问题的答案。
Istio knowledge map 知识图谱
自动将字幕与视频同步。
PHP中的设计模式示例
Doctrine Inflector是一个小型库，可以对大写/小写和单数/复数形式的单词执行字符串操作。
PHP应用程序的即时升级getrector.org
PHP中强类型的枚举支持自动完成和重构
PHPGGC是一个unserialize（）有效负载库，以及一个从命令行或以编程方式生成它们的工具。
Html菜单生成器

February 25, 2019

rust stackful coroutine library
基于Bulma的Vue.js的轻量级UI组件
基于生成器的PHP集合
Swoole 提案
简单，灵活的多主机容器网络等。
适用于Linux的采样CPU分析器
IPv4和IPv6用户空间网络堆栈
键入时格式化输入文本内容...
适用于iOS 11.0  -  12.1.2的unc0ver越狱
一个框架不可知的PHP库，用于构建聊天机器人
🚀 一个为任何MySql数据库生成REST API的命令。
输出复杂，灵活的AJAX / RESTful数据结构。
将任何数据库转换为API平台。

February 24, 2019

一个基于Yii2高级框架的快速开发应用引擎 
🕹macOS http://openemu.org的复古视频游戏模拟
防止Mac进入睡眠状态。
macOS的通用纯文本编辑器。

February 23, 2019

基于Google Material Design的Bootstrap 4，React，Vue.js，React Native和Sketch的免费开源UI工具包
macOS的剪贴板扩展应用程序。
Spring Boot 教程、技术栈示例代码，快速简单上手教程。
70万条对联数据库。
每秒解析千兆字节的JSON
平滑滚动网页
常见数据结构的CRDT，如map，vecs，sets，text和JSON
结合实际PHP面试遇到的问题，尝试提供简洁准确的答案
检测未使用的编写器依赖项
用于创建Web应用程序的Rust框架
资源搜索型软件 macOS OSX magnet
小米手机内核开源
SmartisanOS kernel opensource contain T1Kernel, T2Kernel, U1Kernel(JianGuo), M1Kernel(M1 and M1L), U2ProKernel(JianGuo Pro)
可扩展的数据存储区，用于指标，事件和实时分析
⭐️一系列精彩的列表，手册，博客，黑客，单行，cli / web工具等等。
TypeScript Web服务器 - 比Deno快15倍
百度网盘下载神器
企业级持续交付和DevOps自动化开源平台
为任何Eloquent模型及其关系创建修订版
中国大陆 2018 年 12 月XX站访问百强榜单
The HTTP client for Vue.js
基于Vue和WeUI的移动UI组件
WebSockets的命令行客户端
Pretzel是Mac桌面应用程序，可根据您当前的应用程序显示和搜索键盘快捷键。

February 22, 2019

在几秒钟内创建HTML演示文稿
现代JavaScript教程
用于编程工具的增量解析系统
《Go2编程指南》开源图书
ngx_php - 用于nginx模块的嵌入式php脚本语言。
为药物发现，量子化学，材料科学和生物学进行民主化深度学习
编写理智，可维护和可扩展Sass的指南。
所有事情的黑暗主题！
Go的理想精炼Web框架。
Rust中一个相对简单的Datalog引擎
Google常用的Java，C ++和JavaScript库，用于解析，格式化和验证国际电话号码。
MarkDown在线简历工具，可在线预览、编辑和生成PDF。[此项目已不再维护，建议使用 cv.ftqq.com 替代 ]
精选的软件和架构相关设计模式列表。
设计模式的超简化解释
在javascript中实现的设计模式的超简化说明
使用JS插件转换样式
Rust的类型安全，编译类似Jinja的模板
每秒解析千兆字节的JSON
IntelliJ IDEA社区版
将设计模型转换为静态网站的神经网络
Rust中文社区 rustlang-cn.org
Rust的在线社区源码
Rustlang相关各种资料！！！
查看从Laravel中提取的PHP模板引擎
使用Python进行科学计算的基础包。
Rdebug  - 真正的调试器
为Laravel测试提供快速的数据库迁移。
一个简单的JSONP实现
jquery jsonp插件

February 21, 2019

Lua redis基于cosocket API的ngx_lua客户端驱动程序
Node.js 微信公众平台 API
工作日每天一道前端大厂面试题，祝大家天天进步，一年后会看到不一样的自己。
本仓库用于记录 B3log 系列站点被攻击的记录，不定期更新。
PHP异步编程: 基于 PHP 实(chao)现(xi) NODEJS web框架 KOA。
50 个有志向的开发者组成的精英圈 kebox.cn
TinyPNG client for Mac
快速Python 3.5+ HTTP工具包与基于uvloop和picohttpparser的流水线HTTP服务器集成。
Laravel中的模块管理
基于laravelS和layim的聊天系统
Go configuration with fangs
该项目基于CNN5 / DenseNet + BLSTM / LSTM + CTC实现验证码识别。
GPU加速C ++用户界面，具有WYSIWYG开发工具，XML支持，内置数据绑定和MVVM功能。
“用于云存储的rsync” -  Google Drive，Amazon Drive，S3，Dropbox，Backblaze B2，One Drive，Swift，Hubic，Cloudfiles，Google Cloud Storage，Yande ......
代码可以帮助您启动个人网站，展示您作为软件开发人员的工作。
Laravel代理包用于在负载均衡器或其他中介后面处理会话。
🔥 「干货集中营」是一款注重体验的 Gank.io 官方客户端
Gank api base △ next.js (react&ssr)
使用 Rust 创建 PHP 扩展
腾讯防水墙
Chrome插件英雄榜

February 20, 2019

渗透测试/APT模拟攻击
Yargs通过解析参数和生成优雅的用户界面来帮助您构建交互式命令行工具。
启动创始人的公开清单：“如果你被公共汽车撞到会怎么样？
平台不可知安全令牌
是安全无状态令牌的规范和参考实现。
由Laravel 5和Sentry提供支持的PHP CMS
Voten.co是一个开源，美丽，高度可定制但致命的简单，温暖的社区。
用于了解应用程序安全性的精选资源列表
PHP 5.x支持random_bytes（）和random_int（）
现代网络的安全内容管理 - “天空只是开始”
使用Slim Framework构建的仅用于公共附加的分类帐微服务
由libsodium提供支持的高级加密接口
PHP项目的快速，可搜索的字段级加密
全功能的反CSRF库
在经过身份验证的加密中包装Bcrypt-SHA2
GnuPG加密的电子邮件变得简单
用于phpseclib的简单安全包装器
易于使用的PDO包装器，适用于PHP项目。
异常和错误使用户更友好
安全API工具包
中文 man 手册页计划
无扩展PHP图形用户界面库
🌌LaravelNova的语言支持。随意提交您的语言或更新现有语言！
具有Web UI的跨平台http嗅探器
最权威最完整的中国省市县数据
CodeReview是一个Git GUI工具，用于执行用Python3和Qt5编写的代码审查（Diff Viewer）。
哪个是最快的Web框架？
将本地端点公开给Internet
蓝天采集器是一款免费的数据采集发布爬虫软件，采用php+mysql开发
Tracy：易于为酷开发人员调试PHP代码的令人上瘾的工具。 友好的设计，日志记录，分析器，高级功能，如调试AJAX调用或CLI支持。
ZoneMinder是一款免费的开源闭路电视软件应用程序，专为Linux开发，支持IP，USB和模拟摄像机。
SQLite通过Emscripten编译为JavaScript
Cordova / PhoneGap插件，可在Android，iOS和Windows上使用HTML5 / Web SQL API打开和使用sqlite数据库

February 19, 2019

easy-swoole/demo
Enterprise application cloud operating system(企业应用云操作系统)
用于C / C ++ / Golang的微型跨平台webview库。使用WebKit（Gtk / Cocoa）和MSHTML（Windows）
一个基于Laravel的开源论坛。
用于Spatie laravel许可库的Laravel Nova工具
Package Management for Golang
Quill是一个现代WYSIWYG编辑器，专为兼容性和可扩展性而构建。
WebRTC Web演示和示例
 一个小巧、轻量的浏览器内核，用来取代wke和libcef
用于生成和验证Google身份验证器双因素身份验证的PHP类
[全文]如何正确的学习Node.js
PHP Protobuf  -  Google的PHP协议缓冲区
该软件包提供了在Elasticsearch中搜索和过滤数据的高级功能。
概述机器学习概念的思维导图，从数据分析到深度学习。
这是一个 Nginx 极简教程，目的在于帮助新手快速入门 Nginx。
社工库半自动处理

February 18, 2019

nginx源码中文注释版
IPIP.net正式支持IP数据库ipdb格式解析库
使用docker快速搭建各大漏洞学习平台，目前可以一键搭建12个平台。
浏览器中增强的电子书。
.files，包括〜/ .macos  -  macOS的明智的黑客默认值
我的dotfiles（由LARBS部署）
PostgreSQL的过程语言PHP
📚 C/C++面试基础知识总结
[WIP] PHP Service Bus（发布 - 订阅模式）实现

February 17, 2019

一种利用深度学习技术识别和交换图片、视频中人物脸部图像的工具
交互式UI组件开发和测试：React，React Native，Vue，Angular，Ember
NAXSI是NGINX的开源，高性能，低规则维护WAF
用Go编写的概念证明OS内核
Keras模型从手绘网站模型生成HTML代码。实现图像字幕体系结构以绘制源图像。
Anki Vector  - 具有交互式AI技术的家庭机器人。
该项目通过python脚本从巨潮网络的服务器获取中国股市（sz,sh）的公告(上市公司和监管机构),把公告信息放到数据库，公告文件下载到本地，并支持网页查询和读取。
自动化检测小工具，主要实现了域名枚举、链接爬取、注入检测、主机扫描、目录枚举、敏感信息检测等功能~
Prometheus操作指南 
用于最佳安全实践的php.ini扫描程序
PoCBox - 漏洞测试验证辅助平台
解析和评估以字符串形式给出的数学公式。
适用于WordPress SEO的All in One SEO Pack插件
Laravel中的可排队行动
使用Google翻译自动翻译您的语言文件
将响应缓存为磁盘上的静态文件，以便快速加载页面。
Google 开源项目风格指南 (中文版) 

February 16, 2019

c++ 顺序表、链表、静态链表、队列、一元多项式、汉诺塔、火车调度问题、操作系统调度问题、背包问题、最大连续子列和问题、KMP算法、稀疏矩阵、广义表、并查集、无向图邻接表、有向图邻接表、Krusskal算法、Prim算法、最短路径Dijsktra算法、最短路径Bellman-Ford算法、最短路径Floyd算法、拓扑排序、关键路径、优化的冒泡排序、快速排序、直接插入排序、折半插入排序、闭散列实现、开散列实现
背包问题九讲
Go的Tiny WebSocket库。
用于Javascript的种子随机数生成器
API文档生成器
PHP的文档生成器
Git for Windows. 国内直接从官网下载比较困难，需要翻墙。这里提供一个国内的下载站，方便网友下载
一个简单而自以为是的包，用于向Laravel提供基于子域的多租户
示例云本机应用程序，包含10个微服务，展示了Kubernetes，Istio，gRPC和OpenCensus。提供用于说明和演示目的。
Mac应用程序，显示所有正在运行的进程正在使用的所有打开文件和套接字。很好的GUI用于lsof。
486行C ++：一个周末的老派FPS
网络范围的广告和跟踪器阻止DNS服务器
用于学习操作系统的简单内核
JavaScript中的x86虚拟化，在浏览器和NodeJS中运行
中文文案排版指北
PHP库允许从URL或html页面生成缩略图，快照或PDF。
使用基于HTTP的API，非常简单的按需图像处理库。
使用相同的Laravel安装运行多个网站，同时保持租户特定数据分离，以实现完全独立的多域设置。
Laravel 5  - 用于抽象数据库层的存储库
微博批量拉黑
Mockery是一个简单而灵活的PHP模拟对象框架，用于使用PHPUnit，PHPSpec或任何其他测试框架进行单元测试。
PHPUnit中文文档
用于API监控和管理的OpenResty / Nginx网关。
一个非常强大和友好的nginx基于lua-nginx-module（openresty），提供WAF，控制面板和仪表板。
VeryNginx 是一个功能强大而对人类友好的 Nginx 扩展程序.

February 15, 2019

在使用Laravel应用程序时修改变量
用于记录企业安全规划，建设，运营，攻防的相关资源 
背景音乐，macOS音频实用程序：自动暂停音乐，设置各个应用程序的音量和录制系统音频。
Kaggle 项目实战（教程） = 文档 + 代码 + 视频（欢迎参与）
数据结构和算法必知必会的50个代码实现
rust 程序设计语言 中文版
awesome-composer 
WDScanner平台目前实现了如下功能：分布式web漏洞扫描、客户管理、漏洞定期扫描、网站爬虫、暗链检测、坏链检测、网站指纹搜集、专项漏洞检测、代理搜集及部署、密码定向破解、社工库查询等功能。
📦 A composer package builder. http://overtrue.me/package-builder
Go的快速脚本语言
GeoLocation限制了Laravel的路线
PHP的轻量级HTTP客户端
目前实现了网络空间资产探测、指纹检索、漏洞检测、漏洞全生命周期管理、poc定向检测、暗链检测、挂马监测、敏感字检测、DNS监测、网站可用性监测、漏洞库管理、安全预警等等~
Webpack的优雅包装，适用于80％的用例。

February 14, 2019

Docker + Node = Dockerode（Docker远程API的Node.js模块）
Hprose Server for Symfony
nginx cheatsheet
TensorFlow教程和最新API初学者示例
用于物联网的超轻量级JavaScript引擎
💯后端面试进阶指南
自己提炼的关于《HTTP权威指南》每章的知识点总结！
用于代码生成的Laravel组件
历史上最伟大的软件工程师列表
有趣的注释
精选的黑客教程，工具和资源的精选列表
程序员最值得关注的10个C开源项目
管理员资源的精选列表。
Google 全球 IP 地址库
开发者工具箱， free-for-dev
GitHub秘籍
Git 风格指南
Octicons是由GitHub为GitHub构建的一组SVG图标。
github上的lighttpd2更易于协作 - 主要的回购仍然在lighttpd.net上

February 13, 2019

C++包管理器
《Go语言四十二章经》
在GO中处理1M WebSockets连接
简单可靠的网站分析。与Golang&Preact一起构建。
机器学习 (CS 229 Stanford)
即时消息服务器；Go中的后端；Android、Web命令行客户端；聊天机器人
sourcerer应用程序从您的GitHub和Git存储库生成一个可视配置文件。
openresty/openresty: Turning Nginx into a Full-Fledged Scriptable Web Platform
说明描述了如何提高nginx性能、安全性和其他重要事项；
用于MacOS的MySQL/Mariadb数据库管理
隐藏MacOS菜单栏项目
一款功能齐全的客户端（iOS、Android）研发助手，你值得拥有。
网易云音乐命令行版本
Theia是一个用TypeScript实现的云和桌面IDE框架。
一个又酷又高效的命令行 GitHub 工具
用于在MacOS的VSCode上隐藏标题栏并内联交通灯（=窗口控件）的扩展。
PouchDB是一个口袋大小的数据库。
Web的实时数据库
用于功能强大的React和React Native应用程序的高性能反应数据库
javascript嵌入/内存数据库
⚡️lowledb是一个由Lodash支持的小型本地JSON数据库（支持Node，Electron和浏览器）
浏览器的无情键值存储。

February 12, 2019

编写和优化Go代码
REST API application generator for Yii2, openapi 3.0 YAML -> Yii2
一个不断发展的如何保护Linux服务器的指南。
在学院的书架上发现了一本不带脑子就能看懂的书《Python数据挖掘与实战》
ElasticSearch的官方Go客户端
用PHP编写的CSS文件的分析器。允许将CSS文件提取到数据结构中，操作所述结构并输出为（优化的）CSS
轻松创建Alfred工作流
Chrome扩展，在使用应用程序时生成Laravel集成测试。
上传图片到终端公共cdn
使网站页面在1分钟内即时更新，并将转换率提高1%
客户没有付款？增加不透明度的身体标签，并减少它每天直到他们的网站完全消失。
下一个用于Web浏览器的开源文件上载程序
💩🚀 Windows 95 in Electron. Runs on macOS, Linux, and Windows.
大规模中文自然语言处理语料
社区驱动的内容聚合器
Global key-value store in the database
Immutable base object and value objects.
语义化版本控制规范（SemVer）
语义化版本控制规范（SemVer）

February 11, 2019

用深度学习对对联。

February 10, 2019

快速浏览任何GitHub文件的历史记录GitHistory.xyz

February 6, 2019

FastHub是Android的终极Github客户端。
基于现代网络的可扩展桌面邮件应用程序。

February 5, 2019

video.js单元库。
kubernetes cli以风格管理集群！
中国5级行政区域mysql库
A Node.js style checker and lint tool for Markdown/CommonMark files.
Collection of awesome podcasts
Helper functions I find super-duper handy

February 1, 2019

基于PHP的全功能颠覆革命性框架，大道至简、大有若无。本框架钦定组件库：packagist.org

January 31, 2019

[已弃用] Dockerfile，包含安装Magento 2所需的扩展，配置和命令
Go（Golang）假结构数据生成器
适用于Web社区的AI OS

January 30, 2019

全语自动填充器：tabnine.com
Empire客户端应用程序
适用于PHP 7的低开销采样分析器
写在19年初的后端社招面试经历(两年经验): 蚂蚁 头条 PingCAP
draw.io是一个在线图表网站，提供此项目中的源代码。
去死吧！996 godie996.com
带有Windows API的最小无边框窗口
面向对象的PHP驱动程序，用于FFMpeg二进制文件
将curl命令转换为python，javascript，php，R
用golang编写的迷你SMTP服务器
一些内网渗透TIPS
Hexo七牛同步插件
PHP的通用SOAP客户端
收集的一些国外能提供提供威胁情报的公司，涵盖网络安全、工控安全、终端安全、移动安全等领域
用Go（Golang）编写的轻量级MVC框架。
Gitter for GitHub - 可能是目前颜值最高的GitHub小程序客户端
尝试解析出知乎官方未开放的 OAuth2 接口，并提供优雅的使用方式，作为 zhihu-py3 项目的替代者，目前还在实验阶段
golang提示
🛁 PHP版的代码整洁之道 中文翻译

January 29, 2019

日更的FlutterDemo合集，今天你fu了吗
PHP非侵入式监控平台- 优化性能，定位Bug的神器，别再让你的PHP程序裸奔。
git commit --fixup，但是自动的
Flutter嵌入API的桌面实现
PHP SSO Platform 生蚝科技统一身份认证平台
open-source-mac-os-apps 
JavaScript web server
迈向 Tech Lead 之路。
elasticsearch-cn/elasticsearch-definitive-guide
看到女装的项目的issue建议妹子建一个男装的项目，但是考虑到github的女性用户 数量貌似并不能达到女装的效果2333总之先建一个。
基于CLI和REST接口的自托管和基于PHP的URL缩短程序
用于大数据的分布式SQL查询引擎
依赖注入系统
发现需要重构的文件。
API平台的服务器组件：超媒体和GraphQL API，只需几分钟
PHP链接检查器
饿了么蜂鸟配送php开发包
华丽的应用程序，纠正您以前的控制台命令。
🐸ASCII在线视频流搭建脚本
可视化GitHub配置文件的工具
HTML5 / JavaScript多人游戏实验
给不了你梦中情人，至少还有硬盘女神
软件版本控制可视化
12306 图片验证码识别测试

January 28, 2019

PHP MySQL类的包装器，它使用MySQLi和预处理语句。
“在互联网上寻找栖息之地”
macOS文件存档
Lanyrd's MySQL to PostgreSQL conversion script
按其属性或关系对Eloquent模型记录进行排序
GitHub和GitLab缺少IntelliSense提示
[Chrome扩展程序]在github.com活动信息中心上过滤活动。

January 27, 2019

一些 CSS 常用样式
将任何网站转为无服务器API（支持SPA！）
Chrome 插件，查看开源中国软件更新资讯，文档导航，GitHub 趋势榜，linux命令索引，浏览历史记录和时钟页面。
从网易云音乐、QQ音乐、酷狗音乐、百度音乐、虾米音乐等搜索和下载歌曲
程序员找工作黑名单
 论文阅读笔记（分布式，虚拟化，容器，自动机器学习）
Odoo。开源应用程序以拓展您的业务。
用于探索各种解析器生成的AST的Web工具。
使用Mac，iOS，tvOS和watchOS的原生API桥接.NET世界。
一个http api网关

January 26, 2019

使用Docker在CI中运行Lighthouse
文件共享实验
我们一起来还原微信。希望通过 iWeChat 这个项目能过勾勒出微信的设计，使用到的技术手段等
A Go (golang) Custom Flutter Engine Embedder for desktop
wudi/PHP-Interview-Best-Practices-in-China: 📙 PHP 面试知识点汇总📙 PHP 面试知识点汇总
Suitable for Work (NSFW) classification

January 25, 2019

各主要城市的互联网公司黑名单
一个非常固执的，高度个性化的脚本来设置一台新的Mac机器，就像我喜欢它一样！
2018/2019/校招/春招/秋招/算法/机器学习(Machine Learning)/深度学习(Deep Learning)/自然语言处理(NLP)/C/C++/Python/面试笔记
网页版微信API，包含终端版微信及微信机器人
这是我为php访谈准备的信息。笔记包括php，mysql，linux等。
Composer 免签约支付宝与微信，直接到个人账户，持续维护，PHP实现免签约支付接口，判断订单号与备注，自带监听。
我大学两年来的笔记，希望对大家有些些帮助（以后会持续更新）
A keygen for Navicat
PHP安全通信库
极简主义的Vim插件管理器

January 24, 2019

在紧急情况下保存您的代码
fastai深度学习库，以及课程和教程
原生，高性能，跨平台的桌面应用程序 - 使用Reason构建！
Futurice开发人员对Android开发的注意事项和注意事项
中文版 《微服务：从设计到部署》
通过预览，编译，自动完成，着色等提高LaTeX排版效率。
构建GitHub应用程序的框架，用于自动化和改进您的工作流程
简单，可扩展的状态管理。
斗图神器 收集了成千上万的撕逼斗图表情包，在这里你可以快速找到想要的表情
Go by Example

January 23, 2019

用于实时可视化的JavaScript库
用于macOS，Windows，Linux和最终Android的下一代Brave浏览器
终端游戏测试git技能
声明用很好的错误消息验证方法输入/输出。
PHP扩展开发及内核应用
显示和控制您的Android设备
编辑器中的真实浏览器预览，您可以调试。
在Laravel应用程序中捕获传入的电子邮件
应用程序仪表板和启动器
基于laravel + reactjs的问题跟踪工具，适用于中小型企业，开源和免费，类似于Jira。
一个渐进而全面的框架，用于构建基于组件的网站，跨越前端和后端
黑箱应用故障注入和资源发现的攻击模式和原语词典。
在线测试驱动器编程字体
用于运行OAuth2服务器的演示应用程序
用于实现OAuth2服务器的包装器
oauth2-server-laravel 

January 22, 2019

任务运行/简单使用Go编写替代
适用于Android应用的字节码优化器
工程师知识管理系统：基于golang go语言（beego框架）。每个行业都有自己的知识管理系统，EngineerCMS旨在为土木工程师们打造一款适用的基于web的知识管理系统。
自动编译js + css + html
基于运营转换（OT）的实时数据库后端
git-subrepo
golang数据库/ sql的通用扩展
建立在自然节点上的NLP库，具有实体提取，情感分析，自动语言识别等功能
PHP的Kafka客户端
小红书通信协议签名
抖音通信协议签名
快手通信协议签名
Google API的公共接口定义。
一个基于golang的web应用快速开发框架，提供了开放平台及相关open api的封装，可以快速的开发微服务、web应用、微信公众号、企业微信、钉钉、云之家等第三方平台应用
GitPython是一个用于与Git存储库交互的python库。
为 Sketch 准备的模拟数据中文版，包含：中文姓名，手机号，省份，城市，地区，公司名，银行名，星期几，详情地址，邮编，邮箱，颜色，广告词等。
令人愉快的JavaScript测试。
PHP源码加密模块
Go的极简主义websocket框架
MISP（核心软件） - 开源威胁情报平台（以前称为恶意软件信息共享平台）
Siler是一组通用的高级抽象，旨在用于PHP中的声明性编程API。
开源图像托管脚本
PHP Link Checker
将云计算，数据和服务无缝扩展到边缘设备。
使Chrome能够将markdown文件呈现为HTML

January 21, 2019

程序员的 macOS 搭建指南
V2Ray 基于 Nginx 的 vmess+ws+tls 一键安装脚本
ThinkPHP5 社会化登录组件
适用于Linux-Gnome桌面的Mac OS主题
微信、支付宝、QQ 三合一收款二维码（单页版）
使用压缩和经过身份验证的加密对归档程序进行重复数据删除。
为PHP 7提供多进程的扩展
🌴上传组件，可让您节省更多播放LOL的时间。
真正专注于让一套代码运行多端的开发框架，提供标准的MVVM架构开发模式统一各类终端
一系列可打印的单页备忘单，由Markdown使用Pandoc和LaTeX生成
这是来自麻省理工学院，斯坦福大学和普林斯顿等知名大学的免费课程的精选清单。
Python开源Web, CMF，可做微信小程序后端, 网站后端等.Restful Api 
使用Go语言开发的版本发布系统
vue.js(element框架)+golang(beego框架)开发的运维发布系统,支持git,jenkins版本发布,go ssh,BT两种文件传输方式选择,支持部署前准备任务和部署后任务钩子函数
Mysql web端sql审核平台 
TCP数据包监控和统计工具
 Layx 新一代Web弹窗组件。
在线协作与文档管理系统
基于G6和React的可视化图形编辑器
MM-Wiki 一个轻量级的企业知识分享与团队协同软件，可用于快速构建企业 Wiki 和团队知识分享平台。
一个记笔记的应用程序，更好地了解程序员和Markdown。
 Teamcat 软件工程团队协作平台！
参考百度文库，使用Beego（Golang）开发的开源文库系统
TeaWeb-可视化的Web代理服务。
基于Lua的跨平台终端ui库
通过简单的面向对象的类似dom的API在画布上绘制图形。支持Vue＆React / Preact。
使用Three.js构建的声明式3D Globe数据可视化库
RedisPlus是为Redis可视化管理开发的一款开源免费的桌面客户端软件
在Go中快速开发微服务的微服务框架
阿布量化交易系统(股票，期权，期货，比特币，机器学习) 基于python的开源量化交易，量化投资架构 
由TypeScript提供支持的阿里巴巴代表的可管理，可衡量和可追踪的Node.js应用程序管理器
金链盟区块链底层平台
scrapy

January 20, 2019

A Go (golang) Custom Flutter Engine Embedder for desktop
golangci-lint
用于在浏览器中制作交互式音乐的Web Audio框架。
macOS的开源Markdown编辑器。
phpspy 

January 19, 2019

Greenplum数据库
实时性能监控
在JavaScript中实现的算法和数据结构，包含解释和进一步读数的链接
JavaScript 算法与数据结构
Prettier是一个固定的代码格式化程序。
Prometheus的Elasticsearch统计数据导出器
ORM for TypeScript和JavaScript（ES7，ES6，ES5）。支持MySQL，PostgreSQL，MariaDB，SQLite，MS SQL Server，Oracle，WebSQL数据库。适用于NodeJS，浏览器，Ionic，Cordova和Electron平台。
Go的快速脚本语言
一个简单的PHP文件管理器。代码是一个单独的php文件。
A Lua VM in Go
基于搜狗微信搜索的微信公众号爬虫接口
Upload big files for Laravel 上传大文件的Laravel扩展
超过400家易于申请的软件工程公司
用Golang编写的高性能PHP应用程序服务器，负载均衡器和进程管理器
swoft课程代码
material-ui 

January 18, 2019

一个用于渗透透测试演练的WEB系统,用于提升寻找网站能力,也可以用于web安全教学
通过监控wifi信号来计算你周围的人数
深度学习书中文翻译
PHP底层内核源码分析和扩展开发
Git快速统计是一种访问git存储库中各种统计信息的简单而有效的方法。
高性能后台系统构建工具, 使用极少代码即可构建出功能完善的后台系统
一个适用于iOS的原生网络调试工具
程序员的全栈资源集合。
一个基于Yii2高级框架的快速开发应用引擎
WeChat SDK for Yii2 , 基于 overtrue/wechat 4.x
我是木易杨，网易高级前端工程师，跟着我每周重点攻克一个前端面试重难点。
生产级集装箱调度和管理
妈妈再也不用担心我的网易云音乐变灰了
比特币核心集成

January 17, 2019

从网易云音乐、虾米音乐、QQ音乐、酷狗音乐、酷我音乐等搜索和下载歌曲
Popcorn Time for music
Dead simple Object schema validation
Docker速查表
go结构和字段验证，包括跨字段、跨结构、映射、切片和数组潜水
将compose中描述的应用程序部署到kubernetes集群上
内存中的键：用于go的值存储/缓存（类似于memcached）库，适用于单机器应用程序。
您的即时Emacs开发环境。
一种限速器中间件
阿里云翻译小组，为社区输出优质的技术文章。
蓝眼系列软件之《蓝眼云盘》
A RESTful Search Engine 
简单、强大、跨平台的SQLite客户端和ORM
一个浏览器内的IDE，用于浏览GRAPHQL。
基于Webkit/Firefox浏览器，辅助用于12306订票的助手软件。
为国内网站分享的网盘下载提供便利的工具
FastD Swoole 基础组件
一个高性能的PHP API框架。
Moby项目-集装箱生态系统组装基于集装箱系统的协作项目
PHP代码审计分段讲解
1000个PHP代码审计案例(2016.7以前乌云公开漏洞)
自动化运维平台: 代码及应用部署CI/CD、资产管理CMDB、计划任务管理平台、SQL审核|回滚、任务调度、站内WIKI
基于放弃的官方应用程序的Android Github客户端
Thor HTTPS 抓包分析，开发调试利器 for iOS
这些是phpsorm中用于代码完成的助手文件，我使用的是一些开源软件。
高度定制的chrome黑色主题
tensorflow
EXLcode - VS Code-based Online IDE Chrome Extension 
由Visual Studio代码支持的在线IDE
为Web应用程序开发量身定制的在线代码编辑器
描述HTTP / 3和QUIC协议的文档
将终端会话记录为SVG动画
Flink Forward China 2018 Slides
P编程语言。
百度AI开放平台 PHP SDK. 
简体中文的逐步区块链教程
TypeScript的权威指南
MIME组件允许操作MIME类型。
QOR是一组用Go编写的库，它抽象了业务应用程序，CMS和电子商务系统所需的常用功能。
 这是一个很酷炫的前端网站搜集器，导航网
用任何编程语言构建强大的管道。
Ansible是一个极其简单的IT自动化平台，可使您的应用程序和系统更易于部署。
Node.js最佳实践中排名最高的内容的总结和分享

January 16, 2019

Go 学习之路：Go 开发者博客、Go 微信公众号、Go 学习资料（文档、书籍、视频）
Arduino framework for node.js
在本地运行您的GitHub操作
appsync的无服务器插件
📜 33 concepts every JavaScript developer should know.
🚀他妈的快速文件管理器（用bash编写）
Laravel 5.4+内容管理框架
Laravel 5.4+内容管理框架
PHP Telegram Bot.
与用于学习与Node.js相比，Golang的示例
TablePlus
Redis Desktop Manager For Mac OSX DMG 

January 15, 2019

A lightweight MVC framework written in Go (Golang).
您的NoSQL数据库由Golang提供支持
麻省理工学院深度学习相关课程的教程，作业和比赛。
开源免费的简易中文分词系统，PHP分词的上乘之选！
TiDB是与MySQL协议兼容的分布式HTAP数据库
api-problem规范的简单实现
基于Swoole扩展开发游戏服务器框架，示例实现h5游戏开发
rust 全文搜索引擎
用go语言编写的Microsoft SQL服务器驱动程序
syncd是一款开源的代码部署工具，它具有简单、高效、易用等特点，可以提高团队的工作效率.
在不到30s内得到一个干净的开箱即用的临时linux系统.
在Mac上计算你写了多少行代码
用于更改React的RFC
Jupyter笔记本用于“深度学习Python”一书的代码示例
Babel是编写下一代JavaScript的编译器。
💻PHPUnit的并行测试
LiteSpeed Cache for WordPress 
LiteSpeed QUIC Client Library
litespeed - 高性能，轻量级，开源的HTTP服务器
12306
强大，无处不在且可大规模扩展的Jabber / XMPP即时消息平台
🧀将图像上传到公共CDN
中英文敏感词、语言检测、中外手机/电话归属地/运营商查询、名字推断性别、手机号抽取、身份证抽取、邮箱抽取、中日文人名库、中文缩写库、拆字词典、词汇情感值、停用词、反动词表、暴恐词表、繁简体转换、英文模拟中文发音、汪峰歌词生成器、职业名称词库、同义词库、反义词库、否定词库、汽车品牌词库、汽车零件词库、连续英文切割、各种中文词向量、公司名字大全、古诗词库、IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库、中文聊天语料、中文谣言数据、百度中文问答数据集、句子相似度匹配算法集合、bert资源、文本生成&摘要相关工具、cocoNLP信息抽取工具
灵活的Zsh插件管理器，具有干净的fpath，报告，完成管理，turbo模式，服务
Go的基于反射的依赖注入工具包。
PHP中的一个简单的Podcast RSS编辑器
用于语义UI的日历模块

January 14, 2019

现代，疯狂快速，可靠，简单且功能强大的平面文件CMS
高效微信公众号历史文章和阅读数据爬虫powered by scrapy
一款专为 deepin 打造的小飞机
🍎笔记本
js 功能函数库
一系列Android进阶文章
在C中从头开始编写sqlite克隆
Fast3kB React采用相同的现代API替代品。 组件和虚拟DOM。
flutter 开发者帮助 APP，包含 flutter 常用 130+ 组件的中文文档与 demo 演示
LeetCode练习, Go语言版本
一种简单快捷的图像处理工具。
ImgBot在GitHub中抓取所有图像文件，并在应用无损压缩后提交拉取请求。这将使文件大小下降，但保持尺寸和质量同样好。
Elasticsearch Web UI
使用简单的HTML和CSS构建漂亮的Electron应用程序的最快方法
伪装115浏览器

January 13, 2019

一种有效的浏览器扩展，可以阻止整个网络上基于浏览器的加密货币挖掘者。
用Go编写的YouTube下载库和CLI
Go（Golang）中的指数退避算法。
一款入门级的人脸、视频、文字检测以及识别的项目。
基于aria2的轻量级多线程下载器。
帮助115导出下载链接到aria2-rpc
遵循gRPC HTTP规范的gRPC到JSON代理生成器
将github贡献图表嵌入图像
flutter 开发者帮助 APP，包含 flutter 常用 130+ 组件的中文文档与 demo 演示
学习PHP的在线书籍

January 12, 2019

用于OpenTracing的NGINX插件
“玄魂工作室--安全圈” 知识星球内资源汇总
📖「一个」、「Time 时光」、「开眼」、「一席」、「梨视频」、「微软必应词典」、「金山词典」、「豆瓣电影」、「中央天气」、「魅族天气」、「每日一文」、「12306」、「途牛」、「快递100」、「快递」应用 Api。
前端决策树
无它术，唯勤读书而多为之，自工
软件版本控制可视化
Powerline是vim的状态行插件，并为其他几个应用程序提供状态和提示，包括zsh，bash，tmux，IPython，Awesome和Qtile。
机器学习备忘录
Tivi是一款正在进行中的电视节目跟踪Android应用程序，它连接到Trakt.tv。
QQ 音乐接口 api
Guzzle Swoole Handler
Electron for PHP. Desktop GUI framework with HTML5 Chrome/IE engine. 
Electron for PHP. Desktop GUI framework with HTML5 Chrome/IE engine. 
php_desktop 像开发网站一样开发桌面应用软件
CRMEB是一款微信公众号和小程序的电商系统，带分销、拼团、秒杀、砍价、优惠券、积分等功能

January 11, 2019

a cron library for go
A theme that adds the dark Incognito Mode colour scheme to the normal mode of Chrome.
Python和CLI的快速，可扩展的进度条。
一个窗口切换器，应用程序启动器和dmenu替换
专为程序员编写的英语学习指南。v1.0
用于移动网页的轻量级，可扩展的前端开发人员工具。
算法实现在GoLang
一个基于Laravel的发布平台
kafka php client
中华人民共和国国家标准 GB/T 2260 行政区划代码
记录前端开发中的技巧以及算法知识
python版本：领域细分的中文分词工具，简单易用，跟现有开源工具相比提高了分词的准确率。
CSS Inspiration，在这里找到写 CSS 的灵感
使用直接SQL查询编写API没有麻烦，让我们重新思考SQL
一个PHP包，用于向用户显示读取内容所需的时间。

January 10, 2019

caliber是一名电子书经理。 它可以查看，转换，编辑和编目所有主要电子书格式的电子书。
Python最佳实践指南
一个轻量级库，用于将复杂对象转换为简单的Python数据类型。
Java 8 Jar和Android APK逆向工程套件
pytorch handbook是一本开源的书籍，目标是帮助那些希望和使用PyTorch进行深度学习开发和研究的朋友快速入门，其中包含的Pytorch教程全部通过测试保证可以成功运行
JavaScript中的PFS实现
Perun是一款主要适用于乙方安服、渗透测试人员和甲方RedTeam红队人员的网络资产漏洞扫描器/扫描框架
Go学习笔记
机器学习初学者公众号作品
查找向项目添加新依赖项的成本
将博客网站转换为合并pdf的示例程序。
功能全面的php命令行应用库。提供控制台参数解析, 命令运行，颜色风格输出, 用户信息交互, 特殊格式信息显示 
100天的ML编码
用于DOM操作的最小独立JS库
一个静态博客写作客户端 
🚂 12306 购票助手，支持分布式，多账号，多任务购票

January 9, 2019

Windows 10的macOS Mojave Dynamic Desktop功能端口
一款用 Java 实现的现代化社区（论坛/BBS/社交网络/博客）平台。
离线存储，改进。 使用简单但功能强大的API包装IndexedDB，WebSQL或localStorage。
在持久性引擎之间同步数据，就像ETL一样，只是不稳定
新浪微博图床 Chrome 扩展，支持同步到微相册
使用libgmp对大整数进行算术运算
用于使用URL语法传输数据的命令行工具和库，支持HTTP，HTTPS，FTP，FTPS，GOPHER，TFTP，SCP，SFTP，SMB，TELNET，DICT，LDAP，LDAPS，FILE，IMAP，SMTP，POP3，RTSP和 RTMP。 libcurl提供了无数强大的功能
通过设计稿一键智能生成视图代码，目前支持生成 Vue、React、Html5、Weex Rax 等常见 DSL。
Swoole支持Expressive应用程序
轻量、可靠的小程序 UI 组件库
XPay个人免签收款支付系统 完全免费 资金直接到达本人账号 无需备案 无需签约支付宝微信 无需挂机APP 无需插件 无需第三方支付SDK 无需营业执照身份证 只需收款码 搞定支付流程 现已支持移动端支付
用于构建优秀社区的简单论坛软件。
网络终端
Matomo是Google Analytics的领先开放替代品，可让您完全控制数据。 Matomo可让您轻松收集来自网站，应用和物联网的数据，并可视化这些数据并提取见解。
以快速，可扩展的方式读写电子表格文件（CSV，XLSX和ODS）
禅道

January 8, 2019

现代复制到剪贴板。
Linuxlinuxbrew.sh的Homebrew包管理器
ScreenToGif允许您录制屏幕的选定区域，编辑并将其保存为gif或视频。
PHP 获取快递物流信息
README文件语法解读，即Github Flavored Markdown语法介绍
秋招面试总结
Apache Dubbo（孵化）是一个基于Java的高性能开源RPC框架。
一个简单的守护进程，允许会话软件更新固件
现代JavaScript日期实用程序库
基于复制和翻译的国外纸质阅读和翻译助手。
websocket命令行工具
记录并重播网络
傳承字形標準化文件
🏝钉钉 SDK • 👷‍♂️2.0 开发中
eSpeak NG是一个开源语音合成器，支持101种语言和口音。
一组匹配中国大陆手机号码的正则表达式。
Pika是与redis兼容的nosql，由Qihoo的DBA和基础架构团队开发
微信小程序开发资源汇总 💯
使用GTK + 3的Linux平铺终端仿真器
使用开放式Web技术构建令人惊叹的原生和渐进式Web应用 一个应用程序在所有东西上运行🎉
Python 艺术二维码生成器
收集整理远程工作相关的资料
一个简单而优雅的客户端，用于访问和控制Kubernetes集群
终端会话记录器
自由·负责·克制 去广告 Hosts 项目
免费中文字体
根据Dice的系数找出两个字符串之间的相似程度，这个系数大多优于Levenshtein距离。
根据Dice的系数找出两个字符串之间的相似程度，这个系数大多优于Levenshtein距离。
可靠的USB格式化实用程序
Darling是OS X应用程序的运行时环境。
收集iOS应用程序中最常见的漏洞
使用Trilium Notes构建您的个人知识库
用于软件和Web开发的免费API的集合列表。

January 6, 2019

一个小型JavaScript库，用于计算太阳/月亮位置和阶段。
AVH版的git扩展，为Vincent Driessen的分支模型提供高级存储库操作
The Kubernetes Package Manager 
GirlsInAI 是一个面向编程零基础女孩子的AI算法工程师养成计划。
Go package captcha实现了图像和音频CAPTCHA的生成和验证。
Here Music 一个 使用 Electron + React 开发的音乐客户端
使用Github GIST在多台计算机上同步Visual Studio代码设置

January 5, 2019

适用于iOS 11.4.1-iOS 12.1的漏洞利用
walle - 瓦力 开源项目代码部署平台
Go to JavaScript中的编译器，用于在浏览器中运行Go代码
技能树
laravel5.5和vue.js结合的前后端分离项目模板。
微服务的标准库。gokit.io
物联网设备的WebUI仪表板喜欢raspberry pi。
12306智能刷票，订票
PhpStorm的PHP运行时和扩展头文件
JSON，CSV，XML和Yaml的世界国家。

January 4, 2019

极客挚爱的在线技术平台
🔎在社交网络中查找用户名
将使用STDIN / STDOUT的任何程序转换为WebSocket服务器。像inetd一样，但对于WebSockets。
JavaScript国际化框架
用Golang编写的高性能PHP应用程序服务器，负载均衡器和进程管理器
WordPress.com for Desktop
明星和感谢作者许可证（SATA许可证）
Zipkin是一个分布式跟踪系统
论文与代码。按星星排序。每周更新。
关于Detour App规则配置的简单介绍
wingy-announcement
💾 Flysystem adapter for the oss storage.
一个实验性的点对点Web浏览器beakerbrowser.com
跨平台HTTP和GraphQL客户端
A Simplenote React app packaged in Electron
通用剪贴板管理应用程序，可以从任何设备上的任何位置轻松访问剪贴板。
Mac和Windows上的Visual Docker容器管理
ZeroNet  - 使用比特币加密和BitTorrent网络zeronet.io的分散式网站

January 3, 2019

以91％的准确率击败最新版本的
有助于管理Git托管的PHP项目版本号的库
带有解析器的HTML5视频播放器可以节省流量
整个百度使用的工业级RPC框架，拥有1,000,000多个实例和数千种服务，在百度内部称为“baidu-rpc”。
tmux源代码
Oss storage filesystem for Laravel.
golang123 是使用 vue、nuxt、node.js 和 golang 开发的知识分享系统 golang123.com
zsh的插件管理器。
一个简单的脚本来创建github toc
使用相同的现代API替代Moment.js的不可变日期库
基于Raft构建的分布式MySQL binlog存储系统
官方redis集群的Python集群客户端。 Redis 3.0+。
一系列精选的面试问题列表。
🔐了解如何使用JSON Web Token（JWT）来保护您的下一个Web应用程序！（教程/测试示例!!）
利用现代浏览器所提供的强大 API 录制并回放任意 web 界面中的用户操作。
成为2019年Web开发人员的路线图

January 2, 2019

polarphp
PHP应用程序的即时升级
Redis集群的Openresty lua客户端。
排名前200位的深度学习Github存储库按星数排序。
按特定日期获得的星数排序的前100个趋势深度学习资源库。
Golang命令教程中文。
我的专栏“Core Golang  -  36课”的示例项目
awesome-go-China
高性能，极简主义的Go web框架echo.labstack.com

January 1, 2019

中国省市区数据
Python和命令行的世界上最简单的面部识别API
GitHubDaily 分享内容定期整理与分类。欢迎推荐、自荐项目，让更多人知道你的项目。
Visual Studio Code的UNOFFICIAL网易音乐扩展
适用于Composer的快速，可靠且安全的NPM / Yarn桥接器
科学上网的有趣项目集锦，欢迎大家pr自己喜欢的项目到这里。
WebTorrent
webtorrent-desktop 
GitHub Dark as a userscript 
Stylus - Userstyles Manager 

December 31, 2018

一个composer包，用于验证以前是否使用Have I Been Pwned API在密码中使用了密码。
微信公众号管理系统，也是一套微信公众号开发框架。支持移动管理，几乎集合微信功能，简洁、快速上手、快速开发微信各种各样应用。
PrestaShop提供完全可扩展的开源电子商务解决方案。
🐘 A PHP prober (一款精美的 PHP 探針, 又名X探針、劉海探針)
建立和管理Phars的申请。
提问的智慧
科学上网的有趣项目集锦。

December 30, 2018

功能齐全的下载管理器。
一个用PHP和Redis编写的Twitter玩具克隆，在早期用于介绍Redis数据类型。
用于golang的socket.io库，一个实时应用程序框架。
丰富的表情符号包资源。
Sublime Text 2和3中PHP项目的智能代码完成。
在一个地方管理你的git存储库。
用于动态生成PDF文档的PHP库。
aveo基于Laravel框架的开源票务系统。
一个小型JavaScript库，可以从数字生成类似YouTube的ID。 当您不希望向用户公开数据库ID时使用它。

December 29, 2018

Spring Boot教程。
💰 微信/支付宝收款监控，个人收款无需签约支付宝、微信支付。为支付宝、微信支付的个人账户，提供即时到账收款服务。
常用交互式命令行用户界面的集合。
JavaScript 代码规范，自带 linter & 代码自动修正。
从HTML表单中提取的PHP表单验证。在同一个地方写一次表格和验证！
一个Web代理工具。
将ArchLinux安装为WSL实例。 支持多重安装。
Laravel核心代码学习。
一个阮一峰博客风格的Hexo主题。
用于长期短期内存网络（LSTM）的可视化工具箱。
Tinker in your browser。
Gonum是Go编程语言的一组数字库。它包含用于矩阵，统计，优化和更多。
基于 PAYJS 微信支付个人接口开发的 Laravel Package，可直接用于生产环境。
基于 PAYJS 微信支付个人接口开发的 Package，可直接用于生产环境。
用于快速文本表示和分类的库。
一个集审核、执行、备份及生成回滚语句于一身的MySQL自动化运维工具之手册部分
Go模板的有用模板函数。
PostgreSQL数据库的跨平台客户端。
macOS的简单SSH快捷菜单。
务实地搜索模型和其他来源。
 iBrand EC 是一个免费的开源电子商务解决方案，使用 PHP 基于 Laravel 框架进行编写。
Laravel的Web安装程序。

December 28, 2018

生辰八字，五行，算命。
用于安全和可扩展的网络流量分析的框架。
一个检测移动设备的简单JS库。
Laravel 5的数据清理程序和表单请求输入卫生。
具有GPL许可证的高性能MySQL代理。
中国省/自治区/直辖市、市/自治州、区/县/旗数据，包含名称、拼音、拼音首字母、行政代码、区号。
用于构建JSON API的规范。
AliSQL是一个源自阿里巴巴集团的MySQL分支。
Laravel + go-micro + grpc + Zipkin。
Laravel + go-micro + grpc + Zipkin
在Symfony sylius.com之上的开源电子商务框架。
Spree是一个完整的，模块化的，API驱动的开源电子商务解决方案，适用于Ruby on Rails。
微信个人号接口、微信机器人及命令行微信，三十行即可自定义个人号机器人。
VM and compiler for Lua in Go。

December 27, 2018

 Segment Fault 在线讲堂 代码工程。
「小马哥技术周报」

December 26, 2018

PHP TensorFlow绑定。
一个字体系列，为程序员提供了一个很好的等宽变体。
📚学习机器学习的实用方法。
异步WebSocket客户端。
Événement是一个非常简单的PHP事件调度库。
开箱即用的中台前端/设计解决方案。
ant-design-mobile
深入理解PHP内核。
GitUp 快速，安全，无头痛地工作。 你终生 失去的Git界面 终于到来了。
具有交互式TLS功能的拦截HTTP代理，适用于渗透测试人员和软件开发人员。
htop是Unix系统的交互式文本模式进程查看器。
go-internals 本书是关于 Go 程序设计语言内部实现原理的阐释，当前正在进行中。
Repo for gRPC PHP。

December 25, 2018

由Visual Studio Code提供支持的在线IDE。
将终端记录转换为GIF动画。
在 Windows 上用 WSL 优雅开发。
Lua和OpenResty的验证库（输入验证和过滤）。
Hprose基于swoole的异步客户端和独立服务器。
Cloud-Native API网关和服务网格。
PHP client/server for the telegram MTProto protocol 
🐜 A UI Design Language。

December 24, 2018

功能强大的免费软件，也恰好是开源Python。
乡村信息系统（SID）。
 ss-panel-v3-mod是一款专为shadowsocks设计的web前端面板。
一个各种方式突破Disable_functions达到命令执行的shell。
收集一些小型实用的工具。
从零开始内网渗透学习。
自动化收集linux信息。
Gin是一个用Go（Golang）编写的HTTP Web框架。
python-web入坑指南。

December 23, 2018

Flexihash是一个小型PHP库，可实现一致的hashing。
基于iView的Vue 2.0管理系统模板。
通过动画QR码传输数据。
PHP制作了加密货币。
Supervisord监控工具。
用Golang编写的高性能PHP应用程序服务器，负载均衡器和进程管理器。
这是ZipArchive方法的简单包装器，带有一些方便的功能。
用于Git repos和npm包的CLI发布工具。
PHP的事件驱动，非阻塞I/O。

December 22, 2018

通过LD_PRELOA绕过disable_functions
Rubix ML是一个高级机器学习库，可让您构建使用PHP语言从数据中学习的程序。
一个方便实用的工具，用于在redis组之间迁移数据。
基于Laravel的API服务端架构代码。
时尚的CLI提示用户友好，直观且易于创建。
RedisDesktopManager-Windows 安装包和编译教程。
建立在yaf的基础上，集成了Smarty引擎，加入了封装好的各种功能类。
开源Node.js无头CMS，轻松构建可定制的API。
学习Python 3示例代码。
一个类似jquery的python库。
持续收集国内免费优质API。
ip2region - 最自由的ip地址查询库，ip到地区的映射库，提供Binary,B树和纯内存三种查询算法，妈妈再也不用担心我的ip地址定位。
JavaScript的“警报”的美丽替代品。

December 21, 2018

oh-my-posh。
适用于WordPress的GraphQL API。
麻省理工学院6.824分布式系统类的基本资源。
序列化闭包（匿名函数）。
精通以太坊 （中文版）。
用于创建彩色控制台输出的简单库。
一个美丽的hexo博客主题与材料设计和响应设计。
电子表格分析器和编写器。
Java 编程思想。

December 20, 2018

Rails Girls Guides。
输入SQL，输出索引优化建议。
f-admin是一套基于Laravel框架开发的基础权限后台系统。
用于#golang（WIP）的实验性新HTTP客户端API。
包 goconfig 是一个易于使用，支持注释的 Go 语言配置文件解析器，该文件的书写格式和 Windows 下的 INI 文件一样。
💫一系列精彩的列表，手册，博客，黑客，单行，cli / web工具等等。特别是对于系统和网络管理员，DevOps，Pentesters或安全研究人员。
NumPy和Pandas与大数据的接口。
PHP 中文工具类，支持汉字转拼音、拼音分词、简繁互转。
Docker  - 初学者|中级|高级。
基于TP3.1的多用户BT离线下载。
ftl-desktop 下载器。
MongoDB到Elasticsearch连接器。

December 19, 2018

一个更新鲜的“在GitHub上叉我”标注。
将markdown文档可视化为思维导图。
用纯Python编写的计算机代数系统。
Golang好文推荐；收录平时阅读到的一些Go相关写的比较好、质量较高的干货文章。
memcached和redis的快速，轻量级代理。

December 18, 2018

GRPC服务的GUI客户端。
Github iOS客户端用RxSwift和MVVM编写的干净架构。
PHP的Diff实现，从PHPUnit中分解为一个独立的组件。
Soli PHP框架。
MIT课程《Distributed Systems 》学习和翻译。
中国低线城市（或称三四线城市）的机会。
Phar安装和验证环境（PHIVE）。
对文件/目录中的所有PHP命名空间进行前缀，以隔离PHAR中捆绑的代码。
Navicat Keygen。
Laravel 社区门户网站。
纯Go中的高度可扩展的Git实现。
Laravel Dusk 基于laravel测试的漂亮仪表板。
Awesome English。
一种更快，更简单的方式来驱动支持Chrome DevTools协议的浏览器。
Package Repo Search。
php-lisp is a Lisp dialect written in PHP. 
GitHub Workflow for Alfred 3.
用于防止睡眠的macOS应用程序。
小明VPN。
一个可以观看国内主流视频平台所有视频的客户端（Mac、Windows、Linux）

December 17, 2018

TinySSH是小型服务器（少于100000字的代码）。
在Node.js和浏览器中生成大量真实的假数据。
广告过滤Adblock，uBlock Origin，Adguard。
Linux内核源代码。
用于Graphite，InfluxDB和Prometheus的漂亮监控和度量分析和仪表板的工具。
快速轻松地管理和切换多个代理。
Go By Example 中文版。
徽章。
基于 Vue.js 的小程序开发框架，从底层支持 Vue.js 语法和构建工具体系。
Tideland GoLib。
一个小型PHP库，用于从数字生成类似YouTube的ID。 当您不希望向用户公开数据库ID时使用它。
MinTTY的一些配色方案。

December 16, 2018

Docker官方映像包装PHP。
由Firebase提供支持的协作文本编辑器。
秒杀系统设计与实现.互联网工程师进阶与分析。
为PHP代码覆盖率信息提供收集，处理和呈现功能的库。
Dillinger是一款支持云端，移动就绪的离线存储，AngularJS支持的HTML5 Markdown编辑器。
Win32 port of OpenSSH。

December 15, 2018

HTTP负载测试工具和库。
自动生成 ppt。
基于vegeta和boom的http基准web应用程序。
录制终端并生成动画gif图像或共享网络播放器。
go 支持移动设备。
node.js命令行界面变得简单。
CKEditor 5的开发环境 - 最好的基于浏览器的富文本编辑器。

December 14, 2018

一个通用字幕查找器，可以查找字幕并下载。
检查中文 markdown 编写格式规范的命令行工具，基于 AST，方便集成 ci，写博客 / 文档必备。
在macOS和Linux上更好的微信。用Electron制造。
一套完整的学习手册帮助自己准备 Google 的面试。
适用于Windows的软件发行版和构建平台。
JSON-RPC 1/2传输实现。 支持python 2/3和pypy。
在Android上安装并运行GNU / Linux。
WeHalo 简约风 的微信小程序版博客。

December 13, 2018

GitLab CE Mirror。
Windows的命令行安装程序。
一个更现代化的终端。
在PHP中实现Token Bucket算法。
Debian，Ubuntu和CentOS的OpenVPN road warrior安装程序。
分布式可靠键值存储，用于分布式系统的最关键数据。
TiDB是与MySQL协议兼容的分布式HTAP数据库。
DevHub: TweetDeck for GitHub - Android, iOS and Web。
基于DuerOS的个人的智能语音助手。
在切换开关中转动复选框和单选按钮。
适用于Google翻译的免费且无限制的API。
Node.js的可移植Unix shell命令。
学习Golang。
准备好使用JSONP端点/有效负载来帮助绕过不同网站的内容安全策略。
HookPHP一款基于C扩展搭建支持AI在线编程的PHP框架-安全秒杀ThinkPHP-性能秒Laravel-功能秒YAF-易用秒Symfony-入门秒Zend-组件秒Yii-耦合秒Phalcon 。

December 12, 2018

Python代码的静态分析器。
Hoa是一个模块化，可扩展和结构化的PHP库集。
Hoa是一个模块化，可扩展和结构化的PHP库集。
此工具可帮助您测试socket.io服务器。
ClickHouse是一个用于大数据的免费分析DBMS。
公众号 swoole4.0之打造自己的web开发框架 代码。
1 kB用于构建声明性Web应用程序的JavaScript微框架。
awesome design systems。
向你的贡献者展示一些爱！您的repo README的小部件。每小时刷新一次。
Python开发工作流程。
jsliang 的文档库. 里面包含了所有的前端文章，例如 vue、react,、angular、微信小程序、设计模式等……
⚡️通过在空闲时间预取视口内链接来加载后续页面加载。
有史以来为PHP创建的最棒的验证引擎。
收集工具和改进使PhpStorm更好一点。
Atom文件特定的图标。

December 11, 2018

基于NodeJS的跨平台，免费和开源密码管理器。
A proxyee-down extension for baiduyun。
Proxyee Down扩展存储库。
演示React项目如何接入Fundebug错误监控服务。
Go的简单文件嵌入器。
The official Go package for NSQ。
For macOS.百度网盘 破解SVIP、下载速度限制~
OpenTracing API for PHP。
将任何脚本或程序的输出放在Mac OS X菜单栏中。
这个javascript库解析PHP代码并将其转换为AST。
Chrome扩展以树格式显示Gitlab代码。
一个授权库，支持Golang中的ACL，RBAC，ABAC等访问控制模型。
专为ThinkPHP5.1定制的Casbin的扩展包，Casbin是一个功能强大，高效的开源访问控制库。
TomatoIDC虚拟主机销售系统。
快速，高效且易于使用的JSON pull解析器。
Composer更新后显示更好的摘要。
一个授权库，支持PHP casbin.org中的 ACL，RBAC，ABAC等访问控制模型。
一个授权库，支持PHP casbin.org中的 ACL，RBAC，ABAC等访问控制模型。
PHP电子邮件验证器库受到。
检查以下RFC的电子邮件地址：3696,1123,4291,5321,5322 isemail.info
Spotify Web API的Go包装器。
《Github 帮助文档》 中文翻译。

December 10, 2018

为每个人提供客户端JavaScript PDF生成。
重新编写，使用react ，babel，webpack和其他现代东西。
phpstorm插件,用于thinkphp5框架的视图,配置,路由,数据库,模型智能提示和跳转。
输入git open打开浏览器中存储库的GitHub页面或网站。

December 9, 2018

构建OAuth和OpenID Connect服务器的终极Python库。 包括JWS，JWE，JWK，JWA，JWT。
基于workerman的PHP中socket.io的服务器端替代实现。
使用一个命令设置git，vim，zsh，SublimeText，tmux等。
验证表单异步。
eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee
在HOME中以纯PHP形式存储和管理PHP版本。

December 8, 2018

适用于Golang的类型安全的Redis客户端。
JavaScript的Base64实现。
谷歌访问助手破解版。
php框架基准测试（包括laravel，symfony，silex，lumen，slim，yii2，tastphp等）
为GitHub README.md轻松创建TOC。
用动画的形式呈现解LeetCode题目的思路。
Tampermonkey脚本从Stack Overflow等复制代码。
aria2是一个轻量级的多协议和多源，跨平台下载实用程序，在命令行中运行。
webui-aria2。
更优雅的驾车体验。
Charles 破解工具。
listen1 desktop。

December 7, 2018

Larastan - 在不运行代码的情况下发现代码中的错误。
Go设计模式，食谱和习语的精选列表。
PHP的简单多进程管理器，基于pcntl和posix。
PHP的可扩展微框架。
我的Python示例 http://www.thegeekblog.co.uk
基于workerman的shadowocks的php端口。
Flutter可以轻松快速地构建漂亮的移动应用程序。
PHP的基本CURL包装器。
easyProxy是一款轻量级、高性能、功能最为强大的内网穿透代理服务器。

December 6, 2018

用于处理Tumblr博客的工具，Tumblr备份。
JSBox 扩展 demo。
jsbox、pin 使用技巧。 
在iOS设备上无需越狱即可检索InfoPlist。
应用程序中开放系统设置的演示（iOS 10.2）。
命令行的艺术。
基于浏览器的代码编辑器。
一个简单的本地图像，用PHP编写的缩略图生成脚本.
我的ZSH配置和dotfiles。
One for all free music in china for Windows with fluent UI. https://github.com/oyrx/listen1_desktop。

December 5, 2018

golang库用于读写Microsoft Excel。
Dan Abramov 的个人博客。
Rules / 规则：Surge / Shadowrocket / Quantumult。
You-Dont-Know-JS中文版。
计算机操作系统慕课笔记。
Laravel专用OSS扩展包。
基于C的gRPC（C ++，Python，Ruby，Objective-C，PHP，C＃）。
将JSON数据转换为PHP类对象。
开源书籍：《Shell 编程范例》，面向操作对象学 Shell！
廖雪峰 learn java。
极客时间：nginx核心知识100讲配置文件与代码分享。
Laravel 的 Websockets。
项目管理系统接口。
墙外到墙内搬运工。
暗网网址大全TOR。
Tor Browser。
中文暗网爬虫。
一个简单的交互式Go解释器。

December 4, 2018

基于Web技术构建的终端。
基于UWP和Web技术的终端。
谷歌蜻蜓计划。
CLI爱好者的终端框架，插件和资源的精选列表。
使用Hexo构建您自己的网站。
将MySQL binlog解析为您想要的SQL。
一个和Laravel的dd一样方便调试的包。
Golang 高效编码引擎。
《The Way to Go》中文译本，中文正式名《Go 入门指南》
Training for Golang。
如何使用golang构建Web应用。
中文版 awesome-go。
Golang标准库。

December 3, 2018

微信里面的黑科技。
了解如何将动画引入您的Web项目。

December 2, 2018

👄小程序And公众号商城，外加后台，功能齐全！
🎬豆瓣电影传送门。

December 1, 2018

微信支付单文件版。一个PHP文件搞定微信支付系列。包括原生支付（扫码支付），H5支付，公众号支付，现金红包、企业付款到零钱等。
一个PHP文件搞定支付宝支付系列，包括电脑网站支付，手机网站支付，扫码支付，JSAPI支付、单笔转账到支付宝账户、交易结算（分账、分润）、网页授权获取用户信息等。
Laravel最佳实践。

November 30, 2018

Go Web的Go-Mega教程。
免费的Kindle电子书资源。
精选的博客列表。
Laravel Nova资源的精选列表。
开源php加密运行扩展，基于screw二次开发。
golang 学习笔记。
Go语言学习笔记。
《Go Web 编程》 
《Go 入门指南》
beego是Go编程语言的开源，高性能Web框架。

November 29, 2018

🛁适用于JavaScript的Clean Code概念。
为PhpStorm和IntelliJ添加PHP注释支持。
Node.js CMS和Web应用程序框架。

November 28, 2018

Chrome 扩展：麻麻再也不用担心 Google API 抽风了。
这是PHP的一个实现，用纯Go编写（尽可能，现在pcre在pure go中不存在并且需要使用libpcre）。
功能强大且易于使用的PHP微框架，旨在帮助您快速构建动态，强大的Web应用程序！
此PHP类使用FastCGI协议处理与FastCGI（FCGI）应用程序的通信。
用于处理二进制和十六进制数据的工具箱。与NodeJS Buffer类似。
将Laravel，LaraDock [Laravel + Docker]和PHPStorm连接在一起。
由微信开发的高效，小型移动键值存储框架。适用于iOS，macOS和Android。
跨平台异步I / O http://libuv.org/。
用 electron 生成的 Shadowsocksr客户端。
MongoDB对象文档映射器（ODM）
用于在PHP中使用MongoDb。

November 27, 2018

91云服务器一键测试包。
用Go（golang）编写的完整比特币解决方案。
Go的执行日志。
⏲️定时任务脚本，推送前端资讯到微信/Telegram。
煎鱼的博客，啊（golang 的一些文章）。
区块链钱包技术指南。
锁定库提供PHP代码的序列化执行。
Yii3 web application template。
Yii Framework 3.0 core。

November 26, 2018

Deepin wine for ubuntu。
Flash OS映像到SD卡和USB驱动器，安全，轻松。
Chinese Identity Card package （中国大陆）公民身份证类。
Linux 内核揭密。
带有详细注释的 Redis 3.0 代码（annotated Redis 3.0 source code）。

November 25, 2018

淘气字符串的大清单是一个字符串列表，当用作用户输入数据时很可能导致问题。
“不要编写你的UI代码，画它！”
一个到处运行的科幻桌面。

November 24, 2018

一套用于Laravel的高级Eloquent。
延迟队列。
我的油猴脚本。
查看 Github 其他用户的时间表。
如何成为一名程序员 中文版。

November 23, 2018

Tipask是一款开放源码的PHP问答系统，基于Laravel框架开发，容易扩展，具有强大的负载能力和稳定性。
NGiИX配置生成器。
京价保（京价宝）—— 一个帮助你自动申请京东价格保护的chrome拓展。

November 22, 2018

标准图书馆。
kafka php客户端。
The Elements of Statistical Learning (ESL)的中文翻译、代码实现及其习题解答。
最好用的PHP汉字转拼音类，支持获取汉字的拼音以及拼音的缩写，能准确匹配6千多个汉字。
GitHub code tree。
macOS hosts 文件管理器。
 Go 语言高性能分词。
CLI用于将各种网站的流提取到您选择的视频播放器。
Go Web服务器的实时重新加载实用程序。
GraphQL是一种与任何后端服务相关联的查询语言和执行引擎。
一个浏览器扩展，为GitHub，Gitlab，Bitbucket，gitea和gogs提供不同的文件类型。
兼容的Redis协议NoSQL数据库。
Kubernetes中文指南/云原生应用架构实践手册。
学习正则。
一个命令行工具，用于派生bip32地址和私钥。
在云端发送您的项目。
网站讨论平台composer包。
swoole队列。

November 21, 2018

直接在CSV或TSV文件上运行SQL。
多集群 Kubernetes 的Web UI。
WeUI的轻量级JavaScript库。
golang的算法和数据结构。
使JSON / JSONP易于阅读。
一个JavaScript / Python / PHP加密货币交易库，支持超过100个比特币/ altcoin交换。
📚 W3School 教程整理 http://www.w3cschool.cc
中英文敏感词、语言检测、中外手机/电话归属地/运营商查询、名字推断性别、手机号抽取、身份证抽取、邮箱抽取、中日文人名库、中文缩写库、拆字词典、词汇情感值、停用词、反动词表、暴恐词表、繁简体转换、英文模拟中文发音、汪峰歌词生成器、职业名称词库、同义词库、反义词库、否定词库、汽车品牌&零件词库、时间抽取、连续英文切割、中文词向量大全、公司名字大全、古诗词库、IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库、中文聊天语料、中文谣言数据。
灵活而强大的通用路由解决方案。
开源看板（用Meteor建造）。
开放中文知识图谱的schema。

November 20, 2018

公共维护的Pholcus爬虫规则库。
Swagger 2.0实现go。
一个开源的自托管持续集成和部署系统。
后台技术栈/全栈开发/架构师之路，秋招/春招/校招/面试。
""结巴""中文分词的Node.js版本。
微信ipad、微信mac协议，可实现微信80%功能；支持62数据登录、扫码登录、收发朋友圈、查看朋友圈、微信建群、微信拉人进群、微信公众号阅读、微信消息收发、微信附近的人定位、微信添加好友、微信红包接收、微信防撤回、分享小程序、微信加粉、微信收藏、微信标签等。
淘气字符串的大清单是一个字符串列表，当用作用户输入数据时很可能导致问题。
在浏览器中运行SQL。
支付宝（蚂蚁金服）开放平台第三方 PHP SDK，基于官方 3.3.0 版本，助力支付宝小程序后端开发。
PHP中的简单加密。
Spring Boot 教程、技术栈示例代码，快速简单上手教程。
基于Swift的iTu​​nes插件，用于在桌面上显示歌词。
适用于Mac的TinyPNG客户端.
作曲家并行安装插件-加速包安装。
Caddy是一款可立即投入生产的开源Web服务器，它快速，易用，并且可以提高您的工作效率。
Certbot是EFF的工具，用于从Let's Encrypt获取证书，并且（可选）在您的服务器上自动启用HTTPS。它还可以充当使用ACME协议的任何其他CA的客户端。
Laravel Echo的Socket.io服务器。
总结关于科学上网的概念方法及工具。
这里唯一一个gfwlist。
微小版本的gfwlist，仅关注常见网站。
实时网络的开源数据库。
thumbor是一个开源的照片缩略图服务。

November 19, 2018

股票期权，RSU，税收阅读。
HTTP 相关的 RFC 中文翻译（中英文对照）。
静态网页生成器大合集汇总网站。
一个用于在 MacOS 上平滑你的鼠标滚动效果或单独设置滚动方向的小工具, 让你的滚轮爽如触控板 。
使用vue构建electron应用程序。
Postman中文使用说明。

November 18, 2018

使用Go + HTML5构建跨平台的现代桌面应用程序。
iOS 12 捷径创建者。

November 17, 2018

Cloudflare CNAME接入。
小米笔记本PRO安装macOS Mojave & High Sierra 使用说明。
Go的微型和可插拔Web框架。
PHP静态分析工具 - 发现代码中的错误而不运行它！
可组合Docker管理。
Compose setup for Portainer。

November 16, 2018

Golang实现JSON网络令牌（JWT）。
轻松的文档。
灵活且可扩展的CMS，可在网络上及以后创建定制的数字体验。

November 15, 2018

Docker UI管理器。
用于构建代理以绕过网络限制的平台。
最好用的 V2Ray 一键安装脚本 & 管理脚本。
机器人/机器人/爬虫/刮刀/蜘蛛使用的HTTP用户代理的语法模式。
符合规范，默认情况下是安全的PHP OAuth 2.0服务器。
swoole 开发的mysql数据库连接池。
Go的解析器库。

November 14, 2018

MySQL JSON Explain Analyzer。
将Lua的强大功能嵌入到NGINX HTTP服务器中。
PHP图像处理。
Laravel的实时信使。
快速而强大的Web服务器和应用程序服务器。
强大的音乐API框架。
为您的Electron应用程序创建一个Windows包。
Zephir是一种编译的高级语言，旨在为PHP创建C扩展。
Go的快速HTTP包。调整为高性能。热路径中的零内存分配。比net / http快10倍。
Redis在PHP中分布锁。
基于Node.js的中文分词模块。
下一代ShadowsocksX。

November 13, 2018

跨平台Go日志库。
用 go 实现 laravel。
zsh的下一代插件管理器。
awesome-zsh-plugins。
google 的 ggrc-core。
Go的结构化可插入日志记录。

November 12, 2018

Vue.js的移动UI元素。
PHP的一个面向对象的多进程管理器。
PHP中基于Web的文件管理器，使用Tiny File Manager高效，轻松地管理文件。
JetBrains 系列软件汉化包。

November 11, 2018

go的依赖工具。
闲耘的 rime 输入法配置。
【鼠鬚管】Rime for macOS。

November 10, 2018

iHosts非常适合在Mac OS X上编辑/etc/hosts。
host管理chrome插件。
📝开发工具，用于记录laravel应用程序的所有查询。
《精通比特币2》中文版。
Docker的简单管理UI。
在Laravel应用程序中记录活动。
Photopea是在线图像编辑器。
一个面向全平台的代理客户端。
一个Clash的Windows用户图形界面。

November 9, 2018

与KeePass兼容的免费跨平台密码管理器。
macOS  KeePass 客户端。
KeePass插件通过HTTP安全地公开密码条目。
archiver。
命令行JSON处理工具🔥
for PHP的感知图像散列https://jenssegers.com

November 8, 2018

一刻社区后端 API 源码。
一刻社区前端源码。
Chatter是一个简单的Laravel论坛包。
docker中文文档。

November 7, 2018

飞冰 - 让前端开发简单而友好，海量可复用物料，配套桌面工具极速构建前端应用，效率提升 100% 。
Yii的依赖注入。
Gitbook 的高亮插件。
SQL优化器和重写器。
Go中基于设计的API和微服务。
简单的PHP版本管理。
一个简单的HTML5，YouTube和Vimeo播放器。
宁皓网课程的学习路径。
PHP API 文档。
用于本地开发的现代Docker LAMP堆栈和MEAN堆栈。
多端统一开发框架，支持用 React 的开发方式编写一次代码，生成能运行在微信小程序/百度智能小程序/支付宝小程序、H5、React Native 等的应用。
将macOS“快速查看”功能带到Window。
🚥 从MongoDB到Elasticsearch的数据集迁移工具，反之亦然。
为MongoDB生成随机数据。
《一起学 Node.js》
百度网盘客户端 - Go语言编写。
为云音乐客户端解决不可用的歌曲。
网易云音乐第三方。

November 6, 2018

JSS是CSS的创作工具，它使用JavaScript作为宿主语言。
PHP Curl Class可以轻松发送HTTP请求并与Web API集成。
PHP的快速请求路由器。
Next Generation of ShadowsocksX。

November 5, 2018

HTTP API 设计指南。
基于Vue.js 2.0构建的高质量UI工具包。
开源项目挣钱实用手册。
静态网站生成器Hexo的简单，精致和现代主题。
使用tensorflow.js在浏览器中进行面部检测和面部识别的JavaScript API。
🤘Alfred3工作流程的集合，将震撼您的世界。
Alfred工作流程的公共集合。http://www.alfredworkflow.com
shortcuts workflow 集合。
What-happens-when 的中文翻译。
使任何网页成为桌面应用程序。
简约的Vue驱动的静态站点生成器。
一个神奇的文档站点生成器。

November 3, 2018

深度学习500问，以问答形式对常用的概率知识、线性代数、机器学习、深度学习、计算机视觉等热点问题进行阐述，以帮助自己及有需要的读者。
覃健祥的学习笔记，各种几十分钟入门的文档。
中国运营商IP地址库-每日更新。
使用JavaScript，HTML和CSS构建跨平台桌面应用程序。
最快的shell插件管理器。
一种使用Web技术构建的开源屏幕录像机。
在终端中获取Linux桌面截图的系统/主题信息。

November 2, 2018

PHP 7中的正确舍入，任意精度的十进制浮点运算。
PHP终端NES模拟器。
Sentry是跨平台应用程序监控，专注于错误报告。
浏览器中的Markdown编辑器。
Biny是一个用于Web应用程序的小型高性能PHP框架。
将MySQL binlog解析为您想要的SQL。
输入SQL，输出索引优化建议。
适用于macOS的现代视频播放器。
使我的macOS体验更加惊人的应用程序和工具列表。
使我的iOS体验更加惊人的应用程序和工具列表。

November 1, 2018

程序员应该访问的最佳网站中文版。
使用Golang实现PHP的常见内置函数。
【新】微信开（微信服务号+微信小程序+微信支付）。
MIT-18.06-线性代数-完整笔记。
《Redis Command Reference》全文的中文翻译版。
PHP非侵入式监控平台- 优化性能，定位Bug的神器，别再让你的PHP程序裸奔。
Surge、Quantumult、Kitsunebi、Shadowrocket、Pepi(ShadowRay)、Surfboard 的配置规则文件 。
Swagger整合到Laravel 5。
API接口辅助库包，生成和校验api签名。
以开发人员为中心的HTTP客户端，针对大多数常见用例进行了优化。

October 30, 2018

 收集 PHP 最佳实践、编码规范和权威学习指南，方便 PHP 开发者阅读和查找。
🔗一些有用的程序员网站。
用 vue 写小程序，基于 mpvue 框架重写 weui。
CSS重置的现代替代方案。
立即将JSON转换为浏览器中的Go类型（原始）。
学习如何设计大型系统。
一种自动修复PHP编码标准问题的工具。
确保你的项目没有依赖一些已知易受攻击的依赖。
用于在PHP应用程序中查找错误的静态分析工具。
易于使用的PDO包装器，适用于PHP项目。
密码哈希Argon2，PHC的获胜者。
兼容PHP 5.5附带的password_ *函数。
PHP项目的自动cacert.pem管理。

October 29, 2018

一个简单的Jekyll主题。
SparkPHP框架。
PHP中函数式编程。
百度网盘命令行工具。
PHP的有效，快速，稳定的日志扩展。
📖 原则 · 中文版。
用于将PHP变量记录到Google Chrome控制台。
在Google Chrome中远程执行PHP代码，处理PHP错误，转储变量。
一个适用于的OpenWRT全的平台个人文库翻墙路由方案。
互联网公司技术架构，微信/淘宝/微博/腾讯/阿里/美团点评/百度/Google/Facebook/Amazon/eBay的架构。
后端架构师技术图谱。

October 26, 2018

vuejs Database Manager数据管理系统——前端。
vuejs Database Manager数据管理系统——后端。
A PHP framework for console artisans。
收集整理一些常用的PHP类库，资源以及技巧。

October 25, 2018

吾爱破解论坛 爱盘 down.52pojie.cn 页面的源代码。
微信个人号接口、微信机器人及命令行微信，三十行即可自定义个人号机器人。
基于yaf开发的免费、安全、稳定、高效的发卡系统，值得拥有! 
基于yaf开发的全新、免费、开源、高效好用的知识付费平台（自用型）。
基于Swoole的PHP中的高性能Web框架和应用程序服务器。
适用于Web客户端的gRPC。
类似与PHP Simple HTML DOM Parser，但是比它快好几倍。

October 24, 2018

Vagrant Manager for Windows。
SQL优化器和重写器。
将实时段落，单词和字符计数添加到HTML元素。
让自己轻松选择阿里巴巴风味名称（又名，花名）。
所有算法都在Python中实现。
个人网站即时到账收款解决方案。
适用于PayPal RESTful API的PHP SDK。
百度网盘不限速下载 支持Windows和Mac。
Laravel框架的优雅调试助手。 Telescope可深入了解进入应用程序的请求，异常，日志条目，数据库查询，排队作业，邮件，通知，缓存操作，计划任务，变量转储等。

October 23, 2018

Nginx开发从入门到精通。
opcache状态页面。
dcloudio 支付相关。
DCloud开源项目集锦 http://www.dcloud.io。
每个 JavaScript 工程师都应懂的33个概念。
swoft 框架文件。
macOS的MySQL / MariaDB数据库管理。

October 22, 2018

区块链3.0 -> 超级账本hyperledger fabirc教程 v1.1。
Google Hosts。

October 21, 2018

一些经典且高质量的电子书分享。
中国程序员容易发音错误的单词。
中华人民共和国行政区划：省级（省份直辖市自治区）、 地级（城市）、 县级（区县）、 乡级（乡镇街道）、 村级（村委会居委会） ，中国省市区镇村二级三级四级五级联动地址数据 Node.js 爬虫。
🌈 An elegant dashboard https://d2-projects.github.io/d2-admin/。
.vimrc简单配置，没有插件。

October 20, 2018

在macOS上安装开发环境。
加盐密码哈希：如何正确使用。
Metabase是一个开源的BI工具，最大的特点是具有可视化操作界面的数据分析和查询功能，让不懂SQL得用户可能够快速掌握业务数据，支持团队共享业务数据，并且支持MySQL、Postgresql等多种数据源，部署方便，为企业提供了一个很不错的BI解决方案。
在 Windows 上用 WSL 优雅开发。
适用于开发人员的有用Quick Look插件列表。
Mysql web端sql审核平台。
WordPress 主题 Puma。
这是书籍《深度学习框架PyTorch：入门与实践》的对应代码，但是也可以作为一个独立的PyTorch入门指南和教程。
以各种方式使用RabbitMQ的教程。
纯Python MySQL客户端。

October 19, 2018

Shadowsocks for Windows。
Nginx安装维护入门学习笔记，以及各种实例。
中国最大的API接口管理平台。
A PHP terminal NES emulator。
护网杯2018 easy_laravel Docker环境。
萌音影视 - 在线影视应用 http://www.moeins.cn。
一个简单的图书 SDK，你可以使用它用于获取指定书籍的基本信息。
php开源商城系统，基于swoole、easyswoole框架开发 https://www.fashop.cn。

October 18, 2018

算法学习 Golang 版。
阿里巴巴mysql数据库binlog的增量订阅&消费组件 。
和我一步步部署 kubernetes 集群。
PHPConChina 相关资源。
Redis、Lua、Nginx、OpenResty笔记。
MeepoPS是Meepo PHP Socket的缩写，旨在提供稳定的Socket服务。可以轻松构建在线实时聊天、即时游戏、视频流媒体播放等。

October 17, 2018

Git 常见问题、用法。
国家标准的软件开发文档。
阮一峰 - 技术分享周刊，每周五发布。

October 16, 2018

平常学习中收集的教程整理。
Twitter 的 Snowflake 的PHP版。
基于有赞云和有赞微小店实现个人收款解决方案。
PHPForker是一个PHP多进程编程骨架，借鉴了Workerman诸多优良编程思想，剥离了其中的网络事件库抽象部分，集中围绕多进程编程，为了便于直观的调试以及保持最轻的多进程骨架，所以简单的内嵌了一个基于select多路复用技术的 TCP & UDP Server。
lnmp 一键安装包。

October 15, 2018

Yaf MVC框架集成了一些常用类库。
Valitron是一个简单，优雅，独立的验证库，没有依赖关系。
复制GitHub Markdown样式的最小CSS量。

October 14, 2018

一个结构清晰的，易于维护的，现代的PHP Markdown解析器。
OpenCV-Python-Tutorial。
跨域本地存储，具有权限。
通过缓存整个响应来加速Laravel应用程序。
redis web 客户端。

October 12, 2018

后端架构师技术图谱。
🔴蓝灯最新版本下载。

October 11, 2018

IPsec VPN 服务器一键安装脚本。
搬瓦工一键搭建酸酸 Shad0ws0cks 图文教程。
一个可体验 Windows 95 的app。
从0到1构建分布式秒杀系统，脱离案例讲架构都是耍流氓（数据库，服务器文章）。
找到一个好用的验证码程序(5种验证码)。
Luosimao 创新开发的人机验证，免去了复杂的输入过程，具有更加优秀的操作体验，更加美观的设计，可更好地融入到您的网站中。
一个基于PHP的jQuery中文点击验证码插件 （php, jquery, captcha）。
ckplayer (超酷网页视频播放器),支持http协议下的flv,f4v,mp4,支持rtmp视频流和rtmp视频回放,支持m3u8格式,是你做视频直播,视频点播的理想播放器 http://www.ckplayer.com
🌻 HTML5播放器、M3U8直播/点播、RTMP直播、低延迟、推流/播流地址鉴权、优化浏览器兼容性，HLS+扩展 http://github.tinywan.com/html5-dash-…
一个支持自定义UI布局,流式API, 加密,直播 ,亮度,音量,快进等手势 ,广告视频预览,多种加载模式 ,多种分辨率切换 ,多种封面图, 自定义数据源,列表播放,倍数播放,边播变缓存不是使用AndroidVideoCache,离线播放,神奇的播放器。
SGPlayer 是一款基于 AVPlayer、FFmpeg 的媒体资源播放器框架。支持360°全景视频，VR视频，RTMP、RTSP 等直播流；同时支持 iOS、macOS、tvOS 三个平台。
云教务，摩码创想开源版云教务系统主要由教学、系统、账户三个大模块组成 http://www.yunjiaowu.cn。

October 10, 2018

一款功能强大的 macOS 版微信小助手。
网易云音乐 Node.js API service。
我个人曾经做过的技术分享... http://xiaorui.cc
中文版 《微服务：从设计到部署》。
基于laravel免费的开源IT资产/许可证管理系统。
腾讯云COS对象存储 V5。
运用swoole在浏览器更友好的实现vmstat。

October 9, 2018

Laravel为Sentry整合。
中文人名语料库。中文姓名,姓氏,名字,称呼,日本人名,翻译人名,英文人名。
Wooyun知识库，乌云知识库。
一个能够 Hook 绝大多数函数/类、部分 opcode 的 PHP7 扩展。

October 8, 2018

提供用于规范化composer.json的composer插件。

October 7, 2018

终端主题配色。
spf13-vim。

October 6, 2018

Vue.js的国际化插件。

October 5, 2018

收集那些优秀的软件（Windows & Mac）。

October 4, 2018

《Redis 设计与实现》（网络版）的书稿源码。
Aria2GUI for macOS。

October 3, 2018

Laravel 深入浅出指南 —— Laravel 5.7 源代码解析，新手进阶指南。

October 2, 2018

GitHub 虚假 Star 净网行动。
Go实战开发。

October 1, 2018

hitokoto本地源。

September 30, 2018

在Blade视图中使用自定义html组件。
我的macOS配置：Zsh, Karabiner, VS Code, Sublime, Neovim, Nix, Hammerspoon。
Fira代码：具有编程连字的等宽字体。
具有编程连字的等宽字体。
基于Laravel开发的在线点播系统。

September 29, 2018

蓝灯最新版本下载。
来自Laravel生态系统的精选资源大全，包括书签、包、教程、视频以及其它诸多很酷的资源。
用于备份Laravel应用程序的软件包。
一个用vue写的后台模板

September 28, 2018

PHP开发知识结构。

September 27, 2018

前端笔试面试题题库。
快应用的例子。

September 26, 2018

使用开源Laravel Envoy工具提供基本的“零停机”部署。
QR码生成器。
VPN Chrome是基于Google Chromium的浏览器，具有内置的VPN功能，可让用户以安全和私密的方式上网。
此存储库包含使用大多数纯PHP的比特币实现。
Minera是一个管理和监控比特币采矿硬件的完整系统。
用于与blockchain.info API交互的官方PHP库。
BitWasp是一个开源PHP项目，允许任何人建立一个独立于其他集中服务的安全比特币市场。
10 个你应该知道的 PHP 比特币开源项目。

September 25, 2018

前端发展很快，现代浏览器原生 API 已经足够好用。我们并不需要为了操作 DOM、Event 等再学习一下 jQuery 的 API。同时由于 React、Angular、Vue 等框架的流行，直接操作 DOM 不再是好的模式，jQuery 使用场景大大减少。本项目总结了大部分 jQuery API 替代的方法，暂时只支持 IE10 以上浏览器
ClickHouse是一个面向开源列的数据库管理系统，可以实时生成分析数据报告。
社会需求收集器 - 一些非技术文章。

September 24, 2018

基于PHP的反病毒反木马反恶意软件解决方案。
一个简单的图书 SDK，你可以使用它用于获取指定书籍的基本信息。

September 23, 2018

重新定义微信小程序的开发。
一个安全的私有聊天软件。
微信/支付宝监控个人收款，无需签约支付宝、微信支付。为支付宝、微信支付的个人账户提供即时到账服务。
独立的qrcode生成（不依赖于外部服务）。

September 22, 2018

PHP Server Monitor是一个脚本，用于检查您的网站和服务器是否已启动并正在运行。它带有一个基于Web的用户界面，您可以在其中管理您的服务和网站，还可以使用手机号码和电子邮件地址管理每个服务器的用户。
php-interview-2018: 面试总结。
YApi 是一个可本地部署的、打通前后端及QA的、可视化的接口管理平台。
swoole内核分析，基于swoole2.0.13。
基于 Chrome 和 Vue.js 开发的第三方即刻通知插件。
github徽章服务。

September 21, 2018

Leetcode 题解 (跟随思路一步一步撸出代码) 及经典算法实现。
SimPic是一个开源的PHP图床。
谈谈一些有趣的 CSS 话题。
PHP比特币开发详解：本课程面向初学者，内容即涵盖比特币的核心概念。
Laravel 的中大型專案架構 | 點燈坊。
Gitlab 安装和配置。

September 20, 2018

Serialize closures (anonymous functions) https://opis.io/closure。

September 19, 2018

Laravel 5.7 blog application with Vue.js, Docker, Redis, Horizon and Pusher。
独立开发/自由职业/远程工作资源列表。

September 18, 2018

Laravel 5 系列入门教程。
网页微信PHP登录的实现。
PHP底层内核源码分析和扩展开发。

September 17, 2018

PHP代码职业生涯中的一些小技巧🐘http://easy-tips.tigerb.cn。

September 16, 2018

深度有趣 - 人工智能实战项目合集。
为互联网IT人打造的中文版awesome-go。
开源书籍大搜罗。

September 15, 2018

基于 Swoole 开发的协程 PHP 开发框架，常驻内存、协程异步。
Golang标准库。对于程序员而言，标准库与语言本身同样重要，它好比一个百宝箱，能为各种常见的任务提供完美的解决方案。以示例驱动的方式讲解Golang的标准库。

September 14, 2018

又一个Linux VPS测评脚本。
高性能, 并发抢占锁, 并发队列锁。
支持多家云存储的云盘系统。

September 13, 2018

一个网站部署包。
RSSHub 是一个轻量、易于扩展的 RSS 生成器，可以给任何奇奇怪怪的内容生成 RSS 订阅源。

September 12, 2018

搜索互动地图以了解任何内容 https://learn-anything.xyz。
一堆初中生写的类库、框架。
前端精读周刊。

September 11, 2018

2018前端常见题汇总，不定时更新。

September 10, 2018

Node.js API 中文文档。

September 9, 2018

eoLinker是国内最大的在线API接口管理平台，提供自动生成API文档、API自动化测试、Mock测试、团队协作等功能。
V2EX 撕逼大战。
一款功能强大的 macOS 版微信小助手。

September 8, 2018

📚 免费的计算机编程类中文书籍。
可能是让你受益匪浅的英语进阶指南。
用 PHP 像开发网站一样开发桌面应用软件。

September 7, 2018

中华人民共和国居民身份证号码验证工具。

September 6, 2018

php7.x 新特性。
Go语言高级编程 (Advanced Go Programming)。

September 4, 2018

《Go语言高级编程》开源图书，涵盖CGO、Go汇编语言、RPC实现、Protobuf插件实现、Web框架实现、分布式系统等高阶主题。
PHP 面试知识点汇总。
PHP工程师面试题目。
笔记、Laravel、PHP、面试题、HTML、CSS。

September 3, 2018

收集&推荐优秀的 Apps/硬件/技巧/周边等。

September 2, 2018

一个收集在 GitHub 上作弊用户的黑名单项目
中华新华字典数据库。包括歇后语，成语，词语，汉字。提供新华字典API。
大型系统设计的基础知识
当···时发生了什么？
GitHub 上最神奇的项目之一应该是这个了，一行代码都没，但有 2w+ stars。
Awesome macOS open source applications
使用Microsoft Style的GitHub主题。

August 30, 2018

awesome-chrome-devtools
clockwork-chrome - Clockwork是一个浏览器扩展，提供调试和分析PHP应用程序的工具，包括请求数据，应用程序日志，数据库查询，路由，应用程序运行时的可视化等。
remotedebug-gateway - 允许您一次将客户端连接到多个浏览器
AwesomeList top

August 29, 2018

浅谈常见的NoSQL技术方案和选型
A collection of awesome browser extensions for GitHub
composer 范例包
PHP 开发者该知道的 5 个 Composer 小技巧
github star 整理
pt-query-digest: 从日志，进程列表和 tcpdump 分析 MySQL 查询
MySQL 生成千万级的测试数据

",173
lindexi/lindexi.github.io,C#,"README
Email:lindexi_gd@163.com
| blog | Github | 关于我 |
欢迎大家访问我的文章，我这里有系列文章关于 UWP 开发。因为没有整理好顺序，如果是希望入门学习，请到uwp入门
文章包含：


UWP 相关


Win2d


WPF


dotnet core


VisualStudio


git


我的其他网站


个人网站


lindexi - CSDN博客


lindexi - 博客园


如果希望和我交流开发相关，欢迎大家进入 dotnet 职业技术学院
",3
DiveIntoKotlin/DiveIntoKotlinSamples,Kotlin,"DiveIntoKotlinSamples
Kotlin核心编程样例代码
",2
AppFederation/cloud-time-ion,TypeScript,"cloud-time-ion
Timers in the Clouds, Timers everywhere! By the Time+Tasks+Cloud People!
",2
HaimmingYu/NutzSite,Java,"
NutzSite基于Nutz的开源企业级开发框架
简介
一直想做一款后台管理系统，看了很多优秀的开源项目但是发现没有合适自己的。于是利用空闲休息时间开始自己写一套后台系统。如此有了NutzSite管理系统。 网站管理后台 系统会陆续更新一些实用功能。
项目截图


目前支持

Nutz
Nutzboot
Druid
Shiro
Thymeleaf
Quartz 定时任务
SLog日志记录
支付宝
微信公众平台
阿里云消息推送
阿里云短信
高德地图
七牛云
XSS攻击过滤 SQL注入过滤
Excel 导出数据

给自己挖坑 后期支持待完善功能

审批流
CMS

本压缩包是一个maven工程, eclipse/idea均可按maven项目导入
MainLauncher是入口,启动即可
环境要求

必须JDK8+
eclipse或idea等IDE开发工具,可选

配置信息位置
数据库配置信息,jetty端口等配置信息,均位于src/main/resources/application.properties
命令下启动
仅供测试用,使用mvn命令即可
// for windows
set MAVEN_OPTS=""-Dfile.encoding=UTF-8""
mvn compile nutzboot:run

// for *uix
export MAVEN_OPTS=""-Dfile.encoding=UTF-8""
mvn compile nutzboot:run

项目打包
mvn clean package nutzboot:shade

请注意,当前需要package + nutzboot:shade, 单独执行package或者nutzboot:shade是不行的
相关资源

论坛: https://nutz.cn
官网: https://nutz.io
一键生成NB的项目: https://get.nutz.io
项目80%参考 RuoYi: https://gitee.com/y_project/RuoYi
部分代码 参考 nutzwk : https://github.com/Wizzercn/NutzWk/tree/bak-delete-v3-bootstrap

鸣谢

@wendal (代码贡献者,技术大牛,Nutz主要作者,无所不知且乐于助人)

我想改变行业的未来,因为我有一颗改变世界的心
有码走遍天下 无码寸步难行（引自网络）
1024 - 梦想，永不止步!
爱编程 不爱Bug
爱加班 不爱黑眼圈
固执 但不偏执
疯狂 但不疯癫
生活里的菜鸟
工作中的大神
身怀宝藏，一心憧憬星辰大海
追求极致，目标始于高山之巅
一群怀揣好奇，梦想改变世界的孩子
一群追日逐浪，正在改变世界的极客
你们用最美的语言，诠释着科技的力量
你们用极速的创新，引领着时代的变迁
------至所有正在努力奋斗的程序猿们！加油！！
",18
mesa3d/mesa,C,"Mesa - The 3D Graphics Library

Source
This repository lives at https://gitlab.freedesktop.org/mesa/mesa.
Other repositories are likely forks, and code found there is not supported.

Build & install
You can find more information in our documentation (docs/install.html), but the recommended way is to use
Meson (docs/meson.html):
$ mkdir build
$ cd build
$ meson ..
$ sudo ninja install

Support
Many Mesa devs hang on IRC; if you're not sure which channel is
appropriate, you should ask your question on Freenode's #dri-devel, someone will redirect you if
necessary.
Remember that not everyone is in the same timezone as you, so it might
take a while before someone qualified sees your question.
To figure out who you're talking to, or which nick to ping for your
question, check out Who's Who on IRC.
The next best option is to ask your question in an email to the
mailing lists: mesa-dev@lists.freedesktop.org

Bug reports
If you think something isn't working properly, please file a bug report
(docs/bugs.html).

Contributing
Contributions are welcome, and step-by-step instructions can be found in our
documentation (docs/submittingpatches.html).
Note that Mesa uses email mailing-lists for patches submission, review and
discussions.
",259
torcado194/glitch-torcAddons,JavaScript,"torcAddons for glitch
torcAddons is an addon package for glitch.com
to use any or all features, install a code injection/userscript extension such as tampermonkey or Resource Override and add the desired code.
for every addon, the base driver script torcAddons.js must also exist, and must be run prior to the other scripts.
addons
filetree

adds a (real) filetree view to the file browser, including directory collapsing.
it also comes with a file searchbar, automatically hiding files not matching the search.


favorite

adds a ""favorite"" star button next to each file, allowing you to add or remove files to a favorites list.
a list of favorited files is displayed above the filetree. favorites are saved to local storage and are tied to the project.


indentGuides

adds visual guides to indentation in the code


pasteAndIndent

automatically correctly indents code when you paste it. you can prevent this behavior with ctrl+shift+v.


colors

adds a visual color component next to css colors


wrapSelection

allows certain characters to wrap selections rather than replace the selection, such as (parentheses)



",6
juancarlospaco/faster-than-requests,Python,"
Faster-than-Requests





Library
Speed
Files
LOC
Dependencies
Developers




PyWGET
152.39
1
338
Wget
>17


Requests
15.58
>20
2558
>=7
>527


Requests (cached object)
5.50
>20
2558
>=7
>527


Urllib
4.00
???
1200
0 (std lib)
???


Urllib3
3.55
>40
5242
0 (No SSL), >=5 (SSL)
>188


PyCurl
0.75
>15
5932
Curl, LibCurl
>50


PyCurl (no SSL)
0.68
>15
5932
Curl, LibCurl
>50


Faster_than_requests
0.45
1
75
0
1





Lines Of Code counted using CLOC.
Direct dependencies of the package when ready to run.
Benchmarks run on Docker from Dockerfile on this repo.
Developers counted from the Contributors list of Git.
Speed is IRL time to complete 10000 HTTP local requests.
Stats as of year 2019.
x86_64 64Bit AMD, SSD, Arch Linux.


Use
import faster_than_requests as requests

print(requests.gets(""http://httpbin.org/get""))                      # GET
print(requests.posts(""http://httpbin.org/post"", ""Some Data Here""))  # POST
requests.downloads(""http://example.com/foo.jpg"", ""output.jpg"")     # See Docs for more info.
gets()

Description:
Takes an URL string, makes an HTTP GET and returns a dict with the response.
Arguments:

url the remote URL, string type, required, must not be empty string.

Examples:

url = ""http://example.com""

Returns:
Response, dict type, values of the dict are string type,
values of the dict can be empty string, but keys are always consistent.

posts()

Description:
Takes an URL string, makes an HTTP POST and returns a dict with the response.
Arguments:

url the remote URL, string type, required, must not be empty string.
body the Body data, string type, required, can be empty string.

Examples:

url = ""http://example.com""
body = ""My Body Data Here""

Returns:
Response, dict type, values of the dict are string type,
values of the dict can be empty string, but keys are always consistent.

put()

Description:
Takes an URL string, makes an HTTP PUT and returns a dict with the response.
Arguments:

url the remote URL, string type, required, must not be empty string.
body the Body data, string type, required, can be empty string.

Examples:

url = ""http://example.com""
body = ""My Body Data Here""

Returns:
Response, dict type, values of the dict are string type,
values of the dict can be empty string, but keys are always consistent.

deletes()

Description:
Takes an URL string, makes an HTTP DELETE and returns a dict with the response.
Arguments:

url the remote URL, string type, required, must not be empty string.

Examples:

url = ""http://example.com""

Returns:
Response, dict type, values of the dict are string type,
values of the dict can be empty string, but keys are always consistent.

patch()

Description:
Takes an URL string, makes an HTTP PATCH and returns a dict with the response.
Arguments:

url the remote URL, string type, required, must not be empty string.
body the Body data, string type, required, can be empty string.

Examples:

url = ""http://example.com""
body = ""My Body Data Here""

Returns:
Response, dict type, values of the dict are string type,
values of the dict can be empty string, but keys are always consistent.

get2str()

Description:
Takes an URL string, makes an HTTP GET and returns a string with the response Body.
Arguments:

url the remote URL, string type, required, must not be empty string.

Examples:

url = ""http://example.com""

Returns: Response body, string type, can be empty string.

get2str_list()

Description:
Takes a list of URLs, makes 1 HTTP GET for each URL, and returns a list of strings with the response Body.
Arguments:

list_of_urls A list of the remote URLs, list type, required. Objects inside the list must be string type.

Examples:

list_of_urls = [""http://example.com/foo"", ""http://example.com/bar""]

Returns:
List of response bodies, list type, values of the list are string type,
values of the list can be empty string, can be empty list.

get2ndjson_list()

Description:
Takes a list of URLs, makes 1 HTTP GET for each URL, returns a list of strings with the response, and writes the responses to a NDJSON file, it can accumulate several JSON responses into a single file.
Arguments:

list_of_urls A list of the remote URLs, list type, required. Objects inside the list must be string type.
ndjson_file_path Full path to a local writable NDJSON file, string type, required, file can be non-existent and it will be created, if it exists it will the overwritten.

Examples:

list_of_urls = [""http://example.com/foo"", ""http://example.com/bar""]
ndjson_file_path = ""/some/folder/some/file.ndjson""

Returns: None.

get2dict()

Description:
Takes an URL, makes an HTTP GET, returns a dict with the response Body.
Arguments:

url the remote URL, string type, required, must not be empty string.

Examples:

url = ""http://example.com""

Returns:
Response, dict type, values of the dict are string type,
values of the dict can be empty string, but keys are always consistent.

get2json()

Description:
Takes an URL, makes an HTTP GET, returns a Minified Computer-friendly single-line JSON with the response Body.
Arguments:

url the remote URL, string type, required, must not be empty string.

Examples:

url = ""http://example.com""

Returns: Response Body, Minified JSON, on a single line.

get2json_pretty()

Description:
Takes an URL, makes an HTTP GET, returns a Pretty-Printed Human-friendly Multi-line JSON with the response Body.
Arguments:

url the remote URL, string type, required, must not be empty string.

Examples:

url = ""http://example.com""

Returns: Response Body, Pretty-Printed JSON, multi-line.

get2assert()

Description:
Takes an URL, makes an HTTP GET, returns nothing, makes an assertion, useful for Unittest and Debug purposes.
Arguments:

url the remote URL, string type, required, must not be empty string.
expected Response expected content, string type, required, can be empty string.

Returns: None.

getlist2list()

Description:
Takes a list of URLs, makes 1 HTTP GET for each URL, returns a list of responses.
Arguments:

list_of_urls the remote URLS, list type, required, the objects inside the list must be string type.

Examples:

list_of_urls = [""http://example.com/foo"", ""http://example.com/bar""]

Returns:
List of response bodies, list type, values of the list are string type,
values of the list can be empty string, can be empty list.

post2str()

Description:
Takes an URL, makes an HTTP POST, returns the response Body as string type.
Arguments:

url the remote URL, string type, required, must not be empty string.
body the Body data, string type, required, can be empty string.

Examples:

url = ""http://example.com""
body = ""My Body Data Here""

Returns: Response body, string type, can be empty string.

post2dict()

Description:
Takes an URL, makes a HTTP POST on that URL, returns a dict with the response.
Arguments:

url the remote URL, string type, required, must not be empty string.
body the Body data, string type, required, can be empty string.

Examples:

url = ""http://example.com""
body = ""My Body Data Here""

Returns:
Response, dict type, values of the dict are string type,
values of the dict can be empty string, but keys are always consistent.

post2json()

Description:
Takes a list of URLs, makes 1 HTTP GET for each URL, returns a list of responses.
Arguments:

url the remote URL, string type, required, must not be empty string.
body the Body data, string type, required, can be empty string.

Examples:

url = ""http://example.com""
body = ""My Body Data Here""

Returns: Response, string type.

post2json_pretty()

Description:
Takes a list of URLs, makes 1 HTTP GET for each URL, returns a list of responses.
Arguments:

url the remote URL, string type, required, must not be empty string.
body the Body data, string type, required, can be empty string.

Examples:

url = ""http://example.com""
body = ""My Body Data Here""

Returns: Response, string type.

post2assert()

Description:
Takes an URL, makes an HTTP POST on that URL, returns a response.
Arguments:

url the remote URL, string type, required, must not be empty string.
body the Body data, string type, required, can be empty string.
expected Response expected content, string type, required, can be empty string.

Returns: None.

post2list()

Description:
Takes a list of URLs, makes 1 HTTP POST for each URL, returns a list of responses.
Arguments:

list_of_urls the remote URLS, list type, required, the objects inside the list must be string type.
body the Body data, string type, required, can be empty string.

Examples:

list_of_urls = [""http://example.com/foo"", ""http://example.com/bar""]
body = ""My Body Data Here""

Returns:
List of response bodies, list type, values of the list are string type,
values of the list can be empty string, can be empty list.

downloads()

Description:
Takes a list of URLs, makes 1 HTTP GET for each URL, returns a list of responses.
Arguments:

url the remote URL, string type, required, must not be empty string.
filename the local filename, string type, required, must not be empty string, full path recommended, can be relative path, includes file extension.

Examples:

url = ""http://example.com""
filename = ""cat-memes.jpg""

Returns: None.

downloads_list()

Description:
Takes a list of URLs, makes 1 HTTP GET Download for each URL of the list.
Arguments:

list_of_files list of tuples, tuples must be 2 items long, first item is URL and second item is filename.
The remote URL, string type, required, must not be empty string, is the first item on the tuple.
The local filename, string type, required, must not be empty string, can be full path, can be relative path, must include file extension.

Examples:

list_of_files = [(""http://example.com/cat.jpg"", ""kitten.jpg"")]
list_of_files = [(""http://example.com/cat.jpg"", ""kitten.jpg""), (""http://example.com/dog.jpg"", ""doge.jpg"")]

Returns: None.

downloads_list_delay()

Description:
Takes a list of URLs, makes 1 HTTP GET Download for each URL of the list of with a delay, optional randomized delay.
Arguments:

delay Delay between a download and the next one, integer type, required, must be non-zero positive integer value.
randoms Randomize the Delay, bool type, required, default is False.
debugs Debug the downloads, bool type, required, default is False.
list_of_files list of tuples, tuples must be 2 items long, first item is URL and second item is filename.
The remote URL, string type, required, must not be empty string, is the first item on the tuple.
The local filename, string type, required, must not be empty string, can be full path, can be relative path, must include file extension.

Examples:

list_of_files = [(""http://example.com/cat.jpg"", ""kitten.jpg"")]
list_of_files = [(""http://example.com/cat.jpg"", ""kitten.jpg""), (""http://example.com/dog.jpg"", ""doge.jpg"")]

Returns: None.

setHeaders()

Description:
Set the HTTP Headers from the arguments.
Arguments:

headers HTTP Headers, list type, required,
a list of tuples, tuples must be 2 items long,
must not be empty list, must not be empty tuple,
the first item of the tuple is the key and second item of the tuple is value,
keys must not be empty string, values can be empty string, both must the stripped.

Examples:

headers = [(""key"", ""value"")]
headers = [(""key0"", ""value0""), (""key1"", ""value1"")]
headers = [(""content-type"", ""text/plain""), (""dnt"", ""1"")]

Returns: None.

requests()

Description:
Low level API of Requests with everything available as argument to build a detailed custom HTTP request.
Arguments:

url the remote URL, string type, required, must not be empty string.
body the Body data, string type, required, can be empty string.
http_method HTTP method, string type, required, must not be empty string, values can be ""GET"", ""POST"", etc.
debugs Debug mode, bool type, required, default is False.
http_headers HTTP Headers, list type, required,
a list of tuples, tuples must be 2 items long,
must not be empty list, must not be empty tuple,
the first item of the tuple is the key and second item of the tuple is value,
keys must not be empty string, values can be empty string, both must the stripped.

Returns:
Response, dict type, values of the dict are string type,
values of the dict can be empty string, but keys are always consistent.

requests2()

Description:
Low level API of Requests with everything available as argument with extra options to build a detailed custom HTTP request.
Its like request() function but re-imagined with different features.
May be slower than request().
Arguments:

url Remote URL, string type, required, must not be empty string.
body the Body data, string type, required, can be empty string.
proxyUrl Full URL of your Network Proxy, string type, required, can be empty string.
proxyAuth Auth of your Network Proxy if any, string type, required, can be empty string.
userAgent User Agent to use for the requests, string type, required, can be empty string.
timeout Timeout, integer type, Milliseconds precision, must be non-zero positive integer, can be -1.
maxRedirects Maximum Redirects, integer type, must be non-zero positive integer, can be 1.
http_method HTTP method, string type, required, must not be empty string, values can be ""GET"", ""POST"", etc.
http_headers HTTP Headers, list type, required,
a list of tuples, tuples must be 2 items long,
must not be empty list, must not be empty tuple,
the first item of the tuple is the key and second item of the tuple is value,
keys must not be empty string, values can be empty string, both must the stripped.

Returns:
Response, dict type, values of the dict are string type,
values of the dict can be empty string, but keys are always consistent.

For more Examples check the Examples.
Instead of having a pair of functions with a lot of arguments that you should provide to make it work,
we have tiny functions with very few arguments that do one thing and do it as fast as possible.
A lot of functions are oriented to Data Science, Big Data, Open Data, Web Scrapping, working with HTTP REST JSON APIs.
Install

pip install faster_than_requests

Docker

Make a quick test drive on Docker!.

$ ./build-docker.sh
$ ./run-docker.sh
$ ./server4benchmarks &  # Inside Docker.
$ python3 benchmark.py   # Inside Docker.
Platforms

✅ Linux
✅ Windows
✅ Mac
✅ Android
✅ Raspberry Pi
✅ BSD

Extras
More Faster Libraries...

https://github.com/juancarlospaco/faster-than-csv#faster-than-csv
https://github.com/juancarlospaco/faster-than-walk#faster-than-walk
We want to make Open Source faster, better, stronger.

Requisites

Python 3.
GCC.
64 Bit.

Low Level API Extras
To control the default values the following optional Bash environment variables are available:

requests_timeout Timeout, int type, must be a non-zero positive value, milliseconds precision.
requests_maxredirects Maximum Redirects, int type, must be a non-zero positive value.
requests_useragent User Agent, str type, can be empty string.
requests_debugprogress Debug Progress, bool type, slows down performance, not recommended for general use.

Examples:
$ export https_proxy = ""http://yourProxyUrl:8080""
$ export requests_maxredirects = ""42""

This is 100% Optional. This is provided as Extra features.

FAQ

Whats the idea, inspiration, reason, etc ?.

Feel free to Fork, Clone, Download, Improve, Reimplement, Play with this Open Source. Make it 10 times faster, 10 times smaller.

This works with SSL ?.

Yes.

This works without SSL ?.

Yes.

This requires Cython ?.

No.

This runs on PyPy ?.

No.

This runs on Python2 ?.

I dunno. (Not supported)

This runs on 32Bit ?.

No.

This runs with Clang ?.

No.

How can I Install it ?.

pip install faster_than_requests

Developer Documentation ?.

Yes.
(Zip because GitHub marks the Repo as being JavaScript)

Where to get help ?.

https://github.com/juancarlospaco/faster-than-requests/issues

How to set the URL ?.

url=""http://example.com"" (1st argument always).

How to set the HTTP Method ?.

http_method=""get"" for GET.
http_method=""post"" for POST.
http_method=""put"" for PUT.
http_method=""patch"" for PATCH.

How to set the HTTP Body ?.

body=""my body""

How to set an HTTP Header key=value ?.

setHeaders()

How to set HTTP Proxy ?.

export https_proxy = ""http://yourProxyUrl:8080""
export http_proxy =  ""http://yourProxyUrl:8080""
Standard Linux Bash environment variables for proxy.
It will be automatically read from the environment variables.

Whats NDJSON ?.

https://github.com/ndjson/ndjson-spec

How can be faster than PyCurl ?.

I dunno.

Why use Tuple instead of Dict for HTTP Headers ?.

For speed performance reasons, dict is slower, bigger, heavier and mutable compared to tuple.

Why needs 64Bit ?.

Maybe it works on 32Bit, but is not supported, integer sizes are too small, and performance can be worse.

Why needs Python 3 ?.

Maybe it works on Python 2, but is not supported, and performance can be worse, we suggest to migrate to Python3.

Can I wrap the functions on a try: except: block ?.

Functions do not have internal try: except: blocks,
so you can wrap them inside try: except: blocks if you need very resilient code.
",36
rusoto/rusoto,Rust,"


Linux / OS X



API docs



Windows



Ceph and Minio support
 









Rusoto is an AWS SDK for Rust

You may be looking for:

An overview of Rusoto
AWS services supported by Rusoto
API documentation
Getting help with Rusoto

Installation
Rusoto is available on crates.io.
To use Rusoto in your Rust program built with Cargo, add it as a dependency and rusoto_$SERVICENAME for any supported AWS service you want to use.
For example, to include only S3 and SQS:
[dependencies]
rusoto_core = ""0.38.0""
rusoto_sqs = ""0.38.0""
rusoto_s3 = ""0.38.0""
Migration notes
Breaking changes and migration details are documented at https://rusoto.org/migrations.html.
Usage
Rusoto has a crate for each AWS service, containing Rust types for that service's API.
A full list of these services can be found here.
All other public types are reexported to the crate root.
Consult the rustdoc documentation for full details by running cargo doc or visiting the online documentation for the latest crates.io release.
A simple example of using Rusoto's DynamoDB API to list the names of all tables in a database:
extern crate rusoto_core;
extern crate rusoto_dynamodb;

use rusoto_core::Region;
use rusoto_dynamodb::{DynamoDb, DynamoDbClient, ListTablesInput};

fn main() {
    let client = DynamoDbClient::new(Region::UsEast1);
    let list_tables_input: ListTablesInput = Default::default();

    match client.list_tables(list_tables_input).sync() {
        Ok(output) => {
            match output.table_names {
                Some(table_name_list) => {
                    println!(""Tables in database:"");

                    for table_name in table_name_list {
                        println!(""{}"", table_name);
                    }
                },
                None => println!(""No tables in database!""),
            }
        },
        Err(error) => {
            println!(""Error: {:?}"", error);
        },
    }
}
Credentials
For more information on Rusoto's use of AWS credentials such as priority and refreshing, see AWS Credentials.
Semantic versioning
Rusoto complies with semantic versioning 2.0.0.
Until reaching 1.0.0 the API is to be considered unstable.
See Cargo.toml or rusoto on crates.io for current version.
Releases
Information on release schedules and procedures are in RELEASING.
Contributing
See CONTRIBUTING.
Supported OSs and Rust versions
Linux, OSX and Windows are supported and tested via TravisCI and Appveyor.
Rust stable is supported.  Older versions of Rust are supported and tested via TravisCI.  The minimum Rust version is incremented when it becomes inconvenient to support older versions.  The current minimum version of Rust supported can be found in .travis.yml.  If a version number is not specified in the rust section, only the named versions listed are supported.  This should be stable, beta and nightly.
License
Rusoto is distributed under the terms of the MIT license.
See LICENSE for details.
",952
rusoto/rusoto,Rust,"


Linux / OS X



API docs



Windows



Ceph and Minio support
 









Rusoto is an AWS SDK for Rust

You may be looking for:

An overview of Rusoto
AWS services supported by Rusoto
API documentation
Getting help with Rusoto

Installation
Rusoto is available on crates.io.
To use Rusoto in your Rust program built with Cargo, add it as a dependency and rusoto_$SERVICENAME for any supported AWS service you want to use.
For example, to include only S3 and SQS:
[dependencies]
rusoto_core = ""0.38.0""
rusoto_sqs = ""0.38.0""
rusoto_s3 = ""0.38.0""
Migration notes
Breaking changes and migration details are documented at https://rusoto.org/migrations.html.
Usage
Rusoto has a crate for each AWS service, containing Rust types for that service's API.
A full list of these services can be found here.
All other public types are reexported to the crate root.
Consult the rustdoc documentation for full details by running cargo doc or visiting the online documentation for the latest crates.io release.
A simple example of using Rusoto's DynamoDB API to list the names of all tables in a database:
extern crate rusoto_core;
extern crate rusoto_dynamodb;

use rusoto_core::Region;
use rusoto_dynamodb::{DynamoDb, DynamoDbClient, ListTablesInput};

fn main() {
    let client = DynamoDbClient::new(Region::UsEast1);
    let list_tables_input: ListTablesInput = Default::default();

    match client.list_tables(list_tables_input).sync() {
        Ok(output) => {
            match output.table_names {
                Some(table_name_list) => {
                    println!(""Tables in database:"");

                    for table_name in table_name_list {
                        println!(""{}"", table_name);
                    }
                },
                None => println!(""No tables in database!""),
            }
        },
        Err(error) => {
            println!(""Error: {:?}"", error);
        },
    }
}
Credentials
For more information on Rusoto's use of AWS credentials such as priority and refreshing, see AWS Credentials.
Semantic versioning
Rusoto complies with semantic versioning 2.0.0.
Until reaching 1.0.0 the API is to be considered unstable.
See Cargo.toml or rusoto on crates.io for current version.
Releases
Information on release schedules and procedures are in RELEASING.
Contributing
See CONTRIBUTING.
Supported OSs and Rust versions
Linux, OSX and Windows are supported and tested via TravisCI and Appveyor.
Rust stable is supported.  Older versions of Rust are supported and tested via TravisCI.  The minimum Rust version is incremented when it becomes inconvenient to support older versions.  The current minimum version of Rust supported can be found in .travis.yml.  If a version number is not specified in the rust section, only the named versions listed are supported.  This should be stable, beta and nightly.
License
Rusoto is distributed under the terms of the MIT license.
See LICENSE for details.
",952
urbanit/OrchardCMS.Localization-Extensions,C#,"Localization Extensions Module Readme
This Orchard module extends the functionality of Orchard.Localization. Fixes the container-containable localization synchronization and other further special synchronization-related things for Taxonomies.
How to localize items step-by-step

Turn on the module.
Attach LocalizationPart both to the container item's type definition and to the containable item's type definition.
First create the container and localized containers then the containable and localized containable items. It's obvious that always need a parent to create a child.

Example 1 - Blogs

Turn on the module.
Add a new culture besides the default one. Let's say the site has 2 cultures: en-US and el-GR.
Attach LocalizationPart to the Blog content type.
Attach LocalizationPart to the BlogPost content type.
Create a new Blog with en-US culture and ""Blog EN"" title.
From ""Blog EN""'s editor create a localized Blog with the ""+ New translation"" link. With el-GR culture and ""Blog GR"" title.
When you click on the Blog link on the dashboard you'll see these 2 Blogs.
Add a new BlogPost to ""Blog EN"" with the title ""BlogPost EN"". This will be listed under ""Blog EN"".
From ""BlogPost EN""'s editor create a localized BlogPost with the ""+ New translation"" link with el-GR culture and ""BlogPost GR"" title. This will be listed under ""Blog GR"".
Now you have a Blog with 2 cultures. You can translate the individual BlogPosts easily and the two Blogs' posts are nicely separated.

Example 2 - Taxonomies

Turn on the module.
Add a new culture besides the default one. Let's say the site has 2 cultures: en-US and el-GR.
Attach LocalizationPart to the Taxonomy content type. Now you can translate Taxonomies (not Terms).
Create a new Taxonomy (""Categories EN"") and translate it (""Categories GR"").
Attach LocalizationPart to the automatically created ""Categories EN Term""'s and ""Categories GR Term""'s type definition. This has to be done always when you add a new Taxonomy or Taxonomy translation.
Add ""Term 1 EN"" Term to ""Categories EN"" Taxonomy. It will be listed under ""Categories EN"" Taxonomy.
Translate it with the ""+ New translation"" link in ""Term 1 EN""'s editor. Name it ""Term 1 GR"". It will be listed under ""Categories GR"" Taxonomy.
Add some more Terms and translate them. At the end you will see 2 nicely synchronized Taxonomy trees.

Special cases in Taxonomy translation

When you delete a Taxonomy or Term, all the children will be deleted too. But all the translated pairs will be left.
In case you try to translate a Term whose parent hasn't a translated version, you can't save that Term because we can't synchronize it into the translation tree.
When you move a Term, its translated pair will be moved too. The only exception is when you move a Term with such a parent that doesn't have a translated pair, in this case the Term's translated pair and its children will be deleted. This feature is needed to preserve consistency so use the move feature wisely!

Other cases
The module doesn't let you to add content with different culture to the container. E.g. if you try to add a Greek BlogPost to an english Blog, you can't do that, the BlogPost's culture will be automatically set to English.
Localized content items with Taxonomy Field step-by-step

Create some localized Taxonomies, see ""Example 2 - Taxonomies"".
Attach LocalizationPart to the content item.
Attach as many Taxonomy Fields for all the cultures you have Taxonomies for.
Select 1-1 Taxonomies for the attached Taxonomy Fields. E.g. attach 2 Taxonomy Fields, ""English Category"" and ""Greek Category"". Select ""Categories EN"" taxonomy for the ""English Category"" field and select ""Categories GR"" taxonomy for the ""Greek Category"" field.
Next time when you create a new content item you'll get a culture-first editor and after saving you can select only from those Taxonomy Fields where the cultures match.

",6
hrbrmstr/waffle,R,"


waffle
Create Waffle Chart Visualizations
Description
Square pie charts (a.k.a. waffle charts) can be used to communicate
parts of a whole for categorical quantities. To emulate the percentage
view of a pie chart, a 10x10 grid should be used with each square
representing 1% of the total. Modern uses of waffle charts do not
necessarily adhere to this rule and can be created with a grid of any
rectangular shape. Best practices suggest keeping the number of
categories small, just as should be done when creating pie charts. Tools
are provided to create waffle charts as well as stitch them together,
and to use glyphs for making isotype pictograms.
It uses ggplot2 and returns a ggplot2 object.
The following functions are implemented:

waffle : make a waffle chart ggplot2 object
iron : vertically stitch together multiple waffle plots,
left-aligning edges (best if used with the waffle pad parameter)
fa_grep: Search FontAwesome names for a pattern
fa_list: List all FontAwesome names
geom_waffle/stat_waffle: Waffle geoms! (WIP)

Installation
install.packages(""waffle"")

# OR

install.packages(""devtools"")
install_github(""hrbrmstr/waffle"")
Usage
library(waffle)

# current verison
packageVersion(""waffle"")
## [1] '0.9.2'
Geoms! (WIP)
library(hrbrthemes)
library(waffle)
library(tidyverse)

tibble(
  parts = factor(rep(month.abb[1:3], 3), levels=month.abb[1:3]),
  values = c(10, 20, 30, 6, 14, 40, 30, 20, 10),
  fct = c(rep(""Thing 1"", 3), rep(""Thing 2"", 3), rep(""Thing 3"", 3))
) -> xdf

ggplot(xdf, aes(fill=parts, values=values)) +
  geom_waffle(color = ""white"", size=1.125, n_rows = 6) +
  facet_wrap(~fct, ncol=1) +
  scale_x_discrete(expand=c(0,0)) +
  scale_y_discrete(expand=c(0,0)) +
  ggthemes::scale_fill_tableau(name=NULL) +
  coord_equal() +
  labs(
    title = ""Faceted Waffle Geoms""
  ) +
  theme_ipsum_rc(grid="""") +
  theme_enhance_waffle()

Waffle Bar Charts with scales!
library(dplyr)
library(waffle)

storms %>% 
  filter(year >= 2010) %>% 
  count(year, status) -> storms_df

ggplot(storms_df, aes(fill = status, values = n)) + 
  geom_waffle(color = ""white"", size = .25, n_rows = 10, flip = TRUE) +
  facet_wrap(~year, nrow = 1, strip.position = ""bottom"") +
  scale_x_discrete() + 
  scale_y_continuous(labels = function(x) x * 10, # make this multiplyer the same as n_rows
                     expand = c(0,0)) +
  ggthemes::scale_fill_tableau(name=NULL) +
  coord_equal() +
  labs(
    title = ""Faceted Waffle Bar Chart"",
    subtitle = ""{dplyr} storms data"",
    x = ""Year"",
    y = ""Count""
  ) +
  theme_minimal(base_family = ""Roboto Condensed"") +
  theme(panel.grid = element_blank(), axis.ticks.y = element_line()) +
  guides(fill = guide_legend(reverse = TRUE))

Basic example
parts <- c(80, 30, 20, 10)
waffle(parts, rows = 8)

Use a data frame
parts <- data.frame(
  names = LETTERS[1:4],
  vals = c(80, 30, 20, 10)
)

waffle(parts, rows = 8)

Slightly more complex example
parts <- c(`Un-breached\nUS Population` = (318 - 11 - 79), `Premera` = 11, `Anthem` = 79)
waffle(
  parts, rows = 8, size = 1, 
  colors = c(""#969696"", ""#1879bf"", ""#009bda""), legend_pos = ""bottom""
)
Health records breaches as fraction of US Population

One square == 1m ppl
waffle(
  parts / 10, rows = 3,
  colors = c(""#969696"", ""#1879bf"", ""#009bda"")
)
Health records breaches as fraction of US Population

(One square == 10m ppl)
library(extrafont)

waffle(
  parts / 10, rows = 3, colors = c(""#969696"", ""#1879bf"", ""#009bda""),
  use_glyph = ""medkit"", size = 8
)

Replicating an old favourite

Via: https://www.nytimes.com/2008/07/20/business/20debt.html
savings <- c(
  `Mortgage\n($84,911)` = 84911, `Auto and\ntuition loans\n($14,414)` = 14414,
  `Home equity loans\n($10,062)` = 10062, `Credit Cards\n($8,565)` = 8565
)
waffle(
  savings / 392, rows = 7, size = 0.5, legend_pos = ""bottom"",
  colors = c(""#c7d4b6"", ""#a3aabd"", ""#a0d0de"", ""#97b5cf"")
)
Average Household Savings Each Year

(1 square == $392)
More replication
Similar to https://eagereyes.org/techniques/square-pie-charts
professional <- c(`Male` = 44, `Female (56%)` = 56)
waffle(
  professional, rows = 10, size = 0.5,
  colors = c(""#af9139"", ""#544616"")
)
Keeps factor by default levels now
With:
iron(
  waffle(c(thing1 = 0, thing2 = 100), rows = 5),
  waffle(c(thing1 = 25, thing2 = 75), rows = 5)
)

Without (you can disable this via keep parameter now):
iron(
  waffle(c(thing1 = 0, thing2 = 100), rows = 5, keep = FALSE),
  waffle(c(thing1 = 25, thing2 = 75), rows = 5, keep = FALSE)
)

Professional Workforce Makeup

Iron example (left-align & padding for multiple plots)
pain.adult.1997 <- c(`YOY (406)` = 406, `Adult (24)` = 24)

waffle(
  pain.adult.1997 / 2, rows = 7, size = 0.5,
  colors = c(""#c7d4b6"", ""#a3aabd""),
  title = ""Paine Run Brook Trout Abundance (1997)"",
  xlab = ""1 square = 2 fish"", pad = 3
) -> A

pine.adult.1997 <- c(`YOY (221)` = 221, `Adult (143)` = 143)

waffle(
  pine.adult.1997 / 2, rows = 7, size = 0.5,
  colors = c(""#c7d4b6"", ""#a3aabd""),
  title = ""Piney River Brook Trout Abundance (1997)"",
  xlab = ""1 square = 2 fish"", pad = 8
) -> B

stan.adult.1997 <- c(`YOY (270)` = 270, `Adult (197)` = 197)

waffle(
  stan.adult.1997 / 2, rows = 7, size = 0.5,
  colors = c(""#c7d4b6"", ""#a3aabd""),
  title = ""Staunton River Trout Abundance (1997)"",
  xlab = ""1 square = 2 fish""
) -> C

iron(A, B, C)

Code of Conduct
Please note that this project is released with a Contributor Code of
Conduct. By participating in this project you agree to
abide by its terms.
",430
puppetlabs-seteam/control-repo,Puppet,"Puppet SE Demo Environment
This is the control-repo used by the Puppet SE team.

Consume
Contribute

",2
daejong123/wonderbits-sdk,JavaScript,"WonderBits SDK
使用方法
第一步
安装 WonderBits JS SDK
npm install wonderbits-sdk
或者
npm install git+https://github.com/daejong123/wonderbits-sdk.git
第二步
根据对应平台选择接入方式:
安装MFElink软件并打开的方式:

win 64 位
win 32 位
mac

集成开发 SDK 的方式:

Android

第三步
初始化 WonderBits JS SDK
const wonderBitsSdk = require(""wonderbits-sdk"");
wonderBitsSdk.initConnection(() => {
  console.log(""初始化成功"");
});
第四步
使用具体模块。
左侧为具体模块列表。
主控模块连接电脑，其他模块连接到主控模块上。但是，左侧中任意一种模块可能有多块同时插入主控模块，所以，需要有自己的序号来分辨是该种模块的第几个。
模块序号默认从 1 开始。
当左侧某种模块还未插入到主控时，插入该种模块到主控模块上，此模块的序号会被设为 1。再次插入该种模块，则这两个模块的序号会被随机设为 1 和 2，以此类推。
所以，每个模块方法的第一个参数为模块序号。
了解了模块序号，就可以根据需要，在第三步的回调成功之后，即console.log(""初始化成功"");之后，调用各模块方法
例子
下面就是调用具体模块的形式:
wonderBitsSdk.模块名.模块方法

具体模块名请参照左侧模块列表。
常用的函数有两类，一类为获取类函数，一类为设置类函数(返回类型均为Promise):
获取类函数的例子如下:

获取超声波检测的距离值

wonderBitsSdk.ultrasonic.getDistance(1).then(distance => {
  console.log(distance);
});
设置类函数的例子如下:

设置彩灯颜色

wonderBitsSdk.led.setRgb(1, 255, 0, 0);
每个模块的具体方法，请点击左侧模块列表查看.

高级用法说明
监听硬件上传的原始数据
wonderBitsSdk.setOnOriginDataReceivedCallback(data => {
  console.log(data);
});
监听硬件复位回调
wonderBitsSdk.addResetListener(() => {
  console.log(""reset"");
});
清除监听硬件复位回调
wonderBitsSdk.clearResetListener();
获取连接在板子上的模块
let moduleNames = wonderBitsSdk.getConnectedModuleNames();
事件
含义：当指定情况发生时，会收到板子上传的数据，此时会调用注册事件时的callback(obj)，将处理后的数据返回。
返回数据的详细类型参见事件回调
事件回调
事件将会返回 json
value 的类型根据对应事件值的不同而不同
{ ""module"": `${moduleName}`, ""source"": `${sourceName}`, ""value"": `${value}` }
",2
cbuijs/ipasn,None,"ipasn
IP to ASN list
Note: Updated at least once every 24 hours.
",2
ymback/Battle.net-Authenticator-Wechat-Mini-Program,JavaScript,"Battle.net-Authenticator-Wechat-Mini-Program/战网安全令在线版微信小程序
作者:竹井詩織里
功能:支持在微信小程序端进行战网安全令颁发、还原、8位密钥查看，支持暴雪一键安全令功能(即免输安全令一键登录)。
如何使用

使用微信扫描上方小程序码，即可使用。
相关站点信息
https://myauth.us/是作者搭建的网页版战网安全令站点。
如欲捐赠，请参访https://myauth.us/donate.php页面。
其他开发人员
xuelu520、yclnycl
License
    Battle.net Authenticator Wechat Mini Program
    Copyright (C) 2013 Shiori Takei

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

",6
ros/rosdistro,Python,"This repo maintains the lists of repositories defining ROS distributions.
It is the implementation of REP 143
It also the home of the rosdep rules.
Guide to Contributing
Please see CONTRIBUTING.md
",386
ros/rosdistro,Python,"This repo maintains the lists of repositories defining ROS distributions.
It is the implementation of REP 143
It also the home of the rosdep rules.
Guide to Contributing
Please see CONTRIBUTING.md
",386
ropenscilabs/drake-manual,R,"
The drake R package user manual
This is the development repository of the drake R package user manual, hosted here. Please feel free to discuss on the issue tracker and submit pull requests to add new examples and update old ones. The environment for collaboration should be friendly, inclusive, respectful, and safe for everyone, so all participants must obey this repository's code of conduct.
",29
zhyongquan/DBC2Excel,None,"DBC2Excel
Convert DBC to Excel by VBA

Feature

Add auto filter and merge cells
Auto group by message
Highlight transmitter and receiver
Sort by message name and startbit
Support both windows and unix file

To do
Performance



File
ECU Nodes
Messages
Signals
Elapsed time(s)




1
8
25
244
22


2
9
72
754
60


3
9
76
1126
67



",6
Lombiq/HipChat-to-Microsoft-Teams-Migration-Utility,C#,"Lombiq HipChat to Microsoft Teams Migration Utility Readme
Utility to migrate Atlassian HipChat content to Microsoft Teams. You can use this instead of waiting for official support. We're testing this utility at Lombiq Technologies (a web development company working with Microsoft technologies) with a 4GB+ HipChat export package containing more than 200k messages (in rooms; a lot more with private chats). The tool is intended for very technical users and developers.
Currently the app can import rooms, messages and attachments from a HipChat export file into configured Teams channels or existing channels (in configured teams). Messages will appear under the user's name doing the import, but messages will include the name of the original user too. To overcome the rate limit of the Teams API multiple HipChat messages can be imported into a single Teams message. See the issue tracker on GitHub for missing features and bugs. HipChat /quote and /code commands are converted into their Teams equivalents.
Note that this being a utility with just temporary use simplicity of implementation was favored against long-term maintainability. Note that the guide assumes you're using Windows but everything should work equally well under any OS supported by .NET Core. However, released executables are available only for Windows 64b currently.
Running the app
If you're a .NET developer then grab the latest source and run the app from the source (you'll need at least VS 2017 and 2.2 of the .NET Core SDK). Otherwise download the latest release from GitHub and run the exe file in the zip. Be sure to check the full usage guide below.
Usage
Keep in mind that you need to be both a HipChat and a Teams admin in your company for this to work.

As a HipChat admin export your HipChat data from under you HipChat URL (e.g https://lombiq.hipchat.com), Group admin, Data export. Select to export every kind of data and the whole history. Use a password without any special characters or spaces! Save the file under a path without any special characters.
Download the OpenSSL binaries if your system doesn't have them already. Recommended is the 1.0.2a (not any other version!) x64 zip from here (direct link to file). Unzip it to a folder whose path doesn't contain any special characters or spaces, run openssl.exe and decrypt the export file with the following command: aes-256-cbc -d -in C:\path\to\export\file.tar.gz.aes -out C:\export.tar.gz -pass pass:password.
Use your favorite ZIP utility (7-Zip recommended) to extract the gz and tar so finally you'll end up with an unencrypted, unzipped export folder (this will contain folders like rooms and users and some further files like rooms.json and users.json). If you get a ""The parameter is incorrect"" error in 7-Zip then first unzip the gz archive to a folder, then unpack the tar file as a second step. While this decrypt-unzip could be automated it's a yak shaving of epic proportions (but feel free to contribute it if you wish!) but you'll have to do it once any way.
Go to the Graph Explorer and log in. Note that the user account you're logging in there will be visible as the author of the messages you import, so it's recommended to use a special user account for this (like ""HipChat Import""). Confirm the required permissions. Then acquire the necessary permissions as following:

Click on ""show more samples"", turn ""Microsoft Teams"" and ""Microsoft Teams (beta)"" on.
Try to run e.g. the Microsoft Teams / create channel operation. You'll get an error that you don't have the necessary permissions. Click on ""modify your permission"".

Select the following permissions: Group.ReadWrite.All, User.Read.All. You'll need to log in again.


Once the permissions are OK then run an API request (it can be any of the samples, even just /me. Copy the bearer token (just the token, without the ""Bearer"" text) used by the request into the AppSettings.json configuration file under the AuthorizationToken config. You can e.g. use Chrome DevTools (open with F12 in Chrome) to see this token in the Request headers:

Specify the rest of the configuration as well:

ExportFolderPath: The file system path to the folder where you unzipped the HipChat export package.
NumberOfHipChatMessagesToImportIntoTeamsMessage: You may be able to guess :). If it's greater than 1 then multiple HipChat messages will be imported into a single Teams message. You can use this to overcome Graph API throttling limitations. It seems that a safe general maximum is about 25 (suitable for rooms with many HTML bodied notifications too), with more HipChat messages the request will be too large (depends on how long messages usually are, that can vary a lot); go with lower if you want to be sure. If the value is too high you'll get ""Importing x HipChat messages into a Teams message resulted in a message too large."" errors. A good strategy is try the value 50 (or even 100), then lower it if you get a lot of errors to find out what suits your chat history best. The Teams API rate limit is at about 1800 requests a day, so you'll only be able to import 1800 messages if you don't use this option before throttling kicks in. Importing into multiple teams (see the HipChatRoomsToTeams option) may increase this overall limit.
ShortenLongMessagesToCharacterCount: HipChat messages can be longer than allowed by Teams, so importing some longer HipChat messages can fail. If this configuration is 0 then in such a case importing will fail and you'll need to manually shorten the message in the HipChat export package; otherwise the message will automatically be shortened to the given character count.
UploadAttachments: If set to true HipChat file attachments will be uploaded to the respective Teams channels, linked from (or in case of images, embedded into) their corresponding messages. Set to false if you don't want attachments to be uploaded.
HipChatRoomsToTeams: Map HipChat room names to team names in Teams, so their corresponding channels will be created there. This way you can configure under which team to create channels. Since channels can't be moved across teams you need this if you don't want all the channels under a single team. Configure multiple room name-team name pairs like this: ""HipChatRoomsToTeams"": { ""$Default"": ""Team 1"", ""$Archived default"":  ""Archive"", ""Room 1"": ""Team 2"" }. If no team is configured for a room then the one under $Default will be used; similarly the team under $Archived default will be used for archived rooms (if this config is missing then the team under $Default will be used for archived rooms too). If a given team is not found then it'll be created (for security reasons as a Private team, you can change this later).
HipChatRoomsToChannels: Map HipChat room names to channel names in Teams. This way you can import the content of HipChat rooms into existing Teams channels, like utilizing the default General channel. Uses the same syntax as HipChatRoomsToTeams. If there's no mapping for a given room then a channel will be created for it. Note that this works together with HipChatRoomsToTeams: so e.g. if you want to import multiple rooms into multiple teams' General channels, then first configure the teams for the rooms with HipChatRoomsToTeams, then configure the room (""General"" in this example) with HipChatRoomsToChannels.


Run the app and wait for the import to complete. In the console you'll see status and possibly error messages. Since not all errors can be resolved automatically, and the bearer token can expire too. So don't let the app run too long (about half an hour) without checking its status.

Notable features missing and bugs
See the issue tracker on GitHub.
Some implementation notes

Here's some information on the HipChat export's schema.
Some inspiration is taken from https://github.com/microsoftgraph/csharp-teams-sample-graph.

Contribution and Feedback
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/hipchat-to-microsoft-teams-migration-utility/ (Mercurial repository)
https://github.com/Lombiq/HipChat-to-Microsoft-Teams-Migration-Utility (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub. Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",7
pollpa/PollPA,Python,"PollPA
Real-time polling project for Phillips Academy Students
",7
glouw/openempires,C,"Open Empires
This is a reverse engineering of Age of Empires II intended for fast network play.
Dependencies
sdl2
sdl2_ttf

Building
git clone https://github.com/glouw/openempires
cd openempires
make -C src
./openempires --path ""/home/gl/.steam/steam/steamapps/common/Age2HD/Directory/data/""

Current Progress
[x] Assest loading
[x] Multithreaded CPU rendering
[ ] Pathfinding
[ ] Multiplayer networking

Development Blog
http://glouw.com/2019/04/10/Reverse-Engineering-Age-of-Empires-2-Part-0.html
Credits
Thanks to the SFTech OpenAge team for their fantastic reverse engineering documentation.
https://github.com/SFTtech/openage
",2
haml/haml,Ruby,"Haml





Haml is a templating engine for HTML. It's designed to make it both easier and
more pleasant to write HTML documents, by eliminating redundancy, reflecting the
underlying structure that the document represents, and providing an elegant syntax
that's both powerful and easy to understand.
Basic Usage
Haml can be used from the command line or as part of a Ruby web framework. The
first step is to install the gem:
gem install haml
After you write some Haml, you can run
haml document.haml
to compile it to HTML. For more information on these commands, check out
haml --help
To use Haml programatically, check out the YARD documentation.
Using Haml with Rails
To use Haml with Rails, simply add Haml to your Gemfile and run bundle.
gem 'haml'
If you'd like to replace Rails's Erb-based generators with Haml, add
haml-rails to your Gemfile as well.
Formatting
The most basic element of Haml is a shorthand for creating HTML:
%tagname{:attr1 => 'value1', :attr2 => 'value2'} Contents
No end-tag is needed; Haml handles that automatically. If you prefer HTML-style
attributes, you can also use:
%tagname(attr1='value1' attr2='value2') Contents
Adding class and id attributes is even easier. Haml uses the same syntax as
the CSS that styles the document:
%tagname#id.class
In fact, when you're using the <div> tag, it becomes even easier. Because
<div> is such a common element, a tag without a name defaults to a div. So
#foo Hello!
becomes
<div id='foo'>Hello!</div>
Haml uses indentation to bring the individual elements to represent the HTML
structure. A tag's children are indented beneath than the parent tag. Again, a
closing tag is automatically added. For example:
%ul
  %li Salt
  %li Pepper
becomes:
<ul>
  <li>Salt</li>
  <li>Pepper</li>
</ul>
You can also put plain text as a child of an element:
%p
  Hello,
  World!
It's also possible to embed Ruby code into Haml documents. An equals sign, =,
will output the result of the code. A hyphen, -, will run the code but not
output the result. You can even use control statements like if and while:
%p
  Date/Time:
  - now = DateTime.now
  %strong= now
  - if now > DateTime.parse(""December 31, 2006"")
    = ""Happy new "" + ""year!""
Haml provides far more tools than those presented here. Check out the reference
documentation
for full details.
Indentation
Haml's indentation can be made up of one or more tabs or spaces. However,
indentation must be consistent within a given document. Hard tabs and spaces
can't be mixed, and the same number of tabs or spaces must be used throughout.
Contributing
Contributions are welcomed, but before you get started please read the
guidelines.
After forking and then cloning the repo locally, install Bundler and then use it
to install the development gem dependencies:
gem install bundler
bundle install
Once this is complete, you should be able to run the test suite:
rake
You'll get a warning that you need to install haml-spec, so run this:
git submodule update --init
At this point rake should run without error or warning and you are ready to
start working on your patch!
Note that you can also run just one test out of the test suite if you're working
on a specific area:
ruby -Itest test/helper_test.rb -n test_buffer_access
Haml currently supports Ruby 2.0.0 and higher, so please make sure your changes run on 2.0+.
Team
Current Maintainers

Akira Matsuda
Matt Wildig
Tee Parham
Takashi Kokubun

Alumni
Haml was created by Hampton Catlin, the author of
the original implementation. Hampton is no longer involved in day-to-day coding,
but still consults on language issues.
Natalie Weizenbaum was for many years the primary developer
and architect of the ""modern"" Ruby implementation of Haml.
Norman Clarke was the primary maintainer of Haml from 2012 to 2016.
License
Some of Natalie's work on Haml was supported by Unspace Interactive.
Beyond that, the implementation is licensed under the MIT License.
Copyright (c) 2006-2019 Hampton Catlin, Natalie Weizenbaum and the Haml team
Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the ""Software""), to deal in
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
the Software, and to permit persons to whom the Software is furnished to do so,
subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
",3406
veloren/veloren,Rust,"






Welcome To Veloren!
Veloren is a multiplayer voxel RPG written in Rust. Veloren takes inspiration from games such as Cube World, Minecraft and Dwarf Fortress. The game is currently under heavy development, but is playable.
At the moment the engine is undergoing a complete rewrite, so most basic things do not work yet.
(You can see the original engine here, but we concentrate all further development on the new engine.)
Development
Currently the communication of contributors happens mainly on our official Discord server. You can join it to keep up with the development, talk to us or contribute something yourself. Anyone who shows genuine effort to help is welcome in our team. You don't have to know how to program to contribute!
Useful Links
The Book - A collection of all important information relating to Veloren. It includes information on how to compile Veloren and how to contribute.
Future Plans - Go here for information about Veloren's development roadmap and what we're currently working on.
Official social media and websites

Website
Discord Server
Subreddit

Get Veloren
We currently don't provide binary downloads of Veloren, you have to compile it yourself.
Take a look at the How to Compile Guide in the book.
F.A.Q.
Q: How is this game licensed?
A: It's free to play, modify and distribute. Forever. Since it is a community project, we decided to license it under the GNU GPL 3.0 license which means it will always stay free and open source.
Q: What platforms are supported?
A: Veloren can run on Windows, Linux and Mac OS on all architectures. It's probably possible to compile Veloren on/for BSD, Fuchsia and others as well.
Q: Do you accept donations?
A: To keep Veloren a passion project free from financial incentives we will only accept donations to cover server hosting expenses. There is no way to donate yet.
Credit
Many thanks to everyone that has contributed to Veloren's development, provided ideas, crafted art, composed music, hunted bugs, created tools and supported the project.
",175
willbryant/kitchen_sync,C++,"Kitchen Sync
Goal: Fast unidirectional synchronization - make or efficiently update a copy of a database, without slow dumping & reloading.
Kitchen Sync aims to:

Finish the job much faster than full reloads: working in parallel and only modifying the data that's actually changed, Kitchen Sync is usually 5-20 times faster than loading from dumps, which minimizes the length of time you need to take our test environments down to resync and helps bring up new developer and test environments quickly.
Work efficiently over Internet or WAN links: Kitchen Sync can run on separate servers at either end of a long connection, using an internal protocol that's optimised to work well over long links, making it possible to resynchronize database servers at different datacentres or laptops and test servers at remote offices and homes.
Transport over SSH: Kitchen Sync has built-in support for transport over SSH connection(s), so no extra firewall permissions need to be added or extra access granted to protected servers to synchronize from remote database servers.
Synchronise between different database servers or versions: Kitchen Sync performs logical synchronisation using the regular SQL database interface rather than synchronisation of the files on disk, so you can use it to synchronize to database servers with different architectures, operating systems, storage engines, major versions, compression options, and even competing database products (MySQL, MariaDB, and PostgreSQL currently supported).
Minimize write traffic to the target database: as well as maximising update performance & SSD life, if you host your target database on a filesystem or storage cluster that supports Copy-on-Write, the storage requirements will grow only in proportion to the actual changes, you can store many versions of the database with minimal storage growth for datasets that tend to have large amounts of unchanged data between versions.
Produce partial replicas: optionally exclude tables that are not required at the target end, or synchronise only specific tables or even only rows matching certain criteria, with all other data being cleared at the target end to help reset to a known state.
Filter out sensitive data on-the-fly: define column expressions to overwrite certain data as it is retrieved from the source database server, to ensure that sensitive customer or business data never leaves the origin database server - even if a full-sized and otherwise complete production-like copy is needed for testing and development.
Check and update schema: Kitchen Sync will check that the target database schema matches the source, and can (optionally) recreate or alter tables to make them match.

Installation
Please see Installing Kitchen Sync.
Usage
Synopsis:
ks --from mysql://user1:mypassword1@server1/sourcedb \
   --to postgresql://user2:mypassword2@server2/targetdb \
   --filter strip_personal_info.yml \
   --workers 4

Please see Using Kitchen Sync to get started.
Supported databases

MySQL/Percona Server/MariaDB: 5.5 and above.
PostgreSQL: 9.2 and above.

Feature support
Please see Supported schema for a list of the currently supported database objects and types.
Bugs
Please use Github issues and check if your issue has already been reported first.
",156
yourduskquibbles/webannoyances,None,"Web Annoyances Ultralist - unsuck the web!
Intro
Are you tired of not being able to use all of your screen real estate for the text that matters on a website
because floating headers and other distracting elements are blocking your view?
Reclaim your screen real estate with Web Annoyances Ultralist!
Block annoying web elements such as sticky headers, dickbars, floating headers, scrolling headers, scrolling videos, stickynavs, social icons, social share bars, smartphone app banners, app download nag screens, floating navigation boxes, cookie notices, gdpr warnings, jump to top navs, modal overlays, interstitial site overlays, removed or hidden overflow scroll bars, subscription nags, and generally distracting elements that have increasingly been turning the web into a user-hostile environment.
When possible, sticky headers, stickynavs, floating elements, scrolling videos and more will be pinned in place to prevent site breakage.
Demonstration
Typical Website in 2019 Before Installing Web Annoyances Ultralist

Above screenshot taken using uBlock Origin Default Filter Lists
Typical Website in 2019 After Installing Web Annoyances Ultralist

Above screenshot taken using uBlock Origin Default Filter Lists + Web Annoyances Ultralist
Head over to the demonstration page for additional visual examples of Web Annoyances Ultralist in action.
Requirements
This filter list aims to remove annoying web elements that block your view of the screen and is created and optimized using the uBlock Origin Extended Syntax.
For the best user experience, please use this filter list with uBlock Origin or Nano Adblocker
Browser specific download locations are located below:
Desktop

uBlock Origin for Google Chrome
uBlock Origin for Mozilla FireFox
Nano Adblocker for Microsoft Edge
uBlock Origin for Apple Safari
uBlock Origin for Opera

Mobile

uBlock Origin for Mozilla FireFox for Android

One Click Installation
Click the following: Add Web Annoyances Ultralist to Custom uBlock Origin Filters.
Installation Notes
After clicking the Link above, a warning box will appear asking you to confirm the installation - Click 'OK' to add the list to your Custom Filter Lists.
The warning box will look like the following:

After clicking 'OK' the filter list should be installed.
You can validate the list installed properly by navigating to your uBlock Origin Dashboard and clicking on the Filter lists tab. Scroll to the bottom of the screen and verify Web Annoyances Ultralist is showing up in your custom section.

Step-by-Step install instructions are available in the wiki.
Discord Server
Join the yourduskquibbles Discord Server @ https://discord.me/yourduskquibbles for faster reporting of issues and chat.
Mirror Hosts of Web Annoyances Ultralist
https://cdn.staticaly.com/gh/yourduskquibbles/webannoyances/master/ultralist.txt
https://cdn.jsdelivr.net/gh/yourduskquibbles/webannoyances/ultralist.txt
",465
Ovilia/2019-typography-calendar,TypeScript,"2019 字体日历 App



本地调试方法

先安装 Node.js、npm。
npm install -g ionic@3.20.1
npm install -g cordova@8.1.0

然后执行 ionic serve 会起一个本地服务器，在 Chrome 中调试。
",284
kiteco/atom-plugin,JavaScript,"Kite Python Assistant
Kite is an AI-powered programming assistant that helps you write Python code inside Atom. The
Kite Engine needs to be installed in order for the package to work properly. The package itself
provides the frontend that interfaces with the Kite Engine, which performs all the code analysis and machine learning.
Features
Kite's goal is to help you write code faster by showing you the right information at the right time. At a high level,
Kite provides you with:

🧠 Smart autocompletions powered by machine learning models trained on the entire open source code universe
👀 Advanced function signatures that show you not only the official signature of a function, but also the most
popular ways other developers call the function
🔍 Instant documentation for the symbol underneath your cursor

Requirements

macOS 10.11+, Windows 7+ or Linux
Atom v1.13.0+
Kite Engine

Installation
Installing the Kite Engine
macOS Instructions

Download the installer and open the downloaded .dmg file.
Drag the Kite icon into the Applications folder.
Run Kite.app to start the Kite Engine.

Windows Instructions

Download the installer and run the downloaded .exe file.
The installer should run the Kite Engine automatically after installation is complete.

Installing the Kite Assistant for Atom
When running the Kite Engine for the first time, you'll be guided through a setup process which will allow you to install
the Atom package. You can also install or uninstall the Atom package at any time using the Kite Engine's plugin
manager.
Alternatively, you have 2 options to manually install the package:

Search for ""Kite"" in Atom's built-in package manager and install from there.
Run the command apm install kite in your terminal.

Learn more about Kite for Atom.
Usage
The following is a brief guide to using Kite in its default configuration.
Tutorial
When starting Atom with the Kite Assistant for the first time, you'll be guided through a tutorial that shows you how to
use Kite.

This tutorial will only be displayed once. You can show it again at any time by running the command Kite: Tutorial from
Atom's command palette.
Hover
Hover your mouse cursor over a symbol to view a short summary of what the symbol represents.

Documentation
Click on the Docs link in the hover popup to open the documentation for the symbol inside the Copilot, Kite's standalone
reference tool.

Definitions
If a Def link is available in the hover popup, clicking on it will jump to the definition of the symbol.
Autocompletions
Simply start typing in a saved Python file and Kite will automatically suggest completions for what you're typing.

Function Signatures
When you call a function, Kite will show you the arguments required to call it.

Kite also shows you How others used this function, which are the most popular calling patterns inferred from all the
open source code on the internet.
Commands
Kite comes with sevaral commands that you can run from Atom's command palette.




Command
Description




kite:open-copilot
Open the Copilot


kite:docs-at-cursor
Show documentation of the symbol underneath your cursor in the Copilot


kite:status
Show the current status of Kite in the status panel


kite:package-settings
Open the settings for the Kite Atom package


kite:engine-settings
Open the settings for the Kite Engine


kite:tutorial
Open the Kite tutorial file


kite:help
Open Kite's help website in the browser



If you wish, you may also setup keybindings for the commands listed above.
Configuration
You can view and change the Kite Assistant's settings by finding Kite in your list of installed packages, then clicking
the Settings button. Alternatively, you can run the command Kite: Package Settings from the command palette.
Contact Us
Feel free to contact us with bug reports, feature requests, or general comments at feedback@kite.com.
Happy coding!

About Kite
Kite is built by a team in San Francisco devoted to making programming easier and more enjoyable for all. Follow Kite on
Twitter and get the latest news and programming tips on the
Kite Blog.
Kite has been featured in Wired,
VentureBeat,
The Next Web, and
TechCrunch.
",42
mamedev/mame,C++,"MAME

Build status for tiny build only, containing just core parts of project:



OS/Compiler
Status




Linux GCC / OSX Clang



Windows MinGW



Windows MSVC




Static analysis status for entire build (except for third-party parts of project):

What is MAME?
MAME is a multi-purpose emulation framework.
MAME's purpose is to preserve decades of software history. As electronic technology continues to rush forward, MAME prevents this important ""vintage"" software from being lost and forgotten. This is achieved by documenting the hardware and how it functions. The source code to MAME serves as this documentation. The fact that the software is usable serves primarily to validate the accuracy of the documentation (how else can you prove that you have recreated the hardware faithfully?). Over time, MAME (originally stood for Multiple Arcade Machine Emulator) absorbed the sister-project MESS (Multi Emulator Super System), so MAME now documents a wide variety of (mostly vintage) computers, video game consoles and calculators, in addition to the arcade video games that were its initial focus.
How to compile?
If you're on a *NIX or OSX system, it could be as easy as typing
make

for a MAME build,
make SUBTARGET=arcade

for an arcade-only build, or
make SUBTARGET=mess

for MESS build.
See the Compiling MAME page on our documentation site for more information, including prerequisites for Mac OS X and popular Linux distributions.
For recent versions of OSX you need to install Xcode including command-line tools and SDL 2.0.
For Windows users, we provide a ready-made build environment based on MinGW-w64.
Visual Studio builds are also possible, but you still need build environment based on MinGW-w64.
In order to generate solution and project files just run:
make vs2017

or use this command to build it directly using msbuild
make vs2017 MSBUILD=1

Where can I find out more?

Official MAME Development Team Site (includes binary downloads for MAME and MESS, wiki, forums, and more)
Official MESS Wiki
MAME Testers (official bug tracker for MAME and MESS)

Contributing
Coding standard
MAME source code should be viewed and edited with your editor set to use four spaces per tab. Tabs are used for initial indentation of lines, with one tab used per indentation level. Spaces are used for other alignment within a line.
Some parts of the code follow Allman style; some parts of the code follow K&R style -- mostly depending on who wrote the original version. Above all else, be consistent with what you modify, and keep whitespace changes to a minimum when modifying existing source. For new code, the majority tends to prefer Allman style, so if you don't care much, use that.
All contributors need to either add a standard header for license info (on new files) or inform us of their wishes regarding which of the following licenses they would like their code to be made available under: the BSD-3-Clause license, the LGPL-2.1, or the GPL-2.0.
License
The MAME project as a whole is distributed under the terms of the GNU General Public License, version 2 or later (GPL-2.0+), since it contains code made available under multiple GPL-compatible licenses. A great majority of files (over 90% including core files) are under the BSD-3-Clause License and we would encourage new contributors to distribute files under this license.
Please note that MAME is a registered trademark of Gregory Ember, and permission is required to use the ""MAME"" name, logo, or wordmark.



Copyright (C) 1997-2019  MAMEDev and contributors

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License along
with this program; if not, write to the Free Software Foundation, Inc.,
51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

Please see LICENSE.md for further details.
",3734
ablakely/shadow,Perl,"Shadow Perl IRC Bot
Written by Aaron Blakely
What is shadow?
shadow is a modular IRC bot written in Perl,
it's goal is to be a stable IRC bot with cool featues.
How do I install shadow?
Just run installdepends.pl, we'll do the rest! (Might need C libs or sudo depending on your system)
Where can I get support for shadow?

Try irc.bbsn.ca #ephasic
/msg bot help


Installing
To install on debian/ubuntu systems you will need to following pakages:
sudo apt-get install libxml-libxml-perl build-essential
Standard Modules

RSS.pm - RSS Feed Reader.
AutoID.pm - Automatically authenticate with network services.
ChanOP.pm - Basic channel management commands.
Uptime.pm - Basic module which gives *nix uptime info
Autojoin.pm - Automatically join channels on connect.
URLIdentifier.pm - Automatically fetches the title of a URL.
BotStats.pm - [Linux] Returns information about the bot, like memory usage.
Fortune.pm - Wrapper for the fortune command.

",2
fthomas/scala-steward,Scala,"
scala-steward





scala-steward is a robot that helps you keeping library dependencies
and sbt plugins up-to-date.
Quick start guide
Open a pull request that adds the GitHub repository of your Scala project
to repos.md.
Once that PR is merged, @scala-steward will check
periodically for updates of libraries and plugins (see examples)
in your project and will open pull requests in your repository if it can figure out
where version numbers need to be updated.
Show us the pull requests!
If you are curious how @scala-steward's pull requests
look like, here are the ones it has created so far:

Created pull requests
(compact view)
Merged pull requests
(compact view)

Contributors
Thanks goes to these wonderful people (emoji key):
Frank S. Thomas💻 ⚠️Bayram Kiran💻 ⚠️Arulselvan Madhavan💻 ⚠️Piotr Gabara💻 ⚠️kenji yoshida💻Zelenya🎨Filipe Regadas💻 ⚠️ 📦Philippus Baalman💻 ⚠️sullis🚇Michael Wizner💻 ⚠️ 📦Thomas Kaliakos💻 ⚠️Jeff⚠️Dale Wijnand⚠️Renato Cavalcanti💻 ⚠️JCollier💻 ⚠️Mark Canlas📖
Community
The following companies are using scala-steward to manage their scala dependencies. Using scala-steward in your company and don't see it listed here? Consider creating PR to add your name to the list and join the community.

Chartboost
HolidayCheck
iAdvize
SlamData
Snowplow Analytics
Zalando

Participation
The scala-steward project supports the Scala Code of Conduct
and wants all of its channels (GitHub, Gitter, etc.) to be welcoming
environments for everyone.
Credit
scala-steward wouldn't exist without the great sbt-updates
plugin to determine dependency updates and a bunch of Typelevel
and other Scala libraries.
@scala-steward's cute profile picture is by
@impurepics.
Running scala-steward
sbt stage

./modules/core/.jvm/target/universal/stage/bin/scala-steward \
  --workspace  ""$STEWARD_DIR/workspace"" \
  --repos-file ""$STEWARD_DIR/repos.md"" \
  --git-author-name ""Scala steward"" \
  --git-author-email ${EMAIL} \
  --github-api-host ""https://api.github.com"" \
  --github-login ${LOGIN} \
  --git-ask-pass ""$STEWARD_DIR/.github/askpass/$LOGIN.sh"" \
  --sign-commits \
  --env-var FOO=BAR
Or,
sbt docker:publishLocal

docker run -v $STEWARD_DIR:/opt/scala-steward -it scala-steward:0.1.0-SNAPSHOT \
  --workspace  ""/opt/scala-steward/workspace"" \
  --repos-file ""/opt/scala-steward/repos.md"" \
  --git-author-name ""Scala steward"" \
  --git-author-email ${EMAIL} \
  --github-api-host ""https://api.github.com"" \
  --github-login ${LOGIN} \
  --git-ask-pass ""/opt/scala-steward/.github/askpass/$LOGIN.sh"" \
  --sign-commits \
  --env-var FOO=BAR
If you run scala-steward for your own private projects, you can pass additional environment variables from the command line using the --env-var flag as shown in the examples above. You can use this to pass any credentials required by your projects to resolve any private dependencies, e.g.:
--env-var BINTRAY_USER=username \
--env-var BINTRAY_PASS=password
These variables will be accessible (in sbt) to all of the projects that scala-steward checks dependencies for.
License
scala-steward is licensed under the
Apache License, Version 2.0.
",278
d2l-ai/d2l-zh,Python,"动手学深度学习

本书网址：zh.d2l.ai
1.0.0版rc0发布
如何安装和使用书中源代码
英文版 Dive into Deep Learning
加州大学伯克利分校 2019 年春学期 Introduction to Deep Learning 课程教材。
开源地址：https://github.com/d2l-ai/d2l-en
英文版引用
BibTeX entry:
@book{zhang2019dive,
    title={Dive into Deep Learning},
    author={Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},
    note={\url{http://www.d2l.ai}},
    year={2019}
}

贡献
感谢社区贡献者们为每一位读者改进这本开源书。
如何贡献 | 致谢 | 讨论或报告问题 | 其他
",7828
brion/wasmosis,C,"Wasmosis kernel
This is currently a rough design document, the code is in progress and will be added shortly. -brion
Wasmosis is (will be) a ""microkernel-inspired"" JavaScript supervisor for WebAssembly modules to securely interoperate without sharing a memory address space. It's intended to be suitable for running semi-trusted or untrusted plugins in a JS or WebAssembly application.
Background
The name ""Wasmosis"" comes from ""Wasm"", short for ""WebAssembly"", and ""osmosis"", a biochemical process where ions pass across a semi-permeable membrane in a specific direction. This is meant to invoke the secure passing of certain data only across the module/kernel boundary.
L4 and seL4 microkernels inspired the capability-based security and the message-passing via synchronous function call, with modifications for a single-threaded web context.
Goals
High-level:

Provide a way for web apps to embed WebAssembly ""plugins"", ""widgets"", etc with relatively high security guarantees.
Have this model work for both trusted JS-implemented modules and untrusted Wasm-implemented modules.
Allow safe transfer of object references from module to module (eg, modules can safely delegate their permissions on to their own plugins)
Implement an example app+plugin in the web/browser environment.

Low-level:

Define a minimal ""microkernel""-like API+ABI for capability-passing and synchronous calls between modules with separate address spaces.
Define mid-level web-oriented embedding APIs providing an asynchronous event loop, event subscription, and promises.
Implement a JS kernel.
Implement a C API header.
Implement a Rust API crate.

Example plans to exercise the kernel API and implementation:

Implement a sample JS module exposing an API (say, an HTML <canvas> drawing context with mouse/touch events).
Implement a sample Wasm widget module (say, a simple paint app).
Implement a sample Wasm plugin module for it (say, an image filter).

Process model
WebAssembly module instances each have a 4 GiB linear memory address space, with usable memory in a single flat run from position 0 until the current size of memory. Memory can be expanded in 64 KiB increments up to a limit predefined when creating the memory. There is also a table of runtime-callable function references, with C function pointers implemented as table indexes.
In traditional dynamic linking, the memory and table are shared between instances of different modules. This allows any module to modify all the memory and call all the functions of its fellow linkees -- not a desireable property for untrusted plugins.
Wasmosis instead lets each module instance have its own memory and function table address spaces, plus a kernel-maintained capabilities namespace. Function calls between modules are mediated through the kernel, which translates the arguments from one module's caps namespace to the other. The cap values in the target namespace can then be accessed via kernel syscalls, or passed on to another module.
Single vs multi-threading
The JavaScript / web platform is primarily a single-threaded world today, and Wasmosis targets that. This model can be extended to run separate contexts in Web Workers, with async message passing between contexts.
True multithreading with SharedArrayBuffer would allow synchronous calls across workers, and might be considered in the future.
Security model

Memory safety:

(good) Separate address spaces, so buffer transfers are through cap transfer and syscalls.
(good) Transfer buffers have owner-controlled lifetimes, so cannot be used after invalidation.
(good) Transfer buffers are either read-only or write-only, to avoid exposing uninitialized memory.


DoS issues:

(good) Linear memory usage can be bounded by the kernel at module instantiation time.
(good) Number and type of caps created by a process can be tracked by the kernel, so reasonable limits can be applied.
(slightly bad) Deep function call stack recursion will throw a trap, which can be caught by the kernel and the module terminated safely. However if another module gets very close to the stack limit and then calls into your module, you might be the one that gets terminated.
(somewhat bad) A call to a port may never yield control back (the halting problem). If running on the main web thread, this will eventually cause a timeout and prompt the user to halt it, which may leave module data in an inconsistent state due to failure to unwind the stack. If running in a web worker, it will keep running until the worker is terminated.



For web applications, this mostly means that your failure scenario is a ""crash"" of the web app, fixed by a page refresh or ""rebooting"" just your affected WebAssembly modules. However if you ""autorun"" anything, you need to be able to run enough of the app to disable the offending plugin or your users could end up stuck in a broken state.
Kernel object types and syscalls
Capability references may point to one of various types of kernel objects. This is a single shared namespace per module; making a syscall on an object of the wrong type will fail gracefully, but making message calls to a handle with an unexpected interface may be less forgiving! Processes are assumed to be able to keep track of their own indexes, just like they track pointer types. :)

Null sigil (CAP_NULL = 0, cannot be released or reassigned)
Boxed values
Send buffers
Recv buffers
Handles

Objects keep track of which module created them and the owner may perform privileged operations on them, such as revoking the validity of a buffer or retrieving the local pointer value of a handle. Attempting to perform these operations on another module's handle will fail gracefully.
Generic syscalls
The following syscall functions for the module->kernel API are valid on all cap types:

cap_release(cap): Releases the cap index from the local namespace, and queues it for reuse on future cap allocation. Call this on caps you got as a return value from a syscall or message call, but never on ""borrowed"" references from port call arguments. This does not free any resources belonging to the owner process, which may have other live references.
cap_retain(cap) -> cap2: Creates a new index for the cap reference, which you may keep and use after the original one is cap_released.
cap_revoke(cap): Invalidates an owned object, so it can no longer be used. Suitable for calling to revoke buffers and handles from being used after resources are freed. Note that this does not release the index, which is still allocated.

Boxed values
To simplify the calling conversion functions, all port call arguments are transferred through capabilities, including numeric primitives. In the JS kernel this stores a number or boolean primitive in the same array slot that would have been an object reference for other types, so this should be pretty efficient -- no actual heap allocation is required for boxing.

box_i32(val) -> cap, unbox_i32(cap) -> val
box_u32(val) -> cap, unbox_u32(cap) -> val
box_f32(val) -> cap, unbox_f32(cap) -> val
box_f64(val) -> cap, unbox_f64(cap) -> val
box_bool(val) -> cap, unbox_bool(cap) -> val

This also means that boxes are ""unowned"", since they don't have space for a reference. You cannot revoke a boxed value cap. Unboxing a different numeric type from that used to create will convert; unboxing a cap of another type will return 0 (or false).
Note that there is no i64 or u64 boxed type, as until BigInt is reliably available WebAssembly and JS can't send i64s to each other.
Similarly, floating-point transfers will canonicalize NaN values due to the limitations of the JavaScript kernel. If you need to transfer 64-bit ints or custom NaN values across modules, use a byte-wise buffer.
Send buffers
Send buffers are slices of your memory space to be sent to another module, which can read them but not write back to them or access beyond the boundaries. They're suitable for constant data that must not be altered.

sendbuf_create(src_ptr, len) -> cap
sendbuf_read(cap, dest_ptr, len) -> bytes_read: Copies the buffer from one module's memory to another, within defined limits. This advances an internal pointer, and may be called multiple times with different sub-destinations.
sendbuf_bytes_read(cap) -> bytes_read: For an owned cap, returns the internal pointer of how many bytes have been read.

When done with it, use cap_revoke to invalidate the pointer.
Recv buffers
Recv buffers can be written to by other modules, but they can't read what's in it. This allows sending a reference to allocated-but-uninitialized memory without fear of old data being read out of it.

recvbuf_create(dest_ptr, len) -> cap
recvbuf_write(cap, src_ptr, len) -> bytes_written: Copies the buffer from one module's memory to another, within defined limits. This advances an internal pointer, and may be called multiple times with different sub-sources.
recvbuf_bytes_written(cap) -> bytes_written: For an owned cap, returns the internal pointer of how many bytes have been written. This should be used instead of trusting a count returned by another module, to avoid consuming your own uninitialized memory.

When done with it, use cap_revoke to invalidate the pointer.
Handles
A handle holds an opaque pointer in your module's address space, which can be retrieved if you're the owner but not otherwise. This lets modules create un-forgeable references to objects or structs, which can be passed away and then passed back.
Handles may also hold zero or more function pointers, which holders of the handle may invoke as ""message"" calls. This makes handles suitable for opaque handles (state pointer only), closures (state pointer plus one function pointer), or object interfaces (state pointer plus multiple function pointers, indexed by application-specific convention).

handle_create(class_ref, user_data, funcs_ptr, funcs_len) -> cap: create a new handle with the given opaque class ref pointer, instance pointer, and an array of zero or more function pointers.
handle_user_data(cap, class_ref) -> user_data: retrieve the instance pointer for an owned cap, given a matching opaque class ref pointer.

The ""class ref"" opaque pointer is used to validate the handle along with the cap ownership -- it's convenient to use something like a symbol address that's type-unique within the process. If you ask for a handle's user_data without owning it, or without a matching class_ref, you'll get NULL back.
When done with it, use cap_revoke to invalidate the pointer.
Message calls
Given a handle and a method index, you can make synchronous calls to it with some specific number of parameters (currently from 0 to 4) and a single return value, all of which are in the caps namespace.
When you create a handle, you associate it with an array of callable function pointers and an opaque user data pointer, which can be retrieved by the owning module when taking an incoming call. This allows routing runtime-generated callbacks as well as traditional fixed functions.
Calling a port with the wrong number of parameters is not yet fully defined behavior, it will probably work but pass null caps for missing params.

handle_call0(handle, index) -> cap_ret
handle_call1(handle, index, cap1) -> cap_ret
handle_call2(handle, index, cap1, cap2) -> cap_ret
handle_call3(handle, index, cap1, cap2, cap3) -> cap_ret
handle_call4(handle, index, cap1, cap2, cap3, cap4) -> cap_ret
... more may be added, perhaps up to 8.

Note that caps passed as arguments are ""borrowed"" -- on cross-module calls the kernel layer will retain/release the transferred caps on entry/exit. So if you receive a cap as an argument and want to keep it for later, you must cap_retain it and keep the copy.
The cap returned as a return value is ""caller-owned"" so you must call cap_release when you're done with it -- even if it's a boxed integer, you need to release the index so it can be reused.
ABI
The ABI is meant to be as minimal as possible, making few assumptions about the module's memory layout or imports/exports. For instance, the kernel never allocates data on the module's stack or heap, nor alters its function tables. This allows modules to be C-style with a global stack pointer and internal malloc/free... or have wildly custom memory layouts.
Syscall functions will be provided as imports to the WebAssembly module instantiation, in a two-level namespace where the first namespace is __wasmosis and the second is eg cap_release.
At least one entry point function should be provided as an export. Need to consider whether this will be application-protocol-defined or a standard initialization time function created...

@todo work that out

API
In the C API, syscall names will be exposed in a flat namespace as __wasmosis_cap_release etc, probably #define'd back into short names. (?)
A Rust API building on the same syscalls will have prettier structs and functions wrapping these to aid in type-safety and lifetime control.
Portability
Non-JS kernels for non-web WebAssembly embeddings would also be possible, but I have not yet looked into it.
Further, there is nothing really WebAssembly-specific in the API... The C and Rust APIs, if defined with appropriately portable types, could also work when compiling to native code.
A native implementation of the kernel and ABI that uses container isolation is left as an exercise for another time... however it may be useful for testing to make native builds of things that use an in-process dummy kernel. This would provide no memory isolation, but could keep separate caps namespaces for each module, switching active processes at the handle_call* boundary.
License
MIT-style license for the initial JS kernel implementation and C and Rust APIs.
",3
appbaseio/reactivesearch,JavaScript,"


  Reactive Search
  

A React and React Native UI components library for Elasticsearch

Read how to build an e-commerce search UI

a.)  with React, or b.) with React Native.















Web designer templates for sketch.

iOS and Android designer templates for sketch.

TOC

ReactiveSearch: Intro
Features
Component Playground
Live Examples
Comparison with Other Projects
Installation
Docs Manual
Contributing
Other Projects You Might Like


1. ReactiveSearch: Intro
ReactiveSearch is an Elasticsearch UI components library for React and React Native. It has 25+ components consisting of Lists, Ranges, Search UIs, Result displays and a way to bring any existing UI component into the library.
The library is conceptually divided into two parts:

Sensor components and
Actuator components.

Each sensor component is built for applying a specific filter on the data. For example:

A SingleList sensor component applies an exact match filter based on the selected item.
A RangeSlider component applies a numeric range query based on the values selected from the UI.
A DataSearch component applies a suggestions and search query based on the search term typed by the user.

Sensor components can be configured to create a combined query context and render the matching results via an actuator component.
ReactiveSearch primarily comes with two actuators: ResultCard and ResultList. ResultCard displays the results in a card interface whereas ResultList displays them in a list. Both provide built-in support for pagination and infinite scroll views. Besides these, the library also provides low level actuators (ReactiveComponent and ReactiveList) to render in a more customized fashion.

2. Features
Design

The sensor / actuator design pattern allows creating complex UIs where any number of sensors can be chained together to reactively update an actuator (or even multiple actuators).
The library handles the transformation of the UI interactions into database queries. You only need to include the components and associate each with a DB field.
Built in live updates - Actuators can be set to update results even when the underlying data changes in the DB.
Comes with scoped and styled components. Doesn't require any external CSS library import, comes with className and innerClass support.
Is themable via ThemeProvider.

Ease of Use

One step installation with npm i @appbaseio/reactivesearch,
A UMD build that works directly in the browser. Installation steps here,
Styled and scoped components that can be easily extended,
See the reactivesearch starter app.

⬆ Back to Top

3. Component Playground
Try the live component playground at playground. Look out for the knobs section in the playground part of the stories to tweak each prop and see the effects.

4. Live Demos
A set of live demos inspired by real world apps, built with ReactiveSearch.
Web

demos/airbeds - An airbnb-like booking search experience.
demos/technews - A full-text search app on the Hacker News dataset.
demos/gitxplore - Explore all the popular Github repositories.
demos/productsearch - Filtered feed of products based on a small Product Hunt dataset.
demos/booksearch - An book search app based on a goodbooks dataset.
demos/ecommerce - An e-commerce car store app.

You can check all of them on the examples section of website.
Mobile

Booksearch on Play Store: A booksearch app showing a searchable collection of over 10k books built with ReactiveSearch. Also available as a snack.
Gitxplore on Play Store: A Github repository explorer app to  search over the 25k+ most popular github repos.
ReactiveTodos on App Store: A shared collaborative to-do list app to showcase the capability of Reactivesearch.

⬆ Back to Top

5. Comparison with Other Projects
Here, we share how ReactiveSearch compares with other projects that have similar aims.



#
ReactiveSearch
SearchKit
InstantSearch




Backend
Any Elasticsearch index hosted on any Elasticsearch cluster.
Any Elasticsearch index hosted on any Elasticsearch cluster.
Custom-built for Algolia, a proprietary search engine.


Development
Actively developed and maintained.
Active issue responses, some development and maintenance.
Actively developed and maintained.


Onboarding Experience
Starter apps, Live interactive tutorial, getting started guide, component playground, every component has a live working demo with codesandbox.
Getting started tutorial, no live component demos, sparse reference spec for many components.
Starter apps, getting started guide, component playground.


Styling Support
Styled and scoped components. No external CSS import required. Rich theming supported as React props.
CSS based styles with BEM, not scoped to components. Theming supported with SCSS.
CSS based styles, requires external style import. Theming supported by manipulating CSS.


Types of Components
Lists, Ranges, Search, Dates, Maps, Result Displays. Can use your own UI components.
Lists, Ranges, Search*, Result*. Can't use your own UI components. (Only one component for Search and Result, resulting in more code to be written for customizability)
Lists, Range, Search, Result. Can use your own UI components.


Supported Distribution Platforms
React, Vue for Web, React Native for mobile.
React for Web.
React, Vue, Angular, vanilla JS for Web, React Native for mobile but latter has no UI components.



We welcome contributions to this section. If you are building a project or you know of another project that is in the similar space, let us know and we will update the comparisons.
⬆ Back to Top

6. Installation
Installing ReactiveSearch is just one command.

If you're using reactivesearch for web

npm install @appbaseio/reactivesearch
You can check out the quickstart guide with React here.

If you're using reactivesearch for mobile (native)

npm install @appbaseio/reactivesearch-native
You can check out the quickstart guide with React Native here.

7. Docs Manual
The official docs for the Web library are at opensource.appbase.io/reactive-manual.
The components are divided into four sections:

Generic UI components are at reactive-manual/base-components.
List based UI components are at reactive-manual/list-components.
Range based UI components are at reactive-manual/range-components.
Search UI components are at reactive-manual/search-components.
Result components are at reactive-manual/result-components.

Docs for React Native version of the library are available at opensource.appbase.io/reactive-manual/native.
⬆ Back to Top

8. Contributing
Please check the contribution guide.

9. Other Projects You Might Like


dejavu allows viewing raw data within an appbase.io (or Elasticsearch) app. Soon to be released feature: An ability to import custom data from CSV and JSON files, along with a guided walkthrough on applying data mappings.


mirage ReactiveSearch components can be extended using custom Elasticsearch queries. For those new to Elasticsearch, Mirage provides an intuitive GUI for composing queries.


ReactiveSearch Dashboard All your Reactive Search related apps (created via interactive tutorial, shared by others, etc.) can be accessed from here.


ReactiveMaps is a similar project to Reactive Search that allows building realtime maps easily.


appbase-js While building search UIs is dandy with Reactive Search, you might also need to add some input forms. appbase-js comes in handy there.


⬆ Back to Top

",3060
Lombiq/Orchard-Liquid-Markup,C#,"Orchard Liquid Markup Readme
Project Description
Orchard module for adding support for templates written in Liquid Markup (http://liquidmarkup.org/). Uses DotLiquid (http://dotliquidmarkup.org/).
Documentation
Overview
This module adds the ability to use shape templates written in Liquid Markup and uses the DotLiquid library. See what Liquid offers. The module is also available for DotNest sites for templating.
Naming conventions are C#-style! This means that all the properties you can access on viewmodels from your templates will have the same names as usual but built-in Liquid filters (like upcase) will also behave in the same way (i.e. you'll be able to use it as Upcase).
There are the following features in the module:

Liquid Markup: doesn't provide any user-accessible features, just basic services.
Liquid Markup Templates: extends Orchard.Templates so you can write Liquid templates from the admin.
Liquid Markup View Engine: adds a view engine that enables you to use .liquid Liquid-formatted templates in your themes and modules. You can use this to develop Liquid templates quickly from an IDE.

Note that since Liquid was designed to be safe you can't call arbitrary methods on services that may be accessible from templates.
Examples
Do note the following:

Although presented here with C#-style notation, custom tags are usable all lowercase too (tags are conventionally all lowercase in Liquid). E.g. these are both valid: {% Display User %} and {% display User %}.
While strings are wrapped in double quotes here single quotes (') work equally.
When passing parameters to tags you can not just pass simple strings but also variable references. E.g. these will both work: {% Display User, Parameter1: ""some value"" %} and  {% Display User, Parameter1: Model.WorkContext.CurrentUser.UserName %}.

Accessing the model
Accessing shape properties:
{{ Model.Id }}
{{ Model.Metadata.Type }}

Accessing the viewmodel from the admin User shape:
{{ Model.CurrentUser.UserName }}
{{ Model.CurrentUser.Email }}

Dynamic expressions on ContentItems work, e.g. you can do this from the Content shape or content part shapes:
{{ Model.ContentItem.TitlePart.Title }}

Accessing array or list elements work as well. E.g. if you add a MediaLibraryPickerField on the Page content type with the name Images you'll be able to access the attached Image items (given that there are at least two) like following:
{{ Model.ContentItem.Page.Images.MediaParts[0].MediaUrl } <- First image.
{{ Model.ContentItem.Page.Images.MediaParts[1].MediaUrl } <- Second image.
{{ Model.ContentItem.Page.Images.Ids[0] }} <-- ID of the first image.
{{ Model.ContentItem.Page.Images.Ids[1] }} <-- ID of the second image.

Accessing the WorkContext
The properties on the WorkContext (and the properties of those objects) are also accessible:
Accessing the currently authenticated user's name: 
{% if Model.WorkContext.CurrentUser != null %}
  {{ Model.WorkContext.CurrentUser.UserName }}
{% else %}
  Please log in!
{% endif %}

Using the HttpContext:
{{ Model.WorkContext.HttpContext.Request.Url.AbsoluteUri }}

Including static resources
Including stylesheets and scripts:
{% Style ""/url/to/stylesheet.css"" %}
You can omit the single quotes or use double quotes instead if you wish.
Note that relative virtual paths (like ""~/Themes/MyTheme/Styles/styles.css"") will work too as usual.

{% Script ""/url/to/script.js"", head %}
The second parameter is the script location: head or foot. The default is foot so you can omit the parameter if you want to include the script in the footer.

You can also reference resources by their names if they are defined in an enabled feature:
{% ScriptRequire ""jQueryUI"", head %}
{% StyleRequire ""jQueryUI_Orchard"" %}

Working with shapes
Displaying shapes from the viewmodel with the Display filter, e.g. from the Content shape:
<article>
{{ Model.Content | Display }}
</article>
Note that there are no quotes around Model.Content!

Displaying any shape with the display tag, here the User shape:
{% Display User %}

You can also give the shape input parameters as from C#:
{% Display User, Parameter1: ""some value"", Parameter2: ""some other value"" %}
These then can be uses from inside the User shape as Model.Parameter1 and Model.Parameter2 respectively.

CSS classes can be added to shapes much like how Model.Classes.Add(""pagination""); works in Razor:
{% AddClassToCurrentShape ""pagination"" %}

Changing global properties of the HTML document
Adds a <link> tag to the head of the document.
{% RegisterLink, Condition: ""gte IE 7"", Href: ""https://en.wikipedia.org/static/favicon/wikipedia.ico"", Rel: ""shortcut icon"", Title: ""favicon"", Type: ""image/x-icon"" %}
The same as the following in Razor: 
RegisterLink(new Orchard.UI.Resources.LinkEntry
	{
		Condition = ""gte IE 7"",
		Href = ""https://en.wikipedia.org/static/favicon/wikipedia.ico"",
		Rel = ""shortcut icon"",
		Title = ""favicon"",
		Type = ""image/x-icon""
	});

Adds a <meta> tag to the head of the document (or modifies an existing one).
{% SetMeta, Charset: ""utf-8"", Content: ""Wordpress"", HttpEquiv: ""X-Generator"", Name: ""generator"" %}
The same as the following in Razor:
SetMeta(new Orchard.UI.Resources.MetaEntry
	{
		Charset = ""utf-8"",
		Content = ""Wordpress"",
		HttpEquiv = ""X-Generator"",
		Name = ""generator""
	});

Sets the title of the current page. Equivalent to using Html.Title(""Title comes here""); in Razor.
{% PageTitle ""Title comes here"" %}

Set and output page classes like Html.ClassForPage(); would do in Razor:
{% ClassForPage, ""custom-class"" %}
Or multiple classes:
{% ClassForPage, ""custom-class1"", ""custom-class2"" %}
Can be also used to simply output the page classes:
{% ClassForPage %}

Sets page classes like Html.AddPageClassNames(); in Razor:
{% AddPageClassNames, ""custom-class"" %}
Or multiple classes:
{% AddPageClassNames, ""custom-class1"", ""custom-class2"" %}

Accessing the antiforgery token
Displays a hidden form field with the antiforgery token. Equivalent to using Html.AntiForgeryTokenOrchard(); in Razor.
{% AntiForgeryTokenOrchard %}
Displays the value of the antiforgery token. Equivalent to using Html.AntiForgeryTokenValueOrchard(); in Razor.
{% AntiForgeryTokenValueOrchard %}

Helpers
Converts an URL to an app-relative one, similar to Href() in Razor.
{% Href ""~/Admin"" %}
If the site root URL is example.com then this will produce ""/Admin"", if there is an app path like example.com/Orchard then this will produce ""/Orchard/Admin"". These would do exactly the same:
{% Href ""/Admin"" %}
{% Href ""Admin"" %}

So by utilizing the standard Liquid capture tag you can build dynamic URLs like following:
{% capture profileUrl %}~/Profile/{{ Model.WorkContext.CurrentUser.UserName }}{% endcapture %}
{% Href profileUrl %}
Or even with multiple parameters:
{% Href ""~/Profile"", Model.WorkContext.CurrentUser.UserName %}
Or observe how we utilize capture, the Href tag and the RegisterLink to register a favicon with a dynamic URL:
{% capture faviconUrl %}{% Href ""~/Themes/MyTheme/Images/favicon.ico"" %}{% endcapture %}
{% RegisterLink, Href: faviconUrl, Rel: ""shortcut icon"", Title: ""favicon"", Type: ""image/x-icon"" %}

Contribution notes
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/orchard-liquid-markup (Mercurial repository)
https://github.com/Lombiq/Orchard-Liquid-Markup (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",5
b3log/wide,Go,"Wide      
先试试我们搭建好的在线服务，你可以在这里下载并在本地环境运行，然后邀请小伙伴们来玩吧！
简介
Wide 是一个基于 Web 的 Go 语言 IDE。

动机
目前较为流行的 Go IDE 都有一些缺陷或遗憾：

文本编辑器类（vim/emacs/sublime/Atom 等）：对于新手门槛太高，搭建复杂
插件类（goclipse、IDEA 等）：需要原 IDE 支持，不够专业
LiteIDE 界面不够 modern、goland 收费
缺少网络分享、嵌入网站可运行功能

另外，Go IDE 很少，用 Go 本身开发的 IDE 更是没有，这是一个很好的尝试。关于产品定位的讨论请看这里。
特性
基于 Web 的 IDE：

只需要浏览器就能进行开发、运行
跨平台，甚至在移动设备上
易进行功能扩展
易与其他系统集成
极客体验

核心功能：

代码高亮、折叠：Go/HTML/JavaScript/Markdown 等
自动完成：Go/HTML 等
编译检查：编辑器提示编译错误
格式化：Go/HTML/JSON 等
运行：支持同时运行多个程序
代码导航：跳转到声明，查找使用，文件搜索等
Web 开发：前端（HTML/JS/CSS）开发支持
go tool：go get/install/fmt 等
项目文件导出
UI/编辑器多主题
支持交叉编译

界面
主界面

跳转到文件

自动完成

主题

查看表达式

构建报错提示

交叉编译

Playground

架构
构建与运行


一个浏览器 tab 对应一个 Wide 会话
通过 WebSocket 进行程序执行输出推送


客户端浏览器发送 Build 请求
服务器使用 os/exec 执行 go build 命令
2.1. 生成可执行文件
客户端浏览器发送 Run 请求
服务器使用 os/exec 执行文件
4.1. 生成进程
4.2. 运行结果输出到 WebSocket 通道
客户端浏览器监听 ws.onmessage 到消息后做展现

代码辅助


自动完成
查找使用


浏览器客户端发送代码辅助请求
Handler 根据请求对应的 HTTP 会话获取用户工作空间
执行 gocode/ide_stub(gotools) 命令
3.1 设置环境变量（${GOPATH} 为用户工作空间路径）
3.2 gocode 命令需要设置参数 lib-path

文档

用户指南
开发指南

社区

讨论区
报告问题

授权
Wide 使用 Apache License, Version 2 作为开源协议，请务必遵循该开源协议相关约定。
鸣谢

golang
CodeMirror
zTree
LiteIDE
gocode
Gorilla
Docker



",2761
simonsobs/ocs,Python,"OCS - Observatory Control System





Overview
The OCS makes it easy to coordinate hardware operation and I/O tasks in a
distributed system such as an astronomical observatory or test laboratory. OCS
relies on the use of a central WAMP router (currently crossbar.io) for
coordinating the communication and control of these distributed systems.
The OCS provides Python (and JavaScript) functions and classes to allow
""Clients"" to talk to ""Agents"". An Agent is a software program that knows how to
do something interesting and useful, such as acquire data from some device or
perform cleanup operations on a particular file system. A Control Client could
be a web page with control buttons and log windows, or a script written by a
user to perform a series of unattended, interlocking data acquisition tasks.
This repository, OCS, contains library code and core system
components.  Additional code for operating specific hardware can be
found in the Simons Observatory Control System (SOCS) repository.
The time-domain data query system, with grafana integration, is
provided through Sisock.

Dependencies
This code targets Python 3.5+.
There are also several Python package dependencies, which are listed in the
requirements.txt file.

Installation
Clone this repository and install using pip:
git clone https://github.com/simonsobs/ocs.git
cd ocs/
pip3 install -r requirements.txt
python3 setup.py install

Note: If you want to install locally, not globally, throw the --user flag
on both the pip3 and setup.py commands.
Warning: The master branch is not guaranteed to be stable, you might want
to checkout a particular version tag before installation depending on which
other software you are working with. See the latest tags.

Documentation
The OCS documentation can be built using sphinx once you have performed the
installation:
cd docs/
make html

You can then open docs/_build/html/index.html in your preferred web
browser.

Example
A self contained example, demonstrating the operation of a small observatory
with a single OCS Agent is contained in example/miniobs/.  See the readme
in that directory for details.

License
This project is licensed under the BSD 2-Clause License - see the
LICENSE.txt file for details.
",2
ingbrzy/Xiaomi.eu-MIUIv10-XML-Compare,Smali,"
MIUI 10 Daily XML changes by xiaomi.eu / miui.com

MIUI China is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License.
Strings are under a

Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License.

Based on a work at http://miui.com
For more info visit:

Multilang MIUI Support

Xiaomi.eu Translation Guide http://miui.connortumbleson.com/dev_guide
Credits:

ingbrzy miuios.cz
Acid miuipolska.pl

",29
JpressProjects/jpress,JavaScript,"
 JPress 

一个类似 WordPress 的产品，使用Java开发。








特点
模板

模板在线安装、卸载
模板在线启用、切换
在线编辑及实时生效
完善的模板开发文档
极致的模板开发体验

插件

插件在线安装、卸载
插件在线启用、停止
插件在线更新
支持在插件里添加新的 Controller
支持在插件里添加新的 Handler
支持在插件里添加新的 Interceptor
支持在插件里添加新的 Html、Css 和 Js
支持在插件里创建新的数据库表以及对应的 Model
支持在插件里链接不同的数据库
支持通过插件动态扩展后台菜单和用户中心菜单
插件扩展的菜单支持用户权限设置的管理
插件被停止：该插件的所有Controller、Handler、Intercepter 自动被移除
插件被卸载：该插件的所有资源全部被删除

用户

独立登录、注册入口
手机短信、邮箱激活功能
用户中心（投稿、文章管理、评论管理、个人资料管理等）
第三方登录：微信、QQ等
微信浏览时，通过微信授权自动获取用户信息

角色和权限

角色管理
全自动、免维护的权限字典（自动发现后台路由、插件安装卸载自动分配对应）
角色和权限的分配
用户多角色功能
超级管理员

微信

微信公众号对接
微信公众号关键字自动回复
微信公众号菜单设置
微信公众号运营插件
通过运用插件灵活扩展各种微信营销功能
微信小程序对接、和配置

SEO

每篇文章和页面独立的SEO设置
Baidu API 的实时推送
Baidu 和 Google 的自动 Ping 提交
Sitemap 自动生成、后台支持自定义的开启和关闭
robots.txt 爬虫蜘蛛的支持
整站伪静态支持，支持自定义开后缀

其他

WordPress、Hexo、Jekyll 文章一键导入
编写文章随意切换CKeditor和Markdown编辑
最大化、沉侵式的文章编写体验
Docker 一键部署
阿里云、腾讯云CDN在线配置
阿里云、腾讯云短信验证（用户注册手机验证）
附件自动可配置自动同步阿里云OSS
完善的API接口配置管理
... （更多等你发现）

交流

官网：http://www.jpress.io
论坛社区：点击这里
插件列表：点击这里
模板列表：点击这里
QQ群：591396171 ，288397536

帮助文档

了解JPress
快速开始
安装
使用
模板开发
二次开发
插件开发
微信运营插件开发
微信小程序开发
视频教程
常见问题
JPressSchool-VIP会员

微信交流群

",1909
gcallah/OnlineDevops,HTML,"

OnlineDevops
Online DevOps course at NYU Tandon School of Engineering.
",5
JustArchiNET/ArchiSteamFarm,C#,"ArchiSteamFarm























Description
ASF is a C# application with primary purpose of idling Steam cards from multiple accounts simultaneously. Unlike Idle Master which works only for one account at given time, while requiring Steam client running in the background and launching additional processes imitating ""game playing"" status, ASF doesn't require any Steam client running in the background, doesn't launch any additional processes and is made to handle unlimited Steam accounts at once. In addition to that, it's meant to be run on servers or other desktop-less machines, and features full cross-OS support, which makes it possible to launch on any operating system with .NET Core runtime, such as Windows, Linux and OS X. ASF is possible thanks to gigantic amount of work done in marvelous SteamKit2 library.
Today, ASF is one of the most versatile Steam power tools, allowing you to make use of many features that were implemented over time. Apart from idling Steam cards, which remains the primary focus, ASF includes bunch of features on its own, such as a possibility to use it as Steam authenticator or chat logger. In addition to that, ASF includes plugin system, thanks to which anybody can further extend it to his/her needs.

Core features

Automatic idling of available games with card drops using any number of active accounts
No requirement of running or even having official Steam client installed
Guarantee of being VAC-free, focus on security and privacy
Complex error-reporting mechanism, reliability even during Steam issues and other networking quirks
Flexible cards idling algorithm, pushing the performance to the maximum while still allowing a lot of customization
Offline idling, enabling you to skip in-game status and stop confusing your friends with fake playing status
Advanced support for Steam accounts, including ability to redeem keys, redeem gifts, accept trades, send messages and more
Support for latest Steam security features, including SteamGuard, SteamParental and 2-factor authentication
Unique ASF 2FA mechanism allowing ASF to act as a mobile authenticator, if needed
STM-like integration for trades, both passive (accepting) and active (sending), ASF can help you complete your sets
Special plugin system, which allows you to extend ASF in any way you wish through your own code
Powered by .NET Core, cross-OS compatibility, official support for Windows, Linux and OS X
...and many more!

For learning about even more ASF features, we recommend starting with our FAQ entry.

Setting up / Help
Detailed guide regarding setting up and using ASF is available on our wiki in setting up section. It's user-friendly guide that shows basic, as well as a bit more complex ASF setup, covering all the required dependencies and other steps that are required in order to start using ASF software.

Compatibility / Supported operating systems
ASF officially supports Windows, Linux and OS X operating systems, but it can work anywhere where you can obtain working .NET Core runtime. Please visit compatibility section on the wiki for more information regarding environments that ASF can work in.

Want to know more?
Our wiki includes a lot of other articles that will tell you about everything in regards to ASF, as well as show you other features that you can make use of. If you have some time to spare and you'd like to find out what else ASF can do for you, we heavily encourage you to take a look!
",3951
microsoft/computerscience,JavaScript,"Tech Resources for Computer Science and Academic Communities
The content and the code in this repo are intended for computer science instruction as a collaboration with Microsoft developer advocates and Faculty / Students under the MIT license. Please check back regularly for updated versions.
Source: https://github.com/microsoft/computerscience
This repo provides technical resources to help students and faculty learn about Azure and teach others. The content covers cross-platform scenarios in AI and machine learning, data science, web development, mobile app dev, internet of things, and DevOps. It also includes interesting tech talks and engaging, fun tech challenges that Microsoft leads at student hackathons and Imagine Cup.
Important: We are migrating to Microsoft Learn | If you can't find what you're looking for in this repo, check out the 60+ labs on Microsoft Learn too. Many of these labs have their own built-in Azure sandbox making it easier for faculty and students to learn without requiring an Azure Subscription.
Students can get free Azure credits to explore these resources here:

Azure for Students | $100 in Azure for 12 months with free tier of services - no credit card required with academic verification
Azure for Students Starter | use select Azure products like App Services for free - no credit card required with academic verification
Azure Free Account | $200 in Azure for one month with free tier of services - requires a credit card and probably the best fit for faculty evaluating Azure for course instruction unless your organization has a grant or enterprise agreement.

Your feedback is appreciated - please fork this repo and contribute!
To report any issues, please log a GitHub issue. Include the content section, module number, and title, along with any error messages and screenshots.
Learn by doing with our hands-on labs
Check out our hands-on labs that can be used on your own or in the classroom. They also make for fun, easy-to-run workshops!



Lab Categories





AI and Machine Learning
Build bots and apps backed by AI and ML using Azure and Azure Cognitive Services.


Azure Services
Deploy serverless code with Azure Functions, run Docker containers, use Azure to build Blockchain networks and more.


Big Data and Analytics
Spin up Apache Spark Clusters, Use Hadoop to extract information from big datasets or use Power BI to explore and visualize data.


Deep Learning
These labs build on each other to introduce tools and libraries for AI. They're labeled 200-400 level to indicate level of technical detail.


Internet-of-Things
Use Azure to collect and stream IoT data securely and in real time.


Web Development
Quickly create scalable web apps using Node, PHP, MySQL on easy-to-use tools like Visual Studio Code and GitHub.



Host great events and hacks
Want to host an event at your school? We can help with the resources below!



Resource





Events and Hacks
These are keynotes and hack workshops that Microsoft has produced for student events. Feel free to use. Most slides also contain suggested demos and talk tracks. There's also pre-packaged coding challenge to help students explore machine learning.


Tech Talks
One-off presentations on emerging or innovative tech topics with speakers notes and demos.



Other available academic resources
We also have other great educator content to help you use Azure in the classroom.



Resource





Scripts
Scripts and templates built in PowerShell or BASH to help set up your classroom environment.


Azure Guides
Discover what Azure technologies apply to different teaching areas.


Course Content
Learning modules to complement existing course instruction. Includes presentations, speaker notes, and hands-on labs.



Content from other sources



Resource





Microsoft AI School
Content for students, developers and data scientists to get started and dive deep into the Microsoft AI platform and deep learning.


Microsoft Virtual Academy 
Hundrends of free online training by world-class experts to help you build your technical skills on the latest Microsoft technologies.


Technical Community Content
Workshops from the community team.


Cognitive Toolkit
Resources to set up Cognitive Toolkit using notebooks from product team. After installing tutorials will be in c:\repos\bindings\python\tutorials.


Research case studies
Case studies of faculty using Azure for Research collected by Microsoft Research. Submit your own Azure research stories here too!


Microsoft Research Data Sets
Data sets shared by Microsoft Research for academic use.


Machine Learning Data Sets
Data sets shared by Azure Machine Learning team to help explore machine learning.


MS MARCO
Microsoft MAchine Reading COmprehension Dataset generated from real Bing user queries and search results.


IOT School
Resources for learning about Azure IoT solutions, platform services and industry-leading edge technologies.


AI Labs
Experience, learn and code the latest breakthrough AI innovations by Microsoft.


Channel9
Videos for developers from people building Microsoft products and services.



",1365
microsoft/computerscience,JavaScript,"Tech Resources for Computer Science and Academic Communities
The content and the code in this repo are intended for computer science instruction as a collaboration with Microsoft developer advocates and Faculty / Students under the MIT license. Please check back regularly for updated versions.
Source: https://github.com/microsoft/computerscience
This repo provides technical resources to help students and faculty learn about Azure and teach others. The content covers cross-platform scenarios in AI and machine learning, data science, web development, mobile app dev, internet of things, and DevOps. It also includes interesting tech talks and engaging, fun tech challenges that Microsoft leads at student hackathons and Imagine Cup.
Important: We are migrating to Microsoft Learn | If you can't find what you're looking for in this repo, check out the 60+ labs on Microsoft Learn too. Many of these labs have their own built-in Azure sandbox making it easier for faculty and students to learn without requiring an Azure Subscription.
Students can get free Azure credits to explore these resources here:

Azure for Students | $100 in Azure for 12 months with free tier of services - no credit card required with academic verification
Azure for Students Starter | use select Azure products like App Services for free - no credit card required with academic verification
Azure Free Account | $200 in Azure for one month with free tier of services - requires a credit card and probably the best fit for faculty evaluating Azure for course instruction unless your organization has a grant or enterprise agreement.

Your feedback is appreciated - please fork this repo and contribute!
To report any issues, please log a GitHub issue. Include the content section, module number, and title, along with any error messages and screenshots.
Learn by doing with our hands-on labs
Check out our hands-on labs that can be used on your own or in the classroom. They also make for fun, easy-to-run workshops!



Lab Categories





AI and Machine Learning
Build bots and apps backed by AI and ML using Azure and Azure Cognitive Services.


Azure Services
Deploy serverless code with Azure Functions, run Docker containers, use Azure to build Blockchain networks and more.


Big Data and Analytics
Spin up Apache Spark Clusters, Use Hadoop to extract information from big datasets or use Power BI to explore and visualize data.


Deep Learning
These labs build on each other to introduce tools and libraries for AI. They're labeled 200-400 level to indicate level of technical detail.


Internet-of-Things
Use Azure to collect and stream IoT data securely and in real time.


Web Development
Quickly create scalable web apps using Node, PHP, MySQL on easy-to-use tools like Visual Studio Code and GitHub.



Host great events and hacks
Want to host an event at your school? We can help with the resources below!



Resource





Events and Hacks
These are keynotes and hack workshops that Microsoft has produced for student events. Feel free to use. Most slides also contain suggested demos and talk tracks. There's also pre-packaged coding challenge to help students explore machine learning.


Tech Talks
One-off presentations on emerging or innovative tech topics with speakers notes and demos.



Other available academic resources
We also have other great educator content to help you use Azure in the classroom.



Resource





Scripts
Scripts and templates built in PowerShell or BASH to help set up your classroom environment.


Azure Guides
Discover what Azure technologies apply to different teaching areas.


Course Content
Learning modules to complement existing course instruction. Includes presentations, speaker notes, and hands-on labs.



Content from other sources



Resource





Microsoft AI School
Content for students, developers and data scientists to get started and dive deep into the Microsoft AI platform and deep learning.


Microsoft Virtual Academy 
Hundrends of free online training by world-class experts to help you build your technical skills on the latest Microsoft technologies.


Technical Community Content
Workshops from the community team.


Cognitive Toolkit
Resources to set up Cognitive Toolkit using notebooks from product team. After installing tutorials will be in c:\repos\bindings\python\tutorials.


Research case studies
Case studies of faculty using Azure for Research collected by Microsoft Research. Submit your own Azure research stories here too!


Microsoft Research Data Sets
Data sets shared by Microsoft Research for academic use.


Machine Learning Data Sets
Data sets shared by Azure Machine Learning team to help explore machine learning.


MS MARCO
Microsoft MAchine Reading COmprehension Dataset generated from real Bing user queries and search results.


IOT School
Resources for learning about Azure IoT solutions, platform services and industry-leading edge technologies.


AI Labs
Experience, learn and code the latest breakthrough AI innovations by Microsoft.


Channel9
Videos for developers from people building Microsoft products and services.



",1365
ccrama/Slide-iOS,Swift,"Slide for Reddit   

Slide is an open-source, ad-free Reddit browser written in Swift 4 for iOS (now available on the App Store). Feel free to join us on the official subreddit for discussion or requests!




Contributing
Getting started
To get started with Slide iOS development, you need to set up Cocoapods integration and open the Coacoapods workspace, not the default xcworkspace. NOTE: You must open the .xcworkspace file instead of the .xcodeproj file for dependencies to load. If you are having issues with Pods or are setting up the Slide repository for the first time, try the steps below.
Below are the steps to getting started:

Clone this repo and open the Terminal
In Terminal, run pod install
Open ""Slide for Reddit.xcworkspace"" through Finder
Go to the project build settings and change USR_DOMAIN to something else

If you are having trouble building on XCode 10 or MacOS Mojave
Try running chmod 666 Pods/Realm/include/RLMPlatform.h from Terminal in the project root directory.
If you don't have a paid Apple Developer account and you get warnings about iCloud entitlements
Select ""none"" for your team, go to the ""Capabilities"" section of the project build settings, and disable iCloud and IAP support. Then, add yourself back as the team and build!
If you still run into problems, feel free to shoot me a message on Reddit or Discord (above).
What needs to be done
Any issues are fair game, but any issue with the ""Help Wanted"" or ""Enhancement"" tags are issues that we would particulary love help with. If you have any questions or want to be pointed in the right direction, feel free to send me a PM on Reddit to /u/ccrama, or join us on Discord!
Issues
In any project, it's likely that a few bugs will slip through the cracks, so it helps greatly if people document any bugs they find to ensure that they get fixed promptly.
You can view a list of known issues and feature requests using the issue tracker. If you don't see your issue (or you aren't sure) feel free to submit it!
Where appropriate, a screenshot works wonders to help us see exactly what the issue is. You can upload screenshots directly using the GitHub issue tracker or by attaching a link (to Imgur, for example); whichever is easier for you.
Code
If you are a developer and wish to contribute to the app please fork the project and submit a pull request.
If you have any questions, feel free to message me on Discord or drop me a message on Reddit.
Changes
For a detailed look at changes to the app you can view individual commits.
Licensing
Slide is licensed under the Apache 2 License.
If you find Slide's code useful or you use code from this repository, feel free to let me know!
",243
kube-js/kube-ts-server,TypeScript,"kube-ts-server




Dockerized restful API powered by express/typescript ready to be deployed on kubernetes cluster
Demo
https://demo.mariuszrajczakowski.me/api/v1
Docs
https://kubetsserver.docs.apiary.io
Getting started

clone the repo

git clone git@github.com:kube-js/kube-ts-server.git

install all dependencies

npm install

migrates and seeds a project

npm run setup

build the project

npm run build

run the server (for production, after build)

npm run start

run the server (for dev purposes, has a watch mode)

npm run dev

run tests

npm run test
or
npm run test:watch
Currently includes the following models:

users
userRole
roles
rolePermission
permissions
resetPasswordTokens
courses

This repo would not exist if not inspiration coming from:

ryansmith94:

js-entity-repos
js-migrations
rulr


vinaysahni.com:

best-practices-for-a-pragmatic-restful-api
http://dev.enchant.com



Credits:

banzaicloud.com

",5
mdiller/twitchclipmatchfinder,Python,"TwitchClipMatchFinder
This tool takes a twitch clip of someone playing dota and finds the match (match id) of dota that they were playing so you can get a dotabuff/opendota link or retrieve stats for it. I've created a reddit bot that automatically finds clips posted and gets matches for them: /u/DotaClipMatchFinder (The name is slightly different because TwitchClipMatchFinder was too many characters)
How it works

Get some info about the clip via the twitch api
Download the mp4 of the clip
Extract the first frame of the mp4 (using OpenCV)
Find the heroes in the top bar of the image

First, crop the image to only look at the top row where the heroes should be
Load all the hero images (plus arcana images)
Check if the hero is in the given image and if so, where (using OpenCV)
Filter out heroes that are in the wrong place in the image
Sort the results to get the 10 heroes that are most likely in this image in the right order
Do a bunch of other things to make sure we got the right heroes in the right places, because computers are bad at pattern detection


Use this opendota api endpoint to search for matches containing exactly this hero matchup
Pick the correct match out of the results based on which one has a start time closest to the time that the clip was taken

Libraries used

Twitch API
Opendota API
Python PIL / Pillow
OpenCV

Usage
This library was built with the idea of using it as a reddit bot, which is its main use, but it can also be used separately to find specific clips. The find_match function in finder.py can be called to find the match for a given clip slug (the 'slug' is the bit at the end of the twitch clip url that is a bunch of words together.) The finder.py file can also be started directly if you give the clip slug as the argument. Here is an example for https://clips.twitch.tv/CoyCleverOilLitFam:
> python finder.py CoyCleverOilLitFam
finding for CoyCleverOilLitFam
matched for the following heroes:
          Grimstroke:    (16, 8) {0   1.56} [0.9345031380653381]
    Templar Assassin:    (79, 8) {1   1.56} [0.9476589560508728]
         Beastmaster:   (142, 8) {2   1.56} [0.9453526139259338]
         Wraith King:   (205, 8) {3   1.56} [0.9641481637954712]
       Shadow Shaman:   (268, 8) {4   1.56} [0.9190652370452881]
                Ursa:   (539, 8) {5   0.77} [0.9223578572273254]
           Alchemist:   (602, 8) {6   1.11} [0.911087691783905]
            Silencer:   (665, 8) {7   1.11} [0.945387601852417]
                Doom:   (728, 8) {8   0.44} [0.9171727299690247]
        Earth Spirit:   (791, 8) {9   0.77} [0.9245378375053406]
found match 4757318890, started 37 minutes before the clip was taken.
https://www.opendota.com/matches/4757318890

",23
gnembon/carpetmod,Java,"CarpetMod for Minecraft 1.13.2
The most comprehensive and convoluted mod for carpets evar. Built based on jarmod-buildsystem-2 by Earthcomputer using Forge Gradle system by Minecraft Forge team. See Earthcomputer's repo for details on the build system.
Gimme, gimme, I just wanna play

Then go get the install package (zip folder) for your system from https://github.com/gnembon/carpetmod/releases , and follow the README that is inside of the zip package. Each installer contains patches for both server and singleplayer versions, and (the best case scenario) should require you to just doubleclick (running) on one patching script.

I installed it. Need help

Run /carpet list to see all the togglable options with description
Click on a feature in the list in chat, or type /carpet <featureName> to get detailed help about each feature.

tl;dr (to help us develop carpet mod)

Have Java SDK and git installed (prefferably, or download zipped repo to a folder)
open command prompt
type:


git clone https://github.com/gnembon/carpetmod.git




cd carpetmod




gradlew setup




gradlew genPatches




gradlew createRelease


patches for client and server are in /build/distributions
optionally (to patch the server automatically)


have commandline 7za installed




patch_server.cmd




your server should be running in your saves folder already. Connect to localhost


optionally: create installers (depending on the host OS and target OS)


cd installer




create_installer_<win|mac|ux>.<cmd|sh>



Requirements

You need to have at least JDK8 update 92 for recompilation to work, due to a bug in earlier versions of javac. You also cannot use JDK9 or JDK10 yet.
You need to have git installed.
Eclipse Oxygen.3 or later, due to this Eclipse bug.
Or Intellij

OR

Download the patches from the releases section and apply them directly to game's or server's jars.

First-time setup

Copy all the files in this repository into your new project folder.
Run gradlew setup to decompile, deobfuscate the code and apply carpet patches.
Run gradlew eclipse to setup the appropriate Eclipse projects. Do this even if you are planning on using Intellij IDEA.
If you use Eclipse, open Eclipse, and navigate to File -> Import -> General -> Existing Projects into Workspace. Navigate to and select the projects subdirectory, and check your mod project, and optionally the clean (unmodified) project too.
Otherwise, open Intellij IDEA and import the Eclipse project. From 3 available projects choose one in projects/carpetmod

Project layout + management
Once you have setup the project, you should see a file structure which looks something like this:
- src/main/java This is where all of the MINECRAFT classes go, i.e. classes which you may or may not have modified, but no classes you have added.
- src/main/resources Similar to src/main/java except for non-java files.
- main-java This is where all of the MOD classes go, i.e. the classes which you have added.
- main-resources Similar to main-java except for non-java files.

From outside Eclipse, the file structure looks a little different. However, you should avoid editing these files from outside your IDE of choice:
- src/main/java The MOD classes
- src/main/resources The MOD resources
- patches Patches your mod has made to the MINECRAFT classes, which can be pushed to public repositories
- projects/<modname>/src/main/java * The MINECRAFT classes
- projects/<modname>/src/main/resources * The MINECRAFT resources
- projects/clean/src/main/java * The unmodified MINECRAFT classes
- projects/clean/src/main/resources * The unmodified MINECRAFT resources
* = ignored by git


You should be able to run Minecraft directly from within the IDE.
Every time you checkout a branch which has changed files in the patches directory, you need to run gradlew setup again to update the code in src/main/java inside the IDE. This will not try to decompile again like it did the first time, so won't take long. This will overwrite your local changes.
Every time you make changes to MINECRAFT classes and want to push to the public repo, you need to run gradlew genPatches to update the patch files in the patches directory. This takes a few seconds.
When you are ready to create a release, run gradlew createRelease. This may take longer than the other tasks because it is recompiling the code. Once it is done, your releases can be found in the build/distributions directory. It includes patches for server jar as well as standalone client jar

Settings you can change
Accessible in conf/settings.json. Beware that changes to this may significanly modify the carpetmod patches so only do it if you know what your are doing or you want to keep your own fork

modname the name of your mod.
modversion the version your mod is on.
mcpconfig the MCPConfig version you are using.
mappings the MCP mappings you are using.
mcversion the Minecraft version.
pipeline, either joined, client or server - whether your mod is to be a client-side-only or server-side-only mod, or to be both and share the same codebase.
clientmain the main class on the client.
servermain the main class on the server.
reformat whether to run Artistic Style on the code to reformat it. Makes the build process a little slower but does mean you can change the formatting options with conf/astyle.cfg.
customsrg The custom tsrg file inside the conf/ folder, to override the one in the MCPConfig distribution, used to deobfuscate even newer Minecraft versions.

A word of warning
1.13 modding is still in its infancy, and there are already known bugs that occur in the decompiled code which do not occur in vanilla. If you care about maintaining vanilla behaviour, then whenever making a change which may modify a certain vanilla class, make sure to weigh up the benefit of modifying said class against the risk that there might be a decompile bug in the class. This situation is constantly improving as 1.13 modding matures, but for now you can at least minimize the effect by distributing as few modified classes as possible.
",80
sbhs-racepace/racepace-server,Python,"RacePace
Software Design and Development Major Project
 

Welcome
RacePace is an application that generates the best running/cycling route for the user based on a variety of factors. A route can be specified by run type, elevation, greenery and other factors such as user ratings. Coaches can use this app to track and analyse multiple participants in realtime to help runners perform at a higher level.
Project Structure
Backend:

Our internal API written in python uses Sanic as an asynchronous web server.
Clients will make requests to our API which generates a route based on given parameters using data from the Overpass API.
Realtime tracking in the future can be made possible through the use of websockets.

Client:

An android application using the Ionic framework
Uses web technologies

Installing the server
You must have python 3.7+ and pipenv. Clone the repository and run the following command.
pipenv install --dev

To run the server navigate into the server folder and run app.py.
python3 app.py

Make sure to copy the .env.example file and remove the .example suffix so you are left with .env, update the key value pairs accordingly.
",2
kjosib/booze-tools,Python,"What is this?
For now there are three major components. Eventually there will be more. These are:

MiniParse -- Provides LALR(1) and operator-precedence grammar facilities (like Lemon, YACC, or Bison).
MiniScan -- Provides a DFA-based backtracking scanner (like Flex or Lex) with a few extra goodies.
MacroParse -- This is the crown jewel of the package right now. It:

provides for a separate document containing the definitions of a scanner and parser.
uses markdown format to make just such a document into a literate program.
enables a single such definition to be used for different applications on different host languages.
supports a macro language for simplifying otherwise-redundant parser specifications.
provides a suitable runtime library so the examples
run and pass the tests.
still needs more example applications to exercise the remaining features -- which are coming soonish.



Full documentation is at the wiki page.
Worked examples may be found at /example/.
Priorities?

These operate within a Python environment.
They have some features not found in other such tools.
The code is deliberately kept simple, small and well-factored:

Easy to extend, approachable, and informative.
Aiming for suitability in an instructional context.


These modules do not generate code:

They plug directly into your application and work right away.
This makes them excellent for rapid prototyping.
Pickled automatons will be part of the next major feature (MacroParse).
Code generation for other languages is in the foreseeable future.


Performance is accordingly NOT a top priority, but:

if someone wants to play with the profiler they are welcome, and
contributions in that vein will be accepted as long as they are consistent with the higher priorities.



What Else?
There is a complete worked example JSON scanner/parser in the example folder. It has a lot of commentary to walk you through setting up both scanner and parser.
There are unit tests. They're not vast and imposing, but they exercise the interface both directly and via the example code.
There is a wiki linked above. It has background and more detail about what this is and how to use it.
",4
govau/notify,Python,"Notify
Notify is a service developed by the DTA that helps
government agencies communicate with their users.
This is the open source repository that encompasses all projects that make up
Notify.
Running the things
We can orchestrate all of these projects by using a set of common Makefile
conventions.
The targets that every sub-project should implement are:

install
install-dev
build
test
clean
deploy
deploy-dev
check-vulnerabilities

Issuing any of these commands at the root level will forward it on to all
sub-projects. For example, you can set up and build every project by running
make install
make build

You can run all the tests of all projects by running
make install-dev
make test

You can also work with a specific project from the root level by prefixing the
command with the project name, and a full stop separator.
For instance, running the API in dev mode is done with
make api.run

Alternatively, you can change into the api directory and issue the make run command directly.
To start the whole app from nothing, you'll need to run the following in
different shells.
make api.run-celery-worker
make admin.run

... then hit up http://localhost:6012 to get going.
Optionally you can also run this target to run the scheduled tasks:
make api.run-celery-beat

Requirements
Different projects have different needs. Consult them individually to be sure.
Generally though, you'll need Python and pipenv for the frontend and backend.
You'll need Node.js for the frontend and docs,
And we've got a simple open source status page that's written in Ruby, but
we don't usually run it locally.
Credits
Notify is based on GOV.UK Notify.
Thanks GDS team for providing Notify as an open source project that made it
possible for us to build on a stable and reliable foundation.
",2
Lombiq/Pretty-Good-Bootstrap-Base-Theme,HTML,"Pretty Good Bootstrap Base Theme Orchard Theme Readme
Project Description
An Orchard base theme building on Twitter's Bootstrap framework.
We've also created a complete sample theme demonstrating what you can do with PGBBT.
PGBBT is the base for the themes of all Lombiq websites, including Lombiq.com, Orchard Dojo and DotNest.com.
The theme is also available for DotNest sites.
Documentation

What's the Great Purpose?
Structure
How to...
Updating Bootstrap

PGBBT currently includes the Bootstrap 3.3.4.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/pretty-good-bootstrap-base-theme (Mercurial repository)
https://github.com/Lombiq/Pretty-Good-Bootstrap-Base-Theme (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",6
jaredledvina/sensu-go-ansible,Python,"sensu-go-ansible



This role allows for the deployment and management of
Sensu Go.
If you'd like to contribute, please review CONTRIBUTING.md and open an issue to discuss your
idea.
Requirements

Ansible 2.7

Role Variables
See defaults/main.yml
for everything that's configurable. If any of the options are unclear, please
file an issue!
Please note that unless you've configured hash_behaviour to merge
configuring any of the hash variables will override the entire default variable.
Most variables expose an _overrides: {} variable that is merged automatically
in this role for selectively updating each variable. It's strongly recommended
that the _overrides variable be used.
Dependencies
None
Example Playbook
The following example will configure the host in the hostgroup
sensu-backend-server to be configured with both sensu-backend and
sensu-agent. This host will also get the sensuctl CLI tool for further
management of Sensu Go.
The hosts in sensu-agent-severs will only get the sensu-agent install and
will have the sensu-agent's configuration option for backend-url
overriden to ws://sensu-backend-server:8081.
For more information on the availible configuration options, checkout the upstream docs for
sensu-backend and
sensu-agent.
---
-
  hosts: sensu-backend-server
  become: yes
  roles:
    - role: jaredledvina.sensu_go_ansible
-
  hosts: sensu-agent-severs
  roles:
    - role: jaredledvina.sensu_go_ansible
      sensu_go_components:
        - agent
      sensu_go_configs_override:
        agent:
          config:
            backend-url:
              - ws://sensu-backend-server:8081
Testing
This Ansible role is automatically tested via TravisCI on every commit. We
specifically test using the version of Ansible and python declared in the
Pipefile
The following Operating Systems are automatically tested:

Amazon Linux
Amazon Linux 2
CentOS - 6
CentOS - 7
Debian - 8 (Jessie)
Debian - 9 (Stretch)
Fedora - 26
Fedora - 27
Fedora - 28
Fedora - 29
Ubuntu - 14.04 (Trusty Tahr)
Ubuntu - 16.04 (Xenial Xerus)
Ubuntu - 18.04 (Bionic Beaver)

Caveats
If you are using this role with Amazon Linux or Amazon Linux 2, you must
override the following variables on those host(s):
Amazon Linux:
sensu_go_repos_overrides:
  yum:
    rpm: https://packagecloud.io/sensu/stable/el/6/x86_64
    rpm-src: https://packagecloud.io/sensu/stable/el/6/SRPMS
sensu_go_community_repos_overrides:
  yum:
    rpm: https://packagecloud.io/sensu/community/el/6/x86_64
    rpm-src: https://packagecloud.io/sensu/community/el/6/SRPMS
Amazon Linux 2:
sensu_go_repos_overrides:
  yum:
    rpm: https://packagecloud.io/sensu/stable/el/7/x86_64
    rpm-src: https://packagecloud.io/sensu/stable/el/7/SRPMS
sensu_go_community_repos_overrides:
  yum:
    rpm: https://packagecloud.io/sensu/community/el/7/x86_64
    rpm-src: https://packagecloud.io/sensu/community/el/7/SRPMS
If you are using this role with Debian 8 or 9 hosts, you must overide the
following variable:
sensu_go_manage_community_repo: false
This is due to Debian packages not being updated to the community repos
pending the resolution of https://github.com/sensu/sensu-plugins-omnibus/issues/3
License
MIT
",15
jaredledvina/sensu-go-ansible,Python,"sensu-go-ansible



This role allows for the deployment and management of
Sensu Go.
If you'd like to contribute, please review CONTRIBUTING.md and open an issue to discuss your
idea.
Requirements

Ansible 2.7

Role Variables
See defaults/main.yml
for everything that's configurable. If any of the options are unclear, please
file an issue!
Please note that unless you've configured hash_behaviour to merge
configuring any of the hash variables will override the entire default variable.
Most variables expose an _overrides: {} variable that is merged automatically
in this role for selectively updating each variable. It's strongly recommended
that the _overrides variable be used.
Dependencies
None
Example Playbook
The following example will configure the host in the hostgroup
sensu-backend-server to be configured with both sensu-backend and
sensu-agent. This host will also get the sensuctl CLI tool for further
management of Sensu Go.
The hosts in sensu-agent-severs will only get the sensu-agent install and
will have the sensu-agent's configuration option for backend-url
overriden to ws://sensu-backend-server:8081.
For more information on the availible configuration options, checkout the upstream docs for
sensu-backend and
sensu-agent.
---
-
  hosts: sensu-backend-server
  become: yes
  roles:
    - role: jaredledvina.sensu_go_ansible
-
  hosts: sensu-agent-severs
  roles:
    - role: jaredledvina.sensu_go_ansible
      sensu_go_components:
        - agent
      sensu_go_configs_override:
        agent:
          config:
            backend-url:
              - ws://sensu-backend-server:8081
Testing
This Ansible role is automatically tested via TravisCI on every commit. We
specifically test using the version of Ansible and python declared in the
Pipefile
The following Operating Systems are automatically tested:

Amazon Linux
Amazon Linux 2
CentOS - 6
CentOS - 7
Debian - 8 (Jessie)
Debian - 9 (Stretch)
Fedora - 26
Fedora - 27
Fedora - 28
Fedora - 29
Ubuntu - 14.04 (Trusty Tahr)
Ubuntu - 16.04 (Xenial Xerus)
Ubuntu - 18.04 (Bionic Beaver)

Caveats
If you are using this role with Amazon Linux or Amazon Linux 2, you must
override the following variables on those host(s):
Amazon Linux:
sensu_go_repos_overrides:
  yum:
    rpm: https://packagecloud.io/sensu/stable/el/6/x86_64
    rpm-src: https://packagecloud.io/sensu/stable/el/6/SRPMS
sensu_go_community_repos_overrides:
  yum:
    rpm: https://packagecloud.io/sensu/community/el/6/x86_64
    rpm-src: https://packagecloud.io/sensu/community/el/6/SRPMS
Amazon Linux 2:
sensu_go_repos_overrides:
  yum:
    rpm: https://packagecloud.io/sensu/stable/el/7/x86_64
    rpm-src: https://packagecloud.io/sensu/stable/el/7/SRPMS
sensu_go_community_repos_overrides:
  yum:
    rpm: https://packagecloud.io/sensu/community/el/7/x86_64
    rpm-src: https://packagecloud.io/sensu/community/el/7/SRPMS
If you are using this role with Debian 8 or 9 hosts, you must overide the
following variable:
sensu_go_manage_community_repo: false
This is due to Debian packages not being updated to the community repos
pending the resolution of https://github.com/sensu/sensu-plugins-omnibus/issues/3
License
MIT
",15
MicrosoftDocs/azure-docs.tr-tr,PowerShell,"


ms.openlocfilehash
ms.sourcegitcommit
ms.translationtype
ms.contentlocale
ms.lasthandoff
ms.locfileid




d4e9e26f2c6764cc179826266891d861514e23f6
3102f886aa962842303c8753fe8fa5324a52834a
HT
tr-TR
04/23/2019
60344098



Microsoft Azure Belgeleri
Microsoft Azure’ın açık kaynak belgelerine hoş geldiniz. Microsoft Azure belgelerine nasıl katkıda bulunabileceğinizi anlamak için lütfen bu README dosyasını inceleyin.
Başlarken
Açık kaynak belgelerine katkıda bulunmak için yalnızca güncelleştirmeler sağlamanız değil, aynı zamanda bir sorun olduğunda bunu bize bildirmeniz de gerekir. Daha fazla bilgi için Katkıda bulunma rehberimizi okuyun.
Ön koşullar
Katkıda bulunmaya karar vermeniz çok güzel! Belgelere katkıda bulunmak için birkaç araca ihtiyacınız olacak.
Belgelere katkıda bulunmak için bir GitHub hesabı gerekir. Bir hesabınız yoksa katkıda bulunan kılavuzumuzdaki GitHub hesap kurulumu yönergelerini izleyin.
İndirme
Aşağıdaki araçları yükleyin:

Git
Visual Studio Code
Visual Studio Code için Docs Yazma Paketi uzantısı

Yükleme
Katkıda bulunan kılavuzumuzun İçerik yazma araçlarını yükleme bölümünde sağlanan yönergeleri izleyin.
Lisans
Lisanslama hakkındaki tüm bilgiler için lütfen LICENSE, LICENSE-CODE ve ThirdPartyNotices bölümlerine bakın.
Kullanım Kuralları
Bu proje Microsoft Open Source Code of Conduct (Microsoft Açık Kaynak Kullanım Kuralları) belgesinde listelenen kurallara uygundur.
Daha fazla bilgi için Code of Conduct FAQ (Kullanım Kuralları Hakkında SSS) konusuna bakın veya sorularınızı ya da görüşlerinizi bildirmek için opencode@microsoft.com adresinden bize ulaşın.
",6
woocommerce/woocommerce-ios,Swift,"

woocommerce-ios
A Jetpack-powered companion app for WooCommerce.
Build Instructions
Download Xcode
At the moment WooCommerce for iOS uses Swift 4.2 and requires Xcode 10.2 or newer. Previous versions of Xcode can be downloaded from Apple.*
Third party tools
We use a few tools to help with development. To install or update the required dependencies, run the follow command on the command line:
bundle exec pod install
you may also have to:
bundle install
CocoaPods
The woocommerce-ios project uses CocoaPods to manage third party libraries.
Third party libraries and resources managed by CocoaPods will be installed by the bundle exec pod install command above.
Peril
The woocommerce-ios project uses Peril to enforce Pull Request guidelines.
",25
experiment322/controlloid-client,JavaScript,"
controlloid-client
Controlloid is a small open source application that turns your phone into a
real game controller for your PC. It works on Linux and Windows and can
connect through anything that allows the creation of a LAN network: WiFi,
Bluetooth, USB etc. It works over internet too, but for optimal performance
it is recommended to create a hotspot on your PC and disable other network
connections (such as wired) which allows the connected devices to access
the internet.
The server application can be downloaded from the following link.
controlloid-server
Features

All the buttons of a PS2 controller (except L3 and R3)
Supports multiple clients (limited to 16 on Windows)
Analog stick dead zone (set from Preferences)
Scan and connect instantly to LAN servers
Move, resize and overlap buttons in layout editor
Creation of multiple layouts to accommodate different games
More to come! (suggestions welcome on GitHub)

",13
anticensority/generated-pac-scripts,JavaScript,"Here we store PAC-scripts generated by servers.
This repo always contains only one commit that is overriden (like git push --force).
README file survives commit overrides.
PAC-scripts
Each PAC-script is given a name to distinguish it from our other PAC-scripts.
Currently we support the following scripts:



Name
Filename
Homepage
Author




Anticensority
anticensority.pac
Here
ilyaigpetrov


Antizapret
proxy.pac
There
ValdikSS



",7
kubesphere/docs.kubesphere.io,JavaScript,"Documents for KubeSphere.

Develop
Setting up with git
If you choose this way, we recommend you to install some requisites

git
node.js
yarn (or npm, we recommend yarn)

Check your requisites:
git --version
node -v
yarn -v
Then you are ready to go:
git clone https://github.com/kubesphere/docs.kubesphere.io.git

cd docs.kubesphere.io

yarn

yarn develop
Contribute
Tree of repo:
├── content                                         // documents directory
│   ├── express                                     // documents version
│   │   ├── en                                      // documents language 
│   │   │   └── KubeSphere-Installer-Guide.md       // document
│   │   └── zh-CN
│   │       └── KubeSphere-Installer-Guide.md
│   ├── toc_express_en.json                         // table of contents, define the page navigation
│   └── toc_express_zh-CN.json
├── src
└── static                                          // put document images here
    └── daemonset_create_1.png
If you want to edit the document, you can follow the ways below:
Add new version

Create a new directory called the new version name under the content directory

cd content && mkdir version-xxxx

Create subdirectories for each language you want to support

cd content/version-xxxx && mkdir zh-CN en

Create navigation files for each language of the new version under the content directory

cd content

touch toc_version-xxx_en.json toc_version-xxx_zh-CN.json
Edit navigation
navigation file example

id: should match the format of {version}-{language}
chapters: nav items
title: nav title
entry: nav entry, path to the document to display
entries: sub navs

{
  ""id"": ""express-zh-CN"",
  ""chapters"": [
    {
      ""title"": ""简介"",
      ""entry"": ""./express/zh-CN/basic.md""
    },
    {
      ""title"": ""应用负载"",
      ""entries"": [
        {
          ""entry"": ""./express/zh-CN/manage-deployments.md""
        },
        {
          ""entry"": ""./express/zh-CN/manage-statefulsets.md""
        },
        {
          ""entry"": ""./express/zh-CN/manage-daemonsets.md""
        }
      ],
      ""chapters"": [
      ]
    }
  ]
}
Edit Document
document example
---
  title: 'document title, will show in nav'
---

  ## title 1

  content 1

  ![](/image.png) 

  this path will request ``/static/image.png``

  ## title 2

  ### subtitle 2.1
    content 2.1
  
  ### title 3

## will be transformed to an anchor of the page, and will show in the nav.
",9
myrrlyn/bitvec,Rust,"BitVec – Managing memory bit by bit







Summary
This crate provides data structures which allow working with bool as if it
were truly one bit wide in memory, rather than a u8 with only two valid
values. Currently, it only provides [u1], Box<[u1]>, and Vec<u1>
structures.
In addition to compact memory representation, this crate also allows you to
specify the order in which individual bits are stored in Rust fundamentals, and
which fundamental element (u8, u16, u32, and on 64-bit systems, u64) is
used to store the bits.
The data structures provided by this crate track as closely as possible the APIs
and trait implementations of their proper types in the Rust standard library.
BitSlice corresponds to [bool], BitBox to Box<[bool]>, and BitVec to
Vec<bool>, and each of these types should be drop-in compatible replacements
for their standard library counterparts.
What Makes bitvec Different Than All The Other Bit Vector Crates
The most significant differences are that bitvec provides arbitrary bit
ordering through the Cursor trait, and provides a full-featured slice type.
Other crates have fixed orderings, and are often unable to produce slices that
begin at any arbitrary bit.
Additionally, the bitvec types implement the full extent of the standard
library APIs possible, in both inherent methods and trait implementations.
The bitvec types’ handles are exactly the size of their standard library
counterparts, while the other crates carry bit index information in separate
fields, making their handles wider. Depending on your needs, this may sway your
opinion in either direction. bitvec is more compact, but mangles the internal
pointer representation and requires more complex logic to use the bit region,
while other crates’ types are larger, but have more straightforward internal
logic.
Why Would You Use This

You need to directly control a bitstream’s representation in memory.
You need to do unpleasant things with communications protocols.
You need a list of bools that doesn’t waste 7 bits for every bit used.
You need to do set arithmetic, or numeric arithmetic, on those lists.
You are running a find/replace command on your repository from &[bool] to
&BitSlice, or Vec<bool> to BitVec, and expect minimal damage as a
result.

Why Wouldn’t You Use This
Your concern with the memory representation of bitsets includes compression.
BitSlice performs absolutely no compression, and maps bits directly into
memory. Compressed bit sets can be found in other crates, such as the
compacts crate, which uses the Roaring BitSet format.
Why Wouldn’t You Use This
bitvec aims to satisfy all use cases that require compact bit storage. If you
don’t need that compactness, Vec<bool> should be more than sufficient; if
bitvec does not work for you and you do need that compactness, please contact
me so I can either fix the problem, or put your story here.
Usage
Minimum Rust Version: 1.34.0
I wrote this crate because I was unhappy with the other bit-vector crates
available. I specifically need to manage raw memory in bit-level precision, and
this is not a behavior pattern the other bit-vector crates made easily available
to me. This served as the guiding star for my development process on this crate,
and remains the crate’s primary goal.
To this end, the default type parameters for the BitVec type use u8 as the
storage primitive and use big-endian ordering of bits: the forwards direction is
from MSb to LSb, and the backwards direction is from LSb to MSb.
To use this crate, you need to depend on it in Cargo.toml:
[dependencies]
bitvec = ""0.12""
and include it in your crate root src/main.rs or src/lib.rs:
//  Only if you’re in Rust 2015
#[macro_use]
extern crate bitvec;

use bitvec::prelude::*;
This imports the following symbols:


bitvec! – a macro similar to vec!, which allows the creation of BitVecs
of any desired endianness, storage type, and contents. The documentation page
has a detailed explanation of its syntax.


BitSlice<C: Cursor, T: Bits> – the actual bit-slice reference type. It is
generic over a cursor type (C) and storage type (T). Note that BitSlice
is unsized, and can never be held directly; it must always be behind a
reference such as &BitSlice or &mut BitSlice.
Furthermore, it is impossible to put BitSlice into any kind of intelligent
pointer such as a Box or Rc! Any work that involves managing the memory
behind a bitwise type must go through BitBox or BitVec instead. This may
change in the future as I learn how to better manage this library, but for now
this limitation stands.


BitBox<C: Cursor, T: Bits> – a fixed-size bit collection in owned memory.


BitVec<C: Cursor, T: Bits> – the actual bit-vector structure type. It is
generic over a cursor type (C) and storage type (T). This type is the main
worker of the crate. It supports the full Vec<T> API and trait
implementations, with the exception that (at this time) it is impossible to
take a mutable reference to a single bit. This means that everything except
for let elt: &mut bool = &mut bv[index]; and bv[index] = some_bool(); is
possible to express.


Cursor – an open trait that defines an ordering schema for BitVec to use.
Little and big endian orderings are provided by default. If you wish to
implement other ordering types, the Cursor trait requires one function:

fn at<T: Bits>(index: u8) -> u8 takes a semantic index and computes a bit
offset into the primitive T for it.



BigEndian – a marker type that implements Cursor by defining the forward
direction as towards LSb and the backward direction as towards MSb.


LittleEndian – a marker type that implements Cursor by defining the
forward direction as towards MSb and the backward direction as towards LSb.


Bits – a sealed trait that provides generic access to the four Rust
primitives usable as storage types: u8, u16, u32, and u64. usize
and the signed integers do not implement Bits and cannot be used as the
storage type. u128 also does not implement Bits, as I am not confident in
its memory representation.


BitVec has the same API as Vec, and should be easy to use.
The bitvec! macro can take type information in its first two arguments.
Because macros do not have access to the type checker, it currently only accepts
the literal tokens BigEndian or LittleEndian as the first argument, one of
the four unsigned integer primitives as the second argument, and then as many
values as you wish to insert into the collection. It accepts any integer value,
and maps them to bits by comparing against 0. 0 becomes false and any other
integer, whether it is odd or not, becomes true. While the syntax is loose,
you should only use 0 and 1 to fill the macro, for readability and lack of
surprise.
no_std
This crate can be used in #![no_std] libraries, by disabling the default
feature set. In your Cargo.toml, write:
[dependencies]
bitvec = { version = ""0.12"", default-features = false }
or
[dependencies.bitvec]
version = ""0.12""
default-features = false
This turns off the standard library imports and all usage of dynamic memory
allocation. Without an allocator, the bitvec! and bitbox! macros, and the
BitVec and BitBox types, are all disabled and removed from the library,
leaving only the BitSlice type.
To use bitvec in a #![no_std] environment that does have an allocator,
re-enable the alloc feature, like so:
[dependencies.bitvec]
version = ""0.12""
default-features = false
features = [""alloc""]
The alloc feature restores the owned-memory types and their macros. The only
difference between alloc and std is the presence of the standard library
façade and runtime support.
The std feature includes allocation, so using this crate without any feature
flags or by explicitly enabling the std feature will enable full
functionality.
Serde Support
The serde feature, by default, enables serialization for the BitSlice type.
Enabling the alloc or std features enables both serialization and
deserialization for the BitBox and BitVec types.
The serde feature is opt-in, and requires setting it in your Cargo.toml:
# Cargo.toml

[dependencies.bitvec]
version = ""0.12""
features = [
  ""serde"", # enables serialization
  ""std"", # enables deserialization
]
Example
extern crate bitvec;

use bitvec::prelude::*;

use std::iter::repeat;

fn main() {
    let mut bv = bitvec![BigEndian, u8; 0, 1, 0, 1];
    bv.reserve(8);
    bv.extend(repeat(false).take(4).chain(repeat(true).take(4)));

    //  Memory access
    assert_eq!(bv.as_slice(), &[0b0101_0000, 0b1111_0000]);
    //                   index 0 -^               ^- index 11
    assert_eq!(bv.len(), 12);
    assert!(bv.capacity() >= 16);

    //  Stack operations
    bv.push(true);
    bv.push(false);
    bv.push(true);

    assert!(bv[12]);
    assert!(!bv[13]);
    assert!(bv[14]);
    assert!(bv.get(15).is_none());

    bv.pop();
    bv.pop();
    bv.pop();

    //  Set operations
    bv &= repeat(true);
    bv = bv | repeat(false);
    bv ^= repeat(true);
    bv = !bv;

    //  Arithmetic operations
    let one = bitvec![1];
    bv += one.clone();
    assert_eq!(bv.as_slice(), &[0b0101_0001, 0b0000_0000]);
    bv -= one.clone();
    assert_eq!(bv.as_slice(), &[0b0101_0000, 0b1111_0000]);

    //  Borrowing iteration
    let mut iter = bv.iter();
    //  index 0
    assert_eq!(iter.next().unwrap(), false);
    //  index 11
    assert_eq!(iter.next_back().unwrap(), true);
    assert_eq!(iter.len(), 10);
}
Immutable and mutable access to the underlying memory is provided by the AsRef
and AsMut implementations, so the BitVec can be readily passed to transport
functions.
BitVec implements Borrow down to BitSlice, and BitSlice implements
ToOwned up to BitVec, so they can be used in a Cow or wherever this API
is desired. Any case where a Vec/[T] pair cannot be replaced with a
BitVec/BitSlice pair is a bug in this library, and a bug report is
appropriate.
BitVec can relinquish its owned memory with .into_vec() or
.into_boxed_slice(), and BitSlice can relinquish its borrow by going out
of scope.
Warnings
The BitSlice type is able to cause memory aliasing, as multiple independent
&mut BitSlice instances may use the same underlying memory. This crate takes
care to ensure that all observed behavior is exactly as expected, without any
side effects.
The BitSlice methods only use whole-element instructions when the slice spans
the full width of the element; when the slice has only partial use, the methods
crawl each bit individually. This is slower on most architectures, but
guarantees safety.
Race conditions are avoided through use of the atomic read/modify/write
instructions stabilized in 1.34.0.
Warnings
The BitSlice type is able to cause memory aliasing, as multiple independent
&mut BitSlice instances may use the same underlying memory. This crate takes
care to ensure that all observed behavior is exactly as expected, without any
side effects.
The BitSlice methods only use whole-element instructions when the slice spans
the full width of the element; when the slice has only partial use, the methods
crawl each bit individually. This is slower on most architectures, but
guarantees safety.
Race conditions are avoided through use of the atomic read/modify/write
instructions stabilized in 1.34.0.
Planned Features
Contributions of items in this list are absolutely welcome! Contributions of
other features are also welcome, but I’ll have to be sold on them.

Creation of specialized pointers Rc<BitSlice> and Arc<BitSlice>.
Procedural macros for bitvec! and bitbox!
An FFI module, and bindings from other languages.

",43
Automattic/wp-calypso,JavaScript,"Calypso

Calypso is the new WordPress.com front-end – a beautiful redesign of the WordPress dashboard using a single-page web application, powered by the WordPress.com REST API. Calypso is built for reading, writing, and managing all of your WordPress sites in one place.

It’s built with JavaScript – a very light node plus express server, React.js, Redux, wpcom.js, and many other wonderful libraries on the front-end.
You can read more about Calypso at developer.wordpress.com/calypso.
Getting Started
You can try out the user-side of Calypso on WordPress.com (a lot of the logged-in area is Calypso; if in doubt, view source), you can poke around the code here on GitHub, or you can install it and run it locally. The latter is the most fun.

Make sure you have git, node, and npm installed.
Clone this repository locally.
Add 127.0.0.1 calypso.localhost to your local hosts file.
Execute npm start from the root directory of the repository.
Open calypso.localhost:3000 in your browser.

Need more detailed installation instructions? We have them.
Contributing
If Calypso sparks your interest, don’t hesitate to send a pull request, send a suggestion, file a bug, or just ask a question. We promise we’ll be nice. Just don’t forget to check out our CONTRIBUTING doc – it includes a few technical details that will make the process a lot smoother.
Calypso welcomes – and indeed has been built by – contributors from all walks of life, with different backgrounds, and with a wide range of experience. We're committed to doing our part to make both Calypso and the wider WordPress community welcoming to everyone.
You can contribute in many ways. You can help reporting, testing, and detailing bugs, and also test new features we release in our ""beta"" program for testing on Horizon.
To clarify these expectations, Calypso has adopted the code of conduct defined by the Contributor Covenant. It can be read in full here.
Security
Need to report a security vulnerability? Go to https://automattic.com/security/ or directly to our security bug bounty site https://hackerone.com/automattic.
Browser Support
We support the latest two versions of all major browsers, except  IE, where we currently only support 11 and Edge.  (see Browse Happy for current latest versions).
Troubleshooting
If you have any problems running Calypso, please see most common issues.
License
Calypso is licensed under GNU General Public License v2 (or later).
",10985
regardscitoyens/twitter-parlementaires,Python,"Annuaire des comptes Twitter des parlementaires
Des citoyens et associations nous sollicitent souvent pour disposer de la liste de tous les comptes Twitter des députés ou des sénateurs.
L'Assemblée et le Sénat maintenant désormais des listes officielles des comptes de leurs élus, nous récupérons ces listes et réassocions automatiquement les comptes aux identifiants de chaque parlementaire afin de les intégrer aux bases de données de NosDéputés.fr et NosSénateurs.fr et donc également à leurs API.
Vous pouvez donc télécharger les données aux différentes adresses suivantes (changer csv en json ou xml dans lesurl pour les formats éponymes) :


pour les députés :

sur ce git (pour avoir les métadonnées twitter associées aux identifiants AN et NosDéputés)
ou via l'API de NosDéputés.fr (pour avoir les métadonnées parlementaires complétées du seul identifiant twitter)



pour les députés de la 14ème législature (2012-2017) :

sur ce git (pour avoir les métadonnées twitter associées aux identifiants AN et NosDéputés)
ou via l'API de NosDéputés.fr (pour avoir les métadonnées parlementaires complétées du seul identifiant twitter)



pour les sénateurs :

sur ce git (pour avoir les métadonnées twitter associées aux identifiants AN et NosDéputés)
ou via l'API de NosDéputés.fr (pour avoir les métadonnées parlementaires complétées du seul identifiant twitter)



Ces données sont mises-à-jour quotidiennement et redistribuées sous la licence OpenData ODBL.
Pour installer et lancer la collecte :
bin/install.sh #(will ask for sudo rights to install pip & virtualenv)
bin/build.sh
Regards Citoyens
",16
Automattic/wp-calypso,JavaScript,"Calypso

Calypso is the new WordPress.com front-end – a beautiful redesign of the WordPress dashboard using a single-page web application, powered by the WordPress.com REST API. Calypso is built for reading, writing, and managing all of your WordPress sites in one place.

It’s built with JavaScript – a very light node plus express server, React.js, Redux, wpcom.js, and many other wonderful libraries on the front-end.
You can read more about Calypso at developer.wordpress.com/calypso.
Getting Started
You can try out the user-side of Calypso on WordPress.com (a lot of the logged-in area is Calypso; if in doubt, view source), you can poke around the code here on GitHub, or you can install it and run it locally. The latter is the most fun.

Make sure you have git, node, and npm installed.
Clone this repository locally.
Add 127.0.0.1 calypso.localhost to your local hosts file.
Execute npm start from the root directory of the repository.
Open calypso.localhost:3000 in your browser.

Need more detailed installation instructions? We have them.
Contributing
If Calypso sparks your interest, don’t hesitate to send a pull request, send a suggestion, file a bug, or just ask a question. We promise we’ll be nice. Just don’t forget to check out our CONTRIBUTING doc – it includes a few technical details that will make the process a lot smoother.
Calypso welcomes – and indeed has been built by – contributors from all walks of life, with different backgrounds, and with a wide range of experience. We're committed to doing our part to make both Calypso and the wider WordPress community welcoming to everyone.
You can contribute in many ways. You can help reporting, testing, and detailing bugs, and also test new features we release in our ""beta"" program for testing on Horizon.
To clarify these expectations, Calypso has adopted the code of conduct defined by the Contributor Covenant. It can be read in full here.
Security
Need to report a security vulnerability? Go to https://automattic.com/security/ or directly to our security bug bounty site https://hackerone.com/automattic.
Browser Support
We support the latest two versions of all major browsers, except  IE, where we currently only support 11 and Edge.  (see Browse Happy for current latest versions).
Troubleshooting
If you have any problems running Calypso, please see most common issues.
License
Calypso is licensed under GNU General Public License v2 (or later).
",10985
regardscitoyens/twitter-parlementaires,Python,"Annuaire des comptes Twitter des parlementaires
Des citoyens et associations nous sollicitent souvent pour disposer de la liste de tous les comptes Twitter des députés ou des sénateurs.
L'Assemblée et le Sénat maintenant désormais des listes officielles des comptes de leurs élus, nous récupérons ces listes et réassocions automatiquement les comptes aux identifiants de chaque parlementaire afin de les intégrer aux bases de données de NosDéputés.fr et NosSénateurs.fr et donc également à leurs API.
Vous pouvez donc télécharger les données aux différentes adresses suivantes (changer csv en json ou xml dans lesurl pour les formats éponymes) :


pour les députés :

sur ce git (pour avoir les métadonnées twitter associées aux identifiants AN et NosDéputés)
ou via l'API de NosDéputés.fr (pour avoir les métadonnées parlementaires complétées du seul identifiant twitter)



pour les députés de la 14ème législature (2012-2017) :

sur ce git (pour avoir les métadonnées twitter associées aux identifiants AN et NosDéputés)
ou via l'API de NosDéputés.fr (pour avoir les métadonnées parlementaires complétées du seul identifiant twitter)



pour les sénateurs :

sur ce git (pour avoir les métadonnées twitter associées aux identifiants AN et NosDéputés)
ou via l'API de NosDéputés.fr (pour avoir les métadonnées parlementaires complétées du seul identifiant twitter)



Ces données sont mises-à-jour quotidiennement et redistribuées sous la licence OpenData ODBL.
Pour installer et lancer la collecte :
bin/install.sh #(will ask for sudo rights to install pip & virtualenv)
bin/build.sh
Regards Citoyens
",16
chosung-dev/fabric_board,JavaScript,"작업환경 구성
01.설치 및 환경 설정
fabric-board 다운로드
git clone https://github.com/chosung-dev/fabric_board

fabric-samples 디렉토리에 hyperledger/fabric-samples/basic-network(https://github.com/hyperledger/fabric-samples/basic-network) 디렉토리를 복사해서 집어넣는다.
설치된 최종 디렉토리 경로는 fabric_board/fabric-samples/basic-network이다.
basic-network/docker-compose.yml 파일을 다음과 같이 수정
-변경전
service.cli.volume:
		- ./../chaincode/:/opt/gopath/src/github.com/

-변경후
service.cli.volume:
		- ./../../chaincode/:/opt/gopath/src/github.com/

02.컨테이너 생성 및 서버 오픈
startFabric.sh를 통해 컨테이너 생성 및 체인코드 입력.
./startFabric.sh

다음과 같은 컨테이너를 확인 할 수 있다.
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                                            NAMES
8a13ad4662d4        hyperledger/fabric-tools     ""/bin/bash""              2 minutes ago       Up 2 minutes                                                         cli
2deddbb21717        hyperledger/fabric-peer      ""peer node start""        2 minutes ago       Up 2 minutes        0.0.0.0:7051->7051/tcp, 0.0.0.0:7053->7053/tcp   peer0.org1.example.com
1de6ccc4c52e        hyperledger/fabric-ca        ""sh -c 'fabric-ca-se…""   2 minutes ago       Up 2 minutes        0.0.0.0:7054->7054/tcp                           ca.example.com
98bd051eb96f        hyperledger/fabric-couchdb   ""tini -- /docker-ent…""   2 minutes ago       Up 2 minutes        4369/tcp, 9100/tcp, 0.0.0.0:5984->5984/tcp       couchdb
3f5675763f7e        hyperledger/fabric-orderer   ""orderer""                2 minutes ago       Up 2 minutes        0.0.0.0:7050->7050/tcp                           orderer.example.com

npm install 진행
npm install

fabric_board/bin/www 실행하여 http://localhost:3000 접속 확인.
cd bin
node www


",2
dxxzst/free-proxy-list,None,"free-proxy-list
免费的代理IP，定时更新，定期更新
最新更新日期：2019-05-17 09:31:06
    [若本项目对您有所帮助，欢迎Star](https://github.com/dxxzst/free-proxy-list) 

代理IP列表



IP
端口
协议
匿名度
国家




62.210.105.103
3128
https
low
France


139.59.156.228
3128
https
low
Germany


198.211.125.152
3128
https
low
Netherlands


37.79.244.120
3128
https
low
Russian Federation


194.48.212.1
3128
https
high
Ukraine


217.113.122.142
3128
https
high
Russian Federation Tolyatti


138.201.25.241
3128
https
low
Germany


138.68.150.245
3128
https
low
United


185.34.52.156
80
https
high
Lithuania Vilnius


81.171.24.199
3128
https
high
Netherlands


188.165.4.213
8080
https
low
Ireland


80.240.25.63
1080
https
high
Germany


138.68.166.224
8080
https
medium
United Kingdom London


91.208.39.70
8080
https
high
Russian Federation


138.68.166.224
3128
https
medium
United Kingdom London


51.255.198.111
9999
https
high
France


84.22.139.241
3128
https
high
Russian Federation Krasnoyarsk


188.152.127.95
8118
https
high
Italy Genoa


80.211.231.6
3128
https
low
Italy


188.216.98.84
8118
https
high
Italy ""Buttigliera Alta""


188.152.139.204
8118
https
high
Italy Lissone


188.152.136.71
8118
https
high
Italy ""San Gervasio Bresciano""


176.107.133.176
3128
https
high
Poland Warsaw


188.116.8.251
3128
https
low
Poland


54.37.6.196
3128
https
high
United Kingdom


176.107.133.176
8080
https
high
Poland Warsaw


193.194.69.36
3128
https
low
Algeria


2.39.102.197
8118
https
high
Italy Rome


2.38.29.170
8118
https
high
Italy Florence


2.38.119.167
8118
https
high
Italy Empoli


2.39.150.215
8118
https
high
Italy Rome


188.217.119.192
8118
https
high
Italy Rome


188.152.165.14
8118
https
high
Italy Rimini


46.151.155.207
80
https
low
Russia


185.80.130.17
80
https
high
Lithuania


46.163.186.9
3129
https
low
Russia


176.31.69.181
8080
https
low
France


185.161.224.226
3128
https
high
Azerbaijan


2.38.214.240
8118
https
high
Italy Pozzallo


119.81.71.27
8123
https
high
Singapore Singapore


81.171.2.229
3128
https
high
Netherlands


31.173.188.190
3128
https
low
Russia


185.124.86.11
3128
https
high
Turkey Istanbul


89.117.207.217
53281
https
low
Lithuania


185.124.86.10
3128
https
high
Turkey Istanbul


221.126.249.100
8080
https
high
Hong Kong


178.128.29.164
8080
https
medium
Singapore Singapore


107.175.144.205
8118
https
high
United States Buffalo


159.203.79.132
3128
https
low
United


122.183.139.101
8080
https
high
India


157.230.250.137
3128
https
medium
Singapore Singapore


173.192.21.89
25
https
high
United States Dallas


217.182.120.163
8080
https
high
France


173.192.21.89
8123
https
high
United States Dallas


151.106.10.109
8080
https
high
France


85.28.88.155
3128
https
high
Belgium Saint-Josse-ten-Noode


79.123.136.96
3128
https
high
Turkey Erzincan


34.207.150.171
3128
https
low
United


163.172.164.118
3128
https
low
France


80.240.25.119
1080
https
high
Germany


139.59.19.86
8080
https
low
India


128.14.134.174
1080
https
high
United States ""Los Angeles""


157.230.250.137
8080
https
medium
Singapore Singapore


157.230.240.140
8080
https
medium
Singapore Singapore


157.230.33.25
3128
https
medium
Singapore Singapore


165.22.254.197
3128
https
medium
United States ""New York""


185.172.215.74
8080
https
low
Iran


165.22.254.197
8080
https
medium
United States ""New York""


103.108.140.135
8080
https
high
Bangladesh Dhaka


157.230.33.25
8080
https
medium
Singapore Singapore


119.81.71.27
31288
https
high
Singapore Singapore


165.227.56.12
8080
https
low
United


202.63.215.46
3128
https
unknown
Pakistan


119.81.71.27
80
https
high
Singapore Singapore


82.222.47.146
8080
https
low
Turkey


185.80.130.17
8080
https
high
Lithuania


139.59.19.86
3128
https
low
India


185.34.52.156
8080
https
high
Lithuania Vilnius


104.248.7.88
80
https
medium
United States Clifton


150.95.109.143
80
https
medium
Vietnam


47.74.15.236
3128
https
high
Japan Tokyo


165.227.56.12
3128
https
low
United


47.91.29.142
3128
https
high
Japan Tokyo


190.92.64.210
80
https
low
Honduras


128.199.100.124
8080
https
low
Singapore


168.63.139.99
3128
https
low
Hong


182.253.122.142
3128
https
high
Indonesia


201.48.194.210
80
https
low
Brazil


89.236.17.108
3128
https
unknown
Sweden


202.99.172.145
8081
https
low
China


150.109.198.64
1080
https
high
Japan


182.253.201.76
10000
https
low
Indonesia


45.77.203.73
8080
https
low
United


159.65.5.189
3128
https
low
Singapore


92.154.81.90
8080
https
low
France


58.27.217.75
3128
https
high
Pakistan Lahore


221.231.109.40
3128
https
unknown
China


128.199.160.48
8080
https
low
Singapore


128.199.95.132
3128
https
low
Singapore


103.250.155.211
8080
https
high
India


128.199.160.48
3128
https
low
Singapore


149.56.12.146
8080
https
low
Canada


128.199.100.124
80
https
low
Singapore


124.133.97.17
8118
https
unknown
China


128.199.100.124
3128
https
low
Singapore


51.218.176.32
8080
https
low
Saudi


45.4.144.102
8080
https
low
Brazil


165.227.104.78
3128
https
unknown
United


128.199.95.132
80
https
low
Singapore


41.160.222.187
53281
https
high
South


200.29.191.151
3128
https
low
Chile


128.199.172.140
80
https
low
Singapore


128.199.172.140
3128
https
low
Singapore


198.71.63.215
3128
https
low
United


183.88.44.88
8080
https
low
Thailand


52.207.220.249
3128
http
high
United States


111.254.52.132
8998
https
unknown
Taiwan


38.29.144.236
53281
http
high
United States


118.175.220.30
8089
https
medium
Thailand


110.39.166.146
8080
http
low
Pakistan


36.80.76.255
8080
http
high
Indonesia


18.130.161.18
3128
https
high
United Kingdom


168.81.85.97
3199
http
low
United States


175.184.233.210
50340
http
high
Indonesia


208.98.186.80
53630
http
low
United States


83.239.97.26
49396
https
high
Russian Federation


104.221.135.101
3128
http
low
United States


92.82.150.62
41258
http
low
Romania


38.29.144.236
53281
http
high
United States


91.122.243.65
8080
http
low
Russian Federation


140.118.38.212
8080
http
medium
Taiwan


139.162.42.254
80
http
high
Singapore


35.238.13.121
3128
https
medium
United States


192.99.207.23
40067
http
low
Canada


109.94.172.98
8085
http
low
United States


176.100.78.204
8080
http
low
Russian Federation


23.20.214.120
3128
http
low
United States


117.197.41.157
33593
http
low
India


41.188.149.42
8080
http
low
Tanzania


54.205.117.212
80
http
high
United States


216.244.74.138
19009
http
low
United States


68.183.147.220
80
http
high
United States


103.107.133.1
33236
https
high
Bangladesh


204.11.243.70
3128
http
low
United States


125.26.53.51
8080
http
low
Thailand


77.85.169.71
35104
https
high
Bulgaria


149.28.120.36
80
https
high
United States


213.166.79.129
8085
http
low
United States


179.189.225.58
41881
http
low
Brazil


167.249.249.110
41302
http
high
Brazil


118.99.97.54
8080
http
low
Indonesia


50.239.245.102
80
http
medium
United States


168.232.167.85
3128
http
medium
Chile


216.108.226.61
808
http
low
United States


149.28.114.220
80
https
high
United States


41.89.171.220
8080
http
low
Kenya


110.137.148.45
8888
http
low
Indonesia


80.68.76.170
48026
http
low
Russian Federation


125.230.205.121
8080
https
unknown
Taiwan


159.65.73.129
80
http
high
United States


103.94.66.85
46103
https
high
India


34.73.131.13
80
http
medium
United States


180.235.39.9
80
http
high
Japan


134.209.112.110
3128
http
medium
United States


159.203.175.226
3128
http
low
United States


186.90.205.30
8089
https
unknown
Venezuela


195.12.21.130
8080
https
unknown
United Kingdom


194.78.121.114
3128
https
medium
Belgium


104.41.141.2
81
http
medium
United States


185.253.6.77
8085
http
low
United States


45.160.255.123
8080
http
low
Brazil


74.12.30.48
8080
http
low
Canada


208.110.86.178
19020
http
low
United States


69.197.182.218
19003
http
low
United States


78.60.130.181
30664
http
low
Lithuania


201.216.163.206
50892
http
low
Guatemala


1.32.54.254
8080
https
medium
Malaysia


178.237.208.78
35245
https
high
Serbia


177.131.22.128
59431
https
high
Brazil


35.247.240.135
80
http
low
United States


45.163.163.180
8080
http
low
Brazil


196.18.247.21
3199
http
low
United States


116.212.152.128
34423
http
high
Cambodia


188.0.138.147
8080
https
high
Kazakhstan


91.203.240.210
80
http
medium
Russian Federation


209.80.12.182
80
http
low
United States


136.25.2.43
32690
https
high
United States


183.105.51.243
8888
https
medium
Korea, Republic of


34.76.112.189
3128
http
low
United States


112.78.164.87
8080
http
low
Indonesia


209.97.164.200
1080
http
low
Singapore


104.216.26.174
5590
http
low
United States


3.213.222.172
8080
http
medium
United States


198.71.55.41
3128
http
low
United States


144.202.110.251
3128
http
low
United States


52.90.159.68
3128
http
high
United States


180.183.114.198
8213
http
medium
Thailand


149.202.50.167
3128
https
unknown
France


3.94.179.175
80
http
high
United States


69.129.45.226
8080
https
unknown
United States


179.229.252.48
8080
http
low
Brazil


70.169.129.235
48678
http
low
United States


35.185.182.175
80
http
high
Singapore


114.199.115.46
51123
https
high
Indonesia


31.192.146.145
53281
http
high
Russian Federation


31.27.15.244
80
http
high
Italy


3.82.97.24
3128
https
high
United States


185.99.64.75
56217
http
low
Czech Republic


52.37.57.154
80
http
medium
United States


131.100.213.15
31564
https
high
Brazil


3.112.131.213
3128
https
medium
Japan


13.95.71.175
80
http
high
Netherlands


178.167.44.2
8080
http
low
Russian Federation


35.194.71.194
3128
http
low
United States


54.159.198.255
3128
http
low
United States


125.226.224.90
8080
https
unknown
Taiwan


75.98.119.13
30598
https
high
United States


65.160.224.144
80
http
medium
United States


189.76.82.70
45873
http
low
Brazil


1.10.186.167
51907
http
low
Thailand


203.130.227.189
8080
http
low
Indonesia


72.169.66.225
87
http
low
United States


87.226.213.120
8080
http
medium
Russian Federation


206.189.95.220
80
http
medium
Singapore


34.80.236.158
80
http
high
United States


124.41.213.201
30146
http
low
Nepal


35.221.36.6
3128
http
low
United States


134.35.209.156
8080
http
low
Yemen


101.109.60.246
8080
http
low
Thailand


178.132.219.118
57089
http
low
Albania


138.255.195.209
8080
http
low
Brazil


173.54.193.242
50200
http
low
United States


157.230.41.225
8080
http
medium
Singapore


178.128.72.132
80
http
medium
United States


142.93.125.0
8080
http
medium
United States


201.55.46.6
80
https
unknown
Brazil


167.99.155.123
80
http
medium
United States


35.245.53.250
3128
https
high
United States


37.17.12.32
46184
http
high
Belarus


178.124.150.126
81
https
medium
Belarus


213.6.204.153
47698
https
high
Palestinian Territory


104.216.26.169
5587
http
low
United States


104.248.124.35
8080
http
low
United States


51.83.105.113
8080
https
high
France


216.10.0.165
3199
http
low
United States


176.123.164.240
58488
http
low
Russian Federation


45.175.181.100
8080
http
low
Brazil


194.78.121.114
3128
https
medium
Belgium


104.216.26.179
5597
http
low
United States


103.107.133.1
33236
https
high
Bangladesh


112.105.196.100
8998
https
medium
Taiwan


194.186.131.38
47532
http
low
Russian Federation


186.10.82.28
50176
https
high
Chile


165.227.62.167
80
http
medium
United States


125.27.251.24
36048
http
low
Thailand


104.248.220.143
3128
http
medium
United States


109.200.168.87
8080
http
low
Yemen


46.229.206.135
36135
http
low
Bulgaria


208.74.68.2
8080
http
low
United States


186.42.252.46
36521
https
high
Ecuador


36.90.176.107
3128
http
low
Indonesia


35.199.28.24
3128
http
low
United States


31.27.15.216
80
http
high
Italy


110.44.135.30
1080
https
high
Japan


89.208.194.126
3128
http
low
Russian Federation


50.253.229.189
45725
http
high
United States


182.53.206.163
61111
https
high
Thailand


3.81.200.113
3128
http
low
United States


206.125.41.135
80
http
low
United States


166.249.185.131
35518
http
low
United States


168.81.21.101
3199
http
low
United States


186.68.85.26
53281
http
low
Ecuador


1.20.99.125
56613
https
high
Thailand


103.194.251.43
36019
http
low
India


157.230.149.189
80
http
medium
United States


67.185.242.197
8080
http
low
United States


202.166.216.249
23500
http
low
Nepal


195.138.86.164
3128
https
unknown
Ukraine


23.124.185.51
8080
http
low
United States


68.183.124.69
3128
http
low
United States


107.175.150.83
8080
http
low
United States


13.113.181.46
3128
http
high
Japan


107.150.35.202
19014
http
low
United States


68.66.205.216
80
http
low
United States


103.100.96.174
53281
http
high
Indonesia


168.81.39.232
3199
http
low
United States


192.151.156.66
19011
http
low
United States


84.177.225.49
3128
http
medium
Germany


177.38.68.234
8080
http
low
Brazil


178.94.86.176
59604
http
high
Ukraine


167.99.67.234
8080
https
medium
Singapore


104.41.141.2
81
http
medium
United States


66.102.224.180
8080
http
low
United States


157.100.52.124
32652
http
low
Ecuador


100.42.64.222
8080
http
low
United States


199.27.95.186
32463
http
low
United States


46.99.154.188
8089
https
unknown
Albania


138.0.89.152
63141
https
high
Colombia


191.252.185.71
80
http
medium
Brazil


118.175.93.69
54242
http
high
Thailand


111.240.244.154
8998
https
medium
Taiwan


138.68.235.72
8080
http
medium
United States


195.225.49.136
58302
http
low
Ukraine


54.193.23.30
80
http
low
United States


41.223.155.118
53281
http
low
Mozambique


182.253.94.111
8080
http
low
Indonesia


125.25.54.65
42918
https
high
Thailand


95.131.91.130
41384
https
high
Russian Federation


103.229.200.66
39514
http
high
Indonesia


190.124.34.221
30753
http
high
Nicaragua


103.75.239.245
8080
http
low
Bangladesh


14.207.72.150
8080
http
low
Thailand


80.51.100.10
41358
http
low
Poland


110.74.221.18
61498
http
high
Cambodia


52.68.127.244
80
http
high
Japan


182.16.171.18
53281
http
low
Indonesia


168.232.49.223
80
http
medium
El Salvador


91.205.81.15
41258
http
high
Italy


103.79.228.251
61144
http
high
India


104.152.45.45
80
http
medium
United States


139.81.136.60
3199
http
low
United States


157.230.0.117
8080
http
medium
United States


96.9.87.54
45471
http
low
Cambodia


134.209.123.111
21
http
medium
United States


221.126.237.206
8080
https
medium
Hong Kong


116.212.150.7
38132
https
high
Cambodia


3.209.42.10
3128
http
medium
United States


196.18.247.176
3199
http
low
United States


199.27.95.186
32463
http
low
United States


35.245.200.76
3128
http
low
United States


165.22.249.218
8080
http
medium
Singapore


103.124.12.220
8080
http
low
India


50.195.207.133
47593
http
low
United States


31.23.34.40
8080
http
low
Russian Federation


190.239.214.126
8080
http
low
Peru


104.248.115.226
8080
http
medium
United States


31.209.104.3
50728
http
high
Cyprus


52.124.6.146
40834
https
high
United States


104.236.55.48
8080
http
low
United States


118.174.65.137
43494
https
high
Thailand


162.245.239.170
19012
http
low
United States


88.157.149.250
8080
https
medium
Portugal


66.172.114.113
44989
https
high
United States


12.106.88.164
8080
http
low
United States


50.195.207.133
47593
http
low
United States


124.12.32.76
8080
https
unknown
Taiwan


104.194.139.85
3199
http
low
United States


31.40.209.234
8085
http
low
United States


3.87.193.3
3128
http
low
United States


35.245.218.14
3128
https
high
United States


110.74.221.169
59124
https
high
Cambodia


204.12.211.114
19005
http
low
United States


168.81.68.236
3199
http
low
United States


52.207.220.249
3128
http
high
United States


54.153.115.162
80
http
low
United States


103.84.254.190
60854
http
low
Bangladesh


47.90.250.0
3128
https
high
United States


36.81.137.183
8080
http
low
Indonesia


46.52.153.74
49456
https
high
Russian Federation


114.130.31.157
54353
https
high
Bangladesh


66.172.114.113
44989
https
high
United States


177.191.181.253
3128
https
medium
Brazil


89.23.194.174
8080
http
low
Russian Federation


161.117.86.98
3128
https
medium
Singapore


3.112.226.249
3128
https
medium
Japan


85.237.57.198
42566
http
low
Russian Federation


18.218.121.86
8080
http
medium
United States


158.69.104.198
1080
http
low
Canada


75.98.119.13
30598
https
high
United States


13.231.215.21
3128
https
medium
Japan


210.96.153.20
3128
https
unknown
Korea, Republic of


82.99.213.36
80
https
unknown
Iran, Islamic Republic of


103.102.73.72
8080
http
low
India


51.68.73.5
443
https
high
France


134.209.209.76
8080
http
medium
United States


206.189.163.84
80
http
medium
United States


209.155.147.13
8080
https
unknown
United States


192.228.156.251
3128
https
high
Malaysia


171.97.197.247
8888
https
unknown
Thailand


178.128.72.132
80
http
medium
United States


110.34.192.12
1080
http
low
United States


142.93.125.0
8080
http
medium
United States


104.238.147.3
1080
http
low
United States


41.76.158.250
33954
http
low
Nigeria


103.242.15.86
42862
http
high
Cambodia


182.74.198.74
3128
https
medium
India


109.227.106.116
53281
http
high
Ukraine


104.216.26.178
5596
http
low
United States


200.255.220.211
8080
https
unknown
Brazil


103.36.126.254
47041
http
high
India


159.65.73.129
80
http
high
United States


195.170.15.66
8080
http
low
Greece


62.158.76.191
3128
https
medium
Germany


5.249.145.41
3128
http
high
Italy


37.57.253.91
37186
https
high
Ukraine


198.143.182.21
8888
http
low
United States


45.175.182.70
8080
http
low
Brazil


182.23.32.66
30898
http
high
Indonesia


124.41.240.126
55385
http
low
Nepal


124.41.213.211
50117
http
low
Nepal


119.160.183.246
8118
https
medium
Brunei Darussalam


188.191.31.135
41258
https
high
Ukraine


35.236.45.131
3128
http
low
United States


165.22.253.195
3128
https
high
Singapore


175.136.205.137
8080
https
unknown
Malaysia


181.215.58.35
3199
http
low
United States


202.51.188.139
31071
http
low
Bangladesh


45.55.45.80
80
http
high
United States


117.206.13.182
8080
http
low
India


157.230.13.186
8080
http
medium
United States


36.67.27.81
42215
http
low
Indonesia


113.163.141.81
8080
https
medium
Vietnam


46.52.153.74
49456
https
high
Russian Federation


159.65.131.98
3128
https
high
Singapore


125.167.134.100
8080
http
low
Indonesia


197.234.56.169
8083
http
low
Nigeria


52.24.240.49
3128
http
low
United States


188.136.243.51
32129
https
high
Iran


52.192.164.148
3128
https
medium
Japan


35.198.12.227
80
http
medium
United States


103.126.85.10
8080
http
low
Indonesia


202.150.144.14
8080
https
medium
Indonesia


95.88.164.152
3128
https
medium
Germany


153.92.5.239
80
http
high
United States


34.222.249.93
3128
http
low
United States


34.210.184.16
80
http
low
United States


35.236.209.76
3128
http
high
United States


75.98.119.13
30598
https
high
United States


203.223.143.51
8080
https
unknown
Malaysia


109.87.33.2
53381
http
low
Ukraine


138.186.23.5
46098
https
high
Colombia


1.20.101.90
37006
http
low
Thailand


45.175.182.88
8080
http
low
Brazil


35.238.13.121
3128
https
medium
United States


167.99.1.61
3128
http
medium
United States


35.245.208.220
3128
http
low
United States


202.128.22.29
48678
http
low
Guam


54.255.229.98
8080
http
low
Singapore


91.144.147.248
37632
http
low
Russian Federation


104.216.26.162
5599
http
low
United States


37.21.62.119
8080
https
medium
Russian Federation


12.33.254.195
3128
https
unknown
United States


31.3.91.54
56161
https
high
Macedonia


212.155.230.214
8080
https
unknown
France


201.158.105.124
32335
http
high
Mexico


167.99.155.123
80
http
medium
United States


103.79.228.67
59948
http
high
India


165.227.255.220
3128
http
medium
United States


91.243.89.15
8085
http
low
United States


124.120.124.62
8888
https
unknown
Thailand


210.11.181.221
33491
https
high
Australia


176.192.58.78
60412
https
high
Russian Federation


52.90.159.68
3128
http
high
United States


159.192.136.11
23500
http
low
Thailand


168.80.39.235
3199
http
low
United States


104.248.123.136
80
http
medium
United States


203.173.93.20
58485
https
high
Indonesia


212.155.230.208
8080
https
medium
France


110.74.221.169
59124
https
high
Cambodia


103.86.135.62
59538
https
high
Pakistan


3.213.222.172
8080
http
medium
United States


196.22.51.6
37469
http
low
Mozambique


104.194.133.206
3199
http
low
United States


77.55.216.173
3128
http
low
Poland


141.255.151.155
8080
http
high
France


185.144.120.66
80
http
low
Italy


51.68.73.5
443
https
high
France


104.152.45.45
80
http
medium
United States


50.239.245.107
80
http
medium
United States


89.31.126.251
80
http
medium
Japan


186.91.205.15
8080
http
low
Venezuela


160.218.112.165
43781
http
low
Czech Republic


92.52.186.123
50714
http
high
Ukraine


185.28.193.95
8080
https
unknown
Czech Republic


96.9.91.22
54096
https
high
Cambodia


52.247.208.227
3128
http
low
United States


154.66.216.70
44561
http
low
Uganda


80.80.160.251
8080
https
medium
Albania


138.68.235.72
8080
http
medium
United States


191.242.182.132
8081
http
low
Brazil


178.46.160.64
50473
https
high
Russian Federation


103.108.140.135
8080
https
high
Bangladesh


166.249.185.131
35518
http
low
United States


104.248.117.3
8080
http
low
United States


3.95.30.209
3128
http
medium
United States


1.179.206.89
46655
https
high
Thailand


203.176.135.98
42173
https
high
Cambodia


52.90.178.179
3128
http
low
United States


3.209.42.10
3128
http
medium
United States


172.86.107.137
3199
http
low
United States


209.97.172.229
1080
http
low
Singapore


13.68.221.22
8888
https
medium
United States


216.244.77.170
19008
http
low
United States


36.81.115.238
8080
http
low
Indonesia


197.210.217.66
38031
https
high
Nigeria


103.217.156.31
8080
https
high
Myanmar


203.189.153.24
80
http
low
Cambodia


118.174.54.164
8080
http
high
Thailand


103.56.205.212
53460
http
low
Indonesia


89.218.5.109
42748
http
low
Kazakhstan


96.126.111.95
3128
https
high
United States


154.118.243.18
53685
https
high
Somalia


144.217.12.186
80
https
high
Canada


78.32.35.22
50912
http
high
United Kingdom


121.33.226.167
3128
https
medium
China


95.133.30.155
3128
http
low
Ukraine


35.245.114.121
3128
http
low
United States


159.65.131.98
3128
https
high
Singapore


66.228.57.168
3128
http
low
United States


165.22.253.195
3128
https
high
Singapore


157.230.149.189
80
http
medium
United States


47.252.6.232
3128
http
low
United States


31.170.61.205
80
http
low
Iran


206.189.163.84
80
http
medium
United States


96.75.42.213
3128
http
medium
United States


96.65.221.1
45138
http
low
United States


51.83.105.113
8080
https
high
France


173.161.0.227
80
https
unknown
United States


177.42.235.85
8081
http
low
Brazil


128.71.161.1
1080
http
low
Russian Federation


124.120.98.219
3128
http
medium
Thailand


142.44.184.205
80
http
low
United States


50.238.51.150
80
http
medium
United States


181.214.194.245
3199
http
low
United States


91.224.156.197
8080
http
low
Ukraine


47.252.3.4
3128
http
low
United States


66.228.57.168
3128
http
low
United States


188.17.148.31
54256
https
high
Russian Federation


172.104.33.10
3128
http
low
Singapore


13.250.71.149
3128
http
medium
Singapore


125.25.45.7
57530
https
high
Thailand


52.87.238.15
3128
http
low
United States


110.77.231.236
8080
http
low
Thailand


104.236.54.196
8080
https
high
United States


24.227.248.226
31784
http
high
United States


217.76.204.197
8080
https
medium
Ukraine


103.224.101.155
46759
http
low
Indonesia


185.136.150.201
30828
https
high
Iraq


177.191.181.253
3128
https
medium
Brazil


196.18.186.125
3199
http
low
United States


129.205.42.208
8080
http
low
South Africa


216.105.64.186
8080
https
unknown
United States


185.87.199.153
3128
http
medium
Russian Federation


177.92.20.182
50973
https
high
Brazil


24.226.146.46
40319
https
high
Canada


109.174.19.134
8197
http
medium
Russian Federation


205.196.185.218
8080
http
low
United States


35.245.213.254
3128
http
high
United States


78.189.203.182
48680
https
high
Turkey


37.1.32.21
53281
https
high
Russian Federation


46.248.82.123
32970
http
high
Slovenia


134.209.209.76
8080
http
medium
United States


1.10.181.44
8080
http
low
Thailand


35.246.114.83
3128
https
high
United States


173.192.21.89
80
http
low
United States


203.176.135.98
42173
https
high
Cambodia


118.172.227.89
30702
http
low
Thailand


96.126.111.95
3128
https
high
United States


115.127.7.162
46244
http
high
Bangladesh


178.32.80.235
1080
https
medium
France


103.209.140.115
59359
http
high
India


86.123.166.109
8080
http
low
Romania


138.118.34.25
8080
http
low
Brazil


189.126.65.141
53436
http
high
Brazil


31.40.209.29
8085
http
low
United States


203.130.228.60
8080
https
unknown
Indonesia


79.165.40.70
8081
http
low
Russian Federation


191.100.25.15
38139
http
high
Ecuador


170.239.84.239
3128
http
medium
Chile


131.255.4.49
80
https
medium
Argentina


138.197.102.119
80
http
high
United States


103.108.140.135
8080
https
high
Bangladesh


179.124.240.199
44518
http
low
Brazil


209.97.169.203
1080
http
low
Singapore


178.159.107.140
8085
http
low
United States


195.62.255.13
48578
http
high
Italy


153.122.1.38
80
http
high
Japan


37.21.62.119
8080
https
medium
Russian Federation


91.189.244.249
53281
http
high
Russian Federation


35.245.168.59
3128
http
high
United States


124.120.100.163
3128
http
medium
Thailand


159.89.142.5
8080
http
medium
United States


177.184.57.223
8080
http
low
Brazil


40.74.243.24
80
http
high
United States


65.160.224.144
80
http
medium
United States


178.134.32.242
8080
http
low
Georgia


111.68.31.134
50838
http
high
Indonesia


173.230.128.152
3128
http
low
United States


113.11.47.242
40071
http
low
Bangladesh


104.152.45.46
80
http
medium
United States


72.250.28.64
36851
http
low
United States


177.192.169.14
3128
http
low
Brazil


144.202.60.154
80
http
high
United States


177.207.172.206
8080
http
low
Brazil


94.231.216.225
8085
http
low
United States


159.89.142.5
8080
http
medium
United States


54.64.239.229
3128
https
medium
Japan


13.231.217.38
3128
https
medium
Japan


195.46.122.165
41372
https
high
Russian Federation


54.80.119.166
3128
http
low
United States


125.27.251.186
30385
http
low
Thailand


188.136.243.51
32129
https
high
Iran


173.230.142.20
3128
http
low
United States


134.35.68.127
8080
http
low
Yemen


54.205.117.212
80
http
high
United States


125.26.99.178
30703
https
high
Thailand


196.18.245.75
3199
http
low
United States


94.231.216.212
8085
http
low
United States


138.255.205.229
8080
http
low
Brazil


35.194.77.2
3128
http
high
United States


23.162.96.83
3128
https
high
United States


81.91.30.183
8080
http
low
Yemen


177.38.68.226
8080
http
low
Brazil


118.232.61.28
8998
https
unknown
Taiwan


123.201.5.231
47566
http
low
India


5.160.213.48
8080
http
low
Iran


23.92.30.59
3128
http
low
United States


34.237.98.226
3128
http
medium
United States


35.199.28.24
3128
http
low
United States


178.128.108.19
8080
http
low
Singapore


104.236.54.196
8080
https
high
United States


18.237.124.160
3128
http
low
United States


149.28.120.36
80
https
high
United States


50.238.51.150
80
http
medium
United States


51.75.109.87
3128
http
low
France


46.99.164.161
40411
https
high
Albania


180.179.98.22
3128
http
high
India


35.221.21.23
3128
http
low
United States


117.198.129.24
58691
http
low
India


147.139.132.51
3128
http
low
United States


128.199.230.243
8080
http
low
Singapore


131.0.210.10
80
http
low
Brazil


47.90.251.190
3128
http
low
United States


35.194.77.2
3128
http
high
United States


108.59.10.129
55555
https
medium
United States


34.80.236.158
80
http
high
United States


31.10.11.28
8080
http
low
Russian Federation


180.250.219.58
53281
https
high
Indonesia


52.90.81.162
3128
http
low
United States


36.90.5.120
3128
http
low
Indonesia


104.248.211.184
3128
http
low
United States


213.221.58.186
33274
https
high
Russian Federation


118.171.222.183
3128
https
medium
Taiwan


40.74.243.24
80
http
high
United States


167.99.67.234
8080
https
medium
Singapore


3.95.30.209
3128
http
medium
United States


47.90.241.169
3128
http
low
United States


35.245.213.254
3128
http
high
United States


190.214.24.58
65205
https
high
Ecuador


195.208.43.250
45482
https
high
Russian Federation


81.24.88.136
41258
http
high
Russian Federation


45.55.46.222
8080
http
medium
United States


122.102.41.82
55783
http
high
Indonesia


99.100.78.207
8080
http
low
United States


18.218.121.86
8080
http
medium
United States


178.215.177.56
8080
http
low
Ukraine


105.212.59.35
51892
http
high
South Africa


149.28.120.36
80
https
high
United States


103.240.161.59
48809
https
high
India


113.53.83.69
8080
https
high
Thailand


134.209.73.47
8080
http
medium
United States


203.77.252.250
37713
http
low
Indonesia


200.68.27.100
3128
https
unknown
Chile


3.94.179.175
80
http
high
United States


186.250.119.155
42470
http
high
Brazil


157.230.241.94
80
http
medium
Singapore


13.231.112.66
3128
https
medium
Japan


35.246.114.83
3128
https
high
United States


103.240.241.145
80
https
unknown
Lao People's Democratic Republic


161.117.86.98
3128
https
medium
Singapore


88.12.48.61
42365
http
low
Spain


52.37.57.154
80
http
medium
United States


45.70.194.250
8080
http
low
Brazil


95.88.164.152
3128
https
medium
Germany


144.202.60.154
80
http
high
United States


188.243.62.147
49899
http
low
Russian Federation


103.254.185.53
30944
http
high
Nepal


139.193.51.164
8080
http
low
Indonesia


103.87.47.155
8080
http
low
India


210.61.46.165
3128
http
low
Taiwan


171.96.230.233
8888
https
medium
Thailand


125.25.80.54
51409
https
high
Thailand


202.150.144.14
8080
https
medium
Indonesia


189.49.193.60
8080
http
low
Brazil


131.213.89.85
8080
https
high
Japan


12.189.125.134
8080
http
low
United States


34.237.98.226
3128
http
medium
United States


125.212.217.215
80
https
medium
Vietnam


78.134.44.168
8080
http
low
Italy


103.250.158.227
23500
https
high
India


80.80.160.252
8080
https
unknown
Albania


186.192.26.232
8080
http
low
Brazil


198.177.126.218
80
http
low
United States


47.91.244.96
80
http
low
Hong Kong


182.253.233.205
8080
http
low
Indonesia


18.182.47.48
3128
https
medium
Japan


178.94.7.35
8080
http
low
Ukraine


23.162.96.83
3128
https
high
United States


172.104.171.106
80
http
high
Singapore


35.245.168.59
3128
http
high
United States


77.82.253.111
8080
http
low
Russian Federation


118.171.222.183
3128
https
medium
Taiwan


185.206.201.184
8080
http
low
Lebanon


91.135.194.22
60795
https
high
Kazakhstan


202.182.164.165
36741
https
high
Indonesia


54.64.239.229
3128
https
medium
Japan


157.230.13.186
8080
http
medium
United States


182.52.51.48
32425
http
low
Thailand


35.182.94.43
3128
http
medium
Canada


139.255.99.166
50031
http
high
Indonesia


103.243.82.198
45603
http
low
Bangladesh


24.227.248.226
31784
http
high
United States


173.230.137.159
3128
http
low
United States


178.68.38.237
8080
http
low
Russian Federation


77.28.96.206
46659
http
low
Macedonia


103.25.167.200
33575
http
low
Indonesia


125.167.95.124
8080
http
low
Indonesia


118.175.31.65
42931
http
low
Thailand


34.73.70.111
3128
http
low
United States


138.121.32.133
23492
http
low
Brazil


131.100.34.182
8080
http
low
Brazil


198.71.55.41
3128
http
low
United States


112.78.3.27
8080
https
unknown
Vietnam


78.189.203.182
48680
https
high
Turkey


43.229.73.192
61597
http
low
India


1.179.206.89
46655
https
high
Thailand


85.21.8.61
8080
http
low
Russian Federation


104.248.108.33
8080
http
medium
United States


192.99.191.236
1080
http
low
United States


143.202.226.78
8080
http
low
Brazil


18.130.161.18
3128
https
high
United Kingdom


125.24.159.119
8080
http
low
Thailand


119.18.155.170
54605
https
high
Indonesia


40.112.176.201
3128
http
low
United States


74.207.237.217
3128
http
low
United States


142.93.80.120
3128
http
low
United States


104.216.26.189
5606
http
low
United States


117.197.40.85
54343
http
low
India


54.176.202.142
80
http
high
United States


159.65.236.178
3128
http
medium
United States


149.28.114.220
80
https
high
United States


35.221.21.23
3128
http
low
United States


1.2.169.44
48545
https
high
Thailand


195.9.44.50
55197
https
high
Russian Federation


52.25.119.60
3128
http
low
United States


34.73.131.13
80
http
medium
United States


208.114.192.126
8080
http
low
United States


176.9.25.84
80
https
medium
Germany


23.162.96.83
3128
https
high
United States


72.169.67.17
87
http
low
United States


35.238.13.121
3128
https
medium
United States


96.9.69.164
53281
https
high
Cambodia


52.207.214.160
3128
http
low
United States


193.56.64.150
8085
http
low
United States


148.255.95.55
8080
http
low
Dominican Republic


45.73.0.118
49943
http
high
Canada


206.40.115.2
8080
http
low
United States


43.231.215.238
34788
http
high
India


52.90.124.143
3128
http
low
United States


192.151.156.42
19005
http
low
United States


182.74.198.74
3128
https
medium
India


24.106.221.230
53281
http
high
United States


104.152.45.46
80
http
medium
United States


5.23.103.98
40962
http
low
Russian Federation


185.60.240.226
8080
http
low
Italy


194.182.64.102
3128
https
low
Czech


122.70.137.154
3128
https
low
China


180.63.131.247
3128
https
low
Japan


45.77.144.124
80
https
low
United


45.55.39.11
8118
https
high
United


62.133.191.116
8080
https
high
Russia


164.163.239.61
3128
https
low
Brazil


191.252.192.141
3128
https
low
Brazil


118.114.77.47
8080
https
high
China


222.98.44.125
3128
https
low
South


183.89.54.236
8080
https
low
Thailand


78.61.208.53
80
https
low
Lithuania


122.183.139.107
8080
https
high
India


35.198.61.202
80
https
low
United


62.14.178.72
53281
https
high
Spain


185.81.93.232
8080
https
low
Georgia


35.196.26.166
3128
https
unknown
United


13.125.140.215
3128
https
low
South


159.65.168.177
8118
https
high
United


180.211.115.155
808
https
low
India


200.202.208.226
8080
https
low
Brazil


148.251.85.16
3128
https
unknown
Germany


45.77.203.73
80
https
low
United


92.186.51.119
8080
https
high
Spain


103.92.142.147
53281
https
high
Australia


91.210.94.182
3128
https
high
Russia


183.88.48.207
8080
https
low
Thailand


103.94.169.149
8080
https
low
Indonesia


141.196.128.243
8080
https
low
Turkey


183.88.232.207
8080
https
low
Thailand


47.17.62.181
808
https
low
United


177.126.81.244
3128
https
low
Brazil


185.138.113.134
8080
https
low
Ireland


183.89.29.12
8080
https
low
Thailand


96.9.69.167
53281
https
high
Cambodia


177.184.144.130
8080
https
low
Brazil


176.58.68.168
8080
https
high
Palestinian


103.87.139.22
8080
https
unknown
Bangladesh


94.253.51.119
8080
https
low
Russia


178.252.26.99
8080
https
low
Poland


177.72.98.3
3128
https
low
Brazil


58.252.6.165
9000
https
low
China


219.85.63.166
8118
https
unknown
Taiwan


201.221.128.27
8080
https
low
Colombia


118.168.194.45
8080
https
low
Taiwan


186.83.66.119
63909
https
high
Colombia


14.207.11.253
8080
https
low
Thailand


197.210.252.39
8080
https
low
Nigeria


177.183.168.152
8088
https
low
Brazil


77.92.128.2
8080
https
low
Turkey


186.67.116.76
8080
https
low
Chile


109.173.200.123
8080
https
high
Poland


110.136.129.46
80
https
high
Indonesia


36.77.80.216
8080
https
low
Indonesia


85.194.250.162
8080
https
low
Iraq


117.239.50.118
8080
https
low
India


139.0.5.203
3128
https
low
Indonesia


212.112.110.24
8080
https
low
Kyrgyzstan


148.251.85.26
3128
https
unknown
Germany


186.101.136.10
65205
https
high
Ecuador


114.6.55.173
80
https
high
Indonesia


95.31.186.157
53281
https
high
Russia


80.70.193.18
3128
https
low
Germany


190.24.131.250
3128
https
low
Colombia


103.98.120.89
80
https
low
Indonesia


117.4.238.129
8081
https
low
VietNam


201.211.40.52
8080
https
low
Venezuela


167.249.181.250
3128
https
low
Brazil


223.205.164.162
8080
https
low
Thailand


187.115.153.195
53281
https
low
Brazil


191.18.29.149
8080
https
high
Brazil


36.66.76.181
3128
https
low
Indonesia


110.77.173.47
8088
https
low
Thailand


185.12.22.43
53281
https
high
Poland


143.208.57.51
8080
https
low
Guatemala


186.232.145.150
8080
https
low
Brazil


139.59.109.146
8080
https
low
Singapore


109.196.246.150
8080
https
low
Poland


182.23.58.180
8080
https
low
Indonesia


5.35.34.10
53281
https
high
Russia


178.217.37.140
8080
https
low
Poland


209.52.241.147
53281
https
high
Canada


88.99.154.195
80
https
low
Germany


109.224.7.134
8080
https
low
Iraq


173.249.9.82
3128
https
unknown
Germany


2.178.255.157
3128
https
low
Iran


58.52.159.188
808
https
high
China


202.46.1.80
8080
https
low
Indonesia


189.10.56.199
8080
https
low
Brazil


202.69.63.2
8080
https
low
Pakistan


181.112.61.162
65103
https
high
Ecuador


189.84.79.146
8080
https
low
Brazil


91.206.30.218
3128
https
low
Ukraine


187.6.108.42
8080
https
low
Brazil


187.115.154.121
8088
https
low
Brazil


5.25.4.252
8080
https
low
Turkey


197.250.8.162
65103
https
high
Tanzania


88.10.220.74
8080
https
low
Spain


173.45.67.182
8080
https
low
United


151.80.159.18
80
https
low
France


201.249.88.229
80
https
low
Venezuela


202.52.234.222
8080
https
low
Nepal


95.159.69.161
8080
https
low
Iraq


190.151.105.179
8080
https
low
Chile


212.36.30.174
8080
https
low
Bulgaria


196.223.140.170
63909
https
high
South


183.179.199.225
8080
https
low
Hong


109.105.199.52
53281
https
high
Bosnia


51.254.191.239
8080
https
unknown
France


212.126.116.30
8080
https
low
Iraq


62.201.225.42
62225
https
high
Iraq


46.253.12.46
53281
https
low
Bulgaria


190.203.50.121
8080
https
low
Venezuela


45.116.114.6
8080
https
low
India


114.134.186.25
65103
https
high
Cambodia


115.70.28.209
53281
https
low
Australia


186.10.5.139
8080
https
low
Chile


196.12.154.94
8080
https
low
Mauritius


202.188.206.194
8080
https
low
Malaysia


177.1.142.211
8080
https
low
Brazil


41.186.24.69
3128
https
low
Rwanda


91.240.249.34
8088
https
low
Poland


165.255.158.184
8080
https
low
South


218.106.98.166
53281
https
low
China


59.38.241.141
3128
https
unknown
China


61.38.236.98
808
https
unknown
South


91.137.250.91
53281
https
low
Hungary


154.117.208.214
8080
https
low
Burundi


114.6.61.98
8080
https
low
Indonesia


189.41.148.100
8080
https
low
Brazil


86.63.211.224
3128
https
low
Czech


93.142.161.139
8080
https
low
Croatia


37.156.31.70
8080
https
low
Iran


190.77.182.195
3128
https
high
Venezuela


200.66.94.148
8080
https
low
Mexico


103.59.53.29
8080
https
low
Lao


217.13.222.179
8080
https
low
Russia


177.52.95.126
53281
https
high
Brazil


95.210.251.29
53281
https
low
Italy


137.59.2.9
65205
https
high
India


177.185.114.89
53281
https
low
Brazil


110.77.228.217
8080
https
low
Thailand


46.219.116.2
8081
https
low
Ukraine


62.168.3.60
8080
https
low
Czech


78.187.235.13
8080
https
low
Turkey


222.124.189.122
80
https
low
Indonesia





",3
hjhuney/Healthcare-dot-Gov-Notebooks,Jupyter Notebook,"Healthcare-dot-Gov-Notebooks
This repo houses visualizations, exploratory analysis, and models from healthcare.gov data.
2017 Individual Dental Insurance Market Analysis
2017 Individual Medical Insurance Market Analysis
Dental Insurance Premiums
The charts below come from an analysis of the 2017 QHP Landscape Individual Market Dental dataset.
State Premiums
First, we can take a look at premiums by state. Note that the healthcare.gov data comes from the Federal exchanges and thus do not include several states which set up their own exchanges (e.g. California).

You can see an interactive version of this visualization here.
If we want a more detailed look at the state data, the bar chart below lays out all the state data in a more quantitative format.

We can also break the state data down further by county. Note that there seem to be few major differences within states, with a some exceptions.

For instance, the Las Vegas metro area appears to have less expensive dental insurance options than northern Nevada. It also appears that eastern and central North Carolina have cheaper options than the more rural western / Appalachian region of North Carolina. Similarly, western Oregon and its metro areas along the I-5 corridor (Portland, Eugene, Salem) have lower monthly premiums than the more rural / mountainous portions of the state in the East.
You can see an interactive version of the county data here.
Issuers
This chart shows the average monthly premium for the top 20 dental insurance issuers.

We can see Dominion and Humana offer some of the least expensive plans. However, this should be taken with the caveat that this data does not factor in which states / localities these insurers operate in. Nor does it examine quality of coverage.
Population Density
Population density appears to play a slight role in premiums. States with higher population densities tend to have lower premiums. At the extreme, we can see states like Alaska (which has the lowest popuulation density) have significantly higher premiums.

Individual Medical Insurance Premiums
The following visuals look at the individual medical insurance market. This is a work in progress.


Other State Data
Master List - Other State Exchanges
Covered California
",2
walterjgsp/algorithms,C++,"


The documentation for this project can be found at: https://walterjgsp.github.io/algorithms/
Repository for algorithms, problems and data structures that we have used. This serves as a reference for everyone interested.
For any suggestion on algorithm, problem or data structure, just open an issue with question label.
This repo contains questions from LeetCode (most of them), HackerRank, Interviewbit, URI, Codeforces.
We suggest you to try their platform so you can get new problems and articles about algorithms.
Run C++
All the C++ code were done using the C++14 version. To compile use the following command:
g++ -std=c++14 <INPUT_FILE_NAME> -o <OUTPUT_FILE_NAME>
And to run the code:
./<OUTPUT_FILE_NAME>
Run Kotlin
First is necessary to install Kotlin command line kotlinc
To compile use the following command:
kotlinc <INPUT_FILE_NAME> -include-runtime -d <OUTPUT_FILE_NAME>.jar
And to run the code:
java -jar <OUTPUT_FILE_NAME>
Services
Services that i decided to use in this repository so i could test a more professional way of working:

SonarQube
TravisCI
CodeFactor
Codacy

",2
angular/angular,TypeScript,"



Angular
Angular is a development platform for building mobile and desktop web applications using Typescript/JavaScript and other languages.
Quickstart
Get started in 5 minutes.
Changelog
Learn about the latest improvements.
Want to help?
Want to file a bug, contribute some code, or improve documentation? Excellent! Read up on our
guidelines for contributing and then check out one of our issues in the hotlist: community-help.
",48123
realityforge/domgen,Ruby,"Domgen
For documentation see the website.
",5
JLHwung/react-props-viewer,JavaScript,"react-props-viewer





usage
import React from 'react'
import { render } from 'react-dom'
import PropsViewer from 'react-props-viewer'

const TodoComponent = PropsViewer('TodoComponent')

const props = {
    count: 43,
    todos: [{
        id: '1',
        text: 'brunch'
    }, {
        id: '2',
        text: 'grocery'
    }]
}
render(<TodoComponent {...props} />, document.getElementById('main'))
screenshot

",3
ShahanaFarooqui/RTL,TypeScript,"Ride The Lightning (RTL)



Stable Release: v0.3.1
Intro -- Application Features -- Road Map -- LND API Coverage -- Application Configurations

Introduction
Architecture
Prerequisites
Installation
Prep For Execution
Start The Server
Access The Application
Troubleshooting

Introduction
RTL is a full function, device agnostic web user interface for Lightning Network Daemon, to help manage lightning node operations.
Lightning Network Daemon is an implementation of Lightning Network BOLT protocol by Lightning Labs.
Pre-requisite for running RTL is a functioning and synced LND node. You can setup your own node, by following the below guides:

Windows/Mac users can follow Pierre Rochard's Node Launcher
Linux or Raspberry Pi users can follow Stadicus's guide

RTL source code is available at this repo
For detailed screenshots and UI operation guide you can visit our medium post
RTL is already available on:

RaspiBlitz
Nodl
BTCPayserver

Docker Image: https://hub.docker.com/r/shahanafarooqui/rtl
Architecture

Prerequisites

Functioning and synced LND lightning node.
Node.js, which can be downloaded here

On Ubuntu, g++ is required to install the node-sass dependency. This is available in the build-essential package.

The Most recent versions of node.js might give errors while installing node-sass. Use node.js LTS version 8 or 10 as a solution.




Recommended Browsers: Chrome, Firefox, MS Edge

Installation
First time setup

Fetch sources from the RTL git repository, by executing the below on the command prompt:

$ git clone https://github.com/ShahanaFarooqui/RTL.git

Change directory to RTL folder:

$ cd RTL

Fetch the dependencies and build the application by running:

$ npm install
Or: Update existing build
$ cd RTL
$ git reset --hard HEAD
$ git clean -f -d
$ git pull
$ npm install

Prep for Execution
RTL requires its own config file RTL.conf, to start the server and provide user authentication on the app.

Rename sample-RTL.conf file to RTL.conf.
Locate the complete path of the readable macroon file (admin.macroon) on your node and the lnd.conf file.
Modify the RTL.conf file per the example file below

Example RTL.conf:
[Authentication]
macaroonPath=C:\Users\<user>\AppData\Local\Lnd\data\chain\bitcoin\testnet
nodeAuthType=CUSTOM
lndConfigPath=C:\Users\<user>\AppData\Local\Lnd\lnd.conf
rtlPass=***

[SSO]
rtlSSO=0
rtlCookiePath=C:\RTL\cookies\auth.cookie
logoutRedirectLink=/login

[Settings]
flgSidenavOpened=true
flgSidenavPinned=true
menu=Vertical
menuType=Regular
theme=dark-blue
satsToBTC=false
lndServerUrl=https://192.168.0.0:8080/v1
bitcoindConfigPath=
enableLogging=false
port=3000

For details on all the configuration options refer to this page.
User Authentication on RTL
RTL requires the user to be authenticated by the application first, before allowing access to LND functions.
There are two options to configure authentication on RTL, depending on the nodeAuthtype value provided in RTL.conf.

Option 1: nodeAuthType=DEFAULT; Password provided in lnd.conf for the rpc setting for bitcoind will be used for authentication.
Option 2: nodeAuthType=CUSTOM; Specific password must be provided in RTL.conf (in plain text) for authentication. Password should be set with rtlPass=<user defined> in the [Authentication] section of RTL.conf

Start the Server
Run the following command:
$ node rtl
If the server started successfully, you should get the below output on the console:
$ Server is up and running, please open the UI at http://localhost:3000
Optional: Running RTL as a service (Rpi or Linux platform users)
In case you are running a headless Rpi or a Linux node, you can configure RTL as a service.

Create RTL systemd unit and with the following content. Save and exit.

# Raspibolt RTL: systemd unit for RTL
# /etc/systemd/system/RTL.service

[Unit]
Description=RTL daemon
Wants=lnd.service
After=lnd.service

[Service]
ExecStart=/usr/bin/node <Full path of the RTL folder>/rtl
User=<user>
Restart=always
TimeoutSec=120
RestartSec=30

[Install]
WantedBy=multi-user.target

enable and start RTL

$ sudo systemctl enable RTL
$ sudo systemctl start RTL


montior the RTL log file in realtime(exit with Ctrl-C)

$ sudo journalctl -f -u RTL
Accessing the Application
You can access the application in multiple setups (Please make note of the 4th exception):


Same device as the server:
Open your browser at the following address: http://localhost:3000 to access the RTL application.


Remotely from another device on the same local network (home network) as the node(RTL server+LND running on the same device):



Ensure that the if a firewall running on your node, it allows access on port 3000 (or the custom port configured for RTL).
Determine the IP address of your node to access the application.
E.g. if the IP address of your node is 192.168.0.15 then open your browser at the following address: http://192.168.0.15:3000 to access RTL.



Config tweaks for running RTL server and LND on separate devices on the same network can be found here.


Any Other setup: Please be advised, if you are accessing your node remotely via RTL, its critical to encrypt the communication via use of https. You can use solutions like nginx and letsencrypt or TOR to setup secure access for RTL.



Sample SSL setup guide can be found here
(For advanced users) A sample SSL guide to serve remote access over an encrypted Tor connection can be found here

Troubleshooting
In case you are running into issues with the application or if you have feedback, feel free to open issues on our github repo.
You can also reach out to us via twitter DM on @Sauby_k or @RTL_App. Thanks for your interest.
",149
ShahanaFarooqui/RTL,TypeScript,"Ride The Lightning (RTL)



Stable Release: v0.3.1
Intro -- Application Features -- Road Map -- LND API Coverage -- Application Configurations

Introduction
Architecture
Prerequisites
Installation
Prep For Execution
Start The Server
Access The Application
Troubleshooting

Introduction
RTL is a full function, device agnostic web user interface for Lightning Network Daemon, to help manage lightning node operations.
Lightning Network Daemon is an implementation of Lightning Network BOLT protocol by Lightning Labs.
Pre-requisite for running RTL is a functioning and synced LND node. You can setup your own node, by following the below guides:

Windows/Mac users can follow Pierre Rochard's Node Launcher
Linux or Raspberry Pi users can follow Stadicus's guide

RTL source code is available at this repo
For detailed screenshots and UI operation guide you can visit our medium post
RTL is already available on:

RaspiBlitz
Nodl
BTCPayserver

Docker Image: https://hub.docker.com/r/shahanafarooqui/rtl
Architecture

Prerequisites

Functioning and synced LND lightning node.
Node.js, which can be downloaded here

On Ubuntu, g++ is required to install the node-sass dependency. This is available in the build-essential package.

The Most recent versions of node.js might give errors while installing node-sass. Use node.js LTS version 8 or 10 as a solution.




Recommended Browsers: Chrome, Firefox, MS Edge

Installation
First time setup

Fetch sources from the RTL git repository, by executing the below on the command prompt:

$ git clone https://github.com/ShahanaFarooqui/RTL.git

Change directory to RTL folder:

$ cd RTL

Fetch the dependencies and build the application by running:

$ npm install
Or: Update existing build
$ cd RTL
$ git reset --hard HEAD
$ git clean -f -d
$ git pull
$ npm install

Prep for Execution
RTL requires its own config file RTL.conf, to start the server and provide user authentication on the app.

Rename sample-RTL.conf file to RTL.conf.
Locate the complete path of the readable macroon file (admin.macroon) on your node and the lnd.conf file.
Modify the RTL.conf file per the example file below

Example RTL.conf:
[Authentication]
macaroonPath=C:\Users\<user>\AppData\Local\Lnd\data\chain\bitcoin\testnet
nodeAuthType=CUSTOM
lndConfigPath=C:\Users\<user>\AppData\Local\Lnd\lnd.conf
rtlPass=***

[SSO]
rtlSSO=0
rtlCookiePath=C:\RTL\cookies\auth.cookie
logoutRedirectLink=/login

[Settings]
flgSidenavOpened=true
flgSidenavPinned=true
menu=Vertical
menuType=Regular
theme=dark-blue
satsToBTC=false
lndServerUrl=https://192.168.0.0:8080/v1
bitcoindConfigPath=
enableLogging=false
port=3000

For details on all the configuration options refer to this page.
User Authentication on RTL
RTL requires the user to be authenticated by the application first, before allowing access to LND functions.
There are two options to configure authentication on RTL, depending on the nodeAuthtype value provided in RTL.conf.

Option 1: nodeAuthType=DEFAULT; Password provided in lnd.conf for the rpc setting for bitcoind will be used for authentication.
Option 2: nodeAuthType=CUSTOM; Specific password must be provided in RTL.conf (in plain text) for authentication. Password should be set with rtlPass=<user defined> in the [Authentication] section of RTL.conf

Start the Server
Run the following command:
$ node rtl
If the server started successfully, you should get the below output on the console:
$ Server is up and running, please open the UI at http://localhost:3000
Optional: Running RTL as a service (Rpi or Linux platform users)
In case you are running a headless Rpi or a Linux node, you can configure RTL as a service.

Create RTL systemd unit and with the following content. Save and exit.

# Raspibolt RTL: systemd unit for RTL
# /etc/systemd/system/RTL.service

[Unit]
Description=RTL daemon
Wants=lnd.service
After=lnd.service

[Service]
ExecStart=/usr/bin/node <Full path of the RTL folder>/rtl
User=<user>
Restart=always
TimeoutSec=120
RestartSec=30

[Install]
WantedBy=multi-user.target

enable and start RTL

$ sudo systemctl enable RTL
$ sudo systemctl start RTL


montior the RTL log file in realtime(exit with Ctrl-C)

$ sudo journalctl -f -u RTL
Accessing the Application
You can access the application in multiple setups (Please make note of the 4th exception):


Same device as the server:
Open your browser at the following address: http://localhost:3000 to access the RTL application.


Remotely from another device on the same local network (home network) as the node(RTL server+LND running on the same device):



Ensure that the if a firewall running on your node, it allows access on port 3000 (or the custom port configured for RTL).
Determine the IP address of your node to access the application.
E.g. if the IP address of your node is 192.168.0.15 then open your browser at the following address: http://192.168.0.15:3000 to access RTL.



Config tweaks for running RTL server and LND on separate devices on the same network can be found here.


Any Other setup: Please be advised, if you are accessing your node remotely via RTL, its critical to encrypt the communication via use of https. You can use solutions like nginx and letsencrypt or TOR to setup secure access for RTL.



Sample SSL setup guide can be found here
(For advanced users) A sample SSL guide to serve remote access over an encrypted Tor connection can be found here

Troubleshooting
In case you are running into issues with the application or if you have feedback, feel free to open issues on our github repo.
You can also reach out to us via twitter DM on @Sauby_k or @RTL_App. Thanks for your interest.
",149
minio/minio,Go,"MinIO Quickstart Guide
  
MinIO is an object storage server released under Apache License v2.0. It is compatible with Amazon S3 cloud storage service. It is best suited for storing unstructured data such as photos, videos, log files, backups and container / VM images. Size of an object can range from a few KBs to a maximum of 5TB.
MinIO server is light enough to be bundled with the application stack, similar to NodeJS, Redis and MySQL.
Docker Container
Stable
docker pull minio/minio
docker run -p 9000:9000 minio/minio server /data

Edge
docker pull minio/minio:edge
docker run -p 9000:9000 minio/minio:edge server /data

Note: Docker will not display the autogenerated keys unless you start the container with the -it(interactive TTY) argument. Generally, it is not recommended to use autogenerated keys with containers. Please visit MinIO Docker quickstart guide for more information here
macOS
Homebrew
Install minio packages using Homebrew
brew install minio/stable/minio
minio server /data

NOTE: If you previously installed minio using brew install minio then it is recommended that you reinstall minio from minio/stable/minio official repo instead.

brew uninstall minio
brew install minio/stable/minio
Binary Download



Platform
Architecture
URL




Apple macOS
64-bit Intel
https://dl.min.io/server/minio/release/darwin-amd64/minio



chmod 755 minio
./minio server /data
GNU/Linux
Binary Download



Platform
Architecture
URL




GNU/Linux
64-bit Intel
https://dl.min.io/server/minio/release/linux-amd64/minio



wget https://dl.min.io/server/minio/release/linux-amd64/minio
chmod +x minio
./minio server /data



Platform
Architecture
URL




GNU/Linux
ppc64le
https://dl.min.io/server/minio/release/linux-ppc64le/minio



wget https://dl.min.io/server/minio/release/linux-ppc64le/minio
chmod +x minio
./minio server /data
Microsoft Windows
Binary Download



Platform
Architecture
URL




Microsoft Windows
64-bit
https://dl.min.io/server/minio/release/windows-amd64/minio.exe



minio.exe server D:\Photos
FreeBSD
Port
Install minio packages using pkg
pkg install minio
sysrc minio_enable=yes
sysrc minio_disks=/home/user/Photos
service minio start
Install from Source
Source installation is only intended for developers and advanced users. If you do not have a working Golang environment, please follow How to install Golang. Minimum version required is go1.12
GO111MODULE=on go get github.com/minio/minio
Allow port access for Firewalls
By default MinIO uses the port 9000 to listen for incoming connections. If your platform blocks the port by default, you may need to enable access to the port.
iptables
For hosts with iptables enabled (RHEL, CentOS, etc), you can use iptables command to enable all traffic coming to specific ports. Use below command to allow
access to port 9000
iptables -A INPUT -p tcp --dport 9000 -j ACCEPT
service iptables restart
Below command enables all incoming traffic to ports ranging from 9000 to 9010.
iptables -A INPUT -p tcp --dport 9000:9010 -j ACCEPT
service iptables restart
ufw
For hosts with ufw enabled (Debian based distros), you can use ufw command to allow traffic to specific ports. Use below command to allow access to port 9000
ufw allow 9000
Below command enables all incoming traffic to ports ranging from 9000 to 9010.
ufw allow 9000:9010/tcp
firewall-cmd
For hosts with firewall-cmd enabled (CentOS), you can use firewall-cmd command to allow traffic to specific ports. Use below commands to allow access to port 9000
firewall-cmd --get-active-zones
This command gets the active zone(s). Now, apply port rules to the relevant zones returned above. For example if the zone is public, use
firewall-cmd --zone=public --add-port=9000/tcp --permanent
Note that permanent makes sure the rules are persistent across firewall start, restart or reload. Finally reload the firewall for changes to take effect.
firewall-cmd --reload
Test using MinIO Browser
MinIO Server comes with an embedded web based object browser. Point your web browser to http://127.0.0.1:9000 ensure your server has started successfully.

Test using MinIO Client mc
mc provides a modern alternative to UNIX commands like ls, cat, cp, mirror, diff etc. It supports filesystems and Amazon S3 compatible cloud storage services. Follow the MinIO Client Quickstart Guide for further instructions.
Pre-existing data
When deployed on a single drive, MinIO server lets clients access any pre-existing data in the data directory. For example, if MinIO is started with the command  minio server /mnt/data, any pre-existing data in the /mnt/data directory would be accessible to the clients.
The above statement is also valid for all gateway backends.
Explore Further

MinIO Erasure Code QuickStart Guide
Use mc with MinIO Server
Use aws-cli with MinIO Server
Use s3cmd with MinIO Server
Use minio-go SDK with MinIO Server
The MinIO documentation website

Contribute to MinIO Project
Please follow MinIO Contributor's Guide
License

",16110
TeslaCloud/flux-ce,Lua,"
Flux is a WIP gamemode framework designed with performance and convenience in mind.
Alpha release
Current version of Flux is currently in active development as an open alpha. This means that you can install it and it will run, but there will almost inevitably be bugs and issues, as well as a lot of missing features. If you are not a developer, it is probably better for you to wait until Flux is in beta.
Installation
If you want to just get Flux up and running, simply download this repo, as well as the reborn schema, and match the repo folders with your Garry's Mod dedicated server folders. You can also clone this repo directly and then install the server on top, so that you get an easy way to update without hassle.
Flux is only guaranteed to work on dedicated servers (srcds). We do not support ""listen"" servers (launching from Garry's Mod client).
Database setup
Depending on your use case, you may want to setup a database. An SQLite is the default option and requires no further setup. It is perfect if you simply want to take a look at Flux and how it works. If you want to run Flux in production, however, you should consider setting up a MySQL (MariaDB) or PostgreSQL database.
Follow the instructions in /garrysmod/gamemodes/flux/config/database.yml to learn more.
Environment
By default, Flux comes with production environment pre-chosen. It is good if you don't want to write code. If you plan on writing plugins, schemas or modifying the framework, you should set your environment to development. No other environments are supported yet! If you wish to change your environment, copy the gamemodes/flux/config/environment.lua file as environment.local.lua and change production to development inside that file.
What is the difference between production and development?
In production, code runs a little bit faster, but it sacrifices error-tolerance and refreshability. It it perfect when you are running your server properly, because in that case you don't want to refresh the code anyway (since it causes a lot of lag).
In development, code runs slower, but is a lot more tolerant to errors. It uses safe mode on hooks and print lots of useful debug information, such as load order. Due to the speed sacrifice, it is only practical to run development when actually developing.
Upgrading
During Alpha, the database may break between versions. This will be different in beta and beyond, but until then, if you are upgrading Flux you need to recreate the database manually every time.
To do that, simply follow the steps below:

Delete the /garrysmod/gamemodes/**your_schema**/db/ folder.
Follow the database-specific instructions below:

SQLite

Simply delete the /garrysmod/sv.db file.
Restart the server.

MariaDB (MySQL)

Open the MySQL console (mysql command on Linux) or any other means of managing your database.
Drop the table specified in /garrysmod/gamemodes/flux/config/database[.local].yml. To do that from console, simply run DROP DATABASE database_name_here;, replace database_name_here with your database name.
Create a new database. To do that, run CREATE DATABASE database_name_here;, replace database_name_here with your database name.
Restart the server.

PostgreSQL

Open psql or pgAdmin (sudo -u postgres psql).
Drop the table specified in /garrysmod/gamemodes/flux/config/database[.local].yml. To do that from psql, simply run DROP DATABASE database_name_here;, replace database_name_here with your database name.
Create a new database. To do that, run CREATE DATABASE database_name_here;, replace database_name_here with your database name.
Restart the server.

Playing
If you wish to play the gamemode, you should install the content addon to prevent purple-black checkers where the materials should be. You can find it here: https://steamcommunity.com/sharedfiles/filedetails/?id=1518849094
Other info
For more info or technical support, please visit our forums: http://f.teslacloud.net/
",12
Lombiq/Combinator,C#,"Combinator Orchard module Readme
Project Description
An Orchard CMS module that combines and minifies external stylesheets and javascript files to cut down on load times.
Features

Combines and minifies css files
Combines and minifies javascript files
If local and remote resources are mixed (like a local js files with one from a CDN) preserves their original order
Preserves conditional resources and minifies (if multiple with the same condition are after each other, also combines) them
Can combine remote (CDN) resources
Can embed images into stylesheets as data urls
Experimental image sprite generation support
Resource sets can be defined for better client-side caching: you can create sets of resources that are combined separately (e.g. all jQuery scripts can be in their individual file)
Ability to share processed resources between tenants in a multi-tenant application so a set of resources is only processed once, not for every tenant (resource sharing)
Busts browser cache when resources are updated (with a query string parameter containing a time stamp)
Ability to set custom resource domain
Exposing resource processing events
LESS and SASS preprocessors, contribution of Onestop Internet, Inc.
Command line command for emptying cache (""combinator empty"")
Info comment in bundled resources about which resources were combined
Tuned to be fast
With custom IStorageProvider can work in cloud hosting too (if there is no write access to the Media folder anyway)
Import/export settings
Administration page:

Adjust combination exclusion filter
Enable/disable combination of CDN resources
Set up resource domain
Enable/disable minification and adjust exclusion filter
Enable/disable image embedding and adjust exclusion filter
Enable/disable image sprite generation
Define resource sets
Enable/disable for admin site
Empty cache



The module is also available for DotNest sites.
You can download an install the module from the Orchard Gallery.
For known issues and future plans please see the Issue Tracker.
Please make sure to read the Documentation!
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/combinator (Mercurial repository)
https://github.com/Lombiq/Combinator (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",6
blumareks/containers-2019,None,"containers-2019
The workshop on containers in Silicon Valley. If you like it, please star it!
This page is here:
ibm.biz/sv-containers-2019
Start with the lite account
Sign up for the IBM Cloud lite account: cloud.ibm.com
Creating the Kubernetes cluster
https://cloud.ibm.com/docs/tutorials/multi-region-k8s-cis.html#resilient-and-secure-multi-region-kubernetes-clusters-with-cloud-internet-services
Couple words on the enterprise grade private docker image repo
Some steps to use private registry  -

https://console.bluemix.net/docs/services/Registry/registry_setup_cli_namespace.html#registry_setup_cli_namespace
https://console.bluemix.net/docs/services/Registry/registry_images_.html#registry_images_
signing images: https://console.bluemix.net/docs/services/Registry/registry_trusted_content.html#registry_trustedcontent
enforcing trusted images: https://console.bluemix.net/docs/servBd2HGQices/Registry/registry_security_enforce.html#security_enforce
Vulnerability advisor https://console.bluemix.net/docs/services/va/va_index.html#va_index
https://console.bluemix.net/docs/services/Registry/registry_trusted_content.html#registry_trustedcontent

scan images and verify signatures
image trust https://console.bluemix.net/docs/services/Registry/registry_trusted_content.html#registry_trustedcontent
locking down Kubernetes with proper RBACs

https://cloud.ibm.com/docs/tutorials/users-teams-applications.html#assign-roles-within-the-environment
https://kubernetes.io/docs/reference/access-authn-authz/rbac/

discussing resilient multi-region Kubernetes cluster
the simple example of deploying multi-region K8s cluster on IBM Cloud:

https://console.bluemix.net/docs/tutorials/multi-region-k8s-cis.html?pos=2#resilient-and-secure-multi-region-kubernetes-clusters-with-cloud-internet-services
https://www.ibm.com/blogs/bluemix/2018/06/multi-region-kubernetes-applications-ibm-cloud-internet-services/

Knative
Checkout the docs: https://www.knative.dev/docs/
And clone the following example on IBM Cloud: git clone https://github.com/IBM-Cloud/knative-node-deploy

some docs: https://www.knative.dev/docs/

IBM Cloud Private guide

Please find the architecture details on IBM Cloud Private -  https://github.com/ibm-cloud-architecture/refarch-privatecloud/blob/master/README.md
terraform process to install the ICP: https://github.com/ibm-cloud-architecture/terraform-module-icp-deploy/blob/master/README.md

previous meetups on containers
in 2019 we had these webinars on containers:

2019.02.06 Docker, Kubernetes, Istio and Knative
2019.02.06 Best Practices for Kubernetes

in 2018 we ran those webinars on containers:

2018.11.13 How to write a ""Hello World"" in Container technology using Docker, Kubernetes and Istio
2018.11.06 Securing Containers with Istio - cohosted with CodeFresh
2018.10.23 Container Security with NeuVector and IBM Cloud
2018.07.10 Introduction to Istio
2018.07.03 Introduction to Kubernetes
2018.06.26 Introduction to Docker
2018.06.19 Introduction to Containers

subscribe for more
If you want to get updates on this repository please star it. Follow me on Twitter @blumareks. Thank you for your interest in this repo.
",5
APIs-guru/awesome-openapi3,JavaScript,"awesome-openapi3 
A list of awesome projects related to OpenAPI 3.0.x.




Why not make your project discoverable by using the topic openapi3 on GitHub and using the hashtags #openapi3 and #OASv3 on social media?
Tools

Please see APIs.guru Awesome-OpenAPI3

Contributing
The best way to get your project added to the list is to tag it with the github topic openapi3.
Pull requests should only be for visual / functional changes, or projects/products not hosted on GitHub.
API Access

categories.json
tools.json

The raw data contains OpenAPI 2.0 and Swagger 1.x-related projects.
RSS Feed

feed.xml

",325
jongold/figma-js,TypeScript,"Figma.js
A simple wrapper for the Figma API
Cool projects using this:

figma-graphql

Usage
Full documentation is available on the web, and most everything is typed with Typescript.
Creating a client
Quickest start is to grab a personal access token from your Figma account settings page
import * as Figma from 'figma-js';

const token = '12345';

const client = Figma.Client({
  personalAccessToken: token
});
Alternatively, if you're building an app with OAuth authentication, after you get back the OAuth access token…
import * as Figma from 'figma-js';

const token = '12345';

const client = Figma.Client({
  accessToken: token
});
Doing cool things
Once you have instantiated a client, have fun!
client.file('file-id').then(({ data }) => {
  console.log(data);
});
Just reusing types
All of the types in the Figma file format / API are exported.
import * as Figma from 'figma-js';

const textNode: Figma.Text = {
  // … this should autocomplete if your editor is set up for it!
};
Contributing
We used the typescript-starter repo for this - refer to its README for more detailed instructions.
Helpful development commands:
yarn watch
yarn docs
yarn docs:publish

Contributions welcomed

 generate types automatically
 add tests

Committing
yarn global add commitizen

# instead of git commit

git cz

Contributors

@jongold (Airbnb)

",244
glotzerlab/signac-dashboard,Python," signac-dashboard: data visualization for signac







Built on top of the signac framework, signac-dashboard allows users to rapidly visualize and analyze data managed in a signac project.
Resources

Dashboard topic guide:
Introduction to signac-dashboard.
Dashboard documentation:
Package reference and APIs.
Dashboard examples:
Example dashboards demonstrating a variety of use cases.
Framework documentation:
Examples, tutorials, topic guides, and package Python APIs.
Chat Support:
Get help and ask questions on the signac gitter channel.
signac website:
Framework overview and news.

Installation
The recommended installation method for signac-dashboard is through conda or pip.
The software is tested for Python 3.5+ and is built for all major platforms.
To install signac-dashboard via the conda-forge channel, execute:
conda install -c conda-forge signac-dashboard
To install signac-dashboard via pip, execute:
pip install signac-dashboard
Detailed information about alternative installation methods can be found in the documentation.
Quickstart
In an existing signac project directory, create a file dashboard.py:
from signac_dashboard import Dashboard
from signac_dashboard.modules import StatepointList, DocumentList, ImageViewer

if __name__ == '__main__':
    modules = [StatepointList(), DocumentList(), ImageViewer()]
    Dashboard(modules=modules).main()
Then launch the dashboard:
$ python dashboard.py run
",6
ScoopInstaller/Main,PowerShell,"Scoop Main
Core manifests for Scoop, the Windows command-line installer.
How do I install these manifests?
Just scoop install <manifest>. This is the default bucket for Scoop and is added by default.
",88
jdayllon/contratacionestado,None,"contratacionestado
El objetivo del proyecto es generar un dataset sobre contratación del sector público para facilitar la reutilización y el análisis de la información que genera este tipo de contratación
Sobre la información recogida


La información en el repositorio procede de las páginas web de los organismos:

Junta de Andalucía (http://www.juntadeandalucia.es)
Generalitat de Catalunya. (https://contractaciopublica.gencat.cat/)



Las transformaciones aplicadas buscan facilitar el proceso de la información o enriquecer la información ofrecida por las plataformas anteriores.


La estructura de los datos se está adaptando a un modelo inspirado en CODICE 2.1 / UBL
** Más información en: https://contrataciondelestado.es/wps/portal/codice


Propiedad Intelectual
Junta de Andalucía
Tal y como se cita en el aviso legal del Portal de la Junta de Andalucía:
La Junta de Andalucía promueve el libre uso y reutilización de los textos disponibles en el presente Portal sobre los que ostenta derechos de propiedad intelectual. Dichos textos están disponibles a través de una licencia-tipo Creative Commons Reconocimiento 3.0.
De modo general, la Junta de Andalucía te autoriza a:

Copiar, redistribuir y comunicar públicamente los textos del Portal.
Hacer un uso comercial de los contenidos.
Generar obras derivadas.

He impone las siguientes obligaciones:

Reconocer explícitamente la fuente de información Identificada en el presente texto.
Incluir la misma obligación de reconocimiento en los términos de licencia de cualquier producto derivado que haga uso de esta información. * Por lo que usuarios de esta información deben tener presente esta obligación *
No desnaturalizar el sentido de la información reproducida.
Evitar cualquier rasgo de presentación que sugiera que la Junta de Andalucía apoya o promueve el uso que se hace de la información difundida. En ningún caso está permitida la reproducción de logotipos, escudos, símbolos y marcas identificativas de la Junta de Andalucía sin autorización expresa de la institución.

Generalitat de Catalunya
TODO
",2
nao-romasaga/nao-romasaga.github.io,JavaScript,"nao-romasaga.github.io
test
",2
katzNplotkin/IIT-Madras-DC-Hubs,Python,"Last Updated: Fri May 17 07:00:05 2019



Hubs
Address
Status




Hubbakkudu
dchub://10.21.42.57:1209
online


Hakuna Matata
adc://10.22.9.152:1511
offline


Konoha
dchub://10.22.33.59:1209
offline


FileStack
dchub://10.22.25.244:5110
offline


Sovngarde
dchub://10.22.19.160:511
offline


Appetite for Destruction
dchub://10.22.17.247:511
offline


GGMU
dchub://10.21.138.13:1209
offline


5050
dchub://10.22.50.50:5050
online



Note: This page is automatically generated. Internet connectivity may affect results shown

Contribution and Hub Additions
GitHub page : IIT-Madras-DC-Hubs
Webpage : IIT-Madras-DC-Hubs
For changes, fork a copy and create a pull request.
Add unlisted hubs in hublist.csv.

-- Created by ShadowKat --
",3
ropensci/drake,R,"





Usage


Release


Development






























































The drake R package 
Data analysis can be slow. A round of scientific computation can take
several minutes, hours, or even days to complete. After it finishes, if
you update your code or data, your hard-earned results may no longer be
valid. How much of that valuable output can you keep, and how much do
you need to update? How much runtime must you endure all over again?
For projects in R, the drake package can help. It analyzes your
workflow, skips
steps with up-to-date results, and orchestrates the rest with optional
distributed
computing. At the
end, drake provides evidence that your results match the underlying
code and data, which increases your ability to trust your research.
6-minute video
Visit the first page of the
manual to watch a short
introduction.




What gets done stays done.
Too many data science projects follow a Sisyphean
loop:

Launch the code.
Wait while it runs.
Discover an issue.
Rerun from scratch.

Ordinarily, it is hard to avoid rerunning the code from scratch.



But with drake, you can automatically

Launch the parts that changed since last time.
Skip the rest.

How it works
To set up a project, load your packages,
library(drake)
library(dplyr)
library(ggplot2)
#> Registered S3 methods overwritten by 'ggplot2':
#>   method         from 
#>   [.quosures     rlang
#>   c.quosures     rlang
#>   print.quosures rlang
load your custom functions,
create_plot <- function(data) {
  ggplot(data, aes(x = Petal.Width, fill = Species)) +
    geom_histogram()
}
check any supporting files (optional),
# Get the files with drake_example(""main"").
file.exists(""raw_data.xlsx"")
#> [1] TRUE
file.exists(""report.Rmd"")
#> [1] TRUE
and plan what you are going to do.
plan <- drake_plan(
  raw_data = readxl::read_excel(file_in(""raw_data.xlsx"")),
  data = raw_data %>%
    mutate(Species = forcats::fct_inorder(Species)),
  hist = create_plot(data),
  fit = lm(Sepal.Width ~ Petal.Width + Species, data),
  report = rmarkdown::render(
    knitr_in(""report.Rmd""),
    output_file = file_out(""report.html""),
    quiet = TRUE
  )
)
plan
#> # A tibble: 5 x 2
#>   target   command                                                         
#>   <chr>    <expr>                                                          
#> 1 raw_data readxl::read_excel(file_in(""raw_data.xlsx""))                   …
#> 2 data     raw_data %>% mutate(Species = forcats::fct_inorder(Species))   …
#> 3 hist     create_plot(data)                                              …
#> 4 fit      lm(Sepal.Width ~ Petal.Width + Species, data)                  …
#> 5 report   rmarkdown::render(knitr_in(""report.Rmd""), output_file = file_ou…
So far, we have just been setting the stage. Use make() to do the real
work. Targets are built in the correct order regardless of the row order
of plan.
make(plan)
#> target raw_data
#> target data
#> target fit
#> target hist
#> target report
Except for files like report.html, your output is stored in a hidden
.drake/ folder. Reading it back is easy.
readd(data) # See also loadd().
#> # A tibble: 150 x 5
#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#>          <dbl>       <dbl>        <dbl>       <dbl> <fct>  
#> 1          5.1         3.5          1.4         0.2 setosa 
#> 2          4.9         3            1.4         0.2 setosa 
#> 3          4.7         3.2          1.3         0.2 setosa 
#> 4          4.6         3.1          1.5         0.2 setosa 
#> 5          5           3.6          1.4         0.2 setosa 
#> # … with 145 more rows
You may look back on your work and see room for improvement, but it’s
all good! The whole point of drake is to help you go back and change
things quickly and painlessly. For example, we forgot to give our
histogram a bin width.
readd(hist)
#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

So let’s fix the plotting function.
create_plot <- function(data) {
  ggplot(data, aes(x = Petal.Width, fill = Species)) +
    geom_histogram(binwidth = 0.25) +
    theme_gray(20)
}
drake knows which results are affected.
config <- drake_config(plan)
vis_drake_graph(config) # Interactive graph: zoom, drag, etc.

The next make() just builds hist and report.html. No point in
wasting time on the data or model.
make(plan)
#> target hist
#> target report
loadd(hist)
hist

Reproducibility with confidence
The R community emphasizes reproducibility. Traditional themes include
scientific
replicability,
literate programming with knitr, and
version control with
git.
But internal consistency is important too. Reproducibility carries the
promise that your output matches the code and data you say you used.
With the exception of non-default
triggers and
hasty
mode,
drake strives to keep this promise.
Evidence
Suppose you are reviewing someone else’s data analysis project for
reproducibility. You scrutinize it carefully, checking that the datasets
are available and the documentation is thorough. But could you re-create
the results without the help of the original author? With drake, it is
quick and easy to find out.
make(plan)
#> All targets are already up to date.

config <- drake_config(plan)
outdated(config)
#> character(0)
With everything already up to date, you have tangible evidence of
reproducibility. Even though you did not re-create the results, you know
the results are re-creatable. They faithfully show what the code is
producing. Given the right package
environment and system
configuration,
you have everything you need to reproduce all the output by yourself.
Ease
When it comes time to actually rerun the entire project, you have much
more confidence. Starting over from scratch is trivially easy.
clean()       # Remove the original author's results.
make(plan) # Independently re-create the results from the code and input data.
#> target raw_data
#> target data
#> target fit
#> target hist
#> target report
Independent replication
With even more evidence and confidence, you can invest the time to
independently replicate the original code base if necessary. Up until
this point, you relied on basic drake functions such as make(), so
you may not have needed to peek at any substantive author-defined code
in advance. In that case, you can stay usefully ignorant as you
reimplement the original author’s methodology. In other words, drake
could potentially improve the integrity of independent replication.
Readability and transparency
Ideally, independent observers should be able to read your code and
understand it. drake helps in several ways.

The workflow plan data
frame
explicitly outlines the steps of the analysis, and
vis_drake_graph()
visualizes how those steps depend on each other.
drake takes care of the parallel scheduling and high-performance
computing (HPC) for you. That means the HPC code is no longer
tangled up with the code that actually expresses your ideas.
You can generate large collections of
targets
without necessarily changing your code base of imported functions,
another nice separation between the concepts and the execution of
your workflow

Aggressively scale up.
Not every project can complete in a single R session on your laptop.
Some projects need more speed or computing power. Some require a few
local processor cores, and some need large high-performance computing
systems. But parallel computing is hard. Your tables and figures depend
on your analysis results, and your analyses depend on your datasets, so
some tasks must finish before others even begin. drake knows what to
do. Parallelism is implicit and automatic. See the high-performance
computing guide
for all the details.
# Use the spare cores on your local machine.
make(plan, jobs = 4)

# Or scale up to a supercomputer.
drake_batchtools_tmpl_file(""slurm"") # https://slurm.schedmd.com/
library(future.batchtools)
future::plan(batchtools_slurm, template = ""batchtools.slurm.tmpl"", workers = 100)
make(plan, parallelism = ""future_lapply"")
Installation
You can choose among different versions of drake. The CRAN release
often lags behind the online
manual but may have fewer
bugs.
# Install the latest stable release from CRAN.
install.packages(""drake"")

# Alternatively, install the development version from GitHub.
install.packages(""devtools"")
library(devtools)
install_github(""ropensci/drake"")
A few technical details:

You must properly install drake using install.packages(),
devtools::install_github(), or similar. It is not enough to use
devtools::load_all(), particularly for the parallel computing
functionality, in which multiple R sessions initialize and then try
to require(drake).
For make(parallelism = ""Makefile""), Windows users may need to
download and install
Rtools.
To use make(parallelism = ""future"") or make(parallelism = ""future_lapply"") to deploy your work to a computing cluster (see
the high-performance computing
guide), you
will need the
future.batchtools
package.

Documentation
The main resources to learn drake are the user
manual and the reference
website. Others are below.
Shiny app
The Shiny app at
wlandau.shinyapps.io/drakeplanner
makes it easier to learn drake and create new drake-powered
projects. If you have trouble accessing it, you can install and run it
locally.
install.packages(""remotes"")
remotes::install_github(""wlandau/drakeplanner"")
drakeplanner::drakeplanner()
Cheat sheet
Thanks to Kirill for preparing a drake
cheat
sheet
for the workshop.
Frequently asked questions
The FAQ page is
an index of links to appropriately-labeled issues on
GitHub.
To contribute, please submit a new
issue and ask that it be
labeled as a frequently asked question.
Function reference
The reference
section lists
all the available functions. Here are the most important ones.

drake_plan(): create a workflow data frame (like my_plan).
make(): build your project.
r_make(): launch a fresh
callr::r() process to build your
project. Called from an interactive R session, r_make() is more
reproducible than make().
loadd(): load one or more built targets into your R session.
readd(): read and return a built target.
drake_config(): create a master configuration list for other
user-side functions.
vis_drake_graph(): show an interactive visual network
representation of your workflow.
outdated(): see which targets will be built in the next make().
deps(): check the dependencies of a command or function.
failed(): list the targets that failed to build in the last
make().
diagnose(): return the full context of a build, including errors,
warnings, and messages.

Tutorials
Thanks to Kirill for constructing two
interactive learnr tutorials:
one supporting drake
itself, and a
prerequisite
walkthrough of the
cooking package.
Examples
The official rOpenSci use cases and
associated discussion threads
describe applications of drake in action. Here are some more
real-world sightings of drake in the
wild.

efcaguab/demografia-del-voto
efcaguab/great-white-shark-nsw
IndianaCHE/Detailed-SSP-Reports
pat-s/pathogen-modeling
sol-eng/tensorflow-w-r
tiernanmartin/home-and-hope

There are also multiple drake-powered example projects available
here, ranging from
beginner-friendly stubs to demonstrations of high-performance computing.
You can generate the files for a project with drake_example() (e.g.
drake_example(""gsp"")), and you can list the available projects with
drake_examples(). You can contribute your own example project with a
fork and pull
request.
Presentations



Author
Venue
Date
Materials




Amanda Dobbyn
R-Ladies NYC
2019-02-12
slides, source


Will Landau
Harvard DataFest
2019-01-22
slides, source


Karthik Ram
RStudio Conference
2019-01-18
video, slides, resources


Sina Rüeger
Geneva R User Group
2018-10-04
slides, example code


Will Landau
R in Pharma
2018-08-16
video, slides, source


Christine Stawitz
R-Ladies Seattle
2018-06-25
materials


Kirill Müller
Swiss Institute of Bioinformatics
2018-03-05
workshop, slides, source, exercises



Context and history
For context and history, check out this post on the rOpenSci
blog and episode 22 of
the R
Podcast.
Help and troubleshooting
The following resources document many known issues and challenges.

Frequently-asked
questions.
Cautionary notes and edge
cases
Debugging and testing drake
projects
Other known issues
(please search both open and closed ones).

If you are still having trouble, please submit a new
issue with a bug report
or feature request, along with a minimal reproducible example where
appropriate.
The GitHub issue tracker is mainly intended for bug reports and feature
requests. While questions about usage etc. are also highly encouraged,
you may alternatively wish to post to Stack
Overflow and use the drake-r-package
tag.
Contributing
Development is a community effort, and we encourage participation.
Please read
CONTRIBUTING.md
for details.
Similar work
GNU Make
The original idea of a time-saving reproducible build system extends
back at least as far as GNU Make,
which still aids the work of data
scientists
as well as the original user base of complied language programmers. In
fact, the name “drake” stands for “Data Frames in R for Make”.
Make is used widely in reproducible
research. Below are some examples from Karl Broman’s
website.

Bostock, Mike (2013). “A map of flowlines from NHDPlus.”
https://github.com/mbostock/us-rivers. Powered by the Makefile at
https://github.com/mbostock/us-rivers/blob/master/Makefile.
Broman, Karl W (2012). “Halotype Probabilities in Advanced
Intercross Populations.” G3 2(2), 199-202.Powered by the
Makefile at
https://github.com/kbroman/ailProbPaper/blob/master/Makefile.
Broman, Karl W (2012). “Genotype Probabilities at Intermediate
Generations in the Construction of Recombinant Inbred Lines.”
*Genetics 190(2), 403-412. Powered by the Makefile at
https://github.com/kbroman/preCCProbPaper/blob/master/Makefile.
Broman, Karl W and Kim, Sungjin and Sen, Saunak and Ane, Cecile and
Payseur, Bret A (2012). “Mapping Quantitative Trait Loci onto a
Phylogenetic Tree.” Genetics 192(2), 267-279. Powered by the
Makefile at
https://github.com/kbroman/phyloQTLpaper/blob/master/Makefile.

There are several reasons for R users to prefer drake instead.

drake already has a
Make-powered parallel backend.
Just run make(..., parallelism = ""Makefile"", jobs = 2) to enjoy
most of the original benefits of
Make itself.
Improved scalability. With
Make, you must write a
potentially large and cumbersome
Makefile
by hand. But with drake, you can use wildcard
templating
to automatically generate massive collections of targets with
minimal code.
Lower overhead for light-weight tasks. For each
Make target that uses R, a
brand new R session must spawn. For projects with thousands of small
targets, that means more time may be spent loading R sessions than
doing the actual work. With make(..., parallelism = ""mclapply, jobs = 4""), drake launches 4 persistent workers up front and
efficiently processes the targets in R.
Convenient organization of output. With
Make, the user must save each
target as a file. drake saves all the results for you
automatically in a storr cache
so you do not have to micromanage the results.

Remake
drake overlaps with its direct
predecessor, remake. In fact,
drake owes its core ideas to
remake and Rich
FitzJohn.
Remake's development repository
lists several real-world
applications.
drake surpasses
remake in several important ways,
including but not limited to the following.

High-performance computing.
Remake has no native parallel
computing support. drake, on
the other hand, has a thorough selection of parallel computing
technologies and scheduling algorithms. Thanks to
future,
future.batchtools,
and batchtools, it is straightforward
to configure a drake project
for most popular job schedulers, such as
SLURM,
TORQUE, and
the Grid
Engine,
as well as systems contained in Docker
images.
A friendly interface. In
remake, the user must manually
write a
YAML
configuration file to arrange the steps of a workflow, which leads
to some of the same scalability problems as
Make.
drake's domain-specific language
easily generates workflows at scale.
Thorough documentation. drake
contains thorough user
manual, a reference
website, a comprehensive
README,
examples in the help files of user-side functions, and accessible
example code that users
can write with drake::example_drake().
Active maintenance. drake is
actively developed and maintained, and
issues are usually
addressed promptly.
Presence on CRAN. At the time of writing,
drake is available on
CRAN, but
remake is not.

Memoise
Memoization is the strategic caching of the return values of functions.
Every time a memoized function is called with a new set of arguments,
the return value is saved for future use. Later, whenever the same
function is called with the same arguments, the previous return value is
salvaged, and the function call is skipped to save time. The memoise
package is an excellent
implementation of memoization in R.
However, memoization does not go far enough. In reality, the return
value of a function depends not only on the function body and the
arguments, but also on any nested functions and global variables, the
dependencies of those dependencies, and so on upstream. drake
surpasses memoise because it uses
the entire dependency network graph of a project to decide which
pieces need to be rebuilt and which ones can be skipped.
Knitr
Much of the R community uses knitr for
reproducible research. The idea is to intersperse code chunks in an R
Markdown or *.Rnw file and then
generate a dynamic report that weaves together code, output, and prose.
Knitr is not designed to be a serious
pipeline toolkit, and
it should not be the primary computational engine for medium to large
data analysis projects.

Knitr scales far worse than
Make or
remake. The whole point is to
consolidate output and prose, so it deliberately lacks the essential
modularity.
There is no obvious high-performance computing support.
While there is a way to skip chunks that are already up to date
(with code chunk options cache and autodep), this functionality
is not the focus of knitr. It is
deactivated by default, and
remake and drake are more
dependable ways to skip work that is already up to date.

drake was designed to manage the entire workflow with
knitr reports as targets. The strategy is
analogous for knitr reports within
remake projects.
Factual’s Drake
Factual’s Drake is similar in
concept, but the development effort is completely unrelated to the
drake R package.
Other pipeline toolkits
There are countless other successful pipeline
toolkits. The drake
package distinguishes itself with its R-focused approach,
Tidyverse-friendly interface, and a thorough selection of parallel
computing technologies and scheduling
algorithms.
Acknowledgements
Special thanks to Jarad Niemi, my advisor from
graduate school, for first introducing me
to the idea of Makefiles for
research. He originally set me down the path that led to drake.
Many thanks to Julia Lowndes, Ben
Marwick, and Peter
Slaughter for reviewing drake for
rOpenSci, and to
Maëlle Salmon for such active involvement
as the editor. Thanks also to the following people for contributing
early in development.

Alex Axthelm
Chan-Yub Park
Daniel Falster
Eric Nantz
Henrik Bengtsson
Ian Watson
Jasper Clarkberg
Kendon Bell
Kirill Müller
Michael Schubert

Credit for images is attributed
here.

",820
koolshare/ddnsto,Lua,"ddnsto 是什么？

可以配置一个域名，远程到家里的私有网络
可以用网页，远程家里的电脑桌面
可以安装 EasyExplorer 与 APP，备份手机的数据到家里
用 EasyExplorer　对应的 APP 看家里的视频，不管有没有公网 IP

官网：https://www.ddnsto.com
旧：https://www.ddns.to 暂时不用
有问题请在这里反馈。
下载

MAC 桌面版本
Windows 桌面版本
QNAP插件下载并手动安装
Merlin 软件中心提供直接安装
Lede-x64 软件中心提供直接安装

Windows  病毒检测

在线病毒检测报告
绝大多数都是绿色，360 报毒应该是误报

功能使用

EasyExplorer 多平台使用教程
ddnsto远程域名教程: http://koolshare.cn/thread-123567-1-1.html
路由器插件发布：http://koolshare.cn/thread-116500-1-1.html
EasyExplorer远程文件管理教程: http://koolshare.cn/thread-129199-1-1.html
远程桌面与远程协助教程: http://koolshare.cn/thread-143102-1-1.html
Netgear NAS 插件源码: https://github.com/koolshare/readynas-easyexplorer
Lede X86 插件源码: https://github.com/koolshare/ledesoft/tree/master/easyexplorer
QNAP 插件源代码: https://github.com/koolshare/qnap-easyexplorer

动态 (2018-12-08)

支持远程 windows/ssh/telnet 并且支持分享
小程序，请搜索 koolcenter
iOS App 正在开发中，将重点突出远程播放功能。App 的开放，也导致了 DLNA + video_mux  功能的放弃

最新动态 (2019-03-03)

APP 内测中：http://koolshare.cn/thread-156616-1-1.html

APP 公测链接 (2019-04-21)

iOS点此链接测试
基础教程

",88
shd101wyy/mume,TypeScript,"MUME

This library powers:

markdown preview enhanced for atom
markdown preview enhanced for vscode

npm install --save @shd101wyy/mume
Example
// node.js
const mume = require(""@shd101wyy/mume"");

// es6
// import * as mume from ""@shd101wyy/mume""

async function main() {
  await mume.init();

  const engine = new mume.MarkdownEngine({
    filePath: ""/Users/wangyiyi/Desktop/markdown-example/test3.md"",
    config: {
      previewTheme: ""github-light.css"",
      // revealjsTheme: ""white.css""
      codeBlockTheme: ""default.css"",
      printBackground: true,
      enableScriptExecution: true, // <= for running code chunks
    },
  });

  // open in browser
  await engine.openInBrowser({ runAllCodeChunks: true });

  // html export
  await engine.htmlExport({ offline: false, runAllCodeChunks: true });

  // chrome (puppeteer) export
  await engine.chromeExport({ fileType: ""pdf"", runAllCodeChunks: true }); // fileType = 'pdf'|'png'|'jpeg'

  // phantomjs export
  await engine.phantomjsExport({ fileType: ""pdf"", runAllCodeChunks: true }); // fileType = 'pdf'|'png'|'jpeg'

  // prince export
  await engine.princeExport({ runAllCodeChunks: true });

  // ebook export
  await engine.eBookExport({ fileType: ""epub"" }); // fileType = 'epub'|'pdf'|'mobi'|'html'

  // pandoc export
  await engine.pandocExport({ runAllCodeChunks: true });

  // markdown(gfm) export
  await engine.markdownExport({ runAllCodeChunks: true });

  return process.exit();
}

main();
Markdown Engine Configuration
const config = {
  // Enable this option will render markdown by pandoc instead of markdown-it.
  usePandocParser: false,

  // In Markdown, a single newline character doesn't cause a line break in the generated HTML. In GitHub Flavored Markdown, that is not true. Enable this config option to insert line breaks in rendered HTML for single newlines in Markdown source.
  breakOnSingleNewLine: true,

  // Enable smartypants and other sweet transforms.
  enableTypographer: false,

  // Enable conversion of URL-like text to links in the markdown preview.
  enableLinkify: true,

  // Math
  mathRenderingOption: ""KaTeX"",  // ""KaTeX"" | ""MathJax"" | ""None""
  mathInlineDelimiters: [[""$"", ""$""], [""\\("", ""\\)""]],
  mathBlockDelimiters: [[""$$"", ""$$""], [""\\["", ""\\]""]],
  mathRenderingOnLineService: ""https://latex.codecogs.com/gif.latex"", // ""https://latex.codecogs.com/svg.latex"", ""https://latex.codecogs.com/png.latex""

  // Enable Wiki Link syntax support. More information can be found a  https://help.github.com/articles/adding-links-to-wikis/
  enableWikiLinkSyntax: true,
  // By default, the extension for wikilink is `.md`. For example: [[test]] will direct to file path `test.md`.
  wikiLinkFileExtension: '.md'

  // Enable emoji & font-awesome plugin. This only works for markdown-it parser, but not pandoc parser.
  enableEmojiSyntax: true

  // Enable extended table syntax to support merging table cells.
  enableExtendedTableSyntax: false

  // Enable CriticMarkup syntax. Only works with markdown-it parser.
  // Please check http://criticmarkup.com/users-guide.php for more information.
  enableCriticMarkupSyntax: false

  // Front matter rendering option
  frontMatterRenderingOption: 'none', // 'none' | 'table' | 'code block'

  // Mermaid theme
  mermaidTheme: 'mermaid.css', // 'mermaid.css' | 'mermaid.dark.css' | 'mermaid.forest.css'

  // Code Block theme
  // If `auto.css` is chosen, then the code block theme that best matches the current preview theme will be picked.
  codeBlockTheme: 'auto.css',
  //  ""auto.css"",
  //  ""default.css"",
  //  ""atom-dark.css"",
  //  ""atom-light.css"",
  //  ""atom-material.css"",
  //  ""coy.css"",
  //  ""darcula.css"",
  //  ""dark.css"",
  //  ""funky.css"",
  //  ""github.css"",
  //  ""hopscotch.css"",
  //  ""monokai.css"",
  //  ""okaidia.css"",
  //  ""one-dark.css"",
  //  ""one-light.css"",
  //  ""pen-paper-coffee.css"",
  //  ""pojoaque.css"",
  //  ""solarized-dark.css"",
  //  ""solarized-light.css"",
  //  ""twilight.css"",
  //  ""vue.css"",
  //  ""vs.css"",
  //  ""xonokai.css""

  // Preview theme
  previewTheme: 'github-light.css',
  // ""atom-dark.css"",
  // ""atom-light.css"",
  // ""atom-material.css"",
  // ""github-dark.css"",
  // ""github-light.css"",
  // ""gothic.css"",
  // ""medium.css"",
  // ""monokai.css"",
  // ""newsprint.css"",
  // ""night.css"",
  // ""none.css"",
  // ""one-dark.css"",
  // ""one-light.css"",
  // ""solarized-dark.css"",
  // ""solarized-light.css"",
  // ""vue.css""

  // Revealjs presentation theme
  revealjsTheme: ""white.css"",
  // ""beige.css"",
  // ""black.css"",
  // ""blood.css"",
  // ""league.css"",
  // ""moon.css"",
  // ""night.css"",
  // ""serif.css"",
  // ""simple.css"",
  // ""sky.css"",
  // ""solarized.css"",
  // ""white.css"",
  // ""none.css""

  // Accepted protocols for links.
  protocolsWhiteList: ""http://, https://, atom://, file://, mailto:, tel:"",

  // When using Image Helper to copy images, by default images will be copied to root image folder path '/assets'
  imageFolderPath: '/assets',

  // Whether to print background for file export or not. If set to `false`, then `github-light` preview theme will b  used. You can also set `print_background` in front-matter for individual files.
  printBackground: false,

  // PhantomJS executable path
  phantomPath: 'phantomjs',

  // Pandoc executable path
  pandocPath: 'pandoc',

  // Pandoc markdown flavor
  pandocMarkdownFlavor: ""markdown-raw_tex+tex_math_single_backslash"",

  // Pandoc arguments e.g. ['--smart', '--filter=/bin/exe']. Please use long argument names.
  pandocArguments: [],

  // Default latex engine for Pandoc export and latex code chunk.
  latexEngine: 'pdflatex',

  // Enables executing code chunks and importing javascript files.
  // ⚠ ️ Please use this feature with caution because it may put your security at risk!
  //    Your machine can get hacked if someone makes you open a markdown with malicious code while script execution is enabled.
  enableScriptExecution: false,

  // Enables transform audio video link to to html5 embed audio video tags.
  // Internally it enables markdown-it-html5-embed plugins.
  enableHTML5Embed: false,

  // Enables video/audio embed with ![]() syntax (default).
  HTML5EmbedUseImageSyntax: true,

  // Enables video/audio embed with []() syntax.
  HTML5EmbedUseLinkSyntax: false,

  // When true embed media with http:// schema in URLs. When false ignore and don't embed them.
  HTML5EmbedIsAllowedHttp: false,

  // HTML attributes to pass to audio tags.
  HTML5EmbedAudioAttributes: 'controls preload=""metadata"" width=""320""',

  // HTML attributes to pass to video tags.
  HTML5EmbedVideoAttributes: 'controls preload=""metadata"" width=""320"" height=""240""',
}

// Init Engine
const engine = new mume.MarkdownEngine({
  filePath: '...',
  projectDirectoryPath: '...',
  config: config
})
Global Configuration
Global config files are located at ~/.mume directory
Development
Visual Studio Code is recommended.

Clone this project
Run npm install from shell
Open in vscode, then cmd+shift+b to build
Run the tests with npm run test

",140
liquidcarrot/carrot,JavaScript,"























  Carrot is a flexible multi-threaded neural network AI Library for Node.js with neuro-evolution capabilities.

For Documentation, visit https://liquidcarrot.github.io/carrot
Key Features

Multi-threaded
Fully Documented with async-style Docs
Preconfigured GRU, LSTM, NARX Networks
Mutable Neurons, Layers, Groups, and Networks
Neuro-evolution with genetic algorithms
SVG Network Visualizations using D3.js

Install
$ npm i @liquid-carrot/carrot
Carrot files are hosted by GitHub Pages, just copy this link into the <head> tag:
<script src=""https://liquidcarrot.io/carrot/cdn/0.2.11/carrot.js""></script>
Getting Started
Shaping a network with neuro-evolution
let { Network, methods } = require('@liquid-carrot/carrot');

// this network learns the XOR gate (through neuro-evolution)
async function execute () {
   var network = new Network(2,1);

   // XOR dataset
   var trainingSet = [
       { input: [0,0], output: [0] },
       { input: [0,1], output: [1] },
       { input: [1,0], output: [1] },
       { input: [1,1], output: [0] }
   ];

   await network.evolve(trainingSet, {
       mutation: methods.mutation.FFW,
       equal: true,
       error: 0.05,
       elitism: 5,
       mutationRate: 0.5
   });

   network.activate([0,0]); // 0.2413
   network.activate([0,1]); // 1.0000
   network.activate([1,0]); // 0.7663
   network.activate([1,1]); // -0.008
}

execute();
Building neural networks
let Network = require('@liquid-carrot/carrot').Network

let network = new Network([2, 2, 1]) // Builds a neural network with 5 neurons: 2 + 2 + 1
Building custom network architectures
let architect = require('@liquid-carrot/carrot').architect
let Layer = require('@liquid-carrot/carrot').Layer

let input = new Layer.Dense(1);
let hidden1 = new Layer.LSTM(5);
let hidden2 = new Layer.GRU(1);
let output = new Layer.Dense(1);

// connect however you want
input.connect(hidden1);
hidden1.connect(hidden2);
hidden2.connect(output);

let network = architect.Construct([input, hidden1, hidden2, output]);
Building neurons
let Node = require('@liquid-carrot/carrot').Node

let A = new Node() // neuron
let B = new Node() // neuron

A.connect(B)
A.activate(0.5)
console.log(B.activate())
Training, Testing, and Playing Around
Data Sets
* [ ] [MNIST](https://www.npmjs.com/package/mnist)

💬 Contributing

Your contributions are always welcome! Please have a look at the contribution guidelines first. 🎉
To build a community welcome to all, Carrot follows the Contributor Covenant Code of Conduct.
And finally, a big thank you to all of you for supporting! 🤗
Planned Features
* [ ] Performance Enhancements
    * [ ] GPU Acceleration
        * [ ] Tests
        * [ ] Benchmarks
    * [ ] Matrix Multiplications
        * [ ] Tests
        * [ ] Benchmarks
    * [ ] Clustering | Multi-Threading
        * [ ] Tests
        * [ ] Benchmarks
* [ ] Syntax Support
    * [ ] Callbacks
    * [ ] Promises
    * [ ] Streaming
    * [ ] Async/Await
* [ ] Math Support
    * [ ] Big Numbers
    * [ ] Small Numbers

Patrons




Silver Patron







ProtonMail







Patron







DollarBizClub





Contributors
This project exists thanks to all the people who contribute. We can't do it without you! 🙇

Luis Carbonell
Christian Echevarria
Daniel Ryan

Acknowledgements
A special thanks to Neataptic, Synaptic, and Brain.js!
Carrot™ was largely brought about by inspiration from these great libraries.
",67
MicrosoftDocs/azure-docs.zh-cn,PowerShell,"


ms.openlocfilehash
ms.sourcegitcommit
ms.translationtype
ms.contentlocale
ms.lasthandoff
ms.locfileid




d4e9e26f2c6764cc179826266891d861514e23f6
3102f886aa962842303c8753fe8fa5324a52834a
HT
zh-CN
04/23/2019
60344083



Microsoft Azure 文档
欢迎使用 Microsoft Azure 的开源文档。 请查看此自述文件以了解如何帮助参与撰写 Microsoft Azure 文档。
入门
参与开源不仅仅是提供更新，它也让我们知道何时出现问题。 阅读我们的供稿指南以了解详细信息。
先决条件
你已决定参与，太好了！ 若要参与撰写文档，需要几个工具。
参与撰写文档需要一个 GitHub 帐户。 如果还没有帐户，请按照我们参与者指南中的 GitHub 帐户设置说明执行操作。
下载
请安装以下工具：

Git
Visual Studio Code
用于 Visual Studio Code 的 Docs 创作包扩展

安装
按照我们参与者指南的安装内容创作工具中提供的说明执行操作。
许可
请参阅 LICENSE、LICENSE-CODE 和 ThirdPartyNotices 了解所有许可信息。
行为准则
本项目采用 Microsoft 开源行为准则。
有关详细信息，请参阅行为准则常见问题解答；若有其他任何问题或意见，请联系 opencode@microsoft.com。
",40
AlibabaCloudDocs/polardb,None,"polardb
ApsaraDB for POLARDB
",8
liquidcarrot/carrot,JavaScript,"























  Carrot is a flexible multi-threaded neural network AI Library for Node.js with neuro-evolution capabilities.

For Documentation, visit https://liquidcarrot.github.io/carrot
Key Features

Multi-threaded
Fully Documented with async-style Docs
Preconfigured GRU, LSTM, NARX Networks
Mutable Neurons, Layers, Groups, and Networks
Neuro-evolution with genetic algorithms
SVG Network Visualizations using D3.js

Install
$ npm i @liquid-carrot/carrot
Carrot files are hosted by GitHub Pages, just copy this link into the <head> tag:
<script src=""https://liquidcarrot.io/carrot/cdn/0.2.11/carrot.js""></script>
Getting Started
Shaping a network with neuro-evolution
let { Network, methods } = require('@liquid-carrot/carrot');

// this network learns the XOR gate (through neuro-evolution)
async function execute () {
   var network = new Network(2,1);

   // XOR dataset
   var trainingSet = [
       { input: [0,0], output: [0] },
       { input: [0,1], output: [1] },
       { input: [1,0], output: [1] },
       { input: [1,1], output: [0] }
   ];

   await network.evolve(trainingSet, {
       mutation: methods.mutation.FFW,
       equal: true,
       error: 0.05,
       elitism: 5,
       mutationRate: 0.5
   });

   network.activate([0,0]); // 0.2413
   network.activate([0,1]); // 1.0000
   network.activate([1,0]); // 0.7663
   network.activate([1,1]); // -0.008
}

execute();
Building neural networks
let Network = require('@liquid-carrot/carrot').Network

let network = new Network([2, 2, 1]) // Builds a neural network with 5 neurons: 2 + 2 + 1
Building custom network architectures
let architect = require('@liquid-carrot/carrot').architect
let Layer = require('@liquid-carrot/carrot').Layer

let input = new Layer.Dense(1);
let hidden1 = new Layer.LSTM(5);
let hidden2 = new Layer.GRU(1);
let output = new Layer.Dense(1);

// connect however you want
input.connect(hidden1);
hidden1.connect(hidden2);
hidden2.connect(output);

let network = architect.Construct([input, hidden1, hidden2, output]);
Building neurons
let Node = require('@liquid-carrot/carrot').Node

let A = new Node() // neuron
let B = new Node() // neuron

A.connect(B)
A.activate(0.5)
console.log(B.activate())
Training, Testing, and Playing Around
Data Sets
* [ ] [MNIST](https://www.npmjs.com/package/mnist)

💬 Contributing

Your contributions are always welcome! Please have a look at the contribution guidelines first. 🎉
To build a community welcome to all, Carrot follows the Contributor Covenant Code of Conduct.
And finally, a big thank you to all of you for supporting! 🤗
Planned Features
* [ ] Performance Enhancements
    * [ ] GPU Acceleration
        * [ ] Tests
        * [ ] Benchmarks
    * [ ] Matrix Multiplications
        * [ ] Tests
        * [ ] Benchmarks
    * [ ] Clustering | Multi-Threading
        * [ ] Tests
        * [ ] Benchmarks
* [ ] Syntax Support
    * [ ] Callbacks
    * [ ] Promises
    * [ ] Streaming
    * [ ] Async/Await
* [ ] Math Support
    * [ ] Big Numbers
    * [ ] Small Numbers

Patrons




Silver Patron







ProtonMail







Patron







DollarBizClub





Contributors
This project exists thanks to all the people who contribute. We can't do it without you! 🙇

Luis Carbonell
Christian Echevarria
Daniel Ryan

Acknowledgements
A special thanks to Neataptic, Synaptic, and Brain.js!
Carrot™ was largely brought about by inspiration from these great libraries.
",67
MicrosoftDocs/azure-docs.zh-cn,PowerShell,"


ms.openlocfilehash
ms.sourcegitcommit
ms.translationtype
ms.contentlocale
ms.lasthandoff
ms.locfileid




d4e9e26f2c6764cc179826266891d861514e23f6
3102f886aa962842303c8753fe8fa5324a52834a
HT
zh-CN
04/23/2019
60344083



Microsoft Azure 文档
欢迎使用 Microsoft Azure 的开源文档。 请查看此自述文件以了解如何帮助参与撰写 Microsoft Azure 文档。
入门
参与开源不仅仅是提供更新，它也让我们知道何时出现问题。 阅读我们的供稿指南以了解详细信息。
先决条件
你已决定参与，太好了！ 若要参与撰写文档，需要几个工具。
参与撰写文档需要一个 GitHub 帐户。 如果还没有帐户，请按照我们参与者指南中的 GitHub 帐户设置说明执行操作。
下载
请安装以下工具：

Git
Visual Studio Code
用于 Visual Studio Code 的 Docs 创作包扩展

安装
按照我们参与者指南的安装内容创作工具中提供的说明执行操作。
许可
请参阅 LICENSE、LICENSE-CODE 和 ThirdPartyNotices 了解所有许可信息。
行为准则
本项目采用 Microsoft 开源行为准则。
有关详细信息，请参阅行为准则常见问题解答；若有其他任何问题或意见，请联系 opencode@microsoft.com。
",40
AlibabaCloudDocs/polardb,None,"polardb
ApsaraDB for POLARDB
",8
parkr/status,JavaScript,"status
This is a repository hosting a status site for my various web properties.
This repository uses sourcegraph/checkup to write to the updates/ directory.
Web
Normal usage of this repository is just visiting https://www.parkermoore.de/status/. It shows a lovely series of graphs for my web properties. It tracks up, down, and degraded states.
Generating
This repo uses Jess Frazelle's Docker image, r.j3ss.co/checkup to run checkup.
It is passed a configuration file like this:
{
  ""storage"": {
    ""provider"": ""github"",
    ""access_token"": ""some_api_access_token_with_repo_scope"",
    ""repository_owner"": ""owner"",
    ""repository_name"": ""repo"",
    ""committer_name"": ""Commiter Name"",
    ""committer_email"": ""you@yours.com"",
    ""branch"": ""gh-pages"",
    ""dir"": ""updates""
  },
  ""checkers"": [
    {
      ""type"": ""http"",
      ""endpoint_name"": ""Example HTTP"",
      ""endpoint_url"": ""http://www.example.com""
    }
  ]
}
Then, I run checkup on a cron. It will automatically write to GitHub.
",5
TheSpeedX/SOCKS-List,None,"SOCKSLIST
This is a SOCKS5 List
Use it If You Know What SOCKS5 is.....
This List Gets New Working Proxy every 24 hours
This List Might Have duplicate proxies in Very Rare Case!!!
HOW TO DOWNLOAD
To Download SOCKS-List No need to Clone This repository as its too big due to multiple commits
Simply Download socks.txt
For Windows
In Command Prompt(cmd) Type
 bitsadmin.exe /transfer ""SOCKS5"" https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/socks.txt %userprofile%/Desktop/socks.txt
You will Get Downloaded SOCKS List on Your Desktop
For Linux
In Terminal Type
curl -# https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/socks.txt -o socks.txt
For Others
If The Above Methods Don't Work For You Simply Open Link Here And Download it
Developer Please Give Credits, Stars, And Follow If You Use My SOCKS List
SOCKER
You can Also Use My New Tool SOCKER To Check All Valid SOCKS PROXY at the current time...
SOCKER Link : https://github.com/TheSpeedX/socker
Click Here To Go To SOCKER Page
NOTES
NOTE: It is Only For Educational Purposes. Neither I Say Nor I Promote To Do Anything Illegal.
This List Was Created By SpeedX
For Any Queries Join Me On WhatsApp!!!
Group Link: http://bit.do/thespeedxgit
Join My Group
YouTube Channel: https://www.youtube.com/c/GyanaTech
Check My Channel
To Support Me By Either Helping In Project Or Donating Small Amount To Me For That Contact Me By
      Mail: ggspeedx29@gmail.com

That's All !!!
This List Was Generated By SpeedX
CONTACT
For Any Queries Join Me On WhatsApp!!!
Group Link: http://bit.do/thespeedxgit
Join My Group
       Mail: ggspeedx29@gmail.com

       YouTube Channel: https://www.youtube.com/c/GyanaTech

Check My Channel
",10
indieweb/indieweb-chat-archive,PHP,"IndieWeb Chat Archive
This repo contains the full archive of IndieWeb chat log data files visible at https://chat.indieweb.org
Chat logs are added to this repo every 15 minutes.
File Format
Each channel's files can be read using QuartzDB. The files follow a simple format:
2017-12-01 23:15:06.218000 {""type"":""message"",""timestamp"":1512170106.218,""network"":""irc"",""server"":""freenode"",""channel"":{""uid"":""#indieweb"",""name"":""#indieweb""},""author"":{""uid"":""Loqi"",""nickname"":""Loqi"",""username"":""Loqi"",""name"":""Loqi"",""photo"":null,""url"":null,""tz"":""US\/Pacific""},""content"":""[@indiewebcamp] This week in the #indieweb https://indieweb.org/this-week/2017-12-01.html https://pbs.twimg.com/media/DP_z5rCVwAAGdTk.jpg (http://twtr.io/1Yx4r5CHSBC)"",""modes"":[]}


Each line begins with the timestamp.
There will always be 26 characters followed by a space.
The timestamp is UTC and has 6 digits of precision for the seconds.
The rest of the line is a JSON-encoded string representing the IRC message and who sent it.

Spam removal
For a guide on how we deal with spam in these logs, see IRC#Spam on the wiki.
",3
znmeb/silver-potato,Shell,"Data Science Pet Containers
M. Edward (Ed) Borasky znmeb@znmeb.net, 2019-05-16
Update - 16 May 2019

I’ve cleaned out the old docs inherited from Hack Oregon. That
repository is still there and being maintained, so this fork will
have its own docs in the near future.
The reference implementation is Fedora Silverblue 30. I’m also
testing on Clear Linux, another container-centric Linux distro made
by my neighbors at Intel in Hillsboro, Oregon. It’s highly optimized
for Intel processors, of course. It should work anywhere Docker
runs. Even Windows 10 Pro. I hope. :-)
PostgreSQL now goes to 11!
The renaming stays - we are Silver Potato until somebody sends me a
cease-and-desist letter.

Update - 6 April 2019
You probably noticed the new name, silver-potato. I’ve been working a
lot with Fedora Silverblue and wanted something with “silver” in it, so
I asked GitHub to generate “memorable” project names. The second one it
came up with was silver-potato and I said, “OK, cool, done!” Then
today I did a Google search for “Silver Potato”. And …
http://www.kaiju.com/bios/sil_01.htm
So watch this space - there’s probably another rename coming.
Update - 17 September 2018
I’m forking this project to clean it up for use outside of Hack Oregon.
There are a lot of things in there that are specific to Hack Oregon,
like the Amazon Linux image, that don’t make sense for other users.
My working test environments are Windows 10 Pro with Docker for Windows,
Arch Linux and Fedora Silverblue 29. This last - formerly known as
Fedora Ataomic Workstation - was the original inspiration for doing
this. The Silverblue environment is designed for a
“pet-container-centric” workload.
I’m rebuilding the documentation with Bookdown. That’s in the source now
but I haven’t published it anywhere. License will be Creative Commans
Attribution Share-Alike.
",2
ilguyi/gans.tensorflow.v2,Jupyter Notebook,"Implementation of various GANs with TensorFlow version 2.0

Final update: 2019. 05. 17.
All right reserved @ Il Gu Yi 2018

This repository is a collection of various GAN models implemented by TensorFlow version 2.0 style.
This repository moves to ilguyi/generative.models.tensorflow.v2
that is a collection of various generative models including autoregressive models,
latent variable models, normalizing flow models as well as GAN.
You will see more implementations of generative models.
Getting Started
Prerequisites

TensorFlow above 1.12
Python 3.6
Python libraries:

numpy, matplotlib, PIL


Jupyter notebook
OS X and Linux (Not validated on Windows but probably it would work)

Contents (TF version 2.x style)
Generative Adversarial Networks

Original GAN paper arXiv:1406.2661
gan.ipynb

Contents (TF version 1.x style)
Generative Adversarial Networks

Original GAN paper arXiv:1406.2661
gan.ipynb




Deep Convolutional GAN

Unsupervised Representation Learning with Deep Convolutional
Generative Adversarial Networks paper arXiv:1511.06434
dcgan.ipynb




Conditional GAN

Conditional Generative Adversarial Nets arXiv:1411.1784
cgan_based_on_dcgan.ipynb




InfoGAN

InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets arXiv:1606.03657
infogan.ipynb

Bidirectional GAN

Adversarial Feature Learning arXiv:1605.09782
bigan.ipynb

Least Squares GAN

Least Squares Generative Adversarial Networks arXiv:1611.04076
lsgan.ipynb

Wasserstein GAN

Wasserstein GAN arXiv:1701.07875
wgan.ipynb

Boundary Equilibrium GAN

BEGAN: Boundary Equilibrium Generative Adversarial Networks arXiv:1703.10717
began.ipynb

Author
Il Gu Yi
",27
sakov/enkf-c,C,"EnKF-C
EnKF-C provides a compact generic framework for off-line data assimilation (DA) into large-scale layered geophysical models with the ensemble Kalman filter (EnKF).
Following are its other main features:


coded in C for GNU/Linux platform;


model-agnostic;


can conduct DA either in EnKF or ensemble optimal interpolation (EnOI) mode;


permits multiple model grids;


can handle rectangular or curvilinear horizontal grids, z, sigma or hybrid vertical grids.


For more information see README and user guide. (An older version of the user guide is also available from arXiv.) Have a feel for how the code works by running the included example.
Checkout EnKF-C by running git clone https://github.com/sakov/enkf-c
or svn checkout https://github.com/sakov/enkf-c.

",18
openresty/openresty-packaging,Makefile,"Name
openresty-packaging - Official OpenResty packaging source and scripts for various Linux distributions.
Table of Contents

Name
Description
Supported Systems

Fedora
CentOS/RHEL
Amazon Linux
Ubuntu/Debian


Author
Copyright and License
See Also

Description
This code repository holds the source for building the official OpenResty pre-built packages published below:
https://openresty.org/en/linux-packages.html
https://openresty.org/en/rpm-packages.html
https://openresty.org/en/deb-packages.html
If you just want to use these pre-built (binary) packages and the corresponding package repositories, then
simply follow the instructions in these pages instead.
Otherwise, if you want to hack on or customize these packages yourself, for example, please read on.
Supported Systems
Fedora
For Fedora 22+:
# create the makerpm account for building rpms only:
sudo useradd makerpm
sudo usermod -a -G mock makerpm
sudo passwd makerpm

# install rpm build tools:
sudo dnf install @development-tools fedora-packager rpmdevtools

# install openresty's build requirements:
sudo dnf install openssl-devel zlib-devel pcre-devel gcc make perl perl-Data-Dumper

# login as makerpm:
sudo su - makerpm

cd ~
rpmdev-setuptree

cp /path/to/openresty-packaging/rpm/SOURCES/* ~/rpmbuild/SOURCES/

cd ~/rpmbuild/SPECS
cp /path/to/openresty-packaging/rpm/SPECS/*.spec ./

for file in *.spec; do
    spectool -g -R $file
    rpmbuild -ba $file
done
If success, binary rpm files are under ~/rpmbuild/RPMS/ while source rpm files are under
~/rpmbuld/SRPMS/.
See the How to create an RPM package wiki page for more details.
Back to TOC
CentOS/RHEL
For CentOS/RHEL 6+:
# create the makerpm account for building rpms only:
sudo useradd makerpm
sudo groupadd mock
sudo usermod -a -G mock makerpm
sudo passwd makerpm

# install rpm build tools:
sudo yum install rpm-build redhat-rpm-config rpmdevtools

# install openresty's build requirements:
sudo yum install openssl-devel zlib-devel pcre-devel gcc make perl \
    perl-Data-Dumper libtool ElectricFence systemtap-sdt-devel valgrind-devel

# login as makerpm:
sudo su - makerpm

mkdir -p ~/rpmbuild/{BUILD,RPMS,SOURCES,SPECS,SRPMS}
echo '%_topdir %(echo $HOME)/rpmbuild' > ~/.rpmmacros

cp /path/to/openresty-packaging/rpm/SOURCES/* ~/rpmbuild/SOURCES/

cd ~/rpmbuild/SPECS
cp /path/to/openresty-packaging/rpm/SPECS/*.spec ./

for file in *.spec; do
    spectool -g -R $file
    rpmbuild -ba $file
done
See this wiki page for more details.
Back to TOC
Amazon Linux
Similar to Fedora. Just make sure you have installed the following package to genreate those *-debuginfo packages automatically:
sudo yum install redhat-rpm-config
Back to TOC
Ubuntu/Debian
For Ubuntu 14.04+ and Debian 7.x+:
sudo apt-get install libtemplate-perl dh-systemd systemtap-sdt-dev perl gnupg curl make build-essential dh-make bzr-builddeb

cd /path/to/openresty-packaging/deb/
make zlib-build
make pcre-build
make openssl-build
make openssl-debug-build
make openresty-build
make openresty-debug-build
make openresty-valgrind-build
make lemplate-build
make test-nginx-build
Or to build everything from scratch, just run
make build
On Debian 7.x wheezy, you'll also need to enable the wheezy-backports apt source.
To generate degian source packages for uploading to Launchpad PPA servers, one can add the OPTS=-S argument, as in
make zlib-build OPTS=-S
make pcre-build OPTS=-S
It is also possible to generate debian source packages for any other Ubuntu or Debian codenames. For example:
make zlib-build DISTRO=trusty
make zlib-build OPTS=-S DISTRO=trusty
Back to TOC
Back to TOC
Author
Yichun Zhang (agentzh) <agentzh@gmail.com>
Back to TOC
Copyright and License
This module is licensed under the BSD license.
Copyright (C) 2016-2017 by Yichun ""agentzh"" Zhang (章亦春) <agentzh@gmail.com>, OpenResty Inc.
All rights reserved.
Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:


Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.


Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.


THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
Back to TOC
See Also

OpenResty official site

Back to TOC
",75
BioinformaticsFMRP/TCGAbiolinks,R,"






TCGAbiolinks - An R/Bioconductor package for integrative analysis with TCGA data
Installation
devtools::install_github(repo = ""BioinformaticsFMRP/TCGAbiolinks"")

Docker image
TCGAbiolinks is available as Docker image (self-contained environments that contain everything needed to run the software),
which can be easily run on Mac OS, Windows and Linux systems.
This PDF show how to install and execute the image.
The image can be obtained from Docker Hub: https://hub.docker.com/r/tiagochst/tcgabiolinksgui/
For more information please check: https://docs.docker.com/ and https://www.bioconductor.org/help/docker/
Manual
http://bioconductor.org/packages/devel/bioc/vignettes/TCGAbiolinks/inst/doc/tcgaBiolinks.html
http://bioinformaticsfmrp.github.io/TCGAbiolinks/

Citation
Please cite both TCGAbiolinks package:

Colaprico A, Silva TC, Olsen C, Garofano L, Cava C, Garolini D, Sabedot T, Malta TM, Pagnotta SM, Castiglioni I, Ceccarelli M, Bontempi G and Noushmehr H. ""TCGAbiolinks: an R/Bioconductor package for integrative analysis of TCGA data."" Nucleic acids research (2015): gkv1507.

  
Also, if you have used ELMER analysis please cite:

Yao, L., Shen, H., Laird, P. W., Farnham, P. J., & Berman, B. P. ""Inferring regulatory element landscapes and transcription factor networks from cancer methylomes."" Genome Biol 16 (2015): 105.
Yao, Lijing, Benjamin P. Berman, and Peggy J. Farnham. ""Demystifying the secret mission of enhancers: linking distal regulatory elements to target genes."" Critical reviews in biochemistry and molecular biology 50.6 (2015): 550-573.

",95
margyle/MugsyDev,Roff,"MugsyDev
The dev channel for Mugsy, the world's first hackable, customizable, dead simple, robotic coffee maker.
Repo for Mugsy's in progress API is here: https://github.com/margyle/decaf
",54
AkashiGakki/stock,JavaScript,"stock
股票系统课程项目
",3
discourse/discourse,Ruby,"
Discourse is the 100% open source discussion platform built for the next decade of the Internet. Use it as a:

mailing list
discussion forum
long-form chat room

To learn more about the philosophy and goals of the project, visit discourse.org.
Screenshots





Browse lots more notable Discourse instances.
Development
To get your environment setup, follow the community setup guide for your operating system.

If you're on macOS, try the macOS development guide.
If you're on Ubuntu, try the Ubuntu development guide.
If you're on Windows, try the Windows 10 development guide.

If you're familiar with how Rails works and are comfortable setting up your own environment, you can also try out the Discourse Advanced Developer Guide, which is aimed primarily at Ubuntu and macOS environments.
Before you get started, ensure you have the following minimum versions: Ruby 2.5+, PostgreSQL 10+, Redis 2.6+. If you're having trouble, please see our TROUBLESHOOTING GUIDE first!
Setting up Discourse
If you want to set up a Discourse forum for production use, see our Discourse Install Guide.
If you're looking for business class hosting, see discourse.org/buy.
Requirements
Discourse is built for the next 10 years of the Internet, so our requirements are high:



Browsers
Tablets
Phones




Safari 10+
iPad 4+
iOS 10+


Google Chrome 57+
Android 4.4+
Android 4.4+


Internet Explorer 11+




Firefox 52+





Built With

Ruby on Rails — Our back end API is a Rails app. It responds to requests RESTfully in JSON.
Ember.js — Our front end is an Ember.js app that communicates with the Rails API.
PostgreSQL — Our main data store is in Postgres.
Redis — We use Redis as a cache and for transient data.

Plus lots of Ruby Gems, a complete list of which is at /master/Gemfile.
Contributing

Discourse is 100% free and open source. We encourage and support an active, healthy community that
accepts contributions from the public – including you!
Before contributing to Discourse:

Please read the complete mission statements on discourse.org. Yes we actually believe this stuff; you should too.
Read and sign the Electronic Discourse Forums Contribution License Agreement.
Dig into CONTRIBUTING.MD, which covers submitting bugs, requesting new features, preparing your code for a pull request, etc.
Always strive to collaborate with mutual respect.
Not sure what to work on? We've got some ideas.

We look forward to seeing your pull requests!
Security
We take security very seriously at Discourse; all our code is 100% open source and peer reviewed. Please read our security guide for an overview of security measures in Discourse, or if you wish to report a security issue.
The Discourse Team
The original Discourse code contributors can be found in AUTHORS.MD. For a complete list of the many individuals that contributed to the design and implementation of Discourse, please refer to the official Discourse blog and GitHub's list of contributors.
Copyright / License
Copyright 2014 - 2019 Civilized Discourse Construction Kit, Inc.
Licensed under the GNU General Public License Version 2.0 (or later);
you may not use this work except in compliance with the License.
You may obtain a copy of the License in the LICENSE file, or at:
https://www.gnu.org/licenses/old-licenses/gpl-2.0.txt
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
Discourse logo and “Discourse Forum” ®, Civilized Discourse Construction Kit, Inc.
Dedication
Discourse is built with love, Internet style.
",28230
otrejo08/data_bootcamp_project3,Jupyter Notebook,"data_bootcamp_project3
https://otrejo08.github.io/data_bootcamp_project3/website/index.html
",2
smartstore/SmartStoreNET,C#,"Overview



SmartStore.NET is a free, open source, full-featured e-commerce solution for companies of any size. It is web standards compliant and incorporates the newest Microsoft web technology stack.
SmartStore.NET includes all essential features to create multilingual and multi-currency stores targeting desktop or mobile devices and enabling SEO optimized rich product catalogs with support for an unlimited number of products and categories, variants, bundles, datasheets, ESD, discounts, coupons and many more.
A comprehensive set of tools for CRM & CMS, sales, marketing, payment & shipping handling, etc. makes SmartStore.NET a powerful all-in-one solution fulfilling all your needs.
SmartStore.NET delivers a beautiful and configurable shop front-end out-of-the-box, built with a design approach on the highest level, including components like Bootstrap 4, Sass and others. The supplied theme Flex is modern, clean and fully responsive, giving buyers the best possible shopping experience on any device.
The state-of-the-art architecture of SmartStore.NET - with ASP.NET 4.5 + MVC 5, Entity Framework 6 and Domain Driven Design approach - makes it easy to extend, extremely flexible and essentially fun to work with ;-)

Website: http://www.smartstore.com/en/net
Forum: http://community.smartstore.com
Marketplace: http://community.smartstore.com/marketplace
Translations: http://translate.smartstore.com/
Documentation: SmartStore.NET Documentation in English

Highlights
Technology & Design

State of the art architecture thanks to ASP.NET 4.5, ASP.NET MVC 5, Entity Framework 6 and Domain Driven Design
Easy to extend and extremely flexible thanks to modular design
Highly scalable thanks to full page caching and web farm support
A powerful theming engine lets you create themes & skins with minimum effort thanks to theme inheritance
Point&Click Theme configuration
Highly professional search framework based on Lucene.NET, delivering ultra fast faceted search results
Consistent and sophisticated use of modern components such as jQuery, Bootstrap 4, Sass & more in the front and back end.
Easy shop management thanks to modern and clean UI

Features

NEW: CMS Page Builder
NEW: CMS Menu Builder
Multi-Store support
Unlimited number of products and categories
Product Bundles
RESTful WebApi
Multi-language and RTL support
Modern, clean, SEO-optimized and fully responsive Theme based on Bootstrap 4
Ultra fast search framework with faceted search support
Extremely scalable thanks to output caching, REDIS & Microsoft Azure support.
Trusted Shops precertification
100% compliant with German jurisdiction
Sales-, Customer- & Inventory-management
Comprehensive CRM features
Powerful Discount System
Powerful layered navigation in the shop
Numerous Payment and Shipping Providers and options
Sophisticated Marketing & Promotion capabilities (Gift cards, Reward Points, discounts of any type and more)
Reviews & Ratings
CMS (Blog, Forum, custom pages & HTML content etc.)
and many more...

Project Status
SmartStore.NET V3.2.0 has been released on 10 May, 2019. The highlights are:

(NEW) Page Builder: Create fascinating content that will boost your sales. No coding is required thanks to a powerful WYSIWYG editor which utilizes the revolutionary CSS Grid system (commercial plugin)
(NEW) Menu Builder: Visual manager for all sorts of menus. Change existing menus or create your own and place them anywhere you want.
(Perf) Faster MegaSearch thanks to Lucene.NET 4.8
(Perf) Huge performance increase in discount resolution and calculation.

Try it online
We have set up a live online demo for you so you are able to test SmartStore.NET without local installation. Get a first impression and test all available features in the front- and in the backend. Please keep in mind that the backend demo is shared and other testers can modify data at the same time.

Frontend (User: demo, PWD: 1234)
Backend (User: demo, PWD: 1234)

How to install

Download the latest stable release from the download tab and unzip it to your web folder
Setup a website in IIS and point the file directory to your unzipped folder
Fire up your browser and follow the installation instructions
Enjoy ;-)

NOTE: SmartStore.NET 3 requires Visual C++ Redistributable für Visual Studio 2015 which is already pre-installed on most systems. If, nevertheless, it is missing on your web server, just download and execute the installer or ask your hosting provider to do that for you.
System requirements

IIS 7+
ASP.NET 4.5+
MS SQL Server 2008 Express (or higher) OR MS SQL Server Compact 4
Visual C++ Redistributable für Visual Studio 2015 (Download)
Full Trust

License
SmartStore.NET Community Edition is released under the GPLv3 license.
",1487
discourse/discourse,Ruby,"
Discourse is the 100% open source discussion platform built for the next decade of the Internet. Use it as a:

mailing list
discussion forum
long-form chat room

To learn more about the philosophy and goals of the project, visit discourse.org.
Screenshots





Browse lots more notable Discourse instances.
Development
To get your environment setup, follow the community setup guide for your operating system.

If you're on macOS, try the macOS development guide.
If you're on Ubuntu, try the Ubuntu development guide.
If you're on Windows, try the Windows 10 development guide.

If you're familiar with how Rails works and are comfortable setting up your own environment, you can also try out the Discourse Advanced Developer Guide, which is aimed primarily at Ubuntu and macOS environments.
Before you get started, ensure you have the following minimum versions: Ruby 2.5+, PostgreSQL 10+, Redis 2.6+. If you're having trouble, please see our TROUBLESHOOTING GUIDE first!
Setting up Discourse
If you want to set up a Discourse forum for production use, see our Discourse Install Guide.
If you're looking for business class hosting, see discourse.org/buy.
Requirements
Discourse is built for the next 10 years of the Internet, so our requirements are high:



Browsers
Tablets
Phones




Safari 10+
iPad 4+
iOS 10+


Google Chrome 57+
Android 4.4+
Android 4.4+


Internet Explorer 11+




Firefox 52+





Built With

Ruby on Rails — Our back end API is a Rails app. It responds to requests RESTfully in JSON.
Ember.js — Our front end is an Ember.js app that communicates with the Rails API.
PostgreSQL — Our main data store is in Postgres.
Redis — We use Redis as a cache and for transient data.

Plus lots of Ruby Gems, a complete list of which is at /master/Gemfile.
Contributing

Discourse is 100% free and open source. We encourage and support an active, healthy community that
accepts contributions from the public – including you!
Before contributing to Discourse:

Please read the complete mission statements on discourse.org. Yes we actually believe this stuff; you should too.
Read and sign the Electronic Discourse Forums Contribution License Agreement.
Dig into CONTRIBUTING.MD, which covers submitting bugs, requesting new features, preparing your code for a pull request, etc.
Always strive to collaborate with mutual respect.
Not sure what to work on? We've got some ideas.

We look forward to seeing your pull requests!
Security
We take security very seriously at Discourse; all our code is 100% open source and peer reviewed. Please read our security guide for an overview of security measures in Discourse, or if you wish to report a security issue.
The Discourse Team
The original Discourse code contributors can be found in AUTHORS.MD. For a complete list of the many individuals that contributed to the design and implementation of Discourse, please refer to the official Discourse blog and GitHub's list of contributors.
Copyright / License
Copyright 2014 - 2019 Civilized Discourse Construction Kit, Inc.
Licensed under the GNU General Public License Version 2.0 (or later);
you may not use this work except in compliance with the License.
You may obtain a copy of the License in the LICENSE file, or at:
https://www.gnu.org/licenses/old-licenses/gpl-2.0.txt
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
Discourse logo and “Discourse Forum” ®, Civilized Discourse Construction Kit, Inc.
Dedication
Discourse is built with love, Internet style.
",28230
otrejo08/data_bootcamp_project3,Jupyter Notebook,"data_bootcamp_project3
https://otrejo08.github.io/data_bootcamp_project3/website/index.html
",2
smartstore/SmartStoreNET,C#,"Overview



SmartStore.NET is a free, open source, full-featured e-commerce solution for companies of any size. It is web standards compliant and incorporates the newest Microsoft web technology stack.
SmartStore.NET includes all essential features to create multilingual and multi-currency stores targeting desktop or mobile devices and enabling SEO optimized rich product catalogs with support for an unlimited number of products and categories, variants, bundles, datasheets, ESD, discounts, coupons and many more.
A comprehensive set of tools for CRM & CMS, sales, marketing, payment & shipping handling, etc. makes SmartStore.NET a powerful all-in-one solution fulfilling all your needs.
SmartStore.NET delivers a beautiful and configurable shop front-end out-of-the-box, built with a design approach on the highest level, including components like Bootstrap 4, Sass and others. The supplied theme Flex is modern, clean and fully responsive, giving buyers the best possible shopping experience on any device.
The state-of-the-art architecture of SmartStore.NET - with ASP.NET 4.5 + MVC 5, Entity Framework 6 and Domain Driven Design approach - makes it easy to extend, extremely flexible and essentially fun to work with ;-)

Website: http://www.smartstore.com/en/net
Forum: http://community.smartstore.com
Marketplace: http://community.smartstore.com/marketplace
Translations: http://translate.smartstore.com/
Documentation: SmartStore.NET Documentation in English

Highlights
Technology & Design

State of the art architecture thanks to ASP.NET 4.5, ASP.NET MVC 5, Entity Framework 6 and Domain Driven Design
Easy to extend and extremely flexible thanks to modular design
Highly scalable thanks to full page caching and web farm support
A powerful theming engine lets you create themes & skins with minimum effort thanks to theme inheritance
Point&Click Theme configuration
Highly professional search framework based on Lucene.NET, delivering ultra fast faceted search results
Consistent and sophisticated use of modern components such as jQuery, Bootstrap 4, Sass & more in the front and back end.
Easy shop management thanks to modern and clean UI

Features

NEW: CMS Page Builder
NEW: CMS Menu Builder
Multi-Store support
Unlimited number of products and categories
Product Bundles
RESTful WebApi
Multi-language and RTL support
Modern, clean, SEO-optimized and fully responsive Theme based on Bootstrap 4
Ultra fast search framework with faceted search support
Extremely scalable thanks to output caching, REDIS & Microsoft Azure support.
Trusted Shops precertification
100% compliant with German jurisdiction
Sales-, Customer- & Inventory-management
Comprehensive CRM features
Powerful Discount System
Powerful layered navigation in the shop
Numerous Payment and Shipping Providers and options
Sophisticated Marketing & Promotion capabilities (Gift cards, Reward Points, discounts of any type and more)
Reviews & Ratings
CMS (Blog, Forum, custom pages & HTML content etc.)
and many more...

Project Status
SmartStore.NET V3.2.0 has been released on 10 May, 2019. The highlights are:

(NEW) Page Builder: Create fascinating content that will boost your sales. No coding is required thanks to a powerful WYSIWYG editor which utilizes the revolutionary CSS Grid system (commercial plugin)
(NEW) Menu Builder: Visual manager for all sorts of menus. Change existing menus or create your own and place them anywhere you want.
(Perf) Faster MegaSearch thanks to Lucene.NET 4.8
(Perf) Huge performance increase in discount resolution and calculation.

Try it online
We have set up a live online demo for you so you are able to test SmartStore.NET without local installation. Get a first impression and test all available features in the front- and in the backend. Please keep in mind that the backend demo is shared and other testers can modify data at the same time.

Frontend (User: demo, PWD: 1234)
Backend (User: demo, PWD: 1234)

How to install

Download the latest stable release from the download tab and unzip it to your web folder
Setup a website in IIS and point the file directory to your unzipped folder
Fire up your browser and follow the installation instructions
Enjoy ;-)

NOTE: SmartStore.NET 3 requires Visual C++ Redistributable für Visual Studio 2015 which is already pre-installed on most systems. If, nevertheless, it is missing on your web server, just download and execute the installer or ask your hosting provider to do that for you.
System requirements

IIS 7+
ASP.NET 4.5+
MS SQL Server 2008 Express (or higher) OR MS SQL Server Compact 4
Visual C++ Redistributable für Visual Studio 2015 (Download)
Full Trust

License
SmartStore.NET Community Edition is released under the GPLv3 license.
",1487
googleapis/nodejs-pubsub,TypeScript,"
Google Cloud Pub/Sub: Node.js Client



Cloud Pub/Sub is a fully-managed real-time messaging service that allows you to send and receive messages between independent applications.

Using the client library
Samples
Versioning
Contributing
License

Using the client library


Select or create a Cloud Platform project.


Enable billing for your project.


Enable the Google Cloud Pub/Sub API.


Set up authentication with a service account so you can access the
API from your local workstation.


Install the client library:
 npm install --save @google-cloud/pubsub



Try an example:


// Imports the Google Cloud client library
const {PubSub} = require('@google-cloud/pubsub');

async function quickstart(
  projectId = 'your-project-id', // Your Google Cloud Platform project ID
  topicName = 'my-topic' // Name for the new topic to create
) {
  // Instantiates a client
  const pubsub = new PubSub({projectId});

  // Creates the new topic
  const [topic] = await pubsub.createTopic(topicName);
  console.log(`Topic ${topic.name} created.`);
}
Samples
Samples are in the samples/ directory. The samples' README.md
has instructions for running the samples.



Sample
Source Code
Try it




Subscriptions
source code



Topics
source code




The Cloud Pub/Sub Node.js Client API Reference documentation
also contains samples.
Versioning
This library follows Semantic Versioning.
This library is considered to be in beta. This means it is expected to be
mostly stable while we work toward a general availability release; however,
complete stability is not guaranteed. We will address issues and requests
against beta libraries with a high priority.
More Information: Google Cloud Platform Launch Stages
Contributing
Contributions welcome! See the Contributing Guide.
License
Apache Version 2.0
See LICENSE
What's Next

Cloud Pub/Sub Documentation
Cloud Pub/Sub Node.js Client API Reference
github.com/googleapis/nodejs-pubsub

Read more about the client libraries for Cloud APIs, including the older
Google APIs Client Libraries, in Client Libraries Explained.
",150
yanxdd/IDA,Assembly,"This repository is used to store IDA related data, include idc scripts, python scripts, plug-in code and so on.
IDC
GenAsmAndBytes:  Generate .asm file and .bytes file from PE.
IDAPython
GenCallPath Generate the function call path. Include static path and dynamic path.
Plugins on github
keypatch Modify the program.
findcrypt Find encryption algorithm.
Ponce Plugins of symbol execution, No.1 in 2016 IDA plug-in competition.
ida_ipython Embedded ipython.
IDAmetrics Improving fuzzing using software complexity metrics.
",6
IOTA-Ledger/blue-app-iota,C,"IOTA App for Ledger Hardware Wallets

Here we try to use natively available crypto logic to create IOTA seeds and sign transactions on the fly.
It is strongly recommended to take a few minutes to read this document to make sure you fully understand how IOTA and the Ledger Hardware Wallet works, and how they interact together.
Table of contents

Introduction

Terminology
Address Reuse
IOTA Bundle
Parts of an IOTA Transaction


How Ledger Hardware Wallets Work
IOTA Specific Considerations for Ledger Hardware Wallets

IOTA User-Facing App Functions

Functions
Display


Recovery Phrase Entropy
IOTA Security Concerns Relating to Ledger Hardware Wallets
Limitations of Ledger Hardware Wallets


FAQ

I lost my ledger, what should I do now?


Development

Preparing development environment under Ubuntu 17.10
Preparing development environment in other distributions
Compile and load the IOTA Ledger app


Documentation
Contributing

Donations
As a developer



Table of contents generated with markdown-toc

Introduction
IOTA is a unique cryptocurrency with specific design considerations that must be taken into account. This document will attempt to go over how the Ledger hardware wallet functions, and how to stay safe when using a Ledger to store IOTA.
Terminology
Seed: An IOTA seed is equivalent to the password for your ""IOTA account"". It can store a near infinite number of addresses.
Private Key: The private key is used to generate a public key. It is also used to prove you own said public key by means of creating a signature for a specific transaction.
Public Key: Also known as public address or just address. The public key is the key you would give to somebody else to transfer you funds.
Change Tx: After sending funds to a 3rd party, all remaining funds on the account must be transferred to a new address - this is called the change tx (or the change address).
Account: The IOTA app stores indexes for 5 seeds, these can be used as unique accounts. For example one could be a primary or general purpose account, and another could be a donations account.
Address Reuse
IOTA uses a type of quantum resistance called the Winternitz One-Time Signature Scheme. This means that addresses must not be reused. This is because you reveal part of the private key with each transaction. As such, reusing a spent address can allow attackers to forge fake transactions and steal your balance.
Keep in mind you can receive an infinite number of transactions, but as soon as you send from the address, you must NEVER use that address again.
This is handled in IOTA by utilizing what is called a seed index. Each index in the seed generates a unique address. When creating an IOTA transaction, it uses what's called an atomic bundle. This is just a fancy term that means either the entire bundle is accepted, or none of it is.
IOTA Bundle
A bundle is just a group of transactions and IOTA uses both input and output transactions. So if Bob has 10 iota, and wants to send Alice 3 iota, the bundle could look like this:
tx1: Bob -10 iota
tx2: Alice +3 iota
tx3: Bob +7 iota (change tx)
This example highlights how IOTA handles one time signatures. First it takes an input of 10 iota from Bob's address. It sends 3 of it to Alice, and it puts the remaining 7 iota on a new address belonging to Bob's seed.
All input transactions require the private key to generate a signature which proves that you are the owner of the funds.
Because bundles are atomic units, the network will never accept Bob's input tx of -10 iota without also accepting sending 3 to Alice, and 7 to a new address owned by Bob (in this example).
Parts of an IOTA Transaction
An IOTA transaction is broken up into 2 halves. The first half is generating a transaction bundle, and creating signatures for it.
The second half is selecting 2 other transactions to confirm, and performing the proof of work.
The Ledger is only responsible for generating signatures for a specific transaction. After that the host machine (or anybody else for that matter), can take the signatures and broadcast it to the network (however the signatures are only valid for the specific transaction bundle).
Promoting an unconfirmed transaction does not require re-signing on the Ledger.
How Ledger Hardware Wallets Work
The Ledger Hardware Wallet works by deterministically generating an IOTA seed based on your 24 word mnemonic (created when setting up the device).
Instead of storing the seed on your computer (which could be stolen by an attacker), the seed is stored on the Ledger device, and is never broadcast to the host machine it is connected to.
Instead, the host machine must ask the Ledger to provide the information (such as public keys or signatures). When creating transactions the host will generate the necessary information for the transaction bundle, and then will send it to the Ledger device to be signed. The Ledger will then use the private keys associated with the input transactions to generate unique signatures, and will then transfer only the signatures back to the host machine.
The host can then use these signatures (which are only valid for that specific transaction bundle) to broadcast the transaction to the network. However as you can see, neither the IOTA seed, nor any of the private keys ever leave the device.
However keep in mind that in IOTA the signature contains some information about the private key for one specific address.
See Ledger's documentation to get more info about the inner workings of the Ledger Hardware Wallets.
IOTA Specific Considerations for Ledger Hardware Wallets
IOTA User-Facing App Functions
Functions


Display Address: The wallet can ask the Ledger to display the address for a specific index on the seed. It only accepts the index, and subsequently generates the address itself and thus verifies that the address belongs to the Ledger.
Note: this does not pertain to displaying addresses in transactions. Transactions will be denoted as ""output"", ""input"", or ""change"" transactions. Input and Change are verified to belong to the Ledger. Output are not.


Sign Transaction: The wallet will generate a transaction for the user to approve before the Ledger signs it. Ensure all amounts and addresses are correct before signing. These signatures are then sent back to the host machine.


Display
For the Ledger Nano S:


The sides of the display will have an up or down arrow indicating that you can scroll to a new screen.


Two bars along the top (just below the buttons) indicates that there is a double press function available (generally confirm/toggle or back). We will be working to ensure this function is always intuitive.


For the Ledger Nano X:

Behavior is the same as the Nano S. Graphically there are no confirmation bars along the top (but double pressing the buttons still confirms/toggles).

For the Ledger Blue:

The Ledger Blue uses a touchscreen, thus all you need to do is tap the buttons on the screen.

Recovery Phrase Entropy

The 24 word BIP39 recovery phrase (mnemonic) of the hardware wallet represents less information (256 bits of entropy) than a 27 tryte IOTA seed (384 bits of entropy).
An additional function is used to convert this mnemonic seed and the optional passphrase (not your pin number) of your choosing into a 512 bit binary seed. This happens according to the BIP39 standard on the Ledger using system calls.
The extended child key (512 bits) of a corresponding BIP32 path is then hashed (using Kerl) to derive the final 243 trit seed for IOTA.

While having (only) 256 bits of entropy does not pose a security problem, it does not support the full potential of the IOTA seed. Thus, to use the full entropy supported by IOTA, an additional sufficiently long passphrase is needed! On the other hand, there are other factors that might have a higher security impact, like choosing proper random mnemonics (the Ledger uses a TRNG for that matter) or selecting a higher security level.
IOTA Security Concerns Relating to Ledger Hardware Wallets
All warnings on the Ledger are there for a reason, MAKE SURE TO READ THEM and refer to this document if there is any confusion.


Don't rush through signing a transaction.
When generating a transaction for the Ledger to sign, you will scroll through each transaction before signing off on the bundle. The transaction information will come up in order (while scrolling from left to right).
On the Ledger Nano S/X, the first screen will display the tx type (output, input, or change), as well as the amount. The next screen will display the corresponding address for said transaction. This will repeat until all transactions have been displayed, then you can select ""Approve"" or ""Deny"".
On the Ledger Blue each transaction entry in a bundle will fit on the screen, use the next button until you've confirmed all transactions and then select approve if everything is correct.


All output transactions to 3rd party addresses will say ""Output:"" and below that ""1.56 Mi"" (for example). ""Output"" being the key word here.
All input transactions will say ""Input: [0]"", and the final output transaction (the change transaction) will say ""Change: [4]"" ([4] being the seed-index of the change tx). This means the Ledger has verified the addresses used for inputs as well as the change tx all belong to the Ledger.
If the input amount perfectly matches the output amount, there will be no change transaction. If there is no change transaction, double check that you are sending the proper amount to the proper address because there is no remainder being sent back to your seed.
Note: When transferring from one seed (controlled by the Ledger device) to another seed (also controlled by the Ledger device) it will not display a ""Change tx"". As such the wallet should first display the address on the new seed that it intends to send it to on the Ledger (thus verifying it belongs to the Ledger), and then create the transaction. The user would then verify in the transaction that the ""Output:"" tx is in fact going to the address previously displayed on the Ledger.




Verify ALL information in a transaction and NEVER re-sign a transaction.
For other coins you only need to verify the destination address and amount, but for IOTA you must also verify all input transactions. If you sign a transaction on the Ledger but the transaction is not using all of the funds on a specific address, leftover funds on the address are then vulnerable.
As such make sure the input amount(s) are what you expect them to be, make sure the output destination and amount is what you intend to send, and make sure there is a change tx if applicable with all of your remaining funds.
Note: After signing a transaction, you should verify the transaction was successfully broadcast to the network. You should NEVER re-sign the same transaction on the Ledger. If the transaction does not get confirmed, you should promote the transaction on the host machine. This avoids generating a new signature and weakening the security of the addresses.


If the transaction was not broadcast to the network, and you can't find it in the wallet you should be EXTREMELY CAUTIOUS before proceeding to sign a new transaction. If an infected machine is silently storing the signatures without broadcasting them, it could steal your funds after re-signing.
If this situation should arise you should consider going to a more trusted machine before re-signing a transaction.




Limitations of Ledger Hardware Wallets
Due to the memory limitations of the Ledger Nano S/X and the Ledger Blue, the transaction bundles have certain restrictions. The Ledger Nano S/X can only accept a transaction with a maximum bundle size of 10 and the Ledger Blue is limited to a maximum bundle size of 20.
An output and a change transaction each only require 1 bundle entry, however every input transaction requires the same number of bundle entries as the security level being used on the seed. Thus if using a Ledger Nano S or X you could have 1 output + 4 inputs (security level 2) + 1 change transaction and this would take up all 10 bundle entries. For security level 3 you could only have 1 output + 2 inputs + 1 change transaction, or 1 output + 3 inputs without a change transaction.
Security level 2 is the default security level.
FAQ
I lost my ledger, what should I do now?
Hopefully you wrote down your 24 recovery words and your optional passphrase in a safe place. If not, all your funds are lost.
If you did, the best solution is to buy a new Ledger and enter your 24 recovery words and your optional passphrase in the new device.
After installation of the IOTA Ledger app, all your funds are restored. Take care to reinitialize your seed index correctly.
Another approach is to use our seed recovery tool which can be found here: https://github.com/IOTA-Ledger/recover-iota-seed-from-ledger-mnemonics.
WARNING: Only use this tool in emergencies, as exposing your seed defeats the primary purpose of using a hardware wallet.
Development
Preparing development environment under Ubuntu 17.10

Clone this repo
Execute the following commands to setup your development environment:

cd blue-app-iota
chmod +x install_dev_env.sh
chmod +x activate_virt_env.sh
./install_dev_env.sh


If you execute it for the first time, maybe you have to log out and log in again to get correct group rights

Preparing development environment in other distributions

Clone this repo, and set up your development environment according to this: LedgerHQ Getting Started
ATTENTION: Ledger Python 2 library seems to be broken, so you have to use Python 3 and the latest version of the ledger Python lib from their GitHub. Have a look here:


I was unable to install the recent update of the Ledger SDK with their direct instructions. Instead, follow everything from the link above, except for the Python SDK.
Instead of running Python 2 like explicitly being told, use Python 3 instead (I wasn't able to make it work with Python 2 and the commits of their latest updates over the past week suggest they created Python 3 support).
In the part where they tell you to run pip install ledgerblue, run pip install git+https://github.com/LedgerHQ/blue-loader-python.git to install the bleeding-edge version.

Compile and load the IOTA Ledger app

Connect your Ledger to the PC and unlock it
To load the app, be sure that the dashboard is opened in the Ledger
Run the following commands to compile the app from source and load it

cd blue-app-iota

# If you have installed using the automated script:
./activate_virt_env.sh

make load


Accept all the messages on the Ledger
You can now make your first IOTA Ledger address by running the following command (while Ledger is plugged in and the IOTA App is opened):

python demos/python/demo.py

Specification
See: APDU API Specification
Contributing
Donations
This is a community project, done in our spare time for the betterment of the IOTA ecosystem and community.
Donating is not required, but is greatly appreciated. If you would like to donate please send some IOTA to the following address:
TLCZOGKIARUQRBSJYSUTXVQYKPTOYOMQAUWUGBOCJJFQSXELFPEDF9LKDCUVKYDVGCJTCRANLOZJJKKNBEKVDHCJ9B


Please know that the donations made to this address will be shared with everyone who meaningfully contributes to the project.
Thanks!
As a developer
Would you like to contribute as a dev? Please check out our Discord channel to contact us!
",65
long-re/Long,Java,"Long - Long is Only a Not officially supported Ghidra



Long is a forked version of Ghidra that is not officially supported as it is only a community version.
The goal of this project is to allow faster prototyping with more experimental features and for more fluent user experience. Current Ghidra has many ""not so intuitional"" choices, and that's what we want to change.
As for the name, Long means Dragon in Chinese, can be pronounced more like ""lown"".
Some of the decisions are still yet to made, including how to manage this project as it will definitely emerge along with the community. With small community as for now, nothing needs to be done to manage it anyway. :P
Roadmap
Current Roadmap is to find out those influent user experiences and change from the easier ones. Github project may be used to denote what we want to change.
Contributors
We follow official Ghidra, that makes them the main contributors.
Members from Team r3kapig are currently handling those changes deviating from main repo.
",10
kubeflow/pipelines,Python,"

Overview of the Kubeflow pipelines service
Kubeflow is a machine learning (ML) toolkit that is dedicated to making deployments of ML workflows on Kubernetes simple, portable, and scalable.
Kubeflow pipelines are reusable end-to-end ML workflows built using the Kubeflow Pipelines SDK.
The Kubeflow pipelines service has the following goals:

End to end orchestration: enabling and simplifying the orchestration of end to end machine learning pipelines
Easy experimentation: making it easy for you to try numerous ideas and techniques, and manage your various trials/experiments.
Easy re-use: enabling you to re-use components and pipelines to quickly cobble together end to end solutions, without having to re-build each time.

Documentation
Get started with your first pipeline and read further information in the Kubeflow Pipelines documentation.
Blog posts

Getting started with Kubeflow Pipelines (By Amy Unruh)
How to create and deploy a Kubeflow Machine Learning Pipeline (By Lak Lakshmanan)

Part 1: How to create and deploy a Kubeflow Machine Learning Pipeline
Part 2: How to deploy Jupyter notebooks as components of a Kubeflow ML pipeline



Acknowledgments
Kubeflow pipelines uses Argo under the hood to orchestrate Kubernetes resources. The Argo community has been very supportive and we are very grateful.
",815
vuejs/vue,JavaScript,"










Supporting Vue.js
Vue.js is an MIT-licensed open source project with its ongoing development made possible entirely by the support of these awesome backers. If you'd like to join them, please consider:

Become a backer or sponsor on Patreon.
Become a backer or sponsor on Open Collective.
One-time donation via PayPal or crypto-currencies.

What's the difference between Patreon and OpenCollective?
Funds donated via Patreon go directly to support Evan You's full-time work on Vue.js. Funds donated via OpenCollective are managed with transparent expenses and will be used for compensating work and expenses for core team members or sponsoring community events. Your name/logo will receive proper recognition and exposure by donating on either platform.
Special Sponsors





Platinum Sponsors






































Platinum Sponsors (China)











Gold Sponsors











































































































































Sponsors via Open Collective
Platinum


Gold






Introduction
Vue (pronounced /vjuː/, like view) is a progressive framework for building user interfaces. It is designed from the ground up to be incrementally adoptable, and can easily scale between a library and a framework depending on different use cases. It consists of an approachable core library that focuses on the view layer only, and an ecosystem of supporting libraries that helps you tackle complexity in large Single-Page Applications.
Browser Compatibility
Vue.js supports all browsers that are ES5-compliant (IE8 and below are not supported).
Ecosystem



Project
Status
Description




vue-router

Single-page application routing


vuex

Large-scale state management


vue-cli

Project scaffolding


vue-loader

Single File Component (*.vue file) loader for webpack


vue-server-renderer

Server-side rendering support


vue-class-component

TypeScript decorator for a class-based API


vue-rx

RxJS integration


vue-devtools

Browser DevTools extension



Documentation
To check out live examples and docs, visit vuejs.org.
Questions
For questions and support please use the official forum or community chat. The issue list of this repo is exclusively for bug reports and feature requests.
Issues
Please make sure to read the Issue Reporting Checklist before opening an issue. Issues not conforming to the guidelines may be closed immediately.
Changelog
Detailed changes for each release are documented in the release notes.
Stay In Touch

Twitter
Blog
Job Board

Contribution
Please make sure to read the Contributing Guide before making a pull request. If you have a Vue-related project/component/tool, add it with a pull request to this curated list!
Thank you to all the people who already contributed to Vue!

License
MIT
Copyright (c) 2013-present, Yuxi (Evan) You
",138659
sihai00/blog,JavaScript,"blog
博客
目录



标题
地址
类型
功能




web-mobile-cli简易教程
链接
工具
脚手架


clipboard.js
链接
源码
复制


history
链接
源码
历史记录


sal
链接
工具
滚动动画库


fallSnow
链接
组件
下雪效果


leetcode算法题
链接
算法
算法题



",5
qantasairways/runway,JavaScript,"runway
Qantas Style Guide

",6
MicrosoftDocs/azure-docs.pt-br,PowerShell,"


ms.openlocfilehash
ms.sourcegitcommit
ms.translationtype
ms.contentlocale
ms.lasthandoff
ms.locfileid




d4e9e26f2c6764cc179826266891d861514e23f6
3102f886aa962842303c8753fe8fa5324a52834a
HT
pt-BR
04/23/2019
60344082



Documentação do Microsoft Azure
Bem-vindo à documentação de código-fonte aberto do Microsoft Azure. Examine este arquivo LEIAME para entender como você pode ajudar a contribuir para a documentação do Microsoft Azure.
Introdução
Contribuir com o código aberto é mais do que apenas as atualizações, mas também nos informar quando há um problema. Para obter mais informações, leia nossas Diretrizes de contribuição.
Pré-requisitos
Você decidiu contribuir, isso é ótimo! Para contribuir com a documentação, você precisa de algumas ferramentas.
Contribuir com a documentação requer uma conta do GitHub. Se você não tiver uma conta, siga as instruções no guia do Colaborador para Configuração da conta do GitHub.
Baixar
Instale as ferramentas a seguir:

Git
Visual Studio Code
Extensão do Pacote de Criação de Documentos para o Visual Studio Code

Instalar
Siga as instruções fornecidas no guia do Colaborador para Instalar ferramentas de criação de conteúdo.
Licença
Confira os tópicos LICENSE, LICENSE-CODE e ThirdPartyNotices para todas as informações de licenciamento.
Código de Conduta
Este projeto adotou o Código de Conduta Microsoft Open Source.
Para obter mais informações, confira as Perguntas frequentes sobre o código de conduta ou entre em contato com opencode@microsoft.com para enviar outras perguntas ou comentários.
",12
keybase/client,Go,"Keybase  
Hi, and welcome to the Keybase client repo.  All our client apps (macOS,
Windows, Linux, iOS, and Android) are being actively developed in this
repository. Please, dig around.
Warnings
We'd love you to read our source code.
But - some of the things in this repo are explorations, and the app you build
from source just might not do what it says it's doing. So, if you just want
to install Keybase on your computer, you should monitor our releases for macOS, Linux, or Windows.

Code Layout

go: Core crypto libraries; the Keybase service; the command line client. Learn More
shared/react-native: Android and iOS apps developed with React Native.
shared/desktop: Desktop application for macOS, Linux, and Windows, made with the Electron framework, sharing React code with react-native.
packaging: Scripts for releasing packages across the various platforms.
protocol: Defines the protocol for communication for clients to the Keybase services. Uses Avro. Learn More
media: Icons, graphics, media for Keybase apps.
osx: The macOS Keybase.app, development parallel to an Electron-based application above. Learn More

Problems?
Report any issues with client software on this GitHub
issue tracker.
Internally, we track our progress using Jira, but all PRs come through GitHub
for your review!
If you're having problems with the command line keybase client, take a
look at the troubleshooting doc.
If you're having problems with our Website, try the
keybase-issues issue tracker.
We check and update both frequently.
License
Most code is released under the New BSD (3 Clause) License.  If subdirectories
include a different license, that license applies instead.
Development Guidelines
We check all git commits with pre-commit hooks generated via
pre-commit.com pre-commit hooks.
To enable use of these pre-commit hooks:

Install the pre-commit utility. For some common cases:

pip install pre-commit
brew install pre-commit


Remove any existing pre-commit hooks via rm .git/hooks/pre-commit
Configure via pre-commit install

Then proceed as normal.
External Contributors
If you forked this repository on GitHub and made a PR, then it'll show up as
having failed Jenkins CI. We do not build external PRs because it's a security
risk to do so without a review first. If your PR is successfully reviewed by a
member of the Keybase team, then we will merge your commits to a branch on our
primary fork and build from there.
Cryptography Notice
This distribution includes cryptographic software. The country in which you
currently reside may have restrictions on the import, possession, use, and/or
re-export to another country, of encryption software. BEFORE using any
encryption software, please check your country's laws, regulations and policies
concerning the import, possession, or use, and re-export of encryption
software, to see if this is permitted. See http://www.wassenaar.org/ for more
information.
The U.S. Government Department of Commerce, Bureau of Industry and Security
(BIS), has classified this software as Export Commodity Control Number (ECCN)
5D002.C.1, which includes information security software using or performing
cryptographic functions with asymmetric algorithms. The form and manner of this
distribution makes it eligible for export under the License Exception ENC
Technology Software Unrestricted (TSU) exception (see the BIS Export
Administration Regulations, Section 740.13) for both object code and source
code.
",4987
YetaWF/YetaWF-Modules,C#,"

STOP - Do not simply retrieve the repository. There are a few easy steps you need to follow to correctly install YetaWF.
Overview
YetaWF (pronounced ""Yet Another Web Framework"") is a new open source product, developed by Mike van der Meulen (Softel vdm, Inc.).
It was used to develop and host the company's own site, the www.LegacyFax.com fax service, the LinksWithPics service, the WEO Report service (now discontinued), and various other sites.
It builds on ASP.NET Core MVC (also available using ASP.NET 4 MVC 5) to create a web framework for developers.
YetaWF offers Single Page Sites ""out of the box"" (every site is a single page site) and Static Pages. It follows ROCA principles and best practices (like using WEBP for images) so landing pages can easily achieve a Page Insights score in the high 90s, both mobile and desktop).
While it has many attributes commonly found in today's CMS like DNN, Umbraco, Drupal, etc., it instead focuses on rapid application development.
Tightly integrating with Visual Studio 2015, 2017, it lends itself to rapid module development, offering built-in BREAD (Browse, Read, Edit, Add, Delete) module generation, taking advantage of ASP.NET Core MVC, Data Annotation, Components, UIHint and many other innovative features made possible by ASP.NET Core MVC.
Rather than being all things to all people, it narrowly focuses on offering 100% of the features that will satisfy most average websites and offers a platform for rapid custom development.
It makes extensive use of TypeScript, jQuery, jQuery-UI, Kendo UI Core, AJAX and many client-side plugins (like grids, menus, syntax highlighter, lightbox, CKEditor, tabs, overlays and many more).
It combines these with automatic features like CDN support, JavaScript and CSS minifier and bundling, HTTP response compression (take a look at the source for this page) without burdening the developer with administrative tasks.
For the site administrator, it offers site management, automated backups, easy version upgrades, scheduled tasks and of course complete control over user/role authorizations.
YetaWF also takes a new approach to localization by being designed from the ground up with localization in mind (again, without burdening the developer with administrative tasks). Pages, modules, down to SQL tables or file data are fully localizable.
Adding/removing languages is possible at any time, not just at design-time.
Links
STOP - Do not simply retrieve the repository. There are a few easy steps you need to follow to correctly install YetaWF.

YetaWF.com
Documentation
Installing YetaWF
Demo Site
Using YetaWF

",2
sigmaai/self-driving-golf-cart,Makefile,"

    
Welcome! This is an open source self-driving development platform aimed for rapid prototyping, deep learning, and robotics research. The system currently runs on a modified electric golf cart. Here are our goals:
Goals:
Research and develop a deep learning-driven self-driving car. The vehicle should be able to achieve level 4 autonomy within a geofenced area.
The modules in this project.

End-to-End Steering (Behavioral cloning)
Semantic Segmentation
Object Detection 🚙
Drive by Wire (DBW)
CARLA simulator integration
ZED stereoscopic vision system
Mapping with rtabamp
Path planning with ROS nav stack.
Localization, pose tracking, and odom with ZED and rtabmap.

For the full documentation of the development process, please visit my website: www.neilnie.com
Table of Content

Try it out
About ROS
Simulation
Autopilot & End-to-End Behavioral Cloning

What's Behavioral Cloning
Model


Semantic Segmentation
The Navigation Stack

RTABMap
Path Planning
Vehicle Motion Control




Try it out
Before you jump in, let me describe the hardware requirement for this project. A webcam is the minimum requirment. At this point, you can only run the whole system on the actual self-driving vehicle. ROS will throw warnings (even errors) at you if you don't have the hardware connected to your Linux machine. If you don't have access to the hardware setup, don't worry 👇

The best way is to download and play back the ROS bags. [coming soon...]
You can tryout individual packages and nodes, and might find them helpful for your own projects.
You can also tryout the CARLA simulator. (Maybe even improve the current system.)

To compile the project:
Requirements

Make sure that you have ROS installed on your computer. (I am using ROS Melodic)
Make sure you have all the dependencies installed.

Clone & Compile

Clone the repository. $ git clone https://github.com/sigmaai/self-driving-golf-cart.git
$ cd self-driving-golf-cart/ros
$ catkin_make
$ source devel/setup.bash

Launch ZED cameras

$ roslaunch zed_wrapper zed.launch (no rviz)
$ roslaunch zed_display display.launch (with rviz)

Launch the Navigation Stack

$ roslaunch path_planning rtab_mapping_navigation.launch


🚙 Bon Voyage 😀
 
About ROS
This project uses ROS. For more information on ROS, nodes, topics and others please refer to the ROS README.
 
Simulation
(🏗 Construction Zone 🚧)
Building a self-driving car is hard. Not everyone has access to expensive hardware. I am currently trying to integrate this project with the CARLA self-driving simulator. If you are interested in CARLA, please refer to this documentation. The ROS system in this project can partially run on the CARLA simulator.
If you want to try out the simulator, please refer to the documentation here.


Autopilot & End-to-End Behavioral Cloning
The autopilot system, found here in the autopilot node, uses deep learning to predict the steering commands and acceleration commands for the vehicle, only using data collected by the front facing camera.
 
What's Behavioral Cloning
In 2016, NVIDIA proposed a novel deep learning approach allowed their car to accurately perform real-time end-to-end steering command prediction. Around the same time, Udacity held a challenge that asked researchers to create the best end-to-end steering prediction model. Our goal is to further the work in behavioral cloning for self-driving vehicles.
 
Model
NVIDIA's paper used a convolutional neural network with a single frame input. I believe that the single-frame-input CNN doesn't provide any temporal information which is critical in self-driving. This is the motive behind choosing the i3d architecture, which is rich in spacial-temporal information.
The input of the network is a 3d convolutional block, with the shape of n * weight * height * 3. n is the length of the input sequence. A flatten layer and a dense layer are added to the back of the network for the purpose of this regression problem.

Here is a video demo of deep learning model running on the autonomous golf cart.
VIDEO DEMO
 
Semantic Segmentation
The cart understands its surrounding  through semantic segmentation, which is a technique in computer that classifies each pixel in an image into different categories. The vehicle can also make decisions based on the segmentic segmentation results. The cart can change its speed based on the proximity to nearby obstacles.

We deployed the ENet architecture for segmentation. ENet is design to work well in realtime applications. For more information, please visit the paper. We used the CityScape dataset for training and the python code for training and inferencing are located in the ./src/segmentation/scripts directory.
VIDEO DEMO
 
The Navigation Stack

The self-driving vehicle uses a modified version of the ROS navigation stack. The flowchart above illustrate the mapping and path planning process. First, I create a detailed map of the environment with rtabmap_ros. With that global map, I use the localization feature of rtabmap_ros and the odom feature of the zed camera system to localize and plan paths.
 
RTABMap
rtabmap (realtime appearance based mapping) allows me to construct a global map of the environment. For more information on the mapping package, please check out this .launch file.

 
Path Planning
The project uses the move_base node from the navigation stack. The image below shows the costmap (in blue and purple), and the global occupancy grid (in black and gray). move_base also plans the local and global path. Global paths are shown in green and yellow below. You can find the yaml files here.

 
Vehicle Motion Control
The move base node publishes /cmd_vel commands, which are processed and sent directly to the vehicle.

Contact / Info
If you are interested in the detailed development process of this project, you can visit Neil's blog at neilnie.com to find out more about it. Neil will make sure to keep you posted about all of the latest development on the club.
Developers:

Neil (Yongyang) Nie | Email | Github | Website | Linkedin

Michael Meng | Email | Github
",48
zrrrzzt/html-validator-cli,JavaScript,"


html-validator-cli
CLI for validating html using validator.w3.org/nu
Requires Node >= 8.15.3 for older versions use v5.0.0
Sends Page is validto STDOUT and exits with code 0 if page is valid.
Sends Page is not valid to STDOUT and exits with code 1 if page is not valid.
Sends Page not found to STDOUT and exits with code 1 if page is not found.
Installation
$ npm i html-validator-cli -g

Usage
$ html-validator <url>

With file
$ html-validator --file=<path-to-file>

With data
$ html-validator --data=data

Optional pass in format for returned data.
Valid options: json, html, xhtml, xml, gnu and text (default).
$ html-validator <url> --format=gnu

Optional pass in another validator.
It needs to expose the same REST interface.
$ html-validator <url> --validator='http://html5.validator.nu'

Optional pass in strings to ignore
$ html-validator <url> --ignore='Error: Stray end tag “div”.' --ignore='Error: Stray end tag “body”.'

Optional pass in headers
$ html-validator <url> --headers='{""foo"":""doo""}'

To get full result from validator use --verbose
$ html-validator <url> --verbose

Optional, only get errors use --quiet
$ html-validator <url> --quiet

Validate a local document without setting up a tunnel
$ html-validator <local-url> --islocal

returns array of error messages
[
  {
    ""type"": ""error"",
    ""lastLine"": 8,
    ""lastColumn"": 32,
    ""firstColumn"": 27,
    ""message"": ""Stray end tag “div”."",
    ""extract"": ""aaaad code</div></p>\n<"",
    ""hiliteStart"": 10,
    ""hiliteLength"": 6
  }
]
Related

site-validator-cli CLI for validating a whole site or multiple pages
html-validator API for this module

License
MIT
",21
adafruit/Adafruit_NeoPixel,C++,"Adafruit NeoPixel Library 
Arduino library for controlling single-wire-based LED pixels and strip such as the Adafruit 60 LED/meter Digital LED strip, the Adafruit FLORA RGB Smart Pixel, the Adafruit Breadboard-friendly RGB Smart Pixel, the Adafruit NeoPixel Stick, and the Adafruit NeoPixel Shield.
After downloading, rename folder to 'Adafruit_NeoPixel' and install in Arduino Libraries folder. Restart Arduino IDE, then open File->Sketchbook->Library->Adafruit_NeoPixel->strandtest sketch.
Compatibility notes: Port A is not supported on any AVR processors at this time

Supported chipsets
We have included code for the following chips - sometimes these break for exciting reasons that we can't control in which case please open an issue!

AVR ATmega and ATtiny (any 8-bit) - 8 MHz, 12 MHz and 16 MHz
Teensy 3.x and LC
Arduino Due
Arduino 101
ATSAMD21 (Arduino Zero/M0 and other SAMD21 boards) @ 48 MHz
ATSAMD51 @ 120 MHz
Adafruit STM32 Feather @ 120 MHz
ESP8266 any speed
ESP32 any speed
Nordic nRF52 (Adafruit Feather nRF52), nRF51 (micro:bit)

Check forks for other architectures not listed here!

Roadmap
The PRIME DIRECTIVE is to maintain backward compatibility with existing Arduino sketches -- many are hosted elsewhere and don't track changes here, some are in print and can never be changed!
Please don't reformat code for the sake of reformatting code. The resulting large ""visual diff"" makes it impossible to untangle actual bug fixes from merely rearranged lines. (Exception for first item in wishlist below.)
Things I'd Like To Do But There's No Official Timeline So Please Don't Count On Any Of This Ever Being Canonical:

For the show() function (with all the delicate pixel timing stuff), break out each architecture into separate source files rather than the current unmaintainable tangle of #ifdef statements!
Please don't use updateLength() or updateType() in new code. They should not have been implemented this way (use the C++ 'new' operator with the regular constructor instead) and are only sticking around because of the Prime Directive. setPin() is OK for now though, it's a trick we can use to 'recycle' pixel memory across multiple strips.
In the M0 and M4 code, use the hardware systick counter for bit timing rather than hand-tweaked NOPs (a temporary kludge at the time because I wasn't reading systick correctly). (As of 1.4.2, systick is used on M4 devices and it appears to be overclock-compatible. Not for M0 yet, which is why this item is still here.)
As currently written, brightness scaling is still a ""destructive"" operation -- pixel values are altered in RAM and the original value as set can't be accurately read back, only approximated, which has been confusing and frustrating to users. It was done this way at the time because NeoPixel timing is strict, AVR microcontrollers (all we had at the time) are limited, and assembly language is hard. All the 32-bit architectures should have no problem handling nondestructive brightness scaling -- calculating each byte immediately before it's sent out the wire, maintaining the original set value in RAM -- the work just hasn't been done. There's a fair chance even the AVR code could manage it with some intense focus. (The DotStar library achieves nondestructive brightness scaling because it doesn't have to manage data timing so carefully...every architecture, even ATtiny, just takes whatever cycles it needs for the multiply/shift operations.)

",1694
CodaProtocol/coda,OCaml,"Coda
Coda is a new cryptocurrency protocol with a lightweight, constant sized blockchain.
Please see our developer README if you are interested in building coda from source code.
We have a Discord server! Please come by if you
need help or have questions. You might also be interested in the OCaml
Discord, for general OCaml help.
Table of Contents

Coda homepage
Roadmap
Developer homepage
Developer readme
Compiling from source and and running a node

Learn more

Directory structure
Lifecycle of a payment

License
This repository is distributed under the terms of the Apache 2.0 license,
available in the LICENSE file and online at
https://www.apache.org/licenses/LICENSE-2.0. Commits older than 2018-10-03 do
not have a LICENSE file or this notice, but are distributed under the same terms.
",243
sigmaai/self-driving-golf-cart,Makefile,"

    
Welcome! This is an open source self-driving development platform aimed for rapid prototyping, deep learning, and robotics research. The system currently runs on a modified electric golf cart. Here are our goals:
Goals:
Research and develop a deep learning-driven self-driving car. The vehicle should be able to achieve level 4 autonomy within a geofenced area.
The modules in this project.

End-to-End Steering (Behavioral cloning)
Semantic Segmentation
Object Detection 🚙
Drive by Wire (DBW)
CARLA simulator integration
ZED stereoscopic vision system
Mapping with rtabamp
Path planning with ROS nav stack.
Localization, pose tracking, and odom with ZED and rtabmap.

For the full documentation of the development process, please visit my website: www.neilnie.com
Table of Content

Try it out
About ROS
Simulation
Autopilot & End-to-End Behavioral Cloning

What's Behavioral Cloning
Model


Semantic Segmentation
The Navigation Stack

RTABMap
Path Planning
Vehicle Motion Control




Try it out
Before you jump in, let me describe the hardware requirement for this project. A webcam is the minimum requirment. At this point, you can only run the whole system on the actual self-driving vehicle. ROS will throw warnings (even errors) at you if you don't have the hardware connected to your Linux machine. If you don't have access to the hardware setup, don't worry 👇

The best way is to download and play back the ROS bags. [coming soon...]
You can tryout individual packages and nodes, and might find them helpful for your own projects.
You can also tryout the CARLA simulator. (Maybe even improve the current system.)

To compile the project:
Requirements

Make sure that you have ROS installed on your computer. (I am using ROS Melodic)
Make sure you have all the dependencies installed.

Clone & Compile

Clone the repository. $ git clone https://github.com/sigmaai/self-driving-golf-cart.git
$ cd self-driving-golf-cart/ros
$ catkin_make
$ source devel/setup.bash

Launch ZED cameras

$ roslaunch zed_wrapper zed.launch (no rviz)
$ roslaunch zed_display display.launch (with rviz)

Launch the Navigation Stack

$ roslaunch path_planning rtab_mapping_navigation.launch


🚙 Bon Voyage 😀
 
About ROS
This project uses ROS. For more information on ROS, nodes, topics and others please refer to the ROS README.
 
Simulation
(🏗 Construction Zone 🚧)
Building a self-driving car is hard. Not everyone has access to expensive hardware. I am currently trying to integrate this project with the CARLA self-driving simulator. If you are interested in CARLA, please refer to this documentation. The ROS system in this project can partially run on the CARLA simulator.
If you want to try out the simulator, please refer to the documentation here.


Autopilot & End-to-End Behavioral Cloning
The autopilot system, found here in the autopilot node, uses deep learning to predict the steering commands and acceleration commands for the vehicle, only using data collected by the front facing camera.
 
What's Behavioral Cloning
In 2016, NVIDIA proposed a novel deep learning approach allowed their car to accurately perform real-time end-to-end steering command prediction. Around the same time, Udacity held a challenge that asked researchers to create the best end-to-end steering prediction model. Our goal is to further the work in behavioral cloning for self-driving vehicles.
 
Model
NVIDIA's paper used a convolutional neural network with a single frame input. I believe that the single-frame-input CNN doesn't provide any temporal information which is critical in self-driving. This is the motive behind choosing the i3d architecture, which is rich in spacial-temporal information.
The input of the network is a 3d convolutional block, with the shape of n * weight * height * 3. n is the length of the input sequence. A flatten layer and a dense layer are added to the back of the network for the purpose of this regression problem.

Here is a video demo of deep learning model running on the autonomous golf cart.
VIDEO DEMO
 
Semantic Segmentation
The cart understands its surrounding  through semantic segmentation, which is a technique in computer that classifies each pixel in an image into different categories. The vehicle can also make decisions based on the segmentic segmentation results. The cart can change its speed based on the proximity to nearby obstacles.

We deployed the ENet architecture for segmentation. ENet is design to work well in realtime applications. For more information, please visit the paper. We used the CityScape dataset for training and the python code for training and inferencing are located in the ./src/segmentation/scripts directory.
VIDEO DEMO
 
The Navigation Stack

The self-driving vehicle uses a modified version of the ROS navigation stack. The flowchart above illustrate the mapping and path planning process. First, I create a detailed map of the environment with rtabmap_ros. With that global map, I use the localization feature of rtabmap_ros and the odom feature of the zed camera system to localize and plan paths.
 
RTABMap
rtabmap (realtime appearance based mapping) allows me to construct a global map of the environment. For more information on the mapping package, please check out this .launch file.

 
Path Planning
The project uses the move_base node from the navigation stack. The image below shows the costmap (in blue and purple), and the global occupancy grid (in black and gray). move_base also plans the local and global path. Global paths are shown in green and yellow below. You can find the yaml files here.

 
Vehicle Motion Control
The move base node publishes /cmd_vel commands, which are processed and sent directly to the vehicle.

Contact / Info
If you are interested in the detailed development process of this project, you can visit Neil's blog at neilnie.com to find out more about it. Neil will make sure to keep you posted about all of the latest development on the club.
Developers:

Neil (Yongyang) Nie | Email | Github | Website | Linkedin

Michael Meng | Email | Github
",48
zrrrzzt/html-validator-cli,JavaScript,"


html-validator-cli
CLI for validating html using validator.w3.org/nu
Requires Node >= 8.15.3 for older versions use v5.0.0
Sends Page is validto STDOUT and exits with code 0 if page is valid.
Sends Page is not valid to STDOUT and exits with code 1 if page is not valid.
Sends Page not found to STDOUT and exits with code 1 if page is not found.
Installation
$ npm i html-validator-cli -g

Usage
$ html-validator <url>

With file
$ html-validator --file=<path-to-file>

With data
$ html-validator --data=data

Optional pass in format for returned data.
Valid options: json, html, xhtml, xml, gnu and text (default).
$ html-validator <url> --format=gnu

Optional pass in another validator.
It needs to expose the same REST interface.
$ html-validator <url> --validator='http://html5.validator.nu'

Optional pass in strings to ignore
$ html-validator <url> --ignore='Error: Stray end tag “div”.' --ignore='Error: Stray end tag “body”.'

Optional pass in headers
$ html-validator <url> --headers='{""foo"":""doo""}'

To get full result from validator use --verbose
$ html-validator <url> --verbose

Optional, only get errors use --quiet
$ html-validator <url> --quiet

Validate a local document without setting up a tunnel
$ html-validator <local-url> --islocal

returns array of error messages
[
  {
    ""type"": ""error"",
    ""lastLine"": 8,
    ""lastColumn"": 32,
    ""firstColumn"": 27,
    ""message"": ""Stray end tag “div”."",
    ""extract"": ""aaaad code</div></p>\n<"",
    ""hiliteStart"": 10,
    ""hiliteLength"": 6
  }
]
Related

site-validator-cli CLI for validating a whole site or multiple pages
html-validator API for this module

License
MIT
",21
adafruit/Adafruit_NeoPixel,C++,"Adafruit NeoPixel Library 
Arduino library for controlling single-wire-based LED pixels and strip such as the Adafruit 60 LED/meter Digital LED strip, the Adafruit FLORA RGB Smart Pixel, the Adafruit Breadboard-friendly RGB Smart Pixel, the Adafruit NeoPixel Stick, and the Adafruit NeoPixel Shield.
After downloading, rename folder to 'Adafruit_NeoPixel' and install in Arduino Libraries folder. Restart Arduino IDE, then open File->Sketchbook->Library->Adafruit_NeoPixel->strandtest sketch.
Compatibility notes: Port A is not supported on any AVR processors at this time

Supported chipsets
We have included code for the following chips - sometimes these break for exciting reasons that we can't control in which case please open an issue!

AVR ATmega and ATtiny (any 8-bit) - 8 MHz, 12 MHz and 16 MHz
Teensy 3.x and LC
Arduino Due
Arduino 101
ATSAMD21 (Arduino Zero/M0 and other SAMD21 boards) @ 48 MHz
ATSAMD51 @ 120 MHz
Adafruit STM32 Feather @ 120 MHz
ESP8266 any speed
ESP32 any speed
Nordic nRF52 (Adafruit Feather nRF52), nRF51 (micro:bit)

Check forks for other architectures not listed here!

Roadmap
The PRIME DIRECTIVE is to maintain backward compatibility with existing Arduino sketches -- many are hosted elsewhere and don't track changes here, some are in print and can never be changed!
Please don't reformat code for the sake of reformatting code. The resulting large ""visual diff"" makes it impossible to untangle actual bug fixes from merely rearranged lines. (Exception for first item in wishlist below.)
Things I'd Like To Do But There's No Official Timeline So Please Don't Count On Any Of This Ever Being Canonical:

For the show() function (with all the delicate pixel timing stuff), break out each architecture into separate source files rather than the current unmaintainable tangle of #ifdef statements!
Please don't use updateLength() or updateType() in new code. They should not have been implemented this way (use the C++ 'new' operator with the regular constructor instead) and are only sticking around because of the Prime Directive. setPin() is OK for now though, it's a trick we can use to 'recycle' pixel memory across multiple strips.
In the M0 and M4 code, use the hardware systick counter for bit timing rather than hand-tweaked NOPs (a temporary kludge at the time because I wasn't reading systick correctly). (As of 1.4.2, systick is used on M4 devices and it appears to be overclock-compatible. Not for M0 yet, which is why this item is still here.)
As currently written, brightness scaling is still a ""destructive"" operation -- pixel values are altered in RAM and the original value as set can't be accurately read back, only approximated, which has been confusing and frustrating to users. It was done this way at the time because NeoPixel timing is strict, AVR microcontrollers (all we had at the time) are limited, and assembly language is hard. All the 32-bit architectures should have no problem handling nondestructive brightness scaling -- calculating each byte immediately before it's sent out the wire, maintaining the original set value in RAM -- the work just hasn't been done. There's a fair chance even the AVR code could manage it with some intense focus. (The DotStar library achieves nondestructive brightness scaling because it doesn't have to manage data timing so carefully...every architecture, even ATtiny, just takes whatever cycles it needs for the multiply/shift operations.)

",1694
CodaProtocol/coda,OCaml,"Coda
Coda is a new cryptocurrency protocol with a lightweight, constant sized blockchain.
Please see our developer README if you are interested in building coda from source code.
We have a Discord server! Please come by if you
need help or have questions. You might also be interested in the OCaml
Discord, for general OCaml help.
Table of Contents

Coda homepage
Roadmap
Developer homepage
Developer readme
Compiling from source and and running a node

Learn more

Directory structure
Lifecycle of a payment

License
This repository is distributed under the terms of the Apache 2.0 license,
available in the LICENSE file and online at
https://www.apache.org/licenses/LICENSE-2.0. Commits older than 2018-10-03 do
not have a LICENSE file or this notice, but are distributed under the same terms.
",243
LuoShengMen/StudyNotes,None,"前端之路

乾坤未定，诸事可为



前端知识框架


学习读书笔记


每日面试题学习


算法学习


学习资料


前端库


解惑之文


优秀者Blog


面经


前端进阶




讶羽Blog
木易杨进阶系列


高效学习



",6
KillzXGaming/Switch-Toolbox,C#,"Switch-Toolbox
A tool to edit many formats of Nintendo Switch and Wii U.
Changelog 1.0 Experimental / BETA
https://docs.google.com/spreadsheets/d/16JLhGBJL5U5hpKWspL-pzYIaRL23X1YKEmia6pbsGbc/edit#gid=1386834576
Releases
https://github.com/KillzXGaming/Switch-Toolbox/releases
Features
This tool currently features:

BFRES

Fully supports Wii U and Switch.
Model importing (dae, fbx, obj, and csv)
Material editing (Render info, texture mapping, parameters, etc)
Material copying
Animation and model sub section can be exported/imported.
Can delete, add, replace individual objects from an fmdl.
Can create new sub sections and data
Can preview skeletal, SRT, param, texture pattern, and bone visual animations. (Param ones will vary)
Can export and import fully rigged models with bone support.
Can convert gif files to texture pattern animations. Very WIP atm.



Can Edit Formats

BFRES
BNTX
NUTEXB
XTX
SARC
BARS
KCL
BFFNT
BFLIM
GFPAK
BEA
AAMP (Wii U and Switch)
BYAML/BYML (Wii U, 3DS, and Switch)
PTCL (Wii U, 3DS, and Switch)
TMPK

Can Preview

BCRES

Models, materials, and textures.


BFSHA

Can view options, samplers, attributes, and uniform blocks.


BNSH

Can extract shader vertex and fragment shaders from variations/programs


SHARCFB and SHARC

Basic preview of some shader program data.
Can edit both v1 and v2 AAMP (Wii U and Switch)


EFC

Can preview effect tables and link PTCL.


NUT

Can preview NTWU, NTP3, and NTWD variants. Editng will be soon


MSBT

Very basic previewing.


TMPK

Can extract files and decompress
Can save back.


MP3, OGG, IDSP, HPS, WAV, BFWAV, BFSTM, BCWAV, BCWAV

Can listen to audio and convert between certain formats. Thanks to VGAudio and CSCore



Building
To build make sure you have Visual Studio installed (I use 2017, older versions may not work) and open the .sln. Then build the solution as release. It should compile properly on the latest.
In the event that the tool cannot compile, check references. All the libraries are stored in Switch-Toolbox/Lib folder.

Smash Forge Devs (SMG, Ploaj,  jam1garner, smb123w64gb, etc) for some code ported over. Specifically animation stuff and some rendering.
Assimp devs for their massive asset library!
Wexos (helped figure out a few things, ie format list to assign each attribute)
JuPaHe64 for the base 3D renderer.
Every File Explorer devs (Gericom) for Yaz0 stuff
Exelix for Byaml, Sarc and KCL library
Syroot for helpful IO extensions and libraries
GDKChan for PICA shaders stuff used with bcres, structs for bcres, and some DDS decode methods
AboodXD for some foundation stuff with exelix's SARC library, Wii U (GPU7) and Switch (Tegra X1) textures swizzling, and documentation for GTX, XTX, and BNTX
MelonSpeedruns for logo.

Resources

Treeview Icons by icons8
Smash Forge (Currently placeholders)

Libraries

Exelix (Sarc, kcl, and byml libraries)
ZstdNet (Compression)
Be.HexEditor by Bernhard Elbl
GL EditorFramwork by jupahe64
WeifenLuo for docking suite
SF Graphics by SMG (Experimental (currently just a placeholder for shader workflow and some useful things)
Audio & MIDI library
VGAudio
CSCore
Assimp
OpenTK
BezelEngineArchive Library
Syroot BinaryData
Syroot Maths
Syroot Bfres Library (Wii U)

License
in Switch_Toolbox\Lib\Licenses
Please note if you do not want your library used or if i'm missing credits!
",45
zrrrzzt/tfk-api-unoconv,JavaScript,"

tfk-api-unoconv
Unoconv as a webservice
Docker
Build image
$ docker build -t unoconv-webservice .
Run image
$ docker run -d -p 80:3000 --name unoconv-webservice unoconv-webservice
Usage
Post the file you want to convert to the server and get the converted file in return.
See all possible conversions on the unoconv website.
API for the webservice is /unoconv/{format-to-convert-to} so a docx to pdf would be
$ curl --form file=@myfile.docx http://localhost/unoconv/pdf > myfile.pdf
Formats
To see all possible formats for convertion visit /unoconv/formats
To see formats for a given type /unoconv/formats/{document|graphics|presentation|spreadsheet}
Versions
To see all versions of installed dependencies lookup /unoconv/versions
Healthz
Are we alive? /healthz
returns
{
  uptime: 18.849
}
Environment
You can change the webservice port and filesize-limit by changing environment variables.
SERVER_PORT default is 3000
PAYLOAD_MAX_SIZE default is 1048576 (1 MB)
PAYLOAD_TIMEOUT default is 2 minutes (120 000 milliseconds)
TIMEOUT_SERVER default is 2 minutes (120 000 milliseconds)
TIMEOUT_SOCKET default is 2 minutes and 20 seconds (140 000 milliseconds)
Change it in the Dockerfile or create an env-file and load it at containerstart
$ docker run --env-file=docker.env -d -p 80:3000 --name unoconv-webservice unoconv-webservice
License
MIT
",43
CreadDiscans/ServiceDesigner,TypeScript,"development
npm install
npm start
build
npm install
npm run dist
todo
folder manager validation
element manager validation
css manager validation
asset manager validation
color manager validation
data manager validation
layout manager validation
react design 테스트
react-native design 테스트
how to use


Folder : To create folder or file, should select folder to be parent.


State : The scheme is json.


Style : The Style supports following special shceme.
Color.XXX : Support Color variables. The variables should be defined color tab.
Asset.XXX : Support Asset variables. The variables should be defined asset tab.
ex ) { ""backgroundColor"": ""Color.red"" }


Property : the property supports following special shceme.
{this.state.XXX} : XXX is state variable.
{item} or {item.XXX} : This is for ""for loop"" scheme.
Asset.XXX : Asset tab scheme.


",2
pencil1/ApiTestManage,Python,"ApiTestManage
感觉项目不错的点个star，你的支持是作者源源不断的动力~谢谢！！如有疑问可联系qq：362508572   或q群：700387899 或issue
前端传送门：https://github.com/pencil1/ApiTestWeb
线上demo地址：http://47.107.147.188/#/login （账号：ceshi 密码：123456）
Environment
python => 3
安装依赖包
pip install -r requirements.txt

使用flask命令，必须先设置：
设置flask的app(windows和linux的环境变量命令不一样，项目根目录下执行)
set FLASK_APP=manage.py(windows)

export FLASK_APP=manage.py(linux)

然后创建管理员账号（账号：admin，密码：123456，项目根目录下执行）
flask initdata

开发环境
python manage.py

生产环境
gunicorn -c gunicorn_config.py manage:app

生产环境下的一些配置
由于懒，直接把flaskapi.conf文件替换nginx下的nginx.conf
数据库的迁移(数据库迁移不是必要步骤)
初始化：
(venv)  flask db init 这个命令会在项目下创建 migrations 文件夹，所有迁移脚本都存放其中。

创建第一个版本：
(venv) $ flask db migrate

运行升级
(venv) $ flask db upgrade

后缀更新：
更新表格的字段 (models.py)
再次运行一下
flask db migrate -> 相当于commit 更新到/migrate目录
flask db upgrade -> 数据库会更新

",89
pencil1/ApiTestManage,Python,"ApiTestManage
感觉项目不错的点个star，你的支持是作者源源不断的动力~谢谢！！如有疑问可联系qq：362508572   或q群：700387899 或issue
前端传送门：https://github.com/pencil1/ApiTestWeb
线上demo地址：http://47.107.147.188/#/login （账号：ceshi 密码：123456）
Environment
python => 3
安装依赖包
pip install -r requirements.txt

使用flask命令，必须先设置：
设置flask的app(windows和linux的环境变量命令不一样，项目根目录下执行)
set FLASK_APP=manage.py(windows)

export FLASK_APP=manage.py(linux)

然后创建管理员账号（账号：admin，密码：123456，项目根目录下执行）
flask initdata

开发环境
python manage.py

生产环境
gunicorn -c gunicorn_config.py manage:app

生产环境下的一些配置
由于懒，直接把flaskapi.conf文件替换nginx下的nginx.conf
数据库的迁移(数据库迁移不是必要步骤)
初始化：
(venv)  flask db init 这个命令会在项目下创建 migrations 文件夹，所有迁移脚本都存放其中。

创建第一个版本：
(venv) $ flask db migrate

运行升级
(venv) $ flask db upgrade

后缀更新：
更新表格的字段 (models.py)
再次运行一下
flask db migrate -> 相当于commit 更新到/migrate目录
flask db upgrade -> 数据库会更新

",89
csj4032/primavera,JavaScript,"Primavera
스프링부트를 이용한 커뮤니티 사이트 개발
Technical Specification

Java 12 (Switch expressions)
IntelliJ IDEA (2019.1)
MariaDB 10.3.14
Springboot 2.1.4.RELEASE
Thymeleaf
Mybatis
Lombok
Bootstrap

Requirements Specification

Social 정보를 이용한 회원 가입
Social 정보를 이용한 로그인, 로그아웃, 탈퇴
게시글 등록, 수정, 삭제, 조회 기능
게시글 덧글 등록 기능
게시글 댓글 등록, 수정, 삭제 기능 및 대댓글 등록, 수정, 삭제
게시글 관련 첨부파일 등록, 삭제
게시글 편집 기능

Launch
chap01

스프링 부트 설정

chap02

MariaDB 연결 및 테스트

chap03

Springboot DataSource

chap04

Mybatis

chap05

Validation

chap06

Thymeleaf

chap07

Filter

chap08

Spring Security

chap09

Spring OAuth2 Social Login

chap10

Post, Pagination, wysihtml5

Library Version

gradle 라이브러리 버전 정보 정리
lombok = '1.18.6'
logback = '1.2.3'

ERD
SQL
CREATE DATABASE primavera DEFAULT CHARACTER SET utf8mb4;

CREATE USER 'primavera'@'localhost' IDENTIFIED BY 'primavera';
GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, ALTER ON primavera.* TO 'primavera'@'localhost';

CREATE TABLE USER (
  ID BIGINT(20) NOT NULL AUTO_INCREMENT,
  EMAIL VARCHAR(50) NOT NULL,
  PASSWORD VARCHAR(100) NOT NULL,
  NICKNAME VARCHAR(45) NOT NULL,
  STATUS CHAR(1) NOT NULL DEFAULT 'A',
  REG_DATE DATETIME NOT NULL,
  MOD_DATE DATETIME DEFAULT NULL,
  PRIMARY KEY (ID),
  UNIQUE KEY EMAIL_UNIQUE (EMAIL)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

CREATE TABLE USER_CONNECTION (
  ID BIGINT(20) NOT NULL AUTO_INCREMENT,
  EMAIL VARCHAR(50) NOT NULL,
  PROVIDER TINYINT(11) NOT NULL,
  PROVIDER_ID BIGINT(20) NOT NULL,
  DISPLAY_NAME VARCHAR(45) DEFAULT NULL,
  PROFILE_URL VARCHAR(200) DEFAULT NULL,
  IMAGE_URL VARCHAR(200) DEFAULT NULL,
  ACCESS_TOKEN VARCHAR(200) NOT NULL,
  EXPIRE_TIME BIGINT(20) DEFAULT NULL,
  PRIMARY KEY (ID)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

CREATE TABLE USER_ROLE (
  USER_ID BIGINT(20) NOT NULL,
  ROLE_ID INT(11) NOT NULL,
  PRIMARY KEY (USER_ID, ROLE_ID)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

CREATE TABLE ROLE (
  ID INT(11) NOT NULL AUTO_INCREMENT,
  TYPE TINYINT(3) NOT NULL,
  PRIMARY KEY (ID)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

CREATE TABLE POST (
  ID BIGINT(20) NOT NULL AUTO_INCREMENT,
  WRITER_ID BIGINT(20) NOT NULL,
  SUBJECT VARCHAR(200) NOT NULL,
  CONTENTS TEXT NOT NULL,
  STATUS TINYINT(3) NOT NULL,
  REG_DT DATETIME DEFAULT NULL,
  MOD_DT DATETIME DEFAULT NULL,
  PRIMARY KEY (ID),
  KEY FK_WRITER_ID (WRITER_ID),
  CONSTRAINT FK_WRITER_ID FOREIGN KEY (WRITER_ID) REFERENCES USER (ID) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=INNODB DEFAULT CHARSET=utf8mb4;

INSERT INTO ROLE (TYPE) VALUES (1);
INSERT INTO ROLE (TYPE) VALUES (2);
INSERT INTO ROLE (TYPE) VALUES (3);

INSERT INTO `USER`(EMAIL, PASSWORD, NICKNAME, STATUS,REG_DATE) VALUES ('Genius Choi', '{bcrypt}$2a$10$7UEHLpn1r4gZY2qxiZFJ5.7wa3Hdz8IXgxUtFogy0Ac10fh7TG4V.', 'Genius', 1, NOW());
INSERT INTO `USER_CONNECTION`(EMAIL, PROVIDER, PROVIDER_ID, DISPLAY_NAME, PROFILE_URL, IMAGE_URL, ACCESS_TOKEN, EXPIRE_TIME) VALUES ('Genius Choi', 1, 123456789, 'Genius', 'PROFILE', 'IMAGE', '1234567890', -1);
INSERT INTO `USER_ROLE`(`USER_ID`, ROLE_ID) VALUES (1,1);
INSERT INTO `USER_ROLE`(`USER_ID`, ROLE_ID) VALUES (1,2);
INSERT INTO `USER_ROLE`(`USER_ID`, ROLE_ID) VALUES (1,3);

INSERT INTO `USER`(`EMAIL`,`PASSWORD`,`NICKNAME`,`STATUS`,`REG_DATE`) VALUES ('Son Heung-min', '{bcrypt}$2a$10$7UEHLpn1r4gZY2qxiZFJ5.7wa3Hdz8IXgxUtFogy0Ac10fh7TG4V.', 'Son', 1, NOW());
INSERT INTO `USER_ROLE`(`USER_ID`, ROLE_ID) VALUES (2,1);
INSERT INTO `USER_ROLE`(`USER_ID`, ROLE_ID) VALUES (2,2);
INSERT INTO `USER_ROLE`(`USER_ID`, ROLE_ID) VALUES (2,3);

INSERT INTO `USER`(`EMAIL`,`PASSWORD`,`NICKNAME`,`STATUS`,`REG_DATE`) VALUES ('Lionel Messi', '{bcrypt}$2a$10$7UEHLpn1r4gZY2qxiZFJ5.7wa3Hdz8IXgxUtFogy0Ac10fh7TG4V.', 'Messi', 1, NOW());
INSERT INTO `USER_ROLE`(`USER_ID`, ROLE_ID) VALUES (3,1);
INSERT INTO `USER_ROLE`(`USER_ID`, ROLE_ID) VALUES (3,2);

INSERT INTO `USER`(`EMAIL`,`PASSWORD`,`NICKNAME`,`STATUS`,`REG_DATE`) VALUES ('Cristiano Ronaldo', '{bcrypt}$2a$10$7UEHLpn1r4gZY2qxiZFJ5.7wa3Hdz8IXgxUtFogy0Ac10fh7TG4V.', 'Ronaldo', 1, NOW());
INSERT INTO `USER_ROLE`(`USER_ID`, ROLE_ID) VALUES (4,1);

",3
expressjs/express,JavaScript,"
Fast, unopinionated, minimalist web framework for node.





const express = require('express')
const app = express()

app.get('/', function (req, res) {
  res.send('Hello World')
})

app.listen(3000)
Installation
This is a Node.js module available through the
npm registry.
Before installing, download and install Node.js.
Node.js 0.10 or higher is required.
Installation is done using the
npm install command:
$ npm install express
Follow our installing guide
for more information.
Features

Robust routing
Focus on high performance
Super-high test coverage
HTTP helpers (redirection, caching, etc)
View system supporting 14+ template engines
Content negotiation
Executable for generating applications quickly

Docs & Community

Website and Documentation - [website repo]
#express on freenode IRC
GitHub Organization for Official Middleware & Modules
Visit the Wiki
Google Group for discussion
Gitter for support and discussion

PROTIP Be sure to read Migrating from 3.x to 4.x as well as New features in 4.x.
Security Issues
If you discover a security vulnerability in Express, please see Security Policies and Procedures.
Quick Start
The quickest way to get started with express is to utilize the executable express(1) to generate an application as shown below:
Install the executable. The executable's major version will match Express's:
$ npm install -g express-generator@4
Create the app:
$ express /tmp/foo && cd /tmp/foo
Install dependencies:
$ npm install
Start the server:
$ npm start
View the website at: http://localhost:3000
Philosophy
The Express philosophy is to provide small, robust tooling for HTTP servers, making
it a great solution for single page applications, web sites, hybrids, or public
HTTP APIs.
Express does not force you to use any specific ORM or template engine. With support for over
14 template engines via Consolidate.js,
you can quickly craft your perfect framework.
Examples
To view the examples, clone the Express repo and install the dependencies:
$ git clone git://github.com/expressjs/express.git --depth 1
$ cd express
$ npm install
Then run whichever example you want:
$ node examples/content-negotiation
Tests
To run the test suite, first install the dependencies, then run npm test:
$ npm install
$ npm test
Contributing
Contributing Guide
People
The original author of Express is TJ Holowaychuk
The current lead maintainer is Douglas Christopher Wilson
List of all contributors
License
MIT
",43834
NixOS/nixpkgs,Nix,"

Nixpkgs is a collection of packages for the Nix package
manager. It is periodically built and tested by the Hydra
build daemon as so-called channels. To get channel information via git, add
nixpkgs-channels as a remote:
% git remote add channels https://github.com/NixOS/nixpkgs-channels.git

For stability and maximum binary package support, it is recommended to maintain
custom changes on top of one of the channels, e.g. nixos-19.03 for the latest
release and nixos-unstable for the latest successful build of master:
% git remote update channels
% git rebase channels/nixos-19.03

For pull requests, please rebase onto nixpkgs master.
NixOS Linux distribution source code is located inside
nixos/ folder.

NixOS installation instructions
Documentation (Nix Expression Language chapter)
Manual (How to write packages for Nix)
Manual (NixOS)
Community maintained wiki
Continuous package builds for unstable/master
Continuous package builds for 19.03 release
Tests for unstable/master
Tests for 19.03 release

Communication:

Discourse Forum
IRC - #nixos on freenode.net

Note: MIT license does not apply to the packages built by Nixpkgs, merely to
the package descriptions (Nix expressions, build scripts, and so on). It also
might not apply to patches included in Nixpkgs, which may be derivative works
of the packages to which they apply. The aforementioned artifacts are all
covered by the licenses of the respective packages.
",3717
coreswitch/openconfigd,C,"openconfigd
openconfigd is software which manages OpenConfig
common data models for networking. It handles networking protocol configuration
of switch, router, DNS, DHCP, NAT and Firewall.
openconfigd reads YANG model definition then generate configuration schema from
it.
Install
Below command install openconfigd and cli_command to $GOPATH/bin.
$ go get github.com/coreswitch/openconfigd/openconfigd
$ go get github.com/coreswitch/openconfigd/cli_command
CLI command build and set up.
$ cd $GOPATH/src/github.com/coreswitch/openconfigd/cli
$ ./configure
$ make
$ sudo make install
will install cli command to /usr/local/bin.
CLI completion file another cli file exists under bash_completion.d. On
Ubuntu, this file should be installed under /etc/bach_completion.d
$ cd $GOPATH/src/github.com/coreswitch/openconfigd/bash_completion.d
$ sudo cp cli /etc/bash_completion.d/
Quick Start
Invoke openconfigd, then start cli.  ""show version"" display version information.
$ openconfigd &
$ cli
ubuntu>
ubuntu> show version
Developer Preview version of openconfigd
ubuntu>
Options
openconfigd takes YANG module names as arguments.  When no YANG module is specified, default coreswitch.yang is used.  '.yang' suffix is optional.  Use can specify multiple YANG file.  So
$ openconfigd lagopus ietf-ip
will load both lagopus.yang and ietf-ip.yang modules.
There are several other options.

-c, --config-file= active config file name (default: coreswitch.conf)
-p, --config-dir=  config file directory (default: /usr/local/etc)
-y, --yang-paths=  colon separated YANG load path directories
-2, --two-phase    enable two phase commit
-z, --zero-config  do not save or load config other than openconfigd.conf
-h, --help         Show this help message

-c option specify active config file name.  -p option specify config file save directory.  When full path is specified to -c option's base directory overrides the -p option config file directory.
-y option specify YANG file load path.  Use can specify multiple YANG load path with colon separated list.
-2 option enables two phase commit.  It send validate start and end message to protocol modules.
When -z option is specified, only openconfigd.conf file is loaded on start up.  Configuraion is never saved to the config file.
$ openconfigd -y /usr/shared/yang:/opt/yang
will search both /usr/shared/yang and /opt/yang directory.  Default YANG load path $GOPATH/src/github.com/coreswitch/openconfigd/yang is automatically added.
openconfigd scripting
openconfigd support CLI scripting. All operational and configuration mode
commands can run from script.
Here is an example:
#! /bin/bash

source /etc/bash_completion.d/cli

show version
",30
rust-lang-nursery/getopts,Rust,"getopts
A Rust library for option parsing for CLI utilities.

Documentation
Usage
Add this to your Cargo.toml:
[dependencies]
getopts = ""0.2""
and this to your crate root:
extern crate getopts;
Rust Version Support
The minimum supported Rust version is 1.18.
",138
Lombiq/Orchard-Application-Host,C#,"Orchard Application Host Readme
Project Description
A light-weight framework that allows you to write arbitrary code (console or web applications, anything) empowered with Orchard.
Overview
The Orchard Application Host is a portable environment that lets you run your application inside a standalone Orchard shell. I.e. you can write any app with an Orchard developer experience, without using an Orchard web app. This enables you to use Orchard's features and services from any application (not just web applications), including:

Automatic dependency injection
Helpers and utilities
Data access services, including content management
Orchard-style events
Shapes
Caching
Localization
Logging
Background tasks

With Orchard Application Host you can create console applications, Windows services, desktop applications, cloud workers or any other type of app that uses Orchard's capabilities. No more low-level project start: you get an application framework that you can begin developing awesome software with, utilizing your Orchard knowledge and Orchard's power.
You can see a demo of the Orchard Application Host on the recording of the Orchard Community Meeting.
Among others Orchard Application Host powers the reverse proxy of the Hosting Suite and Hastlayer too.
The project's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/orchard-application-host (Mercurial repository)
https://github.com/Lombiq/Orchard-Application-Host (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
Using Orchard App Host as source in a solution

See examples in Lombiq.OrchardAppHost.Sample and for a full usage scenario with a non-Orchard solution in the Orchard Application Host Quick Start.
Disable SessionConfigurationCache otherwise you'll get ""The invoked member is not supported in a dynamic assembly."" exceptions that are harmless but prevent the session cache from being used anyway.
You'll get a ""The invoked member is not supported in a dynamic assembly."" exception during the first startup from AbstractDataServicesProvider but this is harmless.
Also from AbstractDataServicesProvider you'll get a ""Could not load file or assembly 'NHibernate.XmlSerializers ...' or one of its dependencies. The system cannot find the file specified."" exception that is also harmless.
If you want to use anything, even indirectly, from Orchard.Core, you have to add a project reference to it. E.g. even if you don't access anything from Orchard.Core but you use a service that gets ISiteService injected what in turn has an implementation in Orchard.Core then you indirectly depend on Orchard Core; thus, you have to add a project reference to it.
When using SQL CE you should add a reference to its assembly System.Data.SqlServerCe and set it as Copy Local = true.
Imported extensions don't need to declare a Module.txt but still can have features: by default they get a feature with the same name as the assembly's (short) name and also all OrchardFeature attribute usages will be processed and their values registered as features.
Note that starting Orchard App Host will currently take over ASP.NET MVC and Web API controller instantiation, see this Orchard issue.

Solution structure
The solution must follow this folder structure:

NuGet.config (see explanation below)
Lombiq.OrchardAppHost

Lombiq.OrchardAppHost.csproj


Orchard (a full Orchard source, i.e. the lib, src folder under it)
Arbitrarily named subfolder for 3rd party modules, e.g. Modules. Put your own modules here.

Module1

Module1.csproj





The Orchard Application Host Quick Start solution shows these conventions.
Configuring NuGet
A custom NuGet.config file is needed in the root of your solution so NuGet packages used by Orchard can be properly loaded. This should configure repositoryPath as following:
<?xml version=""1.0"" encoding=""utf-8""?>
<configuration>
  <config>
    <add key=""repositoryPath"" value=""Orchard\src\packages"" />
  </config>
</configuration>

Also see the example in the Orchard Application Host Quick Start.
Making assembly references compatible with different solution structures
3rd party modules may reference dlls from the Orchard lib folder or use the same NuGet packages as Orchard. By default these references will break since modules in an Orchard solution are under src/Orchard.Web/Modules, not above the Orchard folder (and thus paths differ). To make a module compatible with both standard Orchard solutions and Orchard App Host solutions add the following elements to the modules's csproj:
<!-- Orchard App Host (https://github.com/Lombiq/Orchard-Application-Host) compatibility start. Enabling the usage of a lib folder at a different location. -->
<ItemGroup>
  <LibReferenceSearchPathFiles Include=""..\..\Orchard\lib\**\*.dll"">
    <InProject>false</InProject>
  </LibReferenceSearchPathFiles>
  <NuGetReferenceSearchPathFiles Include=""..\..\Orchard\src\packages\**\*.dll"">
    <InProject>false</InProject>
  </NuGetReferenceSearchPathFiles>
</ItemGroup>
<Target Name=""BeforeResolveReferences"">
  <RemoveDuplicates Inputs=""@(LibReferenceSearchPathFiles->'%(RootDir)%(Directory)')"">
    <Output TaskParameter=""Filtered"" ItemName=""LibReferenceSearchPath"" />
  </RemoveDuplicates>
  <CreateProperty Value=""@(LibReferenceSearchPath);$(AssemblySearchPaths)"">
    <Output TaskParameter=""Value"" PropertyName=""AssemblySearchPaths"" />
  </CreateProperty>
  <RemoveDuplicates Inputs=""@(NuGetReferenceSearchPathFiles->'%(RootDir)%(Directory)')"">
    <Output TaskParameter=""Filtered"" ItemName=""NuGetReferenceSearchPath"" />
  </RemoveDuplicates>
  <CreateProperty Value=""@(NuGetReferenceSearchPath);$(AssemblySearchPaths)"">
    <Output TaskParameter=""Value"" PropertyName=""AssemblySearchPaths"" />
  </CreateProperty>
</Target>
<PropertyGroup Condition=""Exists('..\..\Orchard\lib')"">
  <ModulesRoot>..\..\Orchard\src\Orchard.Web\Modules\Orchard.Alias\</ModulesRoot>
</PropertyGroup>
<!-- Orchard App Host (https://github.com/Lombiq/Orchard-Application-Host) compatibility end. -->

Also make sure to prefix every project reference that points to one of Orchard's built-in projects with $(ModulesRoot) (assembly references don't need to be changed):
<ProjectReference Include=""$(ModulesRoot)..\..\..\Orchard\Orchard.Framework.csproj"">
  <Project>{2D1D92BB-4555-4CBE-8D0E-63563D6CE4C6}</Project>
  <Name>Orchard.Framework</Name>
</ProjectReference>
<ProjectReference Include=""$(ModulesRoot)..\..\..\Orchard\Orchard.Framework.csproj"">
  <Project>{2D1D92BB-4555-4CBE-8D0E-63563D6CE4C6}</Project>
  <Name>Orchard.Framework</Name>
  <Private>false</Private>
</ProjectReference>

",3
zrrrzzt/generate-pincode,JavaScript,"


generate-pincode
Node.js module for generating random pincodes.
Installation
$ npm install generate-pincode
Usage
var gpc = require('generate-pincode')
var pin = gpc(4)

console.log(pin)

// => 1234
Related

generate-pincode-cli CLI of this module

License
MIT
",2
mlpack/mlpack,C++,"

a fast, flexible machine learning library


Home |
  Documentation |
  Doxygen |
  Community |
  Help |
  IRC Chat









    Download:
    current stable version (3.1.0)


mlpack is an intuitive, fast, and flexible C++ machine learning library with
bindings to other languages.  It is meant to be a machine learning analog to
LAPACK, and aims to implement a wide array of machine learning methods and
functions as a ""swiss army knife"" for machine learning researchers.  In addition
to its powerful C++ interface, mlpack also provides command-line programs and
Python bindings.
0. Contents

Introduction
Citation details
Dependencies
Building mlpack from source
Running mlpack programs
Using mlpack from Python
Further documentation
Bug reporting

1. Introduction
The mlpack website can be found at https://www.mlpack.org and it contains
numerous tutorials and extensive documentation.  This README serves as a guide
for what mlpack is, how to install it, how to run it, and where to find more
documentation. The website should be consulted for further information:

mlpack homepage
mlpack documentation
Tutorials
Development Site (Github)
API documentation (Doxygen)

2. Citation details
If you use mlpack in your research or software, please cite mlpack using the
citation below (given in BibTeX format):
@article{mlpack2018,
    title     = {mlpack 3: a fast, flexible machine learning library},
    author    = {Curtin, Ryan R. and Edel, Marcus and Lozhnikov, Mikhail and
                 Mentekidis, Yannis and Ghaisas, Sumedh and Zhang,
                 Shangtong},
    journal   = {Journal of Open Source Software},
    volume    = {3},
    issue     = {26},
    pages     = {726},
    year      = {2018},
    doi       = {10.21105/joss.00726},
    url       = {https://doi.org/10.21105/joss.00726}
}

Citations are beneficial for the growth and improvement of mlpack.
3. Dependencies
mlpack has the following dependencies:
  Armadillo     >= 6.500.0
  Boost (program_options, math_c99, unit_test_framework, serialization,
         spirit)
  CMake         >= 3.3.2

All of those should be available in your distribution's package manager.  If
not, you will have to compile each of them by hand.  See the documentation for
each of those packages for more information.
If you would like to use or build the mlpack Python bindings, make sure that the
following Python packages are installed:
  setuptools
  cython >= 0.24
  numpy
  pandas >= 0.15.0

If you are compiling Armadillo by hand, ensure that LAPACK and BLAS are enabled.
4. Building mlpack from source
This section discusses how to build mlpack from source.  However, mlpack is in
the repositories of many Linux distributions and so it may be easier to use the
package manager for your system.  For example, on Ubuntu, you can install mlpack
with the following command:
$ sudo apt-get install libmlpack-dev

Note: Older Ubuntu versions may not have the most recent version of mlpack
available---for instance, at the time of this writing, Ubuntu 16.04 only has
mlpack 2.0.1 available.  Options include upgrading your Ubuntu version, finding
a PPA or other non-official sources, or installing with a manual build.
There are some other useful pages to consult in addition to this section:

Building mlpack From Source
Building mlpack From Source on Windows

mlpack uses CMake as a build system and allows several flexible build
configuration options. One can consult any of numerous CMake tutorials for
further documentation, but this tutorial should be enough to get mlpack built
and installed.
First, unpack the mlpack source and change into the unpacked directory.  Here we
use mlpack-x.y.z where x.y.z is the version.
$ tar -xzf mlpack-x.y.z.tar.gz
$ cd mlpack-x.y.z

Then, make a build directory.  The directory can have any name, not just
'build', but 'build' is sufficient.
$ mkdir build
$ cd build

The next step is to run CMake to configure the project.  Running CMake is the
equivalent to running ./configure with autotools. If you run CMake with no
options, it will configure the project to build with no debugging symbols and no
profiling information:
$ cmake ../

You can specify options to compile with debugging information and profiling
information:
$ cmake -D DEBUG=ON -D PROFILE=ON ../

Options are specified with the -D flag.  The allowed options include:
DEBUG=(ON/OFF): compile with debugging symbols
PROFILE=(ON/OFF): compile with profiling symbols
ARMA_EXTRA_DEBUG=(ON/OFF): compile with extra Armadillo debugging symbols
BOOST_ROOT=(/path/to/boost/): path to root of boost installation
ARMADILLO_INCLUDE_DIR=(/path/to/armadillo/include/): path to Armadillo headers
ARMADILLO_LIBRARY=(/path/to/armadillo/libarmadillo.so): Armadillo library
BUILD_CLI_EXECUTABLES=(ON/OFF): whether or not to build command-line programs
BUILD_PYTHON_BINDINGS=(ON/OFF): whether or not to build Python bindings
BUILD_TESTS=(ON/OFF): whether or not to build tests
BUILD_SHARED_LIBS=(ON/OFF): compile shared libraries as opposed to
   static libraries
DOWNLOAD_ENSMALLEN=(ON/OFF): If ensmallen is not found, download it
ENSMALLEN_INCLUDE_DIR=(/path/to/ensmallen/include): path to include directory
   for ensmallen
USE_OPENMP=(ON/OFF): whether or not to use OpenMP if available

Other tools can also be used to configure CMake, but those are not documented
here.  See this section of the build guide
for more details, including a full list of options, and their default values.
By default, command-line programs will be built, and if the Python dependencies
(Cython, setuptools, numpy, pandas) are available, then Python bindings will
also be built.  OpenMP will be used for parallelization when possible by
default.
Once CMake is configured, building the library is as simple as typing 'make'.
This will build all library components as well as 'mlpack_test'.
$ make

You can specify individual components which you want to build, if you do not
want to build everything in the library:
$ make mlpack_pca mlpack_knn mlpack_kfn

If the build fails and you cannot figure out why, register an account on Github
and submit an issue; the mlpack developers will quickly help you figure it out:
mlpack on Github
Alternately, mlpack help can be found in IRC at #mlpack on irc.freenode.net.
If you wish to install mlpack to /usr/local/include/mlpack/ and /usr/local/lib/
and /usr/local/bin/, once it has built, make sure you have root privileges (or
write permissions to those three directories), and simply type
$ make install

You can now run the executables by name; you can link against mlpack with
-lmlpack
and the mlpack headers are found in
/usr/local/include/mlpack/
and if Python bindings were built, they will be accessible with the mlpack
package in Python.
If running the programs (i.e. $ mlpack_knn -h) gives an error of the form
error while loading shared libraries: libmlpack.so.2: cannot open shared object file: No such file or directory

then be sure that the runtime linker is searching the directory where
libmlpack.so was installed (probably /usr/local/lib/ unless you set it
manually).  One way to do this, on Linux, is to ensure that the
LD_LIBRARY_PATH environment variable has the directory that contains
libmlpack.so.  Using bash, this can be set easily:
export LD_LIBRARY_PATH=""/usr/local/lib/:$LD_LIBRARY_PATH""

(or whatever directory libmlpack.so is installed in.)
5. Running mlpack programs
After building mlpack, the executables will reside in build/bin/.  You can call
them from there, or you can install the library and (depending on system
settings) they should be added to your PATH and you can call them directly.  The
documentation below assumes the executables are in your PATH.
Consider the 'mlpack_knn' program, which finds the k nearest neighbors in a
reference dataset of all the points in a query set.  That is, we have a query
and a reference dataset. For each point in the query dataset, we wish to know
the k points in the reference dataset which are closest to the given query
point.
Alternately, if the query and reference datasets are the same, the problem can
be stated more simply: for each point in the dataset, we wish to know the k
nearest points to that point.
Each mlpack program has extensive help documentation which details what the
method does, what each of the parameters is, and how to use them:
$ mlpack_knn --help
Running mlpack_knn on one dataset (that is, the query and reference
datasets are the same) and finding the 5 nearest neighbors is very simple:
$ mlpack_knn -r dataset.csv -n neighbors_out.csv -d distances_out.csv -k 5 -v
The -v (--verbose) flag is optional; it gives informational output.  It is not
unique to mlpack_knn but is available in all mlpack programs.  Verbose
output also gives timing output at the end of the program, which can be very
useful.
6. Using mlpack from Python
If mlpack is installed to the system, then the mlpack Python bindings should be
automatically in your PYTHONPATH, and importing mlpack functionality into Python
should be very simple:
>>> from mlpack import knn
Accessing help is easy:
>>> help(knn)
The API is similar to the command-line programs.  So, running knn()
(k-nearest-neighbor search) on the numpy matrix dataset and finding the 5
nearest neighbors is very simple:
>>> output = knn(reference=dataset, k=5, verbose=True)
This will store the output neighbors in output['neighbors'] and the output
distances in output['distances'].  Other mlpack bindings function similarly,
and the input/output parameters exactly match those of the command-line
programs.
7. Further documentation
The documentation given here is only a fraction of the available documentation
for mlpack.  If doxygen is installed, you can type make doc to build the
documentation locally.  Alternately, up-to-date documentation is available for
older versions of mlpack:

mlpack homepage
mlpack documentation
Tutorials
Development Site (Github)
API documentation

8. Bug reporting
(see also mlpack help)
If you find a bug in mlpack or have any problems, numerous routes are available
for help.
Github is used for bug tracking, and can be found at
https://github.com/mlpack/mlpack/.
It is easy to register an account and file a bug there, and the mlpack
development team will try to quickly resolve your issue.
In addition, mailing lists are available.  The mlpack discussion list is
available at
mlpack discussion list
and the git commit list is available at
commit list
Lastly, the IRC channel #mlpack on Freenode can be used to get help.
",2809
zrrrzzt/generate-pincode,JavaScript,"


generate-pincode
Node.js module for generating random pincodes.
Installation
$ npm install generate-pincode
Usage
var gpc = require('generate-pincode')
var pin = gpc(4)

console.log(pin)

// => 1234
Related

generate-pincode-cli CLI of this module

License
MIT
",2
mlpack/mlpack,C++,"

a fast, flexible machine learning library


Home |
  Documentation |
  Doxygen |
  Community |
  Help |
  IRC Chat









    Download:
    current stable version (3.1.0)


mlpack is an intuitive, fast, and flexible C++ machine learning library with
bindings to other languages.  It is meant to be a machine learning analog to
LAPACK, and aims to implement a wide array of machine learning methods and
functions as a ""swiss army knife"" for machine learning researchers.  In addition
to its powerful C++ interface, mlpack also provides command-line programs and
Python bindings.
0. Contents

Introduction
Citation details
Dependencies
Building mlpack from source
Running mlpack programs
Using mlpack from Python
Further documentation
Bug reporting

1. Introduction
The mlpack website can be found at https://www.mlpack.org and it contains
numerous tutorials and extensive documentation.  This README serves as a guide
for what mlpack is, how to install it, how to run it, and where to find more
documentation. The website should be consulted for further information:

mlpack homepage
mlpack documentation
Tutorials
Development Site (Github)
API documentation (Doxygen)

2. Citation details
If you use mlpack in your research or software, please cite mlpack using the
citation below (given in BibTeX format):
@article{mlpack2018,
    title     = {mlpack 3: a fast, flexible machine learning library},
    author    = {Curtin, Ryan R. and Edel, Marcus and Lozhnikov, Mikhail and
                 Mentekidis, Yannis and Ghaisas, Sumedh and Zhang,
                 Shangtong},
    journal   = {Journal of Open Source Software},
    volume    = {3},
    issue     = {26},
    pages     = {726},
    year      = {2018},
    doi       = {10.21105/joss.00726},
    url       = {https://doi.org/10.21105/joss.00726}
}

Citations are beneficial for the growth and improvement of mlpack.
3. Dependencies
mlpack has the following dependencies:
  Armadillo     >= 6.500.0
  Boost (program_options, math_c99, unit_test_framework, serialization,
         spirit)
  CMake         >= 3.3.2

All of those should be available in your distribution's package manager.  If
not, you will have to compile each of them by hand.  See the documentation for
each of those packages for more information.
If you would like to use or build the mlpack Python bindings, make sure that the
following Python packages are installed:
  setuptools
  cython >= 0.24
  numpy
  pandas >= 0.15.0

If you are compiling Armadillo by hand, ensure that LAPACK and BLAS are enabled.
4. Building mlpack from source
This section discusses how to build mlpack from source.  However, mlpack is in
the repositories of many Linux distributions and so it may be easier to use the
package manager for your system.  For example, on Ubuntu, you can install mlpack
with the following command:
$ sudo apt-get install libmlpack-dev

Note: Older Ubuntu versions may not have the most recent version of mlpack
available---for instance, at the time of this writing, Ubuntu 16.04 only has
mlpack 2.0.1 available.  Options include upgrading your Ubuntu version, finding
a PPA or other non-official sources, or installing with a manual build.
There are some other useful pages to consult in addition to this section:

Building mlpack From Source
Building mlpack From Source on Windows

mlpack uses CMake as a build system and allows several flexible build
configuration options. One can consult any of numerous CMake tutorials for
further documentation, but this tutorial should be enough to get mlpack built
and installed.
First, unpack the mlpack source and change into the unpacked directory.  Here we
use mlpack-x.y.z where x.y.z is the version.
$ tar -xzf mlpack-x.y.z.tar.gz
$ cd mlpack-x.y.z

Then, make a build directory.  The directory can have any name, not just
'build', but 'build' is sufficient.
$ mkdir build
$ cd build

The next step is to run CMake to configure the project.  Running CMake is the
equivalent to running ./configure with autotools. If you run CMake with no
options, it will configure the project to build with no debugging symbols and no
profiling information:
$ cmake ../

You can specify options to compile with debugging information and profiling
information:
$ cmake -D DEBUG=ON -D PROFILE=ON ../

Options are specified with the -D flag.  The allowed options include:
DEBUG=(ON/OFF): compile with debugging symbols
PROFILE=(ON/OFF): compile with profiling symbols
ARMA_EXTRA_DEBUG=(ON/OFF): compile with extra Armadillo debugging symbols
BOOST_ROOT=(/path/to/boost/): path to root of boost installation
ARMADILLO_INCLUDE_DIR=(/path/to/armadillo/include/): path to Armadillo headers
ARMADILLO_LIBRARY=(/path/to/armadillo/libarmadillo.so): Armadillo library
BUILD_CLI_EXECUTABLES=(ON/OFF): whether or not to build command-line programs
BUILD_PYTHON_BINDINGS=(ON/OFF): whether or not to build Python bindings
BUILD_TESTS=(ON/OFF): whether or not to build tests
BUILD_SHARED_LIBS=(ON/OFF): compile shared libraries as opposed to
   static libraries
DOWNLOAD_ENSMALLEN=(ON/OFF): If ensmallen is not found, download it
ENSMALLEN_INCLUDE_DIR=(/path/to/ensmallen/include): path to include directory
   for ensmallen
USE_OPENMP=(ON/OFF): whether or not to use OpenMP if available

Other tools can also be used to configure CMake, but those are not documented
here.  See this section of the build guide
for more details, including a full list of options, and their default values.
By default, command-line programs will be built, and if the Python dependencies
(Cython, setuptools, numpy, pandas) are available, then Python bindings will
also be built.  OpenMP will be used for parallelization when possible by
default.
Once CMake is configured, building the library is as simple as typing 'make'.
This will build all library components as well as 'mlpack_test'.
$ make

You can specify individual components which you want to build, if you do not
want to build everything in the library:
$ make mlpack_pca mlpack_knn mlpack_kfn

If the build fails and you cannot figure out why, register an account on Github
and submit an issue; the mlpack developers will quickly help you figure it out:
mlpack on Github
Alternately, mlpack help can be found in IRC at #mlpack on irc.freenode.net.
If you wish to install mlpack to /usr/local/include/mlpack/ and /usr/local/lib/
and /usr/local/bin/, once it has built, make sure you have root privileges (or
write permissions to those three directories), and simply type
$ make install

You can now run the executables by name; you can link against mlpack with
-lmlpack
and the mlpack headers are found in
/usr/local/include/mlpack/
and if Python bindings were built, they will be accessible with the mlpack
package in Python.
If running the programs (i.e. $ mlpack_knn -h) gives an error of the form
error while loading shared libraries: libmlpack.so.2: cannot open shared object file: No such file or directory

then be sure that the runtime linker is searching the directory where
libmlpack.so was installed (probably /usr/local/lib/ unless you set it
manually).  One way to do this, on Linux, is to ensure that the
LD_LIBRARY_PATH environment variable has the directory that contains
libmlpack.so.  Using bash, this can be set easily:
export LD_LIBRARY_PATH=""/usr/local/lib/:$LD_LIBRARY_PATH""

(or whatever directory libmlpack.so is installed in.)
5. Running mlpack programs
After building mlpack, the executables will reside in build/bin/.  You can call
them from there, or you can install the library and (depending on system
settings) they should be added to your PATH and you can call them directly.  The
documentation below assumes the executables are in your PATH.
Consider the 'mlpack_knn' program, which finds the k nearest neighbors in a
reference dataset of all the points in a query set.  That is, we have a query
and a reference dataset. For each point in the query dataset, we wish to know
the k points in the reference dataset which are closest to the given query
point.
Alternately, if the query and reference datasets are the same, the problem can
be stated more simply: for each point in the dataset, we wish to know the k
nearest points to that point.
Each mlpack program has extensive help documentation which details what the
method does, what each of the parameters is, and how to use them:
$ mlpack_knn --help
Running mlpack_knn on one dataset (that is, the query and reference
datasets are the same) and finding the 5 nearest neighbors is very simple:
$ mlpack_knn -r dataset.csv -n neighbors_out.csv -d distances_out.csv -k 5 -v
The -v (--verbose) flag is optional; it gives informational output.  It is not
unique to mlpack_knn but is available in all mlpack programs.  Verbose
output also gives timing output at the end of the program, which can be very
useful.
6. Using mlpack from Python
If mlpack is installed to the system, then the mlpack Python bindings should be
automatically in your PYTHONPATH, and importing mlpack functionality into Python
should be very simple:
>>> from mlpack import knn
Accessing help is easy:
>>> help(knn)
The API is similar to the command-line programs.  So, running knn()
(k-nearest-neighbor search) on the numpy matrix dataset and finding the 5
nearest neighbors is very simple:
>>> output = knn(reference=dataset, k=5, verbose=True)
This will store the output neighbors in output['neighbors'] and the output
distances in output['distances'].  Other mlpack bindings function similarly,
and the input/output parameters exactly match those of the command-line
programs.
7. Further documentation
The documentation given here is only a fraction of the available documentation
for mlpack.  If doxygen is installed, you can type make doc to build the
documentation locally.  Alternately, up-to-date documentation is available for
older versions of mlpack:

mlpack homepage
mlpack documentation
Tutorials
Development Site (Github)
API documentation

8. Bug reporting
(see also mlpack help)
If you find a bug in mlpack or have any problems, numerous routes are available
for help.
Github is used for bug tracking, and can be found at
https://github.com/mlpack/mlpack/.
It is easy to register an account and file a bug there, and the mlpack
development team will try to quickly resolve your issue.
In addition, mailing lists are available.  The mlpack discussion list is
available at
mlpack discussion list
and the git commit list is available at
commit list
Lastly, the IRC channel #mlpack on Freenode can be used to get help.
",2809
afollestad/recyclical,Kotlin,"Recyclical
recyclical: an easy-to-use, extensible Kotlin DSL for setting up and manipulating RecyclerViews.





Table of Contents
Core

Gradle Dependency
The Basics
More Options
Child View Clicks
Multiple Item Types
DataSource

Construction
Manipulation
Diffing


SelectableDataSource

Construction
Manipulation
Use in Binding


Stable IDs

Swipe

Gradle Dependency
The Basics
Long Swipes
Customization


Core
  
Gradle Dependency
Add this to your module's build.gradle file:
dependencies {

  implementation 'com.afollestad:recyclical:0.9.1'
}

The Basics
First, declare an Item class:
data class Person(
  var name: String,
  var arg: Int
)
Second, a layout and a View Holder:
<LinearLayout ...>

  <TextView 
     android:id=""@+id/text_name""
     ... />    
     
  <TextView 
     android:id=""@+id/text_age""
     ... />
     
</LinearLayout>
class PersonViewHolder(itemView: View) : ViewHolder(itemView) {
  val name: TextView = itemView.findViewById(R.id.text_name)
  val age: TextView = itemView.findViewById(R.id.text_age)
}
Finally, you can begin using the DSL API:
class MainActivity : AppCompatActivity() {

  override fun onCreate(savedInstanceState: Bundle?) {
      super.onCreate(savedInstanceState)
      
      // dataSourceTypedOf(...) here creates a DataSource<Person>
      val dataSource = dataSourceTypedOf(
          Person(""Aidan"", 24),
          Person(""Nina"", 24)
      )
      
      // setup{} is an extension method on RecyclerView
      recyclerView.setup {
          withDataSource(dataSource)
          withItem<Person, PersonViewHolder>(R.layout.person_item_layout) {
            onBind(::PersonViewHolder) { index, item ->
              // PersonViewHolder is `this` here
              name.text = item.name
              age.text = ""${item.age}""
            }
            onClick { index ->
              // item is a `val` in `this` here
              toast(""Clicked $index: ${item.name}"")
            }
            onLongClick { index ->
              // item is a `val` in `this` here 
              toast(""Long clicked $index: ${item.name}"")
            }
         }
      }
  }
}

More Options
There are other things you can give to the setup extension:
recyclerView.setup {
  // Custom layout manager, rather than the default which is a vertical LinearLayoutManager
  withLayoutManager(GridLayoutManager(context, 2))
  // Assigns a view that is made visible when the data source has content, else is hidden (gone)
  withEmptyView(view)
  // Global click listener for any item type. Individual item click listeners are called first.
  withClickListener { index, item -> }
  // Global long click listener for any item type. Individual item long click listeners are called first.
  withLongClickListener { index, item -> }
}

Child View Clicks
There are many cases in which you'd want to get callbacks for a child view in your list items
getting clicked, such as the sender icon in a list of emails.
class EmailViewHolder(itemView: View) : ViewHolder(itemView) {
  val icon = itemView.findViewById<ImageView>(R.id.icon)
}

recyclerView.setup {
  withItem<EmailItem, EmailViewHolder>(R.layout.email_item_layout) {
    ...
    onChildViewClick(EmailViewHolder::icon) { index, view ->
      // `this` includes `item` along with selection-related methods discussed below in SelectableDataSource
      // `view` argument here is automatically an `ImageView`
    }
  }
}

Multiple Item Types
You can mix different types of items - but you need to specify view holders and layouts for them too:
// dataSourceOf(...) without ""typed"" creates a DataSource<Any>
val dataSource = dataSourceOf(
  Car(2012, ""Volkswagen GTI""),
  Motorcycle(2018, ""Triumph"", ""Thruxton R""),
  Person(""Aidan"", 24)
)

recyclerView.setup {
  withDataSource(dataSource)
  withItem<Person, PersonViewHolder>(R.layout.person_item_layout) {
     onBind(::PersonViewHolder) { index, item ->
        name.text = item.name
        age.text = ""${item.age}""
     }
  }
  withItem<Motorcycle, MotorcycleViewHolder>(R.layout.motorcycle_item_layout) {
     onBind(::MotorcycleViewHolder) { index, item ->
        year.text = ""${item.year}""
        make.text = item.make
        model.text = item.model
     }
  }
  withItem<Car, CarViewHolder>(R.layout.car_item_layout) {
     onBind(::CarViewHolder) { index, item ->
        year.text = ""${item.year}""
        name.text = item.name
     } 
  }
}

DataSource
DataSource is an interface which provides data and allows manipulation of the data, to display in a RecyclerView.
Being an interface means you make your own implementations of it, you can mock it in tests, you could even provide it
via Dagger to a presenter and manipulate the RecyclerView outside of your UI layer.
Construction
The included implementation of data source operates on a List of objects (of any type).
// Empty by default, but can still add, insert, etc.
val dataSource: DataSource<Any> = emptyDataSource()
val dataSourceTyped: DataSource<Person> = emptyDataSourceTyped<Person>() 
 

// Initial data set of items from a vararg list
val dataSource: DataSource<Any> = dataSourceOf(item1, item2)
val dataSourceTyped: DataSource<Person> = dataSourceTypedOf(item1, item2)

// Initial data set of items from an existing list
// Could also use dataSourceTypedOf(...)
val items = listOf(item1, item2)
val dataSource: DataSource<Any> = dataSourceOf(items)
val dataSourceTyped: DataSource<Person> = dataSourceTypedOf(items)

Manipulation
val dataSource: DataSource<ItemType> = // ...

// getters
val item: ItemType = dataSource[5]
val contains: Boolean = dataSource.contains(item)
val size: Int = dataSource.size()
val isEmpty: Boolean = dataSource.isEmpty()
val isNotEmpty: Boolean = dataSource.isNotEmpty()
val firstIndex: Int = dataSource.indexOfFirst { }
val lastIndex: Int = dataSource.indexOfLast { }

// mutation
val person = Person(""Aidan"", 24)
dataSource.add(person)
dataSource.set(listOf(person))
dataSource.insert(1, person)
dataSource.removeAt(1)
dataSource.remove(person)
dataSource.swap(1, 4)
dataSource.move(1, 4)
dataSource.clear()

// iteration
for (item in dataSource) { }
dataSource.forEach { }  // emits all items
dataSource.forEachOf<Person> { }  // only emits items that are a Person

// operators
val item: Any = dataSource[5]  // get(5)
val contains: Boolean = item in dataSource  // contains(item)
dataSource += person  // add(person)
dataSource -= person  // remove(person)
Diffing
When performing a set on the data set, you can opt to use diff utils:
dataSource.set(
  newItems = newItems,
  areTheSame = ::areItemsTheSame,
  areContentsTheSame = ::areItemContentsTheSame
)

// Return true if items represent the same entity, e.g. by ID or name
private fun areItemsTheSame(left: Any, right: Any): Boolean {
  return when (left) {
    is Person -> {
      right is Person && right.name == left.name
    }
    else -> false
  }
}

// Return true if all contents in the items are equal
private fun areItemContentsTheSame(left: Any, right: Any): Boolean {
  return when (left) {
    is Person -> {
      right is Person &&
        right.name == left.name &&
        right.age == left.age
    }
    else -> false
  }
}
This will automatically coordinate notifying of adds, moves, and insertions so that
update of the data set is pretty and animated by the RecyclerView.

SelectableDataSource
A SelectableDataSource is built on top of a regular [DataSource]. It provides additional APIs
to manage the selection state of items in your list.
Construction
Construction methods for SelectableDataSource are the same as the DataSource ones, they just
include selectable in their names.
// Empty by default, but can still add, insert, etc.
// Could also use emptySelectableDataSourceTyped()
val dataSource: SelectableDataSource<Any> = emptySelectableDataSource()
val dataSourceTyped: SelectableDataSource<Person> = emptySelectableDataSourceTyped()

// Initial data set of items from a vararg list
// Could also use selectableDataSourceTypedOf(...)
val dataSource: SelectableDataSource<Any> = selectableDataSourceOf(item1, item2)
val dataSourceTyped: SelectableDataSource<Person> = selectableDataSourceTypedOf(item1, item2)

// Initial data set of items from an existing list
// Could also use selectableDataSourceTypedOf(...)
val items = listOf(item1, item2)
val dataSource: SelectableDataSource<Any> = selectableDataSourceOf(items)
val dataSourceTyped: SelectableDataSource<Person> = selectableDataSourceTypedOf(items)
Manipulation
There are some additional methods added on top of the DataSource methods:
val dataSource: SelectableDataSource<Any> = // ...

// Index operations
dataSource.selectAt(1)
dataSource.deselectAt(1)
dataSource.toggleSelectionAt(1)
val selected: Boolean = dataSource.isSelectedAt(1)

// Item operations, uses index operations under the hood
val item: Any = // ...
dataSource.select(item)
dataSource.deselect(item)
dataSource.toggleSelection(item)
val selected: Boolean = dataSource.isSelected(item)

// Mass operations
dataSource.selectAll()
dataSource.deselectAll()

// Misc operations
val count: Int = dataSource.getSelectionCount()
val hasSelection: Boolean = dataSource.hasSelection()

// Set a callback invoked when something is selected or deselected
dataSource.onSelectionChange { dataSource -> }
Use in Binding
During binding of your items, you can access selection states even if you don't have a direct
reference to your DataSource.
In onBind blocks, this is done with extensions in ViewHolder which provide functions to check
selection state and select/deselect the current item that is being bound.
In onClick and onLongClick blocks, this is done using a type that is passed as this which provides the same set
of functions.
recyclerView.setup {
    withEmptyView(emptyView)
    withDataSource(dataSource)
    withItem<MyListItem, MyViewHolder>(R.layout.my_list_item) {
      onBind(::MyViewHolder) { index, item ->
          // Selection-related methods that can be used here:
          isSelected()
          select()
          deselect()
          toggleSelection()
          hasSelection()
      }
      onClick { index ->
          // Selection-related methods that can be used here:
          isSelected()
          select()
          deselect()
          toggleSelection()
          hasSelection()
      }
      onChildViewClick(MyViewHolder::someView) { index, view ->
          // The same methods used in onClick can be used here as well
      }
      onLongClick { index ->
          // The same methods used in onClick can be used here as well
      }
    }
}  

Stable IDs
Stable IDs are an optimization hint for RecyclerView. When using stable IDs, you're telling
the view that each ViewHolder ID is unique and will not change. In Recyclical, to can use stable IDs
by having all of your items provide a unique ID for themselves.
data class AnItemWithAnId(
  val id: Int,
  val name: String
)

recyclerView.setup {
  withDataSource(dataSource)
  withItem<AnItemWithAnId, MyViewHolder>(R.layout.my_item_layout) {
     onBind(::MyViewHolder) { index, item -> ... }
     // The key is this, which says the `id` field of your item represents a unique ID.
     hasStableIds { it.id }
  }
}
If you have more than one item that your RecyclerView can hold, all need to define hasStableIds.

Swipe
The swipe module provides extensions to setup swipe actions, like swipe to delete.
  
Gradle Dependency
Add this to your module's build.gradle file:
dependencies {

  implementation 'com.afollestad:recyclical-swipe:0.9.1'
}
The Basics

This example below sets up swipe to delete, so that it works if you swipe either right or left.
A delete icon and delete text would be shown over a red gutter. The callback returning true means
that the item should be removed from the DataSource when the action triggers.
list.setup {
  ...
  withSwipeAction(LEFT, RIGHT) {
    icon(R.drawable.ic_delete)
    text(R.string.delete)
    color(R.color.md_red)
    callback { index, item -> true }
  }
}
You can target specific item types with withSwipeActionOn, too:
withSwipeActionOn<MyItem>(LEFT, RIGHT) {
  icon(R.drawable.ic_delete)
  text(R.string.delete)
  color(R.color.md_red)
  callback { index, item -> true }
}
With withSwipeActionOn, item in the callback is a MyItem instead of Any as well.
Long Swipes

You can set long swipe actions. These actions require you to swipe an item further before releasing
to trigger. Take this block:
list.setup {
  ...
  withSwipeAction(LEFT) {
    icon(R.drawable.ic_archive)
    text(R.string.archive)
    color(R.color.md_green)
    callback { index, item -> true }
  }
  withSwipeAction(LEFT_LONG) {
    icon(R.drawable.ic_delete)
    text(R.string.delete)
    color(R.color.md_red)
    callback { index, item -> true }
  }
}
If you swipe just a little bit and release, the item is archived. If you swipe further, the item is
deleted. You can use RIGHT_LONG as well.
Customization
As you saw above, you can use icons, text, and background colors easily. There are more details
you can customize about your swipe actions, mainly around text:
list.setup {
  ...
  withSwipeAction(LEFT, RIGHT) {
    text(
      res = R.string.delete,
      color = R.color.black,
      size = R.dimen.small_text_size,
      typefaceRes = R.font.roboto_mono
    )
  }
}
",315
yizibi/LX_Extend,Objective-C,"LX_Extend
开发中常用的工具分类


综述如下

目录结构如下
.
├── Category
│   ├── Map格式化
│   ├── UIBarButtonItem
│   ├── UIColor
│   ├── UIGesture
│   ├── UIImage
│   ├── UIViewFrame
│   ├── 日期判断
│   └── 计算文件总大小(NSString)
│   └── SocketManger
├── Macros
│   └── Single.h
├── README.md
└── Utils
│   └── DateManger
│   └── RSA+3DES
│   └── rashLib
│   └── 系统相册访问
│   └── ConsoleCheseInput
│   └── SocketManger



190515 增加 MQTTManger

项目需要依赖以下两个库:

pod 'MQTTClient'
pod 'MQTTClient/Websocket'

封装了如下功能

开启连接;
订阅主题;
取消订阅;
关闭连接;
失败重连;


新增 0920


字典 内部 key -> value 统一处理, 如果为nil,转换 @"""",或者@""未知信息"",可以自定义

日期管理

将时间戳转化为自定义的格式

举个🌰:  时间戳:2345234523

Usage

#define aDF @""yyyy-MM-dd HH:mm:ss""
#define bDF @""yyyyMMddHHmmss""
#define cDF @""yyyy-MM-dd""
#define dDF @""yyyyMMdd""
#define eDF @""MM-dd HH:mm""
#define fDF @""HH:mm""
#define gDF @""yyyy/MM/dd HH:mm""



(NSString *)stringWithTimeInterval:(NSTimeInterval)interval dateFormat:(NSString *)dateFormat;

UIImage

✅根据颜色生成图片;
✅图片水印;
✅圆形图片裁剪
✅虚线绘制


新增 0816

数组越界,字符串截取越界,崩溃AOP插件

使用方法

直接将文件[防止数组,字符串崩溃crashLib]拖进去,简单,实用;

新增 0810

RSA加密文件


使用方法:


RSA加密


NSString *originalString = @""文档虐我千百遍,我待文档如初恋"";

//使用.der和.p12中的公钥私钥加密解密
NSString *public_key_path = [[NSBundle mainBundle] pathForResource:@""public_key.der"" ofType:nil];
NSString *private_key_path = [[NSBundle mainBundle] pathForResource:@""private_key.p12"" ofType:nil];

NSString *encryptStr = [LXRSAEncryptor encryptString:originalString publicKeyWithContentsOfFile:public_key_path];
NSLog(@""加密前:%@"", originalString);
NSLog(@""加密后:%@"", encryptStr);
NSLog(@""解密后:%@"", [LXRSAEncryptor decryptString:encryptStr privateKeyWithContentsOfFile:private_key_path password:nil]);



打印结果

2017-08-10 08:57:10.553 RAS加密解密[70708:4286249] 加密前:文档虐我千百遍,我待文档如初恋
2017-08-10 08:57:10.553 RAS加密解密[70708:4286249] 加密后:V+RGKgYa05nQUabdX9DtFZvECgzXSIsHGrUNPuxNrc8N+aFliqaqxbLugBDrBhMNyiTzoeFO39dgvnQJFlpcWGXQNaKlMmP8z/LJ/MUUtZGT/686ks/Vl5AonA9nXAmGaZeMniYRMlMWZB1EnxM9fMUbz+wByrjAT89ok0ydFcU=
2017-08-10 08:57:15.987 RAS加密解密[70708:4286249] 解密后:文档虐我千百遍,我待文档如初恋



3DES加密

需要注意:
导入头文件;
不支持ARC,需要配置编译参数 -fno-objc-arc

NSString *scrietString = [LXRSAEncryptor encode3Des:@""哈哈"" key1:@""12345678"" key2:@""12345678"" key3:@""12345678""];
DebugLog(@""加密后:%@"",scrietString);
DebugLog(@""解密后:%@"",[LXRSAEncryptor decode3Des:scrietString key1:@""12345678"" key2:@""12345678"" key3:@""12345678""]);

加密后:OTPrcX9w4Q8/ZyByaIDvLv65WbfUZC/L
解密后:哈哈


单例的使用
//备注:name为当前的类名
直接导入'Single.h'头文件,在想设置单例的时候,.h文件和.m文件分别写入:interfaceSingle(name)和implementationSingle(name),使用单例的时候,方法:[当前类名 share..],就可以了,非常简单;

##UIImage和UIImageView的扩展方法
包括裁剪圆形图片,可拉伸的图片,根据颜色生成图片,保存本地相册,压缩图片到指定大小,图片的模糊处理等
UIColor的分类
主要是十六进制颜色转换,判断色值等
##[NSString+LXExtention.h]沙盒路径的获取封装及计算缓存文件大小
直接拖入分类,方法说明:都为对象方法,给定一个NSString,生成对应的路径
cachesDir:缓存路径
docDir:文档路径
temDir:临时路径
fileSize:计算缓存文件大小


[NSDate和NSCalendar分类]
判断日期,今天,明天等

UIView的分类
说明:通过点语法,便于直接访问UIView的frame
LX_PhotoTool系统相册相关

说明:开发中,有时候会用到,访问系统相册又或者调用相机,用户上传照片或者更改用户头像;还有就是保存网络上的图片到本地中,都较常用,因此整理了相关的方法;

温馨提示:LX_PhotoTool文件中只是整理了相关的方法,只是为了方便开发,而不是分类或者可以直接导入项目中,<** 不能直接导入项目 **>
iOS中,关于系统相册的有两个,一个是UIImagePickerController,用于访问相册,相机,另一个是系统库
UIImagePickerController的使用注意:
注意:此时需要遵守两个代理:
* UIImagePickerControllerDelegate
* UINavigationControllerDelegate

Photo库的使用
注意:导入相关头文件:
* #import <AVFoundation/AVFoundation.h>
* #import <Photos/Photos.h>

#UIBarButtonItem分类和UIGesture分类

说明:系统中的某些类,比如,UIBarButtonItem,初始化的时候,利用的是方法映射,根据方法名--->方法实现,这样做不便于代码维护,以block的形式,是的代码的可读性提高不少;

温馨提示: 导入相关头文件


UIBarButtonItem

runtime 添加actionBlock 属性,捕捉用户点击item的事件




@property (nonatomic, copy) void (^actionBlock)(id);





UIGesture

runtime 以block的方式,添加初始换方法,支持添加手势,移除所有手势;



- (instancetype)initWithActionBlock:(void (^)(id sender))block;

- (void)addActionBlock:(void (^)(id sender))block;

- (void)removeAllActionBlocks;


",4
hexoul/go-cryptoinfo-gather,Go,"go-cryptoinfo-gather




Crypto info gather

Build
dep ensure
go build
Usage
All options are not mandatory unless you use related APIs.
go run main.go \
-gitName=[GIT_NAME] \
-gitEmail=[GIT_EMAIL] \
-gitID=[GIT_ID] \
-gitPW=[GIT_PW] \
-logpath=[LOG_PATH] \
-targetSymbol=[TOKEN_SYMBOL] \
-targetAddr=[TOKEN_CONTRACT_ADDR] \
-targetQuotes=USD,BTC,ETH \
-targetSlugs=binance,okex \
-cmcApikey=[CMC_API_KEY] \
-coinsuper:accesskey=[COINSUPER_ACCESS_KEY] \
-coinsuper:secretkey=[COINSUPER_SECRET_KEY] \
-kucoin:accesskey=[KUCOIN_ACCESS_KEY] \
-kucoin:secretkey=[KUCOIN_SECRET_KEY] \
-abcc:accesskey=[ABCC_ACCESS_KEY] \
-abcc:secretkey=[ABCC_SECRET_KEY] \
...
",2
englercj/tsd-jsdoc,TypeScript,"tsd-jsdoc
This library's goal is to be able to take as input a JSDoc annotated source JavaScript
file (or many files) and output a single TypeScript Declaration File (.d.ts).
It is distributed as a JSDoc3 template. Running JSDoc with this as the template should
result in a TypeScript Definition File.
Installation
You can install this module from npm:
$> npm install tsd-jsdoc

Usage
To use this module, simply specify it as the template for your normal JSDoc generation.
For example, from the command-line you can do:
$> jsdoc -t node_modules/tsd-jsdoc/dist -r .

Or add this to your JSON configuration:
{
    ""opts"": {
        ""template"": ""./node_modules/tsd-jsdoc/dist""
    }
}
Validation
This library provides very little validation beyond what JSDoc provides. Meaning if you
have invalid JSDoc comments, this will likely output an invalid TypeScript Definition File.
Additionally there are things that JSDoc allows, that TypeScript does not. This library
tries to make these differences transparent, and translate from one to the other when
necessary. It can't handle anything though, and you can generate invalid Typescript
even if your JSDoc is valid.
Unsupported Features
Default exports
JSDoc has a bug that prevents it from
correctly parsing export default class Name {}. The workaround is to use named exports
(export class Name {}) or utilize the
jsdoc-export-default-interop plugin.
Tags with no support
Tags that describe the code, but support is not implemented are:

@default - No TS equivalent
@deprecated - No TS equivalent (issue)
@event - No TS equivalent
@exports - Everything is exported
@external - Not sure what behavior would be expected
@fires - No TS equivalent
@listens - No TS equivalent
@override - No TS equivalent (issue)
@throws - No TS equivalent

Ignored tags
Tags that are just metadata and don't actually describe
the code are ignored. These are:

@author
@classdesc
@copyright
@description
@example
@file
@license
@requires
@see
@since
@summary
@todo
@tutorial
@version

All other JSDoc tags should work fine.
Supported ClosureCompiler Tags
ClosureCompiler has a couple tags beyond the built-in JSDoc tags that can improve your TypeScript output. Here is a complete
list of the tags from CC that are supported in this template:

@template - For generics

Extended support for TS features
JSDoc doesn't have a way to express all the features of typescript so we treat some syntax as special case to
create better Typescript.

Class<T> - If we encounter a type that is Class<T> we will treat it as typeof T. See jsdoc3/jsdoc#1349

",139
Lombiq/Orchard-Training-Demo-Module,C#,"Orchard Training Demo module Readme
Project Description
Demo Orchard module for training purposes. This module completes the training materials at https://orcharddojo.net/orchard-resources. Note that this module also has an Orchard Core version in the orchard-core branch of the repository.
For all our training materials and Orchard trainings please visit Orchard Dojo.
Documentation
Download the source and unzip it to the Modules folder in your Orchard solution. Make sure to place the module's files in a folder named OrchardHUN.TrainingDemo (i.e. under Modules you have OrchardHUN.TrainingDemo and inside it OrchardHUN.TrainingDemo.csproj and everything else).
Open StartHere.txt and read the instructions. You will be guided through the module.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/orchard-training-demo-module (Mercurial repository)
https://github.com/Lombiq/Orchard-Training-Demo-Module (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",20
gregspurrier/klam,Ruby,"Klam



Klam is a Ruby implementation of Kl, the small Lisp on top of which the
Shen programming language is implemented.







License


Klam is Copyright © 2014-2015 Greg Spurrier. It is distributed under the terms of the MIT License. See LICENSE.txt for the details.


",8
jetiben/jtbc,PHP,"JTBC
功能
网站内容管理系统(CMS)
特点
纯净开源，商用免费，极简主义
模式
语言/代码/程序 两两分离技术模式
协议
基于 MIT(开源软件许可协议)
网站
https://www.jtbc.cn/
",33
juju/juju-gui,JavaScript,"Juju GUI
The Juju GUI is a web-based GUI for Juju. Juju allows
you to deploy, configure, manage, maintain, and scale cloud applications quickly
and efficiently on public clouds, as well as on physical servers, OpenStack, and containers.
The Juju GUI is open source and the code is available on GitHub.
Accessing the GUI
The latest release of the Juju GUI is made available to all users of Juju automatically
and can be launched by running juju gui in your terminal. For those using
JAAS, you can use the GUI by logging into your account
or creating a new model.
Issues & Feature Requests
Issues and feature requests are tracked on GitHub Issues.
Upgrading the GUI
JAAS users will have their GUI automatically upgraded
whenever there is a new release. For those with their own controllers you can
simply run juju upgrade-gui.
Developing the GUI
Documentation outlining how to develop with the GUI can be found in the docs/hacking.md document
",171
Blockmodo/coin_registry,None,"





Introduction
Coin Registry is a collection of JSON formatted information files that is primarily used by website developers, exchange operators, terminal developers, and news publications to show accurate information about different coins.
The Payload
A COINREGISTRY JSON information file:
{
	""type"": ""COINREGISTRY"",
	""version"": 1,
	""name"": ""Ethereum"",
	""fromSymbol"": ""ETH"",
	""toSymbol"":""ALL"",
	""website"": ""https://www.ethereum.org/"",
	""images"": {
		""image64"": ""https://image_path/img.jpg"",
		""image128"": ""https://image_path/img.jpg"",
		""image256"": ""https://image_path/img.jpg"",
		""image512"": ""https://image_path/img.jpg"",
		""image1024"": ""https://image_path/img.jpg""
	},
	""network"": {
		""t_total_supply"": 17104062,
		""t_available_supply"": 17104062,
		""t_max_supply"": 21000000,
		""t_block_reward"": 12.5
	},
	""is_crypto"": true,
	""is_minable"": true,
	""proof_type"": ""PoW"",
	""algorithm"": ""Ethash"",
	""description"": {
		""en"": ""Ethereum is an open-source, public, blockchain-based distributed computing platform and operating system featuring smart contract functionality. It supports a modified version of Nakamoto consensus via transaction-based state transitions.""
	},
	""quote"": [
		""https://blockmodo.com/quotes/ETH""
	],
	""explorer"": [
		""https://etherscan.io/""
	],
	""chat"": [
		""https://telegram.me/joinchat/AyAwgj-vtnMdUxRvCgicuQ""
	],
	""social"": [
		""https://twitter.com/ethereum"",
		""https://www.facebook.com/ethereumproject/""
	],
	""community"": [
		""https://www.reddit.com/r/ethereum"",
		""https://www.reddit.com/r/ethtrader""
	],
	""source_code"": [
		""https://github.com/ethereum/go-ethereum"",
		""https://github.com/ethereum/cpp-ethereum""
	]
}
Description of Fields

root → type

The Blockmodo API always assigns type names to Payloads for easy parsing. In this paticular case, the type will always be ""COINREGISTRY"" for this payload.

root → version

The version of the payload. From time to time, Blockmodo, or its contributors, might change the schema of the JSON payload when adding or removing fields. In such a scenario, the version number will be incremented.

root → name

The name of the currency.

root → fromSymbol

Each Blockmodo payload must have a fromSymbol and toSymbol unless otherwise specififed. In this case, the fromSymbol will be the symbol of the currency being described.

root → fromSymbol

Like above, the toSymbol will describe the relation to some other currency. In this case, since we are describing a currency with no relation to another, the toSymbol will always be 'ALL'.

root → website

The website for the currency.

root → network

This block holds network related statistics.

root → images

A hash that holds images of the currency with different sizes. Hosting provided by Blockmodo.

root → network → t_total_supply

The number of coins that are currently available in some form.
NOTE: This is a termporal value and might change frequently. If you wish to update this field, please make sure to update it no more than once a day via a pull request.

root → network → t_available_supply

The number of coins that are available to trade. For example, if a currency has coins locked up in escrow the available supply will be a subset of the total supply.
NOTE: This is a termporal value and might change frequently. If you wish to update this field, please make sure to update it no more than 3 times a day via a pull request.

root → network → t_max_supply

The max number of coins that can be mined or generated.
NOTE: This is a termporal value and might change frequently. If you wish to update this field, please make sure to update it no more than 3 times a day via a pull request.

root → network → t_block_reward

The number of coins that are rewarded per mined block.
NOTE: This is a termporal value and might change frequently. If you wish to update this field, please make sure to update it no more than 3 times a day via a pull request.

root → is_crypto

If the fromSymbol being described is a cryptocurrency. Typically this value will be 'true'.

root → is_minable

Is the coin minable using a proof_type.

root → proof_type

The type of work required to verify blocks of transactions.

root → algorithm

The algorithm used to sign blocks.

root → description

Hash of description types. The key in this block will be the language code and the value will be a string.

root → quote

An array of URLs of where users can get quotes for the given fromSymbol.

root → explorer

An array of URLs of where users can get blockchain realted inforation.

root → chat

An array of URLs where users can find information about chat communities. For example, Telegram or Rocket chat links would be listed here.

root → social

An array of social media URLs. For example, Facebook pages or groups would be listed here.

root → social

An array of community URLs. For example, Sub-Reddi's would be listed here or forums.

root → source_code

An array of repo URLs. For example, GitHub repos would be listed here.
Contributions
We are always looking for help. If you have a relevant edit, please feel free to issue a pull request. Some things to keep in mind:


Please do not change the schema. If you wish to suggest a schema enhancement, please open an issue.


Temporal value changes should be done no more than three times a day. If you would like to update temporal values, please drop open an issue.


Contributors
Icon: cryptocurrency by mikicon
Looking for real-time streaming pricing data?
Blockmodo provides real-time streaming pricing data on over 1500+ coins straight from exchanges. In addition, the API also streams news, code checkins, and social posts. Feel free to check out Blockmodo API docs.
Community
Coin Registry is maintained by Blockmodo and we're on Telegram. Visit us on Telegram
",64
jhaber-zz/wem4themes,Jupyter Notebook,"
wem4themes
This repository contains code-in-progress for a generalized inductive method of generating domain-specific dictionaries through word embedding models (WEMs). Starting with a set of seed terms, this workflow has three steps: construct (query model and develop core dictionaries), refine (maximize dictionary coherence and distinctiveness), and validate (using unsupervised clustering and hand coding).
I am optimizing the approach by varying the core dictionary size, WEM algorithm (word2vec, etc.), WEM generation method (pre-trained vs. native), and dictionary application method (count-based vs. vector projection). I am also comparing results from two test cases: charter schools data and a large corpus of journal articles. An optimized, publicly available method would allow researchers in diverse domains an accessible, reproducible, and valid workflow for both dictionary creation and assessing the validity of outside, researcher-generated themes in a text corpus. This represents a significant improvement on the idiosyncratic, domain-restrictive approach to dictionaries used by most social scientists for decades.
",2
MicrosoftDocs/azure-docs.zh-tw,PowerShell,"


ms.openlocfilehash
ms.sourcegitcommit
ms.translationtype
ms.contentlocale
ms.lasthandoff
ms.locfileid




d4e9e26f2c6764cc179826266891d861514e23f6
3102f886aa962842303c8753fe8fa5324a52834a
HT
zh-TW
04/23/2019
60344084



Microsoft Azure 文件
歡迎使用 Microsoft Azure 的開放原始碼文件。 請檢閱這份讀我檔案，以了解您可以如何為 Microsoft Azure 文件增色。
開始使用
參與開放原始碼內容並不只提供更新，同時也能讓我們在第一時間知道發生了問題。 閱讀我們的參與指引以深入了解。
必要條件
您已經決定要參與，太棒了！ 若要參與文件內容，您需要一些工具。
參與文件內容需要 GitHub 帳戶。 如果您還沒有帳戶，請依照下列參與者指南的 GitHub 帳戶設定的指示進行。
下載
安裝下列工具：

Git
Visual Studio Code
適用於 Visual Studio Code 的文件製作套件擴充功能

Install
請遵循參與者指南的安裝內容製作工具中提供的指示進行。
授權
請參閱 LICENSE、LICENSE-CODE和 ThirdPartyNotices 以取得所有授權資訊。
管理辦法
此專案採用 Microsoft 開放原始碼管理辦法。
如需詳細資訊，請參閱管理辦法常見問題集，如有任何其他問題或意見請連絡 opencode@microsoft.com。
",22
jgayfer/apollo,Python,"Summary
Apollo is an all in one solution for managing everything event related right within Discord.

Contributing
Apollo is an open source project; pull requests are encouraged and welcome.
If you are considering contributing to the project, feel free to contact Asal on the Apollo Discord server.
There is also a project board on Trello which outlines current and future units of work.
Development Environment
The following outlines the steps for setting up a local installation of Apollo for development purposes.
Prerequisites

Python 3.5.2+
Pipenv
MySQL server

Installation
The Python environment can be easily setup with pipenv:
pipenv install
pipenv shell

Environment Variables
Apollo requires several environment variables in order to function. The quickest way to get
up and running will be create an .env file in the root directory and populate it with the
contants of .env.example
You will need to set the BOT_TOKEN environment variable to the token of your Discord bot.
The database is setup to connect to localhost with the the root user (no password).
If your database setup is different from this, you will need to uncomment and modify the commented out
database environment variables found in .env.example
Database
Assuming our database credentials are correctly configured in .env, the database be be initialized with:
python bin/setup_db.py

Running the bot
The app can then be run with:
python app.py

",6
zrrrzzt/is-valid-fodselsnummer,JavaScript,"


is-valid-fodselsnummer
Check if supplied input is valid fødselsnummer (Norwegian national identification number).
Identify if number is an F or D-number
Installation
From npm
$ npm i --save is-valid-fodselsnummer
From GitHub
$ git clone https://github.com/zrrrzzt/is-valid-fodselsnummer.git
cd into directory and install dependencies
$ npm run setup
Usage validation
const isValidFodselsnummer = require('is-valid-fodselsnummer')

isValidFodselsnummer('01010750160')
// => true

isValidFodselsnummer('12341234567')
// => false

Usage identification
const isValidFodselsnummer = require('is-valid-fodselsnummer')

isValidFodselsnummer('01010750160', true)
// => F

isValidFodselsnummer('41085801188', true)
// => D

isValidFodselsnummer('12341234567', true)
// => false

Related

is-valid-fodselsnummer-cli CLI version of this module

License
MIT
",2
jgayfer/apollo,Python,"Summary
Apollo is an all in one solution for managing everything event related right within Discord.

Contributing
Apollo is an open source project; pull requests are encouraged and welcome.
If you are considering contributing to the project, feel free to contact Asal on the Apollo Discord server.
There is also a project board on Trello which outlines current and future units of work.
Development Environment
The following outlines the steps for setting up a local installation of Apollo for development purposes.
Prerequisites

Python 3.5.2+
Pipenv
MySQL server

Installation
The Python environment can be easily setup with pipenv:
pipenv install
pipenv shell

Environment Variables
Apollo requires several environment variables in order to function. The quickest way to get
up and running will be create an .env file in the root directory and populate it with the
contants of .env.example
You will need to set the BOT_TOKEN environment variable to the token of your Discord bot.
The database is setup to connect to localhost with the the root user (no password).
If your database setup is different from this, you will need to uncomment and modify the commented out
database environment variables found in .env.example
Database
Assuming our database credentials are correctly configured in .env, the database be be initialized with:
python bin/setup_db.py

Running the bot
The app can then be run with:
python app.py

",6
zrrrzzt/is-valid-fodselsnummer,JavaScript,"


is-valid-fodselsnummer
Check if supplied input is valid fødselsnummer (Norwegian national identification number).
Identify if number is an F or D-number
Installation
From npm
$ npm i --save is-valid-fodselsnummer
From GitHub
$ git clone https://github.com/zrrrzzt/is-valid-fodselsnummer.git
cd into directory and install dependencies
$ npm run setup
Usage validation
const isValidFodselsnummer = require('is-valid-fodselsnummer')

isValidFodselsnummer('01010750160')
// => true

isValidFodselsnummer('12341234567')
// => false

Usage identification
const isValidFodselsnummer = require('is-valid-fodselsnummer')

isValidFodselsnummer('01010750160', true)
// => F

isValidFodselsnummer('41085801188', true)
// => D

isValidFodselsnummer('12341234567', true)
// => false

Related

is-valid-fodselsnummer-cli CLI version of this module

License
MIT
",2
notable/notable,TypeScript,"Notable (DOWNLOAD)



The markdown-based note-taking app that doesn't suck.
I couldn't find a note-taking app that ticked all the boxes I'm interested in: notes are written and rendered in GitHub-flavored Markdown, no WYSIWYG, no proprietary formats, I can run a search & replace across all notes, notes support attachments, the app isn't bloated, the app has a pretty interface, tags are indefinitely nestable and can import Evernote notes (because that's what I was using before).
So I built my own.
Features
/path/to/your/data_directory
├─┬ attachments
│ ├── foo.ext
│ ├── bar.ext
│ └── …
└─┬ notes
  ├── foo.md
  ├── bar.md
  └── …



No proprietary formats: Notable is just a pretty front-end for a folder structured as shown above. Notes are plain Markdown files, their metadata is stored as Markdown front matter. Attachments are also plain files, if you attach a picture.jpg to a note everything about it will be preserved, and it will remain accessible like any other file.


Proper editor: Notable doesn't use any WYSIWYG editor, you just write some Markdown and it gets rendered as GitHub-flavored Markdown. The built-in editor is Monaco Editor, the same one VS Code uses, this means you get things like multi-cursor by default. If you need more advanced editing features with a single shortcut you can open the current note in your default Markdown editor.


Indefinitely nestable tags: Pretty much all the other note-taking apps differentiate between notebooks, tags and templates. IMHO this unnecessarily complicates things. In Notable you can have root tags (foo), indefinitely nestable tags (foo/bar, foo/.../qux) and it still supports notebooks and templates, they are just special tags with a different icon (Notebooks/foo, Templates/foo/bar).


Upon first instantiation, some tutorial notes will be added to the app, check them out for more in-depth details about the app and how to use it. You can also find the raw version here.
Comparison

Part of this comparison is personal opinion: you may disagree on the UI front, things I consider bloat may be considered features by somebody else etc. but hopefully this comparison did a good job at illustrating the main differences.
Demo
Dark Theme

Indefinitely Nestable Tags

Editor

Multi-Note Editor

Split-Editor + Zen Mode + Quick Open

Contributing
There are multiple ways to contribute to this project, read about them here.
Related

enex-dump: Dump the content of Evernote's .enex files, preserving attachments, some metadata and optionally converting notes to Markdown.
Noty: Autosaving sticky note with support for multiple notes without needing multiple windows.
Markdown Todo: Manage todo lists inside markdown files with ease. Have the same todo-related shortcuts that Notable provides, but in Visual Studio Code.
Todo+: Manage todo lists with ease. Powerful, easy to use and customizable.

License
AGPLv3 © Fabio Spampinato
",10163
bulletphysics/bullet3,C++,"

Bullet Physics SDK
This is the official C++ source code repository of the Bullet Physics SDK: real-time collision detection and multi-physics simulation for VR, games, visual effects, robotics, machine learning etc.
New in Bullet 2.85: pybullet Python bindings, improved support for robotics and VR. Use pip install pybullet and see PyBullet Quickstart Guide.
The Bullet 2 API will stay default and up-to-date while slowly moving to a new API.
The steps towards a new API is in a nutshell:

The old Bullet2 demos are being merged into the examples/ExampleBrowser
A new physics-engine agnostic C-API is created, see examples/SharedMemory/PhysicsClientC_API.h
Python bindings in pybullet are on top of this C-API, see examples/pybullet
A Virtual Reality sandbox using openvr for HTC Vive and Oculus Rift is available
The OpenCL examples in the ExampleBrowser can be enabled using --enable_experimental_opencl

You can still use svn or svn externals using the github git repository: use svn co https://github.com/bulletphysics/bullet3/trunk
Requirements for Bullet 2
A C++ compiler for C++ 2003. The library is tested on Windows, Linux, Mac OSX, iOS, Android,
but should likely work on any platform with C++ compiler.
Some optional demos require OpenGL 2 or OpenGL 3, there are some non-graphical demos and unit tests too.
Contributors and Coding Style information
https://docs.google.com/document/d/1u9vyzPtrVoVhYqQOGNWUgjRbfwfCdIts_NzmvgiJ144/edit
Requirements for experimental OpenCL GPGPU support
The entire collision detection and rigid body dynamics can be executed on the GPU.
A high-end desktop GPU, such as an AMD Radeon 7970 or NVIDIA GTX 680 or better.
We succesfully tested the software under Windows, Linux and Mac OSX.
The software currently doesn't work on OpenCL CPU devices. It might run
on a laptop GPU but performance will not likely be very good. Note that
often an OpenCL drivers fails to compile a kernel. Some unit tests exist to
track down the issue, but more work is required to cover all OpenCL kernels.
License
All source code files are licensed under the permissive zlib license
(http://opensource.org/licenses/Zlib) unless marked differently in a particular folder/file.
Build instructions for Bullet using premake. You can also use cmake instead.
Windows
Click on build_visual_studio_vr_pybullet_double.bat and open build3/vs2010/0MySolution.sln
When asked, convert the projects to a newer version of Visual Studio.
If you installed Python in the C:\ root directory, the batch file should find it automatically.
Otherwise, edit this batch file to choose where Python include/lib directories are located.
Windows Virtual Reality sandbox for HTC Vive and Oculus Rift
Build and run the App_SharedMemoryPhysics_VR project, preferably in Release/optimized build.
You can connect from Python pybullet to the sandbox using:
import pybullet as p
p.connect(p.SHARED_MEMORY) #or (p.TCP, ""localhost"", 6667) or (p.UDP, ""192.168.86.10"",1234)

Linux and Mac OSX gnu make
Make sure cmake is installed (sudo apt-get install cmake, brew install cmake, or https://cmake.org)
In a terminal type:
./build_cmake_pybullet_double.sh

This script will invoke cmake and build in the build_cmake directory. You can find pybullet in Bullet/examples/pybullet.
The BulletExampleBrowser binary will be in Bullet/examples/ExampleBrowser.
You can also build Bullet using premake. There are premake executables in the build3 folder.
Depending on your system (Linux 32bit, 64bit or Mac OSX) use one of the following lines
Using premake:
	cd build3
	./premake4_linux --double gmake
	./premake4_linux64 --double gmake
	./premake4_osx --double --enable_pybullet gmake

Then
	cd gmake
	make

Note that on Linux, you need to use cmake to build pybullet, since the compiler has issues of mixing shared and static libraries.
Mac OSX Xcode
Click on build3/xcode4.command or in a terminal window execute
./premake_osx xcode4

Usage
The App_ExampleBrowser executables will be located in the bin folder.
You can just run it though a terminal/command prompt, or by clicking it.
[--start_demo_name=""Demo Name""]     Start with a selected demo  
[--mp4=moviename.mp4]               Create a mp4 movie of the window, requires ffmpeg installed
[--mouse_move_multiplier=0.400000]  Set the mouse move sensitivity
[--mouse_wheel_multiplier=0.01]     Set the mouse wheel sensitivity
[--background_color_red= 0.9]       Set the red component for background color. Same for green and blue
[--fixed_timestep= 0.0]             Use either a real-time delta time (0.0) or a fixed step size (0.016666)

You can use mouse picking to grab objects. When holding the ALT or CONTROL key, you have Maya style camera mouse controls.
Press F1 to create a series of screenshots. Hit ESCAPE to exit the demo app.
Check out the docs folder and the Bullet physics forums for further information.
",4930
elnormous/ouzel,C++,"
Ouzel v0.40
   
Ouzel is a C++ game engine mainly targeted for development of 2D games.
Supported platforms:

Windows 7, 8, 10
macOS 10.8+
Linux
iOS 8+
tvOS 9+
Android 3.0+
Emscripten (sample)

Supported rendering backends:

Direct3D 11
OpenGL 2, OpenGL 3 and OpenGL 4
OpenGL ES 2 and OpenGL ES 3
Metal

Supported audio backends:

XAudio 2
DirectSound
CoreAudio
OpenAL
OpenSL ES
ALSA

Features

Cross-platform (Windows, macOS, iOS, tvOS, Android, Linux, and Emscripten targets supported)
Multi-threaded (separate threads for rendering, sound, and game)
2D and 3D scene management
GUI helper classes and management
Bitmap and true-type font support
Multiple side-by-side viewport support
XInput, DirectInput, IOKit, Apple GameController, and Linux evdev gamepad support
Actor animation (including tweening) system
Particle systems
Resource caching system
Localization support via loading string translations and UTF-8 string support
Software audio mixer for sound effect playback
High DPI support on Windows, macOS, and iOS
Easy to install (just pull the repository and build it)

Example app
The following code will open create a scene with a sprite in the center of it:
#include ""ouzel.hpp""

class Example: public ouzel::Application
{
public:
    Example():
        assets(ouzel::engine->getCache())
    {
        assets->loadAsset(ouzel::assets::Loader::IMAGE, ""player.png"");
        ouzel::engine->getSceneManager().setScene(&scene);
        scene.addLayer(&layer);
        cameraActor.addComponent(&camera);
        layer.addChild(&cameraActor);
        playerSprite.init(""player.png"");
        player.addComponent(&playerSprite);
        layer.addChild(&player);
    }

private:
    ouzel::scene::Scene scene;
    ouzel::scene::Layer layer;
    ouzel::scene::Camera camera;
    ouzel::scene::Actor cameraActor;
    ouzel::scene::Sprite playerSprite;
    ouzel::scene::Actor player;
    ouzel::assets::Bundle assets;
}

std::unique_ptr<ouzel::Application> ouzel::main(const std::vector<std::string>& args)
{
    return std::unique_ptr<ouzel::Application>(new Example());
}
Showcase
2D platformer Bearslayer is being developed using Ouzel engine.

Compilation
GNU makefile, Xcode project, and Visual Studio project files are located in the ""build"" directory. Makefile and project files for sample project are located in the ""samples"" directory.
You will need to download OpenGL (e.g. Mesa), ALSA, and OpenAL drivers installed in order to build Ouzel on Linux. For x86 Linux also libx11, libxcursor, libxi, and libxss are required.
To build Ouzel with Emscripten, pass ""PLATFORM=emscripten"" to ""make"" command, but make sure that you have Emscripten SDK installed before doing so:
$ make PLATFORM=emscripten
You can build Android samples and run them on an Android device by executing the following commands in ""samples/android"" directory (Android SDK and NDK must be installed and added to PATH):
$ gradle assembleDebug
$ gradle installDebug
$ adb shell am start -n org.ouzel/org.ouzel.MainActivity
Because on Raspbian Stretch libEGL.so was renamed to libbrcmEGL.so and libGLESv2.so to libbrcmGLESv2.so you will have to run the following commands before building the samples on Raspbian 8 (Jessie) or older:
$ sudo ln -s /opt/vc/lib/libEGL.so /opt/vc/lib/libbrcmEGL.so 
$ sudo ln -s /opt/vc/lib/libGLESv2.so /opt/vc/lib/libbrcmGLESv2.so
System requirements

Windows 7+ with Visual Studio 2015 or Visual Studio 2017
macOS 10.10+ with Xcode 7.2+
Any reasonable new Linux distro (x86 and ARM are supported)

Getting help
You can ask question in the following locations:

Ouzel Twitter account: https://twitter.com/ouzelengine
Ouzel server on Discord: https://discord.gg/4sWuJE8
#ouzel channel on the Freenode network (Freenode Webchat)
Author of the Ouzel engine: https://twitter.com/elnormous
Development roadmap: https://trello.com/b/5tRlUXKR/ouzel-roadmap

License
Ouzel codebase is licensed under the BSD license. Please refer to the LICENSE file for detailed information.
",306
oubiwann/docker-lfe-yaws-sample-app,None,"LFE+YAWS: Docker
This project has been deprecated
The lfeyawsdeno project has been
converted to use Docker and is the place to go now :-)
",7
cdapio/cdap,Java,"




Introduction
CDAP is an integrated, open source application
development platform for the Hadoop ecosystem that provides developers with data and
application abstractions to simplify and accelerate application development, address a
broader range of real-time and batch use cases, and deploy applications into production
while satisfying enterprise requirements.
CDAP is a layer of software running on top of Apache Hadoop® platforms such as the
Cloudera Enterprise Data Hub or the Hortonworks® Data Platform. CDAP provides these
essential capabilities:

Abstraction of data in the Hadoop environment through logical representations of underlying data;
Portability of applications through decoupling underlying infrastructures;
Services and tools that enable faster application creation in development;
Integration of the components of the Hadoop ecosystem into a single platform;
Metadata management that automatically captures metadata and lineage;
CDAP pipelines with an integrated UI for click-and-drag development; and
Higher degrees of operational control in production through enterprise best-practices.

CDAP exposes developer APIs (Application Programming Interfaces) for creating applications
and accessing core CDAP services. CDAP defines and implements a diverse collection of
services that land applications and data on existing Hadoop infrastructure such as HBase,
HDFS, YARN, MapReduce, Hive, and Spark.
You can run applications ranging from simple MapReduce Jobs and complete ETL (extract,
transform, and load) pipelines all the way up to complex, enterprise-scale data-intensive
applications.
Developers can build and test their applications end-to-end in a full-stack, single-node
installation. CDAP can be run either as a Sandbox, deployed within the Enterprise
on-premises or hosted in the Cloud.
For more information, see our collection of Developers' Manual and other documentation.

Getting Started

Prerequisites
To install and use CDAP, there are a few simple prerequisites:

JDK 8+ (required to run CDAP; note that $JAVA_HOME should be set)
Node.js (required to run the CDAP UI; we recommend any version greater than v4.5.0)
Apache Maven 3.0+ (required to build the example applications; 3.1+ to build CDAP itself)


Build
You can get started with CDAP by building directly from the latest source code:
git clone https://github.com/caskdata/cdap.git
cd cdap
mvn clean package

After the build completes, you will have built all modules for CDAP.
For more build options, please refer to the build instructions.

Introductory Tutorial
Visit our web site for an introductory tutorial for developers that
will guide you through installing CDAP and running an example application.

Where to Go Next
Now that you've had a look at the CDAP Sandbox, take a look at:

Developers' Manual, located in the source distribution in cdap-docs/developers-manual/source
or online.
CDAP Releases and timeline


How to Contribute
Interested in helping to improve CDAP? We welcome all contributions, whether in filing
detailed bug reports, submitting pull requests for code changes and improvements, or by
asking questions and assisting others on the mailing list.
For quick guide to getting your system setup to contribute to CDAP, take a look at our
Contributor Quickstart Guide.

Filing Issues: Bug Reports & Feature Requests
Bugs and suggestions should be made by filing an issue.
Existing issues can be browsed at the CDAP project issues.

Pull Requests
We have a simple pull-based development model with a consensus-building phase, similar to
Apache's voting process. If you’d like to help make CDAP better by adding new features,
enhancing existing features, or fixing bugs, here's how to do it:

If you are planning a large change or contribution, discuss your plans on the
cdap-dev@googlegroups.com mailing list first.
This will help us understand your needs and best guide your solution in a way that fits the project.
Fork CDAP into your own GitHub repository.
Create a topic branch with an appropriate name.
Work on the code to your heart's content.
Once you’re satisfied, create a pull request from your GitHub repo (it’s helpful if you fill in
all of the description fields).
After we review and accept your request, we’ll commit your code to the caskdata/cdap repository.

Thanks for helping to improve CDAP!

Mailing Lists
CDAP User Group and Development Discussions:

cdap-user@googlegroups.com

The cdap-user mailing list is primarily for users using the product to develop
applications. You can expect questions from users, release announcements, and any other
discussions that we think will be helpful to the users.

cdap-dev@googlegroups.com

The cdap-dev mailing list is essentially for developers actively working
on the product, and should be used for all our design, architecture and technical
discussions moving forward. This mailing list will also receive all JIRA and GitHub
notifications.

License and Trademarks
Copyright © 2014-2017 Cask Data, Inc.
Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except
in compliance with the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software distributed under the
License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
either express or implied. See the License for the specific language governing permissions
and limitations under the License.
Cask is a trademark of Cask Data, Inc. All rights reserved.
Apache, Apache HBase, and HBase are trademarks of The Apache Software Foundation. Used with
permission. No endorsement by The Apache Software Foundation is implied by the use of these marks.
",330
GMUEClab/ecj,Java,"The ECJ Evolutionary Computation Toolkit
ECJ is an evolutionary computation framework written in Java. The system was designed for large, heavyweight experimental needs and provides tools which provide many popular EC algorithms and conventions of EC algorithms, but with a particular emphasis towards genetic programming. ECJ is free open-source with a BSD-style academic license (AFL 3.0).
ECJ is now well over fifteen years old and is a mature, stable framework which has (fortunately) exhibited relatively few serious bugs over the years. Its design has readily accommodated many later additions, including multiobjective optimization algorithms, island models, master/slave evaluation facilities, coevolution, steady-state and evolution strategies methods, parsimony pressure techniques, and various new individual representations (for example, rule-sets). The system is widely used in the genetic programming community and is reasonably popular in the EC community at large, where it has formed the basis of many theses, publications, and commercial products.
ECJ's Website
This is ECJ's repository, but ECJ's official website is elsewhere.  Before doing anything else, we'd recommend you started there.
Getting Started
For instructions on how to begin using the ECJ binary distribution and/or build the source package, take a look at the readme in the 'ecj/' subdirectory.
Going forward, you may also want to avail yourself of

the extensive ECJ Manual, which explains most of ECJ's features and algorims in detail, with instructions on how to use them,
the ECJ tutorials,
and the built-in collectin of example applications (source code here, parameter files here).

Citing ECJ
The preferred way to cite ECJ is

Sean Luke. ECJ Evolutionary Computation Library (1998).  Available for free at http://cs.gmu.edu/~eclab/projects/ecj/

or in BibTex like so:
@misc { Luke1998ECJSoftware,
author       = { Sean Luke },
title        = { {ECJ} Evolutionary Computation Library },
year         = { 1998 },
note         = { Available for free at http://cs.gmu.edu/$\sim$eclab/projects/ecj/  }
}

",64
vicner/eu4,None,"eu4
Europa Universalis IV – спільний переклад.
Переклад гри 'Europa Universalis IV' на українську на основі перекладу від толоки Гуртом.
У процесі було вичитано та виправлено тисячі рядків тексту, перекладено з нуля (на разі) із тузінь DLC та офіційних патчів.
Можна форкати репозиторій; звʼязок з перекладацькою спільнотою через Discord-сервер (бажано) або на толоці. Будемо раді не лише перекладачам, а й тестувальникам і просто любителям стратегій українською.
Основні рекомендації щодо перекладу можна знайти у вікі проекту
Будь-яке комерційне використання вмісту цього репозиторію забороняється без узгодження з власником цього репозиторію та спільнотою у відповідному розділі толоки або Discord-сервера.
Підтримайте проект!
ПриватБанк: 5168 7427 0122 4887
Сторінка моду на Стімі.
",4
comiccruncher/comiccruncher-frontend,JavaScript,"Comic Cruncher Web
Comic Cruncher Web is a NodeJS + React app that serves the frontend for comiccruncher.com. (Check out the Golang backend application at github.com/aimeelaplant/comiccruncher).
This repository uses the next.js framework to server-side render the pages.
The components folder houses all the React components, and the pages folder houses all the HTTP frontend.
Check the server.js file to see how the pages get server-side rendered.
Getting Started
yarn install
Install all the dependencies to get started.
yarn dev
Starts the dev server with hot module replacement.
yarn start
Starts the production build.
Creates a new build, optimized for production. Does not start a dev server or anything else.
CSS
To use the grid system rebass with emotion, import the module like this: import { Flex, Box } from 'rebass/emotion'
Styling
We're using the most millennial styling library for react.
Emotion
Make sure to add import styled, { css } from 'react-emotion' at the top of each component or view file.
Deployment
The production application is deployed to GCP's App Engine via a CircleCI configuration.
",2
CodeForFoco/codeforfoco.github.io,HTML,"Code For Fort Collins





This repo is for our organization's website at codeforfoco.org.
Getting Started
Before you can run the website on your machine you need to ensure you have some prerequisites installed:

Fork this repository and clone your fork.
cd into the cloned directory.
Run git submodule update --init --recursive to install several third-party submodules.
Install Ruby V2 or greater, we recommend you use RVM.
Install bundler by running gem install bundler.
Install any Ruby dependencies by running bundle install.

Once all the dependencies have been installed you run the development server with:
bundle exec jekyll serve
Open your browser up to localhost:8080 to view the development site.
Contributing
We welcome new contributors.  Be sure to check out guide on contributing, which includes instructions on how to fork, clone, branch, commit, pull request and sync your fork.
Not sure where to start? Look for open issues on GitHub, or message the team on our Slack site. If you aren't on our Slack, click here for an invite.
TL;DR Contribution Workflow:

Fork this repository and Clone your fork locally.
Checkout a new branch on which to make your changes.
Make edits. Try to match existing coding style.
Test your changes.
Commit your changes. Push your changes to your fork on GitHub.
Submit a new pull request and your changes will be reviewed and merged.

Code Standards
Development in a team environment can result in code inconsistency, which may
have undesired impacts including a reduction in readability. Therefore we ask
that you standardize and ""beautify"" your code before submitting a pull request.
A few options to simplify this process are outlined below.
EditorConfig project

EditorConfig helps developers define and maintain consistent coding styles between different editors and IDEs.

This tool eliminates common inconsistencies such as tab style and line endings,
and has plugins available for most modern IDEs and text editors. Find yours here.
Linters
Linters validate your code and provide errors and warnings when validation is
not met. Here are some linters for common text editors:

linter for Atom and these
language-specific linters:

linter-scss-lint
linter-jshint
linter-htmlhint


SublimeLinter for Sublime Text and these language-specific linters:

SublimeLinter-scss-lint
SublimeLinter-jshint
SublimeLinter-html-tidy



Beautifiers
The settings for beautifying this repo's code is found in .csscomb.json and
.jsbeautifyrc at the root of the project. Implementation of the settings is
carried out by various plugins specific to each text editor. Here are some of
the most common:

atom-beautify is an all-in-one
package for Atom that will handle beautification of most common languages (FYI
it's a big package so it takes a little while for the installation to complete!).
Sublime Text does not have an all-in-one beautifier, but
Sublime-HTMLPrettify
should suffice for this project.

Site Architecture
The CFFC website will ultimately adhere to an organized, intuitive, and SEO-friendly navigation and content structure, currently based on this model:

Bugs / Feedback / Suggestions
We encourage you to open up an issue if you have any feedback, suggestions or bugs.
License
MIT, see LICENSE for full license.
",8
circleci/circleci-docs,JavaScript,"CircleCI Documentation   
This is the public repository for https://circleci.com/docs/, a static website generated by Jekyll. If you find any errors in our docs or have suggestions, please follow our Contributing Guide to submit an issue or pull request.
For minor changes like typos, you can click Suggest an edit to this page, located at the bottom of each article. This will take you to the source file on GitHub, where you can submit a pull request for your change through the UI.
For larger edits or new articles, you'll want to set up a local environment for editing. Please see our Local Development README to set that up.
If you have a question or need help debugging, please head to CircleCI Discuss where our support team will help you out.
License Information
Documentation (guides, references, and associated images) is licensed as Creative Commons Attribution-NonCommercial-ShareAlike CC BY-NC-SA. The full license can be found here, and the human-readable summary here.
Everything in this repository not covered above is licensed under the included MIT license.
",311
forkgood/easyhosts,None,"最新动态
Easy Hosts 项目一直以来都是根据上游 Github Hosts 规则，在服务器上无人工干预自动完成整合更新的，但是大陆网络环境复杂，Google 网域 IP 封锁严重，如果遇到无法访问Google网域的状况，可在 Issues 进行反馈，核实后我们将会尽快手动更新最新可用的 Google 网域 IP，以确保项目可用性，感谢网友反馈与关注，谢谢。
easyhosts
基于 Github 项目整合的远程 Hosts 直链，适配多种规则、终端。
每 30 分钟 自动同步一次 Github 最新可用项目并提供打包下载。
hosts 支持站点
目前支持访问以下网域，具体自行搜索规则文件：
Google 全家桶，Facebook，Instagram，Twitter，Tumblr，Youtube视频，Google Play下载等。
注意：Hosts方式访问以上站点，大部分只允许以HTTPS方式打开。
文件说明
更多说明请访问 easyhosts 项目主页：https://windows.cat

常规增强 Hosts：(racaljk 常用网站 + sy618 Google Play 下载源 + sy618 Youtube 视频源)
hosts.txt：常规增强 Hosts 规则，适用于Android/iOS/Windows/Mac OS/Linux等。
dnsmasq.txt：常规增强 dnsmasq规则，适用于 Linux 及 OpenWrt，路由器可用。
surge.txt：常规增强 Surge 规则，适用于Surge/Shadowrocket等各种代理软件。

去广告增强 Hosts：(racaljk 常用网站 + sy618 Google Play 下载源 + sy618 Youtube 视频源 + vokins 去广告源)
hosts-noad.txt：去广告增强 Hosts 规则，适用于Android/iOS/Windows/Mac
dnsmasq-noad.txt：去广告增强 dnsmasq规则，适用于 Linux 及 OpenWrt，路由器可用。
surge-noad.txt：去广告增强 Surge 规则，适用于Surge/Shadowrocket等各种代理软件。

其他文件：
log.txt：最新检测时间（更新周期以此文件为准）
检测周期为30分钟，本 Github 项目中的所有规则只会在上游Hosts发生变化时自动与Github同步，同步周期往往超过30分钟。
hosts.tar.gz：包含所有规则的压缩包文件
windows_hosts_manager_tools.zip：Windows Hosts 管理工具(批处理)
感谢
racaljk / sy618 / vokins / 景文互联
",237
aws/amazon-freertos,C,"Getting Started
For more information on Amazon FreeRTOS, refer to the Getting Started section of Amazon FreeRTOS webpage.
To directly access the Getting Started Guide for supported hardware platforms, click the corresponding link in the Supported Hardware section below.
For detailed documentation on Amazon FreeRTOS, refer to the Amazon FreeRTOS User Guide.
Supported Hardware
The following MCU boards are supported for Amazon FreeRTOS:

Texas Instruments - CC3220SF-LAUNCHXL.

Getting Started Guide
IDEs: Code Composer Studio, IAR Embedded Workbench


STMicroelectronics - STM32L4 Discovery kit IoT node.

Getting Started Guide
IDE: STM32 System Workbench


NXP - LPC54018 IoT Module.

Getting Started Guide
IDEs: IAR Embedded Workbench, MCUXpresso IDE


Microchip - Curiosity PIC32MZEF.

Getting Started Guide
IDE: MPLAB X IDE


Espressif - ESP32-DevKitC, ESP-WROVER-KIT.

Getting Started Guide


Infineon - Infineon XMC4800 IoT Connectivity Kit

Getting Started Guide
IDE: DAVE


Xilinx - Xilinx Zynq-7000 based MicroZed Industrial IoT Bundle

Getting Started Guide
IDE: Xilinx SDK


MediaTek - MediaTek MT7697Hx Development Kit

Getting Started Guide
IDE: Keil uVision


Renesas - Renesas Starter Kit+ for RX65N-2MB

Getting Started Guide
IDE: e2 studio


Cypress CYW54907 - Cypress CYW954907AEVAL1F Evaluation Kit

Getting Started Guide
IDE: WICED Studio


Cypress CYW43907 - Cypress CYW943907AEVAL1F Evaluation Kit

Getting Started Guide
IDE: WICED Studio


Windows Simulator - To evaluate Amazon FreeRTOS without using MCU-based hardware, you can use the Windows Simulator.

Requirements: Microsoft Windows 7 or newer, with at least a dual core and a hard-wired Ethernet connection
Getting Started Guide
IDE: Visual Studio Community Edition



",1260
quiltdata/t4,JavaScript,"




Overview
Rethinking S3: Announcing T4, a team data hub.
A team data hub for S3

T4 adds search, content preview, versioning, and a Python API to any S3 bucket
Every file in T4 is versioned and searchable
T4 is for data scientists, data engineers, and data-driven teams


Use cases

Collaborate - get everyone on the same page by pointing them all to the same immutable data version
Experiment faster - blob storage is schemaless and scalable, so iterations are quick
Recover, rollback, and reproduce with immutable packages
Understand what's in S3 - plaintext and faceted search over S3

Key features

Browse, search any S3 bucket
Preview images, Jupyter notebooks, Vega visualizations - without downloading
Read/write Python objects to and from S3
Immutable versions for objects, immutable packages for collections of objects

Components

/catalog (JavaScript) - Search, browse, and preview your data in S3
/api/python - Read, write, and annotate Python objects in S3

Roadmap

Roadmap

",59
smltq/jPublic,JavaScript,"jPublic




交流QQ群：1017567122
前言
在我们开发项目的时候，无论项目规模大小，在所难免会写一些工具型函数来解决一些问题，随着项目开发和维护的时间越来越长，这些工具型函数会越来越多，同时还会穿插在各个项目的各模块或者文件当中，使得项目变的越来越臃肿，也不方便复用和维护。这时我们就会提取出一个类似的工具库或者基础库作为项目基础依赖，在项目中重复利用起来。
为了这样的工具库或类库更易扩展、易维护、易复用和更加稳定，我们就需要更好的去管理完善工具库。
jPublic项目介绍
jPublic 是一个 JavaScript 工具库，它提供了一整套函数式编程的实用功能，但是不依赖任何第三方插件。它弥补了 jQuery、Underscore等没有实现的功能，希望能成为我们项目必不可少的部分。
jPublic 目前提供了80多个函数，包括常用的：debounce、throttle、poll等。
本项目托管在GitHub上。 你可以通过issues page、QQ群等途径报告bug以及参与特性讨论。
jPublic是一个完全开源的JavaScript开源工具库。
组织结构
+-- docs  API文档
|   +-- index.html          文档入口
|   +-- ...
+-- test  测试
|   +-- utility.js
|   +-- testIndex.html      单元测试入口
|   +-- ...
--- .gitignore              git忽略规则
--- LICENSE                 开源协议
--- README.md               项目说明
--- favicon.ico             icon

--- karma.conf.js           karma配置

--- jsdoc.json              文档生成配置
--- package.json            npm配置

--- jPublic.js              源文件
--- jPublic-min.js          压缩文件
--- jPublic-min.map         sourcemap

环境配置



技术
名称
官网




karma
测试框架
https://github.com/karma-runner


qunit
单元测试工具
https://qunitjs.com/


jsdoc
文档生成
https://github.com/jsdoc/jsdoc


nodejs
js运行时
https://nodejs.org/zh-cn/


UglifyJS2
压缩工具
https://github.com/mishoo/UglifyJS2/tree/v2.x



引入方式
Require.js      require([""jPublic""]
页面            <script src=""jPublic.min.js""></script>

在线文档

API文档：https://smltq.github.io/jPublic/
码云文档：https://tqlin.gitee.io/jpublic/module-_.html#.DEFAULT_SCALE（国内建议访问这个地址）
码云仓库：https://gitee.com/tqlin/jPublic

FAQ

JavaScript空字符串判断
侍做事项，有兴趣加入项目的朋友，把帐号发我，最好能接个任务去研究。

许可证
MIT
",31
forkgood/easyhosts,None,"最新动态
Easy Hosts 项目一直以来都是根据上游 Github Hosts 规则，在服务器上无人工干预自动完成整合更新的，但是大陆网络环境复杂，Google 网域 IP 封锁严重，如果遇到无法访问Google网域的状况，可在 Issues 进行反馈，核实后我们将会尽快手动更新最新可用的 Google 网域 IP，以确保项目可用性，感谢网友反馈与关注，谢谢。
easyhosts
基于 Github 项目整合的远程 Hosts 直链，适配多种规则、终端。
每 30 分钟 自动同步一次 Github 最新可用项目并提供打包下载。
hosts 支持站点
目前支持访问以下网域，具体自行搜索规则文件：
Google 全家桶，Facebook，Instagram，Twitter，Tumblr，Youtube视频，Google Play下载等。
注意：Hosts方式访问以上站点，大部分只允许以HTTPS方式打开。
文件说明
更多说明请访问 easyhosts 项目主页：https://windows.cat

常规增强 Hosts：(racaljk 常用网站 + sy618 Google Play 下载源 + sy618 Youtube 视频源)
hosts.txt：常规增强 Hosts 规则，适用于Android/iOS/Windows/Mac OS/Linux等。
dnsmasq.txt：常规增强 dnsmasq规则，适用于 Linux 及 OpenWrt，路由器可用。
surge.txt：常规增强 Surge 规则，适用于Surge/Shadowrocket等各种代理软件。

去广告增强 Hosts：(racaljk 常用网站 + sy618 Google Play 下载源 + sy618 Youtube 视频源 + vokins 去广告源)
hosts-noad.txt：去广告增强 Hosts 规则，适用于Android/iOS/Windows/Mac
dnsmasq-noad.txt：去广告增强 dnsmasq规则，适用于 Linux 及 OpenWrt，路由器可用。
surge-noad.txt：去广告增强 Surge 规则，适用于Surge/Shadowrocket等各种代理软件。

其他文件：
log.txt：最新检测时间（更新周期以此文件为准）
检测周期为30分钟，本 Github 项目中的所有规则只会在上游Hosts发生变化时自动与Github同步，同步周期往往超过30分钟。
hosts.tar.gz：包含所有规则的压缩包文件
windows_hosts_manager_tools.zip：Windows Hosts 管理工具(批处理)
感谢
racaljk / sy618 / vokins / 景文互联
",237
aws/amazon-freertos,C,"Getting Started
For more information on Amazon FreeRTOS, refer to the Getting Started section of Amazon FreeRTOS webpage.
To directly access the Getting Started Guide for supported hardware platforms, click the corresponding link in the Supported Hardware section below.
For detailed documentation on Amazon FreeRTOS, refer to the Amazon FreeRTOS User Guide.
Supported Hardware
The following MCU boards are supported for Amazon FreeRTOS:

Texas Instruments - CC3220SF-LAUNCHXL.

Getting Started Guide
IDEs: Code Composer Studio, IAR Embedded Workbench


STMicroelectronics - STM32L4 Discovery kit IoT node.

Getting Started Guide
IDE: STM32 System Workbench


NXP - LPC54018 IoT Module.

Getting Started Guide
IDEs: IAR Embedded Workbench, MCUXpresso IDE


Microchip - Curiosity PIC32MZEF.

Getting Started Guide
IDE: MPLAB X IDE


Espressif - ESP32-DevKitC, ESP-WROVER-KIT.

Getting Started Guide


Infineon - Infineon XMC4800 IoT Connectivity Kit

Getting Started Guide
IDE: DAVE


Xilinx - Xilinx Zynq-7000 based MicroZed Industrial IoT Bundle

Getting Started Guide
IDE: Xilinx SDK


MediaTek - MediaTek MT7697Hx Development Kit

Getting Started Guide
IDE: Keil uVision


Renesas - Renesas Starter Kit+ for RX65N-2MB

Getting Started Guide
IDE: e2 studio


Cypress CYW54907 - Cypress CYW954907AEVAL1F Evaluation Kit

Getting Started Guide
IDE: WICED Studio


Cypress CYW43907 - Cypress CYW943907AEVAL1F Evaluation Kit

Getting Started Guide
IDE: WICED Studio


Windows Simulator - To evaluate Amazon FreeRTOS without using MCU-based hardware, you can use the Windows Simulator.

Requirements: Microsoft Windows 7 or newer, with at least a dual core and a hard-wired Ethernet connection
Getting Started Guide
IDE: Visual Studio Community Edition



",1260
quiltdata/t4,JavaScript,"




Overview
Rethinking S3: Announcing T4, a team data hub.
A team data hub for S3

T4 adds search, content preview, versioning, and a Python API to any S3 bucket
Every file in T4 is versioned and searchable
T4 is for data scientists, data engineers, and data-driven teams


Use cases

Collaborate - get everyone on the same page by pointing them all to the same immutable data version
Experiment faster - blob storage is schemaless and scalable, so iterations are quick
Recover, rollback, and reproduce with immutable packages
Understand what's in S3 - plaintext and faceted search over S3

Key features

Browse, search any S3 bucket
Preview images, Jupyter notebooks, Vega visualizations - without downloading
Read/write Python objects to and from S3
Immutable versions for objects, immutable packages for collections of objects

Components

/catalog (JavaScript) - Search, browse, and preview your data in S3
/api/python - Read, write, and annotate Python objects in S3

Roadmap

Roadmap

",59
smltq/jPublic,JavaScript,"jPublic




交流QQ群：1017567122
前言
在我们开发项目的时候，无论项目规模大小，在所难免会写一些工具型函数来解决一些问题，随着项目开发和维护的时间越来越长，这些工具型函数会越来越多，同时还会穿插在各个项目的各模块或者文件当中，使得项目变的越来越臃肿，也不方便复用和维护。这时我们就会提取出一个类似的工具库或者基础库作为项目基础依赖，在项目中重复利用起来。
为了这样的工具库或类库更易扩展、易维护、易复用和更加稳定，我们就需要更好的去管理完善工具库。
jPublic项目介绍
jPublic 是一个 JavaScript 工具库，它提供了一整套函数式编程的实用功能，但是不依赖任何第三方插件。它弥补了 jQuery、Underscore等没有实现的功能，希望能成为我们项目必不可少的部分。
jPublic 目前提供了80多个函数，包括常用的：debounce、throttle、poll等。
本项目托管在GitHub上。 你可以通过issues page、QQ群等途径报告bug以及参与特性讨论。
jPublic是一个完全开源的JavaScript开源工具库。
组织结构
+-- docs  API文档
|   +-- index.html          文档入口
|   +-- ...
+-- test  测试
|   +-- utility.js
|   +-- testIndex.html      单元测试入口
|   +-- ...
--- .gitignore              git忽略规则
--- LICENSE                 开源协议
--- README.md               项目说明
--- favicon.ico             icon

--- karma.conf.js           karma配置

--- jsdoc.json              文档生成配置
--- package.json            npm配置

--- jPublic.js              源文件
--- jPublic-min.js          压缩文件
--- jPublic-min.map         sourcemap

环境配置



技术
名称
官网




karma
测试框架
https://github.com/karma-runner


qunit
单元测试工具
https://qunitjs.com/


jsdoc
文档生成
https://github.com/jsdoc/jsdoc


nodejs
js运行时
https://nodejs.org/zh-cn/


UglifyJS2
压缩工具
https://github.com/mishoo/UglifyJS2/tree/v2.x



引入方式
Require.js      require([""jPublic""]
页面            <script src=""jPublic.min.js""></script>

在线文档

API文档：https://smltq.github.io/jPublic/
码云文档：https://tqlin.gitee.io/jpublic/module-_.html#.DEFAULT_SCALE（国内建议访问这个地址）
码云仓库：https://gitee.com/tqlin/jPublic

FAQ

JavaScript空字符串判断
侍做事项，有兴趣加入项目的朋友，把帐号发我，最好能接个任务去研究。

许可证
MIT
",31
MaxXSoft/Ionia,C++,"Ionia
Design and implementation of a functional scripting language.
Ionia is simple, lightweight functional programming language, with implementation of interpreter and VM runtime.
EBNF of Ion Lang
program ::= {stat};
stat    ::= define | funcall;
define  ::= id ""="" expr;
expr    ::= func | funcall | define | id | number;
func    ::= ""("" [id {"","" id}] "")"" "":"" expr;
funcall ::= id ""("" [expr {"","" expr}] "")"";
id      ::= re""~([0-9]|=|\(|\)|,|:)(=|\(|\)|,|:)*"";
number  ::= re""0|([1-9][0-9]*)"";
To-Do List

 Interpreter
 VM runtime
 Compiler
 Documents
 Disassembler
 REPL
 JIT

Copyright and License
Copyright (C) 2010-2019 MaxXing, MaxXSoft. License GPLv3.
",2
InvestmentSystems/static-frame,Python,"















static-frame
The StaticFrame library consists of the Series and Frame, immutable data structures for one- and two-dimensional calculations with self-aligning, labelled axes. StaticFrame offers an alternative to Pandas. While many interfaces for data extraction and manipulation are similar to Pandas, StaticFrame deviates from Pandas in many ways: all data is immutable, and all indices must be unique; all vector processing uses NumPy, and the full range of NumPy data types is preserved; the implementation is concise and lightweight; consistent naming and interfaces are used throughout; and flexible approaches to iteration and function application, with built-in options for parallelization, are provided.
Code: https://github.com/InvestmentSystems/static-frame
Docs: http://static-frame.readthedocs.io
Packages: https://pypi.org/project/static-frame

Installation
Install StaticFrame via PIP:
pip install static-frame

Or, install StaticFrame via conda:
conda install -c conda-forge static-frame


Dependencies
StaticFrame requires Python 3.6+ and NumPy 1.14.1+.

Quick-Start Guide
StaticFrame provides numerous methods for loading and creating data, either as a 1D Series or a 2D Frame. All creation routines are exposed as alternate constructors on the desired class, such as Frame.from_records(), Frame.from_csv() or Frame.from_pandas().
For example, we can load JSON data from a URL using Frame.from_json_url(), and then use Frame.head() to reduce the displayed output to just the first five rows. (Passing explicit dtypes is only necessary on Windows.)
>>> import numpy as np
>>> import static_frame as sf
>>> frame = sf.Frame.from_json_url('https://jsonplaceholder.typicode.com/photos', dtypes=dict(albumId=np.int64, id=np.int64))
>>> frame.head()
<Frame>
<Index> albumId id      title                url                  thumbnailUrl         <<U12>
<Index>
0       1       1       accusamus beatae ... https://via.place... https://via.place...
1       1       2       reprehenderit est... https://via.place... https://via.place...
2       1       3       officia porro iur... https://via.place... https://via.place...
3       1       4       culpa odio esse r... https://via.place... https://via.place...
4       1       5       natus nisi omnis ... https://via.place... https://via.place...
<int64> <int64> <int64> <<U86>               <<U38>               <<U38>

Note
The Pandas CSV reader far out-performs the NumPy-based reader in StaticFrame: thus, for now, using Frame.from_pandas(pd.read_csv(fp)) is recommended for loading CSV files.
For more information on Series and Frame constructors, see Container Import & Creation.

As with a NumPy array, the Frame exposes common attributes of shape and size.
>>> frame.shape
(5000, 5)
>>> frame.size
25000
>>> frame.nbytes
3320000
Unlike a NumPy array, a Frame stores heterogeneous types, where each column is a single type. StaticFrame preserves the full range of NumPy types, including fixed-size character strings. Character strings can be converted to Python objects or other types as needed with the Frame.astype interface, which exposes a __getitem__ style interface for selecting columns to convert. As with all similar functions, a new Frame is returned.
>>> frame.dtypes
<Series>
<Index>
albumId      int64
id           int64
title        <U86
url          <U38
thumbnailUrl <U38
<<U12>       <object>
>>> frame.astype['title':](object).dtypes
<Series>
<Index>
albumId      int64
id           int64
title        object
url          object
thumbnailUrl object
<<U12>       <object>
Utility functions common to Pandas users are available on Frame and Series, such as Series.unique(), Series.isna(), and Series.any().
>>> frame['albumId'].unique()
array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,
        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,
        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,
        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,
        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,
        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,
        92,  93,  94,  95,  96,  97,  98,  99, 100])
>>> frame['id'].isna().any()
False

Note
For more information on Series and Frame utility functions, see Transformations & Utilities.

StaticFrame interfaces for extracting data will be familiar to Pandas users, though with a number of interface refinements to remove redundancies and increase consistency. On a Frame, __getitem__ is (exclusively) a column selector; loc and iloc are (with one argument) row selectors or (with two arguments) row and column selectors.
For example we can select a single column with __getitem__:
>>> frame['albumId'].tail()
<Series: albumId>
<Index>
4995              100
4996              100
4997              100
4998              100
4999              100
<int64>           <int64>
Consistent with other __getitem__ style selectors, a slice or a list can be used to select columns:
>>> frame['id':'title'].head()
<Frame>
<Index> id      title                <<U12>
<Index>
0       1       accusamus beatae ...
1       2       reprehenderit est...
2       3       officia porro iur...
3       4       culpa odio esse r...
4       5       natus nisi omnis ...
<int64> <int64> <<U86>
The loc interface, with one argument, returns a Series for the row found at the given index label.
>>> frame.loc[4]
<Series: 4>
<Index>
albumId      1
id           5
title        natus nisi omnis ...
url          https://via.place...
thumbnailUrl https://via.place...
<<U12>       <object>
With two arguments, loc can select both rows and columns at the same time:
>>> frame.loc[4:8, ['albumId', 'title']]
<Frame>
<Index> albumId title                <<U12>
<Index>
4       1       natus nisi omnis ...
5       1       accusamus ea aliq...
6       1       officia delectus ...
7       1       aut porro officii...
<int64> <int64> <<U86>
Where the loc interface uses index and column labels, the iloc interface uses integer offsets from zero, just as if the Frame were a NumPy array. For example, we can select the last row with -1:
>>> frame.iloc[-1]
<Series: 4999>
<Index>
albumId        100
id             5000
title          error quasi sunt ...
url            https://via.place...
thumbnailUrl   https://via.place...
<<U12>         <object>
Or, using two arguments, we can select the first two columns of the last two rows:
>>> frame.iloc[-2:, 0:2]
<Frame>
<Index> albumId id      <<U12>
<Index>
4998    100     4999
4999    100     5000
<int64> <int64> <int64>
Just as with Pandas, expressions can be used in __getitem__, loc, and iloc statements to create more narrow selections. For example, we can select all ""albumId"" greater than or equal to 98.
>>> frame.loc[frame['albumId'] >= 98, ['albumId', 'title']].head()
<Frame>
<Index> albumId title                <<U12>
<Index>
4850    98      aut aut nulla vol...
4851    98      ducimus neque del...
4852    98      fugit officiis su...
4853    98      pariatur temporib...
4854    98      qui inventore inc...
<int64> <int64> <<U86>
However, unlike Pandas, __getitem__, loc, and iloc cannot be used for assignment or in-place mutation on a Frame or Series. Throughout StaticFrame, all underlying NumPy arrays, and all container attributes, are immutable. Making data and objects immutable reduces opportunities for coding errors and offers, in some situations, greater efficiency by avoiding defensive copies.
>>> frame.loc[4854, 'albumId']
98
>>> frame.loc[4854, 'albumId'] = 200
Traceback (most recent call last):
TypeError: 'GetItem' object does not support item assignment
>>> frame.values[4854, 0] = 200
Traceback (most recent call last):
ValueError: assignment destination is read-only

Note
For more information on Series and Frame selection interfaces, see Selection.

Instead of in-place assignment, an assign interface object (similar to the Frame.astype interface shown above) is provided to expose __getitem__, loc, and iloc interfaces that, when called with an argument, return a new object with the desired changes. These interfaces expose the full range of expressive assignment-like idioms found in Pandas and NumPy. Arguments can be single values, or Series and Frame objects, where assignment will align on the Index.
>>> frame_new = frame.assign.loc[4854, 'albumId'](200)
>>> frame_new.loc[4854, 'albumId']
200
This pattern of specialized interfaces is used throughout StaticFrame, such as with the Frame.mask and Frame.drop interfaces. For example, Frame.mask can be used to create a Boolean Frame that sets rows to True if their ""id"" is even:
>>> frame.mask.loc[frame['id'] % 2 == 0].head()
<Frame>
<Index> albumId id     title  url    thumbnailUrl <<U12>
<Index>
0       False   False  False  False  False
1       True    True   True   True   True
2       False   False  False  False  False
3       True    True   True   True   True
4       False   False  False  False  False
<int64> <bool>  <bool> <bool> <bool> <bool>
Or, using the Frame.drop interface, a new Frame can be created by dropping rows with even ""id"" values and dropping URL columns specified in a list:
>>> frame.drop.loc[frame['id'] % 2 == 0, ['thumbnailUrl', 'url']].head()
<Frame>
<Index> albumId id      title                <<U12>
<Index>
0       1       1       accusamus beatae ...
2       1       3       officia porro iur...
4       1       5       natus nisi omnis ...
6       1       7       officia delectus ...
8       1       9       qui eius qui aute...
<int64> <int64> <int64> <<U86>

Note
For more information on Series and Frame interfaces, see Assignment / Dropping / Masking.

Iteration of rows, columns, and elements, as well as function application on those values, is unified under a family of generator interfaces. These interfaces are distinguished by the form of the data iterated (Series, namedtuple, or array) and whether key-value pairs (e.g., Frame.iter_series_items()) or just values (e.g., Frame.iter_series()) are yielded. For example, we can iterate over each row of a Frame and yield a corresponding Series:
>>> next(iter(frame.iter_series(axis=1)))
<Series>
<Index>
albumId      1
id           1
title        accusamus beatae ...
url          https://via.place...
thumbnailUrl https://via.place...
<<U12>       <object>
Or we can iterate over rows as named tuples, applying a function that matches a substring of the ""title"" or returns None, then drop those None records:
>>> frame.iter_tuple(axis=1).apply(lambda r: r.title if 'voluptatem' in r.title else None).dropna().head()
<Series>
<Index>
19       assumenda volupta...
27       non neque eligend...
29       odio enim volupta...
31       ad enim dignissim...
40       in voluptatem dol...
<int64>  <object>
Element iteration and function application works the same way as for rows or columns (though without an axis argument). For example, here each URL is processed with the same string transformation function:
>>> frame[['thumbnailUrl', 'url']].iter_element().apply(lambda c: c.replace('https://', '')).iloc[-4:]
<Frame>
<Index> thumbnailUrl         url                  <<U12>
<Index>
4996    via.placeholder.c... via.placeholder.c...
4997    via.placeholder.c... via.placeholder.c...
4998    via.placeholder.c... via.placeholder.c...
4999    via.placeholder.c... via.placeholder.c...
<int64> <object>             <object>
Group-by functionality is exposed in a similar manner with Frame.iter_group_items() and Frame.iter_group().
>>> next(iter(frame.iter_group('albumId', axis=0))).shape
(50, 5)
Function application to a group Frame can be used to produce a Series indexed by the group label. For example, a Series, indexed by ""albumId"", can be produced to show the number of unique titles found per album.
>>> frame.iter_group('albumId', axis=0).apply(lambda g: len(g['title'].unique())).head()
<Series>
<Index>
1        50
2        50
3        50
4        50
5        50
<int64>  <int64>

Note
For more information on Series and Frame iterators and tools for function application, see Iterators.

If performing calculations on a Frame that result in a Series with a compatible Index, a grow-only FrameGO can be used to add Series as new columns. This limited form of mutation, i.e., only the addition of columns, provides a convenient compromise between mutability and immutability. (Underlying NumPy array data always remains immutable.)
A FrameGO can be efficiently created from a Frame, as underling NumPy arrays do not have to be copied:
>>> frame_go = frame.to_frame_go()
We can obtain a track number within each album, assuming the records are sorted, by creating the following generator expression pipe-line. Using a Frame grouped by ""albumId"", zip together as pairs the Frame.index and a contiguous integer sequence via range(); chain all of those iterables, and then pass the resulting generator to Series.from_items(). (As much as possible, StaticFrame supports generators as arguments wherever an ordered sequence is expected.)
>>> from itertools import chain
>>> index_to_track = chain.from_iterable(zip(g.index, range(len(g))) for g in frame_go.iter_group('albumId'))
>>> frame_go['track'] = sf.Series.from_items(index_to_track) + 1
>>> frame_go.iloc[45:55]
<FrameGO>
<IndexGO> albumId id      title                url                  thumbnailUrl         track   <<U12>
<Index>
45        1       46      quidem maiores in... https://via.place... https://via.place... 46
46        1       47      et soluta est        https://via.place... https://via.place... 47
47        1       48      ut esse id           https://via.place... https://via.place... 48
48        1       49      quasi quae est mo... https://via.place... https://via.place... 49
49        1       50      et inventore quae... https://via.place... https://via.place... 50
50        2       51      non sunt voluptat... https://via.place... https://via.place... 1
51        2       52      eveniet pariatur ... https://via.place... https://via.place... 2
52        2       53      soluta et harum a... https://via.place... https://via.place... 3
53        2       54      ut ex quibusdam d... https://via.place... https://via.place... 4
54        2       55      voluptatem conseq... https://via.place... https://via.place... 5
<int64>   <int64> <int64> <<U86>               <<U38>               <<U38>               <int64>
Unlike with Pandas, StaticFrame Index objects always enforce uniqueness (there is no ""verify_integrity"" option: integrity is never optional). Thus, an index can never be set from non-unique data:
>>> frame_go.set_index('albumId')
Traceback (most recent call last):
KeyError: 'labels have non-unique values'
For a data set such as the one used in this example, a hierarchical index, by ""albumId"" and ""track"", is practical. StaticFrame implements hierarchical indices as IndexHierarchy objects. The Frame.set_index_hierarchy() method, given columns in a Frame, can be used to create a hierarchical index:
>>> frame_h = frame_go.set_index_hierarchy(['albumId', 'track'], drop=True)
>>> frame_h.head()
<FrameGO>
<IndexGO>                id      title                url                  thumbnailUrl         <<U12>
<IndexHierarchy>
1                1       1       accusamus beatae ... https://via.place... https://via.place...
1                2       2       reprehenderit est... https://via.place... https://via.place...
1                3       3       officia porro iur... https://via.place... https://via.place...
1                4       4       culpa odio esse r... https://via.place... https://via.place...
1                5       5       natus nisi omnis ... https://via.place... https://via.place...
<int64>          <int64> <int64> <<U86>               <<U38>               <<U38>
Hierarchical indices permit specifying selectors, per axis, at each hierarchical level. To distinguish hierarchical levels from axis arguments in a loc expression, the HLoc wrapper, exposing a __getitem__ interface, can be used. For example, we can select, from all albums, the second and fifth track, and then only the ""title"" and ""url"" columns.
>>> frame_h.loc[sf.HLoc[:, [2,5]], ['title', 'url']].head()
<FrameGO>
<IndexGO>                title                url                  <<U12>
<IndexHierarchy>
1                2       reprehenderit est... https://via.place...
1                5       natus nisi omnis ... https://via.place...
2                2       eveniet pariatur ... https://via.place...
2                5       voluptatem conseq... https://via.place...
3                2       eaque iste corpor... https://via.place...
<int64>          <int64> <<U86>               <<U38>
Just as a hierarchical selection can reside in a loc expression with an HLoc wrapper, an integer index selection can reside in a loc expression with an ILoc wrapper. For example, the previous row selection is combined with the selection of the last column:
>>> frame_h.loc[sf.HLoc[:, [2,5]], sf.ILoc[-1]].head()
<Series: thumbnailUrl>
<IndexHierarchy>
1                      2       https://via.place...
1                      5       https://via.place...
2                      2       https://via.place...
2                      5       https://via.place...
3                      2       https://via.place...
<int64>                <int64> <<U38>

Note
For more information on Index and IndexHierarchy, see Index Manipulation.

While StaticFrame offers many of the features of Pandas and similar data structures, exporting directly to NumPy arrays (via the .values attribute) or to Pandas is supported for functionality not found in StaticFrame or compatibility with other libraries. For example, a Frame can export to a Pandas DataFrame with Frame.to_pandas().
>>> df = frame_go.to_pandas()
",107
MicrosoftDocs/azure-docs.ko-kr,PowerShell,"


ms.openlocfilehash
ms.sourcegitcommit
ms.translationtype
ms.contentlocale
ms.lasthandoff
ms.locfileid




d4e9e26f2c6764cc179826266891d861514e23f6
3102f886aa962842303c8753fe8fa5324a52834a
HT
ko-KR
04/23/2019
60344092



Microsoft Azure 설명서
Microsoft Azure의 오픈 소스 설명서에 오신 것을 환영합니다. Microsoft Azure 설명서에 기여할 수 있는 방법을 이해하려면 이 README 파일을 살펴보세요.
시작하기
오픈 소스에 기여하면 단순히 업데이트를 제공하는 것을 넘어서 문제가 발생했을 때 알려줄 수도 있습니다. 지침 기여를 읽고 자세한 내용을 알아보세요.
필수 조건
기여하기로 결정하셨군요. 잘 생각하셨습니다! 설명서에 기여하려면 몇 가지 도구가 필요합니다.
설명서에 기여하려면 GitHub 계정이 필요합니다. 계정이 없으면 기여자 가이드에서 GitHub 계정 설정 지침을 따르세요.
다운로드
다음 도구를 설치합니다.

Git
Visual Studio Code
Visual Studio Code용 Docs Authoring Pack 확장

설치
기여자 가이드의 콘텐츠 제작 도구 설치에 나와 있는 지침을 따르세요.
License
모든 라이선싱 정보는 LICENSE, LICENSE-CODE 및 ThirdPartyNotices를 참조하세요.
준수 사항
이 프로젝트에는 Microsoft Open Source Code of Conduct(Microsoft 오픈 소스 준수 사항)이 적용됩니다.
자세한 내용은 Code of Conduct FAQ(준수 사항 FAQ)를 참조하거나 opencode@microsoft.com에 추가 질문 또는 의견을 알려주세요.
",13
jckantor/ND-Pyomo-Cookbook,Jupyter Notebook,"ND Pyomo Cookbook
Pyomo is a state-of-the-art Python package for modeling and solving optimization problems.
Using Pyomo, a user can embed an optimization model consisting of decision variables, constraints, and
an optimization objective within Python. Pyomo includes a rich set of features to enable modeling of complex systems.
This repository provides instructions on getting started with Pyomo, and a collection of Pyomo modeling notebooks that
have been developed for instructional purposes at Notre Dame. The notebooks were originally developed using the
Anaconda distribution of Python.The notebooks have been recently updated to open
directly on Google Colaboratory which requires only a browser window to run.
PyomoFest at Notre Dame was held June 5-7, 2018. This repository contains the agenda,
slides and exercises distributed during that event.
Table of Contents
Chapter 1. Installing a Pyomo/Python Development Environment

1.1 Running Pyomo on Google Colab
1.2 Running Pyomo on the Notre Dame CRC Cluster

Chapter 2. Linear Programming

2.1 Production Models with Linear Constraints
2.2 Linear Blending Problem
2.3 Design of a Cold Weather Fuel for a Camping Stove
2.4 Gasoline Blending
2.5 Model Predictive Control of a Double Integrator

Chapter 3. Assignment Problems

3.1 Transportation Networks

Chapter 4. Scheduling with Disjunctive Constraints

4.1 Machine Bottleneck
4.2 Job Shop Scheduling
4.3 Maintenance Planning
4.4 Scheduling Multipurpose Batch Processes using State-Task Networks

Chapter 5. Simulation

5.1 Response of a First Order System to Step and Square Wave Inputs
5.2 Exothermic CSTR
5.3 Transient Heat Conduction in Various Geometries

Chapter 6. Differential-Algebraic Equations

6.1 Unconstrained Scalar Optimization
6.2 Maximizing Concentration of an Intermediate in a Batch Reactor
6.3 Path Planning for a Simple Car

Chapter 7. Parameter Estimation

7.1 Parameter Estimation

Chapter 8. Financial Applications

8.1 Binomial Model for Pricing Options
8.2 Historical Stock Data
8.3 Charting Stock Data
8.4 MAD Portfolio Optimization
8.5 Real Options

",32
ascent12/drm_info,C,"Small utility to dump info about DRM devices.
Requires libdrm.
Build with
meson build
cd build
ninja

",9
rinsuki/sea,TypeScript,"Sea 🌊
requires:

Node.js 10.x
Postgres >= 10.x

",5
ascent12/drm_info,C,"Small utility to dump info about DRM devices.
Requires libdrm.
Build with
meson build
cd build
ninja

",9
rinsuki/sea,TypeScript,"Sea 🌊
requires:

Node.js 10.x
Postgres >= 10.x

",5
BinaryParadise/iOSGuide,Ruby,"iOS入门指南
iOS重要知识点介绍、演示
在线阅读：https://joenggaa.gitbook.io/objc
Demo目录

Examples/*

",2
elastic/elasticsearch,Java,"Elasticsearch
A Distributed RESTful Search Engine
https://www.elastic.co/products/elasticsearch
Elasticsearch is a distributed RESTful search engine built for the cloud. Features include:

Distributed and Highly Available Search Engine.
	
Each index is fully sharded with a configurable number of shards.
Each shard can have one or more replicas.
Read / Search operations performed on any of the replica shards.

Multi Tenant.
	
Support for more than one index.
Index level configuration (number of shards, index storage, …).

Various set of APIs
	
HTTP RESTful API
Native Java API.
All APIs perform automatic node operation rerouting.

Document oriented
	
No need for upfront schema definition.
Schema can be defined for customization of the indexing process.

Reliable, Asynchronous Write Behind for long term persistency.
(Near) Real Time Search.
Built on top of Lucene
	
Each shard is a fully functional Lucene index
All the power of Lucene easily exposed through simple configuration / plugins.

Per operation consistency
	
Single document level operations are atomic, consistent, isolated and durable.


Getting Started
First of all, DON’T PANIC. It will take 5 minutes to get the gist of what Elasticsearch is all about.
Requirements
You need to have a recent version of Java installed. See the Setup page for more information.
Installation

Download and unzip the Elasticsearch official distribution.
Run bin/elasticsearch on unix, or bin\elasticsearch.bat on windows.
Run curl -X GET http://localhost:9200/.
Start more servers …

Indexing
Let’s try and index some twitter like information. First, let’s index some tweets (the twitter index will be created automatically):
curl -XPUT 'http://localhost:9200/twitter/_doc/1?pretty' -H 'Content-Type: application/json' -d '
{
    ""user"": ""kimchy"",
    ""post_date"": ""2009-11-15T13:12:00"",
    ""message"": ""Trying out Elasticsearch, so far so good?""
}'

curl -XPUT 'http://localhost:9200/twitter/_doc/2?pretty' -H 'Content-Type: application/json' -d '
{
    ""user"": ""kimchy"",
    ""post_date"": ""2009-11-15T14:12:12"",
    ""message"": ""Another tweet, will it be indexed?""
}'

curl -XPUT 'http://localhost:9200/twitter/_doc/3?pretty' -H 'Content-Type: application/json' -d '
{
    ""user"": ""elastic"",
    ""post_date"": ""2010-01-15T01:46:38"",
    ""message"": ""Building the site, should be kewl""
}'

Now, let’s see if the information was added by GETting it:
curl -XGET 'http://localhost:9200/twitter/_doc/1?pretty=true'
curl -XGET 'http://localhost:9200/twitter/_doc/2?pretty=true'
curl -XGET 'http://localhost:9200/twitter/_doc/3?pretty=true'

Searching
Mmm search…, shouldn’t it be elastic?
Let’s find all the tweets that kimchy posted:
curl -XGET 'http://localhost:9200/twitter/_search?q=user:kimchy&pretty=true'

We can also use the JSON query language Elasticsearch provides instead of a query string:
curl -XGET 'http://localhost:9200/twitter/_search?pretty=true' -H 'Content-Type: application/json' -d '
{
    ""query"" : {
        ""match"" : { ""user"": ""kimchy"" }
    }
}'

Just for kicks, let’s get all the documents stored (we should see the tweet from elastic as well):
curl -XGET 'http://localhost:9200/twitter/_search?pretty=true' -H 'Content-Type: application/json' -d '
{
    ""query"" : {
        ""match_all"" : {}
    }
}'

We can also do range search (the post_date was automatically identified as date)
curl -XGET 'http://localhost:9200/twitter/_search?pretty=true' -H 'Content-Type: application/json' -d '
{
    ""query"" : {
        ""range"" : {
            ""post_date"" : { ""from"" : ""2009-11-15T13:00:00"", ""to"" : ""2009-11-15T14:00:00"" }
        }
    }
}'

There are many more options to perform search, after all, it’s a search product no? All the familiar Lucene queries are available through the JSON query language, or through the query parser.
Multi Tenant – Indices and Types
Man, that twitter index might get big (in this case, index size == valuation). Let’s see if we can structure our twitter system a bit differently in order to support such large amounts of data.
Elasticsearch supports multiple indices. In the previous example we used an index called twitter that stored tweets for every user.
Another way to define our simple twitter system is to have a different index per user (note, though that each index has an overhead). Here is the indexing curl’s in this case:
curl -XPUT 'http://localhost:9200/kimchy/_doc/1?pretty' -H 'Content-Type: application/json' -d '
{
    ""user"": ""kimchy"",
    ""post_date"": ""2009-11-15T13:12:00"",
    ""message"": ""Trying out Elasticsearch, so far so good?""
}'

curl -XPUT 'http://localhost:9200/kimchy/_doc/2?pretty' -H 'Content-Type: application/json' -d '
{
    ""user"": ""kimchy"",
    ""post_date"": ""2009-11-15T14:12:12"",
    ""message"": ""Another tweet, will it be indexed?""
}'

The above will index information into the kimchy index. Each user will get their own special index.
Complete control on the index level is allowed. As an example, in the above case, we would want to change from the default 5 shards with 1 replica per index, to only 1 shard with 1 replica per index (== per twitter user). Here is how this can be done (the configuration can be in yaml as well):
curl -XPUT http://localhost:9200/another_user?pretty -H 'Content-Type: application/json' -d '
{
    ""index"" : {
        ""number_of_shards"" : 1,
        ""number_of_replicas"" : 1
    }
}'

Search (and similar operations) are multi index aware. This means that we can easily search on more than one
index (twitter user), for example:
curl -XGET 'http://localhost:9200/kimchy,another_user/_search?pretty=true' -H 'Content-Type: application/json' -d '
{
    ""query"" : {
        ""match_all"" : {}
    }
}'

Or on all the indices:
curl -XGET 'http://localhost:9200/_search?pretty=true' -H 'Content-Type: application/json' -d '
{
    ""query"" : {
        ""match_all"" : {}
    }
}'

{One liner teaser}: And the cool part about that? You can easily search on multiple twitter users (indices), with different boost levels per user (index), making social search so much simpler (results from my friends rank higher than results from friends of my friends).
Distributed, Highly Available
Let’s face it, things will fail….
Elasticsearch is a highly available and distributed search engine. Each index is broken down into shards, and each shard can have one or more replicas. By default, an index is created with 5 shards and 1 replica per shard (5/1). There are many topologies that can be used, including 1/10 (improve search performance), or 20/1 (improve indexing performance, with search executed in a map reduce fashion across shards).
In order to play with the distributed nature of Elasticsearch, simply bring more nodes up and shut down nodes. The system will continue to serve requests (make sure you use the correct http port) with the latest data indexed.
Where to go from here?
We have just covered a very small portion of what Elasticsearch is all about. For more information, please refer to the elastic.co website. General questions can be asked on the Elastic Discourse forum or on IRC on Freenode at #elasticsearch. The Elasticsearch GitHub repository is reserved for bug reports and feature requests only.
Building from Source
Elasticsearch uses Gradle for its build system.
In order to create a distribution, simply run the ./gradlew assemble command in the cloned directory.
The distribution for each project will be created under the build/distributions directory in that project.
See the TESTING file for more information about running the Elasticsearch test suite.
Upgrading from older Elasticsearch versions
In order to ensure a smooth upgrade process from earlier versions of Elasticsearch, please see our upgrade documentation for more details on the upgrade process.",41076
zeit/next.js,JavaScript,"





Visit nextjs.org/learn to get started with Next.js.

The below readme is the documentation for the canary (prerelease) branch. To view the documentation for the latest stable Next.js version visit nextjs.org/docs


How to use

Setup
Automatic code splitting
CSS

Built-in CSS support
CSS-in-JS
Importing CSS / Sass / Less / Stylus files


Static file serving (e.g.: images)
Populating <head>
Fetching data and component lifecycle
Routing

With <Link>

With URL object
Replace instead of push url
Using a component that supports onClick
Forcing the Link to expose href to its child
Disabling the scroll changes to top on page


Imperatively
Intercepting popstate

With URL object
Router Events
Shallow Routing


Using a Higher Order Component


Prefetching Pages

With <Link>
Imperatively


Custom server and routing

Disabling file-system routing
Dynamic assetPrefix


Dynamic Import

Basic Usage (Also does SSR)
With Named Exports
With Custom Loading Component
With No SSR
With Multiple Modules At Once


Custom <App>
Custom <Document>

Customizing renderPage


Custom error handling
Reusing the built-in error page
Custom configuration

Setting a custom build directory
Disabling etag generation
Configuring the onDemandEntries
Configuring extensions looked for when resolving pages in pages
Configuring the build ID
Configuring Next process script


Customizing webpack config
Customizing babel config
Exposing configuration to the server / client side
Starting the server on alternative hostname
CDN support with Asset Prefix


Production deployment

Serverless deployment

One Level Lower
Summary




Browser support
Typescript

Exported types


AMP Support
Static HTML export

Usage
Copying custom files
Limitation


Multi Zones

How to define a zone
How to merge them


Recipes
FAQ
Contributing
Authors

How to use
Setup
Install it:
npm install --save next react react-dom
and add a script to your package.json like this:
{
  ""scripts"": {
    ""dev"": ""next"",
    ""build"": ""next build"",
    ""start"": ""next start""
  }
}
After that, the file-system is the main API. Every .js file becomes a route that gets automatically processed and rendered.
Populate ./pages/index.js inside your project:
function Home() {
  return <div>Welcome to Next.js!</div>
}

export default Home
and then just run npm run dev and go to http://localhost:3000. To use another port, you can run npm run dev -- -p <your port here>.
So far, we get:

Automatic transpilation and bundling (with webpack and babel)
Hot code reloading
Server rendering and indexing of ./pages
Static file serving. ./static/ is mapped to /static/ (given you create a ./static/ directory inside your project)

Automatic code splitting
Every import you declare gets bundled and served with each page. That means pages never load unnecessary code!
import cowsay from 'cowsay-browser'

function CowsayHi() {
  return <pre>{cowsay.say({ text: 'hi there!' })}</pre>
}

export default CowsayHi
CSS
Built-in CSS support

Examples

Basic css


We bundle styled-jsx to provide support for isolated scoped CSS. The aim is to support ""shadow CSS"" similar to Web Components, which unfortunately do not support server-rendering and are JS-only.
function HelloWorld() {
  return (
    <div>
      Hello world
      <p>scoped!</p>
      <style jsx>{`
        p {
          color: blue;
        }
        div {
          background: red;
        }
        @media (max-width: 600px) {
          div {
            background: blue;
          }
        }
      `}</style>
      <style global jsx>{`
        body {
          background: black;
        }
      `}</style>
    </div>
  )
}

export default HelloWorld
Please see the styled-jsx documentation for more examples.
CSS-in-JS


Examples


Styled components
Styletron
Glamor
Cxs
Aphrodite
Fela


It's possible to use any existing CSS-in-JS solution. The simplest one is inline styles:
function HiThere() {
  return <p style={{ color: 'red' }}>hi there</p>
}

export default HiThere
To use more sophisticated CSS-in-JS solutions, you typically have to implement style flushing for server-side rendering. We enable this by allowing you to define your own custom <Document> component that wraps each page.
Importing CSS / Sass / Less / Stylus files
To support importing .css, .scss, .less or .styl files you can use these modules, which configure sensible defaults for server rendered applications.

@zeit/next-css
@zeit/next-sass
@zeit/next-less
@zeit/next-stylus

Static file serving (e.g.: images)

Examples

Public file serving


Create a folder called static in your project root directory. From your code you can then reference those files with /static/ URLs:
function MyImage() {
  return <img src=""/static/my-image.png"" alt=""my image"" />
}

export default MyImage
To serve static files from the root directory you can add a folder called public and reference those files from the root, e.g: /robots.txt
Note: Don't name the static or public directory anything else. The names can't be changed and are the only directories that Next.js uses for serving static assets
Populating <head>

Examples

Head elements
Layout component


We expose a built-in component for appending elements to the <head> of the page.
import Head from 'next/head'

function IndexPage() {
  return (
    <div>
      <Head>
        <title>My page title</title>
        <meta name=""viewport"" content=""initial-scale=1.0, width=device-width"" />
      </Head>
      <p>Hello world!</p>
    </div>
  )
}

export default IndexPage
To avoid duplicate tags in your <head> you can use the key property, which will make sure the tag is only rendered once:
import Head from 'next/head'

function IndexPage() {
  return (
    <div>
      <Head>
        <title>My page title</title>
        <meta
          name=""viewport""
          content=""initial-scale=1.0, width=device-width""
          key=""viewport""
        />
      </Head>
      <Head>
        <meta
          name=""viewport""
          content=""initial-scale=1.2, width=device-width""
          key=""viewport""
        />
      </Head>
      <p>Hello world!</p>
    </div>
  )
}

export default IndexPage
In this case only the second <meta name=""viewport"" /> is rendered.
Note: The contents of <head> get cleared upon unmounting the component, so make sure each page completely defines what it needs in <head>, without making assumptions about what other pages added.
Note: <title> and <meta> elements need to be contained as direct children of the <Head> element, or wrapped into maximum one level of <React.Fragment>, otherwise the metatags won't be correctly picked up on clientside navigation.
Fetching data and component lifecycle

Examples

Data fetch


When you need state, lifecycle hooks or initial data population you can export a React.Component (instead of a stateless function, like shown above):
import React from 'react'

class HelloUA extends React.Component {
  static async getInitialProps({ req }) {
    const userAgent = req ? req.headers['user-agent'] : navigator.userAgent
    return { userAgent }
  }

  render() {
    return <div>Hello World {this.props.userAgent}</div>
  }
}

export default HelloUA
Notice that to load data when the page loads, we use getInitialProps which is an async static method. It can asynchronously fetch anything that resolves to a JavaScript plain Object, which populates props.
Data returned from getInitialProps is serialized when server rendering, similar to a JSON.stringify. Make sure the returned object from getInitialProps is a plain Object and not using Date, Map or Set.
For the initial page load, getInitialProps will execute on the server only. getInitialProps will only be executed on the client when navigating to a different route via the Link component or using the routing APIs.
Note: getInitialProps can not be used in children components. Only in pages.


If you are using some server only modules inside getInitialProps, make sure to import them properly.
Otherwise, it'll slow down your app.


You can also define the getInitialProps lifecycle method for stateless components:
function Page({ stars }) {
  return <div>Next stars: {stars}</div>
}

Page.getInitialProps = async ({ req }) => {
  const res = await fetch('https://api.github.com/repos/zeit/next.js')
  const json = await res.json()
  return { stars: json.stargazers_count }
}

export default Page
getInitialProps receives a context object with the following properties:

pathname - path section of URL
query - query string section of URL parsed as an object
asPath - String of the actual path (including the query) shows in the browser
req - HTTP request object (server only)
res - HTTP response object (server only)
err - Error object if any error is encountered during the rendering

Routing
Next.js does not ship a routes manifest with every possible route in the application, so the current page is not aware of any other pages on the client side. All subsequent routes get lazy-loaded, for scalability sake.
With <Link>

Examples

Hello World


Client-side transitions between routes can be enabled via a <Link> component.
Basic Example
Consider these two pages:
// pages/index.js
import Link from 'next/link'

function Home() {
  return (
    <div>
      Click{' '}
      <Link href=""/about"">
        <a>here</a>
      </Link>{' '}
      to read more
    </div>
  )
}

export default Home
// pages/about.js
function About() {
  return <p>Welcome to About!</p>
}

export default About
Custom routes (using props from URL)
<Link> component has two main props:

href: the path inside pages directory + query string.
as: the path that will be rendered in the browser URL bar.

Example:


Consider you have the URL /post/:slug.


You created the pages/post.js
class Post extends React.Component {
  static async getInitialProps({ query }) {
    console.log('SLUG', query.slug)
    return {}
  }
  render() {
    return <h1>My blog post</h1>
  }
}

export default Post


You add the route to express (or any other server) on server.js file (this is only for SSR). This will route the url /post/:slug to pages/post.js and provide slug as part of query in getInitialProps.
server.get('/post/:slug', (req, res) => {
  return app.render(req, res, '/post', { slug: req.params.slug })
})


For client side routing, use next/link:
<Link href=""/post?slug=something"" as=""/post/something"">


Note: use <Link prefetch> for maximum performance, to link and prefetch in the background at the same time
Client-side routing behaves exactly like the browser:

The component is fetched
If it defines getInitialProps, data is fetched. If an error occurs, _error.js is rendered
After 1 and 2 complete, pushState is performed and the new component is rendered

To inject the pathname, query or asPath in your component, you can use withRouter.
With URL object

Examples

With URL Object Routing


The component <Link> can also receive a URL object and it will automatically format it to create the URL string.
// pages/index.js
import Link from 'next/link'

function Home() {
  return (
    <div>
      Click{' '}
      <Link href={{ pathname: '/about', query: { name: 'Zeit' } }}>
        <a>here</a>
      </Link>{' '}
      to read more
    </div>
  )
}

export default Home
That will generate the URL string /about?name=Zeit, you can use every property as defined in the Node.js URL module documentation.
Replace instead of push url
The default behaviour for the <Link> component is to push a new url into the stack. You can use the replace prop to prevent adding a new entry.
// pages/index.js
import Link from 'next/link'

function Home() {
  return (
    <div>
      Click{' '}
      <Link href=""/about"" replace>
        <a>here</a>
      </Link>{' '}
      to read more
    </div>
  )
}

export default Home
Using a component that supports onClick
<Link> supports any component that supports the onClick event. In case you don't provide an <a> tag, it will only add the onClick event handler and won't pass the href property.
// pages/index.js
import Link from 'next/link'

function Home() {
  return (
    <div>
      Click{' '}
      <Link href=""/about"">
        <img src=""/static/image.png"" alt=""image"" />
      </Link>
    </div>
  )
}

export default Home
Forcing the Link to expose href to its child
If child is an <a> tag and doesn't have a href attribute we specify it so that the repetition is not needed by the user. However, sometimes, you’ll want to pass an <a> tag inside of a wrapper and the Link won’t recognize it as a hyperlink, and, consequently, won’t transfer its href to the child. In cases like that, you should define a boolean passHref property to the Link, forcing it to expose its href property to the child.
Please note: using a tag other than a and failing to pass passHref may result in links that appear to navigate correctly, but, when being crawled by search engines, will not be recognized as links (owing to the lack of href attribute). This may result in negative effects on your sites SEO.
import Link from 'next/link'
import Unexpected_A from 'third-library'

function NavLink({ href, name }) {
  return (
    <Link href={href} passHref>
      <Unexpected_A>{name}</Unexpected_A>
    </Link>
  )
}

export default NavLink
Disabling the scroll changes to top on page
The default behaviour of <Link> is to scroll to the top of the page. When there is a hash defined it will scroll to the specific id, just like a normal <a> tag. To prevent scrolling to the top / hash scroll={false} can be added to <Link>:
<Link scroll={false} href=""/?counter=10""><a>Disables scrolling</a></Link>
<Link href=""/?counter=10""><a>Changes with scrolling to top</a></Link>
Imperatively

Examples

Basic routing
With a page loading indicator


You can also do client-side page transitions using the next/router
import Router from 'next/router'

function ReadMore() {
  return (
    <div>
      Click <span onClick={() => Router.push('/about')}>here</span> to read more
    </div>
  )
}

export default ReadMore
Intercepting popstate
In some cases (for example, if using a custom router), you may wish
to listen to popstate and react before the router acts on it.
For example, you could use this to manipulate the request, or force an SSR refresh.
import Router from 'next/router'

Router.beforePopState(({ url, as, options }) => {
  // I only want to allow these two routes!
  if (as !== '/' || as !== '/other') {
    // Have SSR render bad routes as a 404.
    window.location.href = as
    return false
  }

  return true
})
If the function you pass into beforePopState returns false, Router will not handle popstate;
you'll be responsible for handling it, in that case.
See Disabling File-System Routing.
Above Router object comes with the following API:

route - String of the current route
pathname - String of the current path excluding the query string
query - Object with the parsed query string. Defaults to {}
asPath - String of the actual path (including the query) shows in the browser
push(url, as=url) - performs a pushState call with the given url
replace(url, as=url) - performs a replaceState call with the given url
beforePopState(cb=function) - intercept popstate before router processes the event.

The second as parameter for push and replace is an optional decoration of the URL. Useful if you configured custom routes on the server.
With URL object
You can use a URL object the same way you use it in a <Link> component to push and replace a URL.
import Router from 'next/router'

const handler = () => {
  Router.push({
    pathname: '/about',
    query: { name: 'Zeit' }
  })
}

function ReadMore() {
  return (
    <div>
      Click <span onClick={handler}>here</span> to read more
    </div>
  )
}

export default ReadMore
This uses the same exact parameters as in the <Link> component.
Router Events
You can also listen to different events happening inside the Router.
Here's a list of supported events:

routeChangeStart(url) - Fires when a route starts to change
routeChangeComplete(url) - Fires when a route changed completely
routeChangeError(err, url) - Fires when there's an error when changing routes
beforeHistoryChange(url) - Fires just before changing the browser's history
hashChangeStart(url) - Fires when the hash will change but not the page
hashChangeComplete(url) - Fires when the hash has changed but not the page


Here url is the URL shown in the browser. If you call Router.push(url, as) (or similar), then the value of url will be as.

Here's how to properly listen to the router event routeChangeStart:
const handleRouteChange = url => {
  console.log('App is changing to: ', url)
}

Router.events.on('routeChangeStart', handleRouteChange)
If you no longer want to listen to that event, you can unsubscribe with the off method:
Router.events.off('routeChangeStart', handleRouteChange)
If a route load is cancelled (for example by clicking two links rapidly in succession), routeChangeError will fire. The passed err will contain a cancelled property set to true.
Router.events.on('routeChangeError', (err, url) => {
  if (err.cancelled) {
    console.log(`Route to ${url} was cancelled!`)
  }
})
Shallow Routing

Examples

Shallow Routing


Shallow routing allows you to change the URL without running getInitialProps. You'll receive the updated pathname and the query via the router prop (injected using withRouter), without losing state.
You can do this by invoking either Router.push or Router.replace with the shallow: true option. Here's an example:
// Current URL is ""/""
const href = '/?counter=10'
const as = href
Router.push(href, as, { shallow: true })
Now, the URL is updated to /?counter=10. You can see the updated URL with this.props.router.query inside the Component (make sure you are using withRouter around your Component to inject the router prop).
You can watch for URL changes via componentDidUpdate hook as shown below:
componentDidUpdate(prevProps) {
  const { pathname, query } = this.props.router
  // verify props have changed to avoid an infinite loop
  if (query.id !== prevProps.router.query.id) {
    // fetch data based on the new query
  }
}

NOTES:
Shallow routing works only for same page URL changes. For an example, let's assume we have another page called about, and you run this:
Router.push('/?counter=10', '/about?counter=10', { shallow: true })
Since that's a new page, it'll unload the current page, load the new one and call getInitialProps even though we asked to do shallow routing.

Using a Higher Order Component

Examples

Using the `withRouter` utility


If you want to access the router object inside any component in your app, you can use the withRouter Higher-Order Component. Here's how to use it:
import { withRouter } from 'next/router'

function ActiveLink({ children, router, href }) {
  const style = {
    marginRight: 10,
    color: router.pathname === href ? 'red' : 'black'
  }

  const handleClick = e => {
    e.preventDefault()
    router.push(href)
  }

  return (
    <a href={href} onClick={handleClick} style={style}>
      {children}
    </a>
  )
}

export default withRouter(ActiveLink)
The above router object comes with an API similar to next/router.
Prefetching Pages
⚠️ This is a production only feature ⚠️

Examples

Prefetching


Next.js has an API which allows you to prefetch pages.
Since Next.js server-renders your pages, this allows all the future interaction paths of your app to be instant. Effectively Next.js gives you the great initial download performance of a website, with the ahead-of-time download capabilities of an app. Read more.

With prefetching Next.js only downloads JS code. When the page is getting rendered, you may need to wait for the data.


<link rel=""preload""> is used for prefetching. Sometimes browsers will show a warning if the resource is not used within 3 seconds, these warnings can be ignored as per https://github.com/zeit/next.js/issues/6517#issuecomment-469063892

With <Link>
You can add prefetch prop to any <Link> and Next.js will prefetch those pages in the background.
import Link from 'next/link'

function Header() {
  return (
    <nav>
      <ul>
        <li>
          <Link prefetch href=""/"">
            <a>Home</a>
          </Link>
        </li>
        <li>
          <Link prefetch href=""/about"">
            <a>About</a>
          </Link>
        </li>
        <li>
          <Link prefetch href=""/contact"">
            <a>Contact</a>
          </Link>
        </li>
      </ul>
    </nav>
  )
}

export default Header
Imperatively
Most prefetching needs are addressed by <Link />, but we also expose an imperative API for advanced usage:
import { withRouter } from 'next/router'

function MyLink({ router }) {
  return (
    <div>
      <a onClick={() => setTimeout(() => router.push('/dynamic'), 100)}>
        A route transition will happen after 100ms
      </a>
      {// but we can prefetch it!
      router.prefetch('/dynamic')}
    </div>
  )
}

export default withRouter(MyLink)
The router instance should be only used inside the client side of your app though. In order to prevent any error regarding this subject, when rendering the Router on the server side, use the imperatively prefetch method in the componentDidMount() lifecycle method.
import React from 'react'
import { withRouter } from 'next/router'

class MyLink extends React.Component {
  componentDidMount() {
    const { router } = this.props
    router.prefetch('/dynamic')
  }

  render() {
    const { router } = this.props

    return (
      <div>
        <a onClick={() => setTimeout(() => router.push('/dynamic'), 100)}>
          A route transition will happen after 100ms
        </a>
      </div>
    )
  }
}

export default withRouter(MyLink)
Custom server and routing

Examples

Basic custom server
Express integration
Hapi integration
Koa integration
Parameterized routing
SSR caching


Typically you start your next server with next start. It's possible, however, to start a server 100% programmatically in order to customize routes, use route patterns, etc.
When using a custom server with a server file, for example called server.js, make sure you update the scripts key in package.json to:
{
  ""scripts"": {
    ""dev"": ""node server.js"",
    ""build"": ""next build"",
    ""start"": ""NODE_ENV=production node server.js""
  }
}
This example makes /a resolve to ./pages/b, and /b resolve to ./pages/a:
// This file doesn't go through babel or webpack transformation.
// Make sure the syntax and sources this file requires are compatible with the current node version you are running
// See https://github.com/zeit/next.js/issues/1245 for discussions on Universal Webpack or universal Babel
const { createServer } = require('http')
const { parse } = require('url')
const next = require('next')

const dev = process.env.NODE_ENV !== 'production'
const app = next({ dev })
const handle = app.getRequestHandler()

app.prepare().then(() => {
  createServer((req, res) => {
    // Be sure to pass `true` as the second argument to `url.parse`.
    // This tells it to parse the query portion of the URL.
    const parsedUrl = parse(req.url, true)
    const { pathname, query } = parsedUrl

    if (pathname === '/a') {
      app.render(req, res, '/b', query)
    } else if (pathname === '/b') {
      app.render(req, res, '/a', query)
    } else {
      handle(req, res, parsedUrl)
    }
  }).listen(3000, err => {
    if (err) throw err
    console.log('> Ready on http://localhost:3000')
  })
})
The next API is as follows:

next(opts: object)

Supported options:

dev (bool) whether to launch Next.js in dev mode - default false
dir (string) where the Next project is located - default '.'
quiet (bool) Hide error messages containing server information - default false
conf (object) the same object you would use in next.config.js - default {}

Then, change your start script to NODE_ENV=production node server.js.
Disabling file-system routing
By default, Next will serve each file in /pages under a pathname matching the filename (eg, /pages/some-file.js is served at site.com/some-file.
If your project uses custom routing, this behavior may result in the same content being served from multiple paths, which can present problems with SEO and UX.
To disable this behavior & prevent routing based on files in /pages, simply set the following option in your next.config.js:
// next.config.js
module.exports = {
  useFileSystemPublicRoutes: false
}
Note that useFileSystemPublicRoutes simply disables filename routes from SSR; client-side routing
may still access those paths. If using this option, you should guard against navigation to routes
you do not want programmatically.
You may also wish to configure the client-side Router to disallow client-side redirects to filename
routes; please refer to Intercepting popstate.
Dynamic assetPrefix
Sometimes we need to set the assetPrefix dynamically. This is useful when changing the assetPrefix based on incoming requests.
For that, we can use app.setAssetPrefix.
Here's an example usage of it:
const next = require('next')
const http = require('http')

const dev = process.env.NODE_ENV !== 'production'
const app = next({ dev })
const handleNextRequests = app.getRequestHandler()

app.prepare().then(() => {
  const server = new http.Server((req, res) => {
    // Add assetPrefix support based on the hostname
    if (req.headers.host === 'my-app.com') {
      app.setAssetPrefix('http://cdn.com/myapp')
    } else {
      app.setAssetPrefix('')
    }

    handleNextRequests(req, res)
  })

  server.listen(port, err => {
    if (err) {
      throw err
    }

    console.log(`> Ready on http://localhost:${port}`)
  })
})
Dynamic Import

Examples

With Dynamic Import


Next.js supports TC39 dynamic import proposal for JavaScript.
With that, you could import JavaScript modules (inc. React Components) dynamically and work with them.
You can think dynamic imports as another way to split your code into manageable chunks.
Since Next.js supports dynamic imports with SSR, you could do amazing things with it.
Here are a few ways to use dynamic imports.
Basic Usage (Also does SSR)
import dynamic from 'next/dynamic'

const DynamicComponent = dynamic(() => import('../components/hello'))

function Home() {
  return (
    <div>
      <Header />
      <DynamicComponent />
      <p>HOME PAGE is here!</p>
    </div>
  )
}

export default Home
With named exports
// components/hello.js
export function Hello() {
  return <p>Hello!</p>
}
import dynamic from 'next/dynamic'

const DynamicComponent = dynamic(() =>
  import('../components/hello').then(mod => mod.Hello)
)

function Home() {
  return (
    <div>
      <Header />
      <DynamicComponent />
      <p>HOME PAGE is here!</p>
    </div>
  )
}

export default Home
With Custom Loading Component
import dynamic from 'next/dynamic'

const DynamicComponentWithCustomLoading = dynamic(
  () => import('../components/hello2'),
  {
    loading: () => <p>...</p>
  }
)

function Home() {
  return (
    <div>
      <Header />
      <DynamicComponentWithCustomLoading />
      <p>HOME PAGE is here!</p>
    </div>
  )
}

export default Home
With No SSR
import dynamic from 'next/dynamic'

const DynamicComponentWithNoSSR = dynamic(
  () => import('../components/hello3'),
  {
    ssr: false
  }
)

function Home() {
  return (
    <div>
      <Header />
      <DynamicComponentWithNoSSR />
      <p>HOME PAGE is here!</p>
    </div>
  )
}

export default Home
With Multiple Modules At Once
import dynamic from 'next/dynamic'

const HelloBundle = dynamic({
  modules: () => {
    const components = {
      Hello1: () => import('../components/hello1'),
      Hello2: () => import('../components/hello2')
    }

    return components
  },
  render: (props, { Hello1, Hello2 }) => (
    <div>
      <h1>{props.title}</h1>
      <Hello1 />
      <Hello2 />
    </div>
  )
})

function DynamicBundle() {
  return <HelloBundle title=""Dynamic Bundle"" />
}

export default DynamicBundle
Custom <App>

Examples

Using `_app.js` for layout
Using `_app.js` to override `componentDidCatch`


Next.js uses the App component to initialize pages. You can override it and control the page initialization. Which allows you to do amazing things like:

Persisting layout between page changes
Keeping state when navigating pages
Custom error handling using componentDidCatch
Inject additional data into pages (for example by processing GraphQL queries)

To override, create the ./pages/_app.js file and override the App class as shown below:
import React from 'react'
import App, { Container } from 'next/app'

class MyApp extends App {
  static async getInitialProps({ Component, ctx }) {
    let pageProps = {}

    if (Component.getInitialProps) {
      pageProps = await Component.getInitialProps(ctx)
    }

    return { pageProps }
  }

  render() {
    const { Component, pageProps } = this.props

    return (
      <Container>
        <Component {...pageProps} />
      </Container>
    )
  }
}

export default MyApp
Custom <Document>

Examples

Styled components custom document
Google AMP



Is rendered on the server side
Is used to change the initial server side rendered document markup
Commonly used to implement server side rendering for css-in-js libraries like styled-components or emotion. styled-jsx is included with Next.js by default.

Pages in Next.js skip the definition of the surrounding document's markup. For example, you never include <html>, <body>, etc. To override that default behavior, you must create a file at ./pages/_document.js, where you can extend the Document class:
// _document is only rendered on the server side and not on the client side
// Event handlers like onClick can't be added to this file

// ./pages/_document.js
import Document, { Html, Head, Main, NextScript } from 'next/document'

class MyDocument extends Document {
  static async getInitialProps(ctx) {
    const initialProps = await Document.getInitialProps(ctx)
    return { ...initialProps }
  }

  render() {
    return (
      <Html>
        <Head>
          <style>{`body { margin: 0 } /* custom! */`}</style>
        </Head>
        <body className=""custom_class"">
          <Main />
          <NextScript />
        </body>
      </Html>
    )
  }
}

export default MyDocument
All of <Head />, <Main /> and <NextScript /> are required for page to be properly rendered.
Note: React-components outside of <Main /> will not be initialised by the browser. Do not add application logic here. If you need shared components in all your pages (like a menu or a toolbar), take a look at the App component instead.
The ctx object is equivalent to the one received in all getInitialProps hooks, with one addition:

renderPage (Function) a callback that executes the actual React rendering logic (synchronously). It's useful to decorate this function in order to support server-rendering wrappers like Aphrodite's renderStatic

Customizing renderPage
🚧 It should be noted that the only reason you should be customizing renderPage is for usage with css-in-js libraries
that need to wrap the application to properly work with server-rendering. 🚧

It takes as argument an options object for further customization

import Document from 'next/document'

class MyDocument extends Document {
  static async getInitialProps(ctx) {
    const originalRenderPage = ctx.renderPage

    ctx.renderPage = () =>
      originalRenderPage({
        // useful for wrapping the whole react tree
        enhanceApp: App => App,
        // useful for wrapping in a per-page basis
        enhanceComponent: Component => Component
      })

    // Run the parent `getInitialProps` using `ctx` that now includes our custom `renderPage`
    const initialProps = await Document.getInitialProps(ctx)

    return initialProps
  }
}

export default MyDocument
Custom error handling
404 or 500 errors are handled both client and server side by a default component error.js. If you wish to override it, define a _error.js in the pages folder:
⚠️ The pages/_error.js component is only used in production. In development you get an error with call stack to know where the error originated from. ⚠️
import React from 'react'

class Error extends React.Component {
  static getInitialProps({ res, err }) {
    const statusCode = res ? res.statusCode : err ? err.statusCode : null
    return { statusCode }
  }

  render() {
    return (
      <p>
        {this.props.statusCode
          ? `An error ${this.props.statusCode} occurred on server`
          : 'An error occurred on client'}
      </p>
    )
  }
}

export default Error
Reusing the built-in error page
If you want to render the built-in error page you can by using next/error:
import React from 'react'
import Error from 'next/error'
import fetch from 'isomorphic-unfetch'

class Page extends React.Component {
  static async getInitialProps() {
    const res = await fetch('https://api.github.com/repos/zeit/next.js')
    const errorCode = res.statusCode > 200 ? res.statusCode : false
    const json = await res.json()

    return { errorCode, stars: json.stargazers_count }
  }

  render() {
    if (this.props.errorCode) {
      return <Error statusCode={this.props.errorCode} />
    }

    return <div>Next stars: {this.props.stars}</div>
  }
}

export default Page

If you have created a custom error page you have to import your own _error component from ./_error instead of next/error

Custom configuration
For custom advanced behavior of Next.js, you can create a next.config.js in the root of your project directory (next to pages/ and package.json).
Note: next.config.js is a regular Node.js module, not a JSON file. It gets used by the Next server and build phases, and not included in the browser build.
// next.config.js
module.exports = {
  /* config options here */
}
Or use a function:
module.exports = (phase, { defaultConfig }) => {
  return {
    /* config options here */
  }
}
phase is the current context in which the configuration is loaded. You can see all phases here: constants
Phases can be imported from next/constants:
const { PHASE_DEVELOPMENT_SERVER } = require('next/constants')
module.exports = (phase, { defaultConfig }) => {
  if (phase === PHASE_DEVELOPMENT_SERVER) {
    return {
      /* development only config options here */
    }
  }

  return {
    /* config options for all phases except development here */
  }
}
Setting a custom build directory
You can specify a name to use for a custom build directory. For example, the following config will create a build folder instead of a .next folder. If no configuration is specified then next will create a .next folder.
// next.config.js
module.exports = {
  distDir: 'build'
}
Disabling etag generation
You can disable etag generation for HTML pages depending on your cache strategy. If no configuration is specified then Next will generate etags for every page.
// next.config.js
module.exports = {
  generateEtags: false
}
Configuring the onDemandEntries
Next exposes some options that give you some control over how the server will dispose or keep in memories pages built:
module.exports = {
  onDemandEntries: {
    // period (in ms) where the server will keep pages in the buffer
    maxInactiveAge: 25 * 1000,
    // number of pages that should be kept simultaneously without being disposed
    pagesBufferLength: 2
  }
}
This is development-only feature. If you want to cache SSR pages in production, please see SSR-caching example.
Configuring extensions looked for when resolving pages in pages
Aimed at modules like @next/mdx, that add support for pages ending with .mdx. pageExtensions allows you to configure the extensions looked for in the pages directory when resolving pages.
// next.config.js
module.exports = {
  pageExtensions: ['mdx', 'jsx', 'js']
}
Configuring the build ID
Next.js uses a constant generated at build time to identify which version of your application is being served. This can cause problems in multi-server deployments when next build is ran on every server. In order to keep a static build id between builds you can provide the generateBuildId function:
// next.config.js
module.exports = {
  generateBuildId: async () => {
    // For example get the latest git commit hash here
    return 'my-build-id'
  }
}
To fall back to the default of generating a unique id return null from the function:
module.exports = {
  generateBuildId: async () => {
    // When process.env.YOUR_BUILD_ID is undefined we fall back to the default
    if (process.env.YOUR_BUILD_ID) {
      return process.env.YOUR_BUILD_ID
    }

    return null
  }
}
Configuring next process script
You can pass any node arguments to next CLI command.
NODE_OPTIONS=""--throw-deprecation"" next
NODE_OPTIONS=""-r esm"" next
NODE_OPTIONS=""--inspect"" next
Customizing webpack config

Examples

Custom webpack bundle analyzer


Some commonly asked for features are available as modules:

@zeit/next-css
@zeit/next-sass
@zeit/next-less
@zeit/next-preact
@next/mdx


Warning: The webpack function is executed twice, once for the server and once for the client. This allows you to distinguish between client and server configuration using the isServer property

Multiple configurations can be combined together with function composition. For example:
const withMDX = require('@next/mdx')
const withSass = require('@zeit/next-sass')

module.exports = withMDX(
  withSass({
    webpack(config, options) {
      // Further custom configuration here
      return config
    }
  })
)
In order to extend our usage of webpack, you can define a function that extends its config via next.config.js.
// next.config.js is not transformed by Babel. So you can only use javascript features supported by your version of Node.js.

module.exports = {
  webpack: (config, { buildId, dev, isServer, defaultLoaders, webpack }) => {
    // Note: we provide webpack above so you should not `require` it
    // Perform customizations to webpack config
    // Important: return the modified config

    // Example using webpack option
    config.plugins.push(
      new webpack.IgnorePlugin(/\/__tests__\//),
    )
    return config
  },
  webpackDevMiddleware: config => {
    // Perform customizations to webpack dev middleware config
    // Important: return the modified config
    return config
  }
}
The second argument to webpack is an object containing properties useful when customizing its configuration:

buildId - String the build id used as a unique identifier between builds
dev - Boolean shows if the compilation is done in development mode
isServer - Boolean shows if the resulting configuration will be used for server side (true), or client size compilation (false).
defaultLoaders - Object Holds loader objects Next.js uses internally, so that you can use them in custom configuration

babel - Object the babel-loader configuration for Next.js.
hotSelfAccept - Object the hot-self-accept-loader configuration. This loader should only be used for advanced use cases. For example @zeit/next-typescript adds it for top-level typescript pages.



Example usage of defaultLoaders.babel:
// Example next.config.js for adding a loader that depends on babel-loader
// This source was taken from the @next/mdx plugin source:
// https://github.com/zeit/next.js/tree/canary/packages/next-mdx
module.exports = {
  webpack: (config, options) => {
    config.module.rules.push({
      test: /\.mdx/,
      use: [
        options.defaultLoaders.babel,
        {
          loader: '@mdx-js/loader',
          options: pluginOptions.options
        }
      ]
    })

    return config
  }
}
Customizing babel config

Examples

Custom babel configuration


In order to extend our usage of babel, you can simply define a .babelrc file at the root of your app. This file is optional.
If found, we're going to consider it the source of truth, therefore it needs to define what next needs as well, which is the next/babel preset.
This is designed so that you are not surprised by modifications we could make to the babel configurations.
Here's an example .babelrc file:
{
  ""presets"": [""next/babel""],
  ""plugins"": []
}
The next/babel preset includes everything needed to transpile React applications. This includes:

preset-env
preset-react
plugin-proposal-class-properties
plugin-proposal-object-rest-spread
plugin-transform-runtime
styled-jsx

These presets / plugins should not be added to your custom .babelrc. Instead, you can configure them on the next/babel preset:
{
  ""presets"": [
    [
      ""next/babel"",
      {
        ""preset-env"": {},
        ""transform-runtime"": {},
        ""styled-jsx"": {},
        ""class-properties"": {}
      }
    ]
  ],
  ""plugins"": []
}
The modules option on ""preset-env"" should be kept to false otherwise webpack code splitting is disabled.
Exposing configuration to the server / client side
There is a common need in applications to provide configuration values.
Next.js supports 2 ways of providing configuration:

Build-time configuration
Runtime configuration

Build time configuration
The way build-time configuration works is by inlining the provided values into the Javascript bundle.
You can add the env key in next.config.js:
// next.config.js
module.exports = {
  env: {
    customKey: 'value'
  }
}
This will allow you to use process.env.customKey in your code. For example:
// pages/index.js
function Index() {
  return <h1>The value of customKey is: {process.env.customKey}</h1>
}

export default Index

Warning: Note that it is not possible to destructure process.env variables due to the webpack DefinePlugin replacing process.env.XXXX inline at build time

// Will not work
const { CUSTOM_KEY, CUSTOM_SECRET } = process.env;
AuthMethod({ key: CUSTOM_KEY, secret: CUSTOM_SECRET });

// Will work as replaced inline
AuthMethod({ key: process.env.CUSTOM_KEY, secret: process.env.CUSTOM_SECRET });
Runtime configuration

Warning: Note that this option is not available when using target: 'serverless'


Warning: Generally you want to use build-time configuration to provide your configuration.
The reason for this is that runtime configuration adds a small rendering / initialization overhead.

The next/config module gives your app access to the publicRuntimeConfig and serverRuntimeConfig stored in your next.config.js.
Place any server-only runtime config under a serverRuntimeConfig property.
Anything accessible to both client and server-side code should be under publicRuntimeConfig.
// next.config.js
module.exports = {
  serverRuntimeConfig: {
    // Will only be available on the server side
    mySecret: 'secret',
    secondSecret: process.env.SECOND_SECRET // Pass through env variables
  },
  publicRuntimeConfig: {
    // Will be available on both server and client
    staticFolder: '/static'
  }
}
// pages/index.js
import getConfig from 'next/config'
// Only holds serverRuntimeConfig and publicRuntimeConfig from next.config.js nothing else.
const { serverRuntimeConfig, publicRuntimeConfig } = getConfig()

console.log(serverRuntimeConfig.mySecret) // Will only be available on the server side
console.log(publicRuntimeConfig.staticFolder) // Will be available on both server and client

function MyImage() {
  return (
    <div>
      <img src={`${publicRuntimeConfig.staticFolder}/logo.png`} alt=""logo"" />
    </div>
  )
}

export default MyImage
Starting the server on alternative hostname
To start the development server using a different default hostname you can use --hostname hostname_here or -H hostname_here option with next dev. This will start a TCP server listening for connections on the provided host.
CDN support with Asset Prefix
To set up a CDN, you can set up the assetPrefix setting and configure your CDN's origin to resolve to the domain that Next.js is hosted on.
const isProd = process.env.NODE_ENV === 'production'
module.exports = {
  // You may only need to add assetPrefix in the production.
  assetPrefix: isProd ? 'https://cdn.mydomain.com' : ''
}
Note: Next.js will automatically use that prefix in the scripts it loads, but this has no effect whatsoever on /static or /public. If you want to serve those assets over the CDN, you'll have to introduce the prefix yourself. One way of introducing a prefix that works inside your components and varies by environment is documented in this example.
If your CDN is on a separate domain and you would like assets to be requested using a CORS aware request you can set a config option for that.
// next.config.js
module.exports = {
  crossOrigin: 'anonymous'
}
Production deployment
To deploy, instead of running next, you want to build for production usage ahead of time. Therefore, building and starting are separate commands:
next build
next start
To deploy Next.js with ZEIT Now see the ZEIT Guide for Deploying Next.js with Now.
Next.js can be deployed to other hosting solutions too. Please have a look at the 'Deployment' section of the wiki.
Note: NODE_ENV is properly configured by the next subcommands, if absent, to maximize performance. if you’re using Next.js programmatically, it’s your responsibility to set NODE_ENV=production manually!
Note: we recommend putting .next, or your custom dist folder, in .gitignore or .npmignore. Otherwise, use files or now.files to opt-into a whitelist of files you want to deploy, excluding .next or your custom dist folder.
Serverless deployment

Examples

now.sh
anna-artemov.now.sh
We encourage contributing more examples to this section


Serverless deployment dramatically improves reliability and scalability by splitting your application into smaller parts (also called lambdas). In the case of Next.js, each page in the pages directory becomes a serverless lambda.
There are a number of benefits to serverless. The referenced link talks about some of them in the context of Express, but the principles apply universally: serverless allows for distributed points of failure, infinite scalability, and is incredibly affordable with a ""pay for what you use"" model.
To enable serverless mode in Next.js, add the serverless build target in next.config.js:
// next.config.js
module.exports = {
  target: 'serverless'
}
The serverless target will output a single lambda per page. This file is completely standalone and doesn't require any dependencies to run:

pages/index.js => .next/serverless/pages/index.js
pages/about.js => .next/serverless/pages/about.js

The signature of the Next.js Serverless function is similar to the Node.js HTTP server callback:
export function render(req: http.IncomingMessage, res: http.ServerResponse) => void

http.IncomingMessage
http.ServerResponse
void refers to the function not having a return value and is equivalent to JavaScript's undefined. Calling the function will finish the request.

Using the serverless target, you can deploy Next.js to ZEIT Now with all of the benefits and added ease of control like for example; custom routes and caching headers. See the ZEIT Guide for Deploying Next.js with Now for more information.
One Level Lower
Next.js provides low-level APIs for serverless deployments as hosting platforms have different function signatures. In general you will want to wrap the output of a Next.js serverless build with a compatibility layer.
For example if the platform supports the Node.js http.Server class:
const http = require('http')
const page = require('./.next/serverless/pages/about.js')
const server = new http.Server((req, res) => page.render(req, res))
server.listen(3000, () => console.log('Listening on http://localhost:3000'))
For specific platform examples see the examples section above.
Summary

Low-level API for implementing serverless deployment
Every page in the pages directory becomes a serverless function (lambda)
Creates the smallest possible serverless function (50Kb base zip size)
Optimized for fast cold start of the function
The serverless function has 0 dependencies (they are included in the function bundle)
Uses the http.IncomingMessage and http.ServerResponse from Node.js
opt-in using target: 'serverless' in next.config.js
Does not load next.config.js when executing the function, note that this means publicRuntimeConfig / serverRuntimeConfig are not supported

Browser support
Next.js supports IE11 and all modern browsers out of the box using @babel/preset-env. In order to support IE11 Next.js adds a global Promise polyfill. In cases where your own code or any external NPM dependencies you are using requires features not supported by your target browsers you will need to implement polyfills.
The polyfills example demonstrates the recommended approach to implement polyfills.
TypeScript
TypeScript is supported out of the box in Next.js. To get started using it create a tsconfig.json in your project:
{
  ""compilerOptions"": {
    ""allowJs"": true, /* Allow JavaScript files to be type checked. */
    ""alwaysStrict"": true, /* Parse in strict mode. */
    ""esModuleInterop"": true, /* matches compilation setting */
    ""isolatedModules"": true, /* to match webpack loader */
    ""jsx"": ""preserve"", /* Preserves jsx outside of Next.js. */
    ""lib"": [""dom"", ""es2017""], /* List of library files to be included in the type checking. */
    ""module"": ""esnext"", /* Specifies the type of module to type check. */
    ""moduleResolution"": ""node"", /* Determine how modules get resolved. */
    ""noEmit"": true, /* Do not emit outputs. Makes sure tsc only does type checking. */

    /* Strict Type-Checking Options, optional, but recommended. */
    ""noFallthroughCasesInSwitch"": true, /* Report errors for fallthrough cases in switch statement. */
    ""noUnusedLocals"": true, /* Report errors on unused locals. */
    ""noUnusedParameters"": true, /* Report errors on unused parameters. */
    ""strict"": true /* Enable all strict type-checking options. */,
    ""target"": ""esnext"" /* The type checking input. */
  }
}
After adding the tsconfig.json you need to install @types to get proper TypeScript typing.
npm install --save-dev @types/react @types/react-dom @types/node
Now can change any file from .js to .ts / .tsx (tsx is for files using JSX). To learn more about TypeScript checkout out its documentation.
Exported types
Next.js provides NextPage type that can be used for pages in the pages directory. NextPage adds definitions for getInitialProps so that it can be used without any extra typing needed.
import { NextPage } from 'next'

interface Props {
  pathname: string
}

const Page: NextPage<Props> = ({ pathname }) => (
  <main>Your request pathname: {pathname}</main>
)

Page.getInitialProps = async ({ pathname }) => {
  return { pathname }
}

export default Page
AMP Support
Enabling AMP Support
To enable AMP support for a page, first enable experimental AMP support in your next.config.js and then import withAmp from next/amp and wrap your page's component in it.
// pages/about.js
import { withAmp } from 'next/amp'

export default withAmp(function AboutPage(props) {
  return (
    <h3>My AMP About Page!</h3>
  )
})
AMP Page Modes
AMP pages can specify two modes:

AMP-only (default)

Pages have no Next.js or React client-side runtime
Pages are automatically optimized with AMP Optimizer, an optimizer that applies the same transformations as AMP caches (improves performance by up to 42%)
Pages have a user-accessible (optimized) version of the page and a search-engine indexable (unoptimized) version of the page
Opt-in via withAmp(Component)


Hybrid

Pages are able to be rendered as traditional HTML (default) and AMP HTML (by adding ?amp=1 to the URL)
The AMP version of the page is not optimized with AMP Optimizer so that it is indexable by search-engines
Opt-in via withAmp(Component, { hybrid: true })



Both of these page modes provide a consistently fast experience for users accessing pages through search engines.
AMP Behavior with next export
When using next export to statically pre-render pages Next.js will detect if the page supports AMP and change the exporting behavior based on that.
Hybrid AMP (pages/about.js) would output:

out/about/index.html - with client-side React runtime
out/about.amp/index.html - AMP page

AMP-only (pages/about.js) would output:

out/about/index.html - Optimized AMP page
out/about.amp/index.html - AMP page

During export Next.js automatically detects if a page is AMP-only and apply dirty/clean optimizations. The dirty version will be output to page/index.html and the clean version will be output to page.amp/index.html. We also automatically insert the <link rel=""amphtml"" href=""/page.amp"" /> and <link rel=""canonical"" href=""/"" /> tags for you.
Adding AMP Components
The AMP community provides many components to make AMP pages more interactive. You can add these components to your page by using next/head:
// pages/hello.js
import Head from 'next/head'
import { withAmp } from 'next/amp'

export default withAmp(function MyAmpPage() {
  return (
    <div>
      <Head>
        <script
          async
          key=""amp-timeago""
          custom-element=""amp-timeago""
          src=""https://cdn.ampproject.org/v0/amp-timeago-0.1.js""
        />
      </Head>

      <p>Some time: {date.toJSON()}</p>
      <amp-timeago
        width=""0""
        height=""15""
        datetime={date.toJSON()}
        layout=""responsive""
      >
        .
      </amp-timeago>
    </div>
  )
})
AMP Validation
AMP pages are automatically validated with amphtml-validator during development. Errors and warnings will appear in the terminal where you started Next.js.
Pages are also validated during next export and any warnings / errors will be printed to the terminal.
Any AMP errors will cause next export to exit with status code 1 because the export is not valid AMP.
Static HTML export

Examples

Static export


next export is a way to run your Next.js app as a standalone static app without the need for a Node.js server.
The exported app supports almost every feature of Next.js, including dynamic urls, prefetching, preloading and dynamic imports.
The way next export works is by pre-rendering all pages possible to HTML. It does so based on a mapping of pathname key to page object. This mapping is called the exportPathMap.
The page object has 2 values:

page - String the page inside the pages directory to render
query - Object the query object passed to getInitialProps when pre-rendering. Defaults to {}

Usage
Simply develop your app as you normally do with Next.js. Then run:
next build
next export

By default next export doesn't require any configuration. It will generate a default exportPathMap containing the routes to pages inside the pages directory. This default mapping is available as defaultPathMap in the example below.
If your application has dynamic routes you can add a dynamic exportPathMap in next.config.js.
This function is asynchronous and gets the default exportPathMap as a parameter.
// next.config.js
module.exports = {
  exportPathMap: async function(defaultPathMap) {
    return {
      '/': { page: '/' },
      '/about': { page: '/about' },
      '/readme.md': { page: '/readme' },
      '/p/hello-nextjs': { page: '/post', query: { title: 'hello-nextjs' } },
      '/p/learn-nextjs': { page: '/post', query: { title: 'learn-nextjs' } },
      '/p/deploy-nextjs': { page: '/post', query: { title: 'deploy-nextjs' } }
    }
  }
}

Note that if the path ends with a directory, it will be exported as /dir-name/index.html, but if it ends with an extension, it will be exported as the specified filename, e.g. /readme.md above. If you use a file extension other than .html, you may need to set the Content-Type header to text/html when serving this content.

Then simply run these commands:
next build
next export
For that you may need to add a NPM script to package.json like this:
{
  ""scripts"": {
    ""build"": ""next build"",
    ""export"": ""npm run build && next export""
  }
}
And run it at once with:
npm run export
Then you have a static version of your app in the out directory.

You can also customize the output directory. For that run next export -h for the help.

Now you can deploy the out directory to any static hosting service. Note that there is an additional step for deploying to GitHub Pages, documented here.
For an example, simply visit the out directory and run following command to deploy your app to ZEIT Now.
now
Copying custom files
In case you have to copy custom files like a robots.txt or generate a sitemap.xml you can do this inside of exportPathMap.
exportPathMap gets a few contextual parameter to aid you with creating/copying files:

dev - true when exportPathMap is being called in development. false when running next export. In development exportPathMap is used to define routes and behavior like copying files is not required.
dir - Absolute path to the project directory
outDir - Absolute path to the out directory (configurable with -o or --outdir). When dev is true the value of outDir will be null.
distDir - Absolute path to the .next directory (configurable using the distDir config key)
buildId - The buildId the export is running for

// next.config.js
const fs = require('fs')
const { join } = require('path')
const { promisify } = require('util')
const copyFile = promisify(fs.copyFile)

module.exports = {
  exportPathMap: async function(
    defaultPathMap,
    { dev, dir, outDir, distDir, buildId }
  ) {
    if (dev) {
      return defaultPathMap
    }
    // This will copy robots.txt from your project root into the out directory
    await copyFile(join(dir, 'robots.txt'), join(outDir, 'robots.txt'))
    return defaultPathMap
  }
}
Limitation
With next export, we build a HTML version of your app. At export time we will run getInitialProps of your pages.
The req and res fields of the context object passed to getInitialProps are not available as there is no server running.

You won't be able to render HTML dynamically when static exporting, as we pre-build the HTML files. If you want to do dynamic rendering use next start or the custom server API

Multi Zones

Examples

With Zones


A zone is a single deployment of a Next.js app. Just like that, you can have multiple zones. Then you can merge them as a single app.
For an example, you can have two zones like this:

https://docs.my-app.com for serving /docs/**
https://ui.my-app.com for serving all other pages

With multi zones support, you can merge both these apps into a single one. Which allows your customers to browse it using a single URL. But you can develop and deploy both apps independently.

This is exactly the same concept as microservices, but for frontend apps.

How to define a zone
There are no special zones related APIs. You only need to do following things:

Make sure to keep only the pages you need in your app. (For an example, https://ui.my-app.com should not contain pages for /docs/**)
Make sure your app has an assetPrefix. (You can also define the assetPrefix dynamically.)

How to merge them
You can merge zones using any HTTP proxy.
You can use micro proxy as your local proxy server. It allows you to easily define routing rules like below:
{
  ""rules"": [
    {
      ""pathname"": ""/docs**"",
      ""method"": [""GET"", ""POST"", ""OPTIONS""],
      ""dest"": ""https://docs.my-app.com""
    },
    { ""pathname"": ""/**"", ""dest"": ""https://ui.my-app.com"" }
  ]
}
For the production deployment, you can use the path alias feature if you are using ZEIT now. Otherwise, you can configure your existing proxy server to route HTML pages using a set of rules as shown above.
Recipes

Setting up 301 redirects
Dealing with SSR and server only modules
Building with React-Material-UI-Next-Express-Mongoose-Mongodb
Build a SaaS Product with React-Material-UI-Next-MobX-Express-Mongoose-MongoDB-TypeScript

FAQ

Is this production ready?
  Next.js has been powering https://zeit.co since its inception.
We’re ecstatic about both the developer experience and end-user performance, so we decided to share it with the community.


How big is it?
The client side bundle size should be measured in a per-app basis.
A small Next main bundle is around 65kb gzipped.


Is this like `create-react-app`?
Yes and No.
Yes in that both make your life easier.
No in that it enforces a structure so that we can do more advanced things like:

Server side rendering
Automatic code splitting

In addition, Next.js provides two built-in features that are critical for every single website:

Routing with lazy component loading: <Link> (by importing next/link)
A way for components to alter <head>: <Head> (by importing next/head)

If you want to create re-usable React components that you can embed in your Next.js app or other React applications, using create-react-app is a great idea. You can later import it and keep your codebase clean!


How do I use CSS-in-JS solutions?
Next.js bundles styled-jsx supporting scoped css. However you can use any CSS-in-JS solution in your Next app by just including your favorite library as mentioned before in the document.


What syntactic features are transpiled? How do I change them?
We track V8. Since V8 has wide support for ES6 and async and await, we transpile those. Since V8 doesn’t support class decorators, we don’t transpile those.
See the documentation about customizing the babel config and next/preset for more information.


Why a new Router?
Next.js is special in that:

Routes don’t need to be known ahead of time
Routes are always lazy-loadable
Top-level components can define getInitialProps that should block the loading of the route (either when server-rendering or lazy-loading)

As a result, we were able to introduce a very simple approach to routing that consists of two pieces:

Every top level component receives a url object to inspect the url or perform modifications to the history
A <Link /> component is used to wrap elements like anchors (<a/>) to perform client-side transitions



How do I define a custom fancy route?
We added the ability to map between an arbitrary URL and any component by supplying a request handler.
On the client side, we have a parameter call as on <Link> that decorates the URL differently from the URL it fetches.


How do I fetch data?
It’s up to you. getInitialProps is an async function (or a regular function that returns a Promise). It can retrieve data from anywhere.


Can I use it with GraphQL?
Yes! Here's an example with Apollo.


Can I use it with Redux and thunk?
Yes! Here's an example


Can I use it with Redux?
Yes! Here's an example


Can I use Next with my favorite Javascript library or toolkit?
Since our first release we've had many example contributions, you can check them out in the examples directory


What is this inspired by?
Many of the goals we set out to accomplish were the ones listed in The 7 principles of Rich Web Applications by Guillermo Rauch.
The ease-of-use of PHP is a great inspiration. We feel Next.js is a suitable replacement for many scenarios where you otherwise would use PHP to output HTML.
Unlike PHP, we benefit from the ES6 module system and every file exports a component or function that can be easily imported for lazy evaluation or testing.
As we were researching options for server-rendering React that didn’t involve a large number of steps, we came across react-page (now deprecated), a similar approach to Next.js by the creator of React Jordan Walke.

Contributing
Please see our contributing.md
Authors

Arunoda Susiripala (@arunoda) – ZEIT
Tim Neutkens (@timneutkens) – ZEIT
Naoyuki Kanezawa (@nkzawa) – ZEIT
Tony Kovanen (@tonykovanen) – ZEIT
Guillermo Rauch (@rauchg) – ZEIT
Dan Zajdband (@impronunciable) – Knight-Mozilla / Coral Project

",37406
gitjuzer/AFP2_2019_C,Java,"AFP2_2019_C
",3
polarphp/polarphp,C++,"
Read the English version of this document: English version Readme
阅读本文档其他语言版本: English, 简体中文.
为什么要做 polarphp 项目
随着Go和NodeJS的强势崛起，PHP的市场份额逐渐被蚕食，而PHP官方仍然坚守在Web编程领域，有些东西越是想守住就越守不住。polarphp借鉴NodeJS和Go的相关特性对zendVM重新封装，去掉PHP一些古老弃用的特性和强Web属性，通过实现一套新的运行时框架libpdk，将PHP语言打造成为一门真正的通用性脚本语言，赋能PHP，让其拥有异步编程，协程，线程，内置的unicode支持，标准的文件IO等等特性，让PHP程序员不仅仅能做web应用，也能从容面对真正的服务端应用。polarphp不是一门新的语言，而是PHP语言的一种运行时容器。
主要特性

 兼容最新的PHP语言标准，移除废弃语言特性
 内置unicode字符标准支持
 全功能型运行时库支持，支持异步编程，多线程和协程等等编程模式
 内置包管理器
 内置文档生成器

开发计划
因为开发资源有限，开发计划暂定如下：

使用cmake对zend VM进行编译，生成polarphp定制版的PHP语言虚拟机
语言支持项目，语言测试框架，移植LLVM项目的lit测试框架
实现polarphp驱动程序，实现从命令行执行PHP代码
对polarphp虚拟机进行回归测试，暂定跑通PHP的语言虚拟机相关回归测试
实现polarphp的内置函数
发布核心虚拟机的docker镜像
整合libpdk运行时框架
实现人性化安装，尽量以最少的步骤进行polarphp的安装
实现包管理器
实现语言配套小工具，比如文档生成工具等等

开始体验
克隆polarphp项目库
git clone https://github.com/polarphp/polarphp.git
cd polarphp
git submodule init
git submodule update
git checkout v0.0.1-alpha

运行脚本
./devtools/scripts/build_polarphp.sh

这个时候脚本开始编译相关镜像，耗时比较长，请您耐心等待。等待编译完成，您运行：
docker images

这个时候请确认在输出中有如下镜像：

polarphp_base_env
polarphp_debug

如果没有问题，我们开始测试polarphp是否在镜像中正常运行。
docker run --rm -it polarphp_debug

进入容器后，输入我们的polarphp命令行程序
polar --version

如果您得到下面的输出：
polarphp 0.0.1-git (built: 2019-01-27 12:22)
Copyright (c) 2016-2018 The polarphp foundation (https://polar.foundation)
Zend Engine v3.3.0-dev, Copyright (c) 1998-2018 Zend Technologies

恭喜您，您已经成功编译了polarphp运行时环境。
在编译镜像的时候，我们在~/temp/文件夹中放入了一个测试脚本
if (function_exists('\php\retrieve_version_str')) {
    echo ""version str: "" . \php\retrieve_version_str() . ""\n"";
}

if (function_exists('\php\retrieve_major_version')) {
    echo ""major version: "" . \php\retrieve_major_version() . ""\n"";
}

if (function_exists('\php\retrieve_minor_version')) {
    echo ""minor version: "" . \php\retrieve_minor_version() . ""\n"";
}

if (function_exists('\php\retrieve_patch_version')) {
    echo ""patch version: "" . \php\retrieve_patch_version() . ""\n"";
}

您可以运行一下命令：
polar ~/temp/main.php

如果没有错误，您将得到下面的输出：
version str: polarphp 0.0.1-git
major version: 0
minor version: 0
patch version: 1

感谢您测试polarphp，有什么问题，请扫描下面的微信二维码进群交流。
社区
目前我们暂时只针对中国的用户，所以采用了微信和QQ群的交流方式，下面是二维码，有兴趣的同学可以扫码加入：

PS：扫码请注明来意，比如：学习polarphp或者PHP爱好者

















目前有以下工作组

语言核心团队
标准库团队
生态链项目团队
文档团队
官方网站维护团队

授权协议
polarphp在php语言项目之上进行二次开发，遵守php项目的协议，详情请看：项目协议
贡献代码引导
===========================

CODING_STANDARDS
README.GIT-RULES
README.MAILINGLIST_RULES
README.RELEASE_PROCESS

特别感谢





















",809
catdad/electronmon,JavaScript,"electronmon
Watch and reload your electron app the easy way!





This is the simplest way to watch and restart/reload electron applications. It requires no quessing, no configuration, and no changing your application or conditionally requiring dependencies. And best of all, it keeps everything in-process, and will not exit on the first application relaunch.
It was inspired by nodemon and largely works the same way (by magic 🧙).
To use it, you don't have to change your application at all. Just use electronmon to launch it, using all the same arguments you would pass to the electron cli:
npx electronmon .
That's it! Now, all your files are watched. Changes to main process files will cause the application to restart entirely, while changes to any of the renderer process files will simply reload the application browser windows.
All you have to do now is write your application code.
",2
JimHokanson/multichannel_systems_stg_matlab,MATLAB,"Multi Channel Systems Stimulus Generator Interface
Multi-Channel Systems (MCS) provides software that allows for communication with their stimulus generators (STG). This Matlab code wraps their drivers.
The streaming interface is not supported. In other words, the goal of this software is to design a stimulus that gets uploaded to the stimulator at one point in time then run. Any changes to the stimulus require reuploading a new stimulus to the stimulator.
**Warning** At some point there were bugs in the stimulator firmware that made it not work properly with this code. If the code does not appear to be working you may need to update your stimulator firmware .... Otherwise feel free to report issues and I can look into them. Additionally, I currently have minimal testing for the library so please test that your code is working as intended. Any help with adding tests would be appreciated.
**Bonus** This library contains code which can be used to generate stimulus pulse trains for use in uploading to a DAQ see here.
Status
Many functions exposed by the driver are not yet implemented in Matlab, although most are relatively trivial to implement. Basic functionality of starting and stopping stimuli on channels is implemented. The one big feature not yet supported is to have multiple segments of stimulation, where each segment gets repeated a different number of times. This feature is exposed in the MC Stimulus II GUI (https://www.multichannelsystems.com/software/mc-stimulus-ii) but not yet in this software.
Basic Usage
Here is a basic usage example.
%This assumes that only one device is present or that we want the first one.
%See help mcs.getStimulator
s = mcs.getStimulator();

%For reference
tr = s.trigger_settings;
disp(tr);

%Typically I run:
%Note this sets all triggers to have infinite repeats (manual stim stopping)
s.setupTrigger('linearize',true,'repeat_all',0);

%Unfortunately you need to get tr again to see the updates. Eventually I'll link everything ...
%See Issue #1
tr = s.trigger_settings;
disp(tr);

%Lots of options here.
%This will create a 40 Hz, 500 uA train of biphasic pulses
pt = 500*mcs.stg.pulse_train.fixed_rate(40);

%Stimulate on channel 1 with this pattern
%Curently pattern is automatically copied to the sync channel as well
s.sentDataToDevice(1,pt);

s.startStim;

%Needed for infinite repeating
s.stopStim;

%------------------------------------------------------------
%Let's use two patterns on two channels:
pt2 = 100*mcs.stg.pulse_train.fixed_rate(10);

%Send the first pattern to channel 1 and the 2nd pattern to channel 3
s.sentDataToDevice([1 3],{pt,pt2});

s.startStim;

s.stopStim;

Dependencies
The driver necessary for this code is included. You might need to have MC Stimulus II installed as well ...
How this Code Works
This code relies on a driver provided by MCS.
https://www.multichannelsystems.com/software/mcsusbnetdll
I've found some bugs in the driver and others may still remain (so test your code). I've downloaded the driver and placed it into the code. (See /+mcs/+stg/@sdk - currently in a version folder of 3_2_71).
Documentation for the driver is provided in 'McsUsbNet_for_STG.chm'
Stimulus Design
The design of stimulation patterns is described here.
",3
dxxzst/Free-SS-SSR,None,"Free-SS-SSR
免费的SS账号、SSR账号，定期更新
最新更新日期：2019-05-17 09:21:28
SS账号



账号
端口
密码
加密方式
更新时间
国家




213.183.48.10
17228
rc4-md5
ss8.pm-19235472
09:17:06
RU


45.79.97.186
443
aes-256-cfb
9d6cceaa373bf2c8acb22e60b6a58be6
09:17:17
US


45.12.205.202
12502
aes-256-cfb
12345678
09:17:17
AU


45.33.48.155
8097
aes-256-cfb
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
09:17:17
US


45.79.95.18
443
aes-256-cfb
9d6cceaa373bf2c8acb22e60b6a58be6
09:17:15
US


91.188.223.72
8080
rc4-md5
http://t.cn/EGJIyrl
09:17:16
RU


45.79.87.208
443
aes-256-cfb
9d6cceaa373bf2c8acb22e60b6a58be6
09:17:16
US


185.243.57.221
80
rc4-md5
t.me/SSRSUB
09:17:16
US


46.29.162.46
1026
rc4-md5
91vpn.cf
09:17:14
RU


185.135.82.80
2333
aes-128-ctr
doub.io
09:17:18
RU


185.224.249.34
9001
aes-256-cfb
UkXRsXvR6buDMG2Y
09:17:14
RU


198.199.109.79
19687
aes-256-cfb
ssx.re-94520056
09:17:05
US


192.241.221.124
11987
aes-256-cfb
ss8.pm-74764838
09:17:05
US


172.104.39.134
8097
aes-256-cfb
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
09:17:14
SG


172.104.179.17
19077
aes-256-cfb
f55.fun-50558565
09:17:06
SG


104.167.97.164
543
rc4-md5
http://t.cn/RD0D7sx
09:17:14
CA


45.33.32.152
8097
aes-256-cfb
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
09:17:13
US


168.62.163.117
993
rc4-md5
2019.03.07
09:12:16
US


91.219.237.119
9001
aes-256-cfb
getvpn20190501
09:17:18
HU


45.79.82.49
443
aes-256-cfb
9d6cceaa373bf2c8acb22e60b6a58be6
09:17:16
US


192.154.197.89
40899
aes-256-cfb
Y3oEquMWO2DL
09:17:19
US


213.226.68.94
9030
aes-256-cfb
GeregetR8cvQHzYr
09:17:18
DE


45.79.95.58
443
aes-256-cfb
9d6cceaa373bf2c8acb22e60b6a58be6
09:17:16
US


128.199.187.62
14811
aes-256-cfb
ssx.re-92498252
09:17:06
SG


192.210.190.101
25581
aes-256-cfb
superssrnet
09:17:16
US


141.98.213.163
988
chacha20
5M57kg11c214qDmK
09:17:18
KR


139.162.115.215
8097
aes-256-cfb
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
09:17:18
JP


172.105.213.201
10456
aes-256-cfb
fafajofdsgc
09:17:07
JP


172.104.188.241
11963
aes-256-cfb
f55.fun-96885875
09:17:06
SG


45.33.69.91
11040
aes-256-cfb
f55.fun-40097695
09:17:04
US


103.124.107.7
9052
aes-256-cfb
jt2ekBNc9HuVtm2a
09:17:18
US


45.79.93.164
443
aes-256-cfb
9d6cceaa373bf2c8acb22e60b6a58be6
09:17:15
US


46.29.162.104
21560
chacha20-ietf
5500
09:17:13
RU


173.255.230.159
11255
aes-256-cfb
f55.fun-82730394
09:17:04
US


207.246.107.206
1996
chacha20
wujie1314
09:17:16
US


66.175.223.22
8097
aes-256-cfb
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
09:17:17
US


45.33.80.198
13327
aes-256-cfb
f55.fun-46814510
09:17:04
US


45.79.94.57
443
aes-256-cfb
9d6cceaa373bf2c8acb22e60b6a58be6
09:17:13
US


45.79.93.178
443
aes-256-cfb
9d6cceaa373bf2c8acb22e60b6a58be6
09:17:16
US


172.104.171.145
19846
aes-256-cfb
f55.fun-58018742
09:17:06
SG


178.128.94.215
10613
aes-256-cfb
ss8.pm-02275771
09:17:06
SG


45.79.96.77
443
aes-256-cfb
9d6cceaa373bf2c8acb22e60b6a58be6
09:17:12
US


103.135.102.119
1257
chacha20
5785455
09:17:12
US


36.228.214.160
443
aes-128-ctr
nexitally
09:07:12
TW


157.230.154.68
11012
aes-256-cfb
ssx.re-75871166
09:17:06
US


172.104.123.158
8097
aes-256-cfb
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
09:17:17
JP


45.79.83.180
443
aes-256-cfb
9d6cceaa373bf2c8acb22e60b6a58be6
09:17:12
US


139.162.37.161
8097
aes-256-cfb
eIW0Dnk69454e6nSwuspv9DmS201tQ0D
09:17:13
SG



SSR账号



#
ssr连接




1
ssr://MTM5LjE2Mi4xMTUuMjE1OjgwOTc6b3JpZ2luOmFlcy0yNTYtY2ZiOnBsYWluOlpVbFhNRVJ1YXpZNU5EVTBaVFp1VTNkMWMzQjJPVVJ0VXpJd01YUlJNRVEvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91YVhwZWFjckMxVWIydDVidyZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


2
ssr://NDUuMTIuMjA1LjIwMjoxMjUwMjpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46TVRJek5EVTJOemcvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91YS1zLVdrcC1XSXFlUzZtaTFSZFdWbGJuTnNZVzVrJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


3
ssr://NDUuMzMuNDguMTU1OjgwOTc6b3JpZ2luOmFlcy0yNTYtY2ZiOnBsYWluOlpVbFhNRVJ1YXpZNU5EVTBaVFp1VTNkMWMzQjJPVVJ0VXpJd01YUlJNRVEvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91ZS1qdVdidlMzbGlxRGxpS25ucG9fbHNMemt1cHJsdDU0Jmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


4
ssr://Y2hlbmdkdS1jaGluYS1wcm94eTEuZGFycmVuLWxlZS5uZXQ6ODA4MTpvcmlnaW46cmM0LW1kNTpwbGFpbjpPREE0TVEvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91V2JtLVczbmVlY2dlYUlrT21EdmVXNGdpRG5sTFhrdjZFJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


5
ssr://NDYuMjkuMTYyLjE2OTo4MDg2Om9yaWdpbjphZXMtMjU2LWNmYjpwbGFpbjpOVGc0TkRRNU56azVPUS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VTX2hPZTlsLWFXcnkxTmIzTmpiM2MmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


6
ssr://MTM4LjY4LjE0My44NDoyMzIwODpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46VGxkd2JUZElaVGxFTVhOMy8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VpTHNlV2J2UzNvaTdIbW9MemxoYkEmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


7
ssr://MTc2LjMyLjMzLjE0Mjo0NTY6YXV0aF9hZXMxMjhfc2hhMTphZXMtMjU2LWNmYjpodHRwX3Bvc3Q6TW1nek4zUm4vP29iZnNwYXJhbT1PR1prTW1ZME5qWXhMbTFwWTNKdmMyOW1kQzVqYjIwJnByb3RvcGFyYW09TkRZMk1UcHNPRWszZFRRJnJlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91U19oT2U5bC1hV3J5MU5iM05qYjNjJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


8
ssr://OTIuMTE4LjQ1LjI0OTo0MjQyODphdXRoX2NoYWluX2E6YWVzLTI1Ni1jZmI6dGxzMS4yX3RpY2tldF9hdXRoOlJXMXdhRUpyLz9vYmZzcGFyYW09NUxpTjVhR3I1WWlaNVktVzZJcUM1NEs1NkllcTVhNmE1TG1KNXJlMzVyZUc1WS1DNXBXdyZwcm90b3BhcmFtPTZJcUM1NEs1NVkyVjU2dXY1WS1qNXBlMjVwZWc1cFdJJnJlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91YVhwZWFjckMxVWIydDVidyZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


9
ssr://MjA5Ljk3LjE2MC4xMzU6MjY4Nzg6b3JpZ2luOmFlcy0yNTYtY2ZiOnBsYWluOlVtUlRjV2d3V2t0NU1UVTQvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91YVdzT1dLb09XZG9TMCZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


10
ssr://MjA5Ljk3LjE4OC4xMjc6MzQzMDg6b3JpZ2luOmFlcy0yNTYtY2ZiOnBsYWluOmFFeHlRekZyVlVWS01VcFEvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91aUxzZVdidlMzb2k3SG1vTHpsaGJBJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


11
ssr://NDUuNzkuNzkuMzc6NDQzOm9yaWdpbjphZXMtMjU2LWNmYjpwbGFpbjpPV1EyWTJObFlXRXpOek5pWmpKak9HRmpZakl5WlRZd1lqWmhOVGhpWlRZLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWUtanVXYnZTM2xpcURsaUtubnBvX2xzTHprdXBybHQ1NCZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


12
ssr://czEwLnpiamcueHl6OjQ0MzphdXRoX2NoYWluX2E6bm9uZTpodHRwX3NpbXBsZTpjM1Z3WlhKemMzSXViV1UvP3Byb3RvcGFyYW09TnpNM01ESTZOMEpEWmtkdyZyZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWFYcGVhY3JDMVViMnQ1YncmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


13
ssr://czM2LnpiamcueHl6OjQ0MzphdXRoX2NoYWluX2E6bm9uZTpodHRwX3NpbXBsZTpjM1Z3WlhKemMzSXViV1UvP3Byb3RvcGFyYW09TnpNM01ESTZOMEpEWmtkdyZyZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWFYcGVhY3JDMVViMnQ1YncmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


14
ssr://czIzLnpiamcueHl6OjQ0MzphdXRoX2NoYWluX2E6bm9uZTpodHRwX3NpbXBsZTpjM1Z3WlhKemMzSXViV1UvP3Byb3RvcGFyYW09TnpNM01ESTZOMEpEWmtkdyZyZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWFYcGVhY3JDMVViMnQ1YncmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


15
ssr://bWYxLnFxc3NyLnRvcDo1NzIwNjpvcmlnaW46cmM0LW1kNTpwbGFpbjpibkJ0VkVOTC8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT2twaGNHRnUmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


16
ssr://dHcyLm5leGl0YWxseS5jbzo0NDM6b3JpZ2luOmFlcy0xMjgtY3RyOnBsYWluOmJtVjRhWFJoYkd4NS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VXUHNPYTV2aTNsajdEbGpKZmx1SUkmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


17
ssr://bmF0aG9zdDU3NC5ndzAyLnZkcy5wcXMucHc6MzIzNjg6YXV0aF9hZXMxMjhfc2hhMTphZXMtMjU2LWNmYjpwbGFpbjpiMmh6YzNJMk5qWS8_cHJvdG9wYXJhbT1NalUzTmpwSk1WWkRSREkmcmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VXUHNPYTV2aTNsajdEbGpKZmx1SUkmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


18
ssr://MTQ5LjEyOS45Ni4yMzQ6MzIzNjg6YXV0aF9hZXMxMjhfc2hhMTphZXMtMjU2LWNmYjpwbGFpbjpiMmh6YzNJMk5qWS8_cHJvdG9wYXJhbT1NalUzTmpwSk1WWkRSREkmcmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VTNHJlV2J2UzNsakpma3VxdyZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


19
ssr://MTMuMTI0LjE0My4xOTA6MzIzNjg6YXV0aF9hZXMxMjhfc2hhMTphZXMtMjU2LWNmYjpwbGFpbjpiMmh6YzNJMk5qWS8_cHJvdG9wYXJhbT1NalUzTmpwSk1WWkRSREkmcmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VtZnFlV2J2UzNwcHBibHNKVG5pYm5saUt2bHVJSSZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


20
ssr://NDYuMjkuMTYyLjEwNDoyMTU2MDpvcmlnaW46Y2hhY2hhMjAtaWV0ZjpwbGFpbjpOVFV3TUEvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91U19oT2U5bC1hV3J5MU5iM05qYjNjJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


21
ssr://dGswMS5idWppZGFvLmdhOjU1MzkxOm9yaWdpbjpjaGFjaGEyMC1pZXRmOnBsYWluOk1UWXlPRFl6TVRVek9BLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWFYcGVhY3JDMVViMnQ1YncmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


22
ssr://c2cwMS5idWppZGFvLmdhOjU1MzkxOm9yaWdpbjpjaGFjaGEyMC1pZXRmOnBsYWluOk1UWXlPRFl6TVRVek9BLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWFXc09XS29PV2RvUzAmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


23
ssr://NDUuNzkuNjYuOTo0NDM6b3JpZ2luOmFlcy0yNTYtY2ZiOnBsYWluOk9XUTJZMk5sWVdFek56TmlaakpqT0dGallqSXlaVFl3WWpaaE5UaGlaVFkvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91ZS1qdVdidlMzbGlxRGxpS25ucG9fbHNMemt1cHJsdDU0Jmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


24
ssr://MjA5LjE0MS4zMi4xNzo2NTUzMTphdXRoX2FlczEyOF9zaGExOmFlcy0yNTYtY2ZiOmh0dHBfc2ltcGxlOlUxTlNMa2R2YkdSQUl6WTFOVE14Lz9wcm90b3BhcmFtPU5qRTJOem81WXpkcE1YayZyZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWUtanVXYnZTM2xpcURsaUtubnBvX2xzTHprdXBybHQ1NCZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


25
ssr://MTkyLjE1NC4xOTcuODk6NDA5MDY6b3JpZ2luOmFlcy0yNTYtY2ZiOnBsYWluOldUTnZSWEYxVFZkUE1rUk0vP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91ZS1qdVdidlMza3VwN2xpS25tb1pIcGdxUGx0NTQmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


26
ssr://NDUuNzkuNzQuNTY6NDQzOm9yaWdpbjphZXMtMjU2LWNmYjpwbGFpbjpPV1EyWTJObFlXRXpOek5pWmpKak9HRmpZakl5WlRZd1lqWmhOVGhpWlRZLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWUtanVXYnZTM2xpcURsaUtubnBvX2xzTHprdXBybHQ1NCZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


27
ssr://MTcyLjEwNC4yMTMuNDU6NDQzOm9yaWdpbjphZXMtMjU2LWNmYjpwbGFpbjpPV1EyWTJObFlXRXpOek5pWmpKak9HRmpZakl5WlRZd1lqWmhOVGhpWlRZLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWUtanVXYnZTM21sckRtczczb3BiX2x0NTQmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


28
ssr://NDUuNzkuNzUuMjUzOjQ0MzpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46T1dRMlkyTmxZV0V6TnpOaVpqSmpPR0ZqWWpJeVpUWXdZalpoTlRoaVpUWS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VlLWp1V2J2UzNsaXFEbGlLbm5wb19sc0x6a3Vwcmx0NTQmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


29
ssr://dXMwLm11bWUucmVkOjk5MzpvcmlnaW46cmM0LW1kNTpwbGFpbjpNakF4T1M0d015NHdOdy8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VlLWp1V2J2UzNsdkpmbGtJbmxzTHprdXBybHQ1NCZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


30
ssr://NDUuNTYuOTQuNDA6NDQzOm9yaWdpbjphZXMtMjU2LWNmYjpwbGFpbjpPV1EyWTJObFlXRXpOek5pWmpKak9HRmpZakl5WlRZd1lqWmhOVGhpWlRZLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWUtanVXYnZTM2x2SmZsa0lubHNMemt1cHJsdDU0Jmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


31
ssr://NDUuNzkuNzUuODY6NDQzOm9yaWdpbjphZXMtMjU2LWNmYjpwbGFpbjpPV1EyWTJObFlXRXpOek5pWmpKak9HRmpZakl5WlRZd1lqWmhOVGhpWlRZLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWUtanVXYnZTM2xpcURsaUtubnBvX2xzTHprdXBybHQ1NCZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


32
ssr://MTk0LjE1Ni4xMjAuMzg6NjU1MzE6YXV0aF9hZXMxMjhfc2hhMTphZXMtMjU2LWNmYjpodHRwX3NpbXBsZTpVMU5TTGtkdmJHUkFJelkxTlRNeC8_cHJvdG9wYXJhbT1OakUyTnpvNVl6ZHBNWGsmcmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VTX2hPZTlsLWFXcnkxT2IzWnZjMmxpYVhKemF5QlBZbXhoYzNRJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


33
ssr://NDUuNTYuOTEuMzc6NDQzOm9yaWdpbjphZXMtMjU2LWNmYjpwbGFpbjpPV1EyWTJObFlXRXpOek5pWmpKak9HRmpZakl5WlRZd1lqWmhOVGhpWlRZLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWUtanVXYnZTM2xpcURsaUtubnBvX2xzTHprdXBybHQ1NCZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


34
ssr://Y2hlbmdkdS1jaGluYS1wcm94eTIuZGFycmVuLWxlZS5uZXQ6ODA4MTpvcmlnaW46cmM0LW1kNTpwbGFpbjpPREE0TVEvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91V2JtLVczbmVlY2dlYUlrT21EdmVXNGdpRG5sTFhrdjZFJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


35
ssr://OTEuMjE5LjIzNy4xMTk6OTAwMTpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46WjJWMGRuQnVNakF4T1RBMU1ERS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VXTWlPZUptZVdJcVMxQ2RXUmhjR1Z6ZEEmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


36
ssr://NDUuNTYuOTAuMTc0OjQ0MzpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46T1dRMlkyTmxZV0V6TnpOaVpqSmpPR0ZqWWpJeVpUWXdZalpoTlRoaVpUWS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VlLWp1V2J2UzNsaXFEbGlLbm5wb19sc0x6a3Vwcmx0NTQmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


37
ssr://Y2hlbmdkdS1jaGluYS1wcm94eTEuZGFycmVuLWxlZS5uZXQ6ODA4MTpvcmlnaW46cmM0LW1kNTpwbGFpbjpPREE0TVEvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91V2JtLVczbmVlY2dlYUlrT21EdmVXNGdpRG5sTFhrdjZFJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


38
ssr://MTA0LjE2Ny45Ny4xNjQ6NTQzOmF1dGhfc2hhMV92NDpyYzQtbWQ1Omh0dHBfc2ltcGxlOmFIUjBjRG92TDNRdVkyNHZVa1F3UkRkemVBLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdVdLb09hTHYtV2tweTNscm9ubHBLZm5sYVUmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


39
ssr://MTAzLjEzNS4xMDIuMTE5OjEyNTc6YXV0aF9zaGExX3Y0OmNoYWNoYTIwOmh0dHBfc2ltcGxlOk5UYzROVFExTlEvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91ZS1qdVdidlMzbGlxRGxpS25ucG9fbHNMemt1cHJsdDU0Jmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


40
ssr://MTM5LjE2Mi43OC4xNTU6NDYxMTI6b3JpZ2luOmFlcy0yNTYtY2ZiOnBsYWluOlozVnpXR3RFY1VjMWVFeEovP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91YVhwZWFjckMxVWIydDVidyZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


41
ssr://MS5oaW5ldC5zc3IuZ29sZDo2NTUzMTphdXRoX2FlczEyOF9zaGExOmFlcy0yNTYtY2ZiOmh0dHBfc2ltcGxlOlUxTlNMa2R2YkdSQUl6WTFOVE14Lz9wcm90b3BhcmFtPU5qRTJOem81WXpkcE1YayZyZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdVdQc09hNXZpM29oN3JuZ2FQbm5JRWdiM0lnNVktdzU0R2o1NXlCJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


42
ssr://NDUuNzkuNjQuMTA5OjQ0MzpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46T1dRMlkyTmxZV0V6TnpOaVpqSmpPR0ZqWWpJeVpUWXdZalpoTlRoaVpUWS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VlLWp1V2J2UzNsaXFEbGlLbm5wb19sc0x6a3Vwcmx0NTQmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


43
ssr://MTk5LjIyMy4xMTkuODM6ODA5NzpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46WlVsWE1FUnVhelk1TkRVMFpUWnVVM2QxYzNCMk9VUnRVekl3TVhSUk1FUS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VlLWp1V2J2U0EmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


44
ssr://MTg1LjI0My41Ny4yMjE6ODA4MDphdXRoX3NoYTFfdjQ6cmM0LW1kNTpodHRwX3NpbXBsZTphSFIwY0RvdkwzUXVZMjR2UlVkS1NYbHliQS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VlLWp1V2J2UzNsaXFEbGlLbm5wb19sc0x6a3Vwcmx0NTQmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


45
ssr://MTcyLjEwNS4yMjguNjI6MzY1MDA6b3JpZ2luOmFlcy0yNTYtY2ZiOnBsYWluOlNHdFBkazVzYmxSNlkyMUYvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91YVhwZWFjckMxVWIydDVidyZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


46
ssr://MTQyLjkzLjMyLjgyOjIxOTcyOm9yaWdpbjphZXMtMjU2LWNmYjpwbGFpbjpiMWxaU0dsbGNrNVdOVlJhLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWlMc2VXYnZTM29pN0htb0x6bGhiQSZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


47
ssr://MjA5LjI1MC4yMjkuMjI5OjM0NTY6YXV0aF9zaGExX3Y0OmFlcy0yNTYtY2ZiOnBsYWluOmQzZDNMbVJoY25KbGJteHBkWGRsYVM1amIyMC8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VpTHNlV2J2UzNvaTdIbW9MemxoYkEmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


48
ssr://Y2hlbmdkdS1jaGluYS1wcm94eTIuZGFycmVuLWxlZS5uZXQ6ODA4MTpvcmlnaW46cmM0LW1kNTpwbGFpbjpPREE0TVEvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91V2JtLVczbmVlY2dlYUlrT21EdmVXNGdpRG5sTFhrdjZFJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


49
ssr://MTM4LjY4LjEyOC4yMzI6NDI0NjE6b3JpZ2luOmFlcy0yNTYtY2ZiOnBsYWluOlYzUnpUelZxUmxaT1Uxb3cvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91aUxzZVdidlMzb2k3SG1vTHpsaGJBJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


50
ssr://OTEuMjE5LjIzNy4xMTk6OTAwMTpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46WjJWMGRuQnVNakF4T1RBMU1ERS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VXTWlPZUptZVdJcVMxQ2RXUmhjR1Z6ZEEmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


51
ssr://MTcyLjEwNS4yMjguNjI6MzY0OTU6b3JpZ2luOmFlcy0yNTYtY2ZiOnBsYWluOlNHdFBkazVzYmxSNlkyMUYvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91YVhwZWFjckMxVWIydDVidyZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


52
ssr://MTcyLjEwNC42Ny42MzoxOTE0NjpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46VGt4eE9WTlVkbE5pY1dJeS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VhWHBlYWNyQzFVYjJ0NWJ3Jmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


53
ssr://MTg1LjE4Ni4yNDUuNDQ6ODA5NzpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46WlVsWE1FUnVhelk1TkRVMFpUWnVVM2QxYzNCMk9VUnRVekl3TVhSUk1FUS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VpTnQtV0ZzQzFPYjNKMGFDQkliMnhzWVc1ayZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


54
ssr://MjMuMjU0LjIwNC4yMDM6ODA5NzpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46WlVsWE1FUnVhelk1TkRVMFpUWnVVM2QxYzNCMk9VUnRVekl3TVhSUk1FUS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VlLWp1V2J2UzNsalk3bm01dnBvYl9sdDU0Jmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


55
ssr://MjA0LjQ1LjE4Mi4zNDo4MDk3Om9yaWdpbjphZXMtMjU2LWNmYjpwbGFpbjpaVWxYTUVSdWF6WTVORFUwWlRadVUzZDFjM0IyT1VSdFV6SXdNWFJSTUVRLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdVM0cmVXYnZTM2x1Yl9rdUp3Jmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


56
ssr://MTcyLjEwNS4yMjguNjI6MzY0OTk6b3JpZ2luOmFlcy0yNTYtY2ZiOnBsYWluOlNHdFBkazVzYmxSNlkyMUYvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91YVhwZWFjckMxVWIydDVidyZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


57
ssr://MTk4LjI1NS4xMDMuNjI6ODA5NzpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46WlVsWE1FUnVhelk1TkRVMFpUWnVVM2QxYzNCMk9VUnRVekl3TVhSUk1FUS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VlLWp1V2J2U0EmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


58
ssr://NTAuNy4xMi4xNDc6ODA5NzpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46WlVsWE1FUnVhelk1TkRVMFpUWnVVM2QxYzNCMk9VUnRVekl3TVhSUk1FUS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VXM3RPaWx2eTFUdzZOdklGQmhkV3h2Jmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


59
ssr://NDUuMzMuMzIuMTUyOjgwOTc6b3JpZ2luOmFlcy0yNTYtY2ZiOnBsYWluOlpVbFhNRVJ1YXpZNU5EVTBaVFp1VTNkMWMzQjJPVVJ0VXpJd01YUlJNRVEvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91ZS1qdVdidlMzbGlxRGxpS25ucG9fbHNMemt1cHJsdDU0Jmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


60
ssr://MTQxLjk4LjIxMy4xNjM6OTg4Om9yaWdpbjpjaGFjaGEyMDpwbGFpbjpOVTAxTjJ0bk1URmpNakUwY1VSdFN3Lz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdVdrcC1tZnFlYXdrZVdidlMzcHBwYmxzSlRuaWJubGlLdmx1SUkmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


61
ssr://NjguMTgzLjM4LjgwOjE0NTI3Om9yaWdpbjphZXMtMjU2LWNmYjpwbGFpbjpOVTgxYkRCSlQxRkJaWFZoLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWlMc2VXYnZTM29pN0htb0x6bGhiQSZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


62
ssr://NjguMTgzLjM4LjgwOjE0NTIzOm9yaWdpbjphZXMtMjU2LWNmYjpwbGFpbjpOVTgxYkRCSlQxRkJaWFZoLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWlMc2VXYnZTM29pN0htb0x6bGhiQSZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


63
ssr://MTA0LjIyNC4xMzguMTQ2Ojg5OmF1dGhfc2hhMV92NDphZXMtMjU2LWNmYjpwbGFpbjpNalF4TXpFMC8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VlLWp1V2J2UzNsaXFEbGlLbm5wb19sc0x6a3Vwcmx0NTQmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


64
ssr://OTkuNzkuMTIzLjEwMjozMjM2ODphdXRoX2FlczEyOF9zaGExOmFlcy0yNTYtY2ZiOnBsYWluOmIyaHpjM0kyTmpZLz9wcm90b3BhcmFtPU1qVTNOanBKTVZaRFJESSZyZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdVdLb09hTHYtV2tweTNscm9ubHBLZm5sYVUmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


65
ssr://OTEuMTg4LjIyMy43Mjo4MDgwOmF1dGhfc2hhMV92NDpyYzQtbWQ1Omh0dHBfc2ltcGxlOmFIUjBjRG92TDNRdVkyNHZSVWRLU1hseWJBLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdVNfaE9lOWwtYVdyeTFPYjNadmMybGlhWEp6YXlCUFlteGhjM1EmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


66
ssr://NDUuNzYuODAuMjIxOjQ0MzpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46Vms5VFEyRXhXa2MvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91Vy10LVdidlMxSVpYTnpaUSZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


67
ssr://ZGV2Lnh4dy5jYTo4Mzg4OmF1dGhfc2hhMV92NDpjaGFjaGEyMC1pZXRmOnRsczEuMl90aWNrZXRfYXV0aDpNVEl6TkRVMk56Zy8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VXS29PYUx2LVdrcHkzbHJvbmxwS2ZubGFVJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


68
ssr://MTkyLjIxMC4xOTAuMTAxOjI1NTgxOm9yaWdpbjphZXMtMjU2LWNmYjpwbGFpbjpjM1Z3WlhKemMzSnVaWFEvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91ZS1qdVdidlMza3ZJcmxpS25vcjdya3ZJcmx0NTQmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


69
ssr://NDYuMjkuMTYyLjQ2OjEwMjY6b3JpZ2luOnJjNC1tZDU6cGxhaW46T1RGMmNHNHVZMlkvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91U19oT2U5bC1hV3J5MU5iM05qYjNjJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


70
ssr://MTg1LjEzNS44Mi44MDoyMzMzOm9yaWdpbjphZXMtMTI4LWN0cjpwbGFpbjpaRzkxWWk1cGJ3Lz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdVNfaE9lOWwtYVdyeTFOYjNOamIzYyZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


71
ssr://OTEuMTg4LjIyMy43Mjo4MDphdXRoX3NoYTFfdjQ6cmM0LW1kNTpodHRwX3NpbXBsZTpkQzV0WlM5VFUxSlRWVUkvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE9sSjFjM05wWVEmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


72
ssr://OTEuMTg4LjIyMy43Mjo1NDM6YXV0aF9zaGExX3Y0OnJjNC1tZDU6aHR0cF9zaW1wbGU6YUhSMGNEb3ZMM1F1WTI0dlVrUXdSRGR6ZUEvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE9sSjFjM05wWVEmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


73
ssr://MTg1LjI0My41Ny4yMjE6NTQzOmF1dGhfc2hhMV92NDpyYzQtbWQ1Omh0dHBfc2ltcGxlOmFIUjBjRG92TDNRdVkyNHZVa1F3UkRkemVBLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWUtanVXYnZTM2xpcURsaUtubnBvX2xzTHprdXBybHQ1NCZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


74
ssr://MTg1LjE3My45NC4yNDQ6MjMzMzphdXRoX3NoYTFfdjQ6YWVzLTEyOC1jdHI6cGxhaW46Wkc5MVlpNXBidy8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VTX2hPZTlsLWFXcnkxTmIzTmpiM2MmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


75
ssr://MTkzLjE0OC43MC4yNDE6ODA6YXV0aF9jaGFpbl9hOm5vbmU6aHR0cF9zaW1wbGU6VUVobmJtRlovP3Byb3RvcGFyYW09TVRvd1YxcDBZVGMmcmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VlOWwtbXByT1d3dk9TNm1pMUNkV04xY21YRm4zUnAmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


76
ssr://MTk4Ljc0LjU4LjQwOjQ0MzpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46T1dRMlkyTmxZV0V6TnpOaVpqSmpPR0ZqWWpJeVpUWXdZalpoTlRoaVpUWS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VlLWp1V2J2UzNtbHJEbXM3M29wYl9sdDU0Jmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


77
ssr://MTcyLjEwNC4yNC4yMTo0NDM6b3JpZ2luOmFlcy0yNTYtY2ZiOnBsYWluOk9XUTJZMk5sWVdFek56TmlaakpqT0dGallqSXlaVFl3WWpaaE5UaGlaVFkvP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91ZS1qdVdidlMzbWxyRG1zNzNvcGJfbHQ1NCZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


78
ssr://MTAzLjEyMi4yMy4xMzE6ODA6YXV0aF9hZXMxMjhfbWQ1OnJjNC1tZDU6aHR0cF9zaW1wbGU6WTBSRldtcEdSbFkvP3Byb3RvcGFyYW09TWpJeU5UcHRhV0Z1YkdsMSZyZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdVdQc09hNXZpM29oN3JuZ2FQbm5JRWdiM0lnNVktdzU0R2o1NXlCJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


79
ssr://MTg1LjIxOS4xMzIuMjI4OjgwOmF1dGhfYWVzMTI4X21kNTpyYzQtbWQ1Omh0dHBfc2ltcGxlOlkwUkZXbXBHUmxZLz9wcm90b3BhcmFtPU1qSXlOVHB0YVdGdWJHbDEmcmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VXY24taUFzLVdGdGkza3ZJcm1scV9sbmFibHVJUGxzSlEmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


80
ssr://MTcyLjEwNC4zMS4xNTI6NDQzOm9yaWdpbjphZXMtMjU2LWNmYjpwbGFpbjpPV1EyWTJObFlXRXpOek5pWmpKak9HRmpZakl5WlRZd1lqWmhOVGhpWlRZLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWUtanVXYnZTM21sckRtczczb3BiX2x0NTQmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


81
ssr://MTcyLjEwNC4yMTUuMjQ6NDQzOm9yaWdpbjphZXMtMjU2LWNmYjpwbGFpbjpPV1EyWTJObFlXRXpOek5pWmpKak9HRmpZakl5WlRZd1lqWmhOVGhpWlRZLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWUtanVXYnZTM21sckRtczczb3BiX2x0NTQmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


82
ssr://MTcyLjEwNC4yMDkuMTk4OjQ0MzpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46T1dRMlkyTmxZV0V6TnpOaVpqSmpPR0ZqWWpJeVpUWXdZalpoTlRoaVpUWS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VlLWp1V2J2UzNtbHJEbXM3M29wYl9sdDU0Jmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


83
ssr://MTcyLjEwNC4yMTcuMTM4OjQ0MzpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46T1dRMlkyTmxZV0V6TnpOaVpqSmpPR0ZqWWpJeVpUWXdZalpoTlRoaVpUWS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VlLWp1V2J2UzNtbHJEbXM3M29wYl9sdDU0Jmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


84
ssr://MTcyLjEwNC4yMTguMjMwOjQ0MzpvcmlnaW46YWVzLTI1Ni1jZmI6cGxhaW46T1dRMlkyTmxZV0V6TnpOaVpqSmpPR0ZqWWpJeVpUWXdZalpoTlRoaVpUWS8_cmVtYXJrcz1VMU5TVkU5UFRGOU9iMlJsT3VlLWp1V2J2UzNtbHJEbXM3M29wYl9sdDU0Jmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


85
ssr://MzQuMjQ0LjI0LjIzNTozMjM2ODphdXRoX2FlczEyOF9zaGExOmFlcy0yNTYtY2ZiOnBsYWluOmIyaHpjM0kyTmpZLz9wcm90b3BhcmFtPU1qVTNOanBKTVZaRFJESSZyZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWVJc2VXd2xPV0ZzQzNsZ0t2bWxxX25pYm5ubklFJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


86
ssr://amktbG5xLWhrdC0wMS5saXN1YW5sYW9qaS5jbG91ZDo1ODY6YXV0aF9hZXMxMjhfc2hhMTphZXMtMjU2LWNmYjpodHRwX3NpbXBsZTpUa2RETGxOVC8_cHJvdG9wYXJhbT1NekV6TkRJNlpHVkRia2RsJnJlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91bW1tZWE0cnkxTGQzVnVJRlJ2Ym1jJmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


87
ssr://MzUuMTgxLjQ5LjkyOjMyMzY4OmF1dGhfYWVzMTI4X3NoYTE6YWVzLTI1Ni1jZmI6cGxhaW46YjJoemMzSTJOalkvP3Byb3RvcGFyYW09TWpVM05qcEpNVlpEUkRJJnJlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91YXpsZVdidlMzRGpteGxMV1JsTFVaeVlXNWpaUSZncm91cD1WMWRYTGxOVFVsUlBUMHd1UTA5Tg


88
ssr://NjYuMTc1LjIyMy4yMjo4MDk3Om9yaWdpbjphZXMtMjU2LWNmYjpwbGFpbjpaVWxYTUVSdWF6WTVORFUwWlRadVUzZDFjM0IyT1VSdFV6SXdNWFJSTUVRLz9yZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWUtanVXYnZTM2x2SmZsa0lubHNMemt1cHJsdDU0Jmdyb3VwPVYxZFhMbE5UVWxSUFQwd3VRMDlO


89
ssr://MzUuMTY2Ljg1LjIwNjozMjM2ODphdXRoX2FlczEyOF9zaGExOmFlcy0yNTYtY2ZiOnBsYWluOmIyaHpjM0kyTmpZLz9wcm90b3BhcmFtPU1qVTNOanBKTVZaRFJESSZyZW1hcmtzPVUxTlNWRTlQVEY5T2IyUmxPdWUtanVXYnZTM2t2NFRsaTVMbGhvamx0NTQmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4


90
ssr://MTkyLjE1NC4xOTcuODk6NDA4OTk6b3JpZ2luOmFlcy0yNTYtY2ZiOnBsYWluOldUTnZSWEYxVFZkUE1rUk0vP3JlbWFya3M9VTFOU1ZFOVBURjlPYjJSbE91ZS1qdVdidlMza3VwN2xpS25tb1pIcGdxUGx0NTQmZ3JvdXA9VjFkWExsTlRVbFJQVDB3dVEwOU4




若本项目对您有所帮助，欢迎Star

",950
cardiomoon/processR,R,"R package processR  

The processR package aims to be a user-friendly way to perform moderation, mediation, moderated mediation and moderated moderation in R. This package is inspired from famous PROCESS macro for SPSS and SAS created by Andrew Hayes.
processR is under the GPL-3 license. For a commercial license, please
[contact me](mailto: cardiomoon@gmail.com).
If you find processR
useful, please consider supporting its development!





PROCESS macro and R package processR
Andrew F. Hayes was not involved in the development of this R package or application and cannot attest to the quality of the computations implemented in the code you are using. Use at your own risk.
Installation
You can install the processR package from github.
if(!require(devtools)) install.packages(""devtools"")
devtools::install_github(""cardiomoon/processR"")
What does this package cover ?
The processR package covers moderation, mediation, moderated mediation and moderated moderation with R. Supporting models are as follows.
library(processR)
sort(pmacro$no)
 [1]  0.0  1.0  2.0  3.0  4.0  4.2  5.0  6.0  6.3  6.4  7.0  8.0  9.0 10.0
[15] 11.0 12.0 13.0 14.0 15.0 16.0 17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0
[29] 28.0 29.0 30.0 31.0 35.0 36.0 40.0 41.0 45.0 49.0 50.0 58.0 59.0 60.0
[43] 61.0 62.0 63.0 64.0 65.0 66.0 67.0 74.0 75.0 76.0

Currently, 52 models are supported.
Example: Moderated Mediation (PROCESS macro model 8)
I will explain functions of processR package by a example.
Concept Diagram and Statistical Diagram
You can draw concept diagram and statistical diagram easily. For example, you can draw the concept diagram for PROCESS macro model 8.
pmacroModel(8)

You can draw statistical diagram of this model.
statisticalDiagram(8)

Full vignette
You can see full vignette for model 8 at http://rpubs.com/cardiomoon/468602
Shiny App
I have developed a shiny app. You can test the app at http://web-r.space:3838/processR.
I will appreciate any comment.
How to perform this analysis with shiny app
You can see how to perform this analysis at http://rpubs.com/cardiomoon/468600
Sample powerpoint file
In the shiny app, you can download the analysis results as a powerpoint file. You can download the sample file model8.pptx - view with office web viewer.
",4
GoogleCloudPlatform/terraform-google-sql-db,HCL,"terraform-google-sql
terraform-google-sql makes it easy to create Google CloudSQL instance and implement high availability settings.
This module consists of the following submodules:

mysql
postgresql

See more details in each module's README.
Requirements
Installation Dependencies

terraform 0.11.x
terraform-provider-google plugin v1.12.x

Configure a Service Account
In order to execute this module you must have a Service Account with the following:
Roles

roles/cloudsql.admin

Enable APIs
In order to operate with the Service Account you must activate the following APIs on the project where the Service Account was created:

Cloud SQL API

Service Account Credentials
You can pass the service account credentials into this module by setting the following environment variables:

GOOGLE_CREDENTIALS
GOOGLE_CLOUD_KEYFILE_JSON
GCLOUD_KEYFILE_JSON

See more details.
Testing
Requirements

bundler
ruby 2.5.x
python 2.7.x
terraform-docs 0.4.5
google-cloud-sdk

Generate docs automatically
$ make generate_docs
Integration Test
The integration tests for this module leverage kitchen-terraform and kitchen-inspec.
You must set up by manually before running the integration test:
for instance in mysql-simple mysql-ha postgresql-simple postgresql-ha; do
  cp ""test/fixtures/$instance/terraform.tfvars.example"" ""test/fixtures/$instance/terraform.tfvars""
  $EDITOR ""test/fixtures/$instance/terraform.tfvars""
done
And then, you should pass the service account credentials for running inspec by setting the following environment variables:

GOOGLE_APPLICATION_CREDENTIALS

The tests will do the following:

Perform bundle install command

Installs test-kitchen, kitchen-terraform and kitchen-inspec


Perform bundle exec kitchen create command

Performs terraform init


Perform bundle exec kitchen converge command

Performs terraform apply -auto-approve


Perform bundle exec kitchen verify command

Performs inspec tests


Perform bundle exec kitchen destroy command

Performs terraform destroy -force



You can use the following command to run the integration test in the root directory.
$ make test_integration
Linting
The makefile in this project will lint or sometimes just format any shell, Python, golang, Terraform, or Dockerfiles. The linters will only be run if the makefile finds files with the appropriate file extension.
All of the linter checks are in the default make target, so you just have to run
$ make -s
The -s is for 'silent'. Successful output looks like this
Running shellcheck
Running flake8
Running go fmt and go vet
Running terraform validate
Running terraform fmt
Running hadolint on Dockerfiles
Checking for required files
The following lines have trailing whitespace
Generating markdown docs with terraform-docs

The linters
are as follows:

Shell - shellcheck. Can be found in homebrew
Python - flake8. Can be installed with pip install flake8
Golang - gofmt. gofmt comes with the standard golang installation. golang
-s a compiled language so there is no standard linter.
Terraform - terraform has a built-in linter in the terraform validate command.
Dockerfiles - hadolint. Can be found in homebrew

",38
pando-project/iotjs,C,"IoT.js: Platform for Internet of Things with JavaScript






You can find project details on our project page and wiki.
Memory usage and Binary footprint are measured at here with real target daily.
The following table shows the latest results on the devices:



Raspberry Pi 3





Raspberry Pi 2



STM32F4-Discovery




IRC channel: #iotjs on freenode
Mailing list: iotjs-dev@groups.io, you can subscribe here and access the mailing list archive here.
Quick Start
Getting the sources
git clone https://github.com/pando-project/iotjs.git
cd iotjs
How to Build
tools/build.py
How to Test
tools/testrunner.py build/x86_64-linux/debug/bin/iotjs
Trying out with a REPL
build/x86_64-linux/debug/bin/iotjs tools/repl.js
For Additional information see Getting Started.
Documentation

Getting Started
API Reference

License
IoT.js is Open Source software under the Apache 2.0 license. Complete license and copyright information can be found within the code.


Copyright 2015-present Samsung Electronics Co., Ltd. and other contributors


Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.


Copyright Node.js contributors. All rights reserved.


Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to
deal in the Software without restriction, including without limitation the
rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
sell copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:


The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.


THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
IN THE SOFTWARE.


This license applies to parts of '*.js' files in '/src/js', implementing node.js
compatible API, originating from the https://github.com/nodejs/node repository:

",2032
microsoft/BotFramework-WebChat,JavaScript,"
Click here to find out what's new for //build2019!
Bot Framework Web Chat





This repository contains code for the Bot Framework Web Chat component.  The Bot Framework Web Chat component is a highly-customizable web-based client for the Bot Framework V4 SDK. The Bot Framework SDK v4 enable developers to model conversation and build sophisticated bot applications.
This repo is part of the Microsoft Bot Framework - a comprehensive framework for building enterprise-grade conversational AI experiences.
Migrating from Web Chat v3 to v4
There are three possible paths that migration might take when migrating from v3 to v4. First, please compare your beginning scenario:
My current website integrates Web Chat using an <iframe> element obtained from Azure Bot Services. I want to upgrade to v4.
Starting from May 2019, we are rolling out v4 to websites that integrate Web Chat using <iframe> element. Please refer to the embed documentation for information on integrating Web Chat using <iframe>.
My website is integrated with Web Chat v3 and uses customization options provided by Web Chat, no customization at all, or very little of my own customization that was not available with Web Chat.
Please follow the implementation of sample 01.c.getting-started-migration to convert your webpage from v3 to v4 of Web Chat.
My website is integrated with a fork of Web Chat v3. I have implemented a lot of customization in my version of Web Chat, and I am concerned v4 is not compatible with my needs.
One of our team's favorite things about v4 of Web Chat is the ability to add customization without the need to fork Web Chat. Although this creates additional overhead for v3 users who forked Web Chat previously, we will do our best to support customers making the bump. Please use the following suggestions:

Take a look at the implementation of sample 01.c.getting-started-migration. This is a great starting place to get Web Chat up and running.
Next, please go through the samples list to compare your customization requirements to what Web Chat has already provided support for. These samples are made up of commonly asked-for features for Web Chat.
If one or more of your features is not available in the samples, please look through our open and closed issues, Samples label, and the Migration Support label to search for sample requests and/or customization support for a feature you are looking for. Adding your comment to open issues will help the team prioritize requests that are in high demand, and we encourage participation in our community.
If you did not find your feature in the list of open requests, please feel free to file your own request. Just like the item above, other customers adding comments to your open issue will help us prioritize which features are most commonly needed across Web Chat users.
Finally, if you need your feature as soon as possible, we welcome pull requests to Web Chat. If you have the coding experience to implement the feature yourself, we would very much appreciate the additional support! Creating the feature yourself will mean that it is available for your use on Web Chat more quickly, and that other customers looking for the same or similar feature may utilize your contribution.
Make sure to check out the rest of this README to learn more about v4.

How to use

For previous versions of Web Chat (v3), visit the Web Chat v3 branch.

First, create a bot using Azure Bot Service.
Once the bot is created, you will need to obtain the bot's Web Chat secret in Azure Portal. Then use the secret to generate a token and pass it to your Web Chat.
Here is how how you can add Web Chat control to your website:
<!DOCTYPE html>
<html>
  <body>
    <div id=""webchat"" role=""main""></div>
    <script src=""https://cdn.botframework.com/botframework-webchat/latest/webchat.js""></script>
    <script>
      window.WebChat.renderWebChat({
        directLine: window.WebChat.createDirectLine({ token: 'YOUR_DIRECT_LINE_TOKEN' }),
        userID: 'YOUR_USER_ID',
        username: 'Web Chat User',
        locale: 'en-US',
        botAvatarInitials: 'WC',
        userAvatarInitials: 'WW'
      }, document.getElementById('webchat'));
    </script>
  </body>
</html>

userID, username, locale, botAvatarInitials, and userAvatarInitials are all optional parameters to pass into the renderWebChat method. To learn more about Web Chat props, look at the Web Chat API Reference section of this README.


Integrate with JavaScript
Web Chat is designed to integrate with your existing website using JavaScript or React. Integrating with JavaScript will give you moderate styling and customizability.
You can use the full, typical webchat package that contains the most typically used features.
<!DOCTYPE html>
<html>
  <body>
    <div id=""webchat"" role=""main""></div>
    <script src=""https://cdn.botframework.com/botframework-webchat/latest/webchat.js""></script>
    <script>
      window.WebChat.renderWebChat({
        directLine: window.WebChat.createDirectLine({ token: 'YOUR_DIRECT_LINE_TOKEN' }),
        userID: 'YOUR_USER_ID'
      }, document.getElementById('webchat'));
    </script>
  </body>
</html>
See the working sample of the full Web Chat bundle.
Integrate with React
For full customizability, you can use React to recompose components of Web Chat.
To install the production build from NPM, run npm install botframework-webchat.
import { DirectLine } from 'botframework-directlinejs';
import React from 'react';
import ReactWebChat from 'botframework-webchat';

export default class extends React.Component {
  constructor(props) {
    super(props);

    this.directLine = new DirectLine({ token: 'YOUR_DIRECT_LINE_TOKEN' });
  }

  render() {
    return (
      <ReactWebChat directLine={ this.directLine } userID=""YOUR_USER_ID"" />
      element
    );
  }
}

You can also run npm install botframework-webchat@master to install a development build that is synced with Web Chat's GitHub master branch.

See a working sample of Web Chat rendered via React.
Customize Web Chat UI
Web Chat is designed to be customizable without forking the source code. The table below outlines what kind of customizations you can achieve when you are importing Web Chat in different ways. This list is not exhaustive.




CDN bundle
React




Change colors
✔️
✔️


Change sizes
✔️
✔️


Update/replace CSS styles
✔️
✔️


Listen to events
✔️
✔️


Interact with hosting webpage
✔️
✔️


Custom render activities

✔️


Custom render attachments

✔️


Add new UI components

✔️


Recompose the whole UI

✔️



See more about customizing Web Chat to learn more on customization.
Samples list



               Sample Name                    
Description
Link




01.a.getting-started-full-bundle
Introduces Web Chat embed from a CDN, and demonstrates a simple, full-featured Web Chat. This includes Adaptive Cards, Cognitive Services, and Markdown-It dependencies.
Full Bundle Demo


01.b.getting-started-es5-bundle
Introduces full-featured Web Chat embed with backwards compatibility for ES5 browsers using Web Chat's ES5 ponyfill.
ES5 Bundle Demo


01.c.getting-started-migration
Demonstrates how to migrate from your Web Chat v3 bot to v4.
Migration Demo


02.a.getting-started-minimal-bundle
Introduces the minimized CDN with only basic dependencies. This does NOT include Adaptive Cards, Cognitive Services dependencies, or Markdown-It dependencies.
Minimal Bundle Demo


02.b.getting-started-minimal-markdown
Demonstrates how to add the CDN for Markdown-It dependency on top of the minimal bundle.
Minimal with Markdown Demo


03.a.host-with-react
Demonstrates how to create a React component that hosts the full-featured Web Chat.
Host with React Demo


03.b.host-with-Angular
Demonstrates how to create an Angular component that hosts the full-featured Web Chat.
Host with Angular Demo


04.a.display-user-bot-initials-styling
Demonstrates how to display initials for both Web Chat participants.
Bot initials Demo


04.b.display-user-bot-images-styling
Demonstrates how to display images and initials for both Web Chat participants.
User images Demo


05.a.branding-webchat-styling
Introduces the ability to style Web Chat to match your brand. This method of custom styling will not break upon Web Chat updates.
Branding Web Chat Demo


05.b.idiosyncratic-manual-styling
Demonstrates how to make manual style changes, and is a more complicated and time-consuming way to customize styling of Web Chat. Manual styles may be broken upon Web Chat updates.
Idiosyncratic Styling Demo


05.c.presentation-mode-styling
Demonstrates how to set up Presentation Mode, which displays chat history but does not show the send box, and disables the interactivity of Adaptive Cards.
Presentation Mode Demo


05.d.hide-upload-button-styling
Demonstrates how to hide file upload button via styling.
Hide Upload Button Demo


06.a.cognitive-services-bing-speech-js
Introduces speech-to-text and text-to-speech ability using the (deprecated) Cognitive Services Bing Speech API and JavaScript.
Bing Speech with JS Demo


06.b.cognitive-services-bing-speech-react
Introduces speech-to-text and text-to-speech ability using the (deprecated) Cognitive Services Bing Speech API and React.
Bing Speech with React Demo


06.c.cognitive-services-speech-services-js
Introduces speech-to-text and text-to-speech ability using Cognitive Services Speech Services API.
Speech Services with JS Demo


06.d.speech-web-browser
Demonstrates how to implement text-to-speech using Web Chat's browser-based Web Speech API. (link to W3C standard in the sample)
Web Speech API Demo


06.e.cognitive-services-speech-services-with-lexical-result
Demonstrates how to use lexical result from Cognitive Services Speech Services API.
Lexical Result Demo


06.f.hybrid-speech
Demonstrates how to use both browser-based Web Speech API for speech-to-text, and Cognitive Services Speech Services API for text-to-speech.
Hybrid Speech Demo


07.a.customization-timestamp-grouping
Demonstrates how to customize timestamps by showing or hiding timestamps and changing the grouping of messages by time.
Timestamp Grouping Demo


07.b.customization-send-typing-indicator
Demonstrates how to send typing activity when the user start typing on the send box.
User Typing Indicator Demo


08.customization-user-highlighting
Demonstrates how to customize the styling of activities based whether the message is from the user or the bot.
User Highlighting Demo


09.customization-reaction-buttons
Introduces the ability to create custom components for Web Chat that are unique to your bot's needs. This tutorial demonstrates the ability to add reaction emoji such as 👍 and 👎 to conversational activities.
Reaction Buttons Demo


10.a.customization-card-components
Demonstrates how to create custom activity card attachments, in this case GitHub repository cards.
Card Components Demo


10.b.customization-password-input
Demonstrates how to create custom activity for password input.
Password Input Demo


11.customization-redux-actions
Advanced tutorial: Demonstrates how to incorporate redux middleware into your Web Chat app by sending redux actions through the bot. This example demonstrates manual styling based on activities between bot and user.
Redux Actions Demo


12.customization-minimizable-web-chat
Advanced tutorial: Demonstrates how to add the Web Chat interface to your website as a minimizable show/hide chat box.
Minimizable Web Chat Demo


13.customization-speech-ui
Advanced tutorial: Demonstrates how to fully customize key components of your bot, in this case speech, which entirely replaces the text-based transcript UI and instead shows a simple speech button with the bot's response.
Speech UI Demo


14.customization-piping-to-redux
Advanced tutorial: Demonstrates how to pipe bot activities to your own Redux store and use your bot to control your page through bot activities and Redux.
Piping to Redux Demo


15.a.backchannel-piggyback-on-outgoing-activities
Advanced tutorial: Demonstrates how to add custom data to every outgoing activities.
Backchannel Piggybacking Demo


15.b.incoming-activity-event
Advanced tutorial: Demonstrates how to forward all incoming activities to a JavaScript event for further processing.
Incoming Activity Demo


15.c.programmatic-post-activity
Advanced tutorial: Demonstrates how to send a message programmatically.
Programmatic Posting Demo


15.d.backchannel-send-welcome-event
Advanced tutorial: Demonstrates how to send welcome event with client capabilities such as browser language.
Welcome Event Demo


16.customization-selectable-activity
Advanced tutorial: Demonstrates how to add custom click behavior to each activity.
Selectable Activity Demo


17.chat-send-history
Advanced tutorial: Demonstrates the ability to save user input and allow the user to step back through previous sent messages.
Chat Send History Demo


18.customization-open-url
Advanced tutorial: Demonstrates how to customize the open URL behavior.
Customize Open URL Demo



Web Chat API Reference
There are several properties that you might pass into your Web Chat React Component (<ReactWebChat>) or the renderWebChat() method. Feel free to examine the source code starting with packages/component/src/Composer.js. Below is a short description of the available props.



Property
Description




activityMiddleware
A chain of middleware, modeled after Redux middleware, that allows the developer to add new DOM components on the currently existing DOM of Activities. The middleware signature is the following: options => next => card => children => next(card)(children).


activityRenderer
The ""flattened"" version of activityMiddleware, similar to the store enhancer concept in Redux.


adaptiveCardHostConfig
Pass in a custom Adaptive Cards host config.


attachmentMiddleware
A chain of middleware that allows the developer to add their own custom HTML Elements on attachments. The signature is the following: options => next => card => next(card).


attachmentRenderer
The ""flattened"" version of attachmentMiddleware.


cardActionMiddleware
A chain of middleware that allows the developer to modify card actions, like Adaptive Cards or suggested actions. The middleware signature is the following: cardActionMiddleware: () => next => ({ cardAction, getSignInUrl }) => next(cardAction)


createStore
A chain of middleware that allows the developer to modify the store actions. The middleware signature is the following: createStore: ({}, ({ dispatch }) => next => action => next(cardAction)


directLine
Specify the DirectLine object with DirectLine token.


disabled
Disable the UI (i.e. for presentation mode) of Web Chat.


grammars
Specify a grammar list for Speech  (Bing Speech or Cognitive Services Speech Services).


groupTimeStamp
Change default settings for timestamp groupings.


locale
Indicate the default language of Web Chat. Four letter codes (such as en-US) are strongly recommended.


renderMarkdown
Change the default Markdown renderer object.


sendTypingIndicator
Display a typing signal from the user to the bot to indicate that the user is not idling.


store
Specify a custom store, e.g. for adding programmatic activity to the bot.


styleOptions
Object that stores customization values for your styling of Web Chat. For the complete list of (frequently updated) default style options, please see the defaultStyleSetOptions.js file.


styleSet
The non-recommended way of overriding styles.


userID
Specify a userID. There are two ways to specify the userID: in props, or in the token when generating the token call (createDirectLine()). If both methods are used to specify the userID, the token userID property will be used, and a console.warn will appear during runtime. If the userID is provided via props but is prefixed with 'dl', e.g. 'dl_1234', the value will be thrown and a new ID generated. If userID is not specified, it will default to a random user ID. Multiple users sharing the same user ID is not recommended; their user state will be shared.


username
Specify a username.


webSpeechPonyFillFactory
Specify the Web Speech object for text-to-speech and speech-to-text.



Contributing
See our Contributing page for details on how to build the project and our repository guidelines for Pull Requests.
This project has adopted the Microsoft Open Source Code of Conduct.
For more information see the Code of Conduct FAQ or
contact opencode@microsoft.com with any additional questions or comments.
Reporting Security Issues
Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) at secure@microsoft.com. You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the Security TechCenter.
Copyright (c) Microsoft Corporation. All rights reserved.
",827
pilsung-kang/text-mining,Jupyter Notebook,"text-mining
Unstructured Data Analysis (Graduate) @Korea University
Notice

Syllabus (download)
Term project groups

1조: 박성훈, 이수빈(2018021120), 이준걸, 박혜준
2조: 이정호, 천우진, 유초롱, 조규원
3조: 백승호, 목충협, 변준형, 이영재
4조: 박건빈, 이수빈(2018020530), 변윤선, 권순찬
5조: 최종현, 이정훈, 박중민, 노영빈
6조: 백인성, 김은비, 신욱수, 강현규
7조: 전성찬, 박현지, 문관영
8조: 조용원, 정승섭, 민다빈, 최민서
9조: 박명현, 장은아, 유건령


Term project proposal

Evaluation (due: 4/4 Thu. 23:59)
Comments by the lecturer
Comments by students


Term project inteim presentation

Presentation Slides
Evaluation (due: 5/21 Tue. 23:59)



Recommended courses

CS224d @Stanford: Deep Learning for Natural Language Processing

Course Homepage: http://cs224d.stanford.edu/
YouTube Video: https://www.youtube.com/playlist?list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG


CS224n @Stanford: Natural Language Processing Deep Learning

Course Homepage: http://web.stanford.edu/class/cs224n/syllabus.html
Youtube Video: https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6


Deep Natural Lanugage Processing @Oxford

Course Homepage: https://github.com/oxford-cs-deepnlp-2017/lectures



Schedule
Topic 1: Introduction to Text Analytics

The usefullness of large amount of text data and the challenges
Overview of text analytics methods

Topic 2: From Texts to Data

Text data collection: Web scraping

Topic 3: Text Preprocessing

Introduction to Natural Language Processing (NLP)
Lexical analysis
Syntax analysis
Other topics in NLP
Reading materials

Cambria, E., & White, B. (2014). Jumping NLP curves: A review of natural language processing research. IEEE Computational intelligence magazine, 9(2), 48-57. (PDF)
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011). Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug), 2493-2537. (PDF)
Young, T., Hazarika, D., Poria, S., & Cambria, E. (2017). Recent trends in deep learning based natural language processing. arXiv preprint arXiv:1708.02709. (PDF)



Topic 4: Neural Networks Basics

Perception, Multi-layered Perceptron
Convolutional Neural Networks (CNN)
Recurrent Neural Networks (RNN)
Practical Techniques

Topic 5-1: Document Representation I: Classic Methods

Bag of words
Word weighting
N-grams

Topic 5-2: Document Representation II: Distributed Representation

Word2Vec
GloVe
FastText
Doc2Vec
Reading materials

Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language model. Journal of machine learning research, 3(Feb), 1137-1155. (PDF)
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. (PDF)
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119). (PDF)
Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543). (PDF)
Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2016). Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606. (PDF)



Topic 6: Dimensionality Reduction

Dimensionality Reduction
Supervised Feature Selection
Unsupervised Feature Extraction: Latent Semantic Analysis (LSA) and t-SNE
R Example
Reading materials

Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990). Indexing by latent semantic analysis. Journal of the American society for information science, 41(6), 391. (PDF)
Dumais, S. T. (2004). Latent semantic analysis. Annual review of information science and technology, 38(1), 188-230.
Maaten, L. V. D., & Hinton, G. (2008). Visualizing data using t-SNE. Journal of machine learning research, 9(Nov), 2579-2605. (PDF) (Homepage)



Topic 7: Document Similarity & Clustering

Document similarity metrics
Clustering overview
K-Means clustering
Hierarchical clustering
Density-based clustering
Reading materials

Jain, A. K., Murty, M. N., & Flynn, P. J. (1999). Data clustering: a review. ACM computing surveys (CSUR), 31(3), 264-323. (PDF)



Topic 8-1: Topic Modeling I

Topic modeling overview
Probabilistic Latent Semantic Analysis: pLSA
LDA: Document Generation Process
Reading materials

Hofmann, T. (1999, July). Probabilistic latent semantic analysis. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence (pp. 289-296). Morgan Kaufmann Publishers Inc. (PDF)
Hofmann, T. (2017, August). Probabilistic latent semantic indexing. In ACM SIGIR Forum (Vol. 51, No. 2, pp. 211-218). ACM.
Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77-84. (PDF)
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022. (PDF)



Topic 8-2: Topic Modeling II

LDA Inference: Gibbs Sampling
LDA Evaluation
Recommended video lectures

LDA by D. Blei (Lecture Video)
Variational Inference for LDA by D. Blei (Lecture Video)



Topic 9: Document Classification

Document classification overview
Naive Bayesian classifier
RNN-based document classification
CNN-based document classification
Reading materials

Kim, Y. (2014). Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882. (PDF)
Zhang, X., Zhao, J., & LeCun, Y. (2015). Character-level convolutional networks for text classification. In Advances in neural information processing systems (pp. 649-657) (PDF)
Lee, G., Jeong, J., Seo, S., Kim, C, & Kang, P. (2018). Sentiment classification with word localization based on weakly supervised learning with a convolutional neural network. Knowledge-Based Systems, 152, 70-82. (PDF)
Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., & Hovy, E. (2016). Hierarchical attention networks for document classification. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 1480-1489). (PDF)
Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. (PDF)
Luong, M. T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025. (PDF)



Topic 10: Sentiment Analysis

Architecture of sentiment analysis
Lexicon-based approach
Machine learning-based approach
Reading materials

Hamilton, W. L., Clark, K., Leskovec, J., & Jurafsky, D. (2016, November). Inducing domain-specific sentiment lexicons from unlabeled corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing (Vol. 2016, p. 595). NIH Public Access. (PDF)
Zhang, L., Wang, S., & Liu, B. (2018). Deep learning for sentiment analysis: A survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8(4), e1253. (PDF)



",66
microsoft/BotFramework-WebChat,JavaScript,"
Click here to find out what's new for //build2019!
Bot Framework Web Chat





This repository contains code for the Bot Framework Web Chat component.  The Bot Framework Web Chat component is a highly-customizable web-based client for the Bot Framework V4 SDK. The Bot Framework SDK v4 enable developers to model conversation and build sophisticated bot applications.
This repo is part of the Microsoft Bot Framework - a comprehensive framework for building enterprise-grade conversational AI experiences.
Migrating from Web Chat v3 to v4
There are three possible paths that migration might take when migrating from v3 to v4. First, please compare your beginning scenario:
My current website integrates Web Chat using an <iframe> element obtained from Azure Bot Services. I want to upgrade to v4.
Starting from May 2019, we are rolling out v4 to websites that integrate Web Chat using <iframe> element. Please refer to the embed documentation for information on integrating Web Chat using <iframe>.
My website is integrated with Web Chat v3 and uses customization options provided by Web Chat, no customization at all, or very little of my own customization that was not available with Web Chat.
Please follow the implementation of sample 01.c.getting-started-migration to convert your webpage from v3 to v4 of Web Chat.
My website is integrated with a fork of Web Chat v3. I have implemented a lot of customization in my version of Web Chat, and I am concerned v4 is not compatible with my needs.
One of our team's favorite things about v4 of Web Chat is the ability to add customization without the need to fork Web Chat. Although this creates additional overhead for v3 users who forked Web Chat previously, we will do our best to support customers making the bump. Please use the following suggestions:

Take a look at the implementation of sample 01.c.getting-started-migration. This is a great starting place to get Web Chat up and running.
Next, please go through the samples list to compare your customization requirements to what Web Chat has already provided support for. These samples are made up of commonly asked-for features for Web Chat.
If one or more of your features is not available in the samples, please look through our open and closed issues, Samples label, and the Migration Support label to search for sample requests and/or customization support for a feature you are looking for. Adding your comment to open issues will help the team prioritize requests that are in high demand, and we encourage participation in our community.
If you did not find your feature in the list of open requests, please feel free to file your own request. Just like the item above, other customers adding comments to your open issue will help us prioritize which features are most commonly needed across Web Chat users.
Finally, if you need your feature as soon as possible, we welcome pull requests to Web Chat. If you have the coding experience to implement the feature yourself, we would very much appreciate the additional support! Creating the feature yourself will mean that it is available for your use on Web Chat more quickly, and that other customers looking for the same or similar feature may utilize your contribution.
Make sure to check out the rest of this README to learn more about v4.

How to use

For previous versions of Web Chat (v3), visit the Web Chat v3 branch.

First, create a bot using Azure Bot Service.
Once the bot is created, you will need to obtain the bot's Web Chat secret in Azure Portal. Then use the secret to generate a token and pass it to your Web Chat.
Here is how how you can add Web Chat control to your website:
<!DOCTYPE html>
<html>
  <body>
    <div id=""webchat"" role=""main""></div>
    <script src=""https://cdn.botframework.com/botframework-webchat/latest/webchat.js""></script>
    <script>
      window.WebChat.renderWebChat({
        directLine: window.WebChat.createDirectLine({ token: 'YOUR_DIRECT_LINE_TOKEN' }),
        userID: 'YOUR_USER_ID',
        username: 'Web Chat User',
        locale: 'en-US',
        botAvatarInitials: 'WC',
        userAvatarInitials: 'WW'
      }, document.getElementById('webchat'));
    </script>
  </body>
</html>

userID, username, locale, botAvatarInitials, and userAvatarInitials are all optional parameters to pass into the renderWebChat method. To learn more about Web Chat props, look at the Web Chat API Reference section of this README.


Integrate with JavaScript
Web Chat is designed to integrate with your existing website using JavaScript or React. Integrating with JavaScript will give you moderate styling and customizability.
You can use the full, typical webchat package that contains the most typically used features.
<!DOCTYPE html>
<html>
  <body>
    <div id=""webchat"" role=""main""></div>
    <script src=""https://cdn.botframework.com/botframework-webchat/latest/webchat.js""></script>
    <script>
      window.WebChat.renderWebChat({
        directLine: window.WebChat.createDirectLine({ token: 'YOUR_DIRECT_LINE_TOKEN' }),
        userID: 'YOUR_USER_ID'
      }, document.getElementById('webchat'));
    </script>
  </body>
</html>
See the working sample of the full Web Chat bundle.
Integrate with React
For full customizability, you can use React to recompose components of Web Chat.
To install the production build from NPM, run npm install botframework-webchat.
import { DirectLine } from 'botframework-directlinejs';
import React from 'react';
import ReactWebChat from 'botframework-webchat';

export default class extends React.Component {
  constructor(props) {
    super(props);

    this.directLine = new DirectLine({ token: 'YOUR_DIRECT_LINE_TOKEN' });
  }

  render() {
    return (
      <ReactWebChat directLine={ this.directLine } userID=""YOUR_USER_ID"" />
      element
    );
  }
}

You can also run npm install botframework-webchat@master to install a development build that is synced with Web Chat's GitHub master branch.

See a working sample of Web Chat rendered via React.
Customize Web Chat UI
Web Chat is designed to be customizable without forking the source code. The table below outlines what kind of customizations you can achieve when you are importing Web Chat in different ways. This list is not exhaustive.




CDN bundle
React




Change colors
✔️
✔️


Change sizes
✔️
✔️


Update/replace CSS styles
✔️
✔️


Listen to events
✔️
✔️


Interact with hosting webpage
✔️
✔️


Custom render activities

✔️


Custom render attachments

✔️


Add new UI components

✔️


Recompose the whole UI

✔️



See more about customizing Web Chat to learn more on customization.
Samples list



               Sample Name                    
Description
Link




01.a.getting-started-full-bundle
Introduces Web Chat embed from a CDN, and demonstrates a simple, full-featured Web Chat. This includes Adaptive Cards, Cognitive Services, and Markdown-It dependencies.
Full Bundle Demo


01.b.getting-started-es5-bundle
Introduces full-featured Web Chat embed with backwards compatibility for ES5 browsers using Web Chat's ES5 ponyfill.
ES5 Bundle Demo


01.c.getting-started-migration
Demonstrates how to migrate from your Web Chat v3 bot to v4.
Migration Demo


02.a.getting-started-minimal-bundle
Introduces the minimized CDN with only basic dependencies. This does NOT include Adaptive Cards, Cognitive Services dependencies, or Markdown-It dependencies.
Minimal Bundle Demo


02.b.getting-started-minimal-markdown
Demonstrates how to add the CDN for Markdown-It dependency on top of the minimal bundle.
Minimal with Markdown Demo


03.a.host-with-react
Demonstrates how to create a React component that hosts the full-featured Web Chat.
Host with React Demo


03.b.host-with-Angular
Demonstrates how to create an Angular component that hosts the full-featured Web Chat.
Host with Angular Demo


04.a.display-user-bot-initials-styling
Demonstrates how to display initials for both Web Chat participants.
Bot initials Demo


04.b.display-user-bot-images-styling
Demonstrates how to display images and initials for both Web Chat participants.
User images Demo


05.a.branding-webchat-styling
Introduces the ability to style Web Chat to match your brand. This method of custom styling will not break upon Web Chat updates.
Branding Web Chat Demo


05.b.idiosyncratic-manual-styling
Demonstrates how to make manual style changes, and is a more complicated and time-consuming way to customize styling of Web Chat. Manual styles may be broken upon Web Chat updates.
Idiosyncratic Styling Demo


05.c.presentation-mode-styling
Demonstrates how to set up Presentation Mode, which displays chat history but does not show the send box, and disables the interactivity of Adaptive Cards.
Presentation Mode Demo


05.d.hide-upload-button-styling
Demonstrates how to hide file upload button via styling.
Hide Upload Button Demo


06.a.cognitive-services-bing-speech-js
Introduces speech-to-text and text-to-speech ability using the (deprecated) Cognitive Services Bing Speech API and JavaScript.
Bing Speech with JS Demo


06.b.cognitive-services-bing-speech-react
Introduces speech-to-text and text-to-speech ability using the (deprecated) Cognitive Services Bing Speech API and React.
Bing Speech with React Demo


06.c.cognitive-services-speech-services-js
Introduces speech-to-text and text-to-speech ability using Cognitive Services Speech Services API.
Speech Services with JS Demo


06.d.speech-web-browser
Demonstrates how to implement text-to-speech using Web Chat's browser-based Web Speech API. (link to W3C standard in the sample)
Web Speech API Demo


06.e.cognitive-services-speech-services-with-lexical-result
Demonstrates how to use lexical result from Cognitive Services Speech Services API.
Lexical Result Demo


06.f.hybrid-speech
Demonstrates how to use both browser-based Web Speech API for speech-to-text, and Cognitive Services Speech Services API for text-to-speech.
Hybrid Speech Demo


07.a.customization-timestamp-grouping
Demonstrates how to customize timestamps by showing or hiding timestamps and changing the grouping of messages by time.
Timestamp Grouping Demo


07.b.customization-send-typing-indicator
Demonstrates how to send typing activity when the user start typing on the send box.
User Typing Indicator Demo


08.customization-user-highlighting
Demonstrates how to customize the styling of activities based whether the message is from the user or the bot.
User Highlighting Demo


09.customization-reaction-buttons
Introduces the ability to create custom components for Web Chat that are unique to your bot's needs. This tutorial demonstrates the ability to add reaction emoji such as 👍 and 👎 to conversational activities.
Reaction Buttons Demo


10.a.customization-card-components
Demonstrates how to create custom activity card attachments, in this case GitHub repository cards.
Card Components Demo


10.b.customization-password-input
Demonstrates how to create custom activity for password input.
Password Input Demo


11.customization-redux-actions
Advanced tutorial: Demonstrates how to incorporate redux middleware into your Web Chat app by sending redux actions through the bot. This example demonstrates manual styling based on activities between bot and user.
Redux Actions Demo


12.customization-minimizable-web-chat
Advanced tutorial: Demonstrates how to add the Web Chat interface to your website as a minimizable show/hide chat box.
Minimizable Web Chat Demo


13.customization-speech-ui
Advanced tutorial: Demonstrates how to fully customize key components of your bot, in this case speech, which entirely replaces the text-based transcript UI and instead shows a simple speech button with the bot's response.
Speech UI Demo


14.customization-piping-to-redux
Advanced tutorial: Demonstrates how to pipe bot activities to your own Redux store and use your bot to control your page through bot activities and Redux.
Piping to Redux Demo


15.a.backchannel-piggyback-on-outgoing-activities
Advanced tutorial: Demonstrates how to add custom data to every outgoing activities.
Backchannel Piggybacking Demo


15.b.incoming-activity-event
Advanced tutorial: Demonstrates how to forward all incoming activities to a JavaScript event for further processing.
Incoming Activity Demo


15.c.programmatic-post-activity
Advanced tutorial: Demonstrates how to send a message programmatically.
Programmatic Posting Demo


15.d.backchannel-send-welcome-event
Advanced tutorial: Demonstrates how to send welcome event with client capabilities such as browser language.
Welcome Event Demo


16.customization-selectable-activity
Advanced tutorial: Demonstrates how to add custom click behavior to each activity.
Selectable Activity Demo


17.chat-send-history
Advanced tutorial: Demonstrates the ability to save user input and allow the user to step back through previous sent messages.
Chat Send History Demo


18.customization-open-url
Advanced tutorial: Demonstrates how to customize the open URL behavior.
Customize Open URL Demo



Web Chat API Reference
There are several properties that you might pass into your Web Chat React Component (<ReactWebChat>) or the renderWebChat() method. Feel free to examine the source code starting with packages/component/src/Composer.js. Below is a short description of the available props.



Property
Description




activityMiddleware
A chain of middleware, modeled after Redux middleware, that allows the developer to add new DOM components on the currently existing DOM of Activities. The middleware signature is the following: options => next => card => children => next(card)(children).


activityRenderer
The ""flattened"" version of activityMiddleware, similar to the store enhancer concept in Redux.


adaptiveCardHostConfig
Pass in a custom Adaptive Cards host config.


attachmentMiddleware
A chain of middleware that allows the developer to add their own custom HTML Elements on attachments. The signature is the following: options => next => card => next(card).


attachmentRenderer
The ""flattened"" version of attachmentMiddleware.


cardActionMiddleware
A chain of middleware that allows the developer to modify card actions, like Adaptive Cards or suggested actions. The middleware signature is the following: cardActionMiddleware: () => next => ({ cardAction, getSignInUrl }) => next(cardAction)


createStore
A chain of middleware that allows the developer to modify the store actions. The middleware signature is the following: createStore: ({}, ({ dispatch }) => next => action => next(cardAction)


directLine
Specify the DirectLine object with DirectLine token.


disabled
Disable the UI (i.e. for presentation mode) of Web Chat.


grammars
Specify a grammar list for Speech  (Bing Speech or Cognitive Services Speech Services).


groupTimeStamp
Change default settings for timestamp groupings.


locale
Indicate the default language of Web Chat. Four letter codes (such as en-US) are strongly recommended.


renderMarkdown
Change the default Markdown renderer object.


sendTypingIndicator
Display a typing signal from the user to the bot to indicate that the user is not idling.


store
Specify a custom store, e.g. for adding programmatic activity to the bot.


styleOptions
Object that stores customization values for your styling of Web Chat. For the complete list of (frequently updated) default style options, please see the defaultStyleSetOptions.js file.


styleSet
The non-recommended way of overriding styles.


userID
Specify a userID. There are two ways to specify the userID: in props, or in the token when generating the token call (createDirectLine()). If both methods are used to specify the userID, the token userID property will be used, and a console.warn will appear during runtime. If the userID is provided via props but is prefixed with 'dl', e.g. 'dl_1234', the value will be thrown and a new ID generated. If userID is not specified, it will default to a random user ID. Multiple users sharing the same user ID is not recommended; their user state will be shared.


username
Specify a username.


webSpeechPonyFillFactory
Specify the Web Speech object for text-to-speech and speech-to-text.



Contributing
See our Contributing page for details on how to build the project and our repository guidelines for Pull Requests.
This project has adopted the Microsoft Open Source Code of Conduct.
For more information see the Code of Conduct FAQ or
contact opencode@microsoft.com with any additional questions or comments.
Reporting Security Issues
Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) at secure@microsoft.com. You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the Security TechCenter.
Copyright (c) Microsoft Corporation. All rights reserved.
",827
pilsung-kang/text-mining,Jupyter Notebook,"text-mining
Unstructured Data Analysis (Graduate) @Korea University
Notice

Syllabus (download)
Term project groups

1조: 박성훈, 이수빈(2018021120), 이준걸, 박혜준
2조: 이정호, 천우진, 유초롱, 조규원
3조: 백승호, 목충협, 변준형, 이영재
4조: 박건빈, 이수빈(2018020530), 변윤선, 권순찬
5조: 최종현, 이정훈, 박중민, 노영빈
6조: 백인성, 김은비, 신욱수, 강현규
7조: 전성찬, 박현지, 문관영
8조: 조용원, 정승섭, 민다빈, 최민서
9조: 박명현, 장은아, 유건령


Term project proposal

Evaluation (due: 4/4 Thu. 23:59)
Comments by the lecturer
Comments by students


Term project inteim presentation

Presentation Slides
Evaluation (due: 5/21 Tue. 23:59)



Recommended courses

CS224d @Stanford: Deep Learning for Natural Language Processing

Course Homepage: http://cs224d.stanford.edu/
YouTube Video: https://www.youtube.com/playlist?list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG


CS224n @Stanford: Natural Language Processing Deep Learning

Course Homepage: http://web.stanford.edu/class/cs224n/syllabus.html
Youtube Video: https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6


Deep Natural Lanugage Processing @Oxford

Course Homepage: https://github.com/oxford-cs-deepnlp-2017/lectures



Schedule
Topic 1: Introduction to Text Analytics

The usefullness of large amount of text data and the challenges
Overview of text analytics methods

Topic 2: From Texts to Data

Text data collection: Web scraping

Topic 3: Text Preprocessing

Introduction to Natural Language Processing (NLP)
Lexical analysis
Syntax analysis
Other topics in NLP
Reading materials

Cambria, E., & White, B. (2014). Jumping NLP curves: A review of natural language processing research. IEEE Computational intelligence magazine, 9(2), 48-57. (PDF)
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011). Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug), 2493-2537. (PDF)
Young, T., Hazarika, D., Poria, S., & Cambria, E. (2017). Recent trends in deep learning based natural language processing. arXiv preprint arXiv:1708.02709. (PDF)



Topic 4: Neural Networks Basics

Perception, Multi-layered Perceptron
Convolutional Neural Networks (CNN)
Recurrent Neural Networks (RNN)
Practical Techniques

Topic 5-1: Document Representation I: Classic Methods

Bag of words
Word weighting
N-grams

Topic 5-2: Document Representation II: Distributed Representation

Word2Vec
GloVe
FastText
Doc2Vec
Reading materials

Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language model. Journal of machine learning research, 3(Feb), 1137-1155. (PDF)
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. (PDF)
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119). (PDF)
Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543). (PDF)
Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2016). Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606. (PDF)



Topic 6: Dimensionality Reduction

Dimensionality Reduction
Supervised Feature Selection
Unsupervised Feature Extraction: Latent Semantic Analysis (LSA) and t-SNE
R Example
Reading materials

Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990). Indexing by latent semantic analysis. Journal of the American society for information science, 41(6), 391. (PDF)
Dumais, S. T. (2004). Latent semantic analysis. Annual review of information science and technology, 38(1), 188-230.
Maaten, L. V. D., & Hinton, G. (2008). Visualizing data using t-SNE. Journal of machine learning research, 9(Nov), 2579-2605. (PDF) (Homepage)



Topic 7: Document Similarity & Clustering

Document similarity metrics
Clustering overview
K-Means clustering
Hierarchical clustering
Density-based clustering
Reading materials

Jain, A. K., Murty, M. N., & Flynn, P. J. (1999). Data clustering: a review. ACM computing surveys (CSUR), 31(3), 264-323. (PDF)



Topic 8-1: Topic Modeling I

Topic modeling overview
Probabilistic Latent Semantic Analysis: pLSA
LDA: Document Generation Process
Reading materials

Hofmann, T. (1999, July). Probabilistic latent semantic analysis. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence (pp. 289-296). Morgan Kaufmann Publishers Inc. (PDF)
Hofmann, T. (2017, August). Probabilistic latent semantic indexing. In ACM SIGIR Forum (Vol. 51, No. 2, pp. 211-218). ACM.
Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77-84. (PDF)
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022. (PDF)



Topic 8-2: Topic Modeling II

LDA Inference: Gibbs Sampling
LDA Evaluation
Recommended video lectures

LDA by D. Blei (Lecture Video)
Variational Inference for LDA by D. Blei (Lecture Video)



Topic 9: Document Classification

Document classification overview
Naive Bayesian classifier
RNN-based document classification
CNN-based document classification
Reading materials

Kim, Y. (2014). Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882. (PDF)
Zhang, X., Zhao, J., & LeCun, Y. (2015). Character-level convolutional networks for text classification. In Advances in neural information processing systems (pp. 649-657) (PDF)
Lee, G., Jeong, J., Seo, S., Kim, C, & Kang, P. (2018). Sentiment classification with word localization based on weakly supervised learning with a convolutional neural network. Knowledge-Based Systems, 152, 70-82. (PDF)
Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., & Hovy, E. (2016). Hierarchical attention networks for document classification. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 1480-1489). (PDF)
Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. (PDF)
Luong, M. T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025. (PDF)



Topic 10: Sentiment Analysis

Architecture of sentiment analysis
Lexicon-based approach
Machine learning-based approach
Reading materials

Hamilton, W. L., Clark, K., Leskovec, J., & Jurafsky, D. (2016, November). Inducing domain-specific sentiment lexicons from unlabeled corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing (Vol. 2016, p. 595). NIH Public Access. (PDF)
Zhang, L., Wang, S., & Liu, B. (2018). Deep learning for sentiment analysis: A survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8(4), e1253. (PDF)



",66
gajus/eslint-plugin-jsdoc,JavaScript,"
eslint-plugin-jsdoc




JSDoc linting rules for ESLint.

eslint-plugin-jsdoc

Reference to jscs-jsdoc
Installation
Configuration
Settings

Alias Preference
Additional Tag Names
Allow @override Without Accompanying @param Tags
Settings to Configure check-examples


Rules

check-alignment
check-examples
check-indentation
check-param-names
check-syntax
check-tag-names
check-types
newline-after-description
no-undefined-types
require-description-complete-sentence
require-description
require-example
require-hyphen-before-param-description
require-param-description
require-param-name
require-param-type
require-param
require-returns-description
require-returns-type
require-returns-check
require-returns
valid-types






Reference to jscs-jsdoc
This table maps the rules between eslint-plugin-jsdoc and jscs-jsdoc.



eslint-plugin-jsdoc
jscs-jsdoc




check-alignment
N/A


check-examples
N/A


check-indentation
N/A


check-param-names
checkParamNames


check-syntax
N/A


check-tag-names
N/A ~ checkAnnotations


check-types
checkTypes


newline-after-description
requireNewlineAfterDescription and disallowNewlineAfterDescription


require-description
N/A


require-description-complete-sentence
requireDescriptionCompleteSentence


require-example
N/A


require-hyphen-before-param-description
requireHyphenBeforeDescription


require-param
checkParamExistence


require-param-description
requireParamDescription


require-param-name
N/A


require-param-type
requireParamTypes


require-returns
requireReturn


require-returns-check
requireReturn


require-returns-description
requireReturnDescription


require-returns-type
requireReturnTypes


valid-types
N/A


no-undefined-types
N/A


N/A
checkReturnTypes


N/A
checkRedundantParams


N/A
checkReturnTypes


N/A
checkRedundantAccess


N/A
enforceExistence


N/A
leadingUnderscoreAccess




Installation
Install ESLint either locally or globally.
npm install eslint
If you have installed ESLint globally, you have to install JSDoc plugin globally too. Otherwise, install it locally.
npm install eslint-plugin-jsdoc

Configuration
Add plugins section and specify eslint-plugin-jsdoc as a plugin.
{
    ""plugins"": [
        ""jsdoc""
    ]
}
Finally, enable all of the rules that you would like to use.
{
    ""rules"": {
        ""jsdoc/check-alignment"": 1,
        ""jsdoc/check-examples"": 1,
        ""jsdoc/check-indentation"": 1,
        ""jsdoc/check-param-names"": 1,
        ""jsdoc/check-syntax"": 1,
        ""jsdoc/check-tag-names"": 1,
        ""jsdoc/check-types"": 1,
        ""jsdoc/newline-after-description"": 1,
        ""jsdoc/no-undefined-types"": 1,
        ""jsdoc/require-description"": 1,
        ""jsdoc/require-description-complete-sentence"": 1,
        ""jsdoc/require-example"": 1,
        ""jsdoc/require-hyphen-before-param-description"": 1,
        ""jsdoc/require-param"": 1,
        ""jsdoc/require-param-description"": 1,
        ""jsdoc/require-param-name"": 1,
        ""jsdoc/require-param-type"": 1,
        ""jsdoc/require-returns"": 1,
        ""jsdoc/require-returns-check"": 1,
        ""jsdoc/require-returns-description"": 1,
        ""jsdoc/require-returns-type"": 1,
        ""jsdoc/valid-types"": 1
    }
}

Settings

Alias Preference
Use settings.jsdoc.tagNamePreference to configure a preferred alias name for a JSDoc tag. The format of the configuration is: <primary tag name>: <preferred alias name>, e.g.
{
    ""rules"": {},
    ""settings"": {
        ""jsdoc"": {
            ""tagNamePreference"": {
                ""param"": ""arg"",
                ""returns"": ""return""
            }
        }
    }
}

Additional Tag Names
Use settings.jsdoc.additionalTagNames to configure additional, allowed JSDoc tags. The format of the configuration is as follows:
{
    ""rules"": {},
    ""settings"": {
        ""jsdoc"": {
            ""additionalTagNames"": {
                ""customTags"": [""define"", ""extends"", ""record""]
            }
        }
    }
}

Allow @override Without Accompanying @param Tags
Use any of the following settings to override require-param and allow
@param to be omitted when the specified tags are present on the JSDoc
comment or the parent class comment. The default value for each is false.

settings.jsdoc.allowOverrideWithoutParam - Enables behavior with
@override tag
settings.jsdoc.allowImplementsWithoutParam - Enables behavior with
@implements tag
settings.jsdoc.allowAugmentsExtendsWithoutParam - Enables behavior with
@augments tag or its synonym @extends

The format of the configuration is as follows:
{
    ""rules"": {},
    ""settings"": {
        ""jsdoc"": {
            ""allowOverrideWithoutParam"": true,
            ""allowImplementsWithoutParam"": true,
            ""allowAugmentsExtendsWithoutParam"": true
        }
    }
}

Settings to Configure check-examples
The settings below all impact the check-examples rule and default to
no-op/false except as noted.
JSDoc specs use of an optional <caption> element at the beginning of
@example. The following setting requires its use.

settings.jsdoc.captionRequired - Require <caption> at beginning
of any @example

JSDoc does not specify a formal means for delimiting code blocks within
@example (it uses generic syntax highlighting techniques for its own
syntax highlighting). The following settings determine whether a given
@example tag will have the check-examples checks applied to it:

settings.jsdoc.exampleCodeRegex - Regex which whitelists lintable
examples. If a parenthetical group is used, the first one will be used,
so you may wish to use (?:...) groups where you do not wish the
first such group treated as one to include. If no parenthetical group
exists or matches, the whole matching expression will be used.
An example might be ""^```(?:js|javascript)([\\s\\S]*)```$""
to only match explicitly fenced JavaScript blocks.
settings.jsdoc.rejectExampleCodeRegex - Regex blacklist which rejects
non-lintable examples (has priority over exampleCodeRegex). An example
might be ""^`"" to avoid linting fenced blocks which may indicate
a non-JavaScript language.

If neither is in use, all examples will be matched. Note also that even if
settings.jsdoc.captionRequired is not set, any initial <caption>
will be stripped out before doing the regex matching.
The following settings determine which individual ESLint rules will be
applied to the JavaScript found within the @example tags (as determined
to be applicable by the above regex settings). They are ordered by
decreasing precedence:

settings.jsdoc.allowInlineConfig - If not set to false, will allow
inline config within the @example to override other config. Defaults
to true.
settings.jsdoc.noDefaultExampleRules - Setting to true will disable the
default rules which are expected to be troublesome for most documentation
use. See the section below for the specific default rules.
settings.jsdoc.matchingFileName - Setting for a dummy file name to trigger
specific rules defined in one's config; usable with ESLint .eslintrc.*
overrides -> files globs, to apply a desired subset of rules with
@example (besides allowing for rules specific to examples, this setting
can be useful for enabling reuse of the same rules within @example as
with JavaScript Markdown lintable by
other plugins, e.g.,
if one sets matchingFileName to dummy.md so that @example rules will
follow one's Markdown rules).
settings.jsdoc.configFile - A config file. Corresponds to ESLint's -c.
settings.jsdoc.eslintrcForExamples - Defaults to true in adding rules
based on an .eslintrc.* file. Setting to false corresponds to
ESLint's --no-eslintrc.
settings.jsdoc.baseConfig - An object of rules with the same schema
as .eslintrc.* for defaults

Finally, the following rule pertains to inline disable directives:

settings.jsdoc.reportUnusedDisableDirectives - If not set to false,
this will report disabled directives which are not used (and thus not
needed). Defaults to true. Corresponds to ESLint's
--report-unused-disable-directives.


Rules Disabled by Default Unless noDefaultExampleRules is Set to true

eol-last - Insisting that a newline ""always"" be at the end is less likely
to be desired in sample code as with the code file convention
no-console - Unlikely to have inadvertent temporary debugging within
examples
no-undef - Many variables in examples will be undefined.
no-unused-vars - It is common to define variables for clarity without always
using them within examples.
padded-blocks - It can generally look nicer to pad a little even if one's
code follows more stringency as far as block padding.
import/no-unresolved - One wouldn't generally expect example paths to
resolve relative to the current JavaScript file as one would with real code.
import/unambiguous - Snippets in examples are likely too short to always
include full import/export info
node/no-missing-import - See import/no-unresolved
node/no-missing-require -  See import/no-unresolved


Rules

check-alignment
Reports invalid alignment of JSDoc block asterisks.









Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
N/A



The following patterns are considered problems:
/**
  * @param {Number} foo
 */
function quux (foo) {

}
// Message: Expected JSDoc block to be aligned.

/**
 * @param {Number} foo
  */
function quux (foo) {

}
// Message: Expected JSDoc block to be aligned.

/**
 * @param {Number} foo
 */
function quux (foo) {

}
// Message: Expected JSDoc block to be aligned.

/**
  * @param {Number} foo
 */
function quux (foo) {

}
// Message: Expected JSDoc block to be aligned.

/**
 * @param {Number} foo
 */
function quux (foo) {

}
// Message: Expected JSDoc block to be aligned.
The following patterns are not considered problems:
/**
 * Desc
 *
 * @param {Number} foo
 */
function quux (foo) {

}

/**
 * Desc
 *
 * @param {{
  foo: Bar,
  bar: Baz
 * }} foo
 *
 */
function quux (foo) {

}

check-examples
Ensures that (JavaScript) examples within JSDoc adhere to ESLint rules.
Works in conjunction with the following settings:

captionRequired
exampleCodeRegex
rejectExampleCodeRegex
allowInlineConfig - Defaults to true
noDefaultExampleRules
matchingFileName
configFile
eslintrcForExamples - Defaults to true
baseConfig
reportUnusedDisableDirectives - Defaults to true

Inline ESLint config within @example JavaScript is allowed, though the
disabling of ESLint directives which are not needed by the resolved rules
will be reported as with the ESLint --report-unused-disable-directives
command.









Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
param



The following patterns are considered problems:
/**
 * @example alert('hello')
 */
function quux () {

}
// Settings: {""jsdoc"":{""baseConfig"":{""rules"":{""no-alert"":2,""semi"":[""error"",""always""]}},""eslintrcForExamples"":false}}
// Message: @example error (no-alert): Unexpected alert.

/**
 * @example ```js
 alert('hello');
 ```
 */
function quux () {

}
// Settings: {""jsdoc"":{""baseConfig"":{""rules"":{""semi"":[""error"",""never""]}},""eslintrcForExamples"":false,""exampleCodeRegex"":""```js([\\s\\S]*)```""}}
// Message: @example error (semi): Extra semicolon.

/**
 * @example
 * ```js alert('hello'); ```
 */
function quux () {

}
// Settings: {""jsdoc"":{""baseConfig"":{""rules"":{""semi"":[""error"",""never""]}},""eslintrcForExamples"":false,""exampleCodeRegex"":""```js ([\\s\\S]*)```""}}
// Message: @example error (semi): Extra semicolon.

/**
 * @example ```
 * js alert('hello'); ```
 */
function quux () {

}
// Settings: {""jsdoc"":{""baseConfig"":{""rules"":{""semi"":[""error"",""never""]}},""eslintrcForExamples"":false,""exampleCodeRegex"":""```\njs ([\\s\\S]*)```""}}
// Message: @example error (semi): Extra semicolon.

/**
 * @example <b>Not JavaScript</b>
 */
function quux () {

}
/**
 * @example quux2();
 */
function quux2 () {

}
// Settings: {""jsdoc"":{""baseConfig"":{""rules"":{""semi"":[""error"",""never""]}},""eslintrcForExamples"":false,""rejectExampleCodeRegex"":""^\\s*<.*>$""}}
// Message: @example error (semi): Extra semicolon.

/**
 * @example
 * quux(); // does something useful
 */
function quux () {

}
// Settings: {""jsdoc"":{""baseConfig"":{""rules"":{""no-undef"":[""error""]}},""eslintrcForExamples"":false,""noDefaultExampleRules"":true}}
// Message: @example error (no-undef): 'quux' is not defined.

/**
 * @example <caption>Valid usage</caption>
 * quux(); // does something useful
 *
 * @example
 * quux('random unwanted arg'); // results in an error
 */
function quux () {

}
// Settings: {""jsdoc"":{""captionRequired"":true,""eslintrcForExamples"":false}}
// Message: Caption is expected for examples.

/**
 * @example  quux();
 */
function quux () {

}
// Settings: {""jsdoc"":{""baseConfig"":{""rules"":{""indent"":[""error""]}},""eslintrcForExamples"":false,""noDefaultExampleRules"":false}}
// Message: @example error (indent): Expected indentation of 0 spaces but found 1.

/**
 * @example test() // eslint-disable-line semi
 */
function quux () {}
// Settings: {""jsdoc"":{""eslintrcForExamples"":false,""noDefaultExampleRules"":true,""reportUnusedDisableDirectives"":true}}
// Message: @example error: Unused eslint-disable directive (no problems were reported from 'semi').

/**
 * @example
 test() // eslint-disable-line semi
 */
function quux () {}
// Settings: {""jsdoc"":{""allowInlineConfig"":false,""baseConfig"":{""rules"":{""semi"":[""error"",""always""]}},""eslintrcForExamples"":false,""noDefaultExampleRules"":true}}
// Message: @example error (semi): Missing semicolon.
The following patterns are not considered problems:
/**
 * @example ```js
 alert('hello');
 ```
 */
function quux () {

}
// Settings: {""jsdoc"":{""baseConfig"":{""rules"":{""semi"":[""error"",""always""]}},""eslintrcForExamples"":false,""exampleCodeRegex"":""```js([\\s\\S]*)```""}}

/**
 * @example
 * // arbitrary example content
 */
function quux () {

}
// Settings: {""jsdoc"":{""eslintrcForExamples"":false}}

/**
 * @example
 * quux(); // does something useful
 */
function quux () {

}
// Settings: {""jsdoc"":{""baseConfig"":{""rules"":{""no-undef"":[""error""]}},""eslintrcForExamples"":false,""noDefaultExampleRules"":false}}

/**
 * @example quux();
 */
function quux () {

}
// Settings: {""jsdoc"":{""baseConfig"":{""rules"":{""indent"":[""error""]}},""eslintrcForExamples"":false,""noDefaultExampleRules"":false}}

/**
 * @example <caption>Valid usage</caption>
 * quux(); // does something useful
 *
 * @example <caption>Invalid usage</caption>
 * quux('random unwanted arg'); // results in an error
 */
function quux () {

}
// Settings: {""jsdoc"":{""captionRequired"":true,""eslintrcForExamples"":false}}

/**
 * @example test() // eslint-disable-line semi
 */
function quux () {}
// Settings: {""jsdoc"":{""eslintrcForExamples"":false,""noDefaultExampleRules"":true,""reportUnusedDisableDirectives"":false}}

/**
 * @example
 test() // eslint-disable-line semi
 */
function quux () {}
// Settings: {""jsdoc"":{""allowInlineConfig"":true,""baseConfig"":{""rules"":{""semi"":[""error"",""always""]}},""eslintrcForExamples"":false,""noDefaultExampleRules"":true}}

check-indentation
Reports invalid padding inside JSDoc block.









Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
N/A



The following patterns are considered problems:
/**
 * foo
 *
 * @param bar
 *  baz
 */
function quux () {

}
// Message: There must be no indentation.
The following patterns are not considered problems:
/**
 * foo
 *
 * @param bar
 * baz
 */
function quux () {

}

check-param-names
Ensures that parameter names in JSDoc match those in the function declaration.









Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
param



The following patterns are considered problems:
/**
 * @param Foo
 */
function quux (foo = 'FOO') {

}
// Message: Expected @param names to be ""foo"". Got ""Foo"".

/**
 * @arg Foo
 */
function quux (foo = 'FOO') {

}
// Settings: {""jsdoc"":{""tagNamePreference"":{""param"":""arg""}}}
// Message: Expected @arg names to be ""foo"". Got ""Foo"".

/**
 * @param Foo
 */
function quux (foo) {

}
// Message: Expected @param names to be ""foo"". Got ""Foo"".

/**
 * @param Foo.Bar
 */
function quux (foo) {

}
// Message: @param path declaration (""Foo.Bar"") appears before any real parameter.

/**
 * @param foo
 * @param Foo.Bar
 */
function quux (foo) {

}
// Message: @param path declaration (""Foo.Bar"") root node name (""Foo"") does not match previous real parameter name (""foo"").

/**
 * @param foo
 * @param foo.bar
 * @param bar
 */
function quux (bar, foo) {

}
// Message: Expected @param names to be ""bar, foo"". Got ""foo, bar"".

/**
 * @param foo
 * @param bar
 */
function quux (foo) {

}
// Message: @param ""bar"" does not match an existing function parameter.

/**
 * @param foo
 * @param foo
 */
function quux (foo) {

}
// Message: Duplicate @param ""foo""

/**
 * @param foo
 * @param foo
 */
function quux (foo, bar) {

}
// Message: Duplicate @param ""foo""

/**
 * @param foo
 * @param foo
 */
function quux (foo, foo) {

}
// Message: Duplicate @param ""foo""
The following patterns are not considered problems:
/**
 *
 */
function quux (foo) {

}

/**
 * @param foo
 */
function quux (foo) {

}

/**
 * @param foo
 * @param bar
 */
function quux (foo, bar) {

}

/**
 * @param foo
 * @param bar
 */
function quux (foo, bar, baz) {

}

/**
 * @param foo
 * @param foo.foo
 * @param bar
 */
function quux (foo, bar) {

}

/**
 * @param args
 */
function quux (...args) {

}

/**
 * @param foo
 */
function quux ({a, b}) {

}

/**
 * @param foo
 */
function quux ({a, b} = {}) {

}

/**
 * @param foo
 */
function quux ([a, b] = []) {

}

/**
 * Assign the project to a list of employees.
 * @param {Object[]} employees - The employees who are responsible for the project.
 * @param {string} employees[].name - The name of an employee.
 * @param {string} employees[].department - The employee's department.
 */
function assign (employees) {

};

Deconstructing Function Parameter
eslint-plugin-jsdoc does not validate names of parameters in function deconstruction, e.g.
/**
 * @param foo
 */
function quux ({
    a,
    b
}) {

}
{a, b} is an ObjectPattern AST type and does not have a name. Therefore, the associated parameter in JSDoc block can have any name.
Likewise for the pattern [a, b] which is an ArrayPattern.

check-syntax
Reports against Google Closure Compiler syntax.









Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
N/A



The following patterns are considered problems:
/**
 * @param {string=} foo
 */
function quux (foo) {

}
// Message: Syntax should not be Google Closure Compiler style.
The following patterns are not considered problems:
/**
 * @param {string} [foo]
 */
function quux (foo) {

}

/**
 *
 */
function quux (foo) {

}

check-tag-names
Reports invalid block tag names.
Valid JSDoc 3 Block Tags are:
abstract
access
alias
async
augments
author
borrows
callback
class
classdesc
constant
constructs
copyright
default
deprecated
description
enum
event
example
exports
external
file
fires
function
generator
global
hideconstructor
ignore
implements
inheritdoc
inner
instance
interface
kind
lends
license
listens
member
memberof
memberof!
mixes
mixin
module
name
namespace
override
package
param
private
property
protected
public
readonly
requires
returns
see
since
static
summary
this
throws
todo
tutorial
type
typedef
variation
version
yields










Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
N/A



The following patterns are considered problems:
/**
 * @Param
 */
function quux () {

}
// Message: Invalid JSDoc tag name ""Param"".

/**
 * @foo
 */
function quux () {

}
// Message: Invalid JSDoc tag name ""foo"".

/**
 * @arg foo
 */
function quux (foo) {

}
// Message: Invalid JSDoc tag (preference). Replace ""arg"" JSDoc tag with ""param"".

/**
 * @param foo
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""tagNamePreference"":{""param"":""arg""}}}
// Message: Invalid JSDoc tag (preference). Replace ""param"" JSDoc tag with ""arg"".

/**
 * @param foo
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""tagNamePreference"":{""param"":""parameter""}}}
// Message: Invalid JSDoc tag (preference). Replace ""param"" JSDoc tag with ""parameter"".

/**
 * @bar foo
 */
function quux (foo) {

}
// Message: Invalid JSDoc tag name ""bar"".

/**
 * @baz @bar foo
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""additionalTagNames"":{""customTags"":[""bar""]}}}
// Message: Invalid JSDoc tag name ""baz"".

/**
 * @bar
 * @baz
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""additionalTagNames"":{""customTags"":[""bar""]}}}
// Message: Invalid JSDoc tag name ""baz"".
The following patterns are not considered problems:
/**
 * @param foo
 */
function quux (foo) {

}

/**
 * @memberof! foo
 */
function quux (foo) {

}

/**
 * @arg foo
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""tagNamePreference"":{""param"":""arg""}}}

/**
 * @bar foo
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""additionalTagNames"":{""customTags"":[""bar""]}}}

/**
 * @baz @bar foo
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""additionalTagNames"":{""customTags"":[""baz"",""bar""]}}}

/** 
 * @abstract
 * @access
 * @alias
 * @augments
 * @author
 * @borrows
 * @callback
 * @class
 * @classdesc
 * @constant
 * @constructs
 * @copyright
 * @default
 * @deprecated
 * @description
 * @enum
 * @event
 * @example
 * @exports
 * @external
 * @file
 * @fires
 * @function
 * @global
 * @ignore
 * @implements
 * @inheritdoc
 * @inner
 * @instance
 * @interface
 * @kind
 * @lends
 * @license
 * @listens
 * @member
 * @memberof
 * @memberof!
 * @mixes
 * @mixin
 * @module
 * @name
 * @namespace
 * @override
 * @param
 * @private
 * @property
 * @protected
 * @public
 * @readonly
 * @requires
 * @returns
 * @see
 * @since
 * @static
 * @summary
 * @this
 * @throws
 * @todo
 * @tutorial
 * @type
 * @typedef
 * @variation
 * @version
 */
function quux (foo) {}

check-types
Reports invalid types.
Ensures that case of native types is the same as in this list:
undefined
null
boolean
number
string
Object
Array
Date
RegExp


Why not capital case everything?
Why are boolean, number and string exempt from starting with a capital letter? Let's take string as an example. In Javascript, everything is an object. The string Object has prototypes for string functions such as .toUpperCase().
Fortunately we don't have to write new String() everywhere in our code. Javascript will automatically wrap string primitives into string Objects when we're applying a string function to a string primitive. This way the memory footprint is a tiny little bit smaller, and the GC has less work to do.
So in a sense, there two types of strings in Javascript; {string} literals, also called primitives and {String} Objects. We use the primitives because it's easier to write and uses less memory. {String} and {string} are technically both valid, but they are not the same.
new String('lard') // String {0: ""l"", 1: ""a"", 2: ""r"", 3: ""d"", length: 4}
'lard' // ""lard""
new String('lard') === 'lard' // false
To make things more confusing, there are also object literals and object Objects. But object literals are still static Objects and object Objects are instantiated Objects. So an object primitive is still an object Object.
Basically, for primitives, we want to define the type as a primitive, because that's what we use in 99.9% of cases. For everything else, we use the type rather than the primitive. Otherwise it would all just be {object}.
In short: It's not about consistency, rather about the 99.9% use case.



type name
typeof
check-types
testcase




Object
object
Object
({}) instanceof Object -> true


Array
object
Array
([]) instanceof Array -> true


Date
object
Date
(new Date()) instanceof Date -> true


RegExp
object
RegExp
(new RegExp(/.+/)) instanceof RegExp -> true


Boolean
boolean
boolean
(true) instanceof Boolean -> false


Number
number
number
(41) instanceof Number -> false


String
string
string
(""test"") instanceof String -> false












Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
class, constant, enum, member, module, namespace, param, property, returns, throws, type, typedef



The following patterns are considered problems:
/**
 * @param {Number} foo
 */
function quux (foo) {

}
// Message: Invalid JSDoc @param ""foo"" type ""Number"".

/**
 * @arg {Number} foo
 */
function quux (foo) {

}
// Message: Invalid JSDoc @arg ""foo"" type ""Number"".

/**
 * @returns {Number} foo
 * @throws {Number} foo
 */
function quux () {

}
// Message: Invalid JSDoc @returns type ""Number"".

/**
 * @param {(Number|string|Boolean)=} foo
 */
function quux (foo, bar, baz) {

}
// Message: Invalid JSDoc @param ""foo"" type ""Number"".

/**
 * @param {Array<Number|String>} foo
 */
function quux (foo, bar, baz) {

}
// Message: Invalid JSDoc @param ""foo"" type ""Number"".
The following patterns are not considered problems:
/**
 * @param {number} foo
 * @param {Bar} bar
 * @param {*} baz
 */
function quux (foo, bar, baz) {

}

/**
 * @arg {number} foo
 * @arg {Bar} bar
 * @arg {*} baz
 */
function quux (foo, bar, baz) {

}

/**
 * @param {(number|string|boolean)=} foo
 */
function quux (foo, bar, baz) {

}

/**
 * @param {typeof bar} foo
 */
function qux(foo) {
}

/**
 * @param {import('./foo').bar.baz} foo
 */
function qux(foo) {
}

/**
 * @param {(x: number, y: string) => string} foo
 */
function qux(foo) {
}

/**
 * @param {() => string} foo
 */
function qux(foo) {
}

newline-after-description
Enforces a consistent padding of the block description.
This rule takes one argument. If it is ""always"" then a problem is raised when there is a newline after the description. If it is ""never"" then a problem is raised when there is no newline after the description. The default value is ""always"".









Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
N/A



The following patterns are considered problems:
/**
 * Foo.
 *
 * Foo.
 * @foo
 */
function quux () {

}
// Options: [""always""]
// Message: There must be a newline after the description of the JSDoc block.

/**
 * Bar.
 *
 * Bar.
 *
 * @bar
 */
function quux () {

}
// Options: [""never""]
// Message: There must be no newline after the description of the JSDoc block.
The following patterns are not considered problems:
/**
 * Foo.
 */
function quux () {

}
// Options: [""always""]

/**
 * Bar.
 */
function quux () {

}
// Options: [""never""]

/**
 * Foo.
 *
 * @foo
 */
function quux () {

}
// Options: [""always""]

/**
 * Bar.
 * @bar
 */
function quux () {

}
// Options: [""never""]

no-undefined-types
Checks that types in jsdoc comments are defined. This can be used to check unimported types.
When enabling this rule, types in jsdoc comments will resolve as used variables, i.e. will not be marked as unused by no-unused-vars.









Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
param, returns



The following patterns are considered problems:
/**
 * @param {strnig} foo - Bar.
 */
function quux(foo) {

}
// Message: The type 'strnig' is undefined.
The following patterns are not considered problems:
/**
 * @param {string} foo - Bar.
 */
function quux(foo) {

}

/**
 * @param {Promise} foo - Bar.
 */
function quux(foo) {

}

class MyClass {}

/**
 * @param {MyClass} foo - Bar.
 */
function quux(foo) {
  console.log(foo);
}

quux(0);

const MyType = require('my-library').MyType;

/**
 * @param {MyType} foo - Bar.
 */
  function quux(foo) {

}

const MyType = require('my-library').MyType;

/**
 * @param {MyType} foo - Bar.
 */
  function quux(foo) {

}

import {MyType} from 'my-library';

/**
 * @param {MyType} foo - Bar.
 * @param {Object<string, number>} foo
 * @param {Array<string>} baz
 */
  function quux(foo, bar, baz) {

}

/*globals MyType*/

/**
 * @param {MyType} foo - Bar.
 * @param {HisType} bar - Foo.
 */
  function quux(foo, bar) {

}

/**
 * @typedef {Object} hello
 * @property {string} a - a.
 */

/**
 * @param {hello} foo
 */
function quux(foo) {

}

/**
 * @param {Array<syntaxError} foo
 */
function quux(foo) {

}

/**
 * Callback test.
 *
 * @callback addStuffCallback
 * @param {String} sum - An test integer.
 */
/**
 * Test Eslint.
 *
 * @param {addStuffCallback} callback - A callback to run.
 */
function testFunction(callback) {
  callback();
}

require-description-complete-sentence
Requires that block description and tag description are written in complete sentences, i.e.,

Description must start with an uppercase alphabetical character.
Paragraphs must start with an uppercase alphabetical character.
Sentences must end with a period.
Every line in a paragraph (except the first) which starts with an uppercase character must be preceded by a line ending with a period.










Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
param, returns



The following patterns are considered problems:
/**
 * foo.
 */
function quux () {

}
// Message: Sentence should start with an uppercase character.

/**
 * Foo)
 */
function quux () {

}
// Message: Sentence must end with a period.

/**
 * Foo.
 *
 * foo.
 */
function quux () {

}
// Message: Sentence should start with an uppercase character.

/**
 * тест.
 */
function quux () {

}
// Message: Sentence should start with an uppercase character.

/**
 * Foo
 */
function quux () {

}
// Message: Sentence must end with a period.

/**
 * Foo
 * Bar.
 */
function quux () {

}
// Message: A line of text is started with an uppercase character, but preceding line does not end the sentence.

/**
 * Foo.
 *
 * @param foo foo.
 */
function quux (foo) {

}
// Message: Sentence should start with an uppercase character.

/**
 * Foo.
 *
 * @param foo bar
 */
function quux (foo) {

}
// Message: Sentence should start with an uppercase character.

/**
 * {@see Foo.bar} buz
 */
function quux (foo) {

}
// Message: Sentence should start with an uppercase character.

/**
 * Foo.
 *
 * @returns {number} foo
 */
function quux (foo) {

}
// Message: Sentence should start with an uppercase character.

/**
 * Foo.
 *
 * @returns foo.
 */
function quux (foo) {

}
// Message: Sentence should start with an uppercase character.

/**
 * lorem ipsum dolor sit amet, consectetur adipiscing elit. pellentesque elit diam,
 * iaculis eu dignissim sed, ultrices sed nisi. nulla at ligula auctor, consectetur neque sed,
 * tincidunt nibh. vivamus sit amet vulputate ligula. vivamus interdum elementum nisl,
 * vitae rutrum tortor semper ut. morbi porta ante vitae dictum fermentum.
 * proin ut nulla at quam convallis gravida in id elit. sed dolor mauris, blandit quis ante at,
 * consequat auctor magna. duis pharetra purus in porttitor mollis.
 */
function longDescription (foo) {

}
// Message: Sentence should start with an uppercase character.

/**
 * @arg {number} foo - Foo
 */
function quux (foo) {

}
// Message: Sentence must end with a period.

/**
 * @argument {number} foo - Foo
 */
function quux (foo) {

}
// Message: Sentence must end with a period.

/**
 * @return {number} foo
 */
function quux (foo) {

}
// Message: Sentence should start with an uppercase character.

/**
 * Returns bar.
 *
 * @return {number} bar
 */
function quux (foo) {

}
// Message: Sentence should start with an uppercase character.
The following patterns are not considered problems:
/**
 * @param foo - Foo.
 */
function quux () {

}

/**
 * Foo.
 */
function quux () {

}

/**
 * Foo.
 * Bar.
 */
function quux () {

}

/**
 * Foo.
 *
 * Bar.
 */
function quux () {

}

/**
 * Тест.
 */
function quux () {

}

/**
 * Foo
 * bar.
 */
function quux () {

}

/**
 * @returns Foo bar.
 */
function quux () {

}

/**
 * Foo. {@see Math.sin}.
 */
function quux () {

}

/**
 * Foo {@see Math.sin} bar.
 */
function quux () {

}

/**
 * Foo?
 *
 * Bar!
 *
 * Baz:
 *   1. Foo.
 *   2. Bar.
 */
function quux () {

}

/**
 * Hello:
 * World.
 */
function quux () {

}

/**
 * Hello: world.
 */
function quux () {

}

require-description
Requires that all functions have a description.

All functions must have a @description tag.
Every description tag must have a non-empty description that explains the purpose of the method.










Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
class, example



The following patterns are considered problems:
/**
 *
 */
function quux () {

}
// Message: Missing JSDoc @description declaration.

/**
 * @description
 */
function quux () {

}
// Message: Missing JSDoc @description description.
The following patterns are not considered problems:
/**
 * @description
 * // arbitrary description content
 */
function quux () {

}

/**
 * @description
 * quux(); // does something useful
 */
function quux () {

}

/**
 * @description <caption>Valid usage</caption>
 * quux(); // does something useful
 *
 * @description <caption>Invalid usage</caption>
 * quux('random unwanted arg'); // results in an error
 */
function quux () {

}

require-example
Requires that all functions have examples.

All functions must have one or more @example tags.
Every example tag must have a non-empty description that explains the method's usage.










Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
example



The following patterns are considered problems:
/**
 *
 */
function quux () {

}
// Message: Missing JSDoc @example declaration.

/**
 * @example
 */
function quux () {

}
// Message: Missing JSDoc @example description.

/**
 * @constructor
 */
function quux () {

}
// Message: Missing JSDoc @example declaration.

/**
 * @constructor
 * @example
 */
function quux () {

}
// Message: Missing JSDoc @example description.
The following patterns are not considered problems:
/**
 * @example
 * // arbitrary example content
 */
function quux () {

}

/**
 * @example
 * quux(); // does something useful
 */
function quux () {

}

/**
 * @example <caption>Valid usage</caption>
 * quux(); // does something useful
 *
 * @example <caption>Invalid usage</caption>
 * quux('random unwanted arg'); // results in an error
 */
function quux () {

}

/**
 * @constructor
 */
function quux () {

}
// Settings: {""jsdoc"":{""avoidExampleOnConstructors"":true}}

/**
 * @constructor
 * @example
 */
function quux () {

}
// Settings: {""jsdoc"":{""avoidExampleOnConstructors"":true}}

/**
 * @inheritdoc
 */
function quux () {

}

require-hyphen-before-param-description
Requires a hyphen before the @param description.
This rule takes one argument. If it is ""always"" then a problem is raised when there is no hyphen before the description. If it is ""never"" then a problem is raised when there is a hyphen before the description. The default value is ""always"".









Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
param



The following patterns are considered problems:
/**
 * @param foo Foo.
 */
function quux () {

}
// Options: [""always""]
// Message: There must be a hyphen before @param description.

/**
 * @param foo - Foo.
 */
function quux () {

}
// Options: [""never""]
// Message: There must be no hyphen before @param description.
The following patterns are not considered problems:
/**
 * @param foo - Foo.
 */
function quux () {

}
// Options: [""always""]

/**
 * @param foo Foo.
 */
function quux () {

}
// Options: [""never""]

require-param-description
Requires that @param tag has description value.









Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
param



The following patterns are considered problems:
/**
 * @param foo
 */
function quux (foo) {

}
// Message: Missing JSDoc @param ""foo"" description.

/**
 * @arg foo
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""tagNamePreference"":{""param"":""arg""}}}
// Message: Missing JSDoc @arg ""foo"" description.
The following patterns are not considered problems:
/**
 *
 */
function quux (foo) {

}

/**
 * @param foo Foo.
 */
function quux (foo) {

}

require-param-name
Requires that all function parameters have name.

The @param tag requires you to specify the name of the parameter you are documenting. You can also include the parameter's type, enclosed in curly brackets, and a description of the parameter.
JSDoc










Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
param



The following patterns are considered problems:
/**
 * @param
 */
function quux (foo) {

}
// Message: There must be an identifier after @param type.

/**
 * @param {string}
 */
function quux (foo) {

}
// Message: There must be an identifier after @param tag.
The following patterns are not considered problems:
/**
 * @param foo
 */
function quux (foo) {

}

/**
 * @param {string} foo
 */
function quux (foo) {

}

require-param-type
Requires that @param tag has type value.









Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
param



The following patterns are considered problems:
/**
 * @param foo
 */
function quux (foo) {

}
// Message: Missing JSDoc @param ""foo"" type.

/**
 * @arg foo
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""tagNamePreference"":{""param"":""arg""}}}
// Message: Missing JSDoc @arg ""foo"" type.
The following patterns are not considered problems:
/**
 *
 */
function quux (foo) {

}

/**
 * @param {number} foo
 */
function quux (foo) {

}

require-param
Requires that all function parameters are documented.









Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
param



The following patterns are considered problems:
/**
 *
 */
function quux (foo) {

}
// Message: Missing JSDoc @param ""foo"" declaration.

/**
 *
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""tagNamePreference"":{""param"":""arg""}}}
// Message: Missing JSDoc @arg ""foo"" declaration.

/**
 * @param foo
 */
function quux (foo, bar) {

}
// Message: Missing JSDoc @param ""bar"" declaration.

/**
 * @override
 */
function quux (foo) {

}
// Message: Missing JSDoc @param ""foo"" declaration.

/**
 * @implements
 */
function quux (foo) {

}
// Message: Missing JSDoc @param ""foo"" declaration.

/**
 * @augments
 */
function quux (foo) {

}
// Message: Missing JSDoc @param ""foo"" declaration.

/**
 * @extends
 */
function quux (foo) {

}
// Message: Missing JSDoc @param ""foo"" declaration.

/**
 * @override
 */
class A {
  /**
    *
    */
  quux (foo) {

  }
}
// Message: Missing JSDoc @param ""foo"" declaration.

/**
 * @implements
 */
class A {
  /**
   *
   */
  quux (foo) {

  }
}
// Message: Missing JSDoc @param ""foo"" declaration.

/**
 * @augments
 */
class A {
  /**
   *
   */
  quux (foo) {

  }
}
// Message: Missing JSDoc @param ""foo"" declaration.

/**
 * @extends
 */
class A {
  /**
   *
   */
  quux (foo) {

  }
}
// Message: Missing JSDoc @param ""foo"" declaration.
The following patterns are not considered problems:
/**
 * @param foo
 */
function quux (foo) {

}

/**
 * @inheritdoc
 */
function quux (foo) {

}

/**
 * @arg foo
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""tagNamePreference"":{""param"":""arg""}}}

/**
 * @override
 * @param foo
 */
function quux (foo) {

}

/**
 * @override
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""allowOverrideWithoutParam"":true}}

/**
 * @implements
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""allowImplementsWithoutParam"":true}}

/**
 * @implements
 * @param foo
 */
function quux (foo) {

}

/**
 * @augments
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""allowAugmentsExtendsWithoutParam"":true}}

/**
 * @augments
 * @param foo
 */
function quux (foo) {

}

/**
 * @extends
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""allowAugmentsExtendsWithoutParam"":true}}

/**
 * @extends
 * @param foo
 */
function quux (foo) {

}

/**
 * @override
 */
class A {
  /**
  * @param foo
  */
  quux (foo) {

  }
}

/**
 * @override
 */
class A {
  /**
   *
   */
  quux (foo) {

  }
}
// Settings: {""jsdoc"":{""allowOverrideWithoutParam"":true}}

/**
 * @implements
 */
class A {
  /**
   *
   */
  quux (foo) {

  }
}
// Settings: {""jsdoc"":{""allowImplementsWithoutParam"":true}}

/**
 * @implements
 */
class A {
  /**
   * @param foo
   */
  quux (foo) {

  }
}

/**
 * @augments
 */
class A {
  /**
   *
   */
  quux (foo) {

  }
}
// Settings: {""jsdoc"":{""allowAugmentsExtendsWithoutParam"":true}}

/**
 * @augments
 */
class A {
  /**
   * @param foo
   */
  quux (foo) {

  }
}

/**
 * @extends
 */
class A {
  /**
   *
   */
  quux (foo) {

  }
}
// Settings: {""jsdoc"":{""allowAugmentsExtendsWithoutParam"":true}}

/**
 * @extends
 */
class A {
  /**
   * @param foo
   */
  quux (foo) {

  }
}

require-returns-description
Requires that @returns tag has description value.









Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
returns



The following patterns are considered problems:
/**
 * @returns
 */
function quux (foo) {

}
// Message: Missing JSDoc @returns description.

/**
 * @return
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""tagNamePreference"":{""returns"":""return""}}}
// Message: Missing JSDoc @return description.
The following patterns are not considered problems:
/**
 *
 */
function quux () {

}

/**
 * @returns Foo.
 */
function quux () {

}

/**
 * @returns {undefined}
 */
function quux () {

}

/**
 * @returns {void}
 */
function quux () {

}

require-returns-type
Requires that @returns tag has type value.









Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
returns



The following patterns are considered problems:
/**
 * @returns
 */
function quux () {

}
// Message: Missing JSDoc @returns type.

/**
 * @returns Foo.
 */
function quux () {

}
// Message: Missing JSDoc @returns type.

/**
 * @return Foo.
 */
function quux () {

}
// Settings: {""jsdoc"":{""tagNamePreference"":{""returns"":""return""}}}
// Message: Missing JSDoc @return type.
The following patterns are not considered problems:
/**
 * @returns {number}
 */
function quux () {

}

require-returns-check
Checks if the return expression exists in function body and in the comment.









Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
returns



The following patterns are considered problems:
/**
 * @returns
 */
function quux (foo) {

}
// Message: JSDoc @returns declaration present but return expression not available in function.

/**
 * @return
 */
function quux (foo) {

}
// Settings: {""jsdoc"":{""tagNamePreference"":{""returns"":""return""}}}
// Message: JSDoc @return declaration present but return expression not available in function.

/**
 * @returns
 */
const quux = () => {}
// Message: JSDoc @returns declaration present but return expression not available in function.

/**
 * @returns {undefined} Foo.
 * @returns {String} Foo.
 */
function quux () {

  return foo;
}
// Message: Found more than one @returns declaration.

const language = {
  /**
   * @param {string} name
   * @returns {string}
   */
  get name() {
    this._name = name;
  }
}
// Message: JSDoc @returns declaration present but return expression not available in function.
The following patterns are not considered problems:
/**
 * @returns Foo.
 */
function quux () {

  return foo;
}

/**
 * @returns {string} Foo.
 */
function quux () {

  return foo;
}

/**
 * @returns {string} Foo.
 */
function quux () {

  return foo;
}

/**
 *
 */
function quux () {
}

/**
 * @returns {*} Foo.
 */
const quux = () => foo;

/**
 * @returns {undefined} Foo.
 */
function quux () {}

/**
 * @returns { void } Foo.
 */
function quux () {}

/**
 * @returns {Promise<void>}
 */
async function quux() {}

/**
 * @returns {Promise<void>}
 */
const quux = async function () {}

/**
 * @returns {Promise<void>}
 */
const quux = async () => {}

/**
 * @returns Foo.
 * @abstract
 */
function quux () {
  throw new Error('must be implemented by subclass!');
}

/**
 * @returns Foo.
 * @virtual
 */
function quux () {
  throw new Error('must be implemented by subclass!');
}

/**
 * @returns Foo.
 * @constructor
 */
function quux () {
}

/**
 * @returns {undefined} Foo.
 */
function quux () {
}

/**
 * @returns {void} Foo.
 */
function quux () {
}

require-returns
Requires returns are documented.









Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
returns



The following patterns are considered problems:
/**
 *
 */
function quux (foo) {

  return foo;
}
// Message: Missing JSDoc @returns declaration.

/**
 *
 */
const foo = () => ({
  bar: 'baz'
})
// Message: Missing JSDoc @returns declaration.

/**
 *
 */
const foo = bar=>({ bar })
// Message: Missing JSDoc @returns declaration.

/**
 *
 */
const foo = bar => bar.baz()
// Message: Missing JSDoc @returns declaration.

/**
 *
 */
function quux (foo) {

  return foo;
}
// Settings: {""jsdoc"":{""tagNamePreference"":{""returns"":""return""}}}
// Message: Missing JSDoc @return declaration.

/**
 *
 */
async function quux() {}
// Message: Missing JSDoc @returns declaration.

/**
 *
 */
const quux = async function () {}
// Message: Missing JSDoc @returns declaration.

/**
 *
 */
const quux = async () => {}
// Message: Missing JSDoc @returns declaration.

/**
 *
 */
function quux () {
}
// Settings: {""jsdoc"":{""forceRequireReturn"":true}}
// Message: Missing JSDoc @returns declaration.

const language = {
  /**
   * @param {string} name
   */
  get name() {
    return this._name;
  }
}
// Message: Missing JSDoc @returns declaration.
The following patterns are not considered problems:
/**
 * @returns Foo.
 */
function quux () {

  return foo;
}

/**
 *
 */
function quux () {
}

/**
 *
 */
function quux (bar) {
  bar.filter(baz => {
    return baz.corge();
  })
}

/**
 * @returns Array
 */
function quux (bar) {
  return bar.filter(baz => {
    return baz.corge();
  })
}

/**
 * @returns Array
 */
const quux = (bar) => bar.filter(({ corge }) => corge())

/**
 * @inheritdoc
 */
function quux (foo) {
}

/**
 * @override
 */
function quux (foo) {
}

/**
 * @constructor
 */
function quux (foo) {
}

/**
 * @implements
 */
function quux (foo) {
}

/**
 * @override
 */
function quux (foo) {

  return foo;
}

/**
 * @class
 */
function quux (foo) {

}

/**
 * @constructor
 */
function quux (foo) {

}

/**
 * @returns {Object}
 */
function quux () {

  return {a: foo};
}

/**
 * @returns {Object}
 */
const quux = () => ({a: foo});

/**
 * @returns {Object}
 */
const quux = () => {
  return {a: foo}
};

/**
 * @returns {void}
 */
function quux () {
}

/**
 * @returns {void}
 */
const quux = () => {

}

/**
 * @returns {undefined}
 */
function quux () {
}

/**
 * @returns {undefined}
 */
const quux = () => {

}

/**
 *
 */
function quux () {
}

/**
 *
 */
const quux = () => {

}

class Foo {
  /**
   *
   */
  constructor () {
  }
}
// Settings: {""jsdoc"":{""forceRequireReturn"":true}}

const language = {
  /**
   * @param {string} name
   */
  set name(name) {
    this._name = name;
  }
}

valid-types
Requires all types to be valid JSDoc or Closure compiler types without syntax errors.
Also impacts behaviors on namepath-pointing or event-pointing tags:

@alias, @augments, @extends, @lends, @memberof, @memberof!, @mixes, @name, @this
@callback, @event, @listens, @fires, @emits
@borrows

The following apply to the above sets:

Expect tags in set 1 or 2 to have a valid namepath if present
Prevent set 2 from being empty by setting allowEmptyNamepaths to false as these tags might have some indicative value without a path (but set 1 will always fail if empty)
For the special case of set 3, i.e., @borrows <that namepath> as <this namepath>, check that both namepaths are present and valid and ensure there is an as  between them.










Context
ArrowFunctionExpression, FunctionDeclaration, FunctionExpression


Tags
param, returns



The following patterns are considered problems:
/**
 * @param {Array<string} foo
 */
function quux() {

}
// Message: Syntax error in type: Array<string

/**
 * @borrows foo% as bar
 */
function quux() {

}
// Message: Syntax error in type: foo%

/**
 * @borrows foo as bar%
 */
function quux() {

}
// Message: Syntax error in type: bar%

/**
 * @borrows foo
 */
function quux() {

}
// Message: @borrows must have an ""as"" expression. Found """"

/**
 * @see foo%
 */
function quux() {

}
// Settings: {""jsdoc"":{""checkSeesForNamepaths"":true}}
// Message: Syntax error in type: foo%

/**
 * @alias module:abc#event:foo-bar
 */
function quux() {

}
// Message: Syntax error in type: module:abc#event:foo-bar

/**
 * @callback
 */
function quux() {

}
// Settings: {""jsdoc"":{""allowEmptyNamepaths"":false}}
// Message: Syntax error in type: 
The following patterns are not considered problems:
/**
 * @param {Array<string>} foo
 */
function quux() {

}

/**
 * @param {string} foo
 */
function quux() {

}

/**
 * @param foo
 */
function quux() {

}

/**
 * @borrows foo as bar
 */
function quux() {

}

/**
 * @see foo%
 */
function quux() {

}

/**
 * @alias module:svgcanvas.SvgCanvas#event:ext_langReady
 */
function quux() {

}

/**
 * @callback
 */
function quux() {

}

/**
 * @see {@link foo}
 */
function quux() {

}
",234
tomsnail/snail-dev,Java,"Java开发帮助组件集 snail-dev
开发指导文档
docs链接
版本计划
【v2.0】
在项目实践中完善后的发布

【feature/v1.5】【计划中】
nginx+lua安全网关API集成

【feature/v1.4】【计划中】
完成原有第三方工具的集成和修改

【feature/v1.3】【计划中】
集成配置中心和API操作

集成Swagger

集成Zuul

【feature/v1.2】【计划中】
添加国际化消息
    
添加测试用例

【feature/v1.1】【进行中】
集成流程引擎
    
完善示例工程和代码

【feature/v1.0】【进行中】
添加代码检查和BUG插件

完成代码检查和BUG检查修复

去除多余代码

完善代码注释和JavaDoc

【v1.0】【完成】(2019.5.14)
完成核心功能重构，基础功能可用

",2
ProjectTIER/projecttier.org,Python,"





Project TIER (Teaching Integrity in Empirical Research) promotes the integration of principles and practices related to transparency and replicability in the research training of social scientists.
This repository is Project TIER's website (https://www.projecttier.org/), which is developed in Python using the Django-based Wagtail framework.
Local development

Install Vagrant and VirtualBox, if you haven't already.
Clone the project: git clone https://github.com/ProjectTIER/projecttier.org.git
Enter the project directory: cd projecttier.org
Start the Vagrant VM: vagrant up
Shell into the VM: vagrant ssh
Run the development server: djrun
Visit 0.0.0.0:8000 in your browser.

Pull production data/media
Your local version of the project will have an empty database and no media uploads.
You can copy the production database and files to your local version.

To pull the database: fab pull_production_data
To pull file uploads: fab pull_production_media

See fabfile.py for more information.
You'll need authentication to run these commands.
License and credits
Copyright © 2018 Richard Ball.
Licensed under the GNU AGPL 3.0. See the LICENSE file for the full license.
This project was originally developed by PromptWorks.
In April 2016 the project was inherited by Torchbox who continued its development.
In January 2018 the project was transferred to Candlewaster who continues to develop the project.
",6
zkweb-framework/ZKWeb,C#,"ZKWeb






ZKWeb is a flexible web framework for .NET Framework and .NET Core.
Why created this framework?

I want a better plugin system, one folder one plugin just like django
I want an independent mvc framework, because MS change their architecture too often
I want a powerful template system enough to implemente a visual page editor

Features

.NET Core Support

Support both .NET Framework and .NET Core


Plugin System

One folder one plugin, each contains everything it needs
Automatic compile and reload after source code has changed
Based on Roslyn compiler


Template System

Django style overlapping template file system

One plugin can just override other plugin's template


Template specialization for mobile or pc
Area-Widget style dynamic contents system

Able to implement a visual page editor based on this feature


Per-widget render cache

Most times it's better than cache the whole page


Based on DotLiquid


IoC Container

Fast
Provide IServiceProvider integration
ZKWeb own implementation


Multiple Host Environment

Support Asp.NET
Support Asp.NET Core
Support Owin
Use the common abstraction layer can make a plugin support all of this without different code


Multiple ORM

Support Dapper
Support EntityFramework Core (with full automatic database migration)
Support InMemory
Support MongoDB
Support NHibernate (with full automatic database migration)
Use the common abstraction layer can make a plugin support all of this with less different code


Localization

Multi-language support, with gettext style translation
Multi-timezone support


Caching

Policy based isolated cache

Isolated by device, request url, and more...


Abstraction layer for key-value cache


File Storage

Abstraction layer for file storage


Testing

Console and web test runner
Support IoC container overridden
Support Http context overridden
Support temporary database


Project Toolkits

Project Creator
Website Publisher


Linux support

Ubuntu 16.04 LTS 64bit
CentOS 7.2 64bit
Fedora 24 64bit



Features from the default plugin collection

Form generation and validation
Ajax table generation
CRUD page scaffolding
Scheduled Tasks
Captcha
Admin Panel
Automatic pesudo static
Multi-Currency and Region
And More...

Getting Started
In Short:
Open 'Tools\ProjectCreator.Gui.Windows\ZKWeb.Toolkits.ProjectCreator.Gui.exe' and create the project.

You can read README.md under Tools first, for more information please see the documents.
For now there only chinese documents, if you can't read chinese please ask the questions in 'Issues'.
For those chinese software engineers, is recommended to join QQ group 522083886 for further discuss.
Packages

ZKWeb: 
ZKWeb.Hosting.AspNet: 
ZKWeb.Hosting.AspNetCore: 
ZKWeb.Hosting.Owin: 
ZKWeb.ORM.Dapper: 
ZKWeb.ORM.EFCore: 
ZKWeb.ORM.InMemory: 
ZKWeb.ORM.MongoDB: 
ZKWeb.ORM.NHibernate: 

Links and License
Plugins: http://github.com/zkweb-framework/ZKWeb.Plugins
Documents: http://zkweb-framework.github.io (Chinese)
References: http://zkweb-framework.github.io/cn_v2.0/references/zkweb/ZKWebReferences.chm
MIT License
Copyright © 2016~2019 303248153@github
If you have any license issue please contact 303248153@qq.com.
",370
microsoft/TypeScript,TypeScript,"TypeScript





TypeScript is a language for application-scale JavaScript. TypeScript adds optional types to JavaScript that support tools for large-scale JavaScript applications for any browser, for any host, on any OS. TypeScript compiles to readable, standards-based JavaScript. Try it out at the playground, and stay up to date via our blog and Twitter account.
Installing
For the latest stable version:
npm install -g typescript
For our nightly builds:
npm install -g typescript@next
Contribute
There are many ways to contribute to TypeScript.

Submit bugs and help us verify fixes as they are checked in.
Review the source code changes.
Engage with other TypeScript users and developers on StackOverflow.
Join the #typescript discussion on Twitter.
Contribute bug fixes.
Read the language specification (docx,
pdf, md).

This project has adopted the Microsoft Open Source Code of Conduct. For more information see
the Code of Conduct FAQ or contact opencode@microsoft.com
with any additional questions or comments.
Documentation

Quick tutorial
Programming handbook
Language specification
Homepage

Building
In order to build the TypeScript compiler, ensure that you have Git and Node.js installed.
Clone a copy of the repo:
git clone https://github.com/Microsoft/TypeScript.git
Change to the TypeScript directory:
cd TypeScript
Install Gulp tools and dev dependencies:
npm install -g gulp
npm install
Use one of the following to build and test:
gulp local            # Build the compiler into built/local
gulp clean            # Delete the built compiler
gulp LKG              # Replace the last known good with the built one.
                      # Bootstrapping step to be executed when the built compiler reaches a stable state.
gulp tests            # Build the test infrastructure using the built compiler.
gulp runtests         # Run tests using the built compiler and test infrastructure.
                      # You can override the host or specify a test for this command.
                      # Use --host=<hostName> or --tests=<testPath>.
gulp baseline-accept  # This replaces the baseline test results with the results obtained from gulp runtests.
gulp lint             # Runs tslint on the TypeScript source.
gulp help             # List the above commands.

Usage
node built/local/tsc.js hello.ts
Roadmap
For details on our planned features and future direction please refer to our roadmap.
",49043
dotnet/core-setup,C#,".NET Core Runtime & Host Setup Repo
This repo contains the code to build the .NET Core runtime, libraries and shared host (dotnet) installers for
all supported platforms. It does not contain the actual sources to .NET Core runtime; this source is split across
the dotnet/coreclr repo (runtime) and dotnet/corefx repo (libraries).
Installation experience
The all-up installation experience is described in the installation scenarios
document in the dotnet/cli repo. That is the first step to get acquainted with the overall plan and experience we have
thought up for installing .NET Core bits.
Filing issues
This repo should contain issues that are tied to the installation of the ""muxer"" (the dotnet binary) and installation
of the .NET Core runtime and libraries.
For other issues, please use the following repos:

For overall .NET Core SDK issues, file on dotnet/cli repo
For class library and framework functioning issues, file on dotnet/corefx repo
For runtime issues, file on dotnet/coreclr issues

This project has adopted the code of conduct defined by the Contributor Covenant to clarify expected behavior in our community. For more information, see the .NET Foundation Code of Conduct.
Officially Released Builds
They can be downloaded from here.
Daily Builds
What build version has the CoreFX/CoreCLR commits I want?



Platform
Master
Release/3.0.X
Release/2.2.X
Release/2.1.X
Release/1.1.X
Release/1.0.X




Windows (x64)
Installer (Checksum)zip (Checksum)NetHost (zip)Symbols (zip)
Installer (Checksum)zip (Checksum)NetHost (zip)Symbols (zip)
Installer (Checksum)zip (Checksum)Symbols (zip)
Installer (Checksum)zip (Checksum)Symbols (zip)
Installerzip
Installerzip


Windows (x86)
Installer (Checksum)zip (Checksum)NetHost (zip)Symbols (zip)
Installer (Checksum)zip (Checksum)NetHost (zip)Symbols (zip)
Installer (Checksum)zip (Checksum)Symbols (zip)
Installer (Checksum)zip (Checksum)Symbols (zip)
Installerzip
Installerzip


Windows (arm32)
zip (Checksum)NetHost (zip)Symbols (zip)
zip (Checksum)NetHost (zip)Symbols (zip)
zip (Checksum)Symbols (zip)
zip (Checksum)Symbols (zip)
N/A
N/A


Windows (arm64)
zip (Checksum)NetHost (zip)Symbols (zip)
zip (Checksum)NetHost (zip)Symbols (zip)
zip (Checksum)Symbols (zip)
zip (Checksum)Symbols (zip)
N/A
N/A


Mac OS X (x64)
Installer (Checksum)tar.gz (Checksum)NetHost (tar.gz)Symbols (tar.gz)
Installer (Checksum)tar.gz (Checksum)NetHost (tar.gz)Symbols (tar.gz)
Installer (Checksum)tar.gz (Checksum)Symbols (tar.gz)
Installer (Checksum)tar.gz (Checksum)Symbols (tar.gz)
Installertar.gz
Installertar.gz


Linux (x64) (for glibc based OS)
tar.gz (Checksum)NetHost (tar.gz)Symbols (tar.gz)
tar.gz (Checksum)NetHost (tar.gz)Symbols (tar.gz)
tar.gz (Checksum)Symbols (tar.gz)
tar.gz (Checksum)Symbols (tar.gz)
N/A
N/A


Linux (armhf) (for glibc based OS)
tar.gz (Checksum)NetHost (tar.gz)Symbols (tar.gz)
tar.gz (Checksum)NetHost (tar.gz)Symbols (tar.gz)
tar.gz (Checksum)Symbols (tar.gz)
tar.gz (Checksum)Symbols (tar.gz)
N/A
N/A


Linux (arm64) (for glibc based OS)
tar.gz (Checksum)NetHost (tar.gz)Symbols (tar.gz)
tar.gz (Checksum)NetHost (tar.gz)Symbols (tar.gz)
tar.gz (Checksum)Symbols (tar.gz)
tar.gz (Checksum)Symbols (tar.gz)
N/A
N/A


Ubuntu 14.04 (x64)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
HostHost FX ResolverShared Frameworktar.gz
HostHost FX ResolverShared Frameworktar.gz


Ubuntu 16.04 (x64)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
HostHost FX ResolverShared Frameworktar.gz
HostHost FX ResolverShared Frameworktar.gz


Ubuntu 18.04 (x64)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
N/A
N/A


Ubuntu 19.04 (x64)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
N/A
N/A


Debian 8.2 (x64)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
HostHost FX ResolverShared Frameworktar.gz
HostHost FX ResolverShared Frameworktar.gz


Debian 9 (x64)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
N/A
N/A


CentOS 7 (x64)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
tar.gz
tar.gz


RHEL 6
tar.gz
tar.gz
tar.gz
tar.gz
N/A
N/A


RHEL 7.2 (x64)
Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
tar.gz
tar.gz


Fedora 23 (x64)
N/A
N/A
N/A
N/A
tar.gz
tar.gz


Fedora 24 (x64)
N/A
N/A
N/A
N/A
tar.gz
N/A


Fedora 27 (x64)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
N/A
N/A


SLES 12 (x64)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
N/A
N/A


OpenSUSE 42 (x64)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
Runtime-Deps (Checksum)Host (Checksum)Host FX Resolver (Checksum)Shared Framework (Checksum)
tar.gz
N/A


Linux-musl (x64)
tar.gz (Checksum)NetHost (tar.gz)Symbols (tar.gz)
tar.gz (Checksum)NetHost (tar.gz)Symbols (tar.gz)
tar.gz (Checksum)Symbols (tar.gz)
tar.gz (Checksum)Symbols (tar.gz)
N/A
N/A


Linux-musl (arm64)
tar.gz (Checksum)NetHost (tar.gz)Symbols (tar.gz)
tar.gz (Checksum)NetHost (tar.gz)Symbols (tar.gz)
N/A
N/A
N/A
N/A



Note: Our Linux packages (.deb and .rpm) are put together slightly differently than the Windows and Mac specific installers. Instead of combining everything, we have separate component packages that depend on each other. If you're installing these directly from the installer files (via dpkg or similar), then you'll need to install them in the order presented above.
",361
bhlvoong/LBTATools,Swift,"





The cure for boring UI code
Do you suffer from ugly layout code and boring UICollectionViewController boilerplate code?  Yes, we've all been there. I'm not the only one that has written monstrous view setup functions that look like beasts from hell.
Now, there is a solution to this problem: LBTATools.
There are 3 main issues I want to tackle with this library:

Use UIStackView to layout everything in a single line.
Create quick vertical and horizontal lists, but skip the boring cell registration, numItemsForSection, cellForItemAt code.
Generate a UILabel, UIButton, UIView, etc.. with one line of code.

This library contains extensions and classes that speed up development for my client work.  Hopefully, you can take advantage of some of these techniques.
1. Stack vertically and horizontally
Layouts can usually be broken down into some combination of horizontal and vertical UIStackViews. The following examples illustrate usage of stack and hstack for common layout patterns.
Example 1: Simple Vertical Layout

stack(imageView, nameLabel)
Example 2: Horizontal then vertical with center alignment

hstack(imageView,
       stack(nameLabel, messageLabel, spacing: 4),
       spacing: 16, alignment: .center)
// The key is to use stackView.alignment = .center   
Example 3: Embedded stacking with layoutMargins.

stack(imageView,
      stack(titleLabel, 
      	    descriptionLabel, 
      	    UIView(), 
      	    exploreLabel, spacing: 16)).withMargins(.allSides(16)
// Using stackView.layoutMargins allows for easy padding manipulation

2. Fast and Easy ListController
Writing iOS apps will almost always involve lists, lots and lots of lists.  Most of these will be vertical but horizontal ones are quite common too.  Only works for single cell type lists.
Tinder Messages List Example


Using LBTAListController, you can build this common pattern with just a few lines. First, let's simplify the view into one vertical list and a header that contains a horizontal list:


Building this list is very easy now:
class GreenCell: LBTAListCell<UIColor> {
    override var item: UIColor! { didSet { backgroundColor = item }}
}

class SimpleListController: LBTAListController<GreenCell, UIColor, SimpleHeader> {
    override func viewDidLoad() {
        super.viewDidLoad()
        items = [.green, .green, .green, .green]
    }
    // sizing methods
}
The header also contains a ListController component:
class SimpleHeader: UICollectionReusableView {
    
    class BlueCell: LBTAListCell<UIColor> {
        override var item: UIColor! { didSet { backgroundColor = item }}
    }
    class HeaderHorizontalController: LBTAListController<BlueCell,
        UIColor, UICollectionReusableView> {
        override func viewDidLoad() {
            super.viewDidLoad()
            items = [.blue, .blue, .blue, .blue]
        }
    }
    
    let blueCellsHorizontalController = HeaderHorizontalController(scrollDirection: .horizontal)
    
    override init(frame: CGRect) {
        super.init(frame: frame)
        stack(blueCellsHorizontalController.view)
    }
    
    required init?(coder aDecoder: NSCoder) {
        fatalError()
    }
}
Run the example project in this repository to see the code in its entirety.  LBTAListController uses the power of Generics to handle dynamics cell and header classes.
3. One line UI elements
One major issue with creating UI elements is the amount of properties we have to set during the setup phase.  Here's a very common, and ugly, chunk of code that we want to avoid:
let nameLabel: UILabel = {
    let label = UILabel()
    label.text = ""Name""
    label.textColor = .black
    label.textAlignment = .center
    label.font = .boldSystemFont(ofSize: 16)
    label.numberOfLines = 2
    return label
}()
In total, this is 9 lines of code and gets out of control when creating multiple labels, buttons, etc.  So instead, let's make this simple and elegant with one line:
let nameLabel = UILabel(text: ""Name"", font: .boldSystemFont(ofSize: 16), textColor: .black, 
	textAlignment: .center, numberOfLines: 2)
All of the above parameters are optional, this means you can also use:
let nameLabel = UILabel(text: ""Name"", numberOfLines: 2)
Creating UIButtons also fall into this category of code from hell, so let's make it easy with another one-liner:
let nextButton = UIButton(title: ""Next"", titleColor: .white, font: .boldSystemFont(ofSize: 18), 
	backgroundColor: .white, target: self, action: #selector(handleNext))
Video Tutorials
Although this library is very easy to use, I recognize that a lot of you want some visual examples so below are some video tutorials:
Using stack and hstack
Installation - Cocoapods
CocoaPods is an easy to use dependency manager . To install LBTATools, simply add the following line to your Podfile:
pod 'LBTATools'
No, I don't see myself writing this for SPM or any other system.
Author
Brian Voong  @buildthatapp YouTube
License
LBTATools is available under the MIT license. See the LICENSE file for more info.
",217
ukontainer/runu,Go,"

runu
OCI runtime for frankenlibc unikernel
Installation
make
sudo cp runu /usr/local/bin/runu

add an entry to /etc/docker/daemon.json
        ""runu"": {
            ""path"": ""/usr/local/bin/runu"",
            ""runtimeArgs"": [
            ]
        },

",2
bhlvoong/LBTATools,Swift,"





The cure for boring UI code
Do you suffer from ugly layout code and boring UICollectionViewController boilerplate code?  Yes, we've all been there. I'm not the only one that has written monstrous view setup functions that look like beasts from hell.
Now, there is a solution to this problem: LBTATools.
There are 3 main issues I want to tackle with this library:

Use UIStackView to layout everything in a single line.
Create quick vertical and horizontal lists, but skip the boring cell registration, numItemsForSection, cellForItemAt code.
Generate a UILabel, UIButton, UIView, etc.. with one line of code.

This library contains extensions and classes that speed up development for my client work.  Hopefully, you can take advantage of some of these techniques.
1. Stack vertically and horizontally
Layouts can usually be broken down into some combination of horizontal and vertical UIStackViews. The following examples illustrate usage of stack and hstack for common layout patterns.
Example 1: Simple Vertical Layout

stack(imageView, nameLabel)
Example 2: Horizontal then vertical with center alignment

hstack(imageView,
       stack(nameLabel, messageLabel, spacing: 4),
       spacing: 16, alignment: .center)
// The key is to use stackView.alignment = .center   
Example 3: Embedded stacking with layoutMargins.

stack(imageView,
      stack(titleLabel, 
      	    descriptionLabel, 
      	    UIView(), 
      	    exploreLabel, spacing: 16)).withMargins(.allSides(16)
// Using stackView.layoutMargins allows for easy padding manipulation

2. Fast and Easy ListController
Writing iOS apps will almost always involve lists, lots and lots of lists.  Most of these will be vertical but horizontal ones are quite common too.  Only works for single cell type lists.
Tinder Messages List Example


Using LBTAListController, you can build this common pattern with just a few lines. First, let's simplify the view into one vertical list and a header that contains a horizontal list:


Building this list is very easy now:
class GreenCell: LBTAListCell<UIColor> {
    override var item: UIColor! { didSet { backgroundColor = item }}
}

class SimpleListController: LBTAListController<GreenCell, UIColor, SimpleHeader> {
    override func viewDidLoad() {
        super.viewDidLoad()
        items = [.green, .green, .green, .green]
    }
    // sizing methods
}
The header also contains a ListController component:
class SimpleHeader: UICollectionReusableView {
    
    class BlueCell: LBTAListCell<UIColor> {
        override var item: UIColor! { didSet { backgroundColor = item }}
    }
    class HeaderHorizontalController: LBTAListController<BlueCell,
        UIColor, UICollectionReusableView> {
        override func viewDidLoad() {
            super.viewDidLoad()
            items = [.blue, .blue, .blue, .blue]
        }
    }
    
    let blueCellsHorizontalController = HeaderHorizontalController(scrollDirection: .horizontal)
    
    override init(frame: CGRect) {
        super.init(frame: frame)
        stack(blueCellsHorizontalController.view)
    }
    
    required init?(coder aDecoder: NSCoder) {
        fatalError()
    }
}
Run the example project in this repository to see the code in its entirety.  LBTAListController uses the power of Generics to handle dynamics cell and header classes.
3. One line UI elements
One major issue with creating UI elements is the amount of properties we have to set during the setup phase.  Here's a very common, and ugly, chunk of code that we want to avoid:
let nameLabel: UILabel = {
    let label = UILabel()
    label.text = ""Name""
    label.textColor = .black
    label.textAlignment = .center
    label.font = .boldSystemFont(ofSize: 16)
    label.numberOfLines = 2
    return label
}()
In total, this is 9 lines of code and gets out of control when creating multiple labels, buttons, etc.  So instead, let's make this simple and elegant with one line:
let nameLabel = UILabel(text: ""Name"", font: .boldSystemFont(ofSize: 16), textColor: .black, 
	textAlignment: .center, numberOfLines: 2)
All of the above parameters are optional, this means you can also use:
let nameLabel = UILabel(text: ""Name"", numberOfLines: 2)
Creating UIButtons also fall into this category of code from hell, so let's make it easy with another one-liner:
let nextButton = UIButton(title: ""Next"", titleColor: .white, font: .boldSystemFont(ofSize: 18), 
	backgroundColor: .white, target: self, action: #selector(handleNext))
Video Tutorials
Although this library is very easy to use, I recognize that a lot of you want some visual examples so below are some video tutorials:
Using stack and hstack
Installation - Cocoapods
CocoaPods is an easy to use dependency manager . To install LBTATools, simply add the following line to your Podfile:
pod 'LBTATools'
No, I don't see myself writing this for SPM or any other system.
Author
Brian Voong  @buildthatapp YouTube
License
LBTATools is available under the MIT license. See the LICENSE file for more info.
",217
ukontainer/runu,Go,"

runu
OCI runtime for frankenlibc unikernel
Installation
make
sudo cp runu /usr/local/bin/runu

add an entry to /etc/docker/daemon.json
        ""runu"": {
            ""path"": ""/usr/local/bin/runu"",
            ""runtimeArgs"": [
            ]
        },

",2
hajimeo/samples,Shell,"samples
",3
apache/incubator-mxnet-site,HTML,"



Apache MXNet (incubating) for Deep Learning



Master
Docs
License











Apache MXNet (incubating) is a deep learning framework designed for both efficiency and flexibility.
It allows you to mix symbolic and imperative programming
to maximize efficiency and productivity.
At its core, MXNet contains a dynamic dependency scheduler that automatically parallelizes both symbolic and imperative operations on the fly.
A graph optimization layer on top of that makes symbolic execution fast and memory efficient.
MXNet is portable and lightweight, scaling effectively to multiple GPUs and multiple machines.
MXNet is more than a deep learning project. It is a collection of
blue prints and guidelines for building
deep learning systems, and interesting insights of DL systems for hackers.
Ask Questions

Please use discuss.mxnet.io for asking questions.
Please use mxnet/issues for reporting bugs.
Frequent Asked Questions

How to Contribute

Contribute to MXNet

What's New

Version 1.4.0 Release - MXNet 1.4.0 Release.
Version 1.3.1 Release - MXNet 1.3.1 Patch Release.
Version 1.3.0 Release - MXNet 1.3.0 Release.
Version 1.2.0 Release - MXNet 1.2.0 Release.
Version 1.1.0 Release - MXNet 1.1.0 Release.
Version 1.0.0 Release - MXNet 1.0.0 Release.
Version 0.12.1 Release - MXNet 0.12.1 Patch Release.
Version 0.12.0 Release - MXNet 0.12.0 Release.
Version 0.11.0 Release - MXNet 0.11.0 Release.
Apache Incubator - We are now an Apache Incubator project.
Version 0.10.0 Release - MXNet 0.10.0 Release.
Version 0.9.3 Release - First 0.9 official release.
Version 0.9.1 Release (NNVM refactor) - NNVM branch is merged into master now. An official release will be made soon.
Version 0.8.0 Release
Updated Image Classification with new Pre-trained Models
Notebooks How to Use MXNet
MKLDNN for Faster CPU Performance
MXNet Memory Monger, Training Deeper Nets with Sublinear Memory Cost
Tutorial for NVidia GTC 2016
Embedding Torch layers and functions in MXNet
MXNet.js: Javascript Package for Deep Learning in Browser (without server)

Design Note: Design Efficient Deep Learning Data Loading Module
MXNet on Mobile Device
Distributed Training
Guide to Creating New Operators (Layers)
Go binding for inference
Amalgamation and Go Binding for Predictors - Outdated
Large Scale Image Classification

Contents

Documentation and  Tutorials
Design Notes
Code Examples
Installation
Pretrained Models

Features

Design notes providing useful insights that can re-used by other DL projects
Flexible configuration for arbitrary computation graph
Mix and match imperative and symbolic programming to maximize flexibility and efficiency
Lightweight, memory efficient and portable to smart devices
Scales up to multi GPUs and distributed setting with auto parallelism
Support for Python, Scala, C++, Java, Clojure, R, Go, Javascript, Perl, Matlab, and Julia
Cloud-friendly and directly compatible with S3, HDFS, and Azure

License
Licensed under an Apache-2.0 license.
Reference Paper
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao,
Bing Xu, Chiyuan Zhang, and Zheng Zhang.
MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems.
In Neural Information Processing Systems, Workshop on Machine Learning Systems, 2015
History
MXNet emerged from a collaboration by the authors of cxxnet, minerva, and purine2. The project reflects what we have learned from the past projects. MXNet combines aspects of each of these projects to achieve flexibility, speed, and memory efficiency.
",10
mmdmz/athena-web-app,JavaScript,"This project was bootstrapped with Create React App.
Available Scripts
In the project directory, you can run:
npm start
Runs the app in the development mode.
Open http://localhost:3000 to view it in the browser.
The page will reload if you make edits.
You will also see any lint errors in the console.
npm test
Launches the test runner in the interactive watch mode.
See the section about running tests for more information.
npm run build
Builds the app for production to the build folder.
It correctly bundles React in production mode and optimizes the build for the best performance.
The build is minified and the filenames include the hashes.
Your app is ready to be deployed!
See the section about deployment for more information.
npm run eject
Note: this is a one-way operation. Once you eject, you can’t go back!
If you aren’t satisfied with the build tool and configuration choices, you can eject at any time. This command will remove the single build dependency from your project.
Instead, it will copy all the configuration files and the transitive dependencies (Webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except eject will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own.
You don’t have to ever use eject. The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it.
Learn More
You can learn more in the Create React App documentation.
To learn React, check out the React documentation.
Code Splitting
This section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting
Analyzing the Bundle Size
This section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size
Making a Progressive Web App
This section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app
Advanced Configuration
This section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration
Deployment
This section has moved here: https://facebook.github.io/create-react-app/docs/deployment
npm run build fails to minify
This section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify
",2
ninjadotorg/handshake-i18n,JavaScript,"Handshake i18n
A special thank to Crowdin for help us use their tool for translate everything on handshake-app project (https://crowdin.com/project/handshake-app) at no cost.
Install
yarn

Run
All of process
yarn i18n

Build parsed file need to i18n
yarn i18n-prepare

Send to Crowdin
yarn send

Build translated file and re-parsed
yarn handle-i18n-ed

Misc notes
git submodule init
git submodule add git@github.com:ninjadotorg/handshake-app.git app
git submodule update
git submodule foreach git pull origin develop

",2
TurtlePay/blockchain-cache-api,JavaScript,"TurtlePay™ Blockchain Cache API
Master Build Status
 
Prerequisites

MariaDB/MySQL with InnoDB support
RabbitMQ
TurtlePay: Blockchain Data Collection Agent
TurtlePay: Blockchain Relay Agent
Node.js LTS

Foreword
We know that this documentation needs cleaned up and made easier to read. We'll compile it as part of the full documentation as the project moves forward.
Setup

Clone this repository to wherever you'd like the API to run:

git clone https://github.com/TurtlePay/blockchain-cache-api

Install the required Node.js modules

cd blockchain-cache-api && npm install

Use your favorite text editor to change the values as necessary in config.json

Note: Make sure you use a read-only database user for security reasons
{
  ""bindIp"": ""0.0.0.0"",
  ""httpPort"": 80,
  ""corsHeader"": ""*""
}

Fire up the script

export RABBIT_PUBLIC_SERVER=localhost
export RABBIT_PUBLIC_USERNAME=yourrabbitmqusername
export RABBIT_PUBLIC_PASSWORD=yourrabbitmqpassword
export MYSQL_HOST=localhost
export MYSQL_PORT=3306
export MYSQL_USERNAME=yourdbusername
export MYSQL_PASSWORD=yourdbpassword
export MYSQL_DATABASE=yourdbname
node index.js

Optionally, install PM2 or another process manager to keep the service running.

npm install -g pm2@latest
pm2 startup
pm2 start index.js --name blockchain-cache-api -i max
pm2 save
API
See https://docs.turtlepay.io/blockapi/ for the full REST API provided by this package.
(c) 2018 TurtlePay™ Development Team
",2
SAP/SapMachine-infrastructure,Python,"SapMachine Build Infrastructure
Description
This repository contains tools, scripts and infrastructure required to build, test and maintain the SapMachine project.
The jobs run on our Jenkins installation.
Mercurial repos are imported to branches jdk/jdk and jdk/jdk10.
Every few hours, we poll the upstream mercurial repositories and add new changes and tags (update-pipeline on jenkins).
The SapMachine Github Repository https://github.com/SAP/SapMachine is organized into the following branches:

jdk/jdk and jdk/jdk10 are mirrors of the corresponding mercurial repos.
sapmachine10: jdk/jdk10 + our changes.
sapmachine: jdk/jdk + our changes.
sapmachine10-alpine: sapmachine10 + alpine changes.
sapmachine-alpine: sapmachine + alpine changes.

We cherry-pick our changes between sapmachine and sapmachine10.
We merge jdk/jdk10 and jdk/jdk with new build tags.
The job check-tag-pipeline, polls these branches for new tags, opens pull requests and starts validation jobs.
Merge is triggered manually, after reviewing test and build results and resolving conflicts if needed.
Build-jobs run in docker containers to have a reproducible build environment.
Different build-jobs use the same Pipeline with different parameters.
Build jobs start test jobs. However, we don't use the result of the tests as indicator of a failure of the build job, as some failures have to be considered a normal. Some tests are shaky, other failure address open issues that will be fixed with the next build. However, we should compare our results to the results reported here: http://download.java.net/openjdk/testresults/10/testresults.html
Requirements
Jenkins Installation
We run the jobs on a jenkins installation with one master and two slaves.
As most of the jobs run in docker containers, docker must be installed on the slave machines.
Access to SapMachine Repository
Some of the jobs need push access to the SapMachine repository. It is possible to work with a fork of this repository. The credentials have to be configured in Jenkis.
Installation
After installing jenkins, one pipelin job has to be configured that runs the Pipeline jenkins-restore-pipeline. This jobs imports the whole jenkins configuration. After running this job, the configuration has to be reloaded.
To get all the jobs to function, missing credentials have to be added. Depending on the jenkins installation, some missing plugins have to be installed.
If working with a forked SapMachine repository, the repository URL in most of the Jenkinsfiles has to be changed accordingly.
Known Issues
There are no known issues.
How to Obtain Support
This project is provided as is.
License
Copyright (c) 2017-2018 SAP SE or an SAP affiliate company. All rights reserved.
This file is licensed under the Apache Software License, v. 2 except as noted otherwise in the LICENSE file.
",12
chimo/gs-nodeinfo,PHP,"Nodeinfo plugin for GNU social
Plugin that presents basic instance information using the NodeInfo standard.
At the moment, the information is presented at the ""/main/nodeinfo/2.0"" endpoint.
Other tools can then scrape that information and present it in various ways. For example: https://fediverse.network/
Instructions

Make sure the files are in a folder called Nodeinfo if they're not already
Put the folder in your /local/plugins/ directory (create the directory if it doesn't exist)
Tell /config.php to use it with: addPlugin('Nodeinfo');

",5
antonbabenko/terraform-docs-as-pdf,Shell,"Complete Terraform documentation as PDF files. Updating nightly.




Once upon a time, I've been missing Terraform documentation on my flight across Atlantic.

Features

Official content from terraform.io web-site is used. Source: terraform-website.
Include documentation for 95+ officially supported Terraform providers. Source: terraform-providers.
Updating nightly.
Documentation is generated from the web-site running locally.
Get updates in your Dropbox folder - https://www.dropbox.com/sh/pfj9e1m5d7nqhp3/AAAqW1OI3oYPa4UXfYb4fLGaa

How to use this?


Don't have Dropbox account? Open one using my referral link and I will get 500M extra. :)


To get PDFs to your Dropbox folder - click this shared link, then choose ""Download"" and ""Save to my Dropbox"".


Alternatively, browse PDFs online - https://www.dropbox.com/sh/pfj9e1m5d7nqhp3/AAAqW1OI3oYPa4UXfYb4fLGaa


Existing PDF files

All in one: Terraform docs with all official providers
Terraform docs (without providers)
Terraform docs for each provider

Authors
This project is maintained by Anton Babenko with help from these awesome contributors.
License
MIT Licensed. See LICENSE for full details.
Terraform documentation itself is licensed under MPL 2.0. See LICENSE.md for more details.
Latest update: 2019-05-17
",42
antonbabenko/terraform-docs-as-pdf,Shell,"Complete Terraform documentation as PDF files. Updating nightly.




Once upon a time, I've been missing Terraform documentation on my flight across Atlantic.

Features

Official content from terraform.io web-site is used. Source: terraform-website.
Include documentation for 95+ officially supported Terraform providers. Source: terraform-providers.
Updating nightly.
Documentation is generated from the web-site running locally.
Get updates in your Dropbox folder - https://www.dropbox.com/sh/pfj9e1m5d7nqhp3/AAAqW1OI3oYPa4UXfYb4fLGaa

How to use this?


Don't have Dropbox account? Open one using my referral link and I will get 500M extra. :)


To get PDFs to your Dropbox folder - click this shared link, then choose ""Download"" and ""Save to my Dropbox"".


Alternatively, browse PDFs online - https://www.dropbox.com/sh/pfj9e1m5d7nqhp3/AAAqW1OI3oYPa4UXfYb4fLGaa


Existing PDF files

All in one: Terraform docs with all official providers
Terraform docs (without providers)
Terraform docs for each provider

Authors
This project is maintained by Anton Babenko with help from these awesome contributors.
License
MIT Licensed. See LICENSE for full details.
Terraform documentation itself is licensed under MPL 2.0. See LICENSE.md for more details.
Latest update: 2019-05-17
",42
drinkbeer/CodingQuestion,Java,"CodingQuestion
Sharing common interview questions through this reporsity.
Array
Array includes N-Sum series, operation to array(rotate or pivot). DFS or BFS search a subset of array, e.g combination.
Binary Search is also a hot topic for sorted array. When see the coding problem requires O(logN) time complexity to search something in a sorted array, then it's probably using Binary Search.
Give an array, always ask interview: if the array is sorted? Could we sort the array if necessary? If the array is empty or null? If that could be huge array that will be out of memory?
Note:

If a topic is about DP or Greedy, it's put to DP section, e.g. 45. Jump Game II.

K-Sum problem summary



Question
Solution
Tags




★1.Two Sum
LeetCode-1-Two-Sum.java
Array, HashMap


4. Median of Two Sorted Arrays
LeetCode-4-Median-of-Two-Sorted-Arrays.java
Array


11 Container With Most Water
LeetCode-11-Container-With-Most-Water.java
Array, Two Pointers


★15.3Sum
LeetCode-15-3Sum.java
Array, Three Pointers


16 3Sum Closest
LeetCode-16-3Sum-Closest.java
Array, Three Pointers


18.4Sum
LeetCode-18-4Sum.java
Four Pointers


26.Remove Duplicates from Sorted Array
LeetCode-26-Remove-Duplicates-from-Sorted-Array.java
One Pointers


27.Remove Element
LeetCode-27-Remove-Element.java
One or Two Pointers


31.Next Permutation
LeetCode-31-Next-Permutation.java
Array, Hard to Understand


33.Search in Rotated Sorted Array
LeetCode-33-Search-in-Rotated-Sorted-Array.java
Binary Search


34.Search for a Range
LeetCode-34-Search-for-a-Range.java
Binary Search


35 Search Insert Position
LeetCode-35-Search-Insert-Position.java
Binary Search


39.Combination Sum I
LeetCode-39-Combination-Sum-I.java
DFS


40.Combination Sum II
LeetCode-40-Combination-Sum-II.java
DFS


41. First Missing Positive
LeetCode-41-First-Missing-Positive.java
Array, rearrange the array


42. Trapping Rain Water
LeetCode-42-Trapping-Rain-Water.java
Two Pointers, Two Way Scanning


46.Permutations
LeetCode-46-Permutations.java
DFS, BFS


47.Permutations II
LeetCode-47-Permutations-II.java
DFS, BFS


48 Rotate Image
LeetCode-48-Rotate-Image.java
Array


54 Spiral Matrix
LeetCode-54-Spiral-Matrix.java
Array


59 Spiral Matrix II
LeetCode-59-Spiral-Matrix-II.java
Array


56.Merge Intervals
LeetCode-56-Merge-Intervals.java
Array


57.Insert Interval
LeetCode-57-Insert-Interval.java
Array


66.Plus One
LeetCode-66-Plus-One.java
Array


★78.Subsets
LeetCode-78-Subsets.java
DFS


90.Subsets II
LeetCode-90-Subsets-II.java
DFS


283.Move Zeroes
LeetCode-283-Move-Zeroes.java
Array


252.Meeting Rooms
LeetCode-252-Meeting-Rooms.java
Array, similar to 56.Merge Intervals


253.Meeting Rooms II
LeetCode-253-Meeting-Rooms-II.java
Array, similar to 56.Merge Intervals


435. Non-overlapping Intervals
LeetCode-435-Non-overlapping-Intervals.java
Array, similar to 56.Merge Intervals



String
String includes Parentheness series, Palindrome series, some statistic problems.
回文串问题总结
LeetCode总结 —— Palindrome 回文总结
最长回文字符串、二重循环
「LeetCode」试题总结 - 回文数汇总



Question
Solution
Tags




167.Two Sum II - Input array is sorted
LeetCode-167-Two-Sum-II-Input-array-is-sorted.java
Two Pointers


170.Two Sum III - Data structure design
LeetCode-170-Two-Sum-III-Data-structure-design.java
HashMap


259.3Sum Smaller
LeetCode-259-3Sum-Smaller.java
Two Pointers


6 ZigZag Conversion
LeetCode-6-ZigZag-Conversion.java
String


14 Longest Common Prefix
LeetCode-14-Longest-Common-Prefix.java
String


36 Valid Sudoku
LeetCode-36-Valid-Sudoku.java
Array


38 Count and Say
LeetCode-38-Count-and-Say.java
String


58.Length of Last Word
LeetCode-58-Length-of-Last-Word.java
String


71.Simplify Path
LeetCode-71-Simplify-Path.java
String


73.Set Matrix Zeroes
LeetCode-73-Set-Matrix-Zeroes.java
Array


242.Valid Anagram
LeetCode-242-Valid-Anagram.java
Sort, Array Map


49.Group Anagrams
LeetCode-49-Group-Anagrams.java
Merge Sort, Priority Queue


189.Rotate Array
LeetCode-189-Rotate-Array.java
Array


★20.Valid Parentheses
LeetCode-20-Valid-Parentheses.java
HashMap, Stack


22.Generate Parentheses
LeetCode-22-Generate-Parentheses.java
Not Yet


32.Longest Valid Parentheses
LeetCode-32-Longest-Valid-Parentheses.java
Not Yet


241.Different Ways to Add Parentheses
LeetCode-241.Different Ways to Add Parentheses.java
Not Yet


301.Remove Invalid Parentheses
LeetCode-301.Remove Invalid Parentheses.java
Not Yet


★125.Valid Palindrome
LeetCode-125-Valid-Palindrome.java
Two Pointers


5.Longest Palindromic Substring
LeetCode-5-Longest-Palindromic-Substring.java
String


9 Palindrome Number
LeetCode-9-Palindrome-Number.java
Math


234.Palindrome Linked List
LeetCode-234-Palindrome-Linked-List.java
LinkedList


131.Palindrome Partitioning
LeetCode-131-Palindrome-Partitioning.java
DFS


★132.Palindrome Partitioning II
LeetCode-132-Palindrome-Partitioning-II.java
DP


214.Shortest Palindrome
LeetCode-214-Shortest-Palindrome.java
Not Yet, Hard


266.Palindrome Permutation
LeetCode-266-Palindrome-Permutation.java
HashMap


267.Palindrome Permutation II
LeetCode-267-Palindrome-Permutation-II.java
Not Yet


12.Integer to Roman
LeetCode-12-Integer-to-Roman.java
String


13.Roman to Integer
LeetCode-13-Roman-to-Integer.java
String


165.Compare Version Numbers
LeetCode-165-Compare-Version-Numbers.java
String


288.Unique Word Abbreviation
LeetCode-288-Unique-Word-Abbreviation.java
String



Dynamic Programming



Question
Solution
Tags




★★★55.Jump Game
LeetCode-55-Jump-Game.java
DP, Greedy


45.Jump Game II
LeetCode-45-Jump-Game-II.java
DP, Greedy


★53.Maximum Subarray
LeetCode-53-Maximum-Subarray.java
DP, Greedy


Maximum Subarray II
LintCode-Maximum-Subarray-II.java
DP, Greedy


Maximum Subarray III
LintCode-Maximum-Subarray-III.java
not yet, hard


121.Best Time to Buy and Sell Stock
LeetCode-121-Best-Time-to-Buy-and-Sell-Stock.java
DP, Greedy


122.Best Time to Buy and Sell Stock II
LeetCode-122-Best-Time-to-Buy-and-Sell-Stock-II.java
Array


123.Best Time to Buy and Sell Stock III
LeetCode-123-Best-Time-to-Buy-and-Sell-Stock-III.java
hard,not yet???


188.Best Time to Buy and Sell Stock IV
LeetCode-188-Best-Time-to-Buy-and-Sell-Stock-IV.java
hard, not yet???


309.Best Time to Buy and Sell Stock with Cooldown
LeetCode-309-Best-Time-to-Buy-and-Sell-Stock-with-Cooldown.java
not yet???


★62.Unique Paths
LeetCode-62-Unique-Paths.java
DP


63.Unique Paths II
LeetCode-63-Unique-Paths-II.java
DP


64.Minimum Path Sum
LeetCode-64-Minimum-Path-Sum.java
DP



Search(Tree, Binary Search, DFS, BFS, DP)
Search in Tree, in Binary Tree, Binary Search Tree, in String, in Array
Search a required set, search Maximum/Minimu result
Tree大总结

求二叉树中的节点个数: getNodeNumRec（递归），getNodeNum（迭代）
求二叉树的深度: getDepthRec（递归），getDepth
★★★前序遍历，中序遍历，后序遍历: preorderTraversalRec, preorderTraversal, inorderTraversalRec, postorderTraversalRec
★★★分层遍历二叉树（按层次从上往下，从左往右）:levelTraversal, levelTraversalRec（递归解法）
将二叉查找树变为有序的双向链表: convertBST2DLLRec, convertBST2DLL
求二叉树第K层的节点个数：getNodeNumKthLevelRec, getNodeNumKthLevel
求二叉树中叶子节点的个数：getNodeNumLeafRec, getNodeNumLeaf
判断两棵二叉树是否相同的树：isSameRec, isSame
判断二叉树是不是平衡二叉树：isAVLRec
求二叉树的镜像（破坏和不破坏原来的树两种情况）：
mirrorRec, mirrorCopyRec
mirror, mirrorCopy
10.1 判断两个树是否互相镜像：isMirrorRec isMirror
求二叉树中两个节点的最低公共祖先节点：
LAC        求解最小公共祖先, 使用list来存储path
LCABstRec  递归求解BST树
LCARec     递归算法
求二叉树中节点的最大距离：getMaxDistanceRec
由前序遍历序列和中序遍历序列重建二叉树：rebuildBinaryTreeRec
判断二叉树是不是完全二叉树：isCompleteBinaryTree, isCompleteBinaryTreeRec
找出二叉树中最长连续子串(即全部往左的连续节点，或是全部往右的连续节点）: findLongest

面试大总结之二：Java搞定面试中的二叉树题目
轻松搞定面试中的二叉树题目
算法大全(3)二叉树



Question
Solution
Tags




130.Surrounded Region
LeetCode-130-Surrounded-Regions.java
BFS,DFS


200.Number of Islands
LeetCode-200-Number-of-Islands.java
DFS


216.Combination Sum III
LeetCode-216-Combination-Sum-III.java
Not yet


77.Combinations
LeetCode-77-Combinations.java
DFS


278.First Bad Version
LeetCode-278-First-Bad-Version.java
Binary Search


81.Search in Rotated Sorted Array II
LeetCode-81-Search-in-Rotated-Sorted-Array-II.java
Array


74.Search a 2D Matrix
LeetCode-74-Search-a-2D-Matrix.java
Binary Search


240.Search a 2D Matrix II
LeetCode-240-Search-a-2D-Matrix-II.java
not yet


61 Rotate List
LeetCode-61-Rotate-List.java
Two Pointers


70.Climbing Stairs
LeetCode-70-Climbing-Stairs.java
DP


28.Implement strStr()
LeetCode-28-Implement-strStr.java
Two Pointers


75 Sort Colors
LeetCode-75-Sort-Colors.java
Two Pointers


79.Word Search
LeetCode-79-Word-Search.java
DFS


212.Word Search II
LeetCode-212-Word-Search-II.java
Not yet, hard


86.Partition List
LeetCode-86-Partition-List.java
Two Pointers


205.Isomorphic Strings
LeetCode-205-Isomorphic-Strings.java
HashMap


290.Word Pattern
LeetCode-290-Word-Pattern.java
HashMap


291.Word Pattern II
LeetCode-291-Word-Pattern-II.java
Not yet, hard


127.Word Ladder
LeetCode-127-Word-Ladder.java
BFS


126.Word Ladder II
LeetCode-126-Word-Ladder-II.java
Not yet, hard


139.Word Break
LeetCode-139-Word-Break.java
DFS, DP


140.Word Break II
LeetCode-140-Word-Break-II.java
Not yet, hard


91.Decode Ways
LeetCode-91-Decode-Ways.java
DP


93.Restore IP Addresses
LeetCode-93-Restore-IP-Addresses.java
DFS


98.Validate Binary Search Tree
LeetCode-98-Validate-Binary-Search-Tree.java
DFS, BFS


101.Symmetric Tree
LeetCode-101-Symmetric-Tree.java
Recursive, Iterative


110.Balanced Binary Tree
LeetCode-110-Balanced-Binary-Tree.java
DFS


96.Unique Binary Search Trees
LeetCode-96-Unique-Binary-Search-Trees.java
DP


95.Unique Binary Search Trees II
LeetCode-95-Unique-Binary-Search-Trees-II.java
DFS


100.Same Tree
LeetCode-100-Same-Tree.java



★102.Binary Tree Level Order Traversal I
LeetCode-102-Binary-Tree-Level-Order-Traversal-I.java
BFS, DFS


107.Binary Tree Level Order Traversal II
LeetCode-107-Binary-Tree-Level-Order-Traversal-II.java
BFS, DFS


103.Binary Tree Zigzag Level Order Traversal
LeetCode-103-Binary-Tree-Zigzag-Level-Order-Traversal.java
BFS


314.Binary Tree Vertical Order Traversal
LeetCode-314-Binary-Tree-Vertical-Order-Traversal.java
not yet


104.Maximum Depth of Binary Tree
LeetCode-104-Maximum-Depth-of-Binary-Tree.java
DFS


111.Minimum Depth of Binary Tree
LeetCode-111-Minimum-Depth-of-Binary-Tree.java
DFS, BFS


105.Construct Binary Tree from Preorder and Inorder Traversal
LeetCode-105-Construct-Binary-Tree-from-Preorder-and-Inorder-Traversal.java
DFS


106.Construct Binary Tree from Inorder and Postorder Traversal
LeetCode-106-Construct-Binary-Tree-from-Inorder-and-Postorder-Traversal.java
DFS


108.Convert Sorted Array to Binary Search Tree
LeetCode-108-Convert-Sorted-Array-to-Binary-Search-Tree.java
DFS


109.Convert Sorted List to Binary Search Tree
LeetCode-109-Convert-Sorted-List-to-Binary-Search-Tree.java
DFS


114.Flatten Binary Tree to Linked List
LeetCode-114-Flatten-Binary-Tree-to-Linked-List.java
DFS


116.Populating Next Right Pointers in Each Node
LeetCode-116-Populating-Next-Right-Pointers-in-Each-Node.java
BFS, DFS


117.Populating Next Right Pointers in Each Node II
LeetCode-117-Populating-Next-Right-Pointers-in-Each-Node-II.java
BFS, DFS


★112.Path Sum
LeetCode-112-Path-Sum.java
DFS, BFS


113.Path Sum II
LeetCode-113-Path-Sum-II.java
DFS


118.Pascal's Triangle
LeetCode-118-Pascals-Triangle.java
DP


119.Pascal's Triangle II
LeetCode-119-Pascals-Triangle-II.java
DP


129.Sum Root to Leaf Numbers
LeetCode-129-Sum-Root-to-Leaf-Numbers.java
DFS


124.Binary Tree Maximum Path Sum
LeetCode-124-Binary-Tree-Maximum-Path-Sum.java
not yet, hard


257.Binary Tree Paths
LeetCode-257-Binary-Tree-Paths.java
not yet


★★★120.Triangle
LeetCode-120-Triangle.java
DP, Array


144.Binary Tree Preorder Traversal
LeetCode-144-Binary-Tree-Preorder-Traversal.java
DFS, BFS


94.Binary Tree Inorder Traversal
LeetCode-94-Binary-Tree-Inorder-Traversal.java
DFS, BFS


145.Binary Tree Postorder Traversal
LeetCode-145-Binary-Tree-Postorder-Traversal.java
DFS, BFS


199.Binary Tree Right Side View
LeetCode-199-Binary-Tree-Right-Side-View.java
BFS, DFS


235.Lowest Common Ancestor of a Binary Search Tree
LeetCode-235-Lowest-Common-Ancestor-of-a-Binary-Search-Tree.java
Tree, Recursive


236.Lowest Common Ancestor of a Binary Tree
LeetCode-236-Lowest-Common-Ancestor-of-a-Binary-Tree.java
Tree, Recursive


Search in a Big Sorted Array
LintCode-Search-in-a-Big-Sorted-Array.java
Binary Search


133.Clone Graph
LeetCode-133-Clone-Graph.java
BFS, DFS


270.Closest Binary Search Tree Value
LeetCode-270-Closest-Binary-Search-Tree-Value.java
BFS, DFS


156. Binary Tree Upside Down
LeetCode-156-Binary-Tree-Upside-Down.java
BFS, DFS


173. Binary Search Tree Iterator
LeetCode-173-Binary-Search-Tree-Iterator.java
BFS


222. Count Complete Tree Nodes
LeetCode-222-Count-Complete-Tree-Nodes.java
BFS, DFS


226. Invert Binary Tree
LeetCode-226-Invert-Binary-Tree.java
BFS, DFS


230. Kth Smallest Element in a BST
LeetCode-230-Kth-Smallest-Element-in-a-BST.java
BFS, DFS


250. Count Univalue Subtrees
LeetCode-250-Count-Univalue-Subtrees.java
DFS


255. Verify Preorder Sequence in Binary Search Tree
LeetCode-255-Verify-Preorder-Sequence-in-Binary-Search-Tree.java
DFS


285. Inorder Successor in BST
LeetCode-285-Inorder-Successor-in-BST.java
BFS


298. Binary Tree Longest Consecutive Sequence
LeetCode-298-Binary-Tree-Longest-Consecutive-Sequence.java
DFS


333. Largest BST Subtree
LeetCode-333-Largest-BST-Subtree.java
DFS


337. House Robber III
LeetCode-337-House-Robber-III.java
DFS


366. Find Leaves of Binary Tree
LeetCode-366-Find-Leaves-of-Binary-Tree.java.java
DFS


404. Sum of Left Leaves
LeetCode-404-Sum-of-Left-Leaves.java
DFS


437. Path Sum III
LeetCode-437-Path-Sum-III.java
DFS


450. Delete Node in a BST
LeetCode-450-Delete-Node-in-a-BST.java
DFS


501. Find Mode in Binary Search Tree
LeetCode-501-Find-Mode-in-Binary-Search-Tree.java
DFS


508. Most Frequent Subtree Sum
LeetCode-508-Most-Frequent-Subtree-Sum.java
DFS, similar to 501


510. Inorder Successor in BST II
LeetCode-510-Inorder-Successor-in-BST-II.java
DFS, similar to 285


513. Find Bottom Left Tree Value
LeetCode-513-Find-Bottom-Left-Tree-Value.java
DFS, BFS


515. Find Largest Value in Each Tree Row
LeetCode-515-Find-Largest-Value-in-Each-Tree-Row.java
DFS, BFS


530. Minimum Absolute Difference in BST
LeetCode-530-Minimum-Absolute-Difference-in-BST.java
DFS, BFS


538. Convert BST to Greater Tree
LeetCode-538-Convert-BST-to-Greater-Tree.java
inorder traversal


543. Diameter of Binary Tree
LeetCode-543-Diameter-of-Binary-Tree.java
inorder traversal



97.Interleaving String
17.Letter Combinations of a Phone Number
List
Change Order(Reverse, Swap) & Remove
The use of dummy node, use pointer Next to store next node(Merge Two Sort Lists, Merge K Sorted Lists, Reverse List, Reverse Nodes in K-groups)
Find/Remove Kth biggest node from end: use two pointers, one run k step, and then two pointers run parallelly until one reach null.
Find the middle of a list: fast-slow pointers, one run one step, another run two steps. Be carefully of initialization, slow = header, fast = header.next.(Find intersection point, Detect circle)
面试大总结之链表
轻松搞定面试中的链表题目
算法大全（1）单链表



Question
Solution
Tags




3 Longest Substring Without Repeating Characters
LeetCode-3-Longest-Substring-Without-Repeating-Characters.java
Two Pointers


7.Reverse Integer
LeetCode-7-Reverse-Integer.java
Integer


8.String to Integer (atoi)
LeetCode-8-String-to-Integer.java
String


24.Swap Nodes in Pairs
LeetCode-24-Swap-Nodes-in-Pairs.java
LinkedList


25.Reverse Nodes in k-Group
LeetCode-25-Reverse-Nodes-in-k-Group.java
LinkedList


80.Remove Duplicates from Sorted Array II
LeetCode-80-Remove-Duplicates-from-Sorted-Array-II.java
Two Pointers


19.Remove Nth Node From End of List
LeetCode-19-Remove-Nth-Node-From-End-of-List.java
Two Pointers


203.Remove Linked List Elements
LeetCode-203-Remove-Linked-List-Elements.java
LinkedList


83.Remove Duplicates from Sorted List
LeetCode-83-Remove-Duplicates-from-Sorted-List.java
LinkedList


82.Remove Duplicates from Sorted List II
LeetCode-82-Remove-Duplicates-from-Sorted-List-II.java
LinkedList


206.Reverse Linked List
LeetCode-206-Reverse-Linked-List.java
LinskedList


92.Reverse Linked List II
LeetCode-92-Reverse-Linked-List-II.java
LinskedList


151.Reverse Words in a String
LeetCode-151-Reverse-Words-in-a-String.java
String


186.Reverse Words in a String II
LeetCode-186-Reverse-Words-in-a-String-II.java
String


★21.Merge Two Sorted Lists
LeetCode-21-Merge-Two-Sorted-Lists.java
Recursive, Iterative


★23.Merge k Sorted Lists
LeetCode-23-Merge-k-Sorted-Lists.java
Merge Sort, Priority Queue


88.Merge Sorted Array
LeetCode-88-Merge-Sorted-Array.java
Two Pointers


141.Linked List Cycle
LeetCode-141-Linked-List-Cycle.java
Two Pointers


142.Linked List Cycle II
LeetCode-142-Linked-List-Cycle-II.java
Two Pointers



Math
Prime
Operations between strings, between big numbers(larger than RAM: First external sorting (split into several parts), then K-way merge.)



Question
Solution
Tags




2.Add Two Numbers
LeetCode-2-Add-Two-Numbers.java
LinkedList


43.Multiply Strings
LeetCode-43-Multiply-Strings.java
String


67.Add Binary
LeetCode-67-Add-Binary.java
String


29.Divide Two Integers
LeetCode-29-Divide-Two-Integers.java
Binary Search


50.Pow(x, n)
LeetCode-50-Pow-n.java
Binary Search


69.Sqrt(x)
LeetCode-69-Sqrt-x.java
Binary Search


89 Gray Code
LeetCode-89-Gray-Code.java
Math


204.Count Primes
LeetCode-204-Count-Primes.java
Math


60.Permutation Sequence
LeetCode-60-Permutation-Sequence.java
Math


168.Excel Sheet Column Title
LeetCode-168-Excel-Sheet-Column-Title.java
Math


171.Excel Sheet Column Number
LeetCode-171-Excel-Sheet-Column-Number.java
Math



Design



Question
Solution
Tags




★146.LRU Cache
LeetCode-146-LRU-Cache.java
HashMap, List


155.Min Stack
LeetCode-155-Min-Stack.java
Two Stacks



225.Implement Stack using Queues
232.Implement Queue using Stacks
173.Binary Search Tree Iterator
284.Peeking Iterator
281.Zigzag Iterator
209.Minimum Size Subarray Sum
301.Remove Invalid Parentheses
238.Product of Array Except Self
157.Read N Characters Given Read4
158.Read N Characters Given Read4 II - Call multiple times
136.Single Number
137.Single Number II
260.Single Number III
268.Missing Number
287.Find the Duplicate Number
162.Find Peak Element
30.Substring with Concatenation of All Words
/*
LeetCode:
LintCode:
JiuZhang:
ProgramCreek:
Analysis:
*/
地里2016年度所有Facebook面试题总结
Category
经典面试题总结 —— Binary Search 及其变种
BFS/DFS总结
转一些我blog上一些常见的二叉树面试问题和总结 (更新)
Here is a 10-line template that can solve most 'substring' problems
Backtracking Porblems总结(Subsets-Permutations-Combination-Sum-Palindrome-Partitioning)
###Reference
LeetCode   |   LintCode    |   JiuZhang
Top 10 Algorithms for Coding Interview
Top 10 algorithms in Interview Questions(G4G)
Simple Java
",2
flutter/flutter,Dart,"


Flutter is Google's mobile app SDK for crafting high-quality native interfaces
on iOS and Android in record time. Flutter works with existing code, is used by
developers and organizations around the world, and is free and open source.
Documentation

Install Flutter
Flutter documentation
Development wiki
Contributing to Flutter

For announcements about new releases and breaking changes, follow the
flutter-announce@googlegroups.com
mailing list.
About Flutter
We think Flutter will help you create beautiful, fast apps, with a productive,
extensible and open development model.
Beautiful apps
We want to enable designers to deliver their full creative vision without being
forced to water it down due to limitations of the underlying framework.
Flutter's layered architecture gives you control over every pixel on the
screen, and its powerful compositing capabilities let you overlay and animate
graphics, video, text and controls without limitation. Flutter includes a full
set of widgets that deliver pixel-perfect experiences on both
iOS and Android.

Fast apps
Flutter is fast. It's powered by the same hardware-accelerated Skia 2D
graphics library that underpins Chrome and Android. We architected Flutter to
support glitch-free, jank-free graphics at the native speed of your device.
Flutter code is powered by the world-class Dart platform, which enables
compilation to native 32-bit and 64-bit ARM code for iOS and Android.
Productive development
Flutter offers stateful hot reload, allowing you to make changes to your code
and see the results instantly without restarting your app or losing its state.

Extensible and open model
Flutter works with any development tool, but includes editor plug-ins for both
Visual Studio Code and IntelliJ / Android Studio. Flutter provides
thousands of packages to speed your development, regardless
of your target platform. And accessing platform features is easy. Here is a
snippet from our interop example:
Future<void> getBatteryLevel() async {
  var batteryLevel = 'unknown';
  try {
    int result = await methodChannel.invokeMethod('getBatteryLevel');
    batteryLevel = 'Battery level: $result%';
  } on PlatformException {
    batteryLevel = 'Failed to get battery level.';
  }
  setState(() {
    _batteryLevel = batteryLevel;
  });
}
Flutter is a fully open source project, and we welcome contributions.
Information on how to get started can be found at our
contributor guide.
",64091
ConceptJunkie/rpn,Python,"rpn
rpn is a command-line Reverse-Polish Notation calculator.
rpn supports arithmetic with arbitrary precision, powers and roots, logarithms, algebraic functions (including polynomials arithmetic and solving), trigonometric functions, complex numbers, computer science related functions (bitwise math, base conversion), number theory functions, astronomical functions, prime number calculations and lookup, can operate with single operands or lists of operands and supports a wide variety of flexible unit conversions comparable to the GNU units program.
Updates
Update - May 13, 2019
Version 8.1.1 is released.   This version includes several new operators, a number of fixes, and a bunch of additions to the help.  Most operators now have help text and examples.
Update - April 26, 2019
Version 8 is released.   This version includes a small number of new features, but the primary focus was a significant refactor of the unit conversion code.  The 'convert' operator is more powerful and easier to use.
Update - March 8, 2019
rpnChilada 7.2.3 fixes the unit conversion bug.  Please upgrade.
Update - March 7, 2019
This is embarrassing.  I just discovered a long-standing bug with the unit conversion code where for some reason it thinks there are 59021.97 seconds in a day.  I've narrowed the bug down to between the 7.0.0 and 7.1.0 releases.  This is weird because
every other unit conversion I checked, including other conversions with days and seconds work correctly.  The makeUnits code has been in place for about 3 or 4 years and always seemed to be rock-solid.  I'll try to push a 7.2.2 in the next few days, or
possibly even 7.3.0 depending on what else gets included.
Update - February 26, 2019
OK, stick a fork in 7.2.0.  It's done.  I'd intended to get back to releasing often, but 10 months is not ""often"".
Aside from the usual ton of bug fixes and minor improvements, this version offers several operators having to do with the physics of black holes.  See ""rpn help physics"" for details.
The 7.2.0 release will show up on PyPI in the next day or two as soon as I have some time to test the wheel.
Update - February 22, 2019
Not much has happened with rpn lately, but I do have some good plans.  I haven't released 7.2 mostly due to laziness, but I've also got some solid ideas for improvements with the unit conversion functionality.  For one thing, I want to fold the constants into the units database, and I have an idea for adding some implicit unit conversion.
I've also wanted to migrate from pyephem, which is no longer being developed, to Skyfield, which is recommended by the people who used to make pyephem.  Skyfield is also a pure Python library, which makes my life easier.  However, Skyfield is a more low-level library, so there's not a one-to-one correspondence for most of the pyephem functionality I've been using.
Update - February 21, 2018
rpn is available on pypi.org.  Since there always was a project called ""rpn"", I had to come up with a new name, so I'm happy to introduce ""rpnChilada"".
Windows users will want to use Christophe Gohlke's Windows installers for gmpy2 and pyephem at https://www.lfd.uci.edu/~gohlke/pythonlibs/.
Update - August 15, 2016
I am very excited that people have started noticing rpn!
Please continue with comments, suggestions and bug reports. rpn has lots of little bugs and possibly some big ones, too, and although I have unit tests, most of the time, I find bugs from using it.
I especially want to thank the folks at The Nineteenth Byte on Stack Exchange for their nice comments. I also love solving puzzles with rpn, so if there's something you'd like to see it be able to do, drop me a line at rickg@his.com.
I'll try to focus on improving the help in the near future.
Update - July 19, 2016
I don't know if anyone has ever looked at this project... not even my Mom. But anyhow, I wanted to leave an update anyway. The ""imminent"" release of version 7 is anything but. I have been too lazy to tackle trying to make the wheel work correctly. I've been making small additions here and there, including bug fixes whenever I find problems.
Currently, my short list is topped with switching to SQLite for caching function results to disk. I was experimenting with the sigma function and found that once the cache of values got past a million, loading it had become unreasonably slow. I think it would also be a very good idea to convert the prime number data files to SQLite tables as well.
There are a few Python 2 compatibility problems that remain, and those should be easy to fix, but I've been too busy with a combiniation of real life and general laziness. The conversion to using generators mentioned in the last update is complete and has greatly improved performance for a lot of operators.
RPN continues to proceed slowly and it works just fine right now, so it can be used just fine despite being a ""pre-release"".
Update - November 16, 2015
The scope of changes for version 7 keeps growing. The transition to lazy list evaluation (using generators) is going to be a very big change and currently a lot of operators are broken.
It is recommended that anyone using RPN from Git stick with the 7.0.alpha1 tag for the time being.
Update - October 15, 2015
I've decided the upcoming release will be version 7 since so much has been added, a lot has been reorganized and I've gotten serious about unit tests.
An official release of version 7 probably won't be for a while, because I really want to be able to release it on PyPI. There's a lot of work I want to do before cutting another release, and it's going to take some time, but the current git master contains all the latest features and is working fine.
rpn should still work on Android, but there are problems with the ephem library. I think it has to do with building the AstroLib code, and haven't had a chance to try to diagnose the problem.
Update - August 5, 2015
I am working on creating a wheel for rpn, and I'm hoping I can also make it Python 2 compatible before cutting another release. The biggest roadblock is just getting some round tuits instead of adding in new operators, which is much more fun.
Another cool update: rpn can now be run on Android with the Termux app
(http://termux.com/)! Right now, it fails a unit test having to do with date formatting, which I haven't gotten around to investigating, but otherwise it works great. Where else can you factor a 50-digit number on your Android device?

The current release is 8.1.1.
See ""rpn help settings"" for more information.
Running RPN using the source:
rpn is written in Python 3, and requires several libraries for the hard math stuff (gmpy2 is optional, but recommended for improved performance).
Please see requirements.txt for a list of required Python packages.
Windows users will want to use Christophe Gohlke's Windows installers for gmpy2 and pyephem at https://www.lfd.uci.edu/~gohlke/pythonlibs/.
Using rpnChilada:
rpnChilada is very easy to use.  It's just like any RPN calculator: Operands go first, then the operators. All examples assume rpn is an alias for python /<path-to-rpn>/rpn.py.  In interactive mode, you leave off the rpn.
I always create an alias for ""python rpn.py"" called ""rpn"".  If you are using the package installed with pip, there are commands in the scripts directories called ""rpn"" and ""rpnChilada"" to launch rpnChilada.
Unit tests can be run with the testRPN command (when installed from the wheel) or by running testRPN.py from the rpn/ directory.
For instance:
rpn 2 2 +

will calculate 2 + 2.
rpn supports more than 900 operators. (rpn _dump_operators will list them all.)
The entire operator list is also included at the bottom of this document.
rpn has pretty extensive built-in help, although the help files are not complete. However, all operators have at least a brief description, and most are obvious enough to use easily.
Start with rpn help for an overview. To dive right in, see rpn help examples. In interactive mode, typing help will launch help mode. Then, topics will print out a list of help topics and exit will return to rpn.
The data files are stored in the same location as rpn.py in a subdirectory called rpndata/.
If you really want to generate prime numbers, see my ""primes"" project: https://github.com/ConceptJunkie/primes I've calculated the first 15 billion prime numbers and will someday update the rpn lookup tables.
The project https://github.com/ConceptJunkie/rpnChiladaData provides the compiled prime number data files.  If you installed rpnChilada with pip, then this data will be automatically installed.
rpn also provides a simple interface for accessing The On-Line Encyclopedia of Integer Sequences (http://oeis.org), see rpn help special and rpn help oeis.
rpnChilada used to provide a Windows installer, but I haven't been able to do that since version 6.4.0.  I hope to bring that back some day.
Feedback, Comments, Bug Reports:
Any feedback is welcome at rickg@his.com.  This was originally an exercise to learn Python, but slowly blossomed into something really useful and fun, so I wanted to share it. rpn also exposes just a few of the features of the amazing mpmath library (by Fredrik Johansson, http://mpmath.org/) which is where almost all the hard math stuff is actually done.
Rick Gutleber
rickg@his.com
p.s. rpn is licensed under the GNU GPL version 3.0. See (see (http://www.gnu.org/licenses/gpl.html) for more information).
Release Notes
8.0.0
The unit conversion code has been heavily refactored and works much better now.
Added the 'base_units' and 'dimensions' operators, mostly for testing purposes.
Added '_dump_conversions' and '_dump_cache', also for testing purposes.
rpnChilada is now smart enough to recognize when an OEIS request has failed,
and to ignore the cached result stored as a result.  If it detects that the
cached value is empty, it will perform the request again and recache the
result.
Help now supports units and constant operators after way too long.  Filling in
the help info for the units and constant operators, along with all the existing
help info that's missing, will take a while, and is continuing.
rpnChilada has officially dropped Python 2 support.  I rarely tested it anyway.
Added 'wind_chill' and 'heat_index' operators.
The unit tests now confirm that aliases do not collide with other reserved
words.  The alias creation for generated types has also been cleaned up.
The astronomy functionality has been refactored to support migrating to the
skyfield library from pyephem.
Removed the 'break_on' operator because it no longer works.  It will be
re-implemented in the future.
Added 'to_ethiopian', 'to_ethiopian_name' and 'from_ethiopian' operators for
converting to and from the Ethiopian calendar.
7.2.5
I fat-fingered an addition to the requirements.txt file.  :-/
7.2.4
Just a bunch of fixes.  makeUnits has been improved a bit, and I've validated that all conversions exist, and are consistent.
7.2.3
I messed up the upload for 7.2.2.  No code changes, just fixed packaging.
7.2.2
A big change that doesn't affect functionality is that the prime number data now resides in a separate package called rpnChiladaData.  This data rarely changes so there's no reason to download it.
A major bug was uncovered after almost a year.  rpnChilada thought there were 51920.97 seconds in a day because of a typo.  This has been fixed, and I figured out how to detect other similar problems if they exist.  This change will be implemented in the next few days.
7.2.1
Unit conversion is now a lot smarter because the automatically-generated area and volume units are generated more intelligently.  This means expressions using the ""square"" and ""cubic"" units will convert automatically and you won't end up with something like ""foot^2/square_mile"".
...and yes, a few bug fixes.
7.2.0
Added 'random_element' operator.
The gmpy2 digits( ) function is a much faster way to convert numbers to bases 2 through 62.
Added support for using yafu for factoring.
Added 'aliquot_limit' operator.
Added support for user configuration:  'set_config', 'get_config', 'delete_config' and 'dump_config'.
Added the 'mothers_day', 'fathers_day' and 'advent' operators.
Added the 'molar_gas_constant', 'aliquot_limit' and 'distance' operators (the old 'distance' operator is now called 'geo_distance').
Added unit tests for converting units, and made a few fixes accordingly.
Verbose mode for factoring gets turned on with -D.
Oops, there were two operators named 'distance'.  'distance' now refers to the physics operator and the geography operator is now named 'geo_distance'.
The 'acceleration' operator has been implemented.
The derived Planck units are now calculated, instead of hard-coded.
Block Hole operators:  'black_hole_entropy', 'black_hole_lifetime', 'black_hole_luminosity', 'black_hole_mass', 'black_hole_radius' (was 'schwarzchild_radius'), 'black_hole_surface_area', 'black_hole_surface_gravity', 'black_hole_temperature'
...and the usual bug fixes.
7.0.0
Version 7 represents over 2-1/2 years of work and I neglected to keep track of the changes.
There are probably around 200 more operators since 6.4 was released, and I replaced the factoring code with a much faster verison.
rpn now supports user-defined variables and functions, including persistent variables and functions.
6.4.0 - ""Factoring Fun""
Revamped factorization to be much, much faster, using the Brent-Pollard
algorithm instead of just brute-force dividing.   More to come...
Added the 'magnetic_constant', 'electric_constant', 'rydberg_constant',
'newtons_constant' and 'fine_structure' operators.
Added 'eulerphi' operator.
Added caching for factorizations.  I often factor the same numbers over and over (like when I'm playing with the Fibonaccis) so it made sense to cache the results for non-trivial factoring.
Added the 'sigma, 'aliquot', 'polypower', 'mobius' and 'mertens' operators.  The old 'mertens' operator was renamed to 'mertens_constant'.  The 'aliquot' operator is another use-case for caching factorizations... try it with 276.
rpn can can now factor the first 450 or so in a reasonably short time.
Added the 'frobenius', 'slice', 'sublist', 'left' and 'right' operators.
Added 'crt' operator.
...and the usual slew of bug fixes.
6.3.0
Added the 'geomean' operator.
Added the 'argument' and 'conjugate' operators.
Fixed 'trianglearea'.  It's been wrong for a long time.  Sorry.
Added the 'fibonorial' operator.
Added the 'eulerbrick' operator.
Added the 'unlist' operator.
Added the 'makepyth3' and 'makepyth4' operators.
Added the 'equal', 'greater', 'less', 'not_equal', 'not_greater', and
'not_less' operators.
Added the 'reduce' operator.
Added the 'lcm' operator.
The 'pascal' operator was renamed to 'pascaltri' to avoid a collision with the 'pascal' unit.
Fixed several minor bugs.
6.2.0
Experimental support for mpath plotting functionality using the new
operators, 'plot', 'plot2', 'plotc'.  These operators are not supported
in the Windows installer.
'quit' is now an alias for 'exit' in interactive mode and help mode.
Improvements in function definition.  'y' and 'z' are now operators, allowing for defining functions on 2 or 3 variables.
Operators 'eval2' and 'eval3' allow for evaluation of 2 and 3 variable
operators.
rpn now throws an error if a user-defined function is invalidly specified, instead of going into an infinite loop.
'filter' allows filtering a list based on a user-defined function.
If the units in a measurement cancel out, then the measurement is converted back to a numerical value.
Added 'rand_' and 'randint_' operators.
Added the 'debruijn' operator.
Fixed several minor bugs.
6.1.0
New operators:  'maxdouble', 'maxfloat', 'mindouble', 'minfloat'
Base conversion for output is no longer limited to 1000 digits.  There's no reason to do that.
'rpn 0 cf' now throws an error rather than dividing by 0.
6.0.1
Added code to prevent scientific notation from messing up base conversions for the integral part of the number (up to 1000 digits).
6.0.0
Introduced interactive mode, including variable declaration and referencing previous results by number.  (see 'rpn help interactive_mode')
Added caching for OEIS operators.  However, it turns out some OEIS text is non-ASCII, so I'll have to deal with that.
Operator help now includes examples by default.
The 'time' operator type conflicted with the 'time' unit type, so I changed the operator type to 'date'... because they were all about dates!
Fixed a long-standing precision problem with unit conversion.
Lots more bug fixes.
5.28.5
More bug fixes and code cleanup.  Added the 'unfloat' and 'undouble' operators.
5.28.4
Added the 'diffs2' operator.
More bug fixes thanks to the test script!
5.28.3
The operators 'doublebal', doublebal_', 'triplebal', and 'triplebal_' now work
correctly.  The data files have been significantly expanded as well.
More prime number updates will come in the next few weeks.  My target is to
expand every table up to the first 10 billion primes.
5.28.2
Several bug fixes relating to 'estimate' and unit conversion.   Some unit types
were folded together because they had the same basic units (e.g., frequency and
radioactivity were both time ^ -1, which confused the conversion logic).
5.28.1
Added separate installers for the plain-vanilla rpn (with only the ""small
primes"" data file, i.e., the first million primes), and the installer with all
of the prime data files.
The 'primes' operator has been fixed so it works correctly for small values.
I'm currently testing the prime functions, which I haven't touched in a long
time, so more fixes will definitely be coming.  The balanced prime functions
are currently broken and will be fixed shortly, including updated data files.
5.28.0
Added 'x', 'eval', 'nsum', 'nprod', 'limit', 'limitn', 'infinity', and
'negative_infinity', and 'value' operators.
5.27.2
Help for unit types now prints out all aliases for the unit operators.
5.27.1
Added an error message if the 'name' operand is out of range, and added support
for negative numbers.
5.27.0
Added the 'name' operator.
5.26.0
Added dynamic_visocity and frequency unit types and a few bug fixes.
Added units for the days and years of the other 8 planets in the Solar System.
Added several constant units for quaint or archaic number terms like 'score'
and 'gross'.
Added mass units for common particle masses.
Updated some natural values (electron mass, etc.).
Fixed some problems with generating and interpreting compound units.
Added the 'prevost' operator.
5.25.0
Added Julian date operators, ISO date operators, calendar operators and the
'ash_wednesday' operator.  Added support for the density unit type and several
small bug fixes.
5.24.0
A few more bug fixes, plus new calendar-related operators:  easter.
election_day, labor_day, memorial_day, nthday, presidents_day, thanksgiving
5.23.1
The help improvements actually work now.  So much for testing.
There are now some examples of absolute time handling.
5.23.0
Help will now search topics for partial matches if a complete match isn't found.
5.22.0
Added a bunch of new constants for powers of 10.
5.21.2
Added -l to format help output for different line lengths.  However, it still
doesn't format the blockquoted help text.
5.21.1
Added percent operator, weekday now throws a proper error is the operand isn't
a time value.
5.21.0
The long-awaited absolute time feature:  rpn can now handle absolute time
values.  For input, just use ISO 8601 format, or a reasonable subset thereof.
There is also the 'maketime' operator, which takes a list similar to the old
'tounixtime' operator.
5.20.7
Added help for unit types.   Help for individual units will come eventually,
but they are pretty self-explanatory.
5.20.6
The prime? operator wasn't working correctly for small values.
5.20.5
rpn now throws an error when attempting to get the 0th or less prime number.
5.20.4
rpn now correctly reports the argument in question on any error.
5.20.3
Made a fix to improve rpn's reporting of the argument in question when there is
an error.  It's probably not 100% correct yet.
5.20.2
Fixed the list operator parsing so polyprod and polysum work correctly.
5.20.1
Several calls to polyval( ) had hard-coded fractions in them instead of calls
to fdiv( ), resulting in rounding errors.
5.20.0
rpn finally comes with an installer for Windows, in 32-bit and 64-bit flavors.
5.19.3
The test script has been rewritten in Python.  It's still very basic and only
does a sanity test to show every operator works without crashing.  It doesn't
test for correct answers yet.
5.19.2
rpn now outputs an empty list correctly.  The 'append' operator (to append
lists) has been fixed.
5.19.1
Fixed several problems with 'tounixtime' and 'fromunixtime'.
The first version of a test script is available as a batch file.
5.19.0
Added 'randint' operator.
5.18.7
compoundUnits was still being referred to without the ""g."" global specifier.
5.18.6
rpn now prints out an error message if you try to get help for an unknown
topic.
5.18.5
Fixed a bug concerning adding dissimilar units.
5.18.4
rpn now correctly parses ""-0"" as a value again.
5.18.3
Added 'split' as an alias for 'unpack' because I couldn't remember what it was
called.
Made some minor fixes made based on running pyflakes, pylint pep8, and the
test script.
5.18.2
Made a bunch of bug fixes that showed up as a result of reorganizing the code.
5.18.1
It's clear I haven't done any unit conversions in a while because there were
still issues with declarations of variables.  Now, I've started eliminating the
use of ""global"" in favor of a global module.
Operators supported by rpn:
( ) aa_battery abs abundance abundance_ratio acceleration accuracy acos acosh
acot acoth acsc acsch add add_digits add_polynomials advent agm aliquot
aliquot_limit alpha_particle_mass alternate_signs alternate_signs_2
alternating_factorial alternating_sum alternating_sum_2 and and_all
angular_separation angular_size antiprism_area antiprism_volume
antitransit_time apery_constant append april argument arrangements ascension
asec asech ash_wednesday asin asinh astronomical_dawn astronomical_dusk atan
atanh atomic_number atomic_symbol atomic_weight august autumnal_equinox
avogadro_number balanced_prime balanced_prime_ barnesg base base_units
bell_polynomial beta binomial bitwise_and bitwise_nand bitwise_nor bitwise_not
bitwise_or bitwise_xor black_hole_entropy black_hole_lifetime
black_hole_luminosity black_hole_mass black_hole_radius black_hole_surface_area
black_hole_surface_gravity black_hole_temperature bohr_radius
boltzmann_constant build_numbers build_step_numbers calendar calkin_wilf
catalan_constant ceiling centered_cube centered_decagonal centered_dodecahedral
centered_heptagonal centered_hexagonal centered_icosahedral centered_nonagonal
centered_octagonal centered_octahedral centered_pentagonal centered_polygonal
centered_square centered_tetrahedral centered_triangular cf
champernowne_constant char christmas classical_electron_radius collate collatz
columbus_day combinations combine_digits comma comma_mode compare_lists
compositions cone_area cone_volume conjugate convert copeland_erdos_constant
cos cosh cot coth coulomb_constant count count_bits count_different_digits
count_digits count_divisors cousin_prime cousin_prime_ crt csc csch cube
cube_root cumulative_diffs cumulative_ratios cyclic_permutations cyclotomic
dawn day_time debruijn decagonal decagonal_centered_square decagonal_heptagonal
decagonal_hexagonal decagonal_nonagonal decagonal_octagonal
decagonal_pentagonal decagonal_triangular december decimal_grouping decrement
default delete_config denomination_combinations density_of_water describe
deuteron_mass dhms difference diffs digamma digital_root digits dimensions
discriminant distance distance_from_earth divide divisors dms dodecahedral
dodecahedron_area dodecahedron_volume double double_balanced double_balanced_
double_factorial dst_end dst_start dump_config duplicate_digits
duplicate_number duplicate_operator duplicate_term dusk e earth_density
earth_gravity earth_mass earth_radius earth_volume easter echo eclipse_totality
eddington_number egypt election_day electric_constant electron_charge
electron_mass element element_block element_boiling_point element_density
element_description element_group element_melting_point element_name
element_occurrence element_period element_state energy_equivalence enumerate
enumerate_dice enumerate_dice_ epiphany equals_one_of erdos_persistence ernal
operators: escape_velocity estimate eta euler_brick euler_mascheroni_constant
euler_phi eval eval0 eval2 eval3 eval_list eval_list2 eval_list3
eval_polynomial exp exp10 exponential_range expphi factor factorial false
faraday_constant fathers_day february fibonacci fibonorial filter
filter_by_index filter_lists find find_palindrome find_polynomial
find_sum_of_cubes find_sum_of_squares fine_structure_constant flatten float
floor for_each for_each_list fraction friday frobenius from_bahai
from_ethiopian from_hebrew from_indian_civil from_islamic from_julian
from_mayan from_persian from_unix_time function gallon_of_ethanol
gallon_of_gasoline gamma gcd gcd2 generalized_pentagonal
generate_polydivisibles geometric_mean geometric_range geometric_recurrence
geo_distance get_base_k_digits get_combinations get_config get_day get_digits
get_hour get_left_digits get_left_truncations get_minute get_month
get_nonzero_base_k_digits get_nonzero_digits get_partitions get_permutations
get_repeat_combinations get_repeat_permutations get_right_digits
get_right_truncations get_second get_timezone get_variable get_year
glaisher_constant good_friday group_elements harmonic harmonic_mean
has_any_digits has_digits has_only_digits heat_index helion_mass help
heptagonal heptagonal_hexagonal heptagonal_pentagonal heptagonal_square
heptagonal_triangular heptanacci hexagonal hexagonal_pentagonal
hexagonal_square hexanacci hex_mode hms horizon_distance hurwitz_zeta hyper4_2
hyperfactorial hyperfine_transition_frequency_of_cesium hypotenuse i
icosahedral icosahedron_area icosahedron_volume identify identify_mode if
imaginary increment independence_day infinity input_radix integer
integer_grouping interleave intersection interval_range invert_units
isolated_prime iso_date iso_day is_abundant is_achilles is_automorphic
is_base_k_pandigital is_base_k_smith_number is_bouncy is_carmichael
is_composite is_decreasing is_deficient is_digital_permutation is_divisible
is_equal is_even is_friendly is_generalized_dudeney is_greater is_harshad
is_increasing is_integer is_kaprekar is_kth_power is_k_hyperperfect
is_k_morphic is_k_narcissistic is_k_semiprime is_k_sphenic is_less
is_narcissistic is_not_equal is_not_greater is_not_less is_not_zero is_odd
is_order_k_smith_number is_palindrome is_palindrome_list is_pandigital is_pddi
is_pdi is_perfect is_polydivisible is_powerful is_power_of_k is_prime is_pronic
is_rough is_ruth_aaron is_semiprime is_smith_number is_smooth is_sphenic
is_square is_squarefree is_step_number is_strong_pseudoprime is_sum_product
is_trimorphic is_unusual is_zero itoi january july june jupiter jupiter_mass
jupiter_radius jupiter_revolution jupiter_volume khinchin_constant
kinetic_energy k_fibonacci k_persistence k_sphere_area k_sphere_radius
k_sphere_volume labor_day lah lambda lambertw larger latlong_to_nac lat_long
lcm lcm2 leading_zero leading_zero_mode left leyland li limit limitn
linear_recurrence linear_recurrence_with_modulo list_from_file location
location_info log log10 log2 logxy log_gamma long longlong lucas
magnetic_constant magnetic_flux_quantum make_cf make_datetime make_iso_time
make_julian_time make_pyth_3 make_pyth_4 mantissa march mars mars_mass
mars_radius mars_revolution mars_volume martin_luther_king_day mass_equivalence
maximum max_char max_double max_float max_index max_long max_longlong
max_quadlong max_short max_uchar max_ulong max_ulonglong max_uquadlong
max_ushort may mean memorial_day mercury mercury_mass mercury_radius
mercury_revolution mercury_volume merten merten_constant mills_constant minimum
min_char min_double min_float min_index min_long min_longlong min_quadlong
min_short min_uchar min_ulong min_ulonglong min_uquadlong min_ushort mobius
modulo molar_gas_constant molar_mass monday moon moonrise moonset
moon_antitransit moon_gravity moon_mass moon_phase moon_radius moon_revolution
moon_transit moon_volume mothers_day multifactorial multinomial multiply
multiply_digits multiply_digit_powers multiply_nonzero_digits
multiply_nonzero_digit_powers multiply_polynomials muon_mass name nand nand_all
narayana nautical_dawn nautical_dusk nearest_int negative negative_infinity
neptune neptune_mass neptune_radius neptune_revolution neptune_volume
neutron_mass newton_constant new_years_day next_antitransit
next_first_quarter_moon next_full_moon next_last_quarter_moon next_new_moon
next_prime next_primes next_quadruplet_prime next_quintuplet_prime next_rising
next_setting next_transit night_time nonagonal nonagonal_heptagonal
nonagonal_hexagonal nonagonal_octagonal nonagonal_pentagonal nonagonal_square
nonagonal_triangular nonzero nor nor_all not november now nprod nsum nth_apery
nth_bell nth_bernoulli nth_carol nth_catalan nth_centered_decagonal
nth_centered_heptagonal nth_centered_hexagonal nth_centered_nonagonal
nth_centered_octagonal nth_centered_pentagonal nth_centered_polygonal
nth_centered_square nth_centered_triangular nth_decagonal nth_delannoy
nth_heptagonal nth_hexagonal nth_jacobsthal nth_kynea nth_leonardo
nth_linear_recurrence nth_linear_recurrence_with_modulo nth_menage
nth_mersenne_exponent nth_mersenne_prime nth_motzkin nth_nonagonal
nth_octagonal nth_padovan nth_pell nth_pentagonal nth_perfect_number
nth_polygonal nth_prime nth_quadruplet_prime nth_quintuplet_prime nth_schroeder
nth_schroeder_hipparchus nth_square nth_stern nth_sylvester nth_thue_morse
nth_triangular nth_weekday nth_weekday_of_year nuclear_magneton occurrences
occurrence_cumulative occurrence_ratios octagonal octagonal_heptagonal
octagonal_hexagonal octagonal_pentagonal octagonal_square octagonal_triangular
octahedral octahedron_area octahedron_volume octal_mode octanacci october oeis
oeis_comment oeis_ex oeis_name oeis_offset omega_constant or orbital_mass
orbital_period orbital_radius orbital_velocity ordinal_name or_all output_radix
pack parity partitions pascal_triangle pentagonal pentagonal_square
pentagonal_triangular pentanacci pentatope pentecost permutations permute_dice
permute_digits permute_lists persistence phi pi planck_acceleration
planck_angular_frequency planck_area planck_charge planck_constant
planck_current planck_density planck_electrical_inductance planck_energy
planck_energy_density planck_force planck_impedance planck_intensity
planck_length planck_magnetic_inductance planck_mass planck_momentum
planck_power planck_pressure planck_temperature planck_time planck_viscosity
planck_voltage planck_volume planck_volumetric_flow_rate plastic_constant plot
plot2 plotc pluto pluto_mass pluto_radius pluto_revolution pluto_volume polyexp
polygamma polygonal polygon_area polylog polynomial_power polynomial_product
polynomial_sum polyprime polytope power powerset power_tower power_tower2
powmod precision presidents_day previous previous_antitransit
previous_first_quarter_moon previous_full_moon previous_last_quarter_moon
previous_new_moon previous_prime previous_primes previous_rising
previous_setting previous_transit prevost_constant prime primes prime_pi
prime_range primorial prism_area prism_volume product proton_mass pyramid
quadlong quadruplet_prime quadruplet_prime_ quintuplet_prime quintuplet_prime_
radiation_constant radical random random_ random_element random_integer
random_integer_ range ratios real reciprocal recurrence reduce
reduced_planck_constant repeat replace_digits repunit result reversal_addition
reverse reverse_digits rhombic_dodecahedral riesel right robbins_constant
roll_dice roll_dice_ roll_simple_dice root rotate_digits_left
rotate_digits_right round round_by_digits round_by_value rydberg_constant
safe_prime saturday saturn saturn_mass saturn_radius saturn_revolution
saturn_volume sec sech september set_config set_variable sextuplet_prime
sextuplet_prime_ sexy_prime sexy_prime_ sexy_quadruplet sexy_quadruplet_
sexy_triplet sexy_triplet_ shift_left shift_right short show_erdos_persistence
show_k_persistence show_persistence shuffle sidereal_year sigma sigma_k sign
silver_ratio sin sinh sized_range sky_location slice smaller solar_constant
solar_noon solve solve_cubic solve_quadratic solve_quartic sophie_prime sort
sort_descending speed_of_light sphere_area sphere_radius sphere_volume square
square_digit_chain square_root square_triangular star stddev
stefan_boltzmann_constant stella_octangula subfactorial sublist subtract sum
summer_solstice sums_of_k_nonzero_powers sums_of_k_powers sum_digits sun sunday
sunrise sunset sun_antitransit sun_luminosity sun_mass sun_radius sun_volume
superfactorial superprime surface_gravity tan tanh tau_mass tetrahedral
tetrahedron_area tetrahedron_volume tetranacci tetrate thabit thanksgiving
thue_morse_constant thursday timer timer_mode time_dilation today tomorrow
topic torus_area torus_volume to_bahai to_bahai_name to_ethiopian
to_ethiopian_name to_hebrew to_hebrew_name to_indian_civil to_indian_civil_name
to_islamic to_islamic_name to_iso to_iso_name to_julian to_julian_day
to_lilian_day to_mayan to_ordinal_date to_persian to_persian_name to_unix_time
transit_time triangle_area triangular tribonacci trigamma triplet_prime
triplet_prime_ triple_balanced triple_balanced_ triple_point_of_water
triton_mass tropical_year true truncated_octahedral truncated_tetrahedral
tuesday twin_prime twin_prime_ uchar uinteger ulong ulonglong undouble unfilter
unfilter_by_index unfloat union unique unit_roots unlist unpack uquadlong
uranus uranus_mass uranus_radius uranus_revolution uranus_volume ushort uuid
uuid_random vacuum_impedance value velocity venus venus_mass venus_radius
venus_revolution venus_volume vernal_equinox veterans_day von_klitzing_constant
wednesday weekday weekday_name wind_chill winter_solstice x xnor xor y ydhms
year_calendar yesterday z zero zeta zeta_zero [ ] _dump_aliases _dump_cache
_dump_constants _dump_conversions _dump_operators _dump_units _stats
",13
CJWorkbench/cjworkbench,Python,"




License







Gitter







Data workspaces, made simple.
Welcome to Workbench! (CJ for computational journalism)

Our public server, now in beta.

Workbench is a platform that combines data tools and training for journalists. It is a workbook-style data processing system designed around modular tools for data processing -- table in, table out -- with no code required. Features include:

Modules to scrape, clean, analyze and visualize data
An integrated data journalism training program
Connect to Google Drive, Twitter, and API endpoints.
Every action is recorded, so all workflows are repeatable and transparent
All data is live and versioned, and you can monitor for changes.
Write custom modules in Python and add them to the module library

Try it
To see what Workbench does, try our public server, now in beta. Or run your own server.



User Documentation

Imagining the data journalism workflow of the future
What workbench can do for data
A different approach to transparent data journalism
Data journalism made easier, faster, and more collaborative
Our knowledge base has detailed instructions for each module

Contributing

How to set up a development environment.
Creating your own modules
Join us on gitter!

Workbench is licensed under the AGPL 3.0 license. You are free to use the code or parts of it in your own applications, even your own own closed source applications, but if you modify Workbench code or merge it into your own software you must open-source the modifications.
Contact us
Always happy to hear from you:

Twitter
email

We also accept Pull Requests :)
Credits
Workbench is a project of Columbia Journalism School, made possible through the generous support of Krishna Bharat and the Knight Foundation.
",128
uber-go/dosa,Go,"DOSA - Declarative Object Storage Abstraction



Abstract
DOSA is a storage framework that
provides a declarative object storage abstraction for applications in Golang
and (soon) Java. DOSA is designed to relieve common headaches developers face
while building stateful, database-dependent services.
If you'd like to start by writing a small DOSA-enabled program, check out
the getting started guide.
Overview
DOSA is a storage library that supports:

methods to store and retrieve go structs
struct annotations to describe queries against data
tools to create and/or migrate database schemas
implementations that serialize requests to remote stateless servers

Annotations
This project is released under the MIT License.
",107
fanhan-inside/fanhan-inside.github.io,HTML,"fANhAN iNSiDE
Personal Blog
https://fanhan-inside.github.io
",2
wanchain/go-wanchain,Go,"WANChain Go



Branch
Tests




master  



develop




Building the source
Building gwan requires both a Go (version 1.7 or later) and a C compiler.
If build release version,Docker is required
You can install them using your favourite package manager.
Once the dependencies are installed, run
make gwan

or, to build the full suite of utilities:
make all

or, to build the release version
make release	

Running gwan
Full node on the main wanchain network
By far the most common scenario is people wanting to simply interact with the wanchain network:
create accounts; transfer funds; deploy and interact with contracts. For this particular use-case
the user doesn't care about years-old historical data, so we can fast-sync quickly to the current
state of the network. To do so:
$ gwan console

This command will:

Start gwan in fast sync mode (default, can be changed with the --syncmode flag), causing it to
download more data in exchange for avoiding processing the entire history of the wanchain network,
which is very CPU intensive.
This too is optional and if you leave it out you can always attach to an already running gwan instance
with gwan attach.

Full node on the wanchain test network
Transitioning towards developers, if you'd like to play around with creating wanchain contracts, you
almost certainly would like to do that without any real money involved until you get the hang of the
entire system. In other words, instead of attaching to the main network, you want to join the test
network with your node, which is fully equivalent to the main network, but with play-Ether only.
$ gwan --testnet console

The console subcommand have the exact same meaning as above and they are equally useful on the
testnet too. Please see above for their explanations if you've skipped to here.
Specifying the --testnet flag however will reconfigure your gwan instance a bit:

Instead of using the default data directory (~/.wanchain on Linux for example), gwan will nest
itself one level deeper into a testnet subfolder (~/.wanchain/testnet on Linux). Note, on OSX
and Linux this also means that attaching to a running testnet node requires the use of a custom
endpoint since gwan attach will try to attach to a production node endpoint by default. E.g.
gwan attach <datadir>/testnet/gwan.ipc. Windows users are not affected by this.
Instead of connecting the main wanchain network, the client will connect to the test network,
which uses different P2P bootnodes, different network IDs and genesis states.

Note: Although there are some internal protective measures to prevent transactions from crossing
over between the main network and test network, you should make sure to always use separate accounts
for play-money and real-money. Unless you manually move accounts, gwan will by default correctly
separate the two networks and will not make any accounts available between them.
Programatically interfacing gwan nodes
As a developer, sooner rather than later you'll want to start interacting with gwan and the wanchain
network via your own programs and not manually through the console. To aid this, gwan has built in
support for a JSON-RPC based APIs 。These can be
exposed via HTTP, WebSockets and IPC (unix sockets on unix based platforms, and named pipes on Windows).
The IPC interface is enabled by default and exposes all the APIs supported by gwan, whereas the HTTP
and WS interfaces need to manually be enabled and only expose a subset of APIs due to security reasons.
These can be turned on/off and configured as you'd expect.
HTTP based JSON-RPC API options:

--rpc Enable the HTTP-RPC server
--rpcaddr HTTP-RPC server listening interface (default: ""localhost"")
--rpcport HTTP-RPC server listening port (default: 8545)
--rpcapi API's offered over the HTTP-RPC interface (default: ""eth,net,web3"")
--rpccorsdomain Comma separated list of domains from which to accept cross origin requests (browser enforced)
--ws Enable the WS-RPC server
--wsaddr WS-RPC server listening interface (default: ""localhost"")
--wsport WS-RPC server listening port (default: 8546)
--wsapi API's offered over the WS-RPC interface (default: ""eth,net,web3"")
--wsorigins Origins from which to accept websockets requests
--ipcdisable Disable the IPC-RPC server
--ipcapi API's offered over the IPC-RPC interface (default: ""admin,debug,eth,miner,net,personal,shh,txpool,web3"")
--ipcpath Filename for IPC socket/pipe within the datadir (explicit paths escape it)

You'll need to use your own programming environments' capabilities (libraries, tools, etc) to connect
via HTTP, WS or IPC to a gwan node configured with the above flags and you'll need to speak JSON-RPC
on all transports. You can reuse the same connection for multiple requests!
Note: Please understand the security implications of opening up an HTTP/WS based transport before
doing so! Hackers on the internet are actively trying to subvert wanchain nodes with exposed APIs!
Further, all browser tabs can access locally running webservers, so malicious webpages could try to
subvert locally available APIs!
Creating the rendezvous point
With all nodes that you want to run initialized to the desired genesis state, you'll need to start a
bootstrap node that others can use to find each other in your network and/or over the internet. The
clean way is to configure and run a dedicated bootnode:
$ bootnode --genkey=boot.key
$ bootnode --nodekey=boot.key

With the bootnode online, it will display an [enode URL]
that other nodes can use to connect to it and exchange peer information. Make sure to replace the
displayed IP address information (most probably [::]) with your externally accessible IP to get the
actual enode URL.
Note: You could also use a full fledged gwan node as a bootnode, but it's the less recommended way.
Starting up your member nodes
With the bootnode operational and externally reachable (you can try telnet <ip> <port> to ensure
it's indeed reachable), start every subsequent gwan node pointed to the bootnode for peer discovery
via the --bootnodes flag. It will probably also be desirable to keep the data directory of your
private network separated, so do also specify a custom --datadir flag.
$ gwan --datadir=path/to/custom/data/folder --bootnodes=<bootnode-enode-url-from-above>

Note: Since your network will be completely cut off from the main and test networks, you'll also
need to configure a miner to process transactions and create new blocks for you.
Docker quick start
One of the quickest ways to get wanchain up and running on your machine is by using Docker:
docker run -d --name wanchain-node -v /home/ubuntu/wanchain:/root \
           -p 8545:8545 -p 17717:17717 \
           wanchain/client-go --rpc

This will start gwan in fast-sync mode with a DB memory allowance of 1GB just as the above command does.  It will also create a persistent volume in your home directory for saving your blockchain as well as map the default ports.
Do not forget --rpcaddr 0.0.0.0, if you want to access RPC from other containers and/or hosts. By default, gwan binds to the local interface and RPC endpoints is not accessible from the outside.
",255
joel16/ElevenMPV,C,"ElevenMPV - Eleven Music Player VITA
A homebrew music player for Playstation VITA that aims to support many different audio formats compared to the offical PS VITA music application.
Currently supported formats: (16 bit signed samples)

FLAC
IT
MOD
MP3
OGG
OPUS
S3M
WAV (A-law and u-law, Microsoft ADPCM, IMA ADPCM)
XM

Features:

Browse ux0:/ to play the above audio formats.
Pause/Play audio.
Shuffle/Repeat audio.
Next/Previous track in current working directory.
Display ID3v1 and ID3v2 metadata for MP3 files.
Basic touch support.

Controls:
In file manager:

Enter button (cross/circle): enter folder/play supported audio file.
Cancel button (cross/circle): go up parent folder.
DPAD Up/Down: Navigate files.
DPAD Left/Right: Top/Bottom of list.

In audio player: (Note: you can use touch controls here or the following buttons below)

Enter button (cross/circle): Play/Pause.
Cancel button (cross/circle): Return to file manager.
L trigger: Previous audio file in current directory.
R trigger: Next audio file in current directory.
Triangle: Shuffle audio files in current directory.
Square: Repeat audio files in current directory.
Start: Turn off display and keep playing audio in background.

Credits:

MPG123 contributors.
dr_libs by mackron.
stb_vorbis by nothings and contributors.
libxmp-lite contributors.
Preetisketch for startup.png (banner).
Eleven Music Player contributors for design elements.

",10
caian-org/workstation,Python,"workstation

Environment bootstrap script.
usage
curl -fsSL https://git.io/fjcFh | bash
License
To the extent possible under law, Caian Rais Ertl has waived all
copyright and related or neighboring rights to this work.

Acknowledgements
workstation is highly inspired (by functionality or implementation) on these
projects/scripts:

setup by Gabriel Tiossi
my_workstation by Diego Paludo

Icons made by Eucalyp from Flaticon is
licensed by CC 3.0 BY.
",4
joel16/ElevenMPV,C,"ElevenMPV - Eleven Music Player VITA
A homebrew music player for Playstation VITA that aims to support many different audio formats compared to the offical PS VITA music application.
Currently supported formats: (16 bit signed samples)

FLAC
IT
MOD
MP3
OGG
OPUS
S3M
WAV (A-law and u-law, Microsoft ADPCM, IMA ADPCM)
XM

Features:

Browse ux0:/ to play the above audio formats.
Pause/Play audio.
Shuffle/Repeat audio.
Next/Previous track in current working directory.
Display ID3v1 and ID3v2 metadata for MP3 files.
Basic touch support.

Controls:
In file manager:

Enter button (cross/circle): enter folder/play supported audio file.
Cancel button (cross/circle): go up parent folder.
DPAD Up/Down: Navigate files.
DPAD Left/Right: Top/Bottom of list.

In audio player: (Note: you can use touch controls here or the following buttons below)

Enter button (cross/circle): Play/Pause.
Cancel button (cross/circle): Return to file manager.
L trigger: Previous audio file in current directory.
R trigger: Next audio file in current directory.
Triangle: Shuffle audio files in current directory.
Square: Repeat audio files in current directory.
Start: Turn off display and keep playing audio in background.

Credits:

MPG123 contributors.
dr_libs by mackron.
stb_vorbis by nothings and contributors.
libxmp-lite contributors.
Preetisketch for startup.png (banner).
Eleven Music Player contributors for design elements.

",10
caian-org/workstation,Python,"workstation

Environment bootstrap script.
usage
curl -fsSL https://git.io/fjcFh | bash
License
To the extent possible under law, Caian Rais Ertl has waived all
copyright and related or neighboring rights to this work.

Acknowledgements
workstation is highly inspired (by functionality or implementation) on these
projects/scripts:

setup by Gabriel Tiossi
my_workstation by Diego Paludo

Icons made by Eucalyp from Flaticon is
licensed by CC 3.0 BY.
",4
krlaframboise/SmartThings,Groovy,"
Welcome to Kevin LaFramboise's SmartThings Repository
Below you will find a list of the SmartApps and Device Handlers that I've created.  I've included a short description of what they can do and included links to their topics in the SmartThings forum.
If you like the SmartApps and Device Handlers I've created and you would like to make a donation, please go to https://www.paypal.me/krlaframboise

I'm posting this code on GitHub so that anyone can use it, but this is a private repository so pull requests will be ignored.  If you find a problem or want something added, please post a message on the corresponding topic in the SmartThings forum.


SmartApps

Home Presence Manager
Simple Device Viewer
Simple Event Logger




Device Type Handlers

Aeotec Doorbell
Aeotec LED Bulb 6 Multi-White
Aeotec NanoMote One
Aeotec NanoMote Quad
Aeotec Siren
Aeotec TriSensor
BeSense 360 Ceiling Sensor
BeSense Door/Window Sensor
BeSense PIR Wall Sensor
Dome Door Sensor
Dome Door/Window Sensor Pro
Dome Leak Sensor
Dome Motion Sensor
Dome Mouser
Dome On Off Plug
Dome Siren
Dome Water Shut-Off
Ecolink Motion Sensor
Ecolink Motorized Double Rocker Switch
Ecolink Motorized Double Toggle Switch
Ecolink Motorized Rocker Switch
Ecolink Motorized Toggle Switch
Ecolink Siren
Everspring Motion Detector
Everspring Temperature/Humidity Detector
Fibaro Door/Window Sensor 2
Fibaro Motion Sensor ZW5
Fibaro Swipe
GoControl/Linear Door/Window Sensor
GoControl/Linear Multifunction Contact Sensor
GoControl/Linear Motion Sensor
GoControl/Linear/Vision Multifunction Siren
Hank Scene Controller
Hank Four-Key Scene Controller
Hank RGBW LED Bulb
LeakSmart Water Valve
Monoprice Shock Sensor
Monoprice Z-Wave Plus Door/Window Sensor
Neo Coolcam Door Sensor
Neo Coolcam Light Switch 2CH
Neo Coolcam Motion Sensor
Neo Coolcam Power Plug
Qubino Shutter Module
Remotec ZXT-310 IR Extender
Strips Comfort by Sensative
Strips Drip by Sensative
Vision Recessed Door Sensor
Vision Shock Sensor
Wireless Smoke Detector Sensor
Zipato Multisound Siren
Zooz 4-in-1 Sensor
Zooz Double Plug
Zooz Motion Sensor ZSE18
Zooz Outdoor Motion Sensor
Zooz Smart Plug
Zooz Power Strip VER 2.0
Zooz Power Strip
Zooz Power Switch
Zooz Smart Chime
Zooz Smart Plug
Zooz Smart Plug VER 2.0
Zooz Water Sensor




SmartApps
Home Presence Manager

Uses Motion Sensors, Contact Sensors and Virtual Presence Sensors to keep track of the room you're in and turn on and off lights as you move throughout the house.
View Documentation in SmartThings Forum
View Home Presence Manager - SmartApp Code


Simple Device Viewer

Allows you to easily see a list of information about your devices like battery percentages, temperatures, how long since last event, switch state, etc.
Receive Push and/or SMS notifications based on temperature, battery level, and/or time since last event.
It can automatically poll the devices at a specified interval.
Turn Off All Lights and/or Switches with a push of a button.
View Documentation in SmartThings Forum
View Simple Device Viewer - SmartApp Code


Simple Event Logger

Simple Event Logger is a fully customizable SmartApp that allows you to accurately log all device activity to a Google Sheets Spreadsheet.
Each event is stored on a separate row so that you have their exact time and details.
Google Sheets has an easy to use filter feature which allows you to do things like view all events for specific device(s), in a specified date/range and/or specific types of events like temperature.
Since all of your data will be stored in one spreadsheet, advanced users can easily generate pivot tables and graphs for any information they need. It also eliminates the need to update the code in multiple spreadsheets every time a new version is released.
View Installation Instructions and Documentation
View SmartThings Forum Topic
View Simple Event Logger - SmartApp Code
View Simple Event Logger - Google Sheets Web App Code


Device Type Handlers
Aeon Labs Multifunction Siren

DTH for the Aeon Labs Siren that provides features like beeping, auto off, delayed alarm, beep scheduling for things like beeping during entry and exit.
View Documentation in SmartThings Forum
View Aeon Labs Multifunction Siren - Device Handler Code


Aeotec Doorbell

DTH for the Aeon Labs Aeotec Doorbell that allows you to use the device as a Switch, Alarm, Tone Generator, Music Player, and Audio Notification. Implements custom commands to allow you to play tracks by track number and change the volume on the fly.
View Documentation in SmartThings Forum
View Aeotec Doorbell - Device Handler Code


Aeotec LED Bulb 6 Multi-White

This is a device handler for the Aeotec LED Bulb 6 Multi-White (ZWA001-A)
View Documentation in SmartThings Forum
View Aeotec LED Bulb 6 Multi-White - Device Handler Code


Aeotec NanoMote

This is a device handler for:
Aeotec NanoMote One (ZWA003-A)
Aeotec NanoMote Quad (ZWA004-A)
Hank Scene Controller (HKZW-SCN01)
Hank Four-Key Scene Controller (HKZW-SCN04)
View Documentation in SmartThings Forum
View Aeotec NanoMote - Device Handler Code


Aeotec TriSensor

This is a device handler for the Aeotec TriSensor (ZWA005-A)
View Documentation in SmartThings Forum
View Aeotec TriSensor - Device Handler Code


BeSense Motion Sensor ZWave Plus

This is a device handler for:
BeSense 360 Ceiling Sensor (IX32)
BeSense PIR Wall Sensor (IX30)
Supports all functionality that the devices offer
View BeSense Motion Sensor ZWave Plus - Device Handler Code


BeSense Door/Window Sensor ZWave Plus

This is the official device handler for the BeSense Door/Window Sensor (IM20)
Supports all functionality that the device offers
View BeSense Door/Window Sensor ZWave Plus - Device Handler Code


Dome Door Sensor

This is the official device handler for the Dome Door Sensor (DMWD1)
Supports all functionality that the device offers
View Documentation in SmartThings Forum
View Dome Door Sensor - Device Handler Code


Dome Door/Window Sensor Pro

This is the official device handler for the Dome Door/Window Sensor Pro (DMDP1)
Supports all functionality that the device offers
View Documentation in SmartThings Forum
View Dome Door/Window Sensor Pro - Device Handler Code


Dome Leak Sensor

This is the official device handler for the Dome Leak Sensor (DMWS1)
Supports all functionality that the device offers
View Documentation in SmartThings Forum
View Dome Leak Sensor - Device Handler Code


Dome Motion Sensor

This is the official device handler for the Dome Motion Sensor (DMMS1)
View Documentation in SmartThings Forum
View Dome Motion Sensor - Device Handler Code


Dome Mouser

This is the official device handler for the Dome Mouser (DMMZ1)
View Documentation in SmartThings Forum
View Dome Mouser - Device Handler Code


Dome On Off Plug

This is the official device handler for the Dome On Off Plug (DMOF1)
View Documentation in SmartThings Forum
View Dome On Off Plug - Device Handler Code


Dome Siren

This is the official device handler for the Dome Siren (DMS01)
Supports all functionality that the device offers
View Documentation in SmartThings Forum
View Dome Siren - Device Handler Code


Dome Water Shut-Off

This is the official device handler for the Dome Water Shut-Off (DMWV1)
You can open the valve with either Valve.open or Switch.on
You can close the valve with either Valve.close or Switch.off
View Documentation in SmartThings Forum
View Dome Water Shut-Off - Device Handler Code


Ecolink Motion Sensor

This is a device handler for the Ecolink Motion Sensor (PIRZWAVE2.5-ECO)
Reports Motion, Battery, Tamper, and allows you to change the wakeup interval.
View Ecolink Motion Sensor - Device Handler Code


Ecolink Wireless Switch

This is a device handler for:
Ecolink Motorized Double Rocker Switch (DDLS2-ZWAVE5)
Ecolink Motorized Double Toggle Switch (DTLS2-ZWAVE5)
Ecolink Motorized Rocker Switch (DLS-ZWAVE5)
Ecolink Motorized Toggle Switch (TLS-ZWAVE5)

Reports Switch, Battery, and allows you to change the wakeup interval.
View Ecolink Wireless Switch - Device Handler Code


Ecolink Siren

This is a device handler for the Ecolink Siren (SC-ZWAVE5-ECO)
The device is fully functional if selected as switch:
 Switch On: Siren On
Switch Off: Turns Everything Off
Set Level 10%: Chime/Beep
Set Level 20%: Entry/Continuous Tone
Set Level 30%: Exit/Repeating Beep

View Ecolink Siren - Device Handler Code


Everspring Motion Detector

This is a device handler for the Everspring Motion Detector (HSP02)
Reports Motion, Battery, Tamper.
Allows you to set a ambient light percentage and it raises the Contact Open event when the light level drops below that percentage and motion is detected.
View Everspring Motion Detector - Device Handler Code


Everspring Temperature/Humidity Detector

This is a device handler for the Everspring Temperature/Humidity Detector (ST814-2)
Reports Relative Humidity, Temperature, and Battery.
Allows you to choose whether Humidity or Temperature is displayed as in the Things list.
You can change the reporting interval for Temperature/Humidity or disable it.
You can specify a Temperature threshold and a Humidity threshold for reporting
Supports Temperature and Humidity offsets
View Everspring Temperature/Humidity Detector - Device Handler Code


Fibaro Door/Window Sensor 2

This is a device handler for the Fibaro Door/Window Sensor 2(FGDW-002)
Reports Contact, Temperature, and Tamper
View Fibaro Door/Window Sensor 2 - Device Handler Code


Fibaro Motion Sensor ZW5

This is a device handler for the Fibaro Motion Sensor ZW5 (FGMS-001)
Reports Motion, Light, Temperature, and Acceleration/Tamper.
It can also report either Earthquake magnitude or Three-Axis x,y,z.
Simplifies all the configuration settings.
View Fibaro Motion Sensor ZW5 - Device Handler Code


Fibaro Swipe

This is a device handler for the Fibaro Swipe (FGGC-001)
The device supports 16 buttons that are mapped to the gestures and sequences
There's a label setting for each button that gets displayed on the device details screen
Allows you to choose the 2-3 gestures to use for each of the 6 sequences
Creates the button held event when circular gestures start and button pushed event when they stop
Double gestures can be disabled.
View Documentation in SmartThings Forum
View Fibaro Swipe - Device Handler Code


GoControl/Linear Door/Window Sensor

DTH for the GoControl Linear Door/Window Sensor, Model: WADWAZ-1
Automatically sets polling attribute so it can be monitored by SmartApps like the Simple Device Viewer to ensure it's stil online.
Supports the Tamper Alert Capability.
View Documentation in SmartThings Forum
View GoControl/Linear Door/Window Sensor - Device Handler Code


GoControl/Linear Multifunction Contact Sensor

Advanced device handler for the GoControl/Linear Contact Sensor (WADWAZ-1) that allows you to use the internal and external sensors as different capabilities.
Supports the Contact Sensor, Water Sensor and Motion Sensor capabilities.
Choose which contact (internal/external/main) and which state (open/close) go with each of the motion and water states (wet/dry/active/inactive).
Choose which capability to use for the main tile.
Choose which capability to use for the secondary status on the main tile.
Choose default state to use for the capabilities that are not being used.
Has all the features that the basic version has like the ability to decide if the internal, external or a combination of both cause the Contact Capability to change.
View Documentation in SmartThings Forum
View GoControl/Linear Multifunction Contact Sensor - Device Handler Code


GoControl/Linear Motion Sensor

DTH for the GoControl/Linear Motion Sensor, Model: WAPIRZ-1
Automatically sets polling attribute so it can be monitored by SmartApps like the Simple Device Viewer to ensure it's stil online.
Supports the Tamper Alert Capability.
Provides offset so you can adjust the temperature
Provides threshold so you can prevent it from bouncing back and forth between the same 2 temperatures.
Allows you to set the frequency that it checks the battery
View Documentation in SmartThings Forum
View GoControl/Linear Motion Sensor - Device Handler Code


GoControl/Linear Multifunction Siren

DTH for the GoControl Siren, Linear Siren and possibly some other generic sirens. Models: ZM1601US / WA105DBZ-1
It allows you to make the alarm turn off automatically, switch between siren/strobe/both on the fly, have it automatically turn off after a specified amount of time and it also allows you to make the device beep.  The custom commands can be sent to the device using he speaktext and playtext commands of the Music Player capability.
View Documentation in SmartThings Forum
View GoControl/Linear Multifunction Siren - Device Handler Code


Hank RGBW LED Bulb

This is a DTH for the Hank RGBW LED Bulb (Model: HKZW-RGB01)
View Documentation in SmartThings Forum
View Hank RGBW LED Bulb - Device Handler Code


LeakSmart Water Valve

DTH for the LeakSmart Water Valve and it polls regularly so you can use a SmartApp like the Simple Device Viewer to monitor it and receive notifications if it stops reporting.
View Documentation in SmartThings Forum
View LeakSmart Water Valve - Device Handler Code


Monoprice Z-Wave Plus Door/Window Sensor

This is a device handler for the Monoprice Z-Wave Plus Door/Window Sensor (Model: P/N 15270)
It has the setting Enable External Sensor which enables the terminals so you can attach an external sensor.
The device wakes up every 6 hours by default, but there's a setting for Minimum Check-in Interval (Hours) which accepts the range 1 to 167.
There's also a setting for Battery Reporting Interval (Hours) which accepts the same range of values.
When the cover of the device is opened, it raises the ""tamper"" event with the value ""detected"".
The setting Automatically Clear Tamper allows you to choose whether it raises the tamper clear event when the device cover is closed or if you have to press the ""Refresh"" button to clear it.
View Documentation in SmartThings Forum
View Monoprice Z-Wave Plus Door/Window Sensor - Device Handler Code


Neo Coolcam Door Sensor

This is a device handler for the Neo Coolcam Door Sensor (NAS-DS02ZU / NAS-DS02ZE)
View Documentation in SmartThings Forum
View Neo Coolcam Door Sensor - Device Handler Code


Neo Coolcam Light Switch 2CH

This is a device handler for the Neo Coolcam Light Switch 2CH (NAS-SC02ZU-2 / NAS-SC02ZE-2)
View Documentation in SmartThings Forum
View Neo Coolcam Light Switch 2CH - Device Handler Code


Neo Coolcam Motion Sensor

This is a device handler for the Neo Coolcam Motion Sensor (NAS-PD01ZU-T / NAS-PD01ZE-T)
View Documentation in SmartThings Forum
View Neo Coolcam Motion Sensor - Device Handler Code


Neo Coolcam Power Plug

This is a device handler for the Neo Coolcam Power Plug (NAS-WR02ZU, NAS-WR02ZE)
View Documentation in SmartThings Forum
View Neo Coolcam Power Plug - Device Handler Code


Qubino Roller Shade Controller

This is a device handler for the Qubino DC Shutter Module (ZMNHOD3)
View Documentation in SmartThings Forum
View Qubino Roller Shade Controller - Device Handler Code


Remotec ZXT-310 IR Extender

This is a device handler for the Remotec Z-Wave-to-AV IR Extender (Model: ZXT-310)
The device handler provides 6 sets of 9 buttons which allows you to learn up to 54 IR Codes from other remote controls.
Each set of buttons can be configured to use the internal IR Port or any of the External Ports. The device has 5 external ports and comes with 3 - 6' external cables.
You can specify triggers for the 9 buttons. The options are Switch On, Switch Off, Switch On/Off, and Momentary Switch Push.
You can also push the buttons using any SmartApp that supports the Switch Level capability. Level 10% pushes button 1, 20% pushes button 2, etc.
The triggers and switch levels push the buttons for the active set of buttons, but to switch between the sets of buttons you need to tap the E1-E6 tiles or use a SmartApp like CoRE to execute the custom commands setActiveEP1 - setActiveEP6. Or use the optional SmartApp which will generate a separate virtual device for each set of buttons.
All you have to do to program a button is tap the ""Learn"" tile, tap the button you want to program, hold down the key on the remote control until the LED flashes twice, and then tap the ""Learn"" tile again.
View Documentation in SmartThings Forum
View Remotec ZXT-310 IR Extender - Device Handler Code
View optional SmartApp code and optional Child Device code that allow you to use this device as 6 devices.


Strips Multi-Sensor

This is a device handler for the Strips Comfort by Sensative and Strips Drip by Sensative
Allows you to change the primary and secondary tiles
Supports configuration parameters
Reports Light, Temperature, and Water for both devices
View Documentation in SmartThings Forum
View Strips Multi-Sensor - Device Handler Code


Vision Recessed Door Sensor

This is a device handler for the Vision Recessed Door Sensor (ZD2105US-5).
View Vision Recessed Door Sensor - Device Handler Code


Vision/Monoprice Shock Sensor

This is a device handler for the Vision Shock Sensor (ZS 5101).
It's also a device handler for the Monoprice Shock Sensor (P/N 15269)
Choose between Motion and Acceleration as the capability to use for the primary status shown in the main tile which is activated by vibration.
Primary status automatically resets back to inactive shortly after vibration stops
Choose between None, Motion, Contact, Tamper and Water for the secondary status which is activated by the external sensor and/or tamper switch.
Once the secondary status is activated, the Refresh button needs to be tapped in order to reset it
View Documentation in SmartThings Forum
View Vision Shock Sensor - Device Handler Code


Wireless Smoke Detector Sensor

This is a device handler for the Wireless Smoke Detector Sensor (ZWN-SD).
View Wireless Smoke Detector Sensor - Device Handler Code


Zipato Multisound Siren

This is a device handler for the Zipato Z-Wave Indoor Multi-Sound Siren (PH-PSE02.US).  It's been tested on the US version, but it should work with the EU version.
View Documentation in SmartThings Forum
View Zipato Multisound Siren - Device Handler Code


Zooz 4-in-1 Sensor

This is a device handler for the Zooz 4-in-1 Sensor (ZSE40).
After updating the settings, pressing the button on the bottom with a paperclip will automatically apply them.
If you want to force all the values to refresh, tap the refresh button and then press the button on the bottom with a paperclip.
The device wakes up every 6 hours by default, but there's a setting for Minimum Check-in Interval (Hours) which accepts the range 1 to 167.
There's also a setting for Battery Reporting Interval (Hours) which accepts the same range of values.
When the cover of the device is opened, it raises the ""tamper"" event with the value ""detected"".
The setting Automatically Clear Tamper allows you to choose whether it raises the tamper clear event when the device cover is closed or if you have to press the ""Refresh"" button to clear it.
View Documentation in SmartThings Forum
View Zooz 4-in-1 Sensor - Device Handler Code


Zooz Double Plug

This is a device handler for the Zooz Double Plug (ZEN25)
View Documentation in SmartThings Forum
View Zooz Double Plug - Device Handler Code
View Zooz Double Plug Outlet - Device Handler Code


Zooz Motion Sensor ZSE18

This is a device handler for the Zooz Motion Sensor (ZSE18)
Reports Motion, Acceleration, and Battery
View Documentation in SmartThings Forum
View Zooz Motion Sensor ZSE18 - Device Handler Code


Zooz Outdoor Motion Sensor

This is a device handler for the Zooz Outdoor Motion Sensor (ZSE29)
View Zooz Outdoor Motion Sensor - Device Handler Code


Zooz Power Strip VER 2.0

This is a device handler for the Zooz Power Strip VER 2.0 (ZEN20).
A device is created for the Power Strip and each Outlet
Power Strip reports combined power/energy
Outlets report power/energy
A Component Switch is created for each USB Port and they report ON when devices plugged into them are using power. The USB Ports can not be controlled and will not appear in your device list, but they will appear in Smart Apps
Keeps history of low and high power values
The Power Switch device can turn on/off all outlets at the same time or you can choose a delay to use between them.
Enable/Disable Manual Operation
Power recovery options
LED options
Auto on/off intervals for each Outlet
Power and Energy reporting intervals
Power reporting threshold
View Documentation in SmartThings Forum
View Zooz Power Strip VER 2.0 - Power Strip Device Handler Code
View Zooz Power Strip VER 2.0 - Outlet Device Handler Code
View Zooz Power Strip VER 2.0 - USB Port Device Handler Code


Zooz Power Strip

This is a device handler for the OLD Discontinued version of the Zooz Z-Wave Power Strip (ZEN20).
The 5 outlets can be controlled separately using the custom commands ch1On, ch1Off, ch2On, ch2Off, etc.  This requires the use of a SmartApp that supports custom commands, like CoRE.
Creates Digital events when turned on/off from SmartApp and Physical events when the buttons on the power strip are pushed.
The Main Switch Behavior setting for each outlet determines how it responds to the switch.on/switch.off commands.

On/Off: Switch.on command turns it on and Switch.off command turns it off.
On: Switch.on command turns the outlet on, but Switch.off doesn't turn it off.
Off: Switch.off turns the outlet off, but Switch.on doesn't turn it on.
None: The outlet ignores the Switch.on and Switch.off commands.


The Main Switch shows ""on"" when ANY of the outlets with the Main Switch Behavior set to ""on"" or ""on/off"" are on.
The Main Switch shows ""off"" when ALL of the outlets with the Main Switch Behavior set to ""off"" or ""on/off"" are off.
Main Switch Delay setting allows the Main Switch to turn the outlets on/off gradually instead of all at once.
View Documentation in SmartThings Forum
View Zooz Power Strip - Device Handler Code


Zooz Power Switch/Zooz Smart Plug

This is a device handler for the Zooz Z-Wave Power Switch (ZEN15) and Zooz Z-Wave Smart Plug (ZEN06).
Reports Power, Energy, Voltage, and Current
Tracks high and low values for Power, Voltage, and Current.
Reports Energy Duration and Cost.
Creates Digital events when turned on/off from SmartApp and Physical events when the button is used.
Optionally display Power, Energy, Voltage, and Current events in the Recently tab.
View Documentation in SmartThings Forum
View Zooz Power Switch/Zooz Smart Plug - Device Handler Code


Zooz Smart Chime

This is a device handler for the Zooz Smart Chime (ZSE33).
Has 10 sounds that can be used as chimes or sirens
Has 3 volume settings that can be set for chime and siren.
Optionally use flashing LED for chime and siren.
Use device as alarm to play the siren sound, switch to play the chime.
Use the customChime command to play a sound by number.
View Documentation in SmartThings Forum
View Zooz Smart Chime - Device Handler Code


Zooz Smart Plug VER 2.0

This is a device handler for the Zooz Smart Plug VER 2.0 (ZEN06)
View Zooz Smart Plug VER 2.0 - Device Handler Code


Zooz Water Sensor

This is a device handler for the Zooz Water Sensor (ZSE30).
Red LED and optional audible alarm when water is detected.
Specify the first alarm beep duration, reminder beep duration, interval between beeps, and the total length of time it should send reminders.
View Documentation in SmartThings Forum
View Zooz Water Sensor - Device Handler Code

",380
ppy/osu-web,PHP,"osu!web
   
The browser-facing portion of osu!.
Requirements

A PHP 7.1+ environment
MySQL 5.7
Elasticsearch

Getting Started
See the setup guide for a rundown on how to get a development environment up.
Contributing
We welcome all contributions, but keep in mind that we already have the full site designed (mock-ups). If you wish to work on a new section, please open a ticket and we will give you what you need from a design perspective to proceed. If you want to make changes to the design, we recommend you open an issue with your intentions before spending too much time, to ensure no effort is wasted.
Contributions can be made via pull requests to this repository. We hope to credit and reward larger contributions via a bounty system. If you want to contribute to localization, feel free to update our strings on Crowdin Translation System. If you're unsure of what you can help with, check out the list of open issues.
Note that while we already have certain standards in place, nothing is set in stone. If you have an issue with the way code is structured; with any libraries we are using; with any processes involved with contributing, please bring it up. I welcome all feedback so we can make contributing to this project as pain-free as possible.
Please see CONTRIBUTING.md for information about the code standards we expect from pull requests.
Seeking Help
If you need help with anything, you have two options:
Create an Issue
If you have something you want to discuss in detail, or have hit an issue which you believe others will also have in deployment or development of the system, opening an issue is the best way to get help. It creates a permanent resource for others wishing to contribute to conversation. Please make sure to search first in case someone else has already addressed the same issue!
Discord
Alternatively, you can join the development discord for assistance.
Licence
osu!web is licensed under AGPL version 3 or later. Please see the licence file for more information. tl;dr if you want to use any code, design or artwork from this project, attribute it and make your project open source under the same licence.
",451
lucascouts/bentoo,Shell,"Bentoo
Bentōō is an initiative to distribute an user-friendly version of Funtoo linux Stage4 to new users, with more update packages, focusing on agility, security privacy and games.
our Telegram group -> https://t.me/joinchat/DG0l5Rez0MRt4p2GHnNohQ
with local overlays
Local overlays should be managed via /etc/portage/repos.conf/.
create a /etc/portage/repos.conf/bentoo.conf file containing precisely:
[bentoo]
location = /usr/local/portage/bentoo
sync-type = git
sync-uri = https://github.com/lucascouts/bentoo.git
priority= 99

Afterwards, simply run ego sync, and Portage should seamlessly make all our ebuilds available.
with layman
Invoke the following:
# layman -o https://raw.github.com/lucascouts/bentoo/master/repositories.xml -f -a bentoo

Installation guide
To install just follow the Funtoo installation steps and replace the downloaded stage3 with the Bentoo Stage4.
https://www.funtoo.org/install

Bentoo Portage config
Here you can see the portage files configurations : https://github.com/lucascouts/bentoo-cfg
",19
php/php-src,C,"




The PHP Interpreter
PHP is a popular general-purpose scripting language that is especially suited to
web development. Fast, flexible and pragmatic, PHP powers everything from your
blog to the most popular websites in the world. PHP is distributed under the PHP
License v3.01.


Documentation
The PHP manual is available at php.net/docs.
Installation
Prebuilt packages and binaries
Prebuilt packages and binaries can be used to get up and running fast with PHP.
For Windows, the PHP binaries can be obtained from
windows.php.net. After extracting the archive the
*.exe files are ready to use.
For other systems, see the installation chapter.
Building PHP source code
For Windows, see Build your own PHP on Windows.
PHP uses autotools on Unix systems to configure the build:
./buildconf
./configure [options]

See ./configure -h for configuration options.
make [options]

See make -h for make options.
The -j option shall set the maximum number of jobs make can use for the
build:
make -j4

Shall run make with a maximum of 4 concurrent jobs: Generally the maximum
number of jobs should not exceed the number of cores available.
Testing PHP source code
PHP ships with an extensive test suite, the command make test is used after
successful compilation of the sources to run this test suite.
It is possible to run tests using multiple cores by setting -jN in
TEST_PHP_ARGS:
make TEST_PHP_ARGS=-j4 test

Shall run make test with a maximum of 4 concurrent jobs: Generally the maximum
number of jobs should not exceed the number of cores available.
The qa.php.net site provides more detailed info about
testing and quality assurance.
Installing PHP built from source
After a successful build (and test), PHP may be installed with:
make install

Depending on your permissions and prefix, make install may need super user
permissions.
PHP extensions
Extensions provide additional functionality on top of PHP. PHP consists of many
essential bundled extensions. Additional extensions can be found in the PHP
Extension Community Library - PECL.
Contributing
The PHP source code is located in the Git repository at
git.php.net. Contributions are most welcome by forking
the GitHub mirror repository and sending a
pull request.
Discussions are done on GitHub, but depending on the topic can also be relayed
to the official PHP developer mailing list internals@lists.php.net.
New features require an RFC and must be accepted by the developers. See
Request for comments - RFC and
Voting on PHP features for more information
on the process.
Bug fixes do not require an RFC but require a bug tracker ticket. Open a
ticket at bugs.php.net and reference the bug id using
#NNNNNN.
Fix #55371: get_magic_quotes_gpc() throws deprecation warning

After removing magic quotes, the get_magic_quotes_gpc function caused a
deprecated warning. get_magic_quotes_gpc can be used to detect the
magic_quotes behavior and therefore should not raise a warning at any time.
The patch removes this warning.

Pull requests are not merged directly on GitHub. All PRs will be pulled and
pushed through git.php.net. See
Git workflow for more details.
Guidelines for contributors
See further documents in the repository for more information on how to
contribute:

Contributing to PHP
PHP coding standards
Mailinglist rules
PHP release process

Credits
For the list of people who've put work into PHP, please see the
PHP credits page.
",23234
FyuriStudios/SFG,JavaScript,"Struggle for Gera
This is an original digital card game, currently in the very early pre-alpha development stages.
The SFG team consists of a group of friends who love to argue about what makes Hearthstone a bad game, or why Gwent sucks. When we realized that we could do something productive with our arguments like make our own game, we blew our own minds. So here we are. Here is the link to the official rules document. If the rules don't make any sense, sorry: We wrote them, so maybe it only makes sense to us.
The code
The frontend code is located here. The backend code is located here. Tests are located here.
The backend code is meant to be run using NodeJS on a server. It serves up a webpage (which is mostly just an embedded JS script, lol.) /static contains everything regarding the webpage.
",2
ppizarror/TEFAME,MATLAB,"




  TEFAME
Resuelve estructuras de varios grados de libertad utilizando métodos numéricos matriciales






Métodos de análisis

Método de rigidez, análisis estático
Método modal espectral, análisis estático

Tipo de elementos

Biela 2D
Biela 3D
Membrana (Shell 2D)
Viga 2D
Viga-columna 2D

Tipo de disipadores

Viscoso 2D
Triangular 2D
Friccional puro 2D

Patrones de carga

Constante
Dinámico

Combinaciones de cargas
Se da soporte a combinación elástica de cargas
Tipo de cargas
Estáticas

Carga biela temperatura
Carga membrana distribuída
Carga en nodo
Carga distribuida en viga-columna
Carga puntual en viga-columna
Carga distribuída en viga
Carga puntual en viga

Dinámicas

Carga de pulso
Registro sísmico
Carga sinusoidal

Documentacion
La documentación del proyecto se puede encontrar en https://ppizarror.com/TEFAME/docs/
Autor

Pablo Pizarro (pablo.pizarro@ing.uchile.cl)
Carlos López
Jaime Reveco

Licencia
Este proyecto está licenciado bajo la licencia MIT https://opensource.org/licenses/MIT
",2
ppizarror/TEFAME,MATLAB,"




  TEFAME
Resuelve estructuras de varios grados de libertad utilizando métodos numéricos matriciales






Métodos de análisis

Método de rigidez, análisis estático
Método modal espectral, análisis estático

Tipo de elementos

Biela 2D
Biela 3D
Membrana (Shell 2D)
Viga 2D
Viga-columna 2D

Tipo de disipadores

Viscoso 2D
Triangular 2D
Friccional puro 2D

Patrones de carga

Constante
Dinámico

Combinaciones de cargas
Se da soporte a combinación elástica de cargas
Tipo de cargas
Estáticas

Carga biela temperatura
Carga membrana distribuída
Carga en nodo
Carga distribuida en viga-columna
Carga puntual en viga-columna
Carga distribuída en viga
Carga puntual en viga

Dinámicas

Carga de pulso
Registro sísmico
Carga sinusoidal

Documentacion
La documentación del proyecto se puede encontrar en https://ppizarror.com/TEFAME/docs/
Autor

Pablo Pizarro (pablo.pizarro@ing.uchile.cl)
Carlos López
Jaime Reveco

Licencia
Este proyecto está licenciado bajo la licencia MIT https://opensource.org/licenses/MIT
",2
GoogleChromeLabs/text-app,JavaScript,"Text Chrome App
Just a text editor for Chrome OS and Chrome. Install via the Chrome Web Store: stable version or canary version.
Getting the code
You can download the whole source code as one archive, or get it from the repository using git:
git clone --recursive git://github.com/GoogleChromeLabs/text-app.git

Running the development version

Check Developer Mode in chrome://extensions
Click ""Load unpacked extension..."" in chrome://extensions and select the text-app directory.

Building the package
You do not have to build the app to install it in Chrome. Building will just extract all the required files and minify the JS code.
Building script requires Python3 and will use online Closure Compiler. Just run
python3 build.py

and the package will be written to text-app/build/ directory in zipped and unzipped formats (canary version). To build the stable version run the build script with the flag -s.
",389
callstack/react-native-testing-library,JavaScript,"

React Native Testing Library
Lightweight React Native testing utilities helping you write better tests with less effort.






Appreciation notice: This project is heavily inspired by react-testing-library. Go check it out and use it to test your web React apps.
The problem
You want to write maintainable tests for your React Native components without testing implementation details, but then you're told to use Enzyme, which you learn has no React Native adapter, meaning only shallow rendering is supported. And you want to render deep! But deep rendering may otherwise require jsdom (React Native isn't the web!), while doing deep rendering with react-test-renderer is so painful.
You would also like to use the newest React features, but you need to wait for your testing library's abstractions to catch up and it takes a while.
You finally want to approach testing using only best practices, while Enzyme may encourage assertions on implementation details.
This solution
The react-native-testing-library is a lightweight solution for testing your React Native components. It provides light utility functions on top of react-test-renderer letting you always be up to date with latest React features and write any component tests you like. But really not any, it prevents you from testing implementation details because we stand this is a very bad practice.
This library is a replacement for Enzyme. It is tested to work with Jest, but it should work with other test runners as well.
Example
import { render, fireEvent } from 'react-native-testing-library';
import { QuestionsBoard } from '../QuestionsBoard';
import { Question } from '../Question';

function setAnswer(question, answer) {
  fireEvent.changeText(question, answer);
}

test('should verify two questions', () => {
  const { getAllByType, getByText } = render(<QuestionsBoard {...props} />);
  const allQuestions = getAllByType(Question);

  setAnswer(allQuestions[0], 'a1');
  setAnswer(allQuestions[1], 'a2');

  fireEvent.press(getByText('submit'));

  expect(props.verifyQuestions).toBeCalledWith({
    '1': { q: 'q1', a: 'a1' },
    '2': { q: 'q2', a: 'a2' },
  });
});
Installation
Open a Terminal in your project's folder and run:
yarn add --dev react-native-testing-library
This library has a peerDependencies listing for react-test-renderer and, of course, react. Make sure to install them too!
As you may have noticed, it's not tied to React Native at all – you can safely use it in your React components if you feel like not interacting directly with DOM.
API / Usage
The public API of react-native-testing-library is focused around these essential methods:

render – deeply renders given React element and returns helpers to query the output components.
fireEvent - invokes named event handler on the element.
waitForElement - waits for non-deterministic periods of time until your element appears or times out.
flushMicrotasksQueue - waits for microtasks queue to flush.

Note to users who are more familiar with react-testing-library: This API does not expose cleanup because it doesn't interact with the DOM. There's nothing to clean up.
Made with ❤️ at Callstack
React Native Testing Library is an open source project and will always remain free to use. If you think it's cool, please star it 🌟. Callstack is a group of React and React Native geeks, contact us at hello@callstack.com if you need any help with these or just want to say hi!

Supported and used by Rally Health.
",738
GIScience/osm-vis-data,None,"OSMvis - OpenStreetMap Visualization - Data
This repository contains the data used for OSMvis. The files contain license information. For further information, please refer to http://github.com/mocnik-science/osm-vis.
",6
codecombat/codecombat,CoffeeScript,"CodeCombat







NOTE: The process for setting up the dev environment has changed. Please refer to the docs for details.
CodeCombat is a multiplayer programming game for learning how to code.
See the Archmage (coder) developer wiki for a dev
setup guide, extensive documentation, and much more to get started hacking!
It's both a startup and a community project, completely open source under the
MIT and Creative Commons licenses. It's the
largest open source CoffeeScript project by lines of
code, and since it's a game (with really cool tech),
it's really fun to hack on. Join us in teaching the world to code! Your
contribution will go on to show millions of players how cool programming can be.
Getting Started
We've made it easy to fork the project, run a simple script that'll install all
the dependencies, and get a local copy of CodeCombat running right away on
Mac, Linux,
Windows, or Vagrant.
See the docs for details.
Getting In Touch
Whether you're novice or pro, the CodeCombat team is ready to help you implement
your ideas. Reach out on our forum, our
issue tracker, or
our developer chat room on Slack, or
see the docs for more on how to contribute.

License
MIT for the code, and CC-BY for the
art and music. Please also
sign the CodeCombat contributor license agreement
so we can accept your pull requests. It is easy.
Note: the levels on codecombat.com are not open source.
Join Us!



























































",6749
Lombiq/Helpful-Extensions,C#,"Helpful Extensions Orchard module Readme
Project Description
Orchard module containing some handy extensions (e.g. filters for Projector).
Extensions
The module consists of the following independent extensions (all in their own features):
Code Generation Extensions
Content type code generation
Generates migration code from content definitions. You can use this to create (or edit) a content type on the admin and then move its creation to a migration class. Generated migration code is displayed under the content types' editors.
Commands Extensions
Permission commands
There are two commands:

lock frontend: locks access to the frontend of the site, i.e. only authenticated users will be able to access it.
unlock frontend: anonymous users will be able to access the site's frontend too.

Contents Extensions
Hint Field
The Hint Field is a content field. It displays a text in the editor of the content items that can be configured in the content type's editor. This way you can add hint texts to your content items' editors.
Projector Extensions
ContainedByFilter Projector filter
Use this Projector filter to filter for content items that are contained by a specific content item. E.g. if there is a top page with ContainerPart that contains various other pages, then you can use this filter to fetch items that are contained by the top page.
On the UI you only have to specify the id of the container.
IdsInFilter Projector filter
You can give this filter a comma-separated list of content item ids. The filter will then fetch the contents with those ids. This is useful if you want to handle a specific set of content items together in a query, probably with other, differently filtered content items.
EnumFilterEditor Projector filter editor
When you have a Projector binding set up for an enum property on a record this will provide an editor for it when you edit the query. You are able to use this to add an equals or not equals filter on an enum property.
ForeignKeyFilterEditor Projector filter editor
When you have a Projector binding set up for a property on a record that represents a foreign key you can use this to filter on its value.
Projectior Extensions - Search
SearchFilter Projector filter
This filter lets you specify a search query (can come from tokens) that will be used to match items against and then filter items with.
Tokens Extensions
Current content token
Adds the Content.Current token that will return the content item for the given page (e.g. the currently opened blog post). You can chain Content tokens to it.
WrapNotEmpty token
Wraps the text if it's not empty from left and optionally from right with the specified texts.
You can use these extensions as described on the above pages.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/helpful-extensions (Mercurial repository)
https://github.com/Lombiq/Helpful-Extensions (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",5
spack/spack,Python," Spack




Spack is a multi-platform package manager that builds and installs
multiple versions and configurations of software. It works on Linux,
macOS, and many supercomputers. Spack is non-destructive: installing a
new version of a package does not break existing installations, so many
configurations of the same package can coexist.
Spack offers a simple ""spec"" syntax that allows users to specify versions
and configuration options. Package files are written in pure Python, and
specs allow package authors to write a single script for many different
builds of the same package.  With Spack, you can build your software
all the ways you want to.
See the
Feature Overview
for examples and highlights.
To install spack and your first package, make sure you have Python.
Then:
$ git clone https://github.com/spack/spack.git
$ cd spack/bin
$ ./spack install libelf

Documentation
Full documentation for Spack is
the first place to look.
Try the
Spack Tutorial,
to learn how to use spack, write packages, or deploy packages for users
at your site.
See also:

Technical paper and
slides on Spack's design and implementation.
Short presentation from the Getting Scientific Software Installed BOF session at Supercomputing 2015.

Get Involved!
Spack is an open source project.  Questions, discussion, and
contributions are welcome. Contributions can be anything from new
packages to bugfixes, or even new core features.
Mailing list
If you are interested in contributing to spack, join the mailing list.
We're using Google Groups for this:

Spack Google Group

Slack channel
Spack has a Slack channel where you can chat about all things Spack:

Spack on Slack

Sign up here to get an invitation mailed
to you.
Twitter
You can follow @spackpm on Twitter for
updates. Also, feel free to @mention us in in questions or comments
about your own experience with Spack.
Contributions
Contributing to Spack is relatively easy.  Just send us a
pull request.
When you send your request, make develop the destination branch on the
Spack repository.
Your PR must pass Spack's unit tests and documentation tests, and must be
PEP 8 compliant.  We enforce
these guidelines with Travis CI.  To
run these tests locally, and for helpful tips on git, see our
Contribution Guide.
Spack uses a rough approximation of the
Git Flow
branching model.  The develop branch contains the latest
contributions, and master is always tagged and points to the latest
stable release.
Authors
Many thanks go to Spack's contributors.
Spack was created by Todd Gamblin, tgamblin@llnl.gov.
Citing Spack
If you are referencing Spack in a publication, please cite the following paper:

Todd Gamblin, Matthew P. LeGendre, Michael R. Collette, Gregory L. Lee,
Adam Moody, Bronis R. de Supinski, and W. Scott Futral.
The Spack Package Manager: Bringing Order to HPC Software Chaos.
In Supercomputing 2015 (SC’15), Austin, Texas, November 15-20 2015. LLNL-CONF-669890.

License
Spack is distributed under the terms of both the MIT license and the
Apache License (Version 2.0). Users may choose either license, at their
option.
All new contributions must be made under both the MIT and Apache-2.0
licenses.
See LICENSE-MIT,
LICENSE-APACHE,
COPYRIGHT, and
NOTICE for details.
SPDX-License-Identifier: (Apache-2.0 OR MIT)
LLNL-CODE-647188
",966
riicchhaarrd/cidscropt,C,"cidscropt
embeddable scripting language in C
",13
stripe/stripe-dotnet,C#,"Stripe.net
 

The official Stripe library, supporting .NET Standard 1.2+, .NET Core 1.0+, and .NET Framework 4.5+
Documentation
See the .NET API docs.
Installation
Install Stripe.net via NuGet
From the command line:
nuget install Stripe.net

From Package Manager:
PM> Install-Package Stripe.net

From within Visual Studio:

Open the Solution Explorer.
Right-click on a project within your solution.
Click on Manage NuGet Packages...
Click on the Browse tab and search for ""Stripe.net"".
Click on the Stripe.net package, select the appropriate version in the right-tab and click Install.

Set the API Key for your project
You can configure the Stripe.net package to use your secret API key in one of two ways:
a) In your application initialization, set your API key (only once once during startup):
StripeConfiguration.SetApiKey(""[your api key here]"");
b) Pass the API key to RequestOptions:
var planService = new PlanService();
planService.Get(*planId*, new RequestOptions { ApiKey = ""[your api key here]"" });
You can obtain your secret API key from the API Settings in the Dashboard.
Xamarin/Mono Developers (Optional)
If you are using Xamarin/Mono, you may want to provide your own HttpMessageHandler. You can do so by passing an instance to StripeConfiguration.HttpMessageHandler on your application's startup. See this thread for details.
Additional Resources

Stripe.net Fundamentals with ASP.NET (MVC) [PluralSight: Craig McKeachie]

Support

Make sure to review open issues and pull requests before opening a new issue.
Feel free to leave a comment or reaction on any existing issues.
For all other support requests, please reach out to Stripe via email.

Helpful Library Information
Request Options
All of the service methods accept an optional RequestOptions object. This is used if you need an Idempotency Key, if you are using Stripe Connect, or if you want to pass the secret API key on each method.
var requestOptions = new RequestOptions();
requestOptions.ApiKey = ""SECRET API KEY"";                        // (optional) set the api key on a per-request basis
requestOptions.IdempotencyKey = ""SOME STRING"";                   // (optional) create an idempotent request
requestOptions.StripeConnectAccountId = ""CONNECTED ACCOUNT ID"";  // (optional) authenticate as a connected account
Responses
The StripeResponse object is an attribute (with the same name) attached to all entities in Stripe.net when they are returned from a service call.
Example: Access the StripeResponse
var chargeService = new ChargeService();
StripeCharge charge = chargeService.Create(...);
StripeResponse response = charge.StripeResponse;
The information that can be derived from the StripeResponse is available from the StripeResponse Class.
public class StripeResponse
{
	// ResponseJson will always tell you the complete json Stripe returned to Stripe.net.
	// this will be the same as the ObjectJson when you execute a create/get/delete call.
	// however, if you execute a List() method, the ResponseJson will have the full api result
	// from Stripe (a charge list with 10 charges, for example).
	public string ResponseJson { get; set; }

	// when you call a List() method, the object json is the object in the response array that represents
	// the entity. The ResponseJson will be the full array returned from Stripe on every entity, however,
	// since that was the full response from Stripe. ObjectJson is always the same as ResponseJson when
	// you are doing a regular create/get/delete, because you are dealing with a single object.
	public string ObjectJson { get; set; }

	// this is the request id of the call, as seen in the Stripe dashboard. I would recommend logging
	// this and/or saving it to your database. this is very useful to help you find your request
	// in the dashboard, or ask Stripe a question about your api call
	public string RequestId { get; set; }

	// this is the request date and time of the call. I would also recommend logging this and/or
	// saving it to your database, as it tells you when Stripe processed the request.
	public DateTime RequestDate { get; set; }
}
Date Filtering
Many of the List()-methods support parameters to filter by date.  You can use the DateFilter class to combine the filters to make more interesting and complex queries.
Example: Interesting Queries with DateFilter
var chargeService = new ChargeService();

var chargesToday = chargeService.List(new ChargeListOptions {
	Created = new DateFilter { GreaterThanOrEqual = DateTime.UtcNow.Date }
});

var chargesYesterday = chargeService.List(new ChargeListOptions {
	Created = new DateFilter {
		GreaterThanOrEqual = DateTime.Now.AddDays(-1).Date,
		LessThan = DateTime.Now.Date
	}
});
Contribution Guidelines
We welcome contributions from anyone interested in Stripe or Stripe.net development. If you'd like to submit a pull request, it's best to start with an issue to describe what you'd like to build.
Once you've written your pull request, please make sure you test your changes.
",866
Randrian45/pylustrator,Python,"Pylustrator
 
 

Visualisations of data are at the core of every publication of scientific research results. They have to be as clear as
possible to facilitate the communication of research. As data can have different formats and shapes, the visualisations
often have to be adapted to reflect the data as well as possible. We developed Pylustrator, an interface to directly
edit python generated matplotlib graphs to finalize them for publication. Therefore, subplots can be resized and dragged
around by the mouse, text and annotations can be added. The changes can be saved to the initial plot file as python code.
Keywords: python, matplotlib, draggable subplots, texts, annotations, code generation
Please refer to our Documentation for more information and instructions on installing it.
",4
Randrian45/pylustrator,Python,"Pylustrator
 
 

Visualisations of data are at the core of every publication of scientific research results. They have to be as clear as
possible to facilitate the communication of research. As data can have different formats and shapes, the visualisations
often have to be adapted to reflect the data as well as possible. We developed Pylustrator, an interface to directly
edit python generated matplotlib graphs to finalize them for publication. Therefore, subplots can be resized and dragged
around by the mouse, text and annotations can be added. The changes can be saved to the initial plot file as python code.
Keywords: python, matplotlib, draggable subplots, texts, annotations, code generation
Please refer to our Documentation for more information and instructions on installing it.
",4
HuanGong/LightingIO,C++,"LigthingIO is a 'light' network IO application framework with some base impliment for better coding experience;
it has implemnet follow code/component


base code


message loop


repeat timer


coroutine scheduler


lazyinstance


net io based on loop


raw protocol


line protocol


http 1.x protocol


async redis client side support



add sendfile support for zero copy between kernel and user space
client support full async call when task not running on coroutine task


thread safe lru component
inverted indexer table
count min sketch with its utils
bloom filter
source loader (TP)

integration:

add async mysql client support; [pre]

About MessageLoop:
like mostly messageloop class, all PostTask/PostDelayTask/PostTaskWithReply implemented, it's ispired by chromium messageloop code; it's task also support location info for more convinience debug
/*
  follow code will give you infomathion about the crash task where from
  W1127 08:36:09.786927  9476 closure_task.h:46] Task Error Exception, From:LocationTaskTest@/path_to_project/base/message_loop/test/run_loop_test.cc:24
*/
void LocationTaskTest() {
  base::MessageLoop loop;

  loop.Start();

  loop.PostTask(NewClosure([](){
    printf(""FailureDump throw failed exception"");
    throw -1;
  }));

  loop.PostDelayTask(NewClosure([&]() {
    loop.QuitLoop();
  }), 2000);
  loop.WaitLoopEnd();
}
About Coroutine:
coroutine sheduler was a 'none-system-hack' type implement, why?, most of blablabla wonderful coroutine library will has some not clear behavior,(eg: thread locale storage. cross thread schedule, not-compatiable system api hack); the coroutine schedule implement depend a clear resume and run condition youself control; only work in a thread your arranged;
chinese:
coroutine调度的实现依赖于message loop， 没有对系统调用进行hack， 往往那些实现了很NB的coroutine库看上去的确很牛逼， 但是大多数却有着他们没有提到的这样或者那样的问题。比如说跨线程调度/thread locale storage的支持. 系统api的兼容等等。这些问题上这些库往往一言带过或者避而不谈。我遇到的95%甚至更多的coroutine库都有thread locale storage的问题;谁能保证我们引入的第三方代码里面没有使用呢？有些实现NB的库实现了coroutine local storage的确很叼，谁能保证我们引入的第三方代码里面没有使用呢？在C++没有垃圾回收机制的情况下。其实这些行为都非常危险; LightingIO中的现实没有syscall的hack， 不支持自动的跨线程调度，支持使用TLS（本身也依赖TLS）。他的切换与唤起需要MessageLoop和coroutine本身的配合，看上去不那么自动化，但是能非常符合我们预期的进行控制. 本质上,corotine 只是MessageLoop的一个TaskRunner; 但是我们却没有使用Crorontine作为任务队列的默认Runner。其实对于任何stackfull的coroutine来说，都是有代价的。而大多数时候，我们并不需要这些corutine，只是在某些必要的场景下， 我们希望很好的切换来使得代码更加可读，可控;特别对我这样的菜鸟级选手而言
//the only requirement, be sure in a messageloop; it's LightingIO's fundamentals
void coro_c_function();
void coro_fun(std::string tag);
// this should be your application main
{
    co_go coro_c_function; //schedule c function use coroutine

    co_go std::bind(&coro_fun, ""tag_from_go_synatax""); // c++11 bind

    co_go [&]() {  // lambda support
        LOG(INFO) << "" run lambda in coroutine"" ;
    };

    co_go &loop << []() { //run corotine with specify loop runing on
      LOG(INFO) << ""go coroutine in loop ok!!!"";
    };

    // 使用net.client定时去获取网络资源
    bool stop = false;
    co_go &loop << [&]() {
       do {
            // do http/redis/raw/line request async based on coroutine
            // see net/client for detail

            // do resource fetching

            co_sleep(1000); // every 1s
       } while(!stop);
    };
}

{
  MessageLoop loop;
  loop.Start();

  auto will_yield_fn = [&]() {

    // do something here util has something need async done
    // 例如这件事情需要很大的栈空间来完成或者是需要异步完成，这里挂起当前coroutine，并在这里就指定好resume的逻辑
    loop.PostTaskWithReply(NewClosure([]() {
      // some thing especial need big stack
    }),
    NewClosure(co_resumer));

    //or another function in coroutine with current corotine's resume_closure
    co_go std::bind(AnotherCoroutineWithResume, co_resumer);

    co_yield; // paused here, util co_resumer be called in any where;

    //  do other things
  }

  //keep in loop run this code
  loop.PostTask(NewClosure([&]() {
    co_go will_yield_fn; //just schedule, not entry now
  });
  loop.WaitLoopEnd();
}
LazyInstance:
  //class Foo, only when use to create Foo Object
  static base::LazyInstance<Foo> gFoo = LAZY_INSTANCE_INIT;
net performance:
作为对比，你可以随机的使用当前cpu支持的线程数和Nginx进行对比，不需要额外的设置. 无论你使用coroutine调度器
或者非coroutine调度器。具体数据自己验证.我跑出来的数据和你跑出来的数据都不一定具有代表性，最好的测试就是你
自己的测试;
repeated timer:
// Sample RepeatingTimer usage:
//
//   class MyClass {
//    public:
//     void StartDoingStuff() {
//       timer_.Start(ms, ClosureTask);
//     }
//     void StopStuff() {
//       timer_.Stop();
//     }
//    private:
//     void DoStuff() {
//       // This method is called every delay_ms to do stuff.
//       ...
//     }
//     base::RepeatingTimer timer_;
//   };
//

  base::RepeatingTimer timer(&loop);
  timer.Start(interval_ms, [&]() {
    // do something;
  });


NOTE
INPROCESS;
This library still in progress, do not use it in your project or production environments;
email me if any question and problem;
Learning and integrate what i think into LightingIO
LightingIO itself just a normal network application framework; all its technology from my practice and work
at the start time of this project; i use libevent, but i find many things was limited by the exsiting code; it's hard to hack or change
the libevent code; so... I give up libvent at last, and then start all things from zero, in thoes days and night, thanks the support from
my girl friends; and other reference books
From HuanGong 2018-01-08
",4
NyaaCat/RPGItems-reloaded,Java,"RPGItems 
The RPGItems2 plugin continued from TheCreeperOfRedstone/RPG-Items-2
RPGitem starting from 3.6 depends on NyaaCore to work! See Installation for detail.
Project Discussion
Discord Server: 
Status
The develop branch is 1.13.2 version 3.7 and is in beta. Ensure you have latest Spigot/Paper and WorldEdit & WorldGuard. Note upgrading from 3.6 to 3.7 changes item NBT so it may not behavior correctly with other plugins like shops that compares nbt or plugins relying on old item lore format.
The stable/main branch is 1.13.2 version 3.6. It should be fully compatible with Spigot/Paper 1.13.2.
The stable branch is 1.11 version 3.5 compatibles with Spigot 1.11 ~ 1.12.x and the work is focused on fixing bugs and compatibility issues resolving. It will not port to 1.13 but it should work (with some minor issue).
Old branches (1.7 ~ 1.10, 1.12, 1.13, 1.13.1) are no longer maintained.
Critical bug fix (e.g. item duplication) may be backported upon request (by opening a issue).
Backport PR is welcomed.
All builds can be found in the Releases Page.
Please choose builds with prefixes matching your server version.
Some version of this plugin use code from NBT API internally.
Resources
Wiki | Spigot page | Javadoc
",88
achiku/cnps,Python,"cnps


See who's comming to your event through connpass
Installation
pip install cnps

Usage
Usage: cnps [OPTIONS] COMMAND [ARGS]...

  cnps cli

Options:
  --help  Show this message and exit.

Commands:
  dump    dump basic event applicants data
  filter  filter applicants data with multiple options

dump applicants data
You should first dump all applicants data in json format. It contains Twitter/Facebook/GitHub links, connpass user id, and event dates that a user recently participated (or applied). This scrapes event page, so just be nice to connpass, they are cool.
Usage: cnps dump [OPTIONS] EVENT_URL

  dump basic event applicants data

Options:
  --help  Show this message and exit.

# example
cnps dump https://fintech-engineers-drink-up.connpass.com/event/56057/ > user.json
filter user data
Once you dumped applicants data, you can now filter with the following options. You might want to find users with no social links, trying to join more than two events at the same date, etc.
Usage: cnps filter [OPTIONS] FILE_PATH

  filter  filter applicants data with multiple options

Options:
  --facebook-link / --no-facebook-link
  --github-link / --no-github-link
  --twitter-link / --no-twitter-link
  --duplicate-event / --no-duplicate-event
  --own-event / --no-own-event
  --presentation / --no-presentation
  --avg-event-interval FLOAT
  --help                          Show this message and exit.

# example to find users trying to participate more than two events in the same date in the latest 10 events
cnps filter ./user.json  --duplicate-event
",2
silicator/PiFunk,C,"README

PiFunk Radio Transmitter in FM/AM for CB and PMR446 etc.
Early Experimental!
based on PiFM/AM-Scripts

Preparations:
get this program via:
git clone https://github.com/silicator/PiFunk
1-Wire by default BCM4 setting needs to be activated in boot-config for autostart
via command: sudo modprobe w1-gpio,gpiopin=4
Using w1-gpio sometimes needs a 4.7 kΩ pullup resistor connected on GPIO pin
(if you have problems deactivate 1-wire config!)
manually open with nano-editor: sudo nano /boot/config.txt (i provide one too)
add line: dtoverlay=w1-gpio,gpiopin=4,pullup=0 (add pullup=1 if needed)

Build:
You will need some libraries for this:
sudo apt update for system updates
sudo apt upgrade for system upgrades
sudo apt-get install libsndfile1-dev
sudo apt-get install python-dev python3-dev for py3
RPi-lib (i use v0.6.5 from Nov 2018, also in repo)
sudo pip-3.7 install RPi.GPIO for Py3 (easiest way)
sudo pip install RPi.GPIO for Py2
or alternative ways: sudo apt-get -y install python3-rpi.gpio
wget https://pypi.python.org/packages/source/R/RPi.GPIO/RPi.GPIO-0.6.5.tar.gz
then extract tar -xvf RPi.GPIO-0.6.5.tar.gz and install
then go to directory:
cd PiFunk
compile with:
GNU installer sudo apt-get install gcc
-g for debuggerinformations (optional 0-3 optimalization level)
-std=c99 (sometimes gnu99) for C99-standard
-lm for math lib is obligatory!
-Iinclude  for using include-directory with headerfiles
-Llib for using library-directory
-lsndfile for ALSA ""snd""-lib
-c for compiling without linking
-fPIC for generating
-shared for generating position independent code (PIC) for shared libs
-o for output-filename
command:
generating libraries:
gcc -g -std=c99 -lm -Iinclude -Llib -c -fPIC pifunk.c -shared -o pifunk.o pifunk.so pifunk.a
generating executable binary:
gcc -g -std=c99 -lm -Iinclude -Llib -lsndfile -fPIC pifunk.c -shared -o bin/pifunk.out bin/pifunk
make
make install

Usage:
run with admin/root permissions:
Arguments: [filename (.wav)] [freq (MHz)] [samplerate (kHz)] [modulation (fm/am)] [callsign (optional)] 
extra single Arguments:
[menu] as step-by-step assistent
[help] for more infos and arguments
Use '. dot' as decimal-comma separator!
default: sudo pifunk sound.wav 100.000 22050 fm callsign
Radio works with *.wav-file with 16-bit @ 22050.000 [Hz] mono / 1-700+ MHz range.
CTSS-Tones for PMR can be found here CTSS

Warnings:


Use (original) power supply 10 W, 5V @ ~2 A or ~5 V/500 mA via miniUSB 2.0 or 5.5 V Pins possible)


PWM on GPIO 4/Pin 7 @ 4 mA (50 mA max. on ALL pins or 16 per bank!!!)


(in example: Pi B+ v1.2 @ 700 MHz/512 MB RAM on ARM processor bcm2835-v1.55)
for more Specifications just visit Adafruit


Antenna should be grounded if possible (PIN 9 right one next to GPIO4)


You can try to smooth it out with a 1:X-balloon if using long HF antenna


Dummy-load: 1-100 W @ 50 Ohm ""cement"" or similar with cooling-ribs with fan for testing.


For transmission you should use tested Antennas!


Tip: You could use just a copper wire for 2m/70cm-band or
other lambda(1/4)-antennas (17.5cm/6.9in for PMR)



Disclaimer:


Private Project! Work in Progress (WIP)


I'm not a professional so NO guarantees or warranty for any damage or similar!!


Usage at your own risk !!


Check laws of your country first! Some Frequencies are prohibited or need a Ham-License!


Pi operates with square-waves (²/^2)!!- Use Low-/High-Band-Pass-Filters with ~10 uF caps
with solenoids or resistors/diodes to prevent transmitting (TX) simultaneously on permitted frequencies!




Help / Testers and Feedback always appreciated!*


Thank you and have fun!*



Links:
GitPage
Readme Guideline
Contribution Guideline
Code of Conduct Guideline
Copying Guideline
License Guideline under Open-Source GPLv3.0
Would appreciate beeing named in the source
",4
tiagoifsp/CertiFind,C#,"CertiFind
Gerenciador de Certificados.
Sistema desenvolvido como avaliação da disciplina Projeto Integrado I.
Documentação:
https://docs.google.com/document/d/1GQTh-4wJRzOXwebKNwHl4De-wAt0L2Lvp9mYmn2J8m0/edit?usp=sharing


As alterações no documento de requisitos devem ser enviadas como sugestão.


Backup inicial do banco de dados:
https://github.com/tiagoifsp/CertiFind/tree/master/Backup
Grupos:
Grupo 1
Marcelo, Rafael, Fabio, André, Giovane, Harbs, Gabriel Rocha
Grupo 2
Juliene, Igor, Danilo, Eduardo, Jonathas, Gabriel Reis
Grupo 3
Nathan, Bruno, Emerson, Evelyn, Alan, Getulio
Grupo 4
Fabricio, Eliane, Agatha, João, Mikael, Murilo, Vinicius
Requisitos:
Grupo 4
RF01	Configuração inicial
RF02	Entrada no sistema
RF03	Edição das configurações do sistema
RF04	Criação de usuário
RF05	Manutenção de usuário
RF06	Alteração de senha por parte do próprio usuário
RF07	Recuperação de senha por parte do usuário
RF08	Saída do sistema
Grupo 1
RF09	Cadastro de Campo
RF10	Manutenção de Campo
RF11	Cadastro de Tipo de Atividade
RF12	Manutenção de Tipo de Atividade
RF13	Cadastro de Certificado
RF14	Manutenção de Certificado
RF15	Relatório de Certificado
Grupo 2
RF16	Cadastro de Grupo de Pesquisa
RF17	Manutenção de Grupo de Pesquisa
RF18	Vinculação Docente / Grupo de Pesquisa
RF19	Transferência de liderança do Grupo de Pesquisa
RF20	Relatório por grupo de pesquisa
RF21	Comunicados do sistema
Grupo 3
RF22	Histórico de erros do sistema
RF23	Backup do sistema
RF24	Envio e leitura de sugestões
RF25	Status do sistema
RF26	Relatório de utilização do sistema
RF27	Auditoria do sistema
Regras:
@GitHub

Ninguém deverá alterar a branch Master;
Cada grupo deverá trabalhar em suas próprias branches;
Cada requisito deverá ser desenvolvido em sua própria branch;
Cada bug deverá ser corrigido em sua própria branch;
O pull request deverá ser solicitado pelo grupo, e apenas o gerente poderá fazer o merge.

@Banco de Dados

O backup inicial do banco de dados não poderá ser alterado;
Qualquer alteração deverá ser realizada por consultas SQL;
Todas as alterações devem ser inseridas no arquivo #https://github.com/tiagoifsp/CertiFind/blob/master/Backup/Alteracoes;
Restaurar o backup no PC do desenvolvedor. Futuramente teremos um servidor com um banco de dados para testes.

@Código-fonte

Utilizar o FormTeste para chamada de outros formulários;
Após o login do sistema estar funcionando, o FormTeste será removido;
Seguir o padrão MVC;
Nomenclatura das classes:
a) Model : M. Exemplo: MAluno
b) DAL: D. Exemplo: DAluno
c) Controller: C. Exemplo: CAluno
d) View: V. Exemplo: VAlunoPesquisar
Nomenclatura dos arquivos: cada arquivo deverá conter uma classe apenas, e o nome do arquivo deve ser igual ao nome da classe. Exemplo: MAluno.cs
Não alterar os nomes dos namespaces;
Exemplo de nomenclatura dos controles:
a) TextBox/MaskedTextBox: txtNome
b) ComboBox: cboTipo
c) Label (quando o valor é alterado no código): lblSituacao
d) DateTimePicker: dtpData
e) DataGridView: dgvResultado
f) NumericUpDown: nudFilhos
g) Checkbox: chkAtivo
h) PictureBox: pcbFoto
i) RadioButton: rdoGrupo
j) RichTextBox: rtbDescricao
k) Button: btnSalvar
Verificar os forms de Exemplo para desenvolver a interface;
Em CRUDs básicos, o mesmo form deve ser utilizado para salvar/editar informações;
Utilizar a classe Model.Erros para armazenar as mensagens de erro que devem ser exibidadas ao usuário;
Utilizar a classe DAL.ExcecaoPadrao para disparar exceções esperadas pelo sistema;
Utilizar a classe DAL.Conexao para abrir/fechar conexões com o SGBD.

",9
ifrontend-xyz/awesome-bookmark,CSS,"书签
前端技术
用AOP改善javascript代码
JS无形装逼，最为致命
npm如何管理依赖包的版本
优化浏览器前端
你真的理解了MVC, MVP, MVVM吗？
14招搞定JavaScript调试
HTML5中手势原理分析与数学知识的实践
你必须要懂的原生JS
前端跨页面通信，你知道哪些方法？
rrweb：打开 web 页面录制与回放的黑盒子
前端必备！最全nginx技术分析
Content Security Policy 入门教程
Serverless
深入理解javascript错误处理机制
JSON Schema 介绍及应用
12 个令人惊叹的 CSS 项目
Web 性能优化： 图片优化让网站大小减少 62%
30 seconds of code
635000 个 npm 包中我应该用哪个？
一口（很长的）气了解 babel
PWA学习手册
quicklink：实现原理与给前端的启发
JavaScript 如何正确处理 Unicode 编码问题！
前端面试官的套路，你懂吗？
为什么越来越少的人用 jQuery？
前端模块化详解(完整版)
停止学习框架
为什么前端监控要用GIF打点
像写作一样去写代码，如何把异步的形式改写成同步的形式
浏览器存储之争
console的那些事儿
JavaScript错误处理权威指南
如何优雅处理前端异常？
抛开 Vue、React、JQuery 这类第三方js，我们该怎么写代码？
浅谈 web 前端开发中的国际化
26个精选的JavaScript面试问题
前端常用开发工具的路径解析配置
面向体验的重构优化
我从没理解过 JavaScript 闭包
精读《高性能 javascript》
Common webpage design mistakes
写给前端工程师的10条实用原则
Node.js 前端开发指南
一些 JavaScript 中的代码小技巧
小哥哥小姐姐，来尝尝 Async 函数这块语法糖
通过Recompose库掌握React函数组件
彻底搞懂JavaScript中的this指向问题
Vue.js是如何做到数据响应的？
深入理解 React 高阶组件
JS 函数式编程指南
如何正确的学习Node.js
图解 React
反击爬虫，前端工程师的脑洞可以有多大？
如何自己写一个公用的NPM包
前端面试图普
JS高程中的垃圾回收机制与常见内存泄露的解决方法
前段进阶必备：JavaScript 内存机制
JavaScript如何工作:内存管理+如何处理4个常见的内存泄漏
前端可用性保障实践
javascript 秘密花园
前端中的函数式编程
JavaScript核心概念归纳整理
ESLint配置参数介绍
React 常用面试题目与分析
2017前端性能优化清
Chrome DevTools 之 Profiles，深度性能优化必备
Web Worker 使用教程
合并HTTP请求 vs 并行HTTP请求，到底谁更快？
浏览器数据库 IndexedDB 入门教程
Web框架的架构模式探讨（JavaScript语言）
你需要了解的23种JavaScript设计模式
Promise 的内部是如何工作的？
Webpack 4 配置最佳实践
Mathjax与LaTex公式简介
Service Worker
构建 Web 应用之 Service Worker 初探
Immutable
不可变数据
Immutable 详解及 React 中实践
可变对象与 immutable.js
Immutable 常用API简介
Flutter
深入理解Flutter多线程
Flutter 您需要知道的知识点
Flutter在2019年会有怎样的表现？
Fish Redux中的Dispatch是怎么实现的
Flutter自定义绘制Widget初探
为什么说 Flutter 不一定是趋势?
流言终结者- Flutter和RN谁才是更好的跨端开发方案？
GMTC-闲鱼Flutter实践效果访谈
关于Flutter，你想知道的都在这里了！
Flutter实战
让我们在2019年重新认识 Flutter
为前端工程师准备的 Flutter 入门指南
Flutter项目优雅的使用ORM数据库
Nodejs
用Node.js开发一个Command Line Interface (CLI)
Java能抵挡住JavaScript的进攻吗？
顶尖 API 文档管理工具 (Yapi)
api 接口管理工具
计算机技术
终于有人把Elasticsearch原理讲透了！
数据与广告系列四：搜索广告来源和竞价策略
英特尔的发家史
GraphQL
GraphQL到底怎么用？看看这个例子就知道了
docker
docker镜像瘦身&优化
Docker 入门教程
Docker 快速上手指南
Docker的学习--命令使用详解
Docker 江湖】之初入江湖
docker-compose.yml 语法说明
漫话：如何给女朋友解释什么是并发和并行
HTTP协议冷知识大全
鲜为人知的HTTP协议头字段详解大全
原前缀树和后缀树
再好好聊一聊 HTTP 中的 Cookie 细节
正则表达
vscode: Visual Studio Code 常用快捷键
Git的奇技淫巧
浅谈 MVC、MVP 和 MVVM 架构模式
从项目工程化的角度，谈一下 MVC 与 MVVM
深入理解git merge 和 git rebase
Base64 你可能不知道的几个细节
HTTP 缓存
GET和POST的区别
如何使用rsync命令
Jenkins 学习使用实践
数字世界中的纸张——理解 PDF
数据库索引是什么？新华字典来帮你
各种算法的动态演示网站
不就是个短信登录API嘛，有这么复杂吗？
一刻钟学会Go语言
Sonic：用Rust编写的Elasticsearch的极简替代品
掌握它才说明你真正懂Elasticsearch
Sentry
sentry使用实践
Vue SPA项目 + Sentry 实现前端错误监控
互联网
一对一沟通关系着团队的产出，你重视了吗？
技术选型指南
中国互联网20年简史（1998-2018）
腾讯的产品思维VS阿里的终局思维
程序员你为什么这么累？
为什么美国程序员工作比中国程序员工作轻松、加班少？
知者不言，言者不知：论华人工程师之领导力
Pornhub，一个神奇的网站
创业一年半项目经验分享
投资理财
基金定投一定赚钱吗？
程序员的房产、金融与第二战场
期权入门篇
三大报表：财务界的通用语言
从 25 倍稀释下的蘑菇街期权说起
远望资本程浩：资本寒冬下的创业真相
设计
9个小知识教你更好的理解动画原则，提升动效品质
不是你的错，是设计的错
杂文
一个“如何变得更靠谱”的小案例
努力就会成功
如何高效的主持会议
你是否能被轻易地取代？
数学乐
医学微视
我是一名职业IT培训讲师
学两招，快速准备好一次技术分享
浅谈OKR- 什么是OKR？
AI
个性化推荐技术
机器学习入门系列(1)--机器学习概览(上)
什么是机器学习
一刻钟学会Python3
漫画人工智能：啥是机器学习？
",2
ak239/thetool,JavaScript,"thetool



thetool is a CLI tool to capture different cpu, memory and other profiles for your node app in Chrome DevTools friendly format.

Quick start
npx thetool -o . -t cpu npm run test
Installation
# global install with npm:
npm install -g thetool

# alternatively, with yarn:
yarn global add thetool
Getting Started

thetool works only with Node >= 10.x.

Backed command example:
thetool -o . -t cpu npm run test
thetool interface is simple as 1-2-3.

Specify output folder for captured data using -o flag, e.g. -o . to put output in current folder.
Specify tool name using -t, currently supported tools: CPU profiler (cpu), Sampling Memory Profiler (memorysampling), Allocation Memory Profiler (memoryallocation), Coverage Profiler (coverage), Type Profiler (type), you can find a little bit more details below about each of them.
Put any command that runs node process after arguments, e.g. node index.js or npx thetool or npm run test, .. thetool supports child processes.

When report is ready, thetool will dump thetool> Report captured in ... message in terminal with a hint how to analyze it.
On-demand tooling
You can use --ondemand flag to profile only part of your app:

Add --ondemand flag to the list of thetool arguments.
Call startTheTool/stopTheTool from your Node scripts (thetool will add these methods to Node context).

startTheTool/stopTheTool methods are asynchronous, so you should await them or chain them using promise.then
Couple examples:
async function main() {
  await startTheTool();
  // code of your app
  await stopTheTool();
}
// .. or using promises..
function main() {
  startTheTool().then(() => {
    // code of your app
  }).then(() => stopTheTool());
}
Tools
CPU Profiler
thetool -o . -t cpu npm run test
To analyze: open Chrome DevTools, to to Performance tab, click load button, select file with data.
Sampling Memory Profiler
thetool -o . -t memorysampling npm run test
To analyze: open Chrome DevTools, go to Memory tab, click load button, select file with data.
--samplingInterval option is available: average sample interval in bytes, poisson distribution is used for the intervals. The default value is 32768 bytes
Allocation Memory Profiler
thetool -o . -t memoryallocation npm run test
To analyze: open Chrome DevTools, go to Memory tab, click load button, select file with data.
Heap Snapshot tool
thetool -o . -t heapsnapshot node -e ""captureTheTool.then(captureTheTool).then(captureTheTool)""
Given command will capture three heap snapshots.
To analyze: open Chrome DevTools, go to Memory tab, click load button, select file with data. You can load multiple snapshots and compare them from DevTools UI.
Tracing
thetool -o . -t tracing --recordMode recordAsMuchAsPossible --includedCategories node,v8 npm run test
To analyze: open Chrome DevTools, go to Performance tab, click load button, select file with data.
--recordMode controls how the trace buffer stores data (recordUntilFull, recordContinuously, recordAsMuchAsPossible)
--includedCategories please take a look on different available categories on https://nodejs.org/api/tracing.html
E.g. you can capture V8 sampling profiler using following command:
thetool -o . -t tracing --recordMode recordAsMuchAsPossible --includedCategories v8.execute,v8.cpu_profiler,v8.cpu_profiler.hires npm run test
Coverage Profiler
thetool -o . -t coverage npm run test
To analyze: in current folder create ./coverage/tmp folder and move files with data to this folder, run c8: npx c8 report. Please take a look at c8 README.md to see what output formats are supported.
Type Profiler
thetool -o . -t type npm run test
To analyze: please build something and I will put it here.
",8
freestrings/jsonpath,Rust,"jsonpath_lib




Rust 버전 JsonPath 구현이다. Webassembly와 Javascript에서도 유사한 API 인터페이스를 제공 한다.
It is JsonPath JsonPath engine written in Rust. it provide a similar API interface in Webassembly and Javascript also.

Webassembly Demo
NPM jsonpath-wasm - webassembly
NPM jsonpath-rs - native addon

Rust API

jsonpath_lib crate
Rust - jsonpath::Selector struct
Rust - jsonpath::select(json: &serde_json::value::Value, jsonpath: &str)
Rust - jsonpath::select_as_str(json_str: &str, jsonpath: &str)
Rust - jsonpath::select_as<T: serde::de::DeserializeOwned>(json_str: &str, jsonpath: &str)
Rust - jsonpath::compile(jsonpath: &str)
Rust - jsonpath::selector(json: &serde_json::value::Value)
Rust - jsonpath::selector_as<T: serde::de::DeserializeOwned>(json: &serde_json::value::Value)
Rust - Other Examples

Javascript API

npm package
Javascript - jsonpath.Selector class
Javascript - jsonpath.select(json: string|object, jsonpath: string)
Javascript - jsonpath.compile(jsonpath: string)
Javascript - jsonpath.selector(json: string|object)
Javascript - allocJson, deallocJson (Webassembly Only)
Javascript - Other Examples


Rust API
jsonpath_lib crate
Go to creates.io
extern crate jsonpath_lib as jsonpath;
#[macro_use]
extern crate serde_json;
Rust - jsonpath::Selector struct
#[derive(Serialize, Deserialize, PartialEq, Debug)]
struct Friend {
    name: String,
    age: Option<u8>,
}

let json_obj = json!({
    ""school"": {
        ""friends"": [
            {""name"": ""친구1"", ""age"": 20},
            {""name"": ""친구2"", ""age"": 20}
        ]
    },
    ""friends"": [
        {""name"": ""친구3"", ""age"": 30},
        {""name"": ""친구4""}
]});

let mut selector = Selector::new();

let result = selector
    .path(""$..[?(@.age >= 30)]"").unwrap()
//    .value_from_str(&serde_json::to_string(&json_obj).unwrap() /*&str*/).unwrap()
//    .value_from(&json_obj /*&impl serde::ser::Serialize*/).unwrap()
    .value(&json_obj /*serde_json::value::Value*/).unwrap()
    .select_as_value().unwrap();

assert_eq!(json!([{""name"": ""친구3"", ""age"": 30}]), result);

let result = selector.select_as_str().unwrap();
assert_eq!(r#""[{""name"":""친구3"",""age"":30}]""#, result);

let result = selector.select_as::<Vec<Friend>>().unwrap();
assert_eq!(vec![Friend { name: ""친구3"".to_string(), age: Some(30) }], result);

let _ = selector.map(|v| {
    let r = match v {
        Value::Array(mut vec) => {
            for mut v in &mut vec {
                v.as_object_mut().unwrap().remove(""age"");
            }
            Value::Array(vec)
        }
        _ => Value::Null
    };
    Some(r)
});
assert_eq!(json!([{ ""name"": ""친구3"" }]), selector.get().unwrap());

let _ = selector.value(&json_obj).unwrap()
    .map_as(|mut v: Vec<Friend>| {
        let mut f = v.pop().unwrap();
        f.name = ""friend3"".to_string();
        f.age = None;
        Some(vec![f])
    });

assert_eq!(vec![Friend { name: ""friend3"".to_string(), age: None }],
           selector.get_as::<Vec<Friend>>().unwrap());
Rust - jsonpath::select(json: &serde_json::value::Value, jsonpath: &str)
let json_obj = json!({
    ""school"": {
        ""friends"": [
            {""name"": ""친구1"", ""age"": 20},
            {""name"": ""친구2"", ""age"": 20}
        ]
    },
    ""friends"": [
        {""name"": ""친구3"", ""age"": 30},
        {""name"": ""친구4""}
]});

let json = jsonpath::select(&json_obj, ""$..friends[0]"").unwrap();

let ret = json!([
    {""name"": ""친구3"", ""age"": 30},
    {""name"": ""친구1"", ""age"": 20}
]);
assert_eq!(json, ret);
Rust - jsonpath::select_as_str(json: &str, jsonpath: &str)
let ret = jsonpath::select_as_str(r#""
{
    ""school"": {
        ""friends"": [
                {""name"": ""친구1"", ""age"": 20},
                {""name"": ""친구2"", ""age"": 20}
            ]
    },
    ""friends"": [
        {""name"": ""친구3"", ""age"": 30},
        {""name"": ""친구4""}
    ]
}
""#, ""$..friends[0]"").unwrap();

assert_eq!(ret, r#""[{""name"":""친구3"",""age"":30},{""name"":""친구1"",""age"":20}]""#);
Rust - jsonpath::select_as<T: serde::de::DeserializeOwned>(json: &str, jsonpath: &str)
#[derive(Deserialize, PartialEq, Debug)]
struct Person {
    name: String,
    age: u8,
    phones: Vec<String>,
}

let ret: Person = jsonpath::select_as(r#""
{
    ""person"":
        {
            ""name"": ""Doe John"",
            ""age"": 44,
            ""phones"": [
                ""+44 1234567"",
                ""+44 2345678""
            ]
        }
}
""#, ""$.person"").unwrap();

let person = Person {
    name: ""Doe John"".to_string(),
    age: 44,
    phones: vec![""+44 1234567"".to_string(), ""+44 2345678"".to_string()],
};

assert_eq!(person, ret);
Rust - jsonpath::compile(jsonpath: &str)
let mut template = jsonpath::compile(""$..friends[0]"");

let json_obj = json!({
    ""school"": {
        ""friends"": [
            {""name"": ""친구1"", ""age"": 20},
            {""name"": ""친구2"", ""age"": 20}
        ]
    },
    ""friends"": [
        {""name"": ""친구3"", ""age"": 30},
        {""name"": ""친구4""}
]});

let json = template(&json_obj).unwrap();

let ret = json!([
    {""name"": ""친구3"", ""age"": 30},
    {""name"": ""친구1"", ""age"": 20}
]);

assert_eq!(json, ret);
Rust - jsonpath::selector(json: &serde_json::value::Value)
let json_obj = json!({
    ""school"": {
        ""friends"": [
            {""name"": ""친구1"", ""age"": 20},
            {""name"": ""친구2"", ""age"": 20}
        ]
    },
    ""friends"": [
        {""name"": ""친구3"", ""age"": 30},
        {""name"": ""친구4""}
]});

let mut selector = jsonpath::selector(&json_obj);

let json = selector(""$..friends[0]"").unwrap();

let ret = json!([
    {""name"": ""친구3"", ""age"": 30},
    {""name"": ""친구1"", ""age"": 20}
]);

assert_eq!(json, ret);

let json = selector(""$..friends[1]"").unwrap();

let ret = json!([
    {""name"": ""친구4""},
    {""name"": ""친구2"", ""age"": 20}
]);

assert_eq!(json, ret);
Rust - jsonpath::selector_as<T: serde::de::DeserializeOwned>(json: &serde_json::value::Value)
let json_obj = json!({
    ""school"": {
       ""friends"": [
            {""name"": ""친구1"", ""age"": 20},
            {""name"": ""친구2"", ""age"": 20}
        ]
    },
    ""friends"": [
        {""name"": ""친구3"", ""age"": 30},
        {""name"": ""친구4""}
]});

#[derive(Serialize, Deserialize, PartialEq, Debug)]
struct Friend {
    name: String,
    age: Option<u8>,
}

let mut selector = jsonpath::selector_as::<Vec<Friend>>(&json_obj);

let json = selector(""$..friends[0]"").unwrap();

let ret = vec!(
    Friend { name: ""친구3"".to_string(), age: Some(30) },
    Friend { name: ""친구1"".to_string(), age: Some(20) }
);
assert_eq!(json, ret);

let json = selector(""$..friends[1]"").unwrap();

let ret = vec!(
    Friend { name: ""친구4"".to_string(), age: None },
    Friend { name: ""친구2"".to_string(), age: Some(20) }
);

assert_eq!(json, ret);

Javascript API
npm package
jsonpath-wasm
// browser
import * as jsonpath from ""jsonpath-wasm"";
// NodeJs
const jsonpath = require('jsonpath-wasm');
jsonpath-rs (NodeJS only)
Goto npmjs.org
const jsonpath = require('jsonpath-rs');
javascript - Selector class
jsonpath-wasm
wasm-bindgen 리턴 타입 제약 때문에 빌더 패턴은 지원하지 않는다.
It does not support builder-pattern due to the return type restriction of wasm-bindgen.
let jsonObj = {
    ""school"": {
        ""friends"": [
            {""name"": ""친구1"", ""age"": 20},
            {""name"": ""친구2"", ""age"": 20}
        ]
    },
    ""friends"": [
        {""name"": ""친구3"", ""age"": 30},
        {""name"": ""친구4""}
    ]
};

let ret = [
    {""name"": ""친구3"", ""age"": 30},
    {""name"": ""친구1"", ""age"": 20}
];

let selector = new jsonpath.Selector();
selector.path('$..friends[0]');
selector.value(jsonObj);

let selectAsObj = selector.selectAs();
let selectAsString = selector.selectAsStr();

console.log(
    JSON.stringify(ret) == JSON.stringify(selectAsObj),
    JSON.stringify(ret) == selectAsString
);

selector.map(function(v) {
    let f1 = v[0];
    f1.name = 'friend3';
    return [f1];
});

console.log(JSON.stringify(selector.get()) === JSON.stringify([{""name"": ""friend3"", ""age"": 30}]));
// => true
jsonpath-rs
let jsonObj = {
    ""school"": {
        ""friends"": [
            {""name"": ""친구1"", ""age"": 20},
            {""name"": ""친구2"", ""age"": 20}
        ]
    },
    ""friends"": [
        {""name"": ""친구3"", ""age"": 30},
        {""name"": ""친구4""}
    ]
};

let ret = [
    {""name"": ""친구3"", ""age"": 30},
    {""name"": ""친구1"", ""age"": 20}
];

let selector = new jsonpath.Selector()
    .path('$..friends[0]')
    .value(jsonObj);

let selectAsObj = selector.selectAs();
let selectAsString = selector.selectAsStr();

console.log(
    JSON.stringify(ret) == JSON.stringify(selectAsObj),
    JSON.stringify(ret) == selectAsString
);

// => true, true
Javascript - jsonpath.select(json: string|object, jsonpath: string)
let jsonObj = {
    ""school"": {
        ""friends"": [
            {""name"": ""친구1"", ""age"": 20},
            {""name"": ""친구2"", ""age"": 20}
        ]
    },
    ""friends"": [
        {""name"": ""친구3"", ""age"": 30},
        {""name"": ""친구4""}
    ]
};

let ret = [
    {""name"": ""친구3"", ""age"": 30},
    {""name"": ""친구1"", ""age"": 20}
];


let selectAsString = jsonpath.select(JSON.stringify(jsonObj), '$..friends[0]');
let selectAsObj = jsonpath.select(jsonObj, '$..friends[0]');

console.log(
    JSON.stringify(ret) == JSON.stringify(selectAsString),
    JSON.stringify(ret) == JSON.stringify(selectAsObj)
);

// => true, true
Javascript - jsonpath.compile(jsonpath: string)
let template = jsonpath.compile('$..friends[0]');

let jsonObj = {
    ""school"": {
        ""friends"": [
            {""name"": ""친구1"", ""age"": 20},
            {""name"": ""친구2"", ""age"": 20}
        ]
    },
    ""friends"": [
        {""name"": ""친구3"", ""age"": 30},
        {""name"": ""친구4""}
    ]
};

let ret = [
    {""name"": ""친구3"", ""age"": 30},
    {""name"": ""친구1"", ""age"": 20}
];

let selectAsString = template(JSON.stringify(jsonObj));
let selectAsObj = template(jsonObj);

console.log(
    JSON.stringify(ret) == JSON.stringify(selectAsString),
    JSON.stringify(ret) == JSON.stringify(selectAsObj)
);

// => true, true

let jsonObj2 = {
    ""school"": {
        ""friends"": [
            {""name"": ""Millicent Norman""},
            {""name"": ""Vincent Cannon""}
        ]
    },
    ""friends"": [ {""age"": 30}, {""age"": 40} ]
};

let ret2 = [
    {""age"": 30},
    {""name"": ""Millicent Norman""}
];

let selectAsString2 = template(JSON.stringify(jsonObj2));
let selectAsObj2 = template(jsonObj2);

console.log(
        JSON.stringify(ret2) == JSON.stringify(selectAsString2),
        JSON.stringify(ret2) == JSON.stringify(selectAsObj2)
);

// => true, true
Javascript - jsonpath.selector(json: string|object)
let jsonObj = {
    ""school"": {
        ""friends"": [
            {""name"": ""친구1"", ""age"": 20},
            {""name"": ""친구2"", ""age"": 20}
        ]
    },
    ""friends"": [
        {""name"": ""친구3"", ""age"": 30},
        {""name"": ""친구4""}
    ]
};

let ret1 = [
    {""name"": ""친구3"", ""age"": 30},
    {""name"": ""친구1"", ""age"": 20}
];

let ret2 = [
    {""name"": ""친구4""},
    {""name"": ""친구2"", ""age"": 20}
];

let selector = jsonpath.selector(jsonObj);
// or as json string 
// let selector = jsonpath.selector(JSON.stringify(jsonObj));

let select1 = selector('$..friends[0]');
let select2 = selector('$..friends[1]');

console.log(
    JSON.stringify(ret1) == JSON.stringify(select1),
    JSON.stringify(ret2) == JSON.stringify(select2)
);

// => true, true
Javascript - allocJson, deallocJson (Webassembly Only)
wasm-bindgen은 Javascript와 Webassembly간 값을 주고받을 때 JSON 객체는 String으로 변환되기 때문에, 반복해서 사용되는 JSON 객체는 Webassembly 영역에 생성해 두면 성능에 도움이 된다.
Since wasm-bindgen converts JSON objects to String when exchanging values between Javascript and Webassembly, creating frequently used JSON objects in the WebAssembly area helps performance.
const jsonpath = require('jsonpath-wasm');

let jsonObj = {
    ""school"": {
        ""friends"": [
            {""name"": ""친구1"", ""age"": 20},
            {""name"": ""친구2"", ""age"": 20}
        ]
    },
    ""friends"": [
        {""name"": ""친구3"", ""age"": 30},
        {""name"": ""친구4""}
    ]
};

// allocate jsonObj in webassembly
let ptr = jsonpath.allocJson(jsonObj);

// `0` is invalid pointer
if(ptr == 0) {
    console.error('invalid ptr'); 
}

let path = '$..friends[0]';
let template = jsonpath.compile(path);
let selector = jsonpath.selector(jsonObj);
// create selector as pointer
let ptrSelector = jsonpath.selector(ptr);

let ret1 = selector(path)
let ret2 = ptrSelector(path)
let ret3 = template(jsonObj);
// select as pointer
let ret4 = template(ptr);
let ret5 = jsonpath.select(jsonObj, path);
// select as pointer
let ret6 = jsonpath.select(ptr, path);

console.log(
    JSON.stringify(ret1) == JSON.stringify(ret2),
    JSON.stringify(ret1) == JSON.stringify(ret3),
    JSON.stringify(ret1) == JSON.stringify(ret4),
    JSON.stringify(ret1) == JSON.stringify(ret5),
    JSON.stringify(ret1) == JSON.stringify(ret6));

// => true true true true true

jsonpath.deallocJson(ptr);
",3
hrbigelow/ae-wavenet,Python,"PyTorch implementation of Jan Chorowski, Jan 2019 paper""
This is a PyTorch implementation of https://arxiv.org/abs/1901.08810.
[Under Construction]
Update April 14, 2019
Began training on Librispeech dev (http://www.openslr.org/resources/12/dev-clean.tar.gz),
see dat/example_train.log
Update May 12, 2019
First runs using vqvae mode.  After ~200 iterations, only one quantized vector is
used as a representative.  Currently troubleshooting.
TODO

VAE and VQVAE versions of the bottleneck / training objectives [DONE]
Inference mode

Example training setup
code_dir=/path/to/ae-wavenet
run_dir=/path/to/my_runs

# Get the data
cd $run_dir
wget http://www.openslr.org/resources/12/dev-clean.tar.gz
tar zxvf dev-clean.tar.gz
$code_dir/scripts/librispeech_to_rdb.sh LibriSpeech/dev-clean > librispeech.dev-clean.rdb 

# Train
cd $code_dir 
python train.py new -af par/arch.basic.json -tf par/train.basic.json -nb 4 -si 10 \
  -rws 100000 -fpu 1.0 $run_dir/model%.ckpt $run_dir/librispeech.dev-clean.10.r1.rdb
",70
sbeltran10/moncrud,JavaScript,"
(WIP) Web app that permits CRUD operations on any Mongo database

Edit operations

TODO: Objects, arrays and dates


TODO: Create operations

",2
nt314p/Zork,Java,"Zork
This is a Zork template. Eventually, it will be a tool that you can use to easily create your own text adventure game!
Visit our Wiki https://github.com/nt314p/Zork/wiki for more information.
Support this project by staring this repository!
",3
jensihnow/AWSPublicIPAddressRanges,None,"AWSPublicIPAddressRanges
Introduction
This repo tracks the changes made to the official AWS IP Address Range list available on https://ip-ranges.amazonaws.com/ip-ranges.json.
Please see official documentation for guidance on: https://docs.aws.amazon.com/.
Content syntax
Please see syntax on the official documentation: https://docs.aws.amazon.com
Usage examples
Since AWS publishes a json file it is easy to filter using tools like jq. AWS published some good examples on https://docs.aws.amazon.com
AWS IP address range notifications
Instead of consuming the json file or this git repo, you can also subscribe to the AWS SNS topic arn:aws:sns:us-east-1:806199016981:AmazonIpSpaceChanged as documented on https://docs.aws.amazon.com
About
As AWS is only keeping the current state it becomes hard to track changes over time. Comparing a saved state with a current state can be done using the publication time, as long as you have a previous file saved.
This repo is updated once a day if updates had been published. This allows to track all changes over time for comparrison.
",2
ScheduleTracker/JSTracking,JavaScript,"JS Change Tracker
Tracks changes in js files through git diffs.
",2
KenanY/twitch-reactions-overlay,JavaScript,"twitch-reactions-overlay
Website that briefly displays emotes from a Twitch chat as they're posted.
Intended to be added as a Browser Source in live streaming software like OBS
Studio, so that the emotes can be shown on stream as an overlay.
Usage
$ npm install
$ cp .env.example .env

# edit `.env`

$ npm run build

Then one can run the server at out/index.js and serve the client at build/.
",2
gnehs/Carter,CSS,"Carter

提醒

記得新增選單，不然 NAV 會壞掉

使用方法

把它下載下來
丟給 Wordpress 說你要安裝主題
更換主題
完成

夜間模式使用方法(2.0 以上)

夜間模式預設在 21~5 點開啟
您可在 header.php 第 37 行找到相關調整選項
目前可調整選項：預設、強制啟用、強制停用
若您要新增切換夜間模式按鈕，請複製下方程式碼並貼上到""自訂 HTML""小工具

<button class=""ts fluid button"" onclick=""NightMode_switchToNightTheme('true')"" id=""nightmode"" data-dark="""">On</button>
翻譯

翻譯文件在  /languages/  
翻譯完成後歡迎推 PR 回來(##

支援的語言

繁體中文(by 棒棒勝)
简体中文(by 棒棒勝)
English(by Google Translate & 棒棒勝)

",14
idebtor/JoyAI,Jupyter Notebook,"JoyAI
""모두를 위한 인공지능의 활용""(GEK10109) 학습 자료실입니다.
세상은 기계학습(머신러닝)으로 인하여 상당히 큰 변화를 맞이하고 있는데, 마치 거대한 파도가 온 세상을 덮치며 세상을 변화시킬 듯합니다. 최근에 자주 듣는 소식 중에 하나만 예로 들자면 “무인화”입니다. 편의점, 주유소, 교통, 유통, 제조, 금융에 이르기까지 상당히 많은 분야에서 일어나고 있는 무인화의 핵심 기술은 기계학습입니다. 기계들이 학습을 통해 지능을 갖추면서 인간 노동을 상당부분 대체하기 시작한 것입니다.  미국의 아마존 고, 중국의 빙고박스, 알리바바의 타오카페 등 거대한 IT기업과 유통 기업들이 폭발적으로 무인 점포를 늘리고 있다는 소식입니다. 예전에는 아마존과 알리바바 직원들이 하루 평균 20 Km를 걸어 다니며 주문된 상품을 찾았다고 하는데, 이제는 물류 로봇 Kiva(Amazon Robotics)가 이 일을 모두 감당합니다.  2000년대 초에 골드만 삭스에 있던 600명의 트레이더들이 인공지능 켄쇼(見性, Kensho, NewYork Times 원문)로 말미암아 결국은 4명으로 감원되었습니다.  우버가 택시업계의 생태계를 완전히 바꾸어 놓은 일을 ""Uber Moments""라고 흔히 부르는데, 이러한 일이 금융, 유통, 교육 등등 각 영역(Domain)에서 이러한 일이 계속 일어날 것입니다. 우리는 이러한 충격을 이끌어 내는 사람이 되어야 할 것입니다.
스탠포드 앤드류 응 교수는 “Artificial Intelligence is the New Electiricy”라고 말합니다.  전기의 발명으로 2차 산업혁명이 있었던 것처럼, 우리 앞에 다가온 4차혁명의 핵심에 있는 것이 바로 인공지능이며, 인공지능의 근간을 이루는 것이 기계학습 곧 딥러닝입니다. 이런 기술이 기존의 산업과 융합하면서 금융, 교육, 의료, 유통, 교통 등 전 산업에 걸쳐 그야말로 파괴적 혁신을 주도하고 있습니다.  우리가 바로 이러한 기술의 핵심을 이제 도전해 보려는 것입니다.
최근에는 다양한 채널을 통해 탁월한 기계학습 강의를 얼마든지 접할 수 있습니다. 하지만 그런 훌륭한 강의도 수학적 지식을 바탕으로 한 이론에만 치중하거나, 복잡한 프로그래밍을 할 수 있는 전공자들이 수강할 수 있는 강의가 대부분입니다. 그러나, 본 과목은 인문사회계열을 포함한 대학교 1학년 학생을 대상으로 파이썬에 대한 기초지식이 없는 학습자를 대상으로 개발된 과정입니다.  무엇보다 학습자는 인공지능, 기계학습과 딥러닝의 원리를 이해하고, 그 원리를 적용할 수 있는 문제를 도출하고 직접 해결하는 역량을 키울 것입니다.
본 과목을 자동차로 비유하자면, 자동차 엔진이나 자동차를 개발하는 과정이 아니라, 딥러닝이라는 자동차를 직접 운전해보는 경험을 맛볼 수 있도록 하는 과정입니다. 자동차 운전을 배우면서, 그 자동차로 무엇을 할 수 있는지 탐색하고, 혹은 운전 자체를 잘 하려면 어떻게 해야 하는지, 혹은 이미 존재하는 다양한 자동차들로 어떤 다양한 일을 할 수 있는지, 혹은 더 좋은 자동차를 만들려면 무엇을 해야 하는지 알 수 있도록 “딥러닝 자동차”를 경험해보도록 하는 과정입니다. 딥러닝으로 할 수 있는 사례들을 찾아서 자신이 직접 실행할 수 있고, 또한 자기 관심분야에 딥러닝을 적용할 수 있는 문제를 발굴하고 도전해 볼 수 있는 학습 능력을 갖추도록 하는 것을 목적으로 본 과목을 진행합니다.
남송리 3번지에서,
전산전자공학부 김영섭 교수
idebtor@gmail.com
(빚진자)<><
",21
XMDS2/homebrew-xmds2,Ruby,"XMDS2
This repository lets you install XMDS2 on MacOS using homebrew.
Installation
Get brew.
Run in a terminal:
brew install xmds2/xmds2/xmds2
Why so many xmds2s?
This tells brew to find xmds2.rb at github.com/xmds2/homebrew-xmds2.
Eventually, the command will be replaced with brew install xmds2 when it's submitted to the main package list.
Updating XMDS2 version
Edit HomebrewFormula/xmds2.rb and replace the tar.gz file with the newest version. Update the SHA256 as well. To do this:
curl -O https://.../xmds-*.tar.gz
openssl sha256 xmds-*.tar.gz
Make sure that all of the dependencies are correct, and update the Pypi links to the latest versions. (These should all point to tar.gz files). Run brew install --verbose --debug xmds2/xmds2/xmds2 to test the new package.
Ensure the installer works on a fresh MacOS install, and ensure brew test works fine.
Create a pull request (here or at the official brew package list.
Information
Project information: xmds.org
Source code: Sourceforge
",3
jkjung-avt/jkjung-avt.github.io,Shell,"JK Jung's blog
https://jkjung-avt.github.io/
My blog is deeply inspired by Andrej Karpathy's work. I write this blog with Jekyll, and I have forked adueck's cayman-blog theme.
My blog is hosted on GitHub Pages. Meanwhile I've also set up Jekyll on my own Ubuntu 14.04 x64 PC for previewing the pages/posts before committing them to GitHub. Below I briefly list the procedure to get Jekyll working locally. For more details, you can refer to the official Jekyll installation documentation.
 ### Installing ruby, version 2.3.3
 $ sudo apt-get install libssl-dev
 $ mkdir -p ~/src
 $ cd ~/src
 $ wget https://cache.ruby-lang.org/pub/ruby/2.3/ruby-2.3.3.tar.gz
 $ tar xzvf ruby-2.3.3.tar.gz
 $ cd ruby-2.3.3/
 $ ./configure
 $ make
 $ sudo make install
 $ ruby --version

 ### Updating gem to the latest version, 2.6.10
 $ sudo gem update --system
 $ gem --version

 ### Installing bundler, version 1.14.3
 $ sudo gem install bundler
 $ bundle --version

 ### I don't explicitly install Jekyll here. Instead, I rely on 'bundle install' in the target direcoty to install the proper version of Jekyll for me.
After checking out jkjung-avt.github.io repository onto my local PC for the first time, I'd run bundle install once to make sure Jekyll (3.3.1) and all required gems for cayman-blog theme are installed properly.
 $ git clone https://github.com/jkjung-avt/jkjung-avt.github.io.git
 $ cd jkjung-avt.github.io/
 $ bundle install
 
 ### Then run the following command to preview the pages/posts
 ### locally @ http://localhost:4000
 $ bundle exec jekyll serve
In addition, I have integrated Disqus onto my blog by referencing Brendan A R Sechter's Development Blog. I've also integrated Google Analytics to track website traffic. My Disqus URL and shortname settings are located in _include/disqus.html, while my Google Analytics tracking code in _include/analytics.html.
JK Jung
jkjung13@gmail.com
",5
CompilandoConocimiento/Reference,TeX," Competitive Programming Reference
Click the image:

",2
yahoo/k8s-athenz-istio-auth,Go,"
A controller that polls Athenz to update the ServiceRole and ServiceRoleBinding Istio custom resources.

K8s-athenz-istio-auth
K8s-athenz-istio-auth is a controller that polls Athenz to update the ServiceRole and ServiceRoleBinding Istio custom
resources; it watches all namespaces and looks up the corresponding Athenz domains associated with them.
Architecture

Table of Contents

Background
Install
Configuration
Usage
Contribute
License

Background
We needed a controller that can dynamically fetch Athenz role / policy mappings and convert them to their corresponding
Istio custom resources, so we built this controller to allow users to define RBAC through Athenz and have it integrate
with the Istio world.
The role name in Athenz specifies the target service in the ServiceRole object and its corresponding HTTP methods are
listed as policies. The services that can access this target service are members of the role and will be listed in the
ServiceRoleBinding.
Example ServiceRole
apiVersion: rbac.istio.io/v1alpha1
kind: ServiceRole
metadata:
  name: backend.domain.details
  namespace: backend-domain
spec:
  rules:
  - methods:
    - GET
    services:
    - details.backend-domain.svc.cluster.local

Example ServiceRoleBinding
apiVersion: rbac.istio.io/v1alpha1
kind: ServiceRoleBinding
metadata:
  name: backend-domain.details
  namespace: backend-domain
spec:
  roleRef:
    kind: ServiceRole
    name: backend-domain.details
  subjects:
  - user: frontend.domain/sa/productpage

Install
Prerequisite
There are a variety of prerequisites required in order to run this controller, they are specified below.

Kubernetes cluster - A running Kubernetes cluster is required with access to the control plane. More
information on how to setup a cluster can be found in the official documentation
here. This controller was developed and tested with the 1.11 release.
Istio - Istio must be fully deployed in the cluster with the ServiceRole and ServiceRoleBinding custom
resources. More information on how to setup Istio can be found here. This
controller was developed and tested with the Istio 1.0.3 release.
Athenz - Athenz must be fully deployed in order for users to be able to define roles and policies in
their domain. More information and setup steps can be found here. The authorization
management service (ZMS) and its apis are primarily used for this controller.
SIA Provider - A service identity agent (SIA) must be running in the Kubernetes cluster in order to provision
X.509 certificates to instances in order to authenticate with Athenz. The approach we currently use in production
can be found here.

Setup
Configuration files which must be applied to run k8s-athenz-istio-auth can be found in the k8s directory.
ServiceAccount
In order to tell SIA which service to provide an X.509 certificate to, a service account must be present. This is required
for the controller to authenticate with ZMS for api calls. Run the following command:
kubectl apply -f k8s/serviceaccount.yaml

or
kubectl create serviceaccount k8s-athenz-istio-auth

ClusterRole and ClusterRoleBinding
This controller requires RBAC to read all namespaces in the cluster and to take various actions on the ServiceRole and
ServiceRoleBindings objects, so make sure you run it in an admin namespace. Run the following commands:
kubectl apply -f k8s/clusterrole.yaml
kubectl apply -f k8s/clusterrolebinding.yaml

Deployment
The deployment for the controller contains three containers: sia init, sia refresh, and the controller itself. Build
a docker image using the Dockerfile and publish to a docker registry. Make sure to replace the docker images inside of
this spec to the ones which are published in your organization. Also, replace the zms url with your instance. Run the
following command in order to deploy:
kubectl apply -f k8s/deployment.yaml

Configuration
K8s-athenz-istio-auth has a variety of parameters that can be configured, they are given below.
Parameters
cert (default: /var/run/athenz/service.cert.pem): path to X.509 certificate file to use for zms authentication
key (default: /var/run/athenz/service.key.pem): path to private key file for zms authentication
zms-url (default: https://zms.url.com): athenz full zms url including api path
poll-interval (default: 1m): controller poll interval

Usage
Once the controller is up and running, a user may go into the Athenz UI and define roles and policies for their
services. For example, if the user has frontend and backend services running, and want to authorize only the frontend
to make http GET requests to the backend, they may run through the following steps.

Create a role in the backends Athenz domain prefixed with service.role. For example:
service.role.backend.domain.backend
Create a policy in the backends Athenz domain with the same name as the role defined above.
Define an assertion which allows the action GET.
Add the frontend service as a member of the role, example: frontend.domain.frontend, in order to authorize it to
make GET requests.

Contribute
Please refer to the contributing.md file for information about how to get involved. We welcome issues, questions, and pull requests.
Maintainers/Contacts
Core Team : omega-core@verizonmedia.com
License
Copyright 2018 Oath Inc. Licensed under the terms of the 3-Clause BSD License.
",2
thegodofwar9732/currency-rate,Java,"Daily Currency Rate Desktop Application
Get the currency exchange rates.
Running the Application
Go to command line
git clone https://github.com/thegodofwar9732/currency-rate.git
This is still working in prgress.
Authors

Ah Hpu Sei
Muhtasim Chowdhury
Esther Sitt
Zhao Wei Wu
Matthew Sterlin

License
This project is licensed under the MIT License - see the LICENSE.md file for details
",3
gmt-china/GMT_docs,Shell,"GMT中文手册


在线阅读 |
PDF下载 |
文档源码 |
GMT中文社区 |
GMT官方网站 |
GMT官方文档
欢迎来到 Generic Mapping Tools 的世界。
本文档是由 GMT 中文社区维护的 GMT 中文手册，既可以作为入门读物，也可以作为日常参考。
文档维护
本文档尚有很多不完善之处，欢迎GMT用户参与到GMT中文手册的维护与更新中。
详情见 维护指南。
许可协议
本作品采用 知识共享署名-非商业性使用 4.0 国际许可协议 。
任何人都可以自由地分享、修改本作品，但必须遵循如下条件：

署名：必须提到原作者，提供指向此许可协议的链接，表明是否有做修改
非商业性使用：不能对本作品进行任何形式的商业性使用

",13
jmiddleton/mybank-poc,CSS,"MyBank Account Aggregator
Third Party Provider or TPP allows to securely access customer’s accounts in order to provide consolidated account information. It aggregates accounts from different banks owned by the customer to manage and control them from one central place.
It also provides analytics to help you stay on track with your spending in just one place. MyBank categories every transaction making it easier to see how your spending affects your planned savings.
Auth0 provides user management and authentication using OpenId Connect Authorization Code Flow. The flow provides information about the end user in the form of an id_token (JSON Web Tokens - JWTs) that verifies the identity of the user and provides basic profile information about the user.
Preview: http://www.mybank.com.ar.s3-website-ap-southeast-2.amazonaws.com

Application architecture
The application is divided into frontend and backend. The frontend is written in Vue.js. Backend is implemented using serverless. Each component of the application is defined as a function and exposed as an API. There are also notification which are consumed by specific functions. The information is stored in AWS DynamoDB.

The application architecture is divided in the following components:
Static Web Hosting
Amazon S3 hosts static web resources including HTML, CSS, JavaScript, and image files which are loaded in the user's browser.
Auth0
Auth0 provides user management and authentication functions to secure the backend API.
RESTful API
The front-end application uses Vue.js to interact with backend API built using Lambda and API Gateway. Asynchronous execution is implemented using Amazon Simple Notification Service.
Persistent Store
DynamoDB provides a persistence layer where data is stored and accessible using Lambda function.
Instalation

Clone repository

git clone https://github.com/jmiddleton/mybank-poc.git
git clone https://github.com/jmiddleton/mybank-serverless.git

Get in the project folder

cd mybank-poc

Install dependencies via npm

npm install

Repeat the steps for mybank-serverless

Quick start - development mode
First start mybank-serverless application
cd mybank-serverless
serverless offline start
Once the server has started an api-key will be generated and displayed in the command line:
i.e.: Key with token: d41d8cd98f00b204e9800998ecf812345
Copy that value to .env.development file before starting mybank-poc web application.
To test the application locally use the following commands:
cd mybank-poc
npm run serve
Production mode
The easiest way to preview production locally is using a Node.js static file server.
npm run build
serve -s dist -l 8888

Please make sure you have configured your environment for production. Create a file in the root folder with the name .env.production.local and add the following variables:

VUE_APP_BASE_URL=http://[YOUR_AWS_LAMBDA_ENDPOINT]:[YOUR_AWS_LAMBDA_PORT]/mybank/v1
VUE_APP_API_KEY=[YOUR_AWS_LAMBDA_API_KEY]

Security
Update the config of Auth0 - Identity Provider in auth_config.json
{
    ""domain"": ""[YOUR AUTH0 DOMAIN i.e. dev-mybank.au.auth0.com]"",
    ""clientId"": ""[YOUR_AUTH0_CLIENT_ID]""
}

Deploy static website to AWS S3
To deploy the static website to AWS S3, just execute:
npm run build
npm run deploy

To configure a AWS S3 Bucket as static website, check the following blog for more details:
https://medium.com/@serverlessguru/deploy-reactjs-app-with-s3-static-hosting-f640cb49d7e6
Test data
Locally run the following json server
cd mybank-poc
json-server --watch data/sample-api-data.json --routes data/routes.json --id accountId --port 4000

",2
circlecloud/TeraWallet,JavaScript,"TeraWallet
开发不易 觉得好用 就给 191864(MiaoWoo) 打点币呗!
泰瑞管家源代码

小程序码

RoadMap

首页模块

 简易通知
 交易所价格查询


仪表盘

 查看流通情况
 查看当前块出币数量
 查看最近价格趋势


账户模块

 绑定手机号(用于非小程序)/微信/支付宝/QQ/百度
 公钥绑定
 公钥/账户 信息查询
 私钥生成
 账户生成
 绑定自定义私钥(暂不确定做不做 安全风险存在)


转账模块

 钱包内互转
 转账白名单
 对外转账
 转账到交易所倒计时


抢红包模块

 基金会账户管理
 红包账户管理
 发红包功能
 抢红包功能


公告模块

 Tera相关新闻
 Tera相关通知
 重大消息通知


通知模块

 价格波动通知
 转账到交易所完成通知


多平台发布

 微信小程序
 百度小程序
 支付宝小程序
 Android APP
 IOS APP


国际化

 基本页面国际化
 标题栏国际化
 TabBar国际化
 多语言

 Chinese Simple
 English





",10
facebook/react-native,JavaScript,"

    React Native
  


Learn once, write anywhere:
  Build mobile apps with React.






















Getting Started
 · 
Learn the Basics
 · 
Showcase
 · 
Contribute
 · 
Community
 · 
Support

React Native brings React's declarative UI framework to iOS and Android. With React Native, you use native UI controls and have full access to the native platform.

Declarative. React makes it painless to create interactive UIs. Declarative views make your code more predictable and easier to debug.
Component-Based. Build encapsulated components that manage their own state, then compose them to make complex UIs.
Developer Velocity. See local changes in seconds. Changes to JavaScript code can be live reloaded without rebuilding the native app.
Portability. Reuse code across iOS, Android, and other platforms.

React Native is developed and supported by many companies and individual core contributors. Find out more in our ecosystem overview.
Contents

Requirements
Building your first React Native app
Documentation
Upgrading
How to Contribute
Code of Conduct
License

📋 Requirements
React Native apps may target iOS 9.0 and Android 4.1 (API 16) or newer. You may use Windows, macOS, or Linux as your development operating system, though building and running iOS apps is limited to macOS. Tools like Expo can be used to work around this.
🎉 Building your first React Native app
Follow the Getting Started guide. The recommended way to install React Native depends on your project. Here you can find short guides for the most common scenarios:

Trying out React Native
Creating a New Application
Adding React Native to an Existing Application

📖 Documentation
The full documentation for React Native can be found on our website.
The React Native documentation discusses components, APIs, and topics that are specific to React Native. For further documentation on the React API that is shared between React Native and React DOM, refer to the React documentation.
The source for the React Native documentation and website is hosted on a separate repo, @facebook/react-native-website.
🚀 Upgrading
Upgrading to new versions of React Native may give you access to more APIs, views, developer tools and other goodies. See the Upgrading Guide for instructions.
React Native releases are discussed in the React Native Community, @react-native-community/react-native-releases.
👏 How to Contribute
The main purpose of this repository is to continue evolving React Native core. We want to make contributing to this project as easy and transparent as possible, and we are grateful to the community for contributing bugfixes and improvements. Read below to learn how you can take part in improving React Native.
Code of Conduct
Facebook has adopted a Code of Conduct that we expect project participants to adhere to.
Please read the full text so that you can understand what actions will and will not be tolerated.
Contributing Guide
Read our Contributing Guide to learn about our development process, how to propose bugfixes and improvements, and how to build and test your changes to React Native.
Open Source Roadmap
You can learn more about our vision for React Native in the Roadmap.
Good First Issues
We have a list of good first issues that contain bugs which have a relatively limited scope. This is a great place to get started, gain experience, and get familiar with our contribution process.
Discussions
Larger discussions and proposals are discussed in @react-native-community/discussions-and-proposals.
📄 License
React Native is MIT licensed, as found in the LICENSE file.
React Native documentation is Creative Commons licensed, as found in the LICENSE-docs file.
",77145
skedge-io/skedge,JavaScript,"skedge
Schedule Appointments, send reminders, gain repeat business, and increase your five star reviews!
Installation
To install the required modules, run the command
$ npm i


To start up your local dev envirement run:
$ npm run dev

Run Test
$ npm t

Serve up development server
$ npm run dev

Contributing
If you wish to leave a contribution, please leave a pull request with as much detail as possible.
",5
node-red/catalogue.nodered.org,HTML,"catalog.nodered.org
Community node module catalogue
",9
JuliaRegistries/TagBot,Go," Julia TagBot


TagBot creates tags and releases for your Julia packages when they're registered, so that your Git tags and GitHub releases are kept in sync with releases you have made on the Julia package registry.
To install the app, click the badge above (enabling for all repositories is recommended).
Afterwards, releases for all of your packages registered with Registrator will be handled automatically.
TagBot does not handle manual registrations.
Usage

Install TagBot and enable it for your package if not already done.
Make a package release using Registrator.
TagBot will automatically tag a GitHub release that matches the package release you just made.

Manually Triggering a Release
If you register a package before enabling TagBot, you can still have a release created retroactively.
To trigger the release, add a comment to your merged registry PR containing the text TagBot tag.
This is also useful when TagBot reports an error.
To include the tag command in a comment without actually triggering a release, include TagBot ignore in your comment.
This should be useful for registry maintainers who want to make recommendations without modifying another repository.
Release Notes
TagBot allows you to write your release notes in the same place that you trigger Registrator, but you don't have to if you're feeling lazy.
When release notes are provided, they are copied into both the Git tag message and the GitHub release.
If you do not write any notes, a changelog is automatically generated from your repository's commit log.
This will appear in the GitHub release, and a link to that release will appear in the Git tag message.

For more information on what TagBot is and isn't, please see the announcement.
",10
Lombiq/Smart-Notifications,C#,"Smart Notifications readme
Orchard CMS module that adds the ability to have closable (can be closed with an X in the corner like windows), fading (fades out in a few seconds) or persistent notifications (will be shown until the user closes them). The standard Notifier service is used so all existing notifications can be changed from site settings (and thus you can e.g. make all appearing notifications closable).
If you want to add closable, fading or persistent notifications from code explicitly just use the extension methods on INotifier added by the module.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/smart-notifications (Mercurial repository)
https://github.com/Lombiq/Smart-Notifications (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",2
capriza/service-now-connector,JavaScript,"Connector for ServiceNow
This connector uses the  ServiceNow REST API.



Getting Started
Follow these instructions to download and configure your ApproveSimple Service-Now connector for development and testing.

Run the following command on your terminal:

npx @capriza/service-now-connector 

The service-now-connector folder is created in the folder from which you ran the command and the service-now-connector is installed in it.
The folder contains config.json, transformer.js and ui-templates.json for the change request,service request and item request use cases.


If you don't have a service-now account, you can create demo here.


Edit systemConfig.json file,  a copy of which is displayed below, replacing the placeholder texts with your instance name and your service-now credentials.


Example of system.json before editing:
 {""instanceName"": ""YOUR_INSTANCE"", ""integrationUser"": { ""username"": ""admin"", ""password"": ""ADMIN_PASSWORD"" }}  

Run the command below to start the Capriza ApproveSimple test tool, the ApproveSimple Inspector:

npm run inspector  

NOTE: If the inspector does not open after running the ""npm run inspector"" command, open http://localhost:8080/ in your browser.
How the connector works
The connector uses 2 types of requests - GET and PUT:
GET requests can be used to fetch a single specific approval request or all open approval requests. Get requests are also used for fetching attachments.
PUT requests are used to take an action such as approve or reject.
Examples of Using GET and PUT Requests
Paragraphs 1, 2 and 3 below illustrate how the service-now connector uses these GET and PUT requests.

The connector uses a request of type GET to retrieve approval request data for a single approval request.
For details click here.

The example below shows how approval request data for a single approval request with with id ""0349403adba52200a6a2b31be0b8f509"" is fetched:
GET: https://dev36661.service-now.com/api/now/table/sysapproval_approver, params = {""sysparm_query"":""sys_id=0349403adba52200a6a2b31be0b8f509^state=requested""}
The next example fetches approval request data for all pending approval requests:
GET  https://dev36661.service-now.com/api/now/table/sysapproval_approver, params = {""sysparm_query"":""state=requested^source_table=change_request"",""sysparm_fields"":""approver.sys_id,approver.name,approver.user_name"",""sysparm_limit"":1000}


GET requests are also used to fetch attachment data. For more details, click here.
The following example of uses a GET request to fetch all the attachments from the sysapproval_approver table (that holds all pending approvals):
GET  https://dev36661.service-now.com/api/now/attachment, params = {""sysparm_query"":""table_name=sysapproval_approver""}


PUT requests are used to modify the table data following approve and reject actions. The service-now connector uses PUT requests to modify the ""state"" key when any action is taken, and the ""comments"" key when a reject reason or comment was provided by the user. For more details click here.


Here is an example of how the Capriza ApproveSimple Service-Now connector updates the table data when the user has taken an approve action on an approval request whose id is ""00e9c07adba52200a6a2b31be0b8f5ae"":
PUT:  https://dev36661.service-now.com/api/now/table/sysapproval_approver/00e9c07adba52200a6a2b31be0b8f5ae, data = {""state"":""approved""}
Config.json (resources/config.json) Parameters
The BlConfig object of Config.json (resources/config.json) defines three mandatory parameters and several optional parameters:



Required
Param Name and Type
Description




Yes
TableName  String
The name of the table from which approvals are fetched e.g: sysapproval_approver, sc_ic_aprvl_type_defn_staging etc. The correct value should be in the file that you download as part of the configuration, and therefore should not be changed. It should be the name of the table in which all the approvals exist.


Yes
useCaseNames Array of Objects
This array of objects contains the use cases your connector will treat, for example: change_request, std_change_proposal, sc_req_item etc. NOTE: .. Each object has two properties: name(String) and sourceTable(Boolean). The name property is required. The default value of sourceTable is true. If sourceTable is true,  the use case uses the approvalsTable value. If sourceTable is false, the use case uses the table whose name is the same as the usecase.name property. So in the example below -.[Example of Using Table other than Approvals Table for a use case ()] -  the sc_request use case uses the sc_request table, while the change_request use case uses the sysapproval_approver table which is the approvalsTable value.


No
additionalTables Array of Objects
This property lets you add extra data from tables that are not related to the main source table (tableName), for example - a conflict table. Note: useCaseLabels are inserted under the each additionalTables key (useCase) in the same manner that useCaseLabels are inserted under the main tableName. You can add several tables under each useCase. for request item variables: if you wants to get the item variables (list of variables), add: tableName: sc_item_option_mtom table, primaryKey: request_item. if the variable type is 'Reference' and you want to fetch the reference table for each item, mark getReferenceTable: true. for specify the reference table field, use referenceTableMap. the default value for case you didn't define the reference table is ""name"". for full example view the config.json


No
additionSysparmQuery String
addition system parm query, for example: additionSysparmQuery: ""^approver.email=david.loo@example.com""


No
userAuthentication Boolean
For making approval actions (approve/reject) via the current AS user credentials. if userAuthentication set to false, then the approval actions is done by the integration user.


No
maxApprovals Integer
Maximum number of approval requests per fetch.


No
delegate Boolean
If true, the connector will check if there are relevant (to current date) delegated approvals in the system. If there are, the connector will duplicate the approvals to the delegated approver.


No
maxCurrentRequests Integer
The maximum number of http: requests that the connector can send at one time. The number should prevent overloading the ServiceNow server. The default value (and recommended value) is 10. If you receive error code 429 from Service-Now, decrease maxCurrentRequests.


No
optimizedFetch Boolean
When true only fetches the changes in the last modified approvals and only the changes in them. By default, the Fetch is optimized. To avoid optimized Fetch, set this parameter to false.


No
fetchApprovalsByManager Boolean
When true, the fetch function fetches only the specific pending approval requests directed to  the users registered in Capriza. When false, the fetch will return all pending approvals, regardless of whether they are directed at ApproveSimple users or not.


No
Pagination Boolean
When true, pending approvals are separated into multi-requests any of which does not exceed the bulksize parameter. When false, all pending approvals are pulled in one http request.


No
authenticationMethod Object
Object that has two properties: type (String) and data (object). type can be: custom/credentials/onbehalf, credentials is the defualt.



Connector actions:
The connector has three methods in order to make actions (approve and reject):

Action on behalf - using the integration user to perform actions.
making approval actions (approve/reject) via the integration user. Note that this requires admin permissions.

Example:
 ""authenticationMethod"": {""type"": ""onbehalf""}

Action using the approver credentials.
making approval actions (approve/reject) via the AS user.

Example:
 ""authenticationMethod"": {""type"": ""credentials""}

Execute script that does the actions for us - this means that the script is already set up in the system. the connector invoke the script.
This data defines how an action script can be executed by remote. Note: To use this parameter, you must create the script and store it in your ServiceNow system.

Example:
 ""authenticationMethod"": {
      ""type"": ""custom"",
      ""data"": {
        ""apiAbsoluteUrlPath"": ""YOUR_FULL_PATH_SCRIPT_URL"",
        ""httpMethod"": ""POST"",
        ""body"": {
          ""approvalRecId"": null,
          ""userId"": null,
          ""comments"": null,
          ""action"": null
        }
      }
    }
Further Reading
For more info about creating Capriza Connectors and the files mentioned above, see: Capriza Connector SDK
For more info about how Capriza Connectors are created and configured, see also Capriza Connector SDK - Connector Creation Steps
For more info about how Capriza Connectors are created and configured, see: Capriza Connector SDK (github) and [Capriza Connector SDK - Connector Creation Steps] (https://docs.approvesimple.com/v1.2/docs/connector-creation-steps-general)
",3
superleexpert/open-source-library-tags,None,"Mobile & Frontend Amazing Library
Please follow the .md
",5
Yuye584312311/iMusic,Java,"iMusic
音乐播放器:Android音乐播放器封装
视频播放器:Android视频播放器封装
基于原生MediaPlayer解码器封装的音乐播放器和视频播放器功能库。极简接入、功能全面、体积甚小。欢迎Star！欢迎下载apk体验！
iMusic示例工程主要界面遵循MVP思想开发，搜索音乐API取自《酷狗音乐》开放API，视频资源API取自《开眼视频》。

功能演示及概述:
功能演示:(更多功能快照在结尾处)

音乐播放器预览(如播放不流畅请点击图片查看)





视频播放器预览




功能概述:
1.音乐播放器

网络音乐播放
本地音乐检索播放
搜索歌手、专辑、歌曲名播放(iMusic)
基本常规操作示例播放器
锁屏播放控制
自定义唱片机、锁屏界面、通知栏
悬浮窗播放
状态栏通知控制
定时关闭播放
播放模式设置
对音乐收藏至本地
最近播放记录浏览
已对音频输出焦点管理作处理


2.视频播放器

支持自定义视频控制器
支持自定义封面控制器
支持自定义手势识别调节器
支持4种画面缩放模式设置
支持界面跳转无缝衔接播放
迷你小窗口播放、支持屏幕中拖拽
全局悬浮窗播放、支持屏幕全局拖拽
全屏播放下手势识别调节音量、屏幕亮度、快进、快退
支持全局悬浮窗播放器中无缝切换至Activity播放界面
列表单例播放
列表横竖屏切换
常规横竖屏切换
已对视频输出焦点管理作处理

历史版本
查看历史版本
一.音乐播放器集成:
1.项目build.gradle中添加
    dependencies {
        implementation 'com.imusic.player:music-player:1.0.3'
    }

若拉取失败，请检查根build.gradle中是否对jcenter支持
2.全局初始化
    //Applicaion中初始化
    MusicPlayerManager.getInstance().init(getApplicationContext());

3.MainActivity中初始化播放器服务组件
    @Override
    protected void onCreate() {
        super.onCreate();
        //初始化内部服务组件
        MusicPlayerManager.getInstance().initialize(MainActivity.this);
    }

    @Override
    protected void onDestroy() {
        super.onDestroy();
        //播放器反初始化
        MusicPlayerManager.getInstance().unInitialize(MainActivity.this);
        //如果你启用了内置悬浮窗口播放器，则还需要对其释放
        MusicWindowManager.getInstance().onDestroy();
    }

4.开始播放你的音频任务
    /**
     * audios:待播放的歌单列表,音频对象需继承BaseAudioInfo类，请阅读类中成员属性注解
     * position：开始播放的位置(位于audios中的index)
     */
    MusicPlayerManager.getInstance().startPlayMusic(List<?> audios,int position);

5.权限声明
    <!--网络状态检查-->
    <uses-permission android:name=""android.permission.ACCESS_NETWORK_STATE"" />
    <!--锁屏下继续缓冲-->
    <uses-permission android:name=""android.permission.WAKE_LOCK""/>

添加混淆
    -keep public class * extends android.app.Service
    -keep public class * extends android.content.BroadcastReceiver
    #java bean
    -keep class com.music.player.lib.bean.**{*;}
    #保持自定义控件类不被混淆
    -keepclasseswithmembers class * {
        public <init>(android.content.Context, android.util.AttributeSet);
    }
    #保持自定义控件类不被混淆
    -keepclasseswithmembers class * {
        public <init>(android.content.Context, android.util.AttributeSet, int);
    }


Demo内置一套完整的UI交互播放器，请注册监听事件MusicPlayerManager.getInstance().addOnPlayerEventListener(this);并参照MusicPlayerActivity集成。

点击查看Music自定义交互及其他功能Wiki

二.视频播放器集成:
1.项目build.gradle中添加
    dependencies {
        implementation 'com.imusic.player:video-player:1.0.1'
    }

2.在你的项目中的.xml中引入播放器布局
    <com.video.player.lib.view.VideoPlayerTrackView
        android:id=""@+id/video_track""
        android:layout_width=""match_parent""
        android:layout_height=""wrap_content""
        app:video_autoSetCoverController=""true""
        app:video_autoSetVideoController=""true""/>

你也可以在java代码中创建播放器，前往[视频播放器Wiki文档]
3.播放器初始化及基本数据设置
    mVideoPlayer = (VideoPlayerTrackView) findViewById(R.id.video_player);
     //播放器控件高度设置，默认是match_parent
    mVideoPlayer.getLayoutParams().height=200dp;
    //开始准备播放
    mVideoPlayer.startPlayVideo(dataSource,title);
    //第二种姿势准备播放
    //mVideoPlayer.setDataSource(dataSource,title,);
    //mVideoPlayer.startPlayVideo();

4.Activity生命周期方法加入
    @Override
    protected void onResume() {
        super.onResume();
        VideoPlayerManager.getInstance().onResume();
    }

    @Override
    protected void onPause() {
        super.onPause();
        VideoPlayerManager.getInstance().onPause();
    }

    @Override
    public void onBackPressed() {
        //尝试弹射返回
        if(VideoPlayerManager.getInstance().isBackPressed()){
            super.onBackPressed();
        }
    }

    @Override
    protected void onDestroy() {
        super.onDestroy();
        VideoPlayerManager.getInstance().onDestroy();
        //如果你的Activity是MainActivity并且你开启过悬浮窗口播放器，则还需要对其释放
        VideoWindowManager.getInstance().onDestroy();
    }

5.权限声明：
    <!--网络状态-->
    <uses-permission android:name=""android.permission.ACCESS_NETWORK_STATE"" />
    <!--锁屏工作，防止休眠-->
    <uses-permission android:name=""android.permission.WAKE_LOCK""/>
    <!--悬浮窗-->
    <uses-permission android:name=""android.permission.SYSTEM_ALERT_WINDOW"" />

6.Activity Manifest文件配置：
    <activity android:name=""xxx.xxx.xxx.MainActivity""
        android:screenOrientation=""portrait""
        <!--在你需要全屏播放的Activity中加上这个属性，告诉系统Activity在横竖屏切换时不要销毁Activity-->
        android:configChanges=""orientation|screenSize"">
    </activity>

至此基础的视频播放器项目集成完毕，更多高级功能和API请阅读文档。
点击查看Video自定义交互和其他功能Wiki文档
iMusic预览及下载:
强烈建议集成前先下载体验此APP，根据APP中的功能对照对应的API集成开发！！
功能快照预览:










下载地址:
fir托管下载： 前往fir下载
或点此直接下载Apk

或者扫描二维码下载





集成中遇到问题请阅读Wiki,BUG提交欢迎issues。如有其他问题，联系邮箱：TinyHung@Outlook.com

",10
JuliaRegistries/TagBot,Go," Julia TagBot


TagBot creates tags and releases for your Julia packages when they're registered, so that your Git tags and GitHub releases are kept in sync with releases you have made on the Julia package registry.
To install the app, click the badge above (enabling for all repositories is recommended).
Afterwards, releases for all of your packages registered with Registrator will be handled automatically.
TagBot does not handle manual registrations.
Usage

Install TagBot and enable it for your package if not already done.
Make a package release using Registrator.
TagBot will automatically tag a GitHub release that matches the package release you just made.

Manually Triggering a Release
If you register a package before enabling TagBot, you can still have a release created retroactively.
To trigger the release, add a comment to your merged registry PR containing the text TagBot tag.
This is also useful when TagBot reports an error.
To include the tag command in a comment without actually triggering a release, include TagBot ignore in your comment.
This should be useful for registry maintainers who want to make recommendations without modifying another repository.
Release Notes
TagBot allows you to write your release notes in the same place that you trigger Registrator, but you don't have to if you're feeling lazy.
When release notes are provided, they are copied into both the Git tag message and the GitHub release.
If you do not write any notes, a changelog is automatically generated from your repository's commit log.
This will appear in the GitHub release, and a link to that release will appear in the Git tag message.

For more information on what TagBot is and isn't, please see the announcement.
",10
Lombiq/Smart-Notifications,C#,"Smart Notifications readme
Orchard CMS module that adds the ability to have closable (can be closed with an X in the corner like windows), fading (fades out in a few seconds) or persistent notifications (will be shown until the user closes them). The standard Notifier service is used so all existing notifications can be changed from site settings (and thus you can e.g. make all appearing notifications closable).
If you want to add closable, fading or persistent notifications from code explicitly just use the extension methods on INotifier added by the module.
The module's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/smart-notifications (Mercurial repository)
https://github.com/Lombiq/Smart-Notifications (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub.
Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",2
capriza/service-now-connector,JavaScript,"Connector for ServiceNow
This connector uses the  ServiceNow REST API.



Getting Started
Follow these instructions to download and configure your ApproveSimple Service-Now connector for development and testing.

Run the following command on your terminal:

npx @capriza/service-now-connector 

The service-now-connector folder is created in the folder from which you ran the command and the service-now-connector is installed in it.
The folder contains config.json, transformer.js and ui-templates.json for the change request,service request and item request use cases.


If you don't have a service-now account, you can create demo here.


Edit systemConfig.json file,  a copy of which is displayed below, replacing the placeholder texts with your instance name and your service-now credentials.


Example of system.json before editing:
 {""instanceName"": ""YOUR_INSTANCE"", ""integrationUser"": { ""username"": ""admin"", ""password"": ""ADMIN_PASSWORD"" }}  

Run the command below to start the Capriza ApproveSimple test tool, the ApproveSimple Inspector:

npm run inspector  

NOTE: If the inspector does not open after running the ""npm run inspector"" command, open http://localhost:8080/ in your browser.
How the connector works
The connector uses 2 types of requests - GET and PUT:
GET requests can be used to fetch a single specific approval request or all open approval requests. Get requests are also used for fetching attachments.
PUT requests are used to take an action such as approve or reject.
Examples of Using GET and PUT Requests
Paragraphs 1, 2 and 3 below illustrate how the service-now connector uses these GET and PUT requests.

The connector uses a request of type GET to retrieve approval request data for a single approval request.
For details click here.

The example below shows how approval request data for a single approval request with with id ""0349403adba52200a6a2b31be0b8f509"" is fetched:
GET: https://dev36661.service-now.com/api/now/table/sysapproval_approver, params = {""sysparm_query"":""sys_id=0349403adba52200a6a2b31be0b8f509^state=requested""}
The next example fetches approval request data for all pending approval requests:
GET  https://dev36661.service-now.com/api/now/table/sysapproval_approver, params = {""sysparm_query"":""state=requested^source_table=change_request"",""sysparm_fields"":""approver.sys_id,approver.name,approver.user_name"",""sysparm_limit"":1000}


GET requests are also used to fetch attachment data. For more details, click here.
The following example of uses a GET request to fetch all the attachments from the sysapproval_approver table (that holds all pending approvals):
GET  https://dev36661.service-now.com/api/now/attachment, params = {""sysparm_query"":""table_name=sysapproval_approver""}


PUT requests are used to modify the table data following approve and reject actions. The service-now connector uses PUT requests to modify the ""state"" key when any action is taken, and the ""comments"" key when a reject reason or comment was provided by the user. For more details click here.


Here is an example of how the Capriza ApproveSimple Service-Now connector updates the table data when the user has taken an approve action on an approval request whose id is ""00e9c07adba52200a6a2b31be0b8f5ae"":
PUT:  https://dev36661.service-now.com/api/now/table/sysapproval_approver/00e9c07adba52200a6a2b31be0b8f5ae, data = {""state"":""approved""}
Config.json (resources/config.json) Parameters
The BlConfig object of Config.json (resources/config.json) defines three mandatory parameters and several optional parameters:



Required
Param Name and Type
Description




Yes
TableName  String
The name of the table from which approvals are fetched e.g: sysapproval_approver, sc_ic_aprvl_type_defn_staging etc. The correct value should be in the file that you download as part of the configuration, and therefore should not be changed. It should be the name of the table in which all the approvals exist.


Yes
useCaseNames Array of Objects
This array of objects contains the use cases your connector will treat, for example: change_request, std_change_proposal, sc_req_item etc. NOTE: .. Each object has two properties: name(String) and sourceTable(Boolean). The name property is required. The default value of sourceTable is true. If sourceTable is true,  the use case uses the approvalsTable value. If sourceTable is false, the use case uses the table whose name is the same as the usecase.name property. So in the example below -.[Example of Using Table other than Approvals Table for a use case ()] -  the sc_request use case uses the sc_request table, while the change_request use case uses the sysapproval_approver table which is the approvalsTable value.


No
additionalTables Array of Objects
This property lets you add extra data from tables that are not related to the main source table (tableName), for example - a conflict table. Note: useCaseLabels are inserted under the each additionalTables key (useCase) in the same manner that useCaseLabels are inserted under the main tableName. You can add several tables under each useCase. for request item variables: if you wants to get the item variables (list of variables), add: tableName: sc_item_option_mtom table, primaryKey: request_item. if the variable type is 'Reference' and you want to fetch the reference table for each item, mark getReferenceTable: true. for specify the reference table field, use referenceTableMap. the default value for case you didn't define the reference table is ""name"". for full example view the config.json


No
additionSysparmQuery String
addition system parm query, for example: additionSysparmQuery: ""^approver.email=david.loo@example.com""


No
userAuthentication Boolean
For making approval actions (approve/reject) via the current AS user credentials. if userAuthentication set to false, then the approval actions is done by the integration user.


No
maxApprovals Integer
Maximum number of approval requests per fetch.


No
delegate Boolean
If true, the connector will check if there are relevant (to current date) delegated approvals in the system. If there are, the connector will duplicate the approvals to the delegated approver.


No
maxCurrentRequests Integer
The maximum number of http: requests that the connector can send at one time. The number should prevent overloading the ServiceNow server. The default value (and recommended value) is 10. If you receive error code 429 from Service-Now, decrease maxCurrentRequests.


No
optimizedFetch Boolean
When true only fetches the changes in the last modified approvals and only the changes in them. By default, the Fetch is optimized. To avoid optimized Fetch, set this parameter to false.


No
fetchApprovalsByManager Boolean
When true, the fetch function fetches only the specific pending approval requests directed to  the users registered in Capriza. When false, the fetch will return all pending approvals, regardless of whether they are directed at ApproveSimple users or not.


No
Pagination Boolean
When true, pending approvals are separated into multi-requests any of which does not exceed the bulksize parameter. When false, all pending approvals are pulled in one http request.


No
authenticationMethod Object
Object that has two properties: type (String) and data (object). type can be: custom/credentials/onbehalf, credentials is the defualt.



Connector actions:
The connector has three methods in order to make actions (approve and reject):

Action on behalf - using the integration user to perform actions.
making approval actions (approve/reject) via the integration user. Note that this requires admin permissions.

Example:
 ""authenticationMethod"": {""type"": ""onbehalf""}

Action using the approver credentials.
making approval actions (approve/reject) via the AS user.

Example:
 ""authenticationMethod"": {""type"": ""credentials""}

Execute script that does the actions for us - this means that the script is already set up in the system. the connector invoke the script.
This data defines how an action script can be executed by remote. Note: To use this parameter, you must create the script and store it in your ServiceNow system.

Example:
 ""authenticationMethod"": {
      ""type"": ""custom"",
      ""data"": {
        ""apiAbsoluteUrlPath"": ""YOUR_FULL_PATH_SCRIPT_URL"",
        ""httpMethod"": ""POST"",
        ""body"": {
          ""approvalRecId"": null,
          ""userId"": null,
          ""comments"": null,
          ""action"": null
        }
      }
    }
Further Reading
For more info about creating Capriza Connectors and the files mentioned above, see: Capriza Connector SDK
For more info about how Capriza Connectors are created and configured, see also Capriza Connector SDK - Connector Creation Steps
For more info about how Capriza Connectors are created and configured, see: Capriza Connector SDK (github) and [Capriza Connector SDK - Connector Creation Steps] (https://docs.approvesimple.com/v1.2/docs/connector-creation-steps-general)
",3
superleexpert/open-source-library-tags,None,"Mobile & Frontend Amazing Library
Please follow the .md
",5
Yuye584312311/iMusic,Java,"iMusic
音乐播放器:Android音乐播放器封装
视频播放器:Android视频播放器封装
基于原生MediaPlayer解码器封装的音乐播放器和视频播放器功能库。极简接入、功能全面、体积甚小。欢迎Star！欢迎下载apk体验！
iMusic示例工程主要界面遵循MVP思想开发，搜索音乐API取自《酷狗音乐》开放API，视频资源API取自《开眼视频》。

功能演示及概述:
功能演示:(更多功能快照在结尾处)

音乐播放器预览(如播放不流畅请点击图片查看)





视频播放器预览




功能概述:
1.音乐播放器

网络音乐播放
本地音乐检索播放
搜索歌手、专辑、歌曲名播放(iMusic)
基本常规操作示例播放器
锁屏播放控制
自定义唱片机、锁屏界面、通知栏
悬浮窗播放
状态栏通知控制
定时关闭播放
播放模式设置
对音乐收藏至本地
最近播放记录浏览
已对音频输出焦点管理作处理


2.视频播放器

支持自定义视频控制器
支持自定义封面控制器
支持自定义手势识别调节器
支持4种画面缩放模式设置
支持界面跳转无缝衔接播放
迷你小窗口播放、支持屏幕中拖拽
全局悬浮窗播放、支持屏幕全局拖拽
全屏播放下手势识别调节音量、屏幕亮度、快进、快退
支持全局悬浮窗播放器中无缝切换至Activity播放界面
列表单例播放
列表横竖屏切换
常规横竖屏切换
已对视频输出焦点管理作处理

历史版本
查看历史版本
一.音乐播放器集成:
1.项目build.gradle中添加
    dependencies {
        implementation 'com.imusic.player:music-player:1.0.3'
    }

若拉取失败，请检查根build.gradle中是否对jcenter支持
2.全局初始化
    //Applicaion中初始化
    MusicPlayerManager.getInstance().init(getApplicationContext());

3.MainActivity中初始化播放器服务组件
    @Override
    protected void onCreate() {
        super.onCreate();
        //初始化内部服务组件
        MusicPlayerManager.getInstance().initialize(MainActivity.this);
    }

    @Override
    protected void onDestroy() {
        super.onDestroy();
        //播放器反初始化
        MusicPlayerManager.getInstance().unInitialize(MainActivity.this);
        //如果你启用了内置悬浮窗口播放器，则还需要对其释放
        MusicWindowManager.getInstance().onDestroy();
    }

4.开始播放你的音频任务
    /**
     * audios:待播放的歌单列表,音频对象需继承BaseAudioInfo类，请阅读类中成员属性注解
     * position：开始播放的位置(位于audios中的index)
     */
    MusicPlayerManager.getInstance().startPlayMusic(List<?> audios,int position);

5.权限声明
    <!--网络状态检查-->
    <uses-permission android:name=""android.permission.ACCESS_NETWORK_STATE"" />
    <!--锁屏下继续缓冲-->
    <uses-permission android:name=""android.permission.WAKE_LOCK""/>

添加混淆
    -keep public class * extends android.app.Service
    -keep public class * extends android.content.BroadcastReceiver
    #java bean
    -keep class com.music.player.lib.bean.**{*;}
    #保持自定义控件类不被混淆
    -keepclasseswithmembers class * {
        public <init>(android.content.Context, android.util.AttributeSet);
    }
    #保持自定义控件类不被混淆
    -keepclasseswithmembers class * {
        public <init>(android.content.Context, android.util.AttributeSet, int);
    }


Demo内置一套完整的UI交互播放器，请注册监听事件MusicPlayerManager.getInstance().addOnPlayerEventListener(this);并参照MusicPlayerActivity集成。

点击查看Music自定义交互及其他功能Wiki

二.视频播放器集成:
1.项目build.gradle中添加
    dependencies {
        implementation 'com.imusic.player:video-player:1.0.1'
    }

2.在你的项目中的.xml中引入播放器布局
    <com.video.player.lib.view.VideoPlayerTrackView
        android:id=""@+id/video_track""
        android:layout_width=""match_parent""
        android:layout_height=""wrap_content""
        app:video_autoSetCoverController=""true""
        app:video_autoSetVideoController=""true""/>

你也可以在java代码中创建播放器，前往[视频播放器Wiki文档]
3.播放器初始化及基本数据设置
    mVideoPlayer = (VideoPlayerTrackView) findViewById(R.id.video_player);
     //播放器控件高度设置，默认是match_parent
    mVideoPlayer.getLayoutParams().height=200dp;
    //开始准备播放
    mVideoPlayer.startPlayVideo(dataSource,title);
    //第二种姿势准备播放
    //mVideoPlayer.setDataSource(dataSource,title,);
    //mVideoPlayer.startPlayVideo();

4.Activity生命周期方法加入
    @Override
    protected void onResume() {
        super.onResume();
        VideoPlayerManager.getInstance().onResume();
    }

    @Override
    protected void onPause() {
        super.onPause();
        VideoPlayerManager.getInstance().onPause();
    }

    @Override
    public void onBackPressed() {
        //尝试弹射返回
        if(VideoPlayerManager.getInstance().isBackPressed()){
            super.onBackPressed();
        }
    }

    @Override
    protected void onDestroy() {
        super.onDestroy();
        VideoPlayerManager.getInstance().onDestroy();
        //如果你的Activity是MainActivity并且你开启过悬浮窗口播放器，则还需要对其释放
        VideoWindowManager.getInstance().onDestroy();
    }

5.权限声明：
    <!--网络状态-->
    <uses-permission android:name=""android.permission.ACCESS_NETWORK_STATE"" />
    <!--锁屏工作，防止休眠-->
    <uses-permission android:name=""android.permission.WAKE_LOCK""/>
    <!--悬浮窗-->
    <uses-permission android:name=""android.permission.SYSTEM_ALERT_WINDOW"" />

6.Activity Manifest文件配置：
    <activity android:name=""xxx.xxx.xxx.MainActivity""
        android:screenOrientation=""portrait""
        <!--在你需要全屏播放的Activity中加上这个属性，告诉系统Activity在横竖屏切换时不要销毁Activity-->
        android:configChanges=""orientation|screenSize"">
    </activity>

至此基础的视频播放器项目集成完毕，更多高级功能和API请阅读文档。
点击查看Video自定义交互和其他功能Wiki文档
iMusic预览及下载:
强烈建议集成前先下载体验此APP，根据APP中的功能对照对应的API集成开发！！
功能快照预览:










下载地址:
fir托管下载： 前往fir下载
或点此直接下载Apk

或者扫描二维码下载





集成中遇到问题请阅读Wiki,BUG提交欢迎issues。如有其他问题，联系邮箱：TinyHung@Outlook.com

",10
jllorencetti/pets,Python,"Pets





Pets is a website where people can publish lost pets
and pets available for adoption.
Users can create an account with a username and password,
or they can use their Twitter and Facebook to login. You can also extend it to use other providers as it's backed by python-social-auth.
Images uploaded by users are cropped with easy-thumbnails to improve the site performance.
Installing
Requirements

Python 3.6 or newer
PostgreSQL running with a database, username and password to be used with Pets.

Fork and clone the repository
First fork the project using GitHub, than clone it locally:
git clone https://github.com/<username>/pets.git
cd pets
Configure your instance
The project configuration uses python-decouple to dynamically read environment variables and .env files.
If you want, you can get started by copying contrib/sample-env as .env:
cp contrib/sample-env pets/.env
Then you have to set following variables:
Basic Django settings

SECRET_KEY: Django's secret key
ALLOWED_HOSTS (e.g. 127.0.0.1, .localhost) Django's allowed hosts
DJANGO_SETTINGS_MODULE: In order to make development and deploy to production simpler there's two settings module; pets.settings.dev for development and pets.settings.prod for production.

Database

DATABASE_URL: (e.g. postgres://username:password@server:port/database_name) Database URL
DB_CONN_MAX_AGE: (e.g. 0) Django's database CONN_MAX_AGE

Email configuration

SENDGRID_API_KEY: API key of you SendGrid account.
DEFAULT_FROM_EMAIL: The email address that will be used as the from email field.

OAuth
If you want to login via social media, you will have to create apps as a developer at Facebook and/or Twitter. Once you're done, set the app secret and app key for each of them:

SOCIAL_AUTH_FACEBOOK_KEY
SOCIAL_AUTH_FACEBOOK_SECRET
SOCIAL_AUTH_TWITTER_KEY
SOCIAL_AUTH_TWITTER_SECRET

Other dependencies
Install Pillow dependencies
As Pets uses Pillow, some extra packages are needed. In a Debian based Linux this should do the job:
sudo apt-get install python-dev python3.x-dev libjpeg8-dev
Download ChromeDriver
You just need to download and unzip the latest ChromeDriver and place it somewhere in your search path.
Install Python packages
You can install the required packages with pipenv.
pipenv install --dev
Test
Execute all tests, it will take some minutes.
cd pets
python manage.py test
Please, do not commit changes if any test fails. Ask for help here instead.
",69
terraform-providers/terraform-provider-huaweicloud,Go,"Terraform HuaweiCloud Provider

Website: https://www.terraform.io

Mailing list: Google Groups


Requirements

Terraform 0.10+
Go 1.11 (to build the provider plugin)

Building The Provider
Clone repository to: $GOPATH/src/github.com/terraform-providers/terraform-provider-huaweicloud
$ mkdir -p $GOPATH/src/github.com/terraform-providers; cd $GOPATH/src/github.com/terraform-providers
$ git clone https://github.com/terraform-providers/terraform-provider-huaweicloud
Enter the provider directory and build the provider
$ cd $GOPATH/src/github.com/terraform-providers/terraform-provider-huaweicloud
$ make build
Exact steps on clean Ubuntu 16.04
# prerequisites are sudo privileges, unzip, make, wget and git.  Use apt install if missing.
$ wget https://storage.googleapis.com/golang/go1.11.1.linux-amd64.tar.gz
$ sudo tar -C /usr/local -xzf go1.11.1.linux-amd64.tar.gz
$ export PATH=$PATH:/usr/local/go/bin # You should put in your .profile or .bashrc
$ go version # to verify it runs and version #
$ go get github.com/terraform-providers/terraform-provider-huaweicloud
$ cd ~/go/src/github.com/terraform-providers/terraform-provider-huaweicloud
$ make build
$ export PATH=$PATH:~/go/bin # You should put in your .profile or .bashrc
$ wget https://releases.hashicorp.com/terraform/0.10.7/terraform_0.10.7_linux_amd64.zip
$ unzip terraform_0.10.7_linux_amd64.zip
$ mv terraform ~/go/bin/
$ terraform version # to verify it runs and version #
$ vi test.tf # paste in Quick Start contents, fix authentication information
$ terraform init
$ terraform plan
$ terraform apply # Should all work if everything is correct.

Quick Start

AK/SK Configuration

# Configure the HuaweiCloud Provider with AK/SK
# This will work with a single defined/default network, otherwise you need to specify network
# to fix errrors about multiple networks found.
provider ""huaweicloud"" {
  tenant_name = ""tenant""
  # the auth url format follows: https://iam.{region_id}.myhwclouds.com:443/v3
  auth_url    = ""https://iam.cn-north-1.myhwclouds.com/v3""
  region      = ""cn-north-1""
  access_key  = ""access key""
  secret_key  = ""secret key""
}

# Create a web server
resource ""huaweicloud_compute_instance_v2"" ""test-server"" {
  name            = ""test-server""
  image_name  = ""Standard_CentOS_7_latest""
  flavor_name = ""s1.medium""
}

Username/password Configuration

# Configure the HuaweiCloud Provider with Username/Password
# This will work with a single defined/default network, otherwise you need to specify network
# to fix errrors about multiple networks found.
provider ""huaweicloud"" {
  user_name   = ""user""
  tenant_name = ""tenant""
  domain_name = ""domain""
  password    = ""pwd""
  # the auth url format follows: https://iam.{region_id}.myhwclouds.com:443/v3
  auth_url    = ""https://iam.cn-north-1.myhwclouds.com:443/v3""
  region      = ""cn-north-1""
}

# Create a web server
resource ""huaweicloud_compute_instance_v2"" ""test-server"" {
  name		  = ""test-server""
  image_name  = ""Standard_CentOS_7_latest""
  flavor_name = ""s1.medium""
}
Full Example

Please see full example at https://github.com/terraform-providers/terraform-provider-huaweicloud/tree/master/examples,
you must fill in the required variables in variables.tf.
Using the provider
Please see the documentation at provider usage.
Or you can browse the documentation within this repo here.
Developing the Provider
If you wish to work on the provider, you'll first need Go installed on your machine (version 1.11+ is required). You'll also need to correctly setup a GOPATH, as well as adding $GOPATH/bin to your $PATH.
To compile the provider, run make build. This will build the provider and put the provider binary in the $GOPATH/bin directory.
$ make build
...
$ $GOPATH/bin/terraform-provider-huaweicloud
...
In order to test the provider, you can simply run make test.
$ make test
In order to run the full suite of Acceptance tests, run make testacc.
Note: Acceptance tests create real resources, and often cost money to run.
$ make testacc
License
Terraform-Provider-Huaweicloud is under the Mozilla Public License 2.0. See the LICENSE file for details.
",7
sarapis/orservices,JavaScript,"This codebase allows you to pull data from this AirTable Template: https://airtable.com/universe/expTMdQFD5r9G6V9Y/open-referral-human-services-template
... into a web app that looks like this: http://orservices.sarapis.org
How does it work? The code pulls data from AirTable via you AirTable API Key and syncs it with a MySQL database. These imports can be triggers as fast as you'd like.
The database is used (and controlled by) a Laravel/PHP app that renders apps using JavaScript frameworks. Our app uses Vue.JS for that but it could just as easily be React.
The result is an online directory app that is responsive, can deliver full search, multi-filter browsing and the mapping, charting and exporting of all data. We built this for Open Referral, but we’d love to adapt it to more use cases and also generalize the features so other people can use it too.
Installation Instructions

Run git clone https://github.com/sarapis/orservices
Create a MySQL database for the project

mysql -u root -p, if using Vagrant: mysql -u homestead -psecret
create database nycconnection;
\q


From the projects root run cp .env.example .env
Configure your .env file
Run composer update from the projects root folder
From the projects root folder run sudo chmod -R 755 ../orservices
From the projects root folder run php artisan key:generate
From the projects root folder run php artisan migrate
From the projects root folder run php artisan db:seed
From the projects root folder run composer dump-autoload
After login in admin panel, try synchronize of Data from Airtable.

",2
cocos2d/cocos2d-x,C++,"
cocos2d-x



Win32
Others









cocos2d-x is a multi-platform framework for building 2d games, interactive books, demos and other graphical applications.
It is based on cocos2d-iphone, but instead of using Objective-C, it uses C++.
It works on iOS, Android, OS X, Windows, Linux and Web platforms.
Cocos2d-x Framework Architecture:

cocos2d-x is:

Fast
Free
Easy to use
Community supported

Git user attention


Clone the repo from GitHub.
  $ git clone https://github.com/cocos2d/cocos2d-x.git



After cloning the repo, please execute download-deps.py to download and install dependencies.
  $ cd cocos2d-x
  cocos2d-x $ python download-deps.py



After running download-deps.py.
  cocos2d-x $ git submodule update --init



Download stable versions

Cocos2d-x stable versions
Cocos2d-JS Lite version

How to start a new game

Download the code from cocos2d download site or clone this repo (instructions above)
Run setup.py
Run the cocos script

Example:
$ cd cocos2d-x
$ ./setup.py
$ source FILE_TO_SAVE_SYSTEM_VARIABLE
$ cocos new MyGame -p com.your_company.mygame -l cpp -d NEW_PROJECTS_DIR
$ cd NEW_PROJECTS_DIR/MyGame

You can also create a JS project or Lua project with -l js or -l lua.
Build and run a new project for Android
Cocos2d-x supports Android Studio. Simple open the proj.android directory from within the Android Studio environment. More information can be found in our documentation.
Build and run a new project for iOS
$ cocos run -p ios

Build and run a new project for OSX
$ cocos run -p mac

Build and run a new project for Linux
If you never run cocos2d-x on Linux, you need to install all dependencies by the
script in cocos2d/build/install-deps-linux.sh
$ cd cocos2d-x/build
$ ./install-deps-linux.sh

Then
$ cd NEW_PROJECTS_DIR/MyGame
$ cocos run -p linux

Run
$ bin/MyGame

Build and run new project for win32
$ cocos run -p win32

Build and run new project for web
Only JS project can be published to web platforms, so you will need to create a JS project first:
$ cocos new -l js WebGame

Then you can run your game in a web browser:
$ cocos run -p web

Or you can publish your game to publish/html5/ folder:
$ cocos run -p web -m release [--advanced]

Using CMake
Cocos2d-x supports CMake, a cross-platform build system. Example usage:
$ cd cocos2d-x
$ mkdir cmake-build && cd cmake-build
$ cmake ..


Detail CMake Guide

Documentations and samples

All Docs in a single place!
Online API Reference Note that Cocos2d-x, Cocos2d-JS and Cocos Creator have different API set
Programmers Guide
Latest Release Note
Changelog

Main features

Scene management (workflow)
Transitions between scenes
Sprites and Sprite Sheets
Effects: Lens, Ripple, Waves, Liquid, etc.
Actions (behaviours):

Transformation Actions: Move, Rotate, Scale, Fade, Tint, etc.
Composable actions: Sequence, Spawn, Repeat, Reverse
Ease Actions: Exp, Sin, Cubic, Elastic, etc.
Misc actions: CallFunc, OrbitCamera, Follow, Tween


Basic menus and buttons
Integrated with physics engines: Box2d and Chipmunk
Particle system
Skeleton Animations: Spine and Armature support
Fonts:

Fast font rendering using Fixed and Variable width fonts
Support for .ttf fonts


Tile Map support: Orthogonal, Isometric and Hexagonal
Parallax scrolling
Motion Streak
Render To Texture
Touch/Accelerometer on mobile devices
Touch/Mouse/Keyboard on desktop
Sound Engine support (CocosDenshion library) based on OpenAL
Integrated Slow motion/Fast forward
Fast and compressed textures: PVR compressed and uncompressed textures, ETC1 compressed textures, and more
Resolution Independent
Language: C++, with Lua and JavaScript bindings
Open Source Commercial Friendly(MIT): Compatible with open and closed source projects
OpenGL ES 2.0 (mobile) / OpenGL 2.1 (desktop) based

Build Requirements

Mac OS X 10.7+, Xcode 8+
or Ubuntu 14.04+, CMake 3.1+
or Windows 7+, VS 2015
Python 2.7.5+(NOT Python 3)
NDK r16+ is required to build Android games
Android Studio 3.0.0+ to build Android games(tested with 3.0.0)
JRE or JDK 1.6+ is required for web publishing

Runtime Requirements

iOS 8.0+ for iPhone / iPad games
Android 3.0.0+ for Android
OS X v10.9+ for Mac games
Windows 7+ for Win games
Modern browsers and IE 9+ for web games

Running Tests
Select the test you want from Xcode Scheme chooser.

Cocos Console

// Enter cpp test folder
cd tests/cpp-tests
// Or enter js test folder
cd tests/js-tests
// Or enter lua test folder
cd tests/lua-tests

// Compile or run test case
cocos compile -p ios|mac|android|win32|win8_1|metro|web -m debug|release
cocos run -p ios|mac|android|win32|win8_1|metro|web -m debug|release


For OS X / iOS

$ cd cocos2d-x/build
$ open cocos2d_tests.xcodeproj


For Linux

$ cd cocos2d-x/build
$ ./install-deps-linux.sh
$ mkdir linux-build
$ cd linux-build
$ cmake ../..

Run Samples
$ bin/Debug/cpp-empty-test/cpp-empty-test
or
$ bin/Debug/lua-empty-test/lua-empty-test


You may meet building errors when building libGLFW.so. It is because libGL.so directs to an error target, you should make it to direct to a correct one. install-deps-linux.sh only has to be run once.


For Windows

Open the cocos2d-x/build/cocos2d-win32.sln

For Android

$ cd cocos2d-x/build
$ python ./android-build.py cpp-empty-test -p 14
$ adb install ../tests/cpp-empty-test/proj.android/bin/CppEmptyTest-debug.apk

Then click item on Android device to run tests. Available value of -p is the API level, cocos2d-x supports from level 14.
Or you can import the project located at tests/cpp-empty-test/proj.android using Android Studio 3.0.0+.
Learning Resources

Programmers Guide
Sonar Systems Videos
Android Fundamentals
Make School Tutorials
Games From Scratch
Cocos2d sample games

Spreading the word!
You can help us spread the word about cocos2d-x! We would surely appreciate it!

Talk about us on Facebook! Our Facebook Page
Tweet, Tweet! Our Twitter
Read our Blog and promote it on your social media.
Become a Regional Coordinator

See what we are planning!
You can see exactly what we are planning to do with the Cocos family of products.

Cocos2d-x roadmap

Where to get help

English Forums
中文社区
Bug Tracker
API Reference.
Latest Release Note
Changelog
IRC. We are in Freenode in the #cocos2d channel
cpp-tests project. This project is our basis for testing. Use this project to
learn how we implement the functionality of the engine. This project is located in
cocos2d-x_root/build.

Contributing to the Project
Cocos2d-x is licensed under the MIT License. We welcome participation!
Did you find a bug? Do you have feature request? Do you want to merge a feature?

contributing to cocos2d-x

Contact us

Forum: http://discuss.cocos2d-x.org
Twitter: http://www.twitter.com/cocos2dx
Weibo: http://t.sina.com.cn/cocos2dx

",13162
rancher/k3s,Go,"k3s - 5 less than k8s
Lightweight Kubernetes.  Easy to install, half the memory, all in a binary less than 40mb.
Great for

Edge
IoT
CI
ARM
Situations where a PhD in k8s clusterology is infeasible

What is this?
k3s is intended to be a fully compliant Kubernetes distribution with the following changes:

Legacy, alpha, non-default features are removed. Hopefully, you shouldn't notice the
stuff that has been removed.
Removed most in-tree plugins (cloud providers and storage plugins) which can be replaced
with out of tree addons.
Add sqlite3 as the default storage mechanism. etcd3 is still available, but not the default.
Wrapped in simple launcher that handles a lot of the complexity of TLS and options.
Minimal to no OS dependencies (just a sane kernel and cgroup mounts needed). k3s packages required
dependencies

containerd
Flannel
CoreDNS
CNI
Host utilities (iptables, socat, etc)



Quick start

Download k3s from latest release, x86_64, armhf, and arm64 are
supported
Run server

sudo k3s server &
# Kubeconfig is written to /etc/rancher/k3s/k3s.yaml
sudo k3s kubectl get node

# On a different node run the below. NODE_TOKEN comes from /var/lib/rancher/k3s/server/node-token
# on your server
sudo k3s agent --server https://myserver:6443 --token ${NODE_TOKEN}

Running server
To run the server just do
k3s server

You should get an output similar to
INFO[2019-01-22T15:16:19.908493986-07:00] Starting k3s dev
INFO[2019-01-22T15:16:19.908934479-07:00] Running kube-apiserver --allow-privileged=true --authorization-mode Node,RBAC --service-account-signing-key-file /var/lib/rancher/k3s/server/tls/service.key --service-cluster-ip-range 10.43.0.0/16 --advertise-port 6445 --advertise-address 127.0.0.1 --insecure-port 0 --secure-port 6444 --bind-address 127.0.0.1 --tls-cert-file /var/lib/rancher/k3s/server/tls/localhost.crt --tls-private-key-file /var/lib/rancher/k3s/server/tls/localhost.key --service-account-key-file /var/lib/rancher/k3s/server/tls/service.key --service-account-issuer k3s --api-audiences unknown --basic-auth-file /var/lib/rancher/k3s/server/cred/passwd --kubelet-client-certificate /var/lib/rancher/k3s/server/tls/token-node.crt --kubelet-client-key /var/lib/rancher/k3s/server/tls/token-node.key
Flag --insecure-port has been deprecated, This flag will be removed in a future version.
INFO[2019-01-22T15:16:20.196766005-07:00] Running kube-scheduler --kubeconfig /var/lib/rancher/k3s/server/cred/kubeconfig-system.yaml --port 0 --secure-port 0 --leader-elect=false
INFO[2019-01-22T15:16:20.196880841-07:00] Running kube-controller-manager --kubeconfig /var/lib/rancher/k3s/server/cred/kubeconfig-system.yaml --service-account-private-key-file /var/lib/rancher/k3s/server/tls/service.key --allocate-node-cidrs --cluster-cidr 10.42.0.0/16 --root-ca-file /var/lib/rancher/k3s/server/tls/token-ca.crt --port 0 --secure-port 0 --leader-elect=false
Flag --port has been deprecated, see --secure-port instead.
INFO[2019-01-22T15:16:20.273441984-07:00] Listening on :6443
INFO[2019-01-22T15:16:20.278383446-07:00] Writing manifest: /var/lib/rancher/k3s/server/manifests/coredns.yaml
INFO[2019-01-22T15:16:20.474454524-07:00] Node token is available at /var/lib/rancher/k3s/server/node-token
INFO[2019-01-22T15:16:20.474471391-07:00] To join node to cluster: k3s agent -s https://10.20.0.3:6443 -t ${NODE_TOKEN}
INFO[2019-01-22T15:16:20.541027133-07:00] Wrote kubeconfig /etc/rancher/k3s/k3s.yaml
INFO[2019-01-22T15:16:20.541049100-07:00] Run: k3s kubectl

The output will probably be much longer as the agent will spew a lot of logs. By default the server
will register itself as a node (run the agent).  It is common and almost required these days
that the control plane be part of the cluster.  To not run the agent by default use the --disable-agent
flag
k3s server --disable-agent

At this point, you can run the agent as a separate process or not run it on this node at all.
If you encounter an error like ""stream server error: listen tcp: lookup some-host on X.X.X.X:53: no such host""
when starting k3s please ensure /etc/hosts contains your current hostname (output of hostname),
set to a 127.x.x.x address. For example:
127.0.1.1	myhost

Joining nodes
When the server starts it creates a file /var/lib/rancher/k3s/server/node-token. Use the contents
of that file as NODE_TOKEN and then run the agent as follows
k3s agent --server https://myserver:6443 --token ${NODE_TOKEN}

That's it.
Accessing cluster from outside
Copy /etc/rancher/k3s/k3s.yaml on your machine located outside the cluster as ~/.kube/config. Then replace
""localhost"" with the IP or name of your k3s server. kubectl can now manage your k3s cluster.
Auto-deploying manifests
Any file found in /var/lib/rancher/k3s/server/manifests will automatically be deployed to
Kubernetes in a manner similar to kubectl apply.
It is also possible to deploy Helm charts. k3s supports a CRD controller for installing charts. A YAML file specification can look as following (example taken from /var/lib/rancher/k3s/server/manifests/traefik.yaml):
apiVersion: k3s.cattle.io/v1
kind: HelmChart
metadata:
  name: traefik
  namespace: kube-system
spec:
  chart: stable/traefik
  set:
    rbac.enabled: ""true""
    ssl.enabled: ""true""
Keep in mind that namespace in your HelmChart resource metadata section should always be kube-system, because k3s deploy controller is configured to watch this namespace for new HelmChart resources. If you want to specify the namespace for the actual helm release, you can do that using targetNamespace key in the spec section:
apiVersion: k3s.cattle.io/v1
kind: HelmChart
metadata:
  name: grafana
  namespace: kube-system
spec:
  chart: stable/grafana
  targetNamespace: monitoring
  set:
    adminPassword: ""NotVerySafePassword""
  valuesContent: |-
    image:
      tag: master
    env:
      GF_EXPLORE_ENABLED: true
    adminUser: admin
    sidecar:
      datasources:
        enabled: true

Also note that besides set you can use valuesContent in the spec section. And it's okay to use both of them.
Building from source
The clone will be much faster on this repo if you do
git clone --depth 1 https://github.com/rancher/k3s.git

This repo includes all of Kubernetes history so --depth 1 will avoid most of that.
For development, you just need go 1.12 and a sane GOPATH.  To compile the binaries run
go build -o k3s
go build -o kubectl ./cmd/kubectl
go build -o hyperkube ./vendor/k8s.io/kubernetes/cmd/hyperkube
This will create the main executable, but it does not include the dependencies like containerd, CNI,
etc.  To run a server and agent with all the dependencies for development run the following
helper scripts
# Server
./scripts/dev-server.sh

# Agent
./scripts/dev-agent.sh
To build the full release binary run make and that will create ./dist/artifacts/k3s
Customizing components
As of v0.3.0 any of the following processes can be customized with extra flags:

kube-apiserver (server)
kube-controller-manager (server)
kube-scheduler (server)
kubelet (agent)
kube-proxy (agent)

Adding extra argument can be done by passing the following flags to server or agent:
--kube-apiserver-arg value
--kube-scheduler-arg value
--kube-controller-arg value
--kubelet-arg value        
--kube-proxy-arg value     

For example to add the following arguments -v=9 and log-file=/tmp/kubeapi.log to the kube-apiserver, you should pass the following:
k3s server --kube-apiserver-arg v=9 --kube-apiserver-arg log-file=/tmp/kubeapi.log

Uninstalling server
If you installed your k3s server with the help of install.sh script from the root directory, you may use the uninstall script generated during installation, which will be created on your server node at /usr/local/bin/k3s-uninstall.sh
Kubernetes source
The source code for Kubernetes is in vendor/ and the location from which that is copied
is in ./vendor.conf.  Go to the referenced repo/tag and you'll find all the patches applied
to upstream Kubernetes.
Open ports / Network security
The server needs port 6443 to be accessible by the nodes.  The nodes need to be able to reach
other nodes over UDP port 8472.  This is used for flannel VXLAN.  If you don't use flannel
and provide your own custom CNI, then 8472 is not needed by k3s. The node should not listen
on any other port.  k3s uses reverse tunneling such that the nodes make outbound connections
to the server and all kubelet traffic runs through that tunnel.
IMPORTANT. The VXLAN port on nodes should not be exposed to the world, it opens up your
cluster network to accessed by anyone.  Run your nodes behind a firewall/security group that
disables access to port 8472.
Server HA
Just don't right now :)  It's currently broken.
Running in Docker (and docker-compose)
I wouldn't be me if I couldn't run my cluster in Docker.  rancher/k3s images are available
to run k3s server and agent from Docker.  A docker-compose.yml is in the root of this repo that
serves as an example of how to run k3s from Docker.  To run from docker-compose from this repo run
docker-compose up --scale node=3
# kubeconfig is written to current dir
kubectl --kubeconfig kubeconfig.yaml get node

NAME           STATUS   ROLES    AGE   VERSION
497278a2d6a2   Ready    <none>   11s   v1.13.2-k3s2
d54c8b17c055   Ready    <none>   11s   v1.13.2-k3s2
db7a5a5a5bdd   Ready    <none>   12s   v1.13.2-k3s2

To run the agent only in Docker use the following docker-compose-agent.yml is in the root of this repo that
serves as an example of how to run k3s agent from Docker. Alternatively the Docker run command can also be used;
sudo docker run -d --tmpfs /run --tmpfs /var/run -e K3S_URL=${SERVER_URL} -e K3S_TOKEN=${NODE_TOKEN} --privileged rancher/k3s:v0.5.0

sudo docker run -d --tmpfs /run --tmpfs /var/run -e K3S_URL=https://k3s.example.com:6443 -e K3S_TOKEN=K13849a67fc385fd3c0fa6133a8649d9e717b0258b3b09c87ffc33dae362c12d8c0::node:2e373dca319a0525745fd8b3d8120d9c --privileged rancher/k3s:v0.5.0

Hyperkube
k3s is bundled in a nice wrapper to remove the majority of the headache of running k8s. If
you don't want that wrapper and just want a smaller k8s distro, the releases includes
the hyperkube binary you can use.  It's then up to you to know how to use hyperkube. If
you want individual binaries you will need to compile them yourself from source
containerd and Docker
k3s includes and defaults to containerd. Why? Because it's just plain better. If you want to
run with Docker first stop and think, ""Really? Do I really want more headache?"" If still
yes then you just need to run the agent with the --docker flag
 k3s agent -s ${SERVER_URL} -t ${NODE_TOKEN} --docker &

k3s will generate config.toml for containerd in /var/lib/rancher/k3s/agent/etc/containerd/config.toml, for advanced customization for this file you can create another file called config.toml.tmpl in the same directory and it will be used instead.
The config.toml.tmpl will be treated as a Golang template file, and the config.Node structure is being passed to the template,the following is an example on how to use the structure to customize the configuration file https://github.com/rancher/k3s/blob/master/pkg/agent/templates/templates.go#L16-L32
systemd
If you are bound by the shackles of systemd (as most of us are), there is a sample unit file
in the root of this repo k3s.service which is as follows
[Unit]
Description=Lightweight Kubernetes
Documentation=https://k3s.io
After=network-online.target

[Service]
Type=notify
EnvironmentFile=/etc/systemd/system/k3s.service.env
ExecStart=/usr/local/bin/k3s server
KillMode=process
Delegate=yes
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
TasksMax=infinity
TimeoutStartSec=0

[Install]
WantedBy=multi-user.target
The k3s install.sh script also provides a convenient way for installing to systemd,
to install the agent and server as a k3s service just run:
curl -sfL https://get.k3s.io | sh -
The install script will attempt to download the latest release, to specify a specific
version for download we can use the INSTALL_K3S_VERSION environment variable, eg:
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh -
To install just the server without an agent we can add a INSTALL_K3S_EXEC
environment variable to the command:
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=""--disable-agent"" sh -
To install just the agent without a server we should pass K3S_URL along with
K3S_TOKEN or K3S_CLUSTER_SECRET, eg:
curl -sfL https://get.k3s.io | K3S_URL=https://example-url:6443 K3S_TOKEN=XXX sh -
The installer can also be run without performing downloads by setting INSTALL_K3S_SKIP_DOWNLOAD=true, eg:
curl -sfL https://github.com/rancher/k3s/releases/download/vX.Y.Z/k3s -o /usr/local/bin/k3s
chmod 0755 /usr/local/bin/k3s

curl -sfL https://get.k3s.io -o install-k3s.sh
chmod 0755 install-k3s.sh

export INSTALL_K3S_SKIP_DOWNLOAD=true
./install-k3s.sh
The full help text for the install script environment variables are as follows:


K3S_*
Environment variables which begin with K3S_ will be preserved for the
systemd service to use. Setting K3S_URL without explicitly setting
a systemd exec command will default the command to ""agent"", and we
enforce that K3S_TOKEN or K3S_CLUSTER_SECRET is also set.


INSTALL_K3S_SKIP_DOWNLOAD
If set to true will not download k3s hash or binary.


INSTALL_K3S_VERSION
Version of k3s to download from github. Will attempt to download the
latest version if not specified.


INSTALL_K3S_BIN_DIR
Directory to install k3s binary, links, and uninstall script to, or use
/usr/local/bin as the default


INSTALL_K3S_SYSTEMD_DIR
Directory to install systemd service and environment files to, or use
/etc/systemd/system as the default


INSTALL_K3S_EXEC or script arguments
Command with flags to use for launching k3s in the systemd service, if
the command is not specified will default to ""agent"" if K3S_URL is set
or ""server"" if not. The final systemd command resolves to a combination
of EXEC and script args ($@).
The following commands result in the same behavior:
curl ... | INSTALL_K3S_EXEC=""--disable-agent"" sh -s -
curl ... | INSTALL_K3S_EXEC=""server --disable-agent"" sh -s -
curl ... | INSTALL_K3S_EXEC=""server"" sh -s - --disable-agent
curl ... | sh -s - server --disable-agent
curl ... | sh -s - --disable-agent


INSTALL_K3S_NAME
Name of systemd service to create, will default from the k3s exec command
if not specified. If specified the name will be prefixed with 'k3s-'.


INSTALL_K3S_TYPE
Type of systemd service to create, will default from the k3s exec command
if not specified.


openrc on Alpine Linux
In order to pre-setup Alpine Linux you have to go through the following steps:
echo ""cgroup /sys/fs/cgroup cgroup defaults 0 0"" >> /etc/fstab

cat >> /etc/cgconfig.conf <<EOF
mount {
cpuacct = /cgroup/cpuacct;
memory = /cgroup/memory;
devices = /cgroup/devices;
freezer = /cgroup/freezer;
net_cls = /cgroup/net_cls;
blkio = /cgroup/blkio;
cpuset = /cgroup/cpuset;
cpu = /cgroup/cpu;
}
EOF
Then update /etc/update-extlinux.conf by adding:
default_kernel_opts=""...  cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory""

Than update the config and reboot
update-extlinux
reboot
After rebooting:

download k3s to /usr/local/bin/k3s
create an openrc file in /etc/init.d

For the server:
#!/sbin/openrc-run

command=/usr/local/bin/k3s
command_args=""server""
pidfile=

name=""k3s""
description=""Lightweight Kubernetes""
For the agent:
#!/sbin/openrc-run

command=/usr/local/bin/k3s
command_args=""agent --server https://myserver:6443 --token ${NODE_TOKEN}""
pidfile=

name=""k3s""
description=""Lightweight Kubernetes""
Flannel
Flannel is included by default, if you don't want flannel then run the agent with --no-flannel as follows
 k3s agent -u ${SERVER_URL} -t ${NODE_TOKEN} --no-flannel &

In this setup you will still be required to install your own CNI driver.  More info here
CoreDNS
CoreDNS is deployed on start of the agent, to disable add --no-deploy coredns to the server
 k3s server --no-deploy coredns

If you don't install CoreDNS you will need to install a cluster DNS provider yourself.
Traefik
Traefik is deployed by default when starting the server; to disable it, start the server with --no-deploy traefik like this
 k3s server --no-deploy traefik

Service load balancer
k3s includes a basic service load balancer that uses available host ports.  If you try to create
a load balancer that listens on port 80, for example, it will try to find a free host in the cluster
for port 80.  If no port is available the load balancer will stay in Pending.
To disable the embedded service load balancer (if you wish to use a different implementation like
MetalLB) just add --no-deploy=servicelb to the server on startup.
Air-Gap Support
k3s supports pre-loading of containerd images by placing them in the images directory for the agent before starting, eg:
sudo mkdir -p /var/lib/rancher/k3s/agent/images/
sudo cp ./k3s-airgap-images-$ARCH.tar /var/lib/rancher/k3s/agent/images/
Images needed for a base install are provided through the releases page, additional images can be created with the docker save command.
Offline Helm charts are served from the /var/lib/rancher/k3s/server/static directory, and Helm chart manifests may reference the static files with a %{KUBERNETES_API}% templated variable. For example, the default traefik manifest chart installs from https://%{KUBERNETES_API}%/static/charts/traefik-X.Y.Z.tgz.
If networking is completely disabled k3s may not be able to start (ie ethernet unplugged or wifi disconnected), in which case it may be necessary to add a default route. For example:
sudo ip -c address add 192.168.123.123/24 dev eno1
sudo ip route add default via 192.168.123.1
k3s additionally provides a --resolv-conf flag for kubelets, which may help with configuring DNS in air-gap networks.
Rootless - (Some advanced magic, user beware)
Initial rootless support has been added but there are a series of significant usability issues surrounding it.
We are releasing the initial support for those interested in rootless and hopefully some people can help to
improve the usability.  First ensure you have proper setup and support for user namespaces.  Refer to the
requirements section in rootlesskit for instructions.
In short, latest Ubuntu is your best bet for this to work.
Issues w/ Rootless
When running rootless a new network namespace is created.  This means that k3s instance is running with networking
fairly detached from the host.  The only way to access services run in k3s from the host is to setup port forwards
to the k3s network namespace.  We have a controller that will automatically bind 6443 and any service port to the
host with an offset of 10000.  That means service port 80 will become 10080 on the host.  Once you kill k3s and then
start a new instance of k3s it will create a new network namespace, but it doesn't kill the old pods.  So you are left
with a fairly broken setup.  This is the main issue at the moment, how to deal with the network namespace.
Running w/ Rootless
Just add --rootless flag to either server or agent.  So run k3s server --rootless and then look for the message
Wrote kubeconfig [SOME PATH] for where your kubeconfig to access you cluster is.  Becareful, if you use -o to write
the kubeconfig to a different directory it will probably not work.  This is because the k3s instance in running in a different
mount namespace.
Upgrades
To upgrade k3s from older version you can either run the installation script if you already ran k3s using the installation script, this can be done using:
curl -sfL https://get.k3s.io | sh -
If you want to upgrade to specific version you can run the following command:
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh -
Upgrades for openrc
To upgrade with openrc you can download newer version of k3s from latest release and replace the binary in /usr/local/bin/k3s and then restart the service:
service k3s restart
TODO
Currently broken or stuff that needs to be done for this to be considered production quality.

Metrics API (fixed: use k3s server --kubelet-arg=""address=0.0.0.0"" and apply recipes/metrics-server)
HA
Work on e2e, sonobouy.
etcd doesn't actually work because args aren't exposed

",6971
zhreshold/decord,C++,"Decode(WIP)

Decord is a reverse procedure of Record. It provides convenient video slicing methods based on a thin wrapper on top of hardware accelerated video decoders, e.g.

FFMPEG/LibAV(Done)
Nvidia Codecs(Done)
Intel Codecs

Decord was designed to handle awkward video shuffling experience in order to provide smooth experiences similar to random image loader for deep learning.
Bridges for deep learning frameworks:

Apache MXNet (on going)

Installation
Install via pip
TODO
Install from source
Linux
Install the system packages for building the shared library, for Debian/Ubuntu users, run:
# official PPA comes with ffmpeg 2.8, which lacks tons of features, we use ffmpeg 4.0 here
sudo add-apt-repository ppa:jonathonf/ffmpeg-4
sudo apt-get update
sudo apt-get install -y build-essential python3-dev python3-setuptools make cmake 
libavcodec-dev libavfilter-dev libavformat-dev libavutil-dev
# note: make sure you have cmake 3.8 or later, you can install from cmake official website if it's too old
Clone the repo recursively(important)
git clone --recursive https://github.com/zhreshold/decord
Build the shared library in source root directory, you can specify -DUSE_CUDA=1 to enable NVDEC hardware accelerated decoding:
cd decord
mkdir build && cd build
cmake .. -DUSE_CUDA=0
make
Install python bindings:
cd ../python
# option 1: add python path to $PYTHONPATH, you will need to install numpy separately
pwd=$PWD
echo ""PYTHONPATH=$PYTHONPATH:$pwd"" >> ~/.bashrc
source ~/.bashrc
# option 2: install with setuptools
python3 setup.py install --user
Mac OS
Installation on macOS is similar to Linux. But macOS users need to install building tools like clang, GNU Make, cmake first.
Tools like clang and GNU Make are packaged in Command Line Tools for macOS. To install:
xcode-select --install
To install other needed packages like cmake, we recommend first installing Homebrew, which is a popular package manager for macOS. Detailed instructions can be found on its homepage.
After installation of Homebrew, install cmake by:
brew install cmake
# note: make sure you have cmake 3.8 or later, you can install from cmake official website if it's too old
Clone the repo recursively(important)
git clone --recursive https://github.com/zhreshold/decord
Then go to root directory build shared library:
cd decord
mkdir build && cd build
cmake ..
make
Install python bindings:
cd ../python
# option 1: add python path to $PYTHONPATH, you will need to install numpy separately
pwd=$PWD
echo ""PYTHONPATH=$PYTHONPATH:$pwd"" >> ~/.bash_profile
source ~/.bash_profile
# option 2: install with setuptools
python3 setup.py install --user
Windows
For windows, you will need CMake and Visual Studio for C++ compilation.

First, install git, cmake, ffmpeg and python. You can use Chocolatey to manage packages similar to Linux/Mac OS.
Second, install Visual Studio 2017 Community, this my take some time.

When dependencies are ready, open command line prompt:
cd your-workspace
git clone --recursive https://github.com/zhreshold/decord
cd decord
mkdir build
cd build
cmake -DCMAKE_CXX_FLAGS=""/DDECORD_EXPORTS"" -DCMAKE_CONFIGURATION_TYPES=""Release"" -G ""Visual Studio 15 2017 Win64"" ..
# open `decord.sln` and build project
Usage
Decord provides minimal API set for bootstraping. You can also check out jupyter notebook examples.
VideoReader
VideoReader is used to access frames directly from video files.
from decord import VideoReader
from decord import cpu, gpu

reader = VideoReader('xxx.mp4', ctx=cpu(0))
print('video frames:', len(reader))
batch = vr.next()
print('frame shape:', batch.shape)
print('numpy frames:', batch.asnumpy())

# skip 100 frames
vr.skip_frames(1000)
# seek to start
vr.seek(0)

VideoLoader
VideoLoader is designed for training deep learning models with tons of video files.
It provides smart video shuffle techniques in order to provide high random access performance (We know that seeking in video is super slow and redundant).
The optimizations are underlying in the C++ code, which are invisible to user.
from decord import VideoLoader
from decord import cpu, gpu

vl = VideoLoader(['1.mp4', '2.avi', '3.mpeg'], ctx=[cpu(0)], shape=(2, 320, 240, 3), interval=1, skip=5, shuffle=1)
print('Total batches:', len(vl))

for batch in vl:
    print(batch.shape)
Shuffling video can be tricky, thus we provide various modes:
shuffle = -1  # smart shuffle mode, based on video properties, (not implemented yet)
shuffle = 0  # all sequential, no seeking, following initial filename order
shuffle = 1  # random filename order, no random access for each video, very efficient
shuffle = 2  # random order
shuffle = 3  # random frame access in each video only
Preliminary Benchmarks



Setting
OpenCV VideoCapture
NVVL
Decord




CPU sequential read
1.0x
-
1.1x


CPU random acess(no accurate seek)
0.08x
-
0.23x


CPU random acess (accurate seek)
-

0.06x


GPU sequential
-
TODO
TODO


GPU random acess
-
TODO
TODO



",39
theoldmoon0602/ShellGeiBot,Shell,"ShellGeiBot
Build Docker image
$ ./build.bash shellgeibot:latest
Test Docker image
$ docker container run --rm \
  -v $(pwd):/root/src \
  shellgeibot:latest \
  /bin/bash -c ""apt update && apt install -y bats && bats /root/src/docker_image.bats""
",38
AndrewYinLi/baseball-cli,Python,"baseball-cli
Work in progress
",2
dancwilliams/homeassistant-config,Shell,"Dan's Home Assistant Config
I am new to all of this so please excuse anything that looks crazy.  If you have any feedback feel free to open an issue or hit me up on Twitter @dancwilliams.
Basic Automation Flow
Most of my automation revolves around the group.all_device status and time/day.  There is also a ""Good Morning"" and ""Good Night"" routine to handle some A/C settings and the like.
Input Selectors
There are two input sliders used to store the state of the house and the state of the air conditioning.  This is due to the fact the home state does not necessarily reflect the state of the A/C in the main house.  And soon there will be an Ecobee device in the guest house that will also need to be controlled separately.
home_state
This input selector tracks the state of the home and has three states to choose from (home, away, sleep).  These states are used to drive automations based on whole house activity.
main_house_thermostat_mode
This input selector tracks the state of the thermostat in the main house.  This has four options currently (home, away, sleep, smart1).  Some of these are self explanatory, but smart1 is random.  This is due to the way Ecobee handles custom comfort settngs.  The vanity name for this mode is ""Home Weekend"" in the Ecobee panel, but the API knows it as smart1.  These states are controlled separately from the home_state as they are dependent on the time & day.
MORE DOCUMENTATION TO COME
",2
ff4j/ff4j-spring-boot-starter-parent,Java,"



Spring boot starter for FF4J (Feature Flipping for Java)

   
Swagger Documentation
This project aims in providing a bootable starter which provides RESTful apis for FF4J.
Create a bootable jar with
mvn clean install
Add dependency in your project
<dependency>
    <groupId>org.ff4j</groupId>
    <artifactId>ff4j-spring-boot-starter</artifactId>
    <version>1.8</version>
</dependency>
Sample
A sample project is located at ff4j-spring-boot-sample
Use mvn spring-boot:run
Once the sample application is booted use the following curl command:
curl -i -H ""Accept: application/json"" -H ""Content-Type: application/json"" -X GET http://localhost:8080/ff4j
Have a look at FF4JConfiguration
What is FF4J ?
FF4J is a proposition of Feature Toggle.
Features can be enabled or disabled through configuration at runtime with dedicated consoles, Web API, or monitor features usage. The same web console can also define any Property and change its value at runtime.
More information can be found at ff4j.org or the reference guide. To access a demo please click [here] (http://cannys.com/ff4j-demo)






",8
li-shuaishuai/react-template,JavaScript,"{{name}}
Project setup
npm install

Compiles and hot-reloads for development
npm run start

Compiles and minifies for production
npm run build

Visualize size of webpack output files with an interactive zoomable treemap
npm run analyz

",3
OfficeDev/office-ui-fabric-react,TypeScript,"Office UI Fabric React
The React-based front-end framework for building experiences for Office and Office 365.


Fabric React is a collection of robust React-based components designed to make it simple for you to create consistent web experiences using the Office Design Language.
Fabric 7 (the next major version of Fabric) is under development. Roadmap, breaking changes, and more details available in the wiki.
Who uses UI Fabric?

+ 45 additional Microsoft sites and products
For more information...
Please see the wiki.
Contents

Using Fabric React

Version policy
Browser support
Right-to-left support
Server-side rendering
Advanced usage


Contribute to Fabric React
Building the repo

Testing
Advanced building tips


Licenses
Changelog

Using Fabric React
Here is a step-by-step tutorial on how to build a simple React app with office-ui-fabric-react components.
How to integrate components into your project depends heavily on your setup. The recommended setup is to use a bundler such as Webpack which can resolve NPM package imports in your code and bundle only the specific things you import.
Within an npm project, you should install the package and save it as a dependency:
npm install --save office-ui-fabric-react

This will add the package as a dependency in your package.json file and download it under node_modules/office-ui-fabric-react.
The library includes ES2015 module entry points under the lib folder (use lib-amd if you need AMD, or lib-commonjs if you need commonjs). To use a control, import it and then use it in your render method:
import * as React from 'react';
import * as ReactDOM from 'react-dom';
import { PrimaryButton } from 'office-ui-fabric-react/lib/Button';

ReactDOM.render(<PrimaryButton>I am a button.</PrimaryButton>, document.body.firstChild);
Version policy
Fabric React adheres to semantic versioning. However, we only consider constructs directly importable at the package level or from files at the root (e.g. office-ui-fabric-react/lib/Utilities or office-ui-fabric-react/lib-amd/Styling) to be part of our API surface. Everything else is considered package-internal and may be subject to changes, moves, renames, etc.
Browser support
Fabric React supports all evergreen browsers, with IE 11 as the min-bar version of Internet Explorer. See the browser support doc for more information.
Right-to-left support
All components can render in LTR or RTL, depending on the dir attribute set on the html element (dir=""rtl"" will flip the direction of everything). You can also use the setRTL API if you don't have control over the html element's rendering. Example:
import { setRTL } from 'office-ui-fabric-react/lib/Utilities';

setRTL(true);
Server-side rendering
Fabric components can be rendered in a server-side Node environment (or used in tests which run in an SSR-like environment), but it requires customizing how styles and SCSS files are loaded. See the server-side rendering documentation for examples of how to handle this.
Advanced usage
For info about advanced usage including module- vs. path-based imports, using an AMD bundler like RequireJS, and deployment features, see our advanced documentation.
Contribute to Fabric React
Please take a look at our contribution guidelines for more info. Also read Contribute bug fixes and Contribute new component.
Building the repo
Before you get started, make sure you have read the Git branch setup instructions
To view the documentation including examples, contracts, component status, and to add functionality or fix issues locally, you can:

git clone https://github.com/OfficeDev/office-ui-fabric-react.git
cd office-ui-fabric-react
npm install
npm start

This will start a demo page from the office-ui-fabric-react package folder, which will open a web browser with the example page. You can make changes to the code which will automatically build and refresh the page using live-reload.
To build and run tests for all packages in the repo, run npm run build from the root.
To build individual packages within the packages/* or apps/* folders, cd to the relevant folder and run npm run build. Note that because the packages are symlinked together, you must manage building dependencies in the right order, or use the rush tool to build to the specific package you want. (See advanced tips below.)
Testing
For info about testing, see our testing documentation.
Advanced building tips
The repo contains many packages, each which may have dependencies on each other. You can use Rush to build projects in the correct order, if you have it globally installed.
npm install -g @microsoft/rush
To use Rush to build, you can run rush build, which will incrementally build the entire repo (only build what has changed since the last build). If you don't have Rush globally installed, you can use the command npm run buildfast, which abstracts rush build.
To build up to a specific project, use the --to <package> argument. For example, to build up to office-ui-fabric-react, run:
rush build --to office-ui-fabric-react
Licenses
All files on the Office UI Fabric React GitHub repository are subject to the MIT license. Please read the License file at the root of the project.
Usage of the fonts and icons referenced in Office UI Fabric is subject to the terms of the assets license agreement.
Changelog
We use GitHub Releases to manage our releases, including the changelog between every release. View a complete list of additions, fixes, and changes on the releases page.

This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.
",4827
jjuraska/slug2slug,Python,"E2E NLG
The E2E NLG module leverages the seq2seq framework for end-to-end natural language generation from meaning representations (MRs). This is a work in progress...

USAGE
In the e2e_nlg folder, optionally put your input files in the data folder. Run main.py in one of the following ways to run the training, or the evaluation (only after the training has been run):
python main.py --train [path_to_trainset] [path_to_devset]
python main.py --test [path_to_testset]

Replace [path_to_trainset], [path_to_devset], [path_to_testset] with relative paths to your trainset, devset, or testset, respectively. They are expected to be CSV files with two columns (their headers must be mr and ref, respectively), the first containing the MRs, and the second containing the corresponding reference utterances.
Once the training is done, the model folder will contain files describing the model, which will be used for evaluation. Therefore, you are not to modify them.
Finally, the evaluation produces output files in the predictions folder. The predictions.txt file contains raw results, while the predictions_final.txt is produced during the postprocessing step.

REQUIREMENTS

Python libraries: tensorflow, numpy, pandas, nltk, networkx
NLTK modules: perluniprops, punkt

install using the following command: python -c ""import nltk; nltk.download('[module_name]')""



",3
gfw-breaker/jiangfeng-subtitles,Shell,"《江峰时刻》节目字幕

友情链接：免翻墙看禁闻  |  手把手翻墙教程  | 《江峰时刻》会员网站



视频节目名称
视频/音频
简体字幕
正體字幕




歷史上的今天20190517第349期 - 梧桐樹協議
下载
下载
下載


歷史上的今天20190516第348期 - 抗日英雄張靈甫
下载
下载
下載


歷史上的今天20190515第347期 - 埃菲尔铁塔
下载
下载
下載


歷史上的今天特刊 - 解讀《白玫瑰傳單一》
下载
下载
下載


歷史上的今天20190514第346期 - 紅都女皇江青
下载
下载
下載


《週末漫談》20190512第24期
下载
下载
下載


《週末漫談》20190511第23期
下载
下载
下載


歷史上的今天20190510第345期 - 曼德拉
下载
下载
下載


歷史上的今天20190509第344期 - 永不凋零的白玫瑰
下载
下载
下載


歷史上的今天20190508第343期 - 美國轟炸中國駐南斯拉夫大使館
下载
下载
下載


歷史上的今天20190507第342期 - 報道水門事件
下载
下载
下載


歷史上的今天20190506第341期 - 《排華法案》
下载
下载
下載


《週末漫談》20190505第22期
下载
下载
下載


歷史上的今天20190503第340期 - 世界新闻自由日
下载
下载
下載


歷史上的今天20190502第339期 - 德国居民参观纳粹集中营
下载
下载
下載


歷史上的今天20190501第338期 - 擊斃本拉登
下载
下载
下載


歷史上的今天20190430第337期 - 路易斯安那
下载
下载
下載


歷史上的今天20190429第336期 - 盧作孚
下载
下载
下載


《週末漫談》20190428第21期
下载
下载
下載


歷史上的今天20190426第335期 - 切爾諾貝利
下载
下载
下載


歷史上的今天20190425第334期 - 425和平上访
下载
下载
下載


歷史上的今天20190424第333期 - SARS
下载
下载
下載


歷史上的今天20190423第332期 - 武訓
下载
下载
下載


歷史上的今天20190422第331期 - 林巧稚
下载
下载
下載


週末漫談》第20期20190421
下载
下载
下載


《中央情報局的紅色鼴鼠》（十二）他還活著
下载
下载
下載


历史上的今天20190419第330期 - 全民打麻雀
下载
下载
下載


歷史上的今天20190418第329期 - 紅色高棉
下载
下载
下載


歷史上的今天20190417第328期 - 富蘭克林
下载
下载
下載


歷史上的今天20190416第327期 - 列寧
下载
下载
下載


歷史上的今天20190415第326期 - 中美合作所
下载
下载
下載


《週末漫談》第19期20190414
下载
下载
下載


《中央情報局的紅色鼴鼠》（十一）風雲際會
下载
下载
下載


歷史上的今天20190412第325期 - 容閎
下载
下载
下載


歷史上的今天20190411第324期 - 愛麗絲島
下载
下载
下載


歷史上的今天20190410第323期 - 朱令中毒案
下载
下载
下載


歷史上的今天20190409第322期 - 波音
下载
下载
下載


歷史上的今天20190408第321期 - 李敦白
下载
下载
下載


《中央情報局的紅色鼴鼠》（十）紅色基因
下载
下载
下載


歷史上的今天20190405第320期 - 麥肯阿瑟
下载
下载
下載


歷史上的今天20190404第319期 - 現代奧運會
下载
下载
下載


歷史上的今天20190403第318期 - 馬歇爾計劃
下载
下载
下載


歷史上的今天20190402第317期 - 世界自閉症關注日
下载
下载
下載


歷史上的今天20190401第316期 - 中美南海撞機
下载
下载
下載


《週末漫談》20190330第18期
下载
下载
下載


《中央情報局的紅色鼴鼠》（九） 顯赫家族
下载
下载
下載


歷史上的今天20190329第315期 - 俄羅斯反華
下载
下载
下載


歷史上的今天20190328第314期 - 艾森豪威爾
下载
下载
下載


歷史上的今天20190327第313期 - 赫鲁晓夫
下载
下载
下載


歷史上的今天20190326第312期 - 鎮反
下载
下载
下載


歷史上的今天20190325第311期 - 麥大志間諜案
下载
下载
下載


《週末漫談》20190323第17期
下载
下载
下載


《中央情報局的紅色鼴鼠》（八）卸磨殺驢
下载
下载
下載


歷史上的今天20190322第310期 - 《鞍鋼憲法》
下载
下载
下載


歷史上的今天20190321第309期 - 最後的日本武士
下载
下载
下載


歷史上的今天20190320第308期 - 孫志剛案
下载
下载
下載


歷史上的今天20190319第307期 - 《歐陽海之歌》
下载
下载
下載


歷史上的今天20190318第306期 - 巴黎公社
下载
下载
下載


《週末漫談》20190316第16期
下载
下载
下載


《中央情報局的紅色鼴鼠》（七）东窗事发
下载
下载
下載


歷史上的今天20190315第305期 - 消費者權益日
下载
下载
下載


歷史上的今天20190314第304期 - 愛因斯坦
下载
下载
下載


歷史上的今天20190313第303期 - 哈佛大学
下载
下载
下載


歷史上的今天20190312第302期 - 可口可樂
下载
下载
下載


歷史上的今天20190311第301期 - 高智晟和他的家
下载
下载
下載


《週末漫談》20190309第15期
下载
下载
下載


《中央情報局的紅色鼴鼠》（六）諜影重重
下载
下载
下載


歷史上的今天20190308第300期 - 三八婦女節
下载
下载
下載


歷史上的今天20190307第299期 - 亨利八世宗教改革
下载
下载
下載


歷史上的今天20190306第298期 - 斯維特蘭娜
下载
下载
下載


歷史上的今天20190305第297期 - 卡廷慘案
下载
下载
下載


歷史上的今天20190304第296期 - 黑船事件
下载
下载
下載


《週末漫談》20190303第14期
下载
下载
下載


《中央情報局的紅色鼴鼠》（五）- 戰俘密報
下载
下载
下載


歷史上的今天20190301第295期 - 黃石公園
下载
下载
下載


歷史上的今天20190228第294期 - 亨利·盧斯
下载
下载
下載


歷史上的今天20190227第293期 - LV 路易威登
下载
下载
下載


歷史上的今天20190226第292期 - 從海瑞罷官到八大樣板戲
下载
下载
下載


歷史上的今天20190225第291期 - 中國遠征軍之殤
下载
下载
下載


《週末漫談》20190223第13期
下载
下载
下載


《中情局的紅色鼴鼠》（四）
下载
下载
下載


歷史上的今天20190222第290期 - 和珅
下载
下载
下載


歷史上的今天20190221第289期 - 聖女貞德
下载
下载
下載


歷史上的今天20190220第288期 - 《天鵝湖》首演
下载
下载
下載


歷史上的今天20190219第287期 - 二戰美國關押日裔美國人
下载
下载
下載


歷史上的今天20190218第286期 - 援外八項原則
下载
下载
下載


《週末漫談》20190216第12期
下载
下载
下載


江峰劇場：中央情報局的紅色鼴鼠（三）
下载
下载
下載


歷史上的今天20190215第285期 - 末日忠臣李鴻章
下载
下载
下載


歷史上的今天20190214第284期 - YouTube创立
下载
下载
下載


歷史上的今天20190213第283期 - 澳洲被偷走的一代人
下载
下载
下載


歷史上的今天20190212第282期 - 三鹿毒奶粉
下载
下载
下載


歷史上的今天20190210第281期 - 莊則棟與乒乓外交
下载
下载
下載


《週末漫談》20190210第11期
下载
下载
下載


江峰劇場：《中情局的紅色鼴鼠》（二）
下载
下载
下載


歷史上的今天20190208第280期 - 春晚來歷
下载
下载
下載


歷史上的今天20190207第279期 - “天运号” 越南船民投奔香港
下载
下载
下載


歷史上的今天20190206第278期 - 王立軍夜闖美領館
下载
下载
下載


歷史上的今天20190205第277期 - 《讀者文摘》
下载
下载
下載


歷史上的今天20190204第276期 - 查韋斯政變
下载
下载
下載


《週末漫談》20190202第10期
下载
下载
下載


江峰劇場：《中情局的紅色鼴鼠》（一）
下载
下载
下載


歷史上的今天20190201第275期 - 延安整風
下载
下载
下載


歷史上的今天20190131第274期 - 漢字簡化
下载
下载
下載


歷史上的今天20190130第273期 - 羅密歐與茱麗葉
下载
下载
下載


歷史上的今天0129第272期 - 鄧麗君
下载
下载
下載


歷史上的今天0128第271期 - 鄧小平訪美
下载
下载
下載


歷史上的今天0125第270期 - 古拉格勞改營
下载
下载
下載


歷史上的今天0124第269期 - 黃埔军校
下载
下载
下載


歷史上的今天0123第268期 - 遣返志願軍
下载
下载
下載


歷史上的今天0122第267期 - 梁羽生
下载
下载
下載


歷史上的今天0121第266期 - 人大确定教师节
下载
下载
下載


《週末漫談》20190119第9期
下载
下载
下載


歷史上的今天0118第265期 - 伯纳诺
下载
下载
下載


歷史上的今天0117第264期 - 赵紫阳
下载
下载
下載


歷史上的今天0116第263期 - 海湾战争
下载
下载
下載


歷史上的今天0115第262期 - 萨利机长
下载
下载
下載


《週末漫談》20190113第8期
下载
下载
下載


歷史上的今天0114第261期 - 四清運動
下载
下载
下載


《週末漫談》20190112第7期
下载
下载
下載


歷史上的今天20190111第260期 - 沙龍
下载
下载
下載


歷史上的今天20190110第259期 - 托馬斯·潘恩
下载
下载
下載


歷史上的今天20190109第258期 - 伊麗莎白女王號
下载
下载
下載


歷史上的今天20190108第257期 - 史蒂芬·霍金
下载
下载
下載


歷史上的今天20190107第256期 - 尼古拉·特拉斯
下载
下载
下載


《週末漫談》20190105第6期
下载
下载
下載


歷史上的今天20190104第255期 - 費邊社
下载
下载
下載


歷史上的今天20190103第254期 - 艾偉德
下载
下载
下載


歷史上的今天20190102第253期 - 羅盛教
下载
下载
下載


歷史上的今天20190101第252期 - 歐元發行
下载
下载
下載


歷史上的今天20181210第236期 - 77年恢復高考
下载
下载
下載


歷史上的今天20181225第247期 - 齊奧塞斯庫
下载
下载
下載


歷史上的今天20181226第248期 - 蘇聯解體
下载
下载
下載


歷史上的今天20181231第251期 - 維也納新年音樂
下载
下载
下載


历史上的今天1203 第231期 - 庚子赔款
下载
下载
下載


歷史上的今天20181113 第217期 - 克林德牌坊
下载
下载
下載


历史上的今天20181116第220期 - 郭沫若
下载
下载
下載


历史上的今天20181029第206期 - 纳粹宣传部长戈培尔
下载
下载
下載


歷史上的今天20180720第135期 - 迫害法轮功
下载
下载
下載


歷史上的今天20180709第127期 - 709律师案
下载
下载
下載


历史上的今天20180615第111期 - 义和团围攻西什库教堂
下载
下载
下載


历史上的今天20180604第102期 - 八九六四屠城
下载
下载
下載



",5
DataBiosphere/toil,Python,"ATTENTION: Toil has moved from https://github.com/BD2KGenomics/toil to https://github.com/DataBiosphere/toil as of July 5th, 2018.
Toil is a scalable, efficient, cross-platform (Linux & macOS) pipeline management system,
written entirely in Python, and designed around the principles of functional
programming.
Our next scheduled release is May 24, 2019.

Check the website for a description of Toil and its features.
Full documentation for the latest stable release can be found at
Read the Docs.
See our occasional blog for tutorials.
Google Groups discussion forum and videochat invite list.



",588
rubocop-hq/rubocop-rails,Ruby,"
RuboCop Rails
A RuboCop extension focused on enforcing Rails best practices and coding conventions.
",68
lazyphp/PESCMS-Ticket,JavaScript,"PESCMS Ticket
 
PESMCS Ticket(下称PT)是一款基于GPLv2协议发布的开源客服工单系统。PT以全新的设计理念，实现一句JS即可嵌入任意页面中，让工单系统变得更加轻便。
反馈和建议
邮箱：sale#pescms.com
演示地址：https://ticket.pescms.com
反馈问题：https://www.pescms.com/page/11.html
开发文档：https://www.pescms.com/d/index
PESCMS官方QQ 1群：451828934(已满) 
PESCMS官方QQ 2群：496804032 
运行环境
PHP 5.6及以上版本
Mysql 5.5及以上版本
IE浏览器不保证兼容
安装使用


下载并解压程序至您的HTTP运行环境所在目录。
没有配置虚拟主机，则访问Public目录。反之，请将虚拟主机目录配置到Public
根据安装程序填写对应数据，完成软件安装。


快速使用
登入系统后台--工单模型--创建工单 。创建完毕后，点击'生成JS'按钮。将JS文件保存到本地。最后在任意的页面中，引入如下代码，则可实现您的工单系统。
<html>
<head>
    <meta charset=""utf-8"">
</head>
<body>
<!--JQ是必须组件-->
<script src=""http://libs.baidu.com/jquery/2.1.4/jquery.min.js""></script>
<!--JQ是必须组件-->

<script src=""PESCMS TICKET生成的工单JS文件"" id=""ticket""></script>
<script>
    var ticket = PT.createForm(""ticket"");
</script>
</body>
</html>
其他说明
除了通过一句话引入JS生成工单，本系统支持站内提交工单，且支持登录验证或沿用JS中的匿名方式提交。
界面预览
工单系统首页

提交工单列表

提交工单详细页

查看工单详细内容以及回复

后台首页

后台工单列表

",26
AguaClara/floc_app,Python,"Floc App
Team Members:
Dana Owens  (Lead),
Richard Yu,
Natalie Isak
Goals for Spring 2018:

Train new members on Github, OpenCV and Aguaclara Safety.
Get familiar with scripts written spring semester.
Use sample images to test code and search for bugs.
Fix gaps in flocs that are in focus.
Create script that can size in focus flocs
Test field of vision for camera.

Calendar:
Lab times/meetings assigned to T/R 1:25-2:30 along with time put in outside of lab.
Week 1: Meet new members, start to look over research from previous semesters.
Week 2: Get familiar with open CV, start to create tutorials for following semesters.
Week 3-5: Work on old code. Fix gaps in flocs. Eventually size flocs.
Week 6: TBD
Week 7: TBD
Week 8-9: TBD
Week 10: TBD
Week 11: TBD
Week 12: TBD
Spring 2018 Final Presentation:
[Spring 2019 Final Youtube Video] (https://www.youtube.com/watch?v=sHuIshcOfOA&list=PLhsGtpY8ipdZL4lExJA8KC0zCkaxwfs8R&index=5&t=0s)
[Spring 2019 Final Presentation] (https://docs.google.com/presentation/d/1Eygbpt9PRS5Cm3Tfm3-SG0krKTNgR4sOA6cbfNLOUBY/edit?usp=sharing)
[Spring 2019 Mid-Year Presentation Slides] (https://docs.google.com/presentation/d/1r-v86uV3EiZ85N1uaAgKwRKOMQ2US-n4p6oVIipjUHs/edit)
[Fall 2018 Presentation Video] https://youtu.be/2SXc7grdD3g
[Fall 2018 Final Presentation Slides] (https://docs.google.com/presentation/d/1QeoitHQfX2f_iCgGeG6z51iU74PFxhHOF9QFAl-lUw8/edit?usp=sharing)
Fall 2018 Mid-year Presentation Slides
Spring 2018 Final Presentation Slides
",5
Gridstone/RxStore,Java,"RxStore
A tiny library that assists in saving and restoring objects to and from disk using RxJava, and observing changes over time.
This library now targets RxJava2. If you're using RxJava1 then take a look at a version 5.1.1.
Details
RxStore is a simple persistence framework for those already making use of RxJava2 in their projects. There are many occasions where you don't need the complexity introduced by a database; you just require a simple put/get API.
We have found this particularly useful on Android, where there are many options, but none of them quite right...

Simple key/value pair? SharedPreferences makes that simple.
Elaborate interconnected data sets? SQLite can help you.
Everything else? We just want to put and get objects from disk with minimal overhead.

By design, RxStore lets you use whatever serialization format you prefer, so long as you provide a valid Converter. Converters for Moshi, Gson and Jackson are provided out of the box, and pull requests for more are always welcome!
Leaning on RxJava, RxStore can help alleviate some threading concerns when reading and writing to disk, and allows for some pretty nifty method chaining once the operation completes. It also lets you observe changes as you write new values into stores.
Usage
Creating Stores
There are two kinds of stores:

ValueStore lets you write, read, and observe changes to a single value you want to persist.
ListStore does the same but for many values, and has convenience methods for adding and removing individual items in the list.

Say we have a model class called Person
public final class Person {
  public final String name;
  public final int age;
}
To persist a single Person, we must first create a ValueStore.
ValueStore<Person> store = RxStore.value(file, converter, Person.class);
In addition to the type we must also provide a File and a Converter. The File gives the object a place to live on disk, and the Converter dictates how it's saved and restored. You can make your own Converter or use one we prepared earlier.
Storing Data
There are two ways we can add a Person to our store: store.put(person) or store.observePut(person). put() is a fire-and-forget method that will asynchronously write the value to disk. observePut() returns an RxJava Single that must be subscribed to in order for the write operation to begin. This is useful when incorporating the write operation into a chain, or would like to know when a write operation has completed.
Perhaps you're making use of Square's Retrofit. You could download and persist data in a single Rx chain.
webServices.getPerson()
    .flatMap((person) -> store.observePut(person))
    .subscribe((person) -> {
      // Do something with newly downloaded and persisted person.
    });
ListStore is useful if you wanted to store many people. In addition to put(people) it also has some handy methods such as add(person) and remove(person).
Retrieving Data
When retrieving from a ValueStore we can use store.get() or store.blockingGet(). The former returns a Maybe, as there may not be a current value. The latter blocks until the disk read and deserialization is complete, and returns a nullable value.
ListStore behaves slightly differently. get() returns a Single, as empty stores can be represented by an immutable empty List. blockingGet() will always return a non-null List.
Observing Data
Another handy trick is to observe a store change over time. Calling store.observe() will give you an Rx Observable. This Observable will immediately deliver the current value of the store upon subscription, and will then deliver updated values if changes occur in onNext().
It's worth noting that valueStore.observe() does not return Observable<T>, but rather Observable<ValueUpdate<T>>. This is because the store cannot use null to represent the absence of a value, and must wrap the update in a non-null object.
listStore.observe() however does return Observable<List<T>>, as an empty ListStore can be represented by an immutable empty List.
Kotlin
If you're working in Kotlin, there are also two convenient functions provided in the rxstore-kotlin artifact that make use of reified type parameters. This removes the need to pass the Type in the store initialisation methods.
val personStore = storeProvider.valueStore<Person>(file, converter)
val peopleStore = storeProvider.listStore<Person>(file, converter)
Download
All artifacts are up on Maven Central.
For the base library
compile 'au.com.gridstone.rxstore:rxstore:6.0.2'
For the kotlin convenience functions
compile 'au.com.gridstone.rxstore:rxstore-kotlin:6.0.2'
For the Moshi converter
compile 'au.com.gridstone.rxstore:converter-moshi:6.0.2'
For the Gson converter
compile 'au.com.gridstone.rxstore:converter-gson:6.0.2'
For the Jackson converter
compile 'au.com.gridstone.rxstore:converter-jackson:6.0.2'
License
Copyright 2017 GRIDSTONE

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

",368
Lombiq/Orchard-Dojo-Library,None,"Orchard Dojo Library
The Orchard Dojo Library is a portable package of Orchard goodies. It supplements Orchard Dojo's trainings and tutorials. For accessing the Library from the web, go to orcharddojo.net/orchard-resources/Library/ (you can also download the Library's textual content as a single document form there). The Library's project page is under source.lombiq.com/orchard-dojo-library.
The Orchard Dojo Library is backed by the team behind Orchard Dojo, Lombiq.
The Orchard Dojo Library is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/orchard-dojo-library (Mercurial repository)
https://github.com/Lombiq/Orchard-Dojo-Library (Git repository)

Bug reports, feature requests and comments are warmly welcome, please do so via GitHub. Feel free to send pull requests too, no matter which source repository you choose for this purpose.
",2
efcl/efcl.github.io,CSS,"Web Scratch 
https://efcl.info/ のブログ用のリポジトリです。
Installation

gem install bundler
bundle install
bundle exec jekyll serve
Open http://0.0.0.0:4000

Contributing
typoなどの間違いはdevelopブランチにPull Requestして頂いて問題ありません。
書いて欲しい記事のIssueを立てるだけなどでもOKです。
また、Gitterを使ったチャットで伝えてもらっても問題ありません。


Fork it!
Create your feature branch: git checkout -b my-new-feature
Commit your changes: git commit -am 'Add some feature'
Push to the branch: git push origin my-new-feature
Submit a pull request :D

License
ソースコードはMIT
文章はCC-BY 
",82
rendy44/rmasjid,PHP,"Masjid
Free WordPress theme for your masjid/mosque.
Installation
Download the latest release, extract it and put folder masjid inside your wp-content\themes\ directory.
Go to your WordPress admin dashboard and go to Appearance and click themes, you should see masjid in the list, and activate it.
After that, please go to Theme Options to adjust some basic setting.
Features
This theme provides some features for your web, such as:

Campaign will help you organize and manage any donation and campaign right on your website, right now it only support manual payment through bank transfer.

Notification, both admin and user will be notified through email every time a new event happen to campaign nor payment.
Report, we provide pdf report for each campaigns with its successful payment records.


Article is a blog portal where you can write articles, news, knowledge nor anything you want.
Lecture is another feature that will make your lecture scheduling lot easier, it supports both single event and recurring event.
Customization is a unique feature that will allow you to change the design to meet your preference.

Logo, you can edit the logo to present your masjid/mosque
Background, use your best image as a default background, if you don't have any, we provide prophet's mosque as the default.
Color Scheme, we all know that every masjid/mosque has their own specific color that might represent its uniqueness, we allow you to chane it to any color you want.
Footer, currently there are two available footers desaign (full & minimalism) with extra customization for its color scheme for each, we have dark, light and default. Default means you will use main color scheme that explained in previous point.


Optimization we not only provide beautiful design, but we also care with its performance.
More to come..

Contribution
If you have any issues or question, please don't hesitate to submit your issue here or email me at rendy[dot]de[dot]p[at]gmail[dot]com
",2
pytorch/glow,C++,"


Glow is a machine learning compiler and execution engine for hardware
accelerators.  It is designed to be used as a backend for high-level machine
learning frameworks.  The compiler is designed to allow state of the art
compiler optimizations and code generation of neural network graphs. This
library is in active development. The project plan is described in the Github
issues section and in the
Roadmap wiki page.
Partners
Contributions to Glow are welcomed and encouraged! Glow is developed in
collaboration with the following partners:


























How does it work?
Glow lowers a traditional neural network dataflow graph into a two-phase
strongly-typed intermediate representation (IR). The high-level
IR allows the optimizer to perform domain-specific optimizations. The
lower-level instruction-based address-only IR allows the compiler to perform
memory-related optimizations, such as instruction scheduling, static memory
allocation and copy elimination. At the lowest level, the optimizer performs
machine-specific code generation to take advantage of specialized hardware
features. Glow features a lowering phase which enables the compiler to support a
high number of input operators as well as a large number of hardware targets by
eliminating the need to implement all operators on all targets. The lowering
phase is designed to reduce the input space and allow new hardware backends to
focus on a small number of linear algebra primitives.
The design philosophy is described in an arXiv paper.

Getting Started
System Requirements
Glow builds and runs on macOS and Linux. The software depends on a modern C++
compiler that supports C++11, on CMake, LLVM, glog, protocol buffers, and
libpng.
Get Glow!
git clone git@github.com:pytorch/glow.git  # or: git clone https://github.com/pytorch/glow.git
cd glow
Submodules
Glow depends on a few submodules: googletest, onnx, and a library
for FP16 conversions.
To get them, from the glow directory, run:
git submodule update --init --recursive
macOS
Install the required dependencies using either Homebrew or
MacPorts. If using Homebrew, run:
brew install cmake graphviz libpng ninja protobuf wget glog
brew install llvm@7
If using MacPorts, run:
port install cmake graphviz libpng ninja protobuf-cpp wget llvm-7.0 google-glog
Note that LLVM is installed in a non-default location to avoid conflicts with
the system's LLVM --Homebrew usually installs LLVM in /usr/local/opt/llvm/,
whereas MacPorts installs it in /opt/local/libexec/llvm-7.0/. This means that
CMake will need to be told where to find LLVM when building; instructions on
that can be found here.
Finally, create a symbolic link to the Homebrew- or MacPorts-installed
clang-* tools so that the utils/format.sh script is able to find them later
on. For a Homebrew-managed installation, run:
ln -s ""/usr/local/opt/llvm/bin/clang-format"" ""/usr/local/bin/clang-format""
ln -s ""/usr/local/opt/llvm/bin/clang-tidy"" ""/usr/local/bin/clang-tidy""

For MacPorts, run:
ln -s ""/opt/local/libexec/llvm-7.0/bin/clang-format"" ""/usr/local/bin/clang-format""
ln -s ""/opt/local/libexec/llvm-7.0/bin/clang-tidy"" ""/usr/local/bin/clang-tidy""


Note: On newer versions of macOS, Xcode's command line tools come with a
non-traditional header layout. In order for Glow to build on newer macOS
versions, you might need to install macOS_SDK_headers_for_macOS_10.14.pkg
manually. For example, on Mojave this package is located in
/Library/Developer/CommandLineTools/Packages/.

Ubuntu
[The following instructions have been tested on Ubuntu 16.04]
In order to build Glow on Ubuntu it is necessary to install a few packages. The
following command should install the required dependencies:
sudo apt-get install clang clang-8 cmake graphviz libpng-dev \
    libprotobuf-dev llvm-8 llvm-8-dev ninja-build protobuf-compiler wget \
    opencl-headers libgoogle-glog-dev
[Note: building Glow on Ubuntu 16.04 with llvm-7 fails because llvm-7 xenial distribution
uses an older c++ ABI]
It may be desirable to use update-alternatives to manage the version of
clang/clang++:
sudo update-alternatives --install /usr/bin/clang clang \
    /usr/lib/llvm-8/bin/clang 50
sudo update-alternatives --install /usr/bin/clang++ clang++ \
    /usr/lib/llvm-8/bin/clang++ 50
Glow uses the system default C/C++ compiler (/usr/bin/c++), and so you may also
want to switch your default C/C++ compiler to clang:
sudo update-alternatives --config cc
    # Select the option corresponding to /usr/bin/clang ...
sudo update-alternatives --config c++
    # Select the option corresponding to /usr/bin/clang++ ...
Glow should build just fine with gcc (e.g. gcc 5.4), but we mostly use clang
and are more attentive to compatibility with clang.
Finally, in order to support the ONNX net serialization format, Glow requires
protobuf >= 2.6.1, but the above command may install older
version on older Ubuntu (e.g. 14.04). If this is the case, we suggest to look
at utils/install_protobuf.sh to install a newer version from source.
For details on installing OpenCL on Ubuntu please see
these instructions.
Configure and Build
To build the compiler, create a build directory and run cmake on the source
directory. It's a good idea to build two configurations (Release and Debug)
because some programs take a really long time to run in Debug mode. It's also a
good idea to build the project outside of the source directory.
mkdir build_Debug
cd build_Debug
cmake -G Ninja -DCMAKE_BUILD_TYPE=Debug ../glow
ninja all
It's possible to configure and build the compiler with any CMake generator,
like GNU Makefiles, Ninja and Xcode build.
For platform-specific build instructions and advanced options, such as
building with Address-Sanitizers refer to this guide:
Building the Compiler.
If you're running Mac OS v10.14 (Mojave) and ninja all fails because it can't
find headers (e.g. string.h), run this command to fix it, and try again.
More information is available here
under ""Command Line Tools"".
open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg

Building with dependencies (LLVM)
By default, Glow will use a system provided LLVM.  Note that Glow requires LLVM
7.0 or later. If you have LLVM installed in a non-default location (for
example, if you installed it using Homebrew on macOS), you need to tell CMake
where to find llvm using -DLLVM_DIR. For example, if LLVM were
installed in /usr/local/opt:
cmake -G Ninja ../glow \
    -DCMAKE_BUILD_TYPE=Debug \
    -DLLVM_DIR=/usr/local/opt/llvm@7/lib/cmake/llvm
If LLVM is not available on your system you'll need to build it manually.  Run
the script '/utils/build_llvm.sh to clone, build and install LLVM in a local
directory. You will need to configure Glow with the flag -DLLVM_DIR to tell
the build system where to find LLVM given the local directory you installed it
in (e.g. -DLLVM_DIR=/path/to/llvm_install/lib/cmake/llvm if using
build_llvm.sh).
Testing and Running
Unit tests
The project has a few unit tests in the tests/unittests subdirectory. To run all
of them, simply run ninja test.
C++ API examples
A few test programs that use Glow's C++ API are found under the examples/
subdirectory. The mnist, cifar10, fr2en and ptb programs train and run digit
recognition, image classification and language modeling benchmarks,
respectively.
To run these programs, build Glow in Release mode, then run the following commands
to download the cifar10, mnist and ptb databases.
python ../glow/utils/download_datasets_and_models.py --all-datasets
Now run the examples. Note that the databases should be in the current working
directory.
./bin/mnist
./bin/cifar10
./bin/fr2en
./bin/ptb
./bin/char-rnn
If everything goes well you should see:

mnist: pictures from the mnist digits database
cifar10: image classifications that steadily improve
fr2en: an interactive French-to-English translator
ptb: decreasing perplexity on the dataset as the network trains
char-rnn: generates random text based on some document

Note that the default build mode is Debug, which means that the compiler
itself is easy to debug because the binary contains debug info, lots of
assertions, and the optimizations are disabled. It also means that the compiler
and runtime are very slow, and the execution time can be hundreds of times
slower than that of release builds. If you wish to benchmark the compiler, run
long benchmarks, or release the product then you should compile the compiler in
Release mode. Check the main CMake file for more details.
More details on testing and running Glow can be found in:
Testing the Glow Compiler.
Ahead-of-time Compilation
Glow can be used to compile neural networks into object files containing native
code.  We provide resnet50 (both quantized and non-quantized versions) as an
example of this capability in examples/bundles/resnet50.  See Creating
Standalone Executable Bundles for more detail.
Contributing
To get started contributing, please refer to the following guides:

Contributing
Coding Standards
Code of Conduct

Communication

Forums: discuss implementations, research, etc: https://discuss.pytorch.org/c/glow.
Make sure to label topic with the ""glow"" category.
GitHub issues: bug reports, feature requests, install issues, RFCs, thoughts, etc.

License
Glow is licensed under the Apache 2.0 License.
",1471
sylabs/sif,Go,"The Singularity Image Format (SIF)




SIF is an open source implementation of the Singularity Container Image Format
that makes it easy to create complete and encapsulated container enviroments
stored in a single file.

Unless otherwise noted, the SIF source files are distributed under the BSD-style
license found in the LICENSE.md file.
Download and Install From Source
To get the sif package to use directly from your programs:
go get -u github.com/sylabs/sif/pkg/sif
To get the siftool CLI program installed to $GOPATH/bin to manipulate SIF container files:
mkdir -p $GOPATH/src/github.com/sylabs
cd $GOPATH/src/github.com/sylabs
git clone https://github.com/sylabs/sif
cd sif
./build.sh
Contributing
SIF and Singularity is the work of many contributors. We appreciate your help!
To contribute, please read the contribution guidelines:
CONTRIBUTING.md
",6
pytorch/glow,C++,"


Glow is a machine learning compiler and execution engine for hardware
accelerators.  It is designed to be used as a backend for high-level machine
learning frameworks.  The compiler is designed to allow state of the art
compiler optimizations and code generation of neural network graphs. This
library is in active development. The project plan is described in the Github
issues section and in the
Roadmap wiki page.
Partners
Contributions to Glow are welcomed and encouraged! Glow is developed in
collaboration with the following partners:


























How does it work?
Glow lowers a traditional neural network dataflow graph into a two-phase
strongly-typed intermediate representation (IR). The high-level
IR allows the optimizer to perform domain-specific optimizations. The
lower-level instruction-based address-only IR allows the compiler to perform
memory-related optimizations, such as instruction scheduling, static memory
allocation and copy elimination. At the lowest level, the optimizer performs
machine-specific code generation to take advantage of specialized hardware
features. Glow features a lowering phase which enables the compiler to support a
high number of input operators as well as a large number of hardware targets by
eliminating the need to implement all operators on all targets. The lowering
phase is designed to reduce the input space and allow new hardware backends to
focus on a small number of linear algebra primitives.
The design philosophy is described in an arXiv paper.

Getting Started
System Requirements
Glow builds and runs on macOS and Linux. The software depends on a modern C++
compiler that supports C++11, on CMake, LLVM, glog, protocol buffers, and
libpng.
Get Glow!
git clone git@github.com:pytorch/glow.git  # or: git clone https://github.com/pytorch/glow.git
cd glow
Submodules
Glow depends on a few submodules: googletest, onnx, and a library
for FP16 conversions.
To get them, from the glow directory, run:
git submodule update --init --recursive
macOS
Install the required dependencies using either Homebrew or
MacPorts. If using Homebrew, run:
brew install cmake graphviz libpng ninja protobuf wget glog
brew install llvm@7
If using MacPorts, run:
port install cmake graphviz libpng ninja protobuf-cpp wget llvm-7.0 google-glog
Note that LLVM is installed in a non-default location to avoid conflicts with
the system's LLVM --Homebrew usually installs LLVM in /usr/local/opt/llvm/,
whereas MacPorts installs it in /opt/local/libexec/llvm-7.0/. This means that
CMake will need to be told where to find LLVM when building; instructions on
that can be found here.
Finally, create a symbolic link to the Homebrew- or MacPorts-installed
clang-* tools so that the utils/format.sh script is able to find them later
on. For a Homebrew-managed installation, run:
ln -s ""/usr/local/opt/llvm/bin/clang-format"" ""/usr/local/bin/clang-format""
ln -s ""/usr/local/opt/llvm/bin/clang-tidy"" ""/usr/local/bin/clang-tidy""

For MacPorts, run:
ln -s ""/opt/local/libexec/llvm-7.0/bin/clang-format"" ""/usr/local/bin/clang-format""
ln -s ""/opt/local/libexec/llvm-7.0/bin/clang-tidy"" ""/usr/local/bin/clang-tidy""


Note: On newer versions of macOS, Xcode's command line tools come with a
non-traditional header layout. In order for Glow to build on newer macOS
versions, you might need to install macOS_SDK_headers_for_macOS_10.14.pkg
manually. For example, on Mojave this package is located in
/Library/Developer/CommandLineTools/Packages/.

Ubuntu
[The following instructions have been tested on Ubuntu 16.04]
In order to build Glow on Ubuntu it is necessary to install a few packages. The
following command should install the required dependencies:
sudo apt-get install clang clang-8 cmake graphviz libpng-dev \
    libprotobuf-dev llvm-8 llvm-8-dev ninja-build protobuf-compiler wget \
    opencl-headers libgoogle-glog-dev
[Note: building Glow on Ubuntu 16.04 with llvm-7 fails because llvm-7 xenial distribution
uses an older c++ ABI]
It may be desirable to use update-alternatives to manage the version of
clang/clang++:
sudo update-alternatives --install /usr/bin/clang clang \
    /usr/lib/llvm-8/bin/clang 50
sudo update-alternatives --install /usr/bin/clang++ clang++ \
    /usr/lib/llvm-8/bin/clang++ 50
Glow uses the system default C/C++ compiler (/usr/bin/c++), and so you may also
want to switch your default C/C++ compiler to clang:
sudo update-alternatives --config cc
    # Select the option corresponding to /usr/bin/clang ...
sudo update-alternatives --config c++
    # Select the option corresponding to /usr/bin/clang++ ...
Glow should build just fine with gcc (e.g. gcc 5.4), but we mostly use clang
and are more attentive to compatibility with clang.
Finally, in order to support the ONNX net serialization format, Glow requires
protobuf >= 2.6.1, but the above command may install older
version on older Ubuntu (e.g. 14.04). If this is the case, we suggest to look
at utils/install_protobuf.sh to install a newer version from source.
For details on installing OpenCL on Ubuntu please see
these instructions.
Configure and Build
To build the compiler, create a build directory and run cmake on the source
directory. It's a good idea to build two configurations (Release and Debug)
because some programs take a really long time to run in Debug mode. It's also a
good idea to build the project outside of the source directory.
mkdir build_Debug
cd build_Debug
cmake -G Ninja -DCMAKE_BUILD_TYPE=Debug ../glow
ninja all
It's possible to configure and build the compiler with any CMake generator,
like GNU Makefiles, Ninja and Xcode build.
For platform-specific build instructions and advanced options, such as
building with Address-Sanitizers refer to this guide:
Building the Compiler.
If you're running Mac OS v10.14 (Mojave) and ninja all fails because it can't
find headers (e.g. string.h), run this command to fix it, and try again.
More information is available here
under ""Command Line Tools"".
open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg

Building with dependencies (LLVM)
By default, Glow will use a system provided LLVM.  Note that Glow requires LLVM
7.0 or later. If you have LLVM installed in a non-default location (for
example, if you installed it using Homebrew on macOS), you need to tell CMake
where to find llvm using -DLLVM_DIR. For example, if LLVM were
installed in /usr/local/opt:
cmake -G Ninja ../glow \
    -DCMAKE_BUILD_TYPE=Debug \
    -DLLVM_DIR=/usr/local/opt/llvm@7/lib/cmake/llvm
If LLVM is not available on your system you'll need to build it manually.  Run
the script '/utils/build_llvm.sh to clone, build and install LLVM in a local
directory. You will need to configure Glow with the flag -DLLVM_DIR to tell
the build system where to find LLVM given the local directory you installed it
in (e.g. -DLLVM_DIR=/path/to/llvm_install/lib/cmake/llvm if using
build_llvm.sh).
Testing and Running
Unit tests
The project has a few unit tests in the tests/unittests subdirectory. To run all
of them, simply run ninja test.
C++ API examples
A few test programs that use Glow's C++ API are found under the examples/
subdirectory. The mnist, cifar10, fr2en and ptb programs train and run digit
recognition, image classification and language modeling benchmarks,
respectively.
To run these programs, build Glow in Release mode, then run the following commands
to download the cifar10, mnist and ptb databases.
python ../glow/utils/download_datasets_and_models.py --all-datasets
Now run the examples. Note that the databases should be in the current working
directory.
./bin/mnist
./bin/cifar10
./bin/fr2en
./bin/ptb
./bin/char-rnn
If everything goes well you should see:

mnist: pictures from the mnist digits database
cifar10: image classifications that steadily improve
fr2en: an interactive French-to-English translator
ptb: decreasing perplexity on the dataset as the network trains
char-rnn: generates random text based on some document

Note that the default build mode is Debug, which means that the compiler
itself is easy to debug because the binary contains debug info, lots of
assertions, and the optimizations are disabled. It also means that the compiler
and runtime are very slow, and the execution time can be hundreds of times
slower than that of release builds. If you wish to benchmark the compiler, run
long benchmarks, or release the product then you should compile the compiler in
Release mode. Check the main CMake file for more details.
More details on testing and running Glow can be found in:
Testing the Glow Compiler.
Ahead-of-time Compilation
Glow can be used to compile neural networks into object files containing native
code.  We provide resnet50 (both quantized and non-quantized versions) as an
example of this capability in examples/bundles/resnet50.  See Creating
Standalone Executable Bundles for more detail.
Contributing
To get started contributing, please refer to the following guides:

Contributing
Coding Standards
Code of Conduct

Communication

Forums: discuss implementations, research, etc: https://discuss.pytorch.org/c/glow.
Make sure to label topic with the ""glow"" category.
GitHub issues: bug reports, feature requests, install issues, RFCs, thoughts, etc.

License
Glow is licensed under the Apache 2.0 License.
",1471
sylabs/sif,Go,"The Singularity Image Format (SIF)




SIF is an open source implementation of the Singularity Container Image Format
that makes it easy to create complete and encapsulated container enviroments
stored in a single file.

Unless otherwise noted, the SIF source files are distributed under the BSD-style
license found in the LICENSE.md file.
Download and Install From Source
To get the sif package to use directly from your programs:
go get -u github.com/sylabs/sif/pkg/sif
To get the siftool CLI program installed to $GOPATH/bin to manipulate SIF container files:
mkdir -p $GOPATH/src/github.com/sylabs
cd $GOPATH/src/github.com/sylabs
git clone https://github.com/sylabs/sif
cd sif
./build.sh
Contributing
SIF and Singularity is the work of many contributors. We appreciate your help!
To contribute, please read the contribution guidelines:
CONTRIBUTING.md
",6
JordanMartinez/purescript-jordans-reference,PureScript,"Purescript-Jordans-Reference
This repo is my way of trying to use the Feynman Technique to help me learn Purescript and its ecosystem. It includes a number of links and other resources I've gathered or created. These have been gathered into the following folders. (For a full overview of this repo's contents, see the Table of Contents.md file.)
All code uses PureScript 0.12.5

00-Getting-Started

Why learn/use PureScript?
How to install Purscript & set up an editor (using Atom)
An overview of the REPL
Other important info.


01-Build-Tools - how to use the tools in the ecosystem to manage dependencies, compile source code, and build projects

Overview of the dependency managers
Other build tools


11-Syntax - Purescript's syntax explained using meta-language and verified by the compiler

Basic Syntax
Foreign Function Interface Syntax
Type-Level Programming Syntax
Module syntax


21-Hello-World - everything you need to know to write, structure, test, and benchmark a Purescript program

Philosophical Foundations
Prelude + Basic data structures
Hello World & Effects
Debugging
Testing
Benchmarking
Application structure
Type-Level Programming Overview


22-Projects - working programs written in an FP style with helpful comments and overviews of the libraries they use

Random Number Game - a game that runs on the console and in the browser using the same ""business logic""
Table of Contents Generator (WIP) - a program that generates a table of contents for this project.


31-Design Patterns - (WIP) commonly-used patterns to solve problems in FP languages
41-Ecosystem - (WIP) a better overview of the libraries in Purescript (categorized by tags)

The following labels give insight into this project's development:

the 'Roadmap' label: a deeper understanding of this project's current direction/goals.
the 'Meta' label: issues related to the project as a whole.
the 'Release-PR' label: the changelog of the code

Note: Sometimes this repo will produce a lot of notifications due to opening/closing issues/PRs and me adding additional thoughts/comments to things. If you wish to watch this repo, you are advised to watch for releases only. Many have unwatched this repo (before the release-only watch option was available) because it sometimes feels like notification spam.
License
Unless stated otherwise in a specific folder or file, this project is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license: (Human-readable version), (Actual License)

Guidelines for this project
Contributing
Feel free to open a new issue for:

Clarification on something you don't understand. If I don't know it yet and I'm interested, it'll force me to learn it
A link to something you'd like me to research more. If I'm interested or see the value, I'll look into it and try to document it or explain the idea in a clear way
Corrections for any mistakes I've made
Improvements to anything I've written thus far

I have written an ISSUE_TEMPLATE file. When you open a new issue, there will be content there that tells you how to write a good, clear issue. Please, follow those instructions!
Learning Purescript Using This Project

Read through these links about learning:

How to teach yourself to learn
Asking good questions: short read and long read


Git clone this repo
Use these rules to read through the folders' contents in the correct order:

Read a folder's ""ReadMe.md"" file first (if it exists). It may provide additional info for how to read a folder's contents.
Read a folder's files or subfolders in numerical order. For example, read things in the order they appear below:

00-Getting-Started/ReadMe.md
00-Getting-Started/01-Install-Guide.md
00-Getting-Started/... (the rest of the folder's contents)
01-Build-Tools/... (the folder's contents)
11-Syntax/ReadMe.md
11-Syntax/01-Basic-Syntax/src/00-Comments-and-Documentation.purs
11-Syntax/01-Basic-Syntax/src/01-Preliminary-Concepts/01-Value-Function-Data-Syntax.purs
11-Syntax/01-Basic-Syntax/src/01-Preliminary-Concepts/02-Explainng-Kinds.md
11-Syntax/01-Basic-Syntax/src/01-Preliminary-Concepts/03-The-Prim-Module.purs
11-Syntax/01-Basic-Syntax/src/02-Data-and-Functions/... (the rest of the folder's content)




Compile the code where possible, either before or after you experiment

Naming Conventions Used In This Repo
Numbering System
When you see this number system:
01-File-Name.md
02-Folder-Name/
03-File-Name2.md
11-File-Name.md

You should understand it like so:
[major theme/idea][minor concept/point]

Each major theme will almost always have 1..9 minor concepts/points. Thus, you will sometimes not see a 10-file-name.md file:
09-first-major-theme--file-9.md
-- 10-file-name is intentionally missing here
11-second-major-theme--file-1.md

In situations, where 9 files were not enough, I converted a file into a folder and each file in that folder further explains it.
An 'x' in a File/Folder Name
If a file or folder name has x in the numerical part of its name (e.g. 0x-File-or-Folder-Name, 9x-File-or-Folder-Name), it means I am still deciding where it should appear in the numerical order (and it is likely still a work in progress).
Referring to Files/Folders in this repo
Lastly, when referring to folders/files, we'll omit the numerical ordering (since the final order is still being determined). So, rather than 00-Getting-Started, you'll see Getting Started folder. Rather than 00-Getting-Started/01-Install-Guide.md, you'll see Getting-Started/Install-Guide.md
Versioning Policy
See this section to help you understand what a new release means
Principles

Provide ""stable"" versions...:

Readers of a given version should be able to read and bookmark files without worrying about those files/links breaking due to changes in its name (via renaming/reordering files, headers in files, etc.)
Older versions should be available via git tag.


...without restricting developer creativity:

I should be able to continue writing new content and re-ordering things without concern


Load the latest release:

This repo should show the latest release version of this project, not the one on which I'm working. In other words, the default branch should coincide with the last release.


Lessen maintenance as much as possible:

There should only be two branches, latestRelease and development since a branch name like master is overloaded with connotations. Those who want to read older versions can checkout a tag.
I currently will not hyperlink to other files within this project until either a 1.0.0 release is made or I find a way to automate that.


Indicate PS version:

As Purescript continues to evolve, the release should use a prefix indicating which major PS version for which this library is up-to-date.



Release Syntax and Explanation
ps-[purescript's major release]-v[Major].[Minor].[Patch] where

purescript's major release means

Normally, this would be 1.x.x, but we don't yet have a 1.0 release yet. Thus, it is currently 0.12.x
x is a placeholder for the latest minor/patch release.


major change means

a file/folder name has changed, so that bookmarks or links to that file/folder are now broken
files/folders have been modified, so that one is recommended to re-read the modified parts


minor change means

a file's contents have been modified/updated to such a degree that one is recommended to re-read the modified parts
a file's header name has changed, so that bookmarks or links to that header/section are now broken


patch means

additional files/folders have been added without breaking links
a file's contents have been modified/updated to a minor degree that one could re-read the modified parts but is not likely to benefit much from it.
a file's contents have been slightly updated (typos, markdown rendering issues, etc.)



",127
tmori/athrill,C,"athrill
Athrill is a CPU emulator.
Athrill was developed to easily execute and debug embedded programs on virtual microcomputer.
You can evaluate bare metal programs or embedded control programs running on real-time OS.
Athrill at the moment supports V850 CPU instructions.
Table of Contents


Requirements
Install
Deomo
License

Requirements

Athrill requires the following to run:

OS

[Linux]

Ubuntu(32bit)


[Windows]

MinGW32_NT-6.2(Windows10, Windows7)





Athrill uses the following editors for source debugging:

Editor

[Linux]

geany


[Windows]

Sakura Editor





Install

After downloading Athrill project, add the following athrill executable binary folder path on the environment-variable (PATH).
export PATH=[athrill root folder]/src/bin:$PATH 

Then make sure you can display usage of athrill on an arbitrary folder:
Usage:athrill -m<memory config file> [OPTION]... <load_file>
-i                             : execute on the interaction mode. if -i is not set, execute on the background mode.
-r                             : execute on the remote mode. this option is valid on the interaction mode.
-t<timeout>                    : set program end time using <timeout> clocks. this option is valid on the background mode.
-m<memory config file>         : set athrill memory configuration. rom, ram region is configured on your system.

Demo

Athrill debugging Real-time OS (asp3) demonstration.

License

Athrill is licensed under the TOPPERS License Agreement (http://www.toppers.jp/en/license.html).
",17
akkadotnet/akka.net,C#,"Akka.NET

 
Builds status




Windows
Linux (Mono)




Build




Unit Tests




MultiNode Tests




Perf Tests





Akka.NET Current Roadmap
Akka.NET is a community-driven port of the popular Java/Scala framework Akka to .NET.

Subscribe to the Akka.NET dev feed: https://twitter.com/AkkaDotNet  (@AkkaDotNet)
Gitter chat: https://gitter.im/akkadotnet/akka.net
Support forum: https://groups.google.com/forum/#!forum/akkadotnet-user-list
Mail: hi@getakka.net
Stack Overflow: http://stackoverflow.com/questions/tagged/akka.net

Documentation and resources
Akka.NET Community Site
Install Akka.NET via NuGet
If you want to include Akka.NET in your project, you can install it directly from NuGet
To install Akka.NET Distributed Actor Framework, run the following command in the Package Manager Console
PM> Install-Package Akka
PM> Install-Package Akka.Remote

And if you need F# support:
PM> Install-Package Akka.FSharp

Contributing
Where Can I Contribute?




All contributions are welcome! Please consider the issues categorized in the Help! column first, as they are areas we could really use your help :)

Contribution Guidelines
If you are interested in helping porting Akka to .NET please take a look at Contributing to Akka.NET.
Our docs are always a work in progress—to contribute to docs, please see the docs contribution guidelines here.
Builds
Please see Building Akka.NET.
To access unstable nightly builds, please see the instructions here.
Support


",3232
NREL/EnergyPlus,C++,"EnergyPlus 
This is the EnergyPlus Development Repository.  EnergyPlus™ is a whole building energy simulation program that engineers, architects, and researchers use to model both energy consumption and water use in buildings.
Contact/Support

The Department of Energy maintains a public website for EnergyPlus where you can find much more information about the program.
For detailed developer information, consult the wiki.
Many users (and developers) of EnergyPlus are active on Unmet Hours, so that's a great place to start if you have a question about EnergyPlus or building simulation.
For more in-depth, developer-driven support, please utilize the EnergyPlus Helpdesk.

Releases 
EnergyPlus is released twice annually, usually in March and September.
It is recommended all use of EnergyPlus is production workflows use these formal, public releases.
Iteration (pre-)releases may be created during a development cycle, however users should generally avoid these, as input syntax may change which won't be supported by the major release version transition tools, and could require manual intervention to remedy.
If an interim release is intended for active use by users, such as a bug-fix-only or performance-only re-release, it will be clearly specified on the release notes and a public announcement will accompany this type of release.
Documentation  
Program documentation is installed alongside the program, with the pdfs also available online.
Big Ladder also produces html based documentation online.
License & Contributing Development 
EnergyPlus is available under a BSD-3-like license.
For more information, check out the license file.
The EnergyPlus team accepts contributions to EnergyPlus source, utilities, test files, documentation, and other materials distributed with the program.
The current EnergyPlus contribution policy is now available on the EnergyPlus contribution policy page.
If you are interested in contributing, please start there, but feel free to reach out to the team.
Building EnergyPlus
Commits to EnergyPlus are built by our team of robots (Tik-Tok, Gort, and Marvin), using the Decent CI continuous integration system.
The testing dashboard gives a continually updated view of the status of all active branches, pull requests, and tags.
A detailed description of compiling EnergyPlus on multiple platforms is available on the wiki.
",365
Eric3911/Stage,None,"Stage：本人从事低质量多尺度成像量化卷积编码和RSIC芯片工程学习：
该项工作主要是监督自己研究过程与工程学结合通过学术指导实现工程学上系统问题同时欢迎大家一起交流学习。
深度学习的核心教程：
GMM  https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/
Opencv  https://docs.opencv.org/3.4.5/d9/df8/tutorial_root.html
个人参与视觉开源学习项目
https://github.com/Dikea/ML-Weekly-Learning/tree/master/CV
图像算法个人开源代码库
https://github.com/Eric3911/Code-with-Life
https://github.com/Eric3911/Coding-learning
边缘分割
https://github.com/MarkMoHR/Awesome-Edge-Detection-Papers
经典论文阅读库
https://arxiv.org/list/cs.AI/recent
https://github.com/Eric3911/Paper-Sharing
https://github.com/Eric3911/Super-Paper
对偶学习：一种全新的深度学习范式
深度学习走向何方？ 非线性动力学与分数阶非线性方程在群论中的模态融合知识表示体系
大数据比赛学习
https://github.com/Eric3911/CDCS
计算机视觉科学竞赛
https://github.com/Eric3911/Visual-Science-Competition
计算机图形学
      Jim Blinn's Corner: Dirty Pixels
      Jim Blinn's Corner: A Trip Down The Graphics Pipeline
      Jim Blinn's Corner: Notation, Notation, Notation

计算机摄影学
      光流场和光线追踪在相控矩阵77GHZ的毫米波雷达融合

视网膜去雨雾项目

成像质量分析项目




证件拍摄阴影去除
https://github.com/Eric3911/icpr2018_ocr_papers

版面分析

交通ADAS项目


遥感小目标检测项目

麻省理工寒假科研交流项目
（那些年里眷恋的您——MIT）








公交车超载评估项目

密集人群检测项目

蒙版弹幕项目

舞姿同步项目

",43
david-a-wheeler/railroader,Ruby,"





Railroader
Railroader is an open source static analysis tool which checks Ruby on Rails applications for security vulnerabilities.
Railroader is a fork of the Brakeman analysis tool version 4.3.1 (the last version of Brakeman that was open source software).  A key distinguishing feature is that Railroader is open source software (OSS), while Brakeman is not open source software any more.  Railroader is licensed under the MIT-LICENSE. As a result, Railroader can be freely used for any purpose, including any commercial purposes.  In addition, contributors to Railroader (unlike Brakeman) retain their copyrights.
If you are interested in Brakeman, please see the Brakeman site instead!
We are currently in a transition process, because we have just started creating Railroader as a fork of Brakeman.  Some names in the process of changing - help is wanted to complete it. We need to change the name, because we assume that Synopsys owns the trademarks and in any case we want to make sure there is no confusion by anyone that Railroader is Brakeman (they are now different projects).
Installation
Using RubyGems:
gem install railroader

Using Bundler:
group :development do
  gem 'railroader', :require => false
end

Usage
From a Rails application's root directory:
railroader

Outside of Rails root:
railroader /path/to/rails/application

Compatibility
Railroader should work with any version of Rails from 2.3.x to 5.x.
Railroader can analyze code written with Ruby 1.8 syntax and newer, but requires at least Ruby 1.9.3 to run.
Basic Options
For a full list of options, use railroader --help or see the OPTIONS.md file.
To specify an output file for the results:
railroader -o output_file

The output format is determined by the file extension or by using the -f option. Current options are: text, html, tabs, json, markdown, csv, and codeclimate.
Multiple output files can be specified:
railroader -o output.html -o output.json

To suppress informational warnings and just output the report:
railroader -q

Note all Railroader output except reports are sent to stderr, making it simple to redirect stdout to a file and just get the report.
To see all kinds of debugging information:
railroader -d

Specific checks can be skipped, if desired. The name needs to be the correct case. For example, to skip looking for default routes (DefaultRoutes):
railroader -x DefaultRoutes

Multiple checks should be separated by a comma:
railroader -x DefaultRoutes,Redirect

To do the opposite and only run a certain set of tests:
railroader -t SQL,ValidationRegex

If Railroader is running a bit slow, try
railroader --faster

This will disable some features, but will probably be much faster (currently it is the same as --skip-libs --no-branching). WARNING: This may cause Railroader to miss some vulnerabilities.
By default, Railroader will return a non-zero exit code if any security warnings are found or scanning errors are encountered. To disable this:
railroader --no-exit-on-warn --no-exit-on-error

To skip certain files or directories that Railroader may have trouble parsing, use:
railroader --skip-files file1,/path1/,path2/

To compare results of a scan with a previous scan, use the JSON output option and then:
railroader --compare old_report.json

This will output JSON with two lists: one of fixed warnings and one of new warnings.
Railroader will ignore warnings if configured to do so. By default, it looks for a configuration file in config/railroader.ignore.
(To help people transition from Brakeman, if config/railroader.ignore
doesn't exist, but config/brakeman.ignore does, then we'' use the
latter file.)
To create and manage this file, use:
railroader -I

Warning information
See warning_types for more information on the warnings reported by this tool.
Warning context
The HTML output format provides an excerpt from the original application source where a warning was triggered. Due to the processing done while looking for vulnerabilities, the source may not resemble the reported warning and reported line numbers may be slightly off. However, the context still provides a quick look into the code which raised the warning.
Confidence levels
Railroader assigns a confidence level to each warning. This provides a rough estimate of how certain the tool is that a given warning is actually a problem. Naturally, these ratings should not be taken as absolute truth.
There are three levels of confidence:

High - Either this is a simple warning (boolean value) or user input is very likely being used in unsafe ways.
Medium - This generally indicates an unsafe use of a variable, but the variable may or may not be user input.
Weak - Typically means user input was indirectly used in a potentially unsafe manner.

To only get warnings above a given confidence level:
railroader -w3

The -w switch takes a number from 1 to 3, with 1 being low (all warnings) and 3 being high (only highest confidence warnings).
Configuration files
Railroader options can stored and read from YAML files. To simplify the process of writing a configuration file, the -C option will output the currently set options.
Options passed in on the commandline have priority over configuration files.
The default config locations are ./config/railroader.yml, ~/.railroader/config.yml, and /etc/railroader/config.yml
The -c option can be used to specify a configuration file to use.
Continuous Integration
There is a plugin available for Jenkins/Hudson.
For even more continuous testing, try the Guard plugin.
Building
git clone git://github.com/david-a-wheeler/railroader.git
cd railroader
gem build railroader.gemspec
gem install railroader*.gem

Contributing
We love contributions!  Please help us!
For more about how to contribute, see CONTRIBUTING.md.
Reporting vulnerabilities in Railroader itself
If you find an exploitable vulnerability in Railroader itself,
see CONTRIBUTING.md.
Homepage/News
Website: http://railroader.org/
Twitter: https://twitter.com/railroader
License
See MIT-LICENSE.
",18
openstax/deploy-manifests,None,"deploy-manifests
Holds deployment manifests (managed with the manifestly gem) for OpenStax
",2
david-a-wheeler/railroader,Ruby,"





Railroader
Railroader is an open source static analysis tool which checks Ruby on Rails applications for security vulnerabilities.
Railroader is a fork of the Brakeman analysis tool version 4.3.1 (the last version of Brakeman that was open source software).  A key distinguishing feature is that Railroader is open source software (OSS), while Brakeman is not open source software any more.  Railroader is licensed under the MIT-LICENSE. As a result, Railroader can be freely used for any purpose, including any commercial purposes.  In addition, contributors to Railroader (unlike Brakeman) retain their copyrights.
If you are interested in Brakeman, please see the Brakeman site instead!
We are currently in a transition process, because we have just started creating Railroader as a fork of Brakeman.  Some names in the process of changing - help is wanted to complete it. We need to change the name, because we assume that Synopsys owns the trademarks and in any case we want to make sure there is no confusion by anyone that Railroader is Brakeman (they are now different projects).
Installation
Using RubyGems:
gem install railroader

Using Bundler:
group :development do
  gem 'railroader', :require => false
end

Usage
From a Rails application's root directory:
railroader

Outside of Rails root:
railroader /path/to/rails/application

Compatibility
Railroader should work with any version of Rails from 2.3.x to 5.x.
Railroader can analyze code written with Ruby 1.8 syntax and newer, but requires at least Ruby 1.9.3 to run.
Basic Options
For a full list of options, use railroader --help or see the OPTIONS.md file.
To specify an output file for the results:
railroader -o output_file

The output format is determined by the file extension or by using the -f option. Current options are: text, html, tabs, json, markdown, csv, and codeclimate.
Multiple output files can be specified:
railroader -o output.html -o output.json

To suppress informational warnings and just output the report:
railroader -q

Note all Railroader output except reports are sent to stderr, making it simple to redirect stdout to a file and just get the report.
To see all kinds of debugging information:
railroader -d

Specific checks can be skipped, if desired. The name needs to be the correct case. For example, to skip looking for default routes (DefaultRoutes):
railroader -x DefaultRoutes

Multiple checks should be separated by a comma:
railroader -x DefaultRoutes,Redirect

To do the opposite and only run a certain set of tests:
railroader -t SQL,ValidationRegex

If Railroader is running a bit slow, try
railroader --faster

This will disable some features, but will probably be much faster (currently it is the same as --skip-libs --no-branching). WARNING: This may cause Railroader to miss some vulnerabilities.
By default, Railroader will return a non-zero exit code if any security warnings are found or scanning errors are encountered. To disable this:
railroader --no-exit-on-warn --no-exit-on-error

To skip certain files or directories that Railroader may have trouble parsing, use:
railroader --skip-files file1,/path1/,path2/

To compare results of a scan with a previous scan, use the JSON output option and then:
railroader --compare old_report.json

This will output JSON with two lists: one of fixed warnings and one of new warnings.
Railroader will ignore warnings if configured to do so. By default, it looks for a configuration file in config/railroader.ignore.
(To help people transition from Brakeman, if config/railroader.ignore
doesn't exist, but config/brakeman.ignore does, then we'' use the
latter file.)
To create and manage this file, use:
railroader -I

Warning information
See warning_types for more information on the warnings reported by this tool.
Warning context
The HTML output format provides an excerpt from the original application source where a warning was triggered. Due to the processing done while looking for vulnerabilities, the source may not resemble the reported warning and reported line numbers may be slightly off. However, the context still provides a quick look into the code which raised the warning.
Confidence levels
Railroader assigns a confidence level to each warning. This provides a rough estimate of how certain the tool is that a given warning is actually a problem. Naturally, these ratings should not be taken as absolute truth.
There are three levels of confidence:

High - Either this is a simple warning (boolean value) or user input is very likely being used in unsafe ways.
Medium - This generally indicates an unsafe use of a variable, but the variable may or may not be user input.
Weak - Typically means user input was indirectly used in a potentially unsafe manner.

To only get warnings above a given confidence level:
railroader -w3

The -w switch takes a number from 1 to 3, with 1 being low (all warnings) and 3 being high (only highest confidence warnings).
Configuration files
Railroader options can stored and read from YAML files. To simplify the process of writing a configuration file, the -C option will output the currently set options.
Options passed in on the commandline have priority over configuration files.
The default config locations are ./config/railroader.yml, ~/.railroader/config.yml, and /etc/railroader/config.yml
The -c option can be used to specify a configuration file to use.
Continuous Integration
There is a plugin available for Jenkins/Hudson.
For even more continuous testing, try the Guard plugin.
Building
git clone git://github.com/david-a-wheeler/railroader.git
cd railroader
gem build railroader.gemspec
gem install railroader*.gem

Contributing
We love contributions!  Please help us!
For more about how to contribute, see CONTRIBUTING.md.
Reporting vulnerabilities in Railroader itself
If you find an exploitable vulnerability in Railroader itself,
see CONTRIBUTING.md.
Homepage/News
Website: http://railroader.org/
Twitter: https://twitter.com/railroader
License
See MIT-LICENSE.
",18
openstax/deploy-manifests,None,"deploy-manifests
Holds deployment manifests (managed with the manifestly gem) for OpenStax
",2
pando-project/libtuv,C,"You can find project details in wiki
",86
TimeWarpEngineering/blazor-state,C#,"Blazor-State



Blazor-State is a State Management architecture utilizing the MediatR pipeline.
Documentation
See the GitHub pages site for Documentation
Installation
Blazor-State is available as a Nuget Package
dotnet add package Blazor-State
Acknowledgements
Jimmy Bogard (MediatR).
Jimmy is an amazing developer and a knowledge sharer.
Through his course at 11x Engieering,
his many blog posts on Los Techies and now JimmyBogard.com.
I have learned great amounts.
Peter Morris (Blazor-Fluxor). Pete and I
have been friends for many years and he is an amazing developer and person who has taught me much.
Not surprisingly Pete and I think much alike.
We independently started working on our State Management
components. Although I started first :P (By like a few days)
Pete's component attempts to solve most of the same problems.
Blazor-State draws on the strengths of a proven pipeline in MediatR where as Fluxor
implements its own middle-ware.
If Blazor-State does not meet your needs be sure to checkout Fluxor.
Tor Hovland (Blazor-Redux).
I have only know Tor for a short time via the Blazor Gitter channel but he is already stimulating ideas.
If you use F# and need state management functionality checkout his library.
Unlicense
The Unlicense
Contributing
Time is of the essence.  Before developing a Pull Request I recommend opening an issue for discussion.
Please feel free to make suggestions and help out with the DocFx documentation.
Refer to Markdown for how to write markdown files.
",46
labs12-should-i-live-here/Front-End,JavaScript,"Live Safe
Deployed Site: https://livesafe.netlify.com/
Design Docs: https://www.figma.com/file/d7Kvf8Wh43QtpOOOOTGYbKy8/Untitled?node-id=0%3A1
Idea
The LiveSafe project gives users a one-stop shop for quantifying the risks of natural disasters for a particular zip code in the US. It displays several layers of data, including different kinds of disasters and different timeframes (present, predictions, etc.)
A user can interact with the data in two ways: maps and comparisons. The user can look at a map with as many layers as needed, or go into a detailed comparison of two different places with all the in-depth information about those places at once.
The data will be granular at the zip code level. The API allows a user to specify an address and directs them to the right zipcode.
Mission statement:
To bring together relevant information about disaster risks in a particular area, to inform the buying and renting decisions of potential homeowners.
Why build this?
What problem does this application solve?
To give people a single, interactive map for visualizing the natural disasters that could affect their real estate choices. Plenty of real estate websites have details about crime and home values, but data about natural disasters is only available in government databases. We will make it accessible and visual.
How does your application solve the problem?
We solve the problem by giving the user an interactive map of the whole US with details about risks in colorful layers (colormaps) that they can turn on and off. This includes currently known risks (based on historical data) as well as projections for the future (details TBD). The user can also do deeper, tabular comparisons between a handful of places that they select.
Integrated Technologies
React, Redux, Axios, SASS, Styled Components, Stripe ,Axios , Material Ui,Moment.js ,Mapbox, ReactMapGl,Knex,StyledComponents,OAuth
",2
Kensuke-Hinata/statistic,None,"statistic
collecting books, papers and docs.
",5
stephane-martin/vssh,Go,"vssh


Author:
Stephane Martin



Contents

1   introduction
2   install
3   compile
4   develop
5   usage
6   examples
7   questions



1   introduction
vssh is a SSH client that uses Hashicorp's vault to authenticate with SSH
certificates.
How it works:

first of all you need to configure the SSH certificate authority in Vault
(see Vault documentation)
inject the CA private and public keys into Vault
configure the target OpenSSH servers to accept keys signed by the CA


just as with an usual SSH client, specify to vssh
which server you want to connect to
witch which remote user
witch private key to use


say how to to connect to Vault
Vault address
Vault authentication (token, login/password, ...)


say which SSH signing role to use in Vault

vssh will then

submit your SSH private key to vault for signing
fetch the signed SSH certificate from vault
use the private key and the certificate to authenticate and connect to the
remote SSH server

vssh can open an interactive shell on the remote server, or execute a command.

2   install
Nothing special. Just copy the binary into your PATH.

3   compile
The dependencies are vondored using dep. You
do not need it to compile vssh. Just clone in an appropriate directoty (GOROOT)
and run make release.
mdkir -p ~/go/src/github.com/stephane-martin
cd ~/go/src/github.com/stephane-martin
git clone https://github.com/stephane-martin/vssh
cd vssh
make release

4   develop

Clone the repository in an appropriate go directory.
Install dep and
golangci-lint.
Compile with make debug and lint with make lint.


5   usage
vssh can open an interactive SSH session (vssh ssh), execute a remote
command (vssh ssh), and download (vssh download) / upload (vssh upload)
files using the scp protocol.
Most of command line options can be specified with environnemt variables instead.
Check vssh --help for details.

5.1   global options
The global options are useful for the different vssh commands. They configure
the connection to Vault.


Global option
Value Example
Definition

--vault-addr
http://127.0.0.1:8200
vault connection URL

--vault-method
userpass
vault authentication method [token, userpass, ldap, approle]

--vault-username
myvaultuser
username for vault authentication

--vault-password
myvaultpass
password for vault authentication

--vault-ssh-role
myrole
name of the SSH sign role you have configured in Vault

--vault-token
s.lIz3muuaUOZe424j2ZI5GTDK
token for vault authentication

--vault-ssh-mount
ssh-client-signer
the path to the SSH signer in Vault

--vault-auth-path
custompath
if the Vault authentication method is mounted to a custom path




5.2   interactive SSH session
vssh [global options] ssh [ssh options] user@host

vssh ssh --help
vssh needs a private key to send to Vault for signature. You can give it:

a private key that is stored locally on your filesystem with --identity
or a private key stored in vault with --videntity

vssh will ask for a passphrase if the private key is stored in encrypted form.


SSH option
Value Example
Definition

--identity
/path/to/id_rsa
file path to the SSH private key that should be signed

--videntity
secret/id_rsa_in_vault
Vault path to the SSH private key that should be signed

--insecure
 
do not check the SSH server host key

--native
 
use the local ssh binary to make the connection

--terminal
 
force pseudo-terminal allocation

--ssh-port
22
SSH server listen port

--login
admin
alternate way to specify the remote user




5.3   remote command
vssh [global options] ssh [ssh options] user@host command

vssh [global options] ssh -t [ssh options] user@host command
Just put the command the execute at the end of the vssh ssh command line.
If the command is meant to be interactive, then you need to add the -t flag.
For example, to launch an alternate shell:
vssh ssh -t me@remote zsh
It is also possible to inject some Vault secrets into the remote command environment,
similarly to --envconsul, with the following flags:


SSH option
Value Example
Definition

--secret
secret/path
path of a secret to read from Vault

--upcase
 
convert environment variable keys to UPPERCASE

--prefix
 
prefix the environment variable keys with names of secrets




5.4   download
vssh [global options] download [download options] --target file1 [--target file2...] user@host

vssh download --help
Specify the remote files/directories you want to download with the --target
flag. It can appear multiple times.
Specify the local destination path with the --destination flag.
The other flags are similar to the vssh ssh command.


download option
Value Example
Definition

--identity
/path/to/id_rsa
file path to the SSH private key that should be signed

--videntity
secret/id_rsa_in_vault
Vault path to the SSH private key that should be signed

--insecure
 
do not check the SSH server host key

--target
remotefile
path to the remote file to be downloaded

--destination
/tmp
local destination path

--ssh-port
22
SSH server listen port

--login
admin
alternate way to specify the remote user

--preserve
 
preserve file mode, access time and modification time




5.5   upload
vssh [global options] upload [upload options] user@host

vssh upload --help
Specify the local files/directories you want to upload with the --source
flag. It can appear multiple times.
Specify the remote destination path with the --destination flag.
The other flags are similar to the vssh ssh command.


download option
Value Example
Definition

--identity
/path/to/id_rsa
file path to the SSH private key that should be signed

--videntity
secret/id_rsa_in_vault
Vault path to the SSH private key that should be signed

--insecure
 
do not check the SSH server host key

--source
localfile
path to the local file to be uploaded

--destination
/tmp
remote destination path

--ssh-port
22
SSH server listen port

--login
admin
alternate way to specify the remote user




5.6   as a library
TODO

6   examples
Let's assume you have configured a few environment variables, to avoid
repetition in the examples.
export VAULT_ADDR=https://vault.example.org:8200
export VAULT_SSH_MOUNT=ssh-client-signer
export VAULT_SIGNING_ROLE=my-vault-ssh-role
With such variables, vssh knowns:

how to connect to the Vault server instance
which certificate authority to use in Vault
which SSH role to use in Vault to produce the certificates

Let's also assume you have generated a SSH private key for your local current
user:
ssh-keygen


6.1   single sign on
Open a terminal, then authenticate yourself with Vault:
vault login -method=userpass username=bob
The vault login command writes the resulting token in ~/.vault_token.
If you don't specify to vssh how to authenticate to Vault, by default it will
use that token.
You can then SSH to any server that recognizes the Vault CA:
vssh ssh me@myserver.example.org

6.2   execute a remote command
vssh ssh me@myserver.example.org ls -al /

6.3   execute a remote command in a pseudo-terminal
vssh ssh -t me@myserver.example.org zsh

6.4   inject Vault secrets in the remote session
Now let's say you want to execute a remote command on a server, but some
part of the configuration for that command is stored in Vault.
vssh can work similar to envconsul:
vssh ssh --secret secret/mysecret me@myserver.example.org backupcommand
Locally, vssh will read the required secret from Vault. Then it opens the SSH
connection. Then the command will be executed, with environment variables
corresponding to the secrets.
So, if secret/mysecret is something like:
foo=bar
ZOG=ZOG

then vssh executes on the remote SSH server:
env foo=bar ZOG=ZOG backupcommand
with the additional --upcase flag, it becomes:
env FOO=bar ZOG=ZOG backupcommand
or with the additional --prefix flag it becomes:
env secret_mysecret_foo=bar secret_mysecret_ZOG=ZOG backupcommand
Your remote SSH environment doesn't have to know anything about Vault by itself.

7   questions

7.1   what does the --native flag do ?
By default vssh uses an internal SSH client implemented in Go.

Go implementation, so vssh does not need to launch another process.
Might behave differently compared to the native ssh command.
Does not read .ssh/config.
The signed certificate is not written to the filesystem, it is passed
directly to the SSH client in memory.

With --native, vssh wraps the native ssh binary. It can be useful it you
wish to enable the native configuration of the SSH client (man 5 ssh_config).

there vssh launches a SSH subprocess
the SSH subprocess will read ssh_config as usual
to pass the signed certificate to SSH, vssh has to write it to the filesystem
(it will be removed at the end of execution)


7.2   what should be the TTL for signed certificates ?
Very short. After Vault has signed the SSH certificate, vssh uses that certificate
immediatly and only once. Every time vssh is executed, another certificate will
be created. So in theory, a TTL of a few seconds is just enough.
",5
reactos/reactos,C,"

























Quick Links
Website •
Official chat •
Wiki •
Forum •
JIRA Bug Tracker •
ReactOS Git mirror •
Testman
What is ReactOS?
ReactOS™ is an Open Source effort to develop a quality operating system that is compatible with applications and drivers written for the Microsoft® Windows™ NT family of operating systems (NT4, 2000, XP, 2003, Vista, Seven).
The ReactOS project, although currently focused on Windows Server 2003 compatibility, is always keeping an eye toward compatibility with Windows Vista and future Windows NT releases.
The code of ReactOS is licensed under GNU GPL 2.0.
Building
    
To build the system it is strongly advised to use the ReactOS Build Environment (RosBE).
Up-to-date versions for Windows and for Unix/GNU-Linux are available from our download page at: ""Build Environment"".
Alternatively one can use Microsoft Visual C++ (MSVC) version 2010+. Building with MSVC is covered here: ""Visual Studio or Microsoft Visual C++"".
Binaries
To build ReactOS you must run the configure script in the directory you want to have your build files. Choose configure.cmd or configure.sh depending on your system. Then run ninja <modulename> to build a module you want or just ninja to build all modules.
Bootable images
To build a bootable CD image run ninja bootcd from the
build directory. This will create a CD image with a filename bootcd.iso.
See ""Building ReactOS"" for more details.
You can always download fresh binary builds of bootable images from the ""Daily builds"" page.
Installing
By default, ReactOS currently can only be installed on a machine that has a FAT16 or FAT32 partition as the active (bootable) partition.
The partition on which ReactOS is to be installed (which may or may not be the bootable partition) must also be formatted as FAT16 or FAT32.
ReactOS Setup can format the partitions if needed.
Starting 0.4.10, ReactOS can be installed using the BtrFS file system. But
consider this as an experimental feature and thus regressions not triggered on
FAT setup may be observed.
To install ReactOS from the bootable CD distribution, extract the archive contents. Then burn the CD image, boot from it, and follow the instructions.
See ""Installing ReactOS"" Wiki page or INSTALL for more details.
Testing
If you discover a bug in ReactOS search on JIRA first - it might be reported already. If not report the bug providing logs and as much information as possible.
See ""File Bugs"" for a guide.
NOTE: The bug tracker is not for discussions. Please use #reactos Freenode IRC channel or our forum.
Contributing  
We are always looking for developers! Check how to contribute if you are willing to participate.
You can also support ReactOS by donating! We rely on our backers to maintain our servers and accelerate development by hiring full-time devs.
More information
ReactOS is a Free and Open Source operating system based on the Windows architecture,
providing support for existing applications and drivers, and an alternative to the current dominant consumer operating system.
It is not another wrapper built on Linux, like WINE. It does not attempt or plan to compete with WINE; in fact, the user-mode part of ReactOS is almost entirely WINE-based and our two teams have cooperated closely in the past.
ReactOS is also not ""yet another OS"". It does not attempt to be a third player like any other alternative OS out there. People are not meant to uninstall Linux and use ReactOS instead; ReactOS is a replacement for Windows users who want a Windows replacement that behaves just like Windows.
More information is available at: reactos.org.
Also see the media/doc subdirectory for some sparse notes.
Who is responsible
Active devs are listed as members of GitHub organization.
See also the CREDITS file for others.
Code mirrors
The main development is done on GitHub. We have an alternative mirror in case GitHub is down.
There is also an obsolete SVN archive repository that is kept for historical purposes.
",6281
jhpyle/docassemble,Python,"See the docassemble web site for
a description of docassemble and installation instructions.
To get help with using docassemble, join the
docassemble mailing list
and/or the docassemble Slack group.
",161
stephane-martin/vssh,Go,"vssh


Author:
Stephane Martin



Contents

1   introduction
2   install
3   compile
4   develop
5   usage
6   examples
7   questions



1   introduction
vssh is a SSH client that uses Hashicorp's vault to authenticate with SSH
certificates.
How it works:

first of all you need to configure the SSH certificate authority in Vault
(see Vault documentation)
inject the CA private and public keys into Vault
configure the target OpenSSH servers to accept keys signed by the CA


just as with an usual SSH client, specify to vssh
which server you want to connect to
witch which remote user
witch private key to use


say how to to connect to Vault
Vault address
Vault authentication (token, login/password, ...)


say which SSH signing role to use in Vault

vssh will then

submit your SSH private key to vault for signing
fetch the signed SSH certificate from vault
use the private key and the certificate to authenticate and connect to the
remote SSH server

vssh can open an interactive shell on the remote server, or execute a command.

2   install
Nothing special. Just copy the binary into your PATH.

3   compile
The dependencies are vondored using dep. You
do not need it to compile vssh. Just clone in an appropriate directoty (GOROOT)
and run make release.
mdkir -p ~/go/src/github.com/stephane-martin
cd ~/go/src/github.com/stephane-martin
git clone https://github.com/stephane-martin/vssh
cd vssh
make release

4   develop

Clone the repository in an appropriate go directory.
Install dep and
golangci-lint.
Compile with make debug and lint with make lint.


5   usage
vssh can open an interactive SSH session (vssh ssh), execute a remote
command (vssh ssh), and download (vssh download) / upload (vssh upload)
files using the scp protocol.
Most of command line options can be specified with environnemt variables instead.
Check vssh --help for details.

5.1   global options
The global options are useful for the different vssh commands. They configure
the connection to Vault.


Global option
Value Example
Definition

--vault-addr
http://127.0.0.1:8200
vault connection URL

--vault-method
userpass
vault authentication method [token, userpass, ldap, approle]

--vault-username
myvaultuser
username for vault authentication

--vault-password
myvaultpass
password for vault authentication

--vault-ssh-role
myrole
name of the SSH sign role you have configured in Vault

--vault-token
s.lIz3muuaUOZe424j2ZI5GTDK
token for vault authentication

--vault-ssh-mount
ssh-client-signer
the path to the SSH signer in Vault

--vault-auth-path
custompath
if the Vault authentication method is mounted to a custom path




5.2   interactive SSH session
vssh [global options] ssh [ssh options] user@host

vssh ssh --help
vssh needs a private key to send to Vault for signature. You can give it:

a private key that is stored locally on your filesystem with --identity
or a private key stored in vault with --videntity

vssh will ask for a passphrase if the private key is stored in encrypted form.


SSH option
Value Example
Definition

--identity
/path/to/id_rsa
file path to the SSH private key that should be signed

--videntity
secret/id_rsa_in_vault
Vault path to the SSH private key that should be signed

--insecure
 
do not check the SSH server host key

--native
 
use the local ssh binary to make the connection

--terminal
 
force pseudo-terminal allocation

--ssh-port
22
SSH server listen port

--login
admin
alternate way to specify the remote user




5.3   remote command
vssh [global options] ssh [ssh options] user@host command

vssh [global options] ssh -t [ssh options] user@host command
Just put the command the execute at the end of the vssh ssh command line.
If the command is meant to be interactive, then you need to add the -t flag.
For example, to launch an alternate shell:
vssh ssh -t me@remote zsh
It is also possible to inject some Vault secrets into the remote command environment,
similarly to --envconsul, with the following flags:


SSH option
Value Example
Definition

--secret
secret/path
path of a secret to read from Vault

--upcase
 
convert environment variable keys to UPPERCASE

--prefix
 
prefix the environment variable keys with names of secrets




5.4   download
vssh [global options] download [download options] --target file1 [--target file2...] user@host

vssh download --help
Specify the remote files/directories you want to download with the --target
flag. It can appear multiple times.
Specify the local destination path with the --destination flag.
The other flags are similar to the vssh ssh command.


download option
Value Example
Definition

--identity
/path/to/id_rsa
file path to the SSH private key that should be signed

--videntity
secret/id_rsa_in_vault
Vault path to the SSH private key that should be signed

--insecure
 
do not check the SSH server host key

--target
remotefile
path to the remote file to be downloaded

--destination
/tmp
local destination path

--ssh-port
22
SSH server listen port

--login
admin
alternate way to specify the remote user

--preserve
 
preserve file mode, access time and modification time




5.5   upload
vssh [global options] upload [upload options] user@host

vssh upload --help
Specify the local files/directories you want to upload with the --source
flag. It can appear multiple times.
Specify the remote destination path with the --destination flag.
The other flags are similar to the vssh ssh command.


download option
Value Example
Definition

--identity
/path/to/id_rsa
file path to the SSH private key that should be signed

--videntity
secret/id_rsa_in_vault
Vault path to the SSH private key that should be signed

--insecure
 
do not check the SSH server host key

--source
localfile
path to the local file to be uploaded

--destination
/tmp
remote destination path

--ssh-port
22
SSH server listen port

--login
admin
alternate way to specify the remote user




5.6   as a library
TODO

6   examples
Let's assume you have configured a few environment variables, to avoid
repetition in the examples.
export VAULT_ADDR=https://vault.example.org:8200
export VAULT_SSH_MOUNT=ssh-client-signer
export VAULT_SIGNING_ROLE=my-vault-ssh-role
With such variables, vssh knowns:

how to connect to the Vault server instance
which certificate authority to use in Vault
which SSH role to use in Vault to produce the certificates

Let's also assume you have generated a SSH private key for your local current
user:
ssh-keygen


6.1   single sign on
Open a terminal, then authenticate yourself with Vault:
vault login -method=userpass username=bob
The vault login command writes the resulting token in ~/.vault_token.
If you don't specify to vssh how to authenticate to Vault, by default it will
use that token.
You can then SSH to any server that recognizes the Vault CA:
vssh ssh me@myserver.example.org

6.2   execute a remote command
vssh ssh me@myserver.example.org ls -al /

6.3   execute a remote command in a pseudo-terminal
vssh ssh -t me@myserver.example.org zsh

6.4   inject Vault secrets in the remote session
Now let's say you want to execute a remote command on a server, but some
part of the configuration for that command is stored in Vault.
vssh can work similar to envconsul:
vssh ssh --secret secret/mysecret me@myserver.example.org backupcommand
Locally, vssh will read the required secret from Vault. Then it opens the SSH
connection. Then the command will be executed, with environment variables
corresponding to the secrets.
So, if secret/mysecret is something like:
foo=bar
ZOG=ZOG

then vssh executes on the remote SSH server:
env foo=bar ZOG=ZOG backupcommand
with the additional --upcase flag, it becomes:
env FOO=bar ZOG=ZOG backupcommand
or with the additional --prefix flag it becomes:
env secret_mysecret_foo=bar secret_mysecret_ZOG=ZOG backupcommand
Your remote SSH environment doesn't have to know anything about Vault by itself.

7   questions

7.1   what does the --native flag do ?
By default vssh uses an internal SSH client implemented in Go.

Go implementation, so vssh does not need to launch another process.
Might behave differently compared to the native ssh command.
Does not read .ssh/config.
The signed certificate is not written to the filesystem, it is passed
directly to the SSH client in memory.

With --native, vssh wraps the native ssh binary. It can be useful it you
wish to enable the native configuration of the SSH client (man 5 ssh_config).

there vssh launches a SSH subprocess
the SSH subprocess will read ssh_config as usual
to pass the signed certificate to SSH, vssh has to write it to the filesystem
(it will be removed at the end of execution)


7.2   what should be the TTL for signed certificates ?
Very short. After Vault has signed the SSH certificate, vssh uses that certificate
immediatly and only once. Every time vssh is executed, another certificate will
be created. So in theory, a TTL of a few seconds is just enough.
",5
reactos/reactos,C,"

























Quick Links
Website •
Official chat •
Wiki •
Forum •
JIRA Bug Tracker •
ReactOS Git mirror •
Testman
What is ReactOS?
ReactOS™ is an Open Source effort to develop a quality operating system that is compatible with applications and drivers written for the Microsoft® Windows™ NT family of operating systems (NT4, 2000, XP, 2003, Vista, Seven).
The ReactOS project, although currently focused on Windows Server 2003 compatibility, is always keeping an eye toward compatibility with Windows Vista and future Windows NT releases.
The code of ReactOS is licensed under GNU GPL 2.0.
Building
    
To build the system it is strongly advised to use the ReactOS Build Environment (RosBE).
Up-to-date versions for Windows and for Unix/GNU-Linux are available from our download page at: ""Build Environment"".
Alternatively one can use Microsoft Visual C++ (MSVC) version 2010+. Building with MSVC is covered here: ""Visual Studio or Microsoft Visual C++"".
Binaries
To build ReactOS you must run the configure script in the directory you want to have your build files. Choose configure.cmd or configure.sh depending on your system. Then run ninja <modulename> to build a module you want or just ninja to build all modules.
Bootable images
To build a bootable CD image run ninja bootcd from the
build directory. This will create a CD image with a filename bootcd.iso.
See ""Building ReactOS"" for more details.
You can always download fresh binary builds of bootable images from the ""Daily builds"" page.
Installing
By default, ReactOS currently can only be installed on a machine that has a FAT16 or FAT32 partition as the active (bootable) partition.
The partition on which ReactOS is to be installed (which may or may not be the bootable partition) must also be formatted as FAT16 or FAT32.
ReactOS Setup can format the partitions if needed.
Starting 0.4.10, ReactOS can be installed using the BtrFS file system. But
consider this as an experimental feature and thus regressions not triggered on
FAT setup may be observed.
To install ReactOS from the bootable CD distribution, extract the archive contents. Then burn the CD image, boot from it, and follow the instructions.
See ""Installing ReactOS"" Wiki page or INSTALL for more details.
Testing
If you discover a bug in ReactOS search on JIRA first - it might be reported already. If not report the bug providing logs and as much information as possible.
See ""File Bugs"" for a guide.
NOTE: The bug tracker is not for discussions. Please use #reactos Freenode IRC channel or our forum.
Contributing  
We are always looking for developers! Check how to contribute if you are willing to participate.
You can also support ReactOS by donating! We rely on our backers to maintain our servers and accelerate development by hiring full-time devs.
More information
ReactOS is a Free and Open Source operating system based on the Windows architecture,
providing support for existing applications and drivers, and an alternative to the current dominant consumer operating system.
It is not another wrapper built on Linux, like WINE. It does not attempt or plan to compete with WINE; in fact, the user-mode part of ReactOS is almost entirely WINE-based and our two teams have cooperated closely in the past.
ReactOS is also not ""yet another OS"". It does not attempt to be a third player like any other alternative OS out there. People are not meant to uninstall Linux and use ReactOS instead; ReactOS is a replacement for Windows users who want a Windows replacement that behaves just like Windows.
More information is available at: reactos.org.
Also see the media/doc subdirectory for some sparse notes.
Who is responsible
Active devs are listed as members of GitHub organization.
See also the CREDITS file for others.
Code mirrors
The main development is done on GitHub. We have an alternative mirror in case GitHub is down.
There is also an obsolete SVN archive repository that is kept for historical purposes.
",6281
jhpyle/docassemble,Python,"See the docassemble web site for
a description of docassemble and installation instructions.
To get help with using docassemble, join the
docassemble mailing list
and/or the docassemble Slack group.
",161
dringkbar/ClickerGame_Practice,C#,"ClickerGame_Practice
#포션 메이커라는 게임의 카피작을 만들면서 게임만들기 공부, 연습하는 중입니다
모작 클리커 게임입니다.
기능

상점기능 (아이템 업그레이드기능)
포션 강화 기능
포션 정보 확인 기능
어떤 재료가 먼저 들어갔냐에 따라서 포션의 색이 바뀝니다.
강화확률도 달라집니다.
클리커게임답게 포션을 팔아서 돈을 법니다. 재료가 들어갈수록 팔때 더많은 돈을 얻을 수 있습니다.
강화할때는 돈이 필요합니다.
나중에는 데이터 저장 기능을 추가할 계획입니다.

",2
Devecstatic/SSMP,Makefile,"SSMP
Second Screen Mode Protocol for iOS
This is a Pre-release









What is SSMP?
SSM or Second Screen Mode Protocol is an open source framework for iOS writen in Swift that makes it easy for apps to take advantage of a second display (through a cable or AirPlay).
What does it do?
When your device is connected to a display, the device becomes a mouse and keyboard. The display has the main view. It adds a mouse pointer which does all the normal touch inputs.
Installation
Cocoapods
Add this to your Podfile
pod 'SSMP'

Carthage
Add this to your Cartfile
github ""Devecstatic/SSMP""

Framework
Download the most recent from the releases.
Usage
Setup
In your AppDelegate, set the view controller the second display should have:
SSMPApp.default.viewController = MyAppMainViewController()
If you want to set whats on the device's display (You will lose the mouse pointer):
SSMPApp.default.deviceViewController = MyOtherViewController()
Replace all gesture recognizers with SSMP{type}GestureRecognizer. For example, UITapGestureRecognizer changes to SSMPTapGestureRecognizer
Options
For SSMPApp:
verboseLogging: Bool
allowedClickTypes: [clickType]
clickType = .tap, .hardpress
primaryBackgroundColor: UIColor
To Start
SSMPApp.default.start()
",51
zhreshold/decord,C++,"Decode(WIP)

Decord is a reverse procedure of Record. It provides convenient video slicing methods based on a thin wrapper on top of hardware accelerated video decoders, e.g.

FFMPEG/LibAV(Done)
Nvidia Codecs(Done)
Intel Codecs

Decord was designed to handle awkward video shuffling experience in order to provide smooth experiences similar to random image loader for deep learning.
Bridges for deep learning frameworks:

Apache MXNet (on going)

Installation
Install via pip
TODO
Install from source
Linux
Install the system packages for building the shared library, for Debian/Ubuntu users, run:
# official PPA comes with ffmpeg 2.8, which lacks tons of features, we use ffmpeg 4.0 here
sudo add-apt-repository ppa:jonathonf/ffmpeg-4
sudo apt-get update
sudo apt-get install -y build-essential python3-dev python3-setuptools make cmake 
libavcodec-dev libavfilter-dev libavformat-dev libavutil-dev
# note: make sure you have cmake 3.8 or later, you can install from cmake official website if it's too old
Clone the repo recursively(important)
git clone --recursive https://github.com/zhreshold/decord
Build the shared library in source root directory, you can specify -DUSE_CUDA=1 to enable NVDEC hardware accelerated decoding:
cd decord
mkdir build && cd build
cmake .. -DUSE_CUDA=0
make
Install python bindings:
cd ../python
# option 1: add python path to $PYTHONPATH, you will need to install numpy separately
pwd=$PWD
echo ""PYTHONPATH=$PYTHONPATH:$pwd"" >> ~/.bashrc
source ~/.bashrc
# option 2: install with setuptools
python3 setup.py install --user
Mac OS
Installation on macOS is similar to Linux. But macOS users need to install building tools like clang, GNU Make, cmake first.
Tools like clang and GNU Make are packaged in Command Line Tools for macOS. To install:
xcode-select --install
To install other needed packages like cmake, we recommend first installing Homebrew, which is a popular package manager for macOS. Detailed instructions can be found on its homepage.
After installation of Homebrew, install cmake by:
brew install cmake
# note: make sure you have cmake 3.8 or later, you can install from cmake official website if it's too old
Clone the repo recursively(important)
git clone --recursive https://github.com/zhreshold/decord
Then go to root directory build shared library:
cd decord
mkdir build && cd build
cmake ..
make
Install python bindings:
cd ../python
# option 1: add python path to $PYTHONPATH, you will need to install numpy separately
pwd=$PWD
echo ""PYTHONPATH=$PYTHONPATH:$pwd"" >> ~/.bash_profile
source ~/.bash_profile
# option 2: install with setuptools
python3 setup.py install --user
Windows
For windows, you will need CMake and Visual Studio for C++ compilation.

First, install git, cmake, ffmpeg and python. You can use Chocolatey to manage packages similar to Linux/Mac OS.
Second, install Visual Studio 2017 Community, this my take some time.

When dependencies are ready, open command line prompt:
cd your-workspace
git clone --recursive https://github.com/zhreshold/decord
cd decord
mkdir build
cd build
cmake -DCMAKE_CXX_FLAGS=""/DDECORD_EXPORTS"" -DCMAKE_CONFIGURATION_TYPES=""Release"" -G ""Visual Studio 15 2017 Win64"" ..
# open `decord.sln` and build project
Usage
Decord provides minimal API set for bootstraping. You can also check out jupyter notebook examples.
VideoReader
VideoReader is used to access frames directly from video files.
from decord import VideoReader
from decord import cpu, gpu

reader = VideoReader('xxx.mp4', ctx=cpu(0))
print('video frames:', len(reader))
batch = vr.next()
print('frame shape:', batch.shape)
print('numpy frames:', batch.asnumpy())

# skip 100 frames
vr.skip_frames(1000)
# seek to start
vr.seek(0)

VideoLoader
VideoLoader is designed for training deep learning models with tons of video files.
It provides smart video shuffle techniques in order to provide high random access performance (We know that seeking in video is super slow and redundant).
The optimizations are underlying in the C++ code, which are invisible to user.
from decord import VideoLoader
from decord import cpu, gpu

vl = VideoLoader(['1.mp4', '2.avi', '3.mpeg'], ctx=[cpu(0)], shape=(2, 320, 240, 3), interval=1, skip=5, shuffle=1)
print('Total batches:', len(vl))

for batch in vl:
    print(batch.shape)
Shuffling video can be tricky, thus we provide various modes:
shuffle = -1  # smart shuffle mode, based on video properties, (not implemented yet)
shuffle = 0  # all sequential, no seeking, following initial filename order
shuffle = 1  # random filename order, no random access for each video, very efficient
shuffle = 2  # random order
shuffle = 3  # random frame access in each video only
Preliminary Benchmarks



Setting
OpenCV VideoCapture
NVVL
Decord




CPU sequential read
1.0x
-
1.1x


CPU random acess(no accurate seek)
0.08x
-
0.23x


CPU random acess (accurate seek)
-

0.06x


GPU sequential
-
TODO
TODO


GPU random acess
-
TODO
TODO



",39
theoldmoon0602/ShellGeiBot,Shell,"ShellGeiBot
Build Docker image
$ ./build.bash shellgeibot:latest
Test Docker image
$ docker container run --rm \
  -v $(pwd):/root/src \
  shellgeibot:latest \
  /bin/bash -c ""apt update && apt install -y bats && bats /root/src/docker_image.bats""
",38
AndrewYinLi/baseball-cli,Python,"baseball-cli
Work in progress
",2
dancwilliams/homeassistant-config,Shell,"Dan's Home Assistant Config
I am new to all of this so please excuse anything that looks crazy.  If you have any feedback feel free to open an issue or hit me up on Twitter @dancwilliams.
Basic Automation Flow
Most of my automation revolves around the group.all_device status and time/day.  There is also a ""Good Morning"" and ""Good Night"" routine to handle some A/C settings and the like.
Input Selectors
There are two input sliders used to store the state of the house and the state of the air conditioning.  This is due to the fact the home state does not necessarily reflect the state of the A/C in the main house.  And soon there will be an Ecobee device in the guest house that will also need to be controlled separately.
home_state
This input selector tracks the state of the home and has three states to choose from (home, away, sleep).  These states are used to drive automations based on whole house activity.
main_house_thermostat_mode
This input selector tracks the state of the thermostat in the main house.  This has four options currently (home, away, sleep, smart1).  Some of these are self explanatory, but smart1 is random.  This is due to the way Ecobee handles custom comfort settngs.  The vanity name for this mode is ""Home Weekend"" in the Ecobee panel, but the API knows it as smart1.  These states are controlled separately from the home_state as they are dependent on the time & day.
MORE DOCUMENTATION TO COME
",2
AndrewYinLi/baseball-cli,Python,"baseball-cli
Work in progress
",2
dancwilliams/homeassistant-config,Shell,"Dan's Home Assistant Config
I am new to all of this so please excuse anything that looks crazy.  If you have any feedback feel free to open an issue or hit me up on Twitter @dancwilliams.
Basic Automation Flow
Most of my automation revolves around the group.all_device status and time/day.  There is also a ""Good Morning"" and ""Good Night"" routine to handle some A/C settings and the like.
Input Selectors
There are two input sliders used to store the state of the house and the state of the air conditioning.  This is due to the fact the home state does not necessarily reflect the state of the A/C in the main house.  And soon there will be an Ecobee device in the guest house that will also need to be controlled separately.
home_state
This input selector tracks the state of the home and has three states to choose from (home, away, sleep).  These states are used to drive automations based on whole house activity.
main_house_thermostat_mode
This input selector tracks the state of the thermostat in the main house.  This has four options currently (home, away, sleep, smart1).  Some of these are self explanatory, but smart1 is random.  This is due to the way Ecobee handles custom comfort settngs.  The vanity name for this mode is ""Home Weekend"" in the Ecobee panel, but the API knows it as smart1.  These states are controlled separately from the home_state as they are dependent on the time & day.
MORE DOCUMENTATION TO COME
",2
ff4j/ff4j-spring-boot-starter-parent,Java,"



Spring boot starter for FF4J (Feature Flipping for Java)

   
Swagger Documentation
This project aims in providing a bootable starter which provides RESTful apis for FF4J.
Create a bootable jar with
mvn clean install
Add dependency in your project
<dependency>
    <groupId>org.ff4j</groupId>
    <artifactId>ff4j-spring-boot-starter</artifactId>
    <version>1.8</version>
</dependency>
Sample
A sample project is located at ff4j-spring-boot-sample
Use mvn spring-boot:run
Once the sample application is booted use the following curl command:
curl -i -H ""Accept: application/json"" -H ""Content-Type: application/json"" -X GET http://localhost:8080/ff4j
Have a look at FF4JConfiguration
What is FF4J ?
FF4J is a proposition of Feature Toggle.
Features can be enabled or disabled through configuration at runtime with dedicated consoles, Web API, or monitor features usage. The same web console can also define any Property and change its value at runtime.
More information can be found at ff4j.org or the reference guide. To access a demo please click [here] (http://cannys.com/ff4j-demo)






",8
li-shuaishuai/react-template,JavaScript,"{{name}}
Project setup
npm install

Compiles and hot-reloads for development
npm run start

Compiles and minifies for production
npm run build

Visualize size of webpack output files with an interactive zoomable treemap
npm run analyz

",3
OfficeDev/office-ui-fabric-react,TypeScript,"Office UI Fabric React
The React-based front-end framework for building experiences for Office and Office 365.


Fabric React is a collection of robust React-based components designed to make it simple for you to create consistent web experiences using the Office Design Language.
Fabric 7 (the next major version of Fabric) is under development. Roadmap, breaking changes, and more details available in the wiki.
Who uses UI Fabric?

+ 45 additional Microsoft sites and products
For more information...
Please see the wiki.
Contents

Using Fabric React

Version policy
Browser support
Right-to-left support
Server-side rendering
Advanced usage


Contribute to Fabric React
Building the repo

Testing
Advanced building tips


Licenses
Changelog

Using Fabric React
Here is a step-by-step tutorial on how to build a simple React app with office-ui-fabric-react components.
How to integrate components into your project depends heavily on your setup. The recommended setup is to use a bundler such as Webpack which can resolve NPM package imports in your code and bundle only the specific things you import.
Within an npm project, you should install the package and save it as a dependency:
npm install --save office-ui-fabric-react

This will add the package as a dependency in your package.json file and download it under node_modules/office-ui-fabric-react.
The library includes ES2015 module entry points under the lib folder (use lib-amd if you need AMD, or lib-commonjs if you need commonjs). To use a control, import it and then use it in your render method:
import * as React from 'react';
import * as ReactDOM from 'react-dom';
import { PrimaryButton } from 'office-ui-fabric-react/lib/Button';

ReactDOM.render(<PrimaryButton>I am a button.</PrimaryButton>, document.body.firstChild);
Version policy
Fabric React adheres to semantic versioning. However, we only consider constructs directly importable at the package level or from files at the root (e.g. office-ui-fabric-react/lib/Utilities or office-ui-fabric-react/lib-amd/Styling) to be part of our API surface. Everything else is considered package-internal and may be subject to changes, moves, renames, etc.
Browser support
Fabric React supports all evergreen browsers, with IE 11 as the min-bar version of Internet Explorer. See the browser support doc for more information.
Right-to-left support
All components can render in LTR or RTL, depending on the dir attribute set on the html element (dir=""rtl"" will flip the direction of everything). You can also use the setRTL API if you don't have control over the html element's rendering. Example:
import { setRTL } from 'office-ui-fabric-react/lib/Utilities';

setRTL(true);
Server-side rendering
Fabric components can be rendered in a server-side Node environment (or used in tests which run in an SSR-like environment), but it requires customizing how styles and SCSS files are loaded. See the server-side rendering documentation for examples of how to handle this.
Advanced usage
For info about advanced usage including module- vs. path-based imports, using an AMD bundler like RequireJS, and deployment features, see our advanced documentation.
Contribute to Fabric React
Please take a look at our contribution guidelines for more info. Also read Contribute bug fixes and Contribute new component.
Building the repo
Before you get started, make sure you have read the Git branch setup instructions
To view the documentation including examples, contracts, component status, and to add functionality or fix issues locally, you can:

git clone https://github.com/OfficeDev/office-ui-fabric-react.git
cd office-ui-fabric-react
npm install
npm start

This will start a demo page from the office-ui-fabric-react package folder, which will open a web browser with the example page. You can make changes to the code which will automatically build and refresh the page using live-reload.
To build and run tests for all packages in the repo, run npm run build from the root.
To build individual packages within the packages/* or apps/* folders, cd to the relevant folder and run npm run build. Note that because the packages are symlinked together, you must manage building dependencies in the right order, or use the rush tool to build to the specific package you want. (See advanced tips below.)
Testing
For info about testing, see our testing documentation.
Advanced building tips
The repo contains many packages, each which may have dependencies on each other. You can use Rush to build projects in the correct order, if you have it globally installed.
npm install -g @microsoft/rush
To use Rush to build, you can run rush build, which will incrementally build the entire repo (only build what has changed since the last build). If you don't have Rush globally installed, you can use the command npm run buildfast, which abstracts rush build.
To build up to a specific project, use the --to <package> argument. For example, to build up to office-ui-fabric-react, run:
rush build --to office-ui-fabric-react
Licenses
All files on the Office UI Fabric React GitHub repository are subject to the MIT license. Please read the License file at the root of the project.
Usage of the fonts and icons referenced in Office UI Fabric is subject to the terms of the assets license agreement.
Changelog
We use GitHub Releases to manage our releases, including the changelog between every release. View a complete list of additions, fixes, and changes on the releases page.

This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.
",4827
jjuraska/slug2slug,Python,"E2E NLG
The E2E NLG module leverages the seq2seq framework for end-to-end natural language generation from meaning representations (MRs). This is a work in progress...

USAGE
In the e2e_nlg folder, optionally put your input files in the data folder. Run main.py in one of the following ways to run the training, or the evaluation (only after the training has been run):
python main.py --train [path_to_trainset] [path_to_devset]
python main.py --test [path_to_testset]

Replace [path_to_trainset], [path_to_devset], [path_to_testset] with relative paths to your trainset, devset, or testset, respectively. They are expected to be CSV files with two columns (their headers must be mr and ref, respectively), the first containing the MRs, and the second containing the corresponding reference utterances.
Once the training is done, the model folder will contain files describing the model, which will be used for evaluation. Therefore, you are not to modify them.
Finally, the evaluation produces output files in the predictions folder. The predictions.txt file contains raw results, while the predictions_final.txt is produced during the postprocessing step.

REQUIREMENTS

Python libraries: tensorflow, numpy, pandas, nltk, networkx
NLTK modules: perluniprops, punkt

install using the following command: python -c ""import nltk; nltk.download('[module_name]')""



",3
gfw-breaker/jiangfeng-subtitles,Shell,"《江峰时刻》节目字幕

友情链接：免翻墙看禁闻  |  手把手翻墙教程  | 《江峰时刻》会员网站



视频节目名称
视频/音频
简体字幕
正體字幕




歷史上的今天20190517第349期 - 梧桐樹協議
下载
下载
下載


歷史上的今天20190516第348期 - 抗日英雄張靈甫
下载
下载
下載


歷史上的今天20190515第347期 - 埃菲尔铁塔
下载
下载
下載


歷史上的今天特刊 - 解讀《白玫瑰傳單一》
下载
下载
下載


歷史上的今天20190514第346期 - 紅都女皇江青
下载
下载
下載


《週末漫談》20190512第24期
下载
下载
下載


《週末漫談》20190511第23期
下载
下载
下載


歷史上的今天20190510第345期 - 曼德拉
下载
下载
下載


歷史上的今天20190509第344期 - 永不凋零的白玫瑰
下载
下载
下載


歷史上的今天20190508第343期 - 美國轟炸中國駐南斯拉夫大使館
下载
下载
下載


歷史上的今天20190507第342期 - 報道水門事件
下载
下载
下載


歷史上的今天20190506第341期 - 《排華法案》
下载
下载
下載


《週末漫談》20190505第22期
下载
下载
下載


歷史上的今天20190503第340期 - 世界新闻自由日
下载
下载
下載


歷史上的今天20190502第339期 - 德国居民参观纳粹集中营
下载
下载
下載


歷史上的今天20190501第338期 - 擊斃本拉登
下载
下载
下載


歷史上的今天20190430第337期 - 路易斯安那
下载
下载
下載


歷史上的今天20190429第336期 - 盧作孚
下载
下载
下載


《週末漫談》20190428第21期
下载
下载
下載


歷史上的今天20190426第335期 - 切爾諾貝利
下载
下载
下載


歷史上的今天20190425第334期 - 425和平上访
下载
下载
下載


歷史上的今天20190424第333期 - SARS
下载
下载
下載


歷史上的今天20190423第332期 - 武訓
下载
下载
下載


歷史上的今天20190422第331期 - 林巧稚
下载
下载
下載


週末漫談》第20期20190421
下载
下载
下載


《中央情報局的紅色鼴鼠》（十二）他還活著
下载
下载
下載


历史上的今天20190419第330期 - 全民打麻雀
下载
下载
下載


歷史上的今天20190418第329期 - 紅色高棉
下载
下载
下載


歷史上的今天20190417第328期 - 富蘭克林
下载
下载
下載


歷史上的今天20190416第327期 - 列寧
下载
下载
下載


歷史上的今天20190415第326期 - 中美合作所
下载
下载
下載


《週末漫談》第19期20190414
下载
下载
下載


《中央情報局的紅色鼴鼠》（十一）風雲際會
下载
下载
下載


歷史上的今天20190412第325期 - 容閎
下载
下载
下載


歷史上的今天20190411第324期 - 愛麗絲島
下载
下载
下載


歷史上的今天20190410第323期 - 朱令中毒案
下载
下载
下載


歷史上的今天20190409第322期 - 波音
下载
下载
下載


歷史上的今天20190408第321期 - 李敦白
下载
下载
下載


《中央情報局的紅色鼴鼠》（十）紅色基因
下载
下载
下載


歷史上的今天20190405第320期 - 麥肯阿瑟
下载
下载
下載


歷史上的今天20190404第319期 - 現代奧運會
下载
下载
下載


歷史上的今天20190403第318期 - 馬歇爾計劃
下载
下载
下載


歷史上的今天20190402第317期 - 世界自閉症關注日
下载
下载
下載


歷史上的今天20190401第316期 - 中美南海撞機
下载
下载
下載


《週末漫談》20190330第18期
下载
下载
下載


《中央情報局的紅色鼴鼠》（九） 顯赫家族
下载
下载
下載


歷史上的今天20190329第315期 - 俄羅斯反華
下载
下载
下載


歷史上的今天20190328第314期 - 艾森豪威爾
下载
下载
下載


歷史上的今天20190327第313期 - 赫鲁晓夫
下载
下载
下載


歷史上的今天20190326第312期 - 鎮反
下载
下载
下載


歷史上的今天20190325第311期 - 麥大志間諜案
下载
下载
下載


《週末漫談》20190323第17期
下载
下载
下載


《中央情報局的紅色鼴鼠》（八）卸磨殺驢
下载
下载
下載


歷史上的今天20190322第310期 - 《鞍鋼憲法》
下载
下载
下載


歷史上的今天20190321第309期 - 最後的日本武士
下载
下载
下載


歷史上的今天20190320第308期 - 孫志剛案
下载
下载
下載


歷史上的今天20190319第307期 - 《歐陽海之歌》
下载
下载
下載


歷史上的今天20190318第306期 - 巴黎公社
下载
下载
下載


《週末漫談》20190316第16期
下载
下载
下載


《中央情報局的紅色鼴鼠》（七）东窗事发
下载
下载
下載


歷史上的今天20190315第305期 - 消費者權益日
下载
下载
下載


歷史上的今天20190314第304期 - 愛因斯坦
下载
下载
下載


歷史上的今天20190313第303期 - 哈佛大学
下载
下载
下載


歷史上的今天20190312第302期 - 可口可樂
下载
下载
下載


歷史上的今天20190311第301期 - 高智晟和他的家
下载
下载
下載


《週末漫談》20190309第15期
下载
下载
下載


《中央情報局的紅色鼴鼠》（六）諜影重重
下载
下载
下載


歷史上的今天20190308第300期 - 三八婦女節
下载
下载
下載


歷史上的今天20190307第299期 - 亨利八世宗教改革
下载
下载
下載


歷史上的今天20190306第298期 - 斯維特蘭娜
下载
下载
下載


歷史上的今天20190305第297期 - 卡廷慘案
下载
下载
下載


歷史上的今天20190304第296期 - 黑船事件
下载
下载
下載


《週末漫談》20190303第14期
下载
下载
下載


《中央情報局的紅色鼴鼠》（五）- 戰俘密報
下载
下载
下載


歷史上的今天20190301第295期 - 黃石公園
下载
下载
下載


歷史上的今天20190228第294期 - 亨利·盧斯
下载
下载
下載


歷史上的今天20190227第293期 - LV 路易威登
下载
下载
下載


歷史上的今天20190226第292期 - 從海瑞罷官到八大樣板戲
下载
下载
下載


歷史上的今天20190225第291期 - 中國遠征軍之殤
下载
下载
下載


《週末漫談》20190223第13期
下载
下载
下載


《中情局的紅色鼴鼠》（四）
下载
下载
下載


歷史上的今天20190222第290期 - 和珅
下载
下载
下載


歷史上的今天20190221第289期 - 聖女貞德
下载
下载
下載


歷史上的今天20190220第288期 - 《天鵝湖》首演
下载
下载
下載


歷史上的今天20190219第287期 - 二戰美國關押日裔美國人
下载
下载
下載


歷史上的今天20190218第286期 - 援外八項原則
下载
下载
下載


《週末漫談》20190216第12期
下载
下载
下載


江峰劇場：中央情報局的紅色鼴鼠（三）
下载
下载
下載


歷史上的今天20190215第285期 - 末日忠臣李鴻章
下载
下载
下載


歷史上的今天20190214第284期 - YouTube创立
下载
下载
下載


歷史上的今天20190213第283期 - 澳洲被偷走的一代人
下载
下载
下載


歷史上的今天20190212第282期 - 三鹿毒奶粉
下载
下载
下載


歷史上的今天20190210第281期 - 莊則棟與乒乓外交
下载
下载
下載


《週末漫談》20190210第11期
下载
下载
下載


江峰劇場：《中情局的紅色鼴鼠》（二）
下载
下载
下載


歷史上的今天20190208第280期 - 春晚來歷
下载
下载
下載


歷史上的今天20190207第279期 - “天运号” 越南船民投奔香港
下载
下载
下載


歷史上的今天20190206第278期 - 王立軍夜闖美領館
下载
下载
下載


歷史上的今天20190205第277期 - 《讀者文摘》
下载
下载
下載


歷史上的今天20190204第276期 - 查韋斯政變
下载
下载
下載


《週末漫談》20190202第10期
下载
下载
下載


江峰劇場：《中情局的紅色鼴鼠》（一）
下载
下载
下載


歷史上的今天20190201第275期 - 延安整風
下载
下载
下載


歷史上的今天20190131第274期 - 漢字簡化
下载
下载
下載


歷史上的今天20190130第273期 - 羅密歐與茱麗葉
下载
下载
下載


歷史上的今天0129第272期 - 鄧麗君
下载
下载
下載


歷史上的今天0128第271期 - 鄧小平訪美
下载
下载
下載


歷史上的今天0125第270期 - 古拉格勞改營
下载
下载
下載


歷史上的今天0124第269期 - 黃埔军校
下载
下载
下載


歷史上的今天0123第268期 - 遣返志願軍
下载
下载
下載


歷史上的今天0122第267期 - 梁羽生
下载
下载
下載


歷史上的今天0121第266期 - 人大确定教师节
下载
下载
下載


《週末漫談》20190119第9期
下载
下载
下載


歷史上的今天0118第265期 - 伯纳诺
下载
下载
下載


歷史上的今天0117第264期 - 赵紫阳
下载
下载
下載


歷史上的今天0116第263期 - 海湾战争
下载
下载
下載


歷史上的今天0115第262期 - 萨利机长
下载
下载
下載


《週末漫談》20190113第8期
下载
下载
下載


歷史上的今天0114第261期 - 四清運動
下载
下载
下載


《週末漫談》20190112第7期
下载
下载
下載


歷史上的今天20190111第260期 - 沙龍
下载
下载
下載


歷史上的今天20190110第259期 - 托馬斯·潘恩
下载
下载
下載


歷史上的今天20190109第258期 - 伊麗莎白女王號
下载
下载
下載


歷史上的今天20190108第257期 - 史蒂芬·霍金
下载
下载
下載


歷史上的今天20190107第256期 - 尼古拉·特拉斯
下载
下载
下載


《週末漫談》20190105第6期
下载
下载
下載


歷史上的今天20190104第255期 - 費邊社
下载
下载
下載


歷史上的今天20190103第254期 - 艾偉德
下载
下载
下載


歷史上的今天20190102第253期 - 羅盛教
下载
下载
下載


歷史上的今天20190101第252期 - 歐元發行
下载
下载
下載


歷史上的今天20181210第236期 - 77年恢復高考
下载
下载
下載


歷史上的今天20181225第247期 - 齊奧塞斯庫
下载
下载
下載


歷史上的今天20181226第248期 - 蘇聯解體
下载
下载
下載


歷史上的今天20181231第251期 - 維也納新年音樂
下载
下载
下載


历史上的今天1203 第231期 - 庚子赔款
下载
下载
下載


歷史上的今天20181113 第217期 - 克林德牌坊
下载
下载
下載


历史上的今天20181116第220期 - 郭沫若
下载
下载
下載


历史上的今天20181029第206期 - 纳粹宣传部长戈培尔
下载
下载
下載


歷史上的今天20180720第135期 - 迫害法轮功
下载
下载
下載


歷史上的今天20180709第127期 - 709律师案
下载
下载
下載


历史上的今天20180615第111期 - 义和团围攻西什库教堂
下载
下载
下載


历史上的今天20180604第102期 - 八九六四屠城
下载
下载
下載



",5
yumi0629/FlutterUI,Dart,"flutter_ui
Stop Updating ！！
For help building amazing UI with Flutter.
VerificationCodeInput
教程：Flutter花式玩转TextField，写一个验证码输入框超简单！
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/verificationcode
效果图：


LiquidCheckButton
A button with liquid wave to show download progress.
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/liquidcheck
效果图：

CircleFloatingMenu
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/circlefloatingmenu
效果图：

LikeButton
A LikeButton like Twitter.
教程：用Flutter实现一个仿Twitter的点赞效果
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/likebutton
效果图：

SlideDrawer：一个仿QQ侧滑菜单栏控件
教程：Flutter：手把手教你实现一个仿QQ侧滑菜单的功能
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/slidedrawer
效果图：

Drag to choose like or dislike
一个仿“探探”的左右滑动选择喜欢/不喜欢的控件
教程：用Flutter实现一个仿“探探”的左右滑动选择喜欢/不喜欢的效果
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/draglike
效果图：

CircleProgressBar
教程：Flutter：教你用CustomPaint画一个自定义的CircleProgressBar
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/circleprogressbar
效果图：

Tip Menu
Just a demo. Not for project use !!!
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/tipmenu

Scrawl & Watermark
A scrawl & adding watermark demo.
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/scrawl


Hero
教程：谈一谈Flutter中的共享元素动画Hero
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/sharedelement
Slivers
教程：Flutter：Slivers大家族，让滑动视图的组合变得很简单！
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/sliver
",333
Moocowsgomoo/StS-ConstructMod,Java,"The Construct
Adds a new character for Slay The Spire, complete with 75 new cards and 8 new relics.

On reaching the first unlock level, you'll unlock an alternate starting relic in the ""Mods"" tab of the game's main menu.
Right now all cards and relics are unlocked at the start, so subsequent unlocks don't do anything.

The Construct is a mix of the many robotic enemies you'll find in the Spire, and uses familiar abilities like Mode Shift, Stasis, and of course the Hyper Beam. Its main strength is adaptability; the new Cycle keyword gives you some control over the composition of your hand at any given time, and your starter Mode Shift cards allow you to switch between attack and defense on the fly.
This character is meant to feel as close to an official character as possible, with multiple potential deck archetypes and rewarding interactions between cards. Balance is an ongoing process and cards/relics may change over time.
Card List

New Keyword: Cycle
Definition: When drawn, discard this and draw another card if the cycle condition is met. Only works once per card per turn.
Examples:

Flak Barrage: Cycle if your Strength < 1. Deal 0 damage to a random enemy 3 times.
Isolate: Cycle if there's more than one enemy. Your attacks deal double damage this turn.
Guard Orb: Cycle. When cycled, gain 2 Block.


Fanart

Art by Spike Berd (u/spikeof2010).
A huge thanks to anyone who's been inspired to create art based on the Construct Mod!
If you have any fanart you'd like to contribute, please let me know through Discord (Moocowsgomoo) or Reddit (u/StS-Moocowsgomoo).
Requirements
Copied from https://github.com/gskleres/FruityMod-StS
General Use

Java 8 (JRE). Currently only Java 8 should be used, issues with Java 9 are being looked into.
BaseMod v.2.10.0+ (https://github.com/daviscook477/BaseMod/releases)
ModTheSpire v2.6.0+ (https://github.com/kiooeht/ModTheSpire/releases)

Installation

If you have ModTheSpire already installed you can skip to step 5. Otherwise continue with step 2:
Download ModTheSpire.jar from the latest release (https://github.com/kiooeht/ModTheSpire/releases)
Move ModTheSpire.jar into your Slay The Spire directory. This directory is likely to be found under C:\Program Files (x86)\Steam\steamapps\common\SlayTheSpire. Place ModTheSpire.jar in that directory so it looks like C:\Program Files (x86)\Steam\steamapps\common\SlayTheSpire\ModTheSpire.jar
Create a mods folder in your Slay The Spire directory so it looks like C:\Program Files (x86)\Steam\steamapps\common\SlayTheSpire\mods
Download BaseMod.jar from the latest release (https://github.com/daviscook477/BaseMod/releases)
Move BaseMod.jar into the mods folder you created in step 4
Download ConstructMod.jar from the latest release (https://github.com/Moocowsgomoo/StS-ConstructMod/releases)
Move ConstructMod.jar into the mods folder you created in step 4
Your modded version of Slay The Spire can now be launched by double-clicking on ModTheSpire.jar
This will open a mod select menu where you need to make sure that both BaseMod and ConstructMod are checked before clicking play

Here is a great video showing how to install mods, by Xterminator: https://www.youtube.com/watch?v=r2m2aL1eEjw
Special Thanks

Thanks to the devs of SlayTheSpire for making such and awesome game, allowing us to mod it, and allowing us to use recolored versions of their art assets in our mod
Thanks to t-larson and contributors (https://github.com/t-larson) for BaseMod!!
Thanks to kiooeht and contributors (https://github.com/kiooeht) for ModTheSpire!!
Thanks to everyone in the Discord community who's played the mod and given feedback!

Other Mods
There are lots of fantastic mods that already exist for Slay the Spire. An updated list can be found at:

https://github.com/kiooeht/ModTheSpire/wiki/List-of-Known-Mods

",13
gfw-breaker/jiangfeng-subtitles,Shell,"《江峰时刻》节目字幕

友情链接：免翻墙看禁闻  |  手把手翻墙教程  | 《江峰时刻》会员网站



视频节目名称
视频/音频
简体字幕
正體字幕




歷史上的今天20190517第349期 - 梧桐樹協議
下载
下载
下載


歷史上的今天20190516第348期 - 抗日英雄張靈甫
下载
下载
下載


歷史上的今天20190515第347期 - 埃菲尔铁塔
下载
下载
下載


歷史上的今天特刊 - 解讀《白玫瑰傳單一》
下载
下载
下載


歷史上的今天20190514第346期 - 紅都女皇江青
下载
下载
下載


《週末漫談》20190512第24期
下载
下载
下載


《週末漫談》20190511第23期
下载
下载
下載


歷史上的今天20190510第345期 - 曼德拉
下载
下载
下載


歷史上的今天20190509第344期 - 永不凋零的白玫瑰
下载
下载
下載


歷史上的今天20190508第343期 - 美國轟炸中國駐南斯拉夫大使館
下载
下载
下載


歷史上的今天20190507第342期 - 報道水門事件
下载
下载
下載


歷史上的今天20190506第341期 - 《排華法案》
下载
下载
下載


《週末漫談》20190505第22期
下载
下载
下載


歷史上的今天20190503第340期 - 世界新闻自由日
下载
下载
下載


歷史上的今天20190502第339期 - 德国居民参观纳粹集中营
下载
下载
下載


歷史上的今天20190501第338期 - 擊斃本拉登
下载
下载
下載


歷史上的今天20190430第337期 - 路易斯安那
下载
下载
下載


歷史上的今天20190429第336期 - 盧作孚
下载
下载
下載


《週末漫談》20190428第21期
下载
下载
下載


歷史上的今天20190426第335期 - 切爾諾貝利
下载
下载
下載


歷史上的今天20190425第334期 - 425和平上访
下载
下载
下載


歷史上的今天20190424第333期 - SARS
下载
下载
下載


歷史上的今天20190423第332期 - 武訓
下载
下载
下載


歷史上的今天20190422第331期 - 林巧稚
下载
下载
下載


週末漫談》第20期20190421
下载
下载
下載


《中央情報局的紅色鼴鼠》（十二）他還活著
下载
下载
下載


历史上的今天20190419第330期 - 全民打麻雀
下载
下载
下載


歷史上的今天20190418第329期 - 紅色高棉
下载
下载
下載


歷史上的今天20190417第328期 - 富蘭克林
下载
下载
下載


歷史上的今天20190416第327期 - 列寧
下载
下载
下載


歷史上的今天20190415第326期 - 中美合作所
下载
下载
下載


《週末漫談》第19期20190414
下载
下载
下載


《中央情報局的紅色鼴鼠》（十一）風雲際會
下载
下载
下載


歷史上的今天20190412第325期 - 容閎
下载
下载
下載


歷史上的今天20190411第324期 - 愛麗絲島
下载
下载
下載


歷史上的今天20190410第323期 - 朱令中毒案
下载
下载
下載


歷史上的今天20190409第322期 - 波音
下载
下载
下載


歷史上的今天20190408第321期 - 李敦白
下载
下载
下載


《中央情報局的紅色鼴鼠》（十）紅色基因
下载
下载
下載


歷史上的今天20190405第320期 - 麥肯阿瑟
下载
下载
下載


歷史上的今天20190404第319期 - 現代奧運會
下载
下载
下載


歷史上的今天20190403第318期 - 馬歇爾計劃
下载
下载
下載


歷史上的今天20190402第317期 - 世界自閉症關注日
下载
下载
下載


歷史上的今天20190401第316期 - 中美南海撞機
下载
下载
下載


《週末漫談》20190330第18期
下载
下载
下載


《中央情報局的紅色鼴鼠》（九） 顯赫家族
下载
下载
下載


歷史上的今天20190329第315期 - 俄羅斯反華
下载
下载
下載


歷史上的今天20190328第314期 - 艾森豪威爾
下载
下载
下載


歷史上的今天20190327第313期 - 赫鲁晓夫
下载
下载
下載


歷史上的今天20190326第312期 - 鎮反
下载
下载
下載


歷史上的今天20190325第311期 - 麥大志間諜案
下载
下载
下載


《週末漫談》20190323第17期
下载
下载
下載


《中央情報局的紅色鼴鼠》（八）卸磨殺驢
下载
下载
下載


歷史上的今天20190322第310期 - 《鞍鋼憲法》
下载
下载
下載


歷史上的今天20190321第309期 - 最後的日本武士
下载
下载
下載


歷史上的今天20190320第308期 - 孫志剛案
下载
下载
下載


歷史上的今天20190319第307期 - 《歐陽海之歌》
下载
下载
下載


歷史上的今天20190318第306期 - 巴黎公社
下载
下载
下載


《週末漫談》20190316第16期
下载
下载
下載


《中央情報局的紅色鼴鼠》（七）东窗事发
下载
下载
下載


歷史上的今天20190315第305期 - 消費者權益日
下载
下载
下載


歷史上的今天20190314第304期 - 愛因斯坦
下载
下载
下載


歷史上的今天20190313第303期 - 哈佛大学
下载
下载
下載


歷史上的今天20190312第302期 - 可口可樂
下载
下载
下載


歷史上的今天20190311第301期 - 高智晟和他的家
下载
下载
下載


《週末漫談》20190309第15期
下载
下载
下載


《中央情報局的紅色鼴鼠》（六）諜影重重
下载
下载
下載


歷史上的今天20190308第300期 - 三八婦女節
下载
下载
下載


歷史上的今天20190307第299期 - 亨利八世宗教改革
下载
下载
下載


歷史上的今天20190306第298期 - 斯維特蘭娜
下载
下载
下載


歷史上的今天20190305第297期 - 卡廷慘案
下载
下载
下載


歷史上的今天20190304第296期 - 黑船事件
下载
下载
下載


《週末漫談》20190303第14期
下载
下载
下載


《中央情報局的紅色鼴鼠》（五）- 戰俘密報
下载
下载
下載


歷史上的今天20190301第295期 - 黃石公園
下载
下载
下載


歷史上的今天20190228第294期 - 亨利·盧斯
下载
下载
下載


歷史上的今天20190227第293期 - LV 路易威登
下载
下载
下載


歷史上的今天20190226第292期 - 從海瑞罷官到八大樣板戲
下载
下载
下載


歷史上的今天20190225第291期 - 中國遠征軍之殤
下载
下载
下載


《週末漫談》20190223第13期
下载
下载
下載


《中情局的紅色鼴鼠》（四）
下载
下载
下載


歷史上的今天20190222第290期 - 和珅
下载
下载
下載


歷史上的今天20190221第289期 - 聖女貞德
下载
下载
下載


歷史上的今天20190220第288期 - 《天鵝湖》首演
下载
下载
下載


歷史上的今天20190219第287期 - 二戰美國關押日裔美國人
下载
下载
下載


歷史上的今天20190218第286期 - 援外八項原則
下载
下载
下載


《週末漫談》20190216第12期
下载
下载
下載


江峰劇場：中央情報局的紅色鼴鼠（三）
下载
下载
下載


歷史上的今天20190215第285期 - 末日忠臣李鴻章
下载
下载
下載


歷史上的今天20190214第284期 - YouTube创立
下载
下载
下載


歷史上的今天20190213第283期 - 澳洲被偷走的一代人
下载
下载
下載


歷史上的今天20190212第282期 - 三鹿毒奶粉
下载
下载
下載


歷史上的今天20190210第281期 - 莊則棟與乒乓外交
下载
下载
下載


《週末漫談》20190210第11期
下载
下载
下載


江峰劇場：《中情局的紅色鼴鼠》（二）
下载
下载
下載


歷史上的今天20190208第280期 - 春晚來歷
下载
下载
下載


歷史上的今天20190207第279期 - “天运号” 越南船民投奔香港
下载
下载
下載


歷史上的今天20190206第278期 - 王立軍夜闖美領館
下载
下载
下載


歷史上的今天20190205第277期 - 《讀者文摘》
下载
下载
下載


歷史上的今天20190204第276期 - 查韋斯政變
下载
下载
下載


《週末漫談》20190202第10期
下载
下载
下載


江峰劇場：《中情局的紅色鼴鼠》（一）
下载
下载
下載


歷史上的今天20190201第275期 - 延安整風
下载
下载
下載


歷史上的今天20190131第274期 - 漢字簡化
下载
下载
下載


歷史上的今天20190130第273期 - 羅密歐與茱麗葉
下载
下载
下載


歷史上的今天0129第272期 - 鄧麗君
下载
下载
下載


歷史上的今天0128第271期 - 鄧小平訪美
下载
下载
下載


歷史上的今天0125第270期 - 古拉格勞改營
下载
下载
下載


歷史上的今天0124第269期 - 黃埔军校
下载
下载
下載


歷史上的今天0123第268期 - 遣返志願軍
下载
下载
下載


歷史上的今天0122第267期 - 梁羽生
下载
下载
下載


歷史上的今天0121第266期 - 人大确定教师节
下载
下载
下載


《週末漫談》20190119第9期
下载
下载
下載


歷史上的今天0118第265期 - 伯纳诺
下载
下载
下載


歷史上的今天0117第264期 - 赵紫阳
下载
下载
下載


歷史上的今天0116第263期 - 海湾战争
下载
下载
下載


歷史上的今天0115第262期 - 萨利机长
下载
下载
下載


《週末漫談》20190113第8期
下载
下载
下載


歷史上的今天0114第261期 - 四清運動
下载
下载
下載


《週末漫談》20190112第7期
下载
下载
下載


歷史上的今天20190111第260期 - 沙龍
下载
下载
下載


歷史上的今天20190110第259期 - 托馬斯·潘恩
下载
下载
下載


歷史上的今天20190109第258期 - 伊麗莎白女王號
下载
下载
下載


歷史上的今天20190108第257期 - 史蒂芬·霍金
下载
下载
下載


歷史上的今天20190107第256期 - 尼古拉·特拉斯
下载
下载
下載


《週末漫談》20190105第6期
下载
下载
下載


歷史上的今天20190104第255期 - 費邊社
下载
下载
下載


歷史上的今天20190103第254期 - 艾偉德
下载
下载
下載


歷史上的今天20190102第253期 - 羅盛教
下载
下载
下載


歷史上的今天20190101第252期 - 歐元發行
下载
下载
下載


歷史上的今天20181210第236期 - 77年恢復高考
下载
下载
下載


歷史上的今天20181225第247期 - 齊奧塞斯庫
下载
下载
下載


歷史上的今天20181226第248期 - 蘇聯解體
下载
下载
下載


歷史上的今天20181231第251期 - 維也納新年音樂
下载
下载
下載


历史上的今天1203 第231期 - 庚子赔款
下载
下载
下載


歷史上的今天20181113 第217期 - 克林德牌坊
下载
下载
下載


历史上的今天20181116第220期 - 郭沫若
下载
下载
下載


历史上的今天20181029第206期 - 纳粹宣传部长戈培尔
下载
下载
下載


歷史上的今天20180720第135期 - 迫害法轮功
下载
下载
下載


歷史上的今天20180709第127期 - 709律师案
下载
下载
下載


历史上的今天20180615第111期 - 义和团围攻西什库教堂
下载
下载
下載


历史上的今天20180604第102期 - 八九六四屠城
下载
下载
下載



",5
yumi0629/FlutterUI,Dart,"flutter_ui
Stop Updating ！！
For help building amazing UI with Flutter.
VerificationCodeInput
教程：Flutter花式玩转TextField，写一个验证码输入框超简单！
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/verificationcode
效果图：


LiquidCheckButton
A button with liquid wave to show download progress.
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/liquidcheck
效果图：

CircleFloatingMenu
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/circlefloatingmenu
效果图：

LikeButton
A LikeButton like Twitter.
教程：用Flutter实现一个仿Twitter的点赞效果
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/likebutton
效果图：

SlideDrawer：一个仿QQ侧滑菜单栏控件
教程：Flutter：手把手教你实现一个仿QQ侧滑菜单的功能
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/slidedrawer
效果图：

Drag to choose like or dislike
一个仿“探探”的左右滑动选择喜欢/不喜欢的控件
教程：用Flutter实现一个仿“探探”的左右滑动选择喜欢/不喜欢的效果
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/draglike
效果图：

CircleProgressBar
教程：Flutter：教你用CustomPaint画一个自定义的CircleProgressBar
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/circleprogressbar
效果图：

Tip Menu
Just a demo. Not for project use !!!
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/tipmenu

Scrawl & Watermark
A scrawl & adding watermark demo.
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/scrawl


Hero
教程：谈一谈Flutter中的共享元素动画Hero
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/sharedelement
Slivers
教程：Flutter：Slivers大家族，让滑动视图的组合变得很简单！
code：https://github.com/yumi0629/FlutterUI/tree/master/lib/sliver
",333
Moocowsgomoo/StS-ConstructMod,Java,"The Construct
Adds a new character for Slay The Spire, complete with 75 new cards and 8 new relics.

On reaching the first unlock level, you'll unlock an alternate starting relic in the ""Mods"" tab of the game's main menu.
Right now all cards and relics are unlocked at the start, so subsequent unlocks don't do anything.

The Construct is a mix of the many robotic enemies you'll find in the Spire, and uses familiar abilities like Mode Shift, Stasis, and of course the Hyper Beam. Its main strength is adaptability; the new Cycle keyword gives you some control over the composition of your hand at any given time, and your starter Mode Shift cards allow you to switch between attack and defense on the fly.
This character is meant to feel as close to an official character as possible, with multiple potential deck archetypes and rewarding interactions between cards. Balance is an ongoing process and cards/relics may change over time.
Card List

New Keyword: Cycle
Definition: When drawn, discard this and draw another card if the cycle condition is met. Only works once per card per turn.
Examples:

Flak Barrage: Cycle if your Strength < 1. Deal 0 damage to a random enemy 3 times.
Isolate: Cycle if there's more than one enemy. Your attacks deal double damage this turn.
Guard Orb: Cycle. When cycled, gain 2 Block.


Fanart

Art by Spike Berd (u/spikeof2010).
A huge thanks to anyone who's been inspired to create art based on the Construct Mod!
If you have any fanart you'd like to contribute, please let me know through Discord (Moocowsgomoo) or Reddit (u/StS-Moocowsgomoo).
Requirements
Copied from https://github.com/gskleres/FruityMod-StS
General Use

Java 8 (JRE). Currently only Java 8 should be used, issues with Java 9 are being looked into.
BaseMod v.2.10.0+ (https://github.com/daviscook477/BaseMod/releases)
ModTheSpire v2.6.0+ (https://github.com/kiooeht/ModTheSpire/releases)

Installation

If you have ModTheSpire already installed you can skip to step 5. Otherwise continue with step 2:
Download ModTheSpire.jar from the latest release (https://github.com/kiooeht/ModTheSpire/releases)
Move ModTheSpire.jar into your Slay The Spire directory. This directory is likely to be found under C:\Program Files (x86)\Steam\steamapps\common\SlayTheSpire. Place ModTheSpire.jar in that directory so it looks like C:\Program Files (x86)\Steam\steamapps\common\SlayTheSpire\ModTheSpire.jar
Create a mods folder in your Slay The Spire directory so it looks like C:\Program Files (x86)\Steam\steamapps\common\SlayTheSpire\mods
Download BaseMod.jar from the latest release (https://github.com/daviscook477/BaseMod/releases)
Move BaseMod.jar into the mods folder you created in step 4
Download ConstructMod.jar from the latest release (https://github.com/Moocowsgomoo/StS-ConstructMod/releases)
Move ConstructMod.jar into the mods folder you created in step 4
Your modded version of Slay The Spire can now be launched by double-clicking on ModTheSpire.jar
This will open a mod select menu where you need to make sure that both BaseMod and ConstructMod are checked before clicking play

Here is a great video showing how to install mods, by Xterminator: https://www.youtube.com/watch?v=r2m2aL1eEjw
Special Thanks

Thanks to the devs of SlayTheSpire for making such and awesome game, allowing us to mod it, and allowing us to use recolored versions of their art assets in our mod
Thanks to t-larson and contributors (https://github.com/t-larson) for BaseMod!!
Thanks to kiooeht and contributors (https://github.com/kiooeht) for ModTheSpire!!
Thanks to everyone in the Discord community who's played the mod and given feedback!

Other Mods
There are lots of fantastic mods that already exist for Slay the Spire. An updated list can be found at:

https://github.com/kiooeht/ModTheSpire/wiki/List-of-Known-Mods

",13
microsoft/pxt-common-packages,TypeScript,"MakeCode Common packages
A set of packages used in various MakeCode editors such as https://makecode.adafruit.com, https://maker.makecode.com, https://makecode.microbit.org, https://makecode.mindstorms.com, etc...


Contributing
This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.
",21
Realank/flutter_datetime_picker,Dart,"Flutter Datetime Picker
(Pub) flutter_datetime_picker
A flutter date time picker inspired by flutter-cupertino-date-picker
you can choose date / time / date&time in multiple languages:

English(en)
Chinese(zh)
Dutch(nl)
Russian(ru)
Italian(it)
French(fr)
Spanish(es)
Polish (pl)
Portuguese(pt)
Korean(ko)
Arabic(ar)
Turkish(tr)
Japanese(jp)

and you can also custom your own picker content



Date picker
Time picker
Date Time picker










International:



Date Time picker (Chinese)
Date Time picker (America)
Date Time picker (Dutch)
Date Time picker (Russian)











Usage
FlatButton(
    onPressed: () {
        DatePicker.showDatePicker(context,
                              showTitleActions: true,
                              minTime: DateTime(2018, 3, 5),
                              maxTime: DateTime(2019, 6, 7), onChanged: (date) {
                            print('change $date');
                          }, onConfirm: (date) {
                            print('confirm $date');
                          }, currentTime: DateTime.now(), locale: LocaleType.zh);
    },
    child: Text(
        'show date time picker (Chinese)',
        style: TextStyle(color: Colors.blue),
    ));

Custom
If you want to customize your own style of date time picker, there is a class called CommonPickerModel, every type of date time picker is extended from this class, you can refer to other picker model (eg. DatePickerModel), and write your custom one, then pass this model to showPicker method, so that your own date time picker will appear, it’s easy, and will perfectly meet your demand
Getting Started
For help getting started with Flutter, view our online documentation.
For help on editing package code, view the documentation.
",88
haosen9527/mobileNet-ssd,C++,"



ABOUT

mobilenet ssd
tensorflow /c++/python
catkin_make

INSTALL
 cd mobileNet-ssd
 catkin_make
RUN

tensorflow c++
rosrun mobileNet mobileNet_pb
or
./devel/lib/mobileNet/mobileNet_pb

opencv dnn
rosrun mobileNet mobileNet_opencv
or
./devel/lib/mobileNet/mobileNet_opencv

python
python src/mobileNet/python/ssd-mobilenet.py


tensorflow模型预测对比总结
将通过以下的方法进行模型预测部分的实现

Tensorflow c++
Tensorflow python
Opencv dnn

对比分析内容

图片输入说明(图片加载)

Tensorflow c++ ：
tensorflow ops::ReadFile/DecodePng

Tensorflow python ：
load_image_into_numpy_array(numpy)

Opencv dnn :
cv::imread(imagePath)



模型加载说明

Tensorflow c++ ：
Status status = ReadBinaryProto(Env::Default(),MODEL_PATH,&graph_def);

Tensorflow python:
with gfile.FastGFile(PATH_TO_PB) as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    sess.graph.as_default()
    tf.import_graph_def(graph_def,name='')
or 
with open(PATH_TO_PB) as f:
    graph_def = tf.GraphDef()
    .......

opencv DNN ：
dnn::Net net = cv::dnn::readNetFromTensorflow(weights, prototxt);




时间对比：

Opencvdnn ：detection time: 618.522 ms
Tensorflow c++ ：detection time:699.195 ms
Tensorflow python : detection time : 5916.458 ms

效果展示

Tensorflow c++ 

Tensorflow python 

Opencv dnn 


",2
snapview/tokio-tungstenite,Rust,"tokio-tungstenite
Asynchronous WebSockets for Tokio stack.



Documentation
Usage
First, you need to add this in your Cargo.toml:
[dependencies]
tokio-tungstenite = ""*""
Next, add this to your crate:
extern crate tokio_tungstenite;
Take a look at the examples/ directory for client and server examples. You may also want to get familiar with
tokio if you don't have any experience with it.
What is tokio-tungstenite?
This crate is based on tungstenite-rs Rust WebSocket library and provides tokio bindings and wrappers for it, so you
can use it with non-blocking/asynchronous TcpStreams from and couple it together with other crates from tokio stack.
",101
cdfarrow/flextools,Python,"FLExTools
FLExTools is a Python scripting utility for SIL FieldWorks Language Explorer (FLEx)


Wiki


Latest version


",6
bheisler/TinyTemplate,Rust,"TinyTemplate
Minimal Lightweight Text Templating

API Documentation
    |
    Changelog





    |
    



TinyTemplate is a small, minimalistic text templating system with limited dependencies.
Table of Contents

Table of Contents

Goals
Why TinyTemplate?
Quickstart
Compatibility Policy
Contributing
Maintenance
License



Goals
The primary design goals are:

Small: TinyTemplate deliberately does not support many features of more powerful template engines.
Simple: TinyTemplate presents a minimal but well-documented user-facing API.
Lightweight: TinyTemplate has minimal required dependencies.

Non-goals include:

Extensibility: TinyTemplate supports custom value formatters, but that is all.
Performance: TinyTemplate provides decent performance, but other template engines are faster.

Why TinyTemplate?
I created TinyTemplate after noticing that none of the existing template libraries really suited my
needs for Criterion.rs. Some had large dependency trees to support features that I didn't use. Some
required adding a build script to convert templates into code at runtime, in search of extreme
performance that I didn't need. Some had elaborate macro-based DSL's to generate HTML, where I just
wanted plain text with some markup. Some expect the templates to be provided in a directory of text
files, but I wanted the template to be included in the binary. I just wanted something small and
minimal with good documentation but there was nothing like that out there so I wrote my own.
TinyTemplate is well-suited to generating HTML reports and similar text files. It could be used for
generating HTML or other text in a web-server, but for more-complex use cases another template
engine may be a better fit.
Quickstart
First, add TinyTemplate and serde-derive to your Cargo.toml file:
[dependencies]
tinytemplate = ""1.0""
serde_derive = ""1.0""
Then add this code to ""src.rs"":
#[macro_use]
extern crate serde_derive;
extern crate tinytemplate;

use tinytemplate::TinyTemplate;
use std::error::Error;

#[derive(Serialize)]
struct Context {
    name: String,
}

static TEMPLATE : &'static str = ""Hello {name}!"";

pub fn main() -> Result<(), Box<dyn Error>> {
    let mut tt = TinyTemplate::new();
    tt.add_template(""hello"", TEMPLATE)?;

    let context = Context {
        name: ""World"".to_string(),
    };

    let rendered = tt.render(""hello"", &context)?;
    println!(""{}"", rendered);

    Ok(())
}
This should print ""Hello World!"" to stdout.
Compatibility Policy
TinyTemplate supports the last three stable minor releases of Rust. At time of writing, this means
Rust 1.29 or later. Older versions may work, but are not tested or guaranteed.
Currently, the oldest version of Rust believed to work is 1.26. Future versions of TinyTemplate may
break support for such old versions, and this will not be considered a breaking change. If you
require TinyTemplate to work on old versions of Rust, you will need to stick to a
specific patch version of TinyTemplate.
Contributing
Thanks for your interest! Contributions are welcome.
Issues, feature requests, questions and bug reports should be reported via the issue tracker above.
In particular, becuase TinyTemplate aims to be well-documented, please report anything you find
confusing or incorrect in the documentation.
Code or documentation improvements in the form of pull requests are also welcome. Please file or
comment on an issue to allow for discussion before doing a lot of work, though.
For more details, see the CONTRIBUTING.md file.
Maintenance
TinyTemplate was created and is currently maintained by Brook Heisler (@bheisler).
License
TinyTemplate is dual-licensed under the Apache 2.0 license and the MIT license.
",24
SciSharp/TensorFlow.NET,C#,"TensorFlow.NET
TensorFlow.NET (TF.NET) provides a .NET Standard binding for TensorFlow. It aims to implement the complete Tensorflow API in CSharp which allows .NET developers to develop, train and deploy Machine Learning models with the cross-platform .NET Standard framework.






TF.NET is a member project of SciSharp STACK.

Why TensorFlow.NET ?
SciSharp STASK's mission is to bring popular data science technology into the .NET world and to provide .NET developers with a powerful Machine Learning tool set without reinventing the wheel. Scince the APIs are kept as similar as possible you can immediately adapt any existing Tensorflow code in C# with a zero learning curve. Take a look at a comparison picture and see how comfortably a   Tensorflow/Python script translates into a C# program with TensorFlow.NET.

SciSharp's philosophy allows a large number of machine learning code written in Python to be quickly migrated to .NET, enabling .NET developers to use cutting edge machine learning models and access a vast number of Tensorflow resources which would not be possible without this project.
In comparison to other projects, like for instance TensorFlowSharp which only provide Tensorflow's low-level C++ API and can only run models that were built using Python, Tensorflow.NET also implements Tensorflow's high level API where all the magic happens. This computation graph building layer is still under active development. Once it is completely implemented you can build new Machine Learning models in C#.
How to use
Install TF.NET through NuGet.
PM> Install-Package TensorFlow.NET
If you are using Linux or Mac OS, please download the pre-compiled dll here and place it in the working folder. This is only need for Linux and Mac OS, and already packed into NuGet for Windows.
Import TF.NET.
using Tensorflow;
Add two constants:
// Create a Constant op
var a = tf.constant(4.0f);
var b = tf.constant(5.0f);
var c = tf.add(a, b);

using (var sess = tf.Session())
{
    var o = sess.run(c);
}
Feed placeholder:
// Create a placeholder op
var a = tf.placeholder(tf.float32);
var b = tf.placeholder(tf.float32);
var c = tf.add(a, b);

using(var sess = tf.Session())
{
    var o = sess.run(c, new FeedItem(a, 3.0f), new FeedItem(b, 2.0f));
}
Linear Regression:
// We can set a fixed init value in order to debug
var W = tf.Variable(-0.06f, name: ""weight"");
var b = tf.Variable(-0.73f, name: ""bias"");

// Construct a linear model
var pred = tf.add(tf.multiply(X, W), b);

// Mean squared error
var cost = tf.reduce_sum(tf.pow(pred - Y, 2.0f)) / (2.0f * n_samples);

// Gradient descent
// Note, minimize() knows to modify W and b because Variable objects are trainable=True by default
var optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost);

// Initialize the variables (i.e. assign their default value)
var init = tf.global_variables_initializer();

// Start training
with(tf.Session(), sess => 
{
    // Run the initializer
    sess.run(init);
    
    // Fit all training data
    for (int epoch = 0; epoch < training_epochs; epoch++)
    {
        foreach (var (x, y) in zip<float>(train_X, train_Y))
            sess.run(optimizer, new FeedItem(X, x), new FeedItem(Y, y));
        
        // Display logs per epoch step
        if ((epoch + 1) % display_step == 0)
        {
            var c = sess.run(cost, new FeedItem(X, train_X), new FeedItem(Y, train_Y));
            Console.WriteLine($""Epoch: {epoch + 1} cost={c} "" + $""W={sess.run(W)} b={sess.run(b)}"");
        }
        
        Console.WriteLine(""Optimization Finished!"");
        var training_cost = sess.run(cost, new FeedItem(X, train_X), new FeedItem(Y, train_Y));
        Console.WriteLine($""Training cost={training_cost} W={sess.run(W)} b={sess.run(b)}"");
        
        // Testing example
        var test_X = np.array(6.83f, 4.668f, 8.9f, 7.91f, 5.7f, 8.7f, 3.1f, 2.1f);
        var test_Y = np.array(1.84f, 2.273f, 3.2f, 2.831f, 2.92f, 3.24f, 1.35f, 1.03f);
        Console.WriteLine(""Testing... (Mean square loss Comparison)"");
        
        var testing_cost = sess.run(tf.reduce_sum(tf.pow(pred - Y, 2.0f)) / (2.0f * test_X.shape[0]), new FeedItem(X, test_X), new FeedItem(Y, test_Y));
        Console.WriteLine($""Testing cost={testing_cost}"");
        
        var diff = Math.Abs((float)training_cost - (float)testing_cost);
        Console.WriteLine($""Absolute mean square loss difference: {diff}"");
    }
});
Read the docs & book The Definitive Guide to Tensorflow.NET.
More examples:


Hello World


Basic Operations


Linear Regression


Logistic Regression


Nearest Neighbor


Naive Bayes Classification


Image Recognition


K-means Clustering


NN XOR


Object Detection


Text Classification


CNN Text Classification


Named Entity Recognition


Contribute:
Feel like contributing to one of the hottest projects in the Machine Learning field? Want to know how Tensorflow magically creates the computational graph? We appreciate every contribution however small. There are tasks for novices to experts alike, if everyone tackles only a small task the sum of contributions will be huge.
You can:

Let everyone know about this project (trivial)
Port Tensorflow unit tests from Python to C# (easy)
Port missing Tensorflow code from Python to C# (easy)
Port Tensorflow examples to C# and raise issues if you come accross missing parts of the API (easy)
Debug one of the unit tests that is marked as Ignored to get it to work (can be challenging)
Debug one of the not yet working examples and get it to work (hard)

How to debug unit tests:
The best way to find out why a unit test is failing is to single step it in C# and its pendant Python at the same time to see where the flow of execution digresses or where variables exhibit different values. Good Python IDEs like PyCharm let you single step into the tensorflow library code.
Git Knowhow for Contributors
Add SciSharp/TensorFlow.NET as upstream to your local repo ...
git remote add upstream git@github.com:SciSharp/TensorFlow.NET.git

Please make sure you keep your fork up to date by regularly pulling from upstream.
git pull upstream master

Contact
Feel free to star or raise issue on Github.
Join our chat on Gitter or
Scan QR code to join Tencent TIM group:

",365
GuaiYiHu/android_device_xiaomi_violet,C++,"Copyright (C) 2019 The MoKee Open Source Project
The Redmi Note 7 Pro (codenamed ""violet"") are high-end mid-range smartphones from Xiaomi.
Xiaomi Redmi Note 7 Pro was announced and released in March 2019.
Device specifications



Device
Xiaomi Redmi Note 7 Pro




SoC
Qualcomm SM6150 Snapdragon 675


CPU
8x Qualcomm® Kryo™ 460 up to 2.0GHz


GPU
Adreno 612


Memory
6GM RAM (LPDDR4X)


Shipped Android version
9.0.0


Storage
128GB eMMC 5.1 flash storage


Battery
Non-removable Li-Po 4000 mAh


Dimensions
159.21 x 75.21 x 8.1 mm


Display
2340 x 1080 (19.5:9), 6.3  inch


Rear camera 1
12MP, f/1.79 Dual LED flash


Rear camera 2
5MP, f/2.4


Front camera
13MP, f/2.2 1080p 30 fps video



Device picture

",13
wangzaiaaa/implement-some-algorithms-and-data-structure,Java,"implement-some-algorithms-and-data-structure
实现一些排序算法并优化，实现一些数据结构，然后还有是刷题的代码
",3
amangup/data-analysis-bootcamp,Jupyter Notebook,"data-analysis-bootcamp
Lectures and assignments to learn data analysis using SQL and Python
",6
Litres/FB3Editor,JavaScript,"FB3Editor
FB3Editor - open source online eBook editor
",13
dotnet/docfx,C#,"Generate your API documentation with DocFX




What is it
DocFX makes it extremely easy to generate your developer hub with API reference, landing page, and how-to.
What's next
Check out the road map of DocFX here.

NOTE:
For more information on DocFX v3, please visit the v3 working branch.

How to use

Option 1: install DocFX through chocolatey package: choco install docfx -y.
Option 2: install DocFX through nuget package: nuget install docfx.console, docfx.exe is under folder docfx.console/tools/.
Option 3: play DocFX inside Visual Studio: create a Class Library (.NET Framework) project, Manage Nuget Packages to install docfx.console nuget package on the project, Build to create the generated website under folder _site.

For more information, please refer to Getting Started.
How to Contribute
For new comers, you can start with issues with help-wanted. Check out the contributing page to see the best places to log issues and start discussions.
This project has adopted the code of conduct defined by the Contributor Covenant to clarify expected behavior in our community.
For more information see the .NET Foundation Code of Conduct.
License
DocFX is licensed under the MIT license.
.NET Foundation
DocFX is supported by the .NET Foundation.
DocFX Project
dev is the default branch accepting Pull Requests. It releases a package daily. master branch is the release branch.
How to build from source code
Prerequisites

Visual Studio 2017 with .NET Core cross-platform development toolset
Node.js

Steps

Option 1: Run build.cmd under DocFX code repo.
Option 2: Open All.sln under DocFX code repo in Visual Studio and build All.sln.

Build Status



master
dev









Packages



Chocolatey
Nuget
Nightly Build










Running Status



Windows with .NET Core v2.0.3
Windows with VS2017
Ubuntu Linux with Mono










",1705
apple/swift,C++,"
Swift Programming Language




Architecture
Master
Package




macOS
x86_64




Ubuntu 14.04
x86_64




Ubuntu 16.04
x86_64




Ubuntu 18.04
x86_64





Swift Community-Hosted CI Platforms



OS
Architecture
Build




Debian 9.1 (Raspberry Pi)
ARMv7



Fedora 27
x86_64



Ubuntu 16.04
x86_64



Ubuntu 16.04 
PPC64LE



Ubuntu 16.04 
AArch64



Android
ARMv7



Android
AArch64



Ubuntu 16.04 (TensorFlow)
x86_64



macOS 10.13 (TensorFlow)
x86_64



Ubuntu 16.04 (TensorFlow with GPU)
x86_64



Debian 9.5
x86_64



Windows 2019
x86_64




Welcome to Swift
Swift is a high-performance system programming language.  It has a clean
and modern syntax, offers seamless access to existing C and Objective-C code
and frameworks, and is memory safe by default.
Although inspired by Objective-C and many other languages, Swift is not itself a
C-derived language. As a complete and independent language, Swift packages core
features like flow control, data structures, and functions, with high-level
constructs like objects, protocols, closures, and generics. Swift embraces
modules, eliminating the need for headers and the code duplication they entail.
To learn more about the programming language, visit swift.org.
Contributing to Swift
Contributions to Swift are welcomed and encouraged! Please see the
Contributing to Swift guide.
To be a truly great community, Swift.org needs to welcome
developers from all walks of life, with different backgrounds, and with a wide
range of experience. A diverse and friendly community will have more great
ideas, more unique perspectives, and produce more great code. We will work
diligently to make the Swift community welcoming to everyone.
To give clarity of what is expected of our members, Swift has adopted the
code of conduct defined by the Contributor Covenant. This document is used
across many open source communities, and we think it articulates our values
well. For more, see the Code of Conduct.
Getting Started
These instructions give the most direct path to a working Swift development
environment. To build from source you will need about 2 GB of disk space for the
source code and up to 70 GB of disk space for the build artifacts with full
debugging. Depending on your machine, a clean build can take a few minutes to
several hours. Naturally, incremental builds are much faster.
System Requirements
macOS, Ubuntu Linux LTS, and the latest Ubuntu Linux release are the current
supported host development operating systems.
Please make sure you use Python 2.x. Python 3.x is not supported currently.
macOS
To build for macOS, you need Xcode 10.2 beta.
The required version of Xcode changes frequently, and is often a beta release.
Check this document or the host information on https://ci.swift.org for the
current required version.
You will also need CMake and Ninja,
which can be installed via a package manager:
Homebrew
brew install cmake ninja

MacPorts
sudo port install cmake ninja

Instructions for installing CMake and Ninja directly can be found below.
Linux
For Ubuntu, you'll need the following development dependencies:
sudo apt-get install git cmake ninja-build clang python uuid-dev libicu-dev icu-devtools libbsd-dev libedit-dev libxml2-dev libsqlite3-dev swig libpython-dev libncurses5-dev pkg-config libblocksruntime-dev libcurl4-openssl-dev systemtap-sdt-dev tzdata rsync

Note: LLDB currently requires at least swig-1.3.40 but will successfully build
with version 2 shipped with Ubuntu.
Build instructions for Ubuntu 14.04 LTS can be found here.
Getting Sources for Swift and Related Projects
First create a directory for all of the Swift sources:
mkdir swift-source
cd swift-source

Note: This is important since update-checkout (see below) checks out
repositories next to the Swift source directory. This means that if one clones
Swift and has other unrelated repositories, update-checkout may not clone those
repositories and will update them instead.
Via HTTPS  For those checking out sources as read-only, HTTPS works best:
git clone https://github.com/apple/swift.git
./swift/utils/update-checkout --clone

Via SSH  For those who plan on regularly making direct commits,
cloning over SSH may provide a better experience (which requires
uploading SSH keys to GitHub):
git clone git@github.com:apple/swift.git
./swift/utils/update-checkout --clone-with-ssh

Building Swift
The build-script is a high-level build automation script that supports basic
options such as building a Swift-compatible LLDB, building the Swift Package
Manager, building for various platforms, running tests after builds, and more.
There are two primary build systems to use: Xcode and Ninja. The Xcode build
system allows you to work in Xcode, but Ninja is a bit faster and supports
more environments.
To build using Ninja, run:
utils/build-script --release-debuginfo

When developing Swift, it helps to build what you're working on in a debug
configuration while building the rest of the project with optimizations. Below
are some examples of using debug variants:
utils/build-script --release-debuginfo --debug-swift # Swift frontend built in debug
utils/build-script --release-debuginfo --debug-swift-stdlib # Standard library built in debug
utils/build-script --release-debuginfo --debug-swift --force-optimized-typechecker # Swift frontend sans type checker built in debug

Limiting the amount of debug code in the compiler has a very large impact on
Swift compile times, and in turn the test execution time. If you want to build
the entire project in debug, you can run:
utils/build-script --debug

For documentation of all available arguments, as well as additional usage
information, see the inline help:
utils/build-script -h

Xcode
To build using Xcode, specify the --xcode argument on any of the above commands.
Xcode can be used to edit the Swift source code, but it is not currently
fully supported as a build environment for SDKs other than macOS. The generated
Xcode project does not integrate with the test runner, but the tests can be run
with the 'check-swift' target.
Build Products
All of the build products are placed in swift-source/build/${TOOL}-${MODE}/${PRODUCT}-${PLATFORM}/.
If macOS Swift with Ninja in DebugAssert mode was built, all of the products
would be in swift-source/build/Ninja-DebugAssert/swift-macosx-x86_64/. It
helps to save this directory as an environment variable for future use.
export SWIFT_BUILD_DIR=""~/swift-source/build/Ninja-DebugAssert/swift-macosx-x86_64""

Ninja
Once the first build has completed, Ninja can perform fast incremental builds of
various products. These incremental builds are a big timesaver when developing
and debugging.
cd ${SWIFT_BUILD_DIR}
ninja swift

This will build the Swift compiler, but will not rebuild the standard library or
any other target. Building the swift-stdlib target as an additional layer of
testing from time to time is also a good idea. To build just the standard
library, run:
ninja swift-stdlib

It is always a good idea to do a full build after using update-checkout.
Using Xcode
To open the Swift project in Xcode, open ${SWIFT_BUILD_DIR}/Swift.xcodeproj.
It will auto-create a lot of schemes for all of the available targets. A
common debug flow would involve:

Select the 'swift' scheme.
Pull up the scheme editor (⌘⇧<).
Select the 'Arguments' tab and click the '+'.
Add the command line options.
Close the scheme editor.
Build and run.

Another option is to change the scheme to ""Wait for executable to be launched"",
then run the build product in Terminal.
Swift Toolchains
Building
Swift toolchains are created using the script
build-toolchain. This
script is used by swift.org's CI to produce snapshots and can allow for one to
locally reproduce such builds for development or distribution purposes. E.x.:
  $ ./utils/build-toolchain $BUNDLE_PREFIX

where $BUNDLE_PREFIX is a string that will be prepended to the build
date to give the bundle identifier of the toolchain's Info.plist. For
instance, if $BUNDLE_PREFIX was com.example, the toolchain
produced will have the bundle identifier com.example.YYYYMMDD. It
will be created in the directory you run the script with a filename
of the form: swift-LOCAL-YYYY-MM-DD-a-osx.tar.gz.
Beyond building the toolchain, build-toolchain also supports the
following (non-exhaustive) set of useful options::

--dry-run: Perform a dry run build. This is off by default.
--test: Test the toolchain after it has been compiled. This is off by default.
--distcc: Use distcc to speed up the build by distributing the c++ part of
the swift build. This is off by default.

More options may be added over time. Please pass --help to
build-toolchain to see the full set of options.
Installing into Xcode
On macOS if one wants to install such a toolchain into Xcode:

Untar and copy the toolchain to one of /Library/Developer/Toolchains/ or
~/Library/Developer/Toolchains/. E.x.:

  $ sudo tar -xzf swift-LOCAL-YYYY-MM-DD-a-osx.tar.gz -C /
  $ tar -xzf swift-LOCAL-YYYY-MM-DD-a-osx.tar.gz -C ~/

The script also generates an archive containing debug symbols which
can be installed over the main archive allowing symbolication of any
compiler crashes.
  $ sudo tar -xzf swift-LOCAL-YYYY-MM-DD-a-osx-symbols.tar.gz -C /
  $ tar -xzf swift-LOCAL-YYYY-MM-DD-a-osx-symbols.tar.gz -C ~/


Specify the local toolchain for Xcode's use via Xcode->Toolchains.

Build Failures
Make sure you are using the correct release of Xcode.
If you have changed Xcode versions but still encounter errors that appear to
be related to the Xcode version, try passing --clean to build-script.
When a new version of Xcode is released, you can update your build without
recompiling the entire project by passing the --reconfigure option.
Make sure all repositories are up to date with the update-checkout command
described above.
Testing Swift
See docs/Testing.md, in particular the section on lit.py.
Learning More
Be sure to look through the docs
directory for more information about the compiler. In particular, the documents
titled Debugging the Swift Compiler and
Continuous Integration for Swift are very
helpful to understand before submitting your first PR.
Building Documentation
To read the compiler documentation, start by installing the
Sphinx documentation generator tool by running the
command:
easy_install -U ""Sphinx < 2.0""

Once complete, you can build the Swift documentation by changing directory into
docs and typing make. This
compiles the .rst files in the docs
directory into HTML in the docs/_build/html directory.
Many of the docs are out of date, but you can see some historical design
documents in the docs directory.
Another source of documentation is the standard library itself, located in
stdlib. Much of the language is actually implemented in the library
(including Int), and the standard library gives some examples of what can be
expressed today.
Build Dependencies
CMake
CMake is the core infrastructure used to configure builds of
Swift and its companion projects; at least version 3.4.3 is required.
On macOS, you can download the CMake Binary Distribution,
bundled as an application, copy it to /Applications, and add the embedded
command line tools to your PATH:
export PATH=/Applications/CMake.app/Contents/bin:$PATH

On Linux, if you have not already installed Swift's development
dependencies, you can download and install the CMake
package separately using the following command:
sudo apt-get install cmake

Ninja
Ninja is the current recommended build system
for building Swift and is the default configuration generated by CMake. Pre-built
packages
are available for macOS and Linux distributions. You can also clone Ninja
next to the other projects and it will be bootstrapped automatically:
Via HTTPS
git clone https://github.com/ninja-build/ninja.git && cd ninja
git checkout release
cat README

Via SSH
git clone git@github.com:ninja-build/ninja.git && cd ninja
git checkout release
cat README

",47737
Lombiq/Hastlayer-SDK,C#,"Hastlayer SDK Readme
Overview
Hastlayer - be the hardware. Automatically transforming .NET assemblies into computer chips to improve performance and lower power consumption.
Hastlayer uses FPGAs (chips that can be ""re-wired"" on the fly): You just need to select the compute-bound part of your .NET program and Hastlayer will seamlessly swap it out with a generated FPGA implementation. Since not C#, VisualBasic or other code but .NET Intermediate Language assemblies are transformed in theory any .NET language can be used, including C#, VB, F#, C++, Python, PHP, JavaScript...
Hastlayer was also featured on .NET Conf 2017; the recorded session covers most of what's interesting about Hastlayer (it's also on YouTube). Check out the FAQ for some more basic info.
This is the PC-side component of Hastlayer, the one that transforms .NET assemblies, programs attached FPGAs and communicates with said FPGAs.
Created by Lombiq Technologies, an open source .NET web development company.
Hastlayer uses ILSpy to process CIL assemblies and Orchard Application Host to utilize Orchard as the application framework.
Notes on Hastlayer's documentation
These text files should only serve as a starting point. On how to use Hastlayer the samples are the best source. The public API of Hastlayer is also documented inline as code comments, so make sure to check those out too if something's not clear. The projects also have further Readme files.
Table of contents

Getting started
Working with Hastlayer
Developing Hastlayer
Release notes
Roadmap
Support

Repositories and contributions
The project's source is available in two public source repositories, automatically mirrored in both directions with Git-hg Mirror:

https://bitbucket.org/Lombiq/hastlayer-sdk (Mercurial repository)
https://github.com/Lombiq/Hastlayer-SDK (Git repository)

(Note that due to a repository purge the repo history doesn't contain anything from before July 2017 though development has been ongoing more or less actively from 2015.)
Bug reports, feature requests and comments are warmly welcome, please do so via GitHub. Feel free to send pull requests too, no matter which source repository you choose for this purpose.
This project is developed by Lombiq Technologies Ltd. Commercial-grade support is available through Lombiq.
",85
KaizerHind/VisualTexture_Pawn_SA-MP,Visual Basic,"__________________________________ __________________________________
Changes made: 05 - May - 2019

A problem was solved in the variables, which did not allow VisualTexture to be closed completely, happening to run in the background.
The options for visualize objects, wikisamp and sounds, they were removed. (Possibly be added in another update)

Changes made: 24 - April - 2019

Fix of errors.

Changes made: 23 - April - 2019

The ZoneEditor tool was temporarily discarded.
Added SA-MP Script Tool.
Mobility and displacement.
Options and others.

Changes made: 22 - April - 2019

Minimal adjustments were corrected.
Added the PawnColor Picker tool. (Zume)
Mobility and displacement.

Changes made: 17 - April - 2019

Minimal adjustments were corrected.
Added the SPRITES form made by Lorenc.
It was added correctly and corrected data.

Changes made: 15 - April - 2019

Added two language options.
More animations were added.
Corrections were made in the List.

Changes made: 26 - March - 2019

Added the information of the recent changes.
Some tools of the Spanish version were eliminated because they are
the same as the English version.

Changes made: 22 - February - 2019

Updates to the Latin Version were permanently canceled.

Changes made: 19 - January - 2019

Updates in the English section.
Added the section of View Animations, now you can look at the
animations of ped.ifp.
Spelling corrections and more.

Changes made: 18 - December - 2018

The WikiSAMP section is currently in constant development.
Added the option to Visualize Guides by Images.*
Display buttons added.
The Textures section is at 10% development.
The interface of the ""Credits"" section was changed.
The language options are in Development.

",2
microsoft/terminal,C++,"Welcome!
This repository contains the source code for:

Windows Terminal
The Windows console host (conhost.exe)
Components shared between the two projects
ColorTool
Sample projects that show how to consume the Windows Console APIs

Other related repositories include:

Console API Documentation

Build Status



Project
Build Status




Terminal



ColorTool




Terminal & Console Overview
Please take a few minutes to review the overview below before diving into the code:
Windows Terminal
Windows Terminal is a new, modern, feature-rich, productive terminal application for command-line users. It includes many of the features most frequently requested by the Windows command-line community including support for tabs, rich text, globalization, configurability, theming & styling, and more.
The Terminal will also need to meet our goals and measures to ensure it remains fast, and efficient, and doesn't consume vast amounts of memory or power.
The Windows console host
The Windows console host, conhost.exe, is Windows' original command-line user experience. It implements Windows' command-line infrastructure, and is responsible for hosting the Windows Console API, input engine, rendering engine, and user preferences. The console host code in this repository is the actual source from which the conhost.exe in Windows itself is built.
Console's primary goal is to remain backwards-compatible with existing console subsystem applications.
Since assuming ownership of the Windows command-line in 2014, the team has added several new features to the Console, including window transparency, line-based selection, support for ANSI / Virtual Terminal sequences, 24-bit color, a Pseudoconsole (""ConPTY""), and more.
However, because the Console's primary goal is to maintain backward compatibility, we've been unable to add many of the features the community has been asking for, and which we've been wanting to add for the last several years--like tabs!
These limitations led us to create the new Windows Terminal.
Shared Components
While overhauling the Console, we've modernized its codebase considerably. We've cleanly separated logical entities into modules and classes, introduced some key extensibility points, replaced several old, home-grown collections and containers with safer, more efficient STL containers, and made the code simpler and safer by using Microsoft's WIL header library.
This overhaul work resulted in the creation of several key components that would be useful for any terminal implementation on Windows, including a new DirectWrite-based text layout and rendering engine, a text buffer capable of storing both UTF-16 and UTF-8, and a VT parser/emitter.
Building a new terminal
When we started building the new terminal application, we explored and evaluated several approaches and technology stacks. We ultimately decided that our goals would be best met by sticking with C++ and sharing the aforementioned modernized components, placing them atop the modern Windows application platform and UI framework.
Further, we realized that this would allow us to build the terminal's renderer and input stack as a reusable Windows UI control that others can incorporate into their applications.
FAQ
Where can I download Windows Terminal?
There are no binaries to download quite yet.
The Windows Terminal is in the very early alpha stage, and not ready for the general public quite yet. If you want to jump in early, you can try building it yourself from source.
Otherwise, you'll need to wait until Mid-June for an official preview build to drop.
I built and ran the new Terminal, but I just get a blank window app!
Make sure your are building for your computer's architecture. If your box has a 64-bit Windows change your Solution Platform to x64.
To check your OS architecture go to Settings -> System -> About (or Win+X -> System) and under Device specifications check for the  System type
I built and ran the new Terminal, but it looks just like the old console! What gives?
Firstly, make sure you're building & deploying CascadiaPackage in Visual Studio, NOT Host.EXE. OpenConsole.exe is just conhost.exe, the same old console you know and love. opencon.cmd will launch openconsole.exe, and unfortunately, openterm.cmd is currently broken.
Secondly, try pressing Ctrl + T. The tabs are hidden when you only have one tab by default. In the future, the UI will be dramatically different, but for now, the defaults are supposed to look like the console defaults.
I tried running WindowsTerminal.exe and it crashes!

Don't try to run it unpackaged. Make sure to build & deploy CascadiaPackage from Visual Studio, and run the Windows Terminal (Dev Build) app.
Make sure you're on the right version of Windows. You'll need to be on Insider's builds, or wait for the 1903 release, as the Windows Terminal REQUIRES features from the latest Windows release.

Getting Started
Prerequisites

You must be running Windows 1903 (build >= 10.0.18362.0) or above in order to run Windows Terminal


As of May 2019 this build is only available through Windows Insider Program. You may register and configure Insider Program through your device's system settings.



You must have the 1903 SDK (build 10.0.18362.0) installed


You must have at least VS 2017 installed


You must install the following Workloads via the VS Installer:

Desktop Development with C++

If you're running VS2019, you'll also need to install the following Individual Components:

MSVC v141 - VS 2017 C++ (x86 and x64) build tools
C++ ATL for v141 build tools (x86 and x64)




Universal Windows Platform Development

Also install the following Individual Component:

C++ (v141) Universal Windows Platform Tools







You must also enable Developer Mode in the Windows Settings app to locally install and run the Terminal app.


Debugging

To debug in VS, right click on CascadiaPackage (from VS Solution Explorer) and go to properties, in the Debug menu, change ""Application process"" and ""Background task process"" to ""Native Only""

Contributing
We are excited to work alongside you, our amazing community, to build and enhance Windows Terminal!
We ask that before you start work on a feature that you would like to contribute, please file an issue describing your proposed change: We will be happy to work with you to figure out the best approach, provide guidance and mentorship throughout feature development, and help avoid any wasted or duplicate effort.

👉 Remember! Your contributions may be incorporated into future versions of Windows! Because of this, all pull requests will be subject to the same level of scrutiny for quality, coding standards, performance, globalization, accessibility, and compatibility as those of our internal contributors.


⚠ Note: The Command-Line Team is actively working out of this repository and will be periodically re-structuring the code to make it easier to comprehend, navigate, build, test, and contribute to, so DO expect significant changes to code layout on a regular basis.

Documentation
All documentation is located in the ./doc folder. If you would like to contribute to the documentation, please submit a pull request.
Communicating with the Team
The easiest way to communicate with the team is via GitHub issues. Please file new issues, feature requests and suggestions, but DO search for similar open/closed pre-existing issues before you do.
Please help us keep this repository clean, inclusive, and fun! We will not tolerate any abusive, rude, disrespectful or inappropriate behavior. Read our Code of Conduct for more details.
If you would like to ask a question that you feel doesn't warrant an issue (yet), please reach out to us via Twitter:


Rich Turner, Program Manager: @richturn_ms


Dustin Howett, Engineering Lead: @dhowett


Michael Niksa, Senior Developer: @michaelniksa


Kayla Cinnamon, Program Manager (especially for UX issues): @cinnamon_msft


Developer Guidance
Building the Code
This repository uses git submodules for some of its dependencies. To make sure submodules are restored or updated, be sure to run the following prior to building:
git submodule update --init --recursive
OpenConsole.sln may be built from within Visual Studio or from the command-line using MSBuild. To build from the command line:
.\tools\razzle.cmd
bcz
We've provided a set of convenience scripts as well as README in the /tools directory to help automate the process of building and running tests.
Coding Guidance
Please review these brief docs below relating to our coding standards etc.

👉 If you find something missing from these docs, feel free to contribute to any of our documentation files anywhere in the repository (or make some new ones!)

This is a work in progress as we learn what we'll need to provide people in order to be effective contributors to our project.

Coding Style
Code Organization
Exceptions in our legacy codebase
Helpful smart pointers and macros for interfacing with Windows in WIL

Code of Conduct
This project has adopted the Microsoft Open Source Code of Conduct.
For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.
",37807
azeDevs/gearNet-Xrd,Kotlin,"GearNet Xrd

Player stat tracking network for Guilty Gear Xrd REV2
An open source fan project for tracking, persisting, and visualizing netplay match statistics for Guilty Gear Xrd REV2. In an effort to better understand where the work is needed to improve, use GearNet to automatically record your unique win ratio records for all character matchups.
Maintainer: aze
Contributors: Labryz, Lost Illusion
For more information, check out the GearNet Wiki.
",2
azeDevs/gearNet-Xrd,Kotlin,"GearNet Xrd

Player stat tracking network for Guilty Gear Xrd REV2
An open source fan project for tracking, persisting, and visualizing netplay match statistics for Guilty Gear Xrd REV2. In an effort to better understand where the work is needed to improve, use GearNet to automatically record your unique win ratio records for all character matchups.
Maintainer: aze
Contributors: Labryz, Lost Illusion
For more information, check out the GearNet Wiki.
",2
whtiehack/pinus-parse-interface,TypeScript,"parse TS interface to pinus-protobuf JSON
解析  ts 的interface 到 pinus-protobuf用的 json格式。
pinus: https://github.com/node-pinus/pinus
changelog
v0.1.6:
支持 notify 消息。可以没有Response.

v0.1.5:
文件统一解析。 加快解析速度

v0.1.3:
修复interface成员全部为可选时会出错的bug。


install
npm install pinus-parse-interface
or
yarn add pinus-parse-interface
usage
const main = require('pinus-parse-interface');

const test = main.parseToPinusProtobuf('path_to_you_interface_dir');
console.log('result',JSON.stringify(test,null,4));

interface 结构约定

以文件名为消息名。
客户端与服务端的结构放到一起。
客户端消息，以 文件名_Req 为结尾
服务端消息，以 文件名_Res 为结尾
文件名中的. 会被转换成 _
服务端推送消息直接以文件名命名
_Res,_Req 可以使用自定义名字


例如: rank.playerHandler.beginGame.ts
会寻找 rank_playerHandler_beginGame_Req,rank_playerHandler_beginGame_Res

auto generate
serverProtos.json,clientProtos.json.
example
it can also be used on the pomelo-protobuf declaration file(serverProtos.json,clientProtos.json).
input
// onAdd.ts
export interface onAdd{
    nickname:string;
    nickname11:number;

    nowplayers:number;
    /**
     * The float of the nowplayers.
     *
     * @TJS-type float
     */
    nowplayers2:number;
    /**
     * The float of the nowplayers.
     *
     * @TJS-type double
     */
    nowplayers3:number;
}


// onRank.ts


import {GGG, MyRank} from ""../share/myrank"";


interface IGG{
    ggenv?:string[];
}

interface IFF{
    ffname:string;
    aa?:IGG[];
}

enum EnumTest{
    AA,
    BB,
    CC
}

export interface onRank extends IFF,IGG{
    /**
     * The float of the nowplayers.
     *
     * @additionalProperties uInt32
     * @TJS-type array
     */
    normalArr:number[];
    /**
     * @TJS-type uInt32
     */
    enum:EnumTest;
    normalStrArr:string[];
    innerGGG?:GGG;
    ranks:MyRank[];
    rk?:MyRank;
    val?:number;
}

// rank.playerHandler.beginGame.ts

export interface rank_playerHandler_beginGame_Req{
    token:number;
    msg?:string;
}

export interface rank_playerHandler_beginGame_Res{
    /**
     * @TJS-type uInt32
     */
    code?:number;
    msg?:string;
    /**
     * @TJS-type uInt32
     */
    currank:number;
}



output
{
    ""client"": {
        ""rank.playerHandler.beginGame"": {
            ""required uInt32 token"": 1,
            ""optional string msg"": 2
        }
    },
    ""server"": {
        ""enumTest"": {
            ""optional string aa"": 1,
            ""required uInt32 bb"": 2,
            ""required uInt32 cc"": 3,
            ""optional string enumstr"": 4
        },
        ""onAdd"": {
            ""required string nickname"": 1,
            ""required uInt32 nickname11"": 2,
            ""required uInt32 nowplayers"": 3,
            ""required float nowplayers2"": 4,
            ""required double nowplayers3"": 5
        },
        ""onRank"": {
            ""repeated uInt32 normalArr"": 1,
            ""required uInt32 enum"": 2,
            ""repeated string normalStrArr"": 3,
            ""message GGG"": {
                ""required uInt32 ccgg"": 1
            },
            ""optional GGG innerGGG"": 4,
            ""message MyRank"": {
                ""required uInt32 nickname"": 1,
                ""message GGG"": {
                    ""required uInt32 ccgg"": 1
                },
                ""required GGG ggg"": 2,
                ""required GGG xxx"": 3
            },
            ""repeated MyRank ranks"": 5,
            ""optional MyRank rk"": 6,
            ""optional uInt32 val"": 7,
            ""required string ffname"": 8,
            ""message IGG"": {
                ""repeated string ggenv"": 1
            },
            ""repeated IGG aa"": 9,
            ""repeated string ggenv"": 10
        },
        ""rank.playerHandler.beginGame"": {
            ""optional uInt32 code"": 1,
            ""optional string msg"": 2,
            ""required uInt32 currank"": 3
        }
    }
}

",8
fatiando/harmonica,Python,"
Documentation |
Documentation (dev version) |
Contact |
Part of the Fatiando a Terra project










Disclaimer
🚨 This package is in early stages of design and implementation. 🚨
We welcome any feedback and ideas!
Let us know by submitting
issues on Github
or send us a message on our
Gitter chatroom.

About
Harmonica is a Python library for processing and modeling gravity and magnetic data.
It includes common processing steps, like calculation of normal gravity, terrain
correction, reduction to the pole, upward continuation, equivalent layers, and more.
There are forward modeling functions for basic geometric shapes, like spheres, prisms,
polygonal prisms, and tesseroids. The inversion methods are implemented as classes with
an interface inspired by scikit-learn (like Verde).

Project goals
These are the long-term goals for Harmonica:

Efficient, well designed, and fully tested code for gravity and magnetic data.
Cover the entire data life-cycle: from raw data to 3D Earth model.
Focus on best-practices to discourage misuse of methods, particularly inversion.
Easily extended code to enable research on the development of new methods.

See the Github milestones for
short-term goals.
Things that will not be covered in Harmonica:

Multi-physics partial differential equation solvers. Use
SimPEG or PyGIMLi instead.
Generic grid processing methods (like horizontal derivatives and FFT). These should be
implemented in Verde.
Data visualization.
GUI applications.


Contacting Us

Most discussion happens on Github.
Feel free to open an issue or comment
on any open issue or pull request.
We have chat room on Gitter
where you can ask questions and leave comments.


Contributing

Code of conduct
Please note that this project is released with a
Contributor Code of Conduct.
By participating in this project you agree to abide by its terms.

Contributing Guidelines
Please read our
Contributing Guide
to see how you can help and give feedback.

Imposter syndrome disclaimer
We want your help. No, really.
There may be a little voice inside your head that is telling you that you're
not ready to be an open source contributor; that your skills aren't nearly good
enough to contribute.
What could you possibly offer?
We assure you that the little voice in your head is wrong.
Being a contributor doesn't just mean writing code.
Equality important contributions include:
writing or proof-reading documentation, suggesting or implementing tests, or
even giving feedback about the project (including giving feedback about the
contribution process).
If you're coming to the project with fresh eyes, you might see the errors and
assumptions that seasoned contributors have glossed over.
If you can write any code at all, you can contribute code to open source.
We are constantly trying out new skills, making mistakes, and learning from
those mistakes.
That's how we all improve and we are happy to help others learn.
This disclaimer was adapted from the
MetPy project.

License
This is free software: you can redistribute it and/or modify it under the terms
of the BSD 3-clause License. A copy of this license is provided in
LICENSE.txt.

Documentation for other versions

Development (reflects the master branch on
Github)
Latest release

",18
dune-universe/dune-universe,C,"Dune universe
This repository contains a snapshot of the latest versions of all opam
packages depending on Dune (Jbuilder). It is updated daily. It is used
to analyse the usage of Dune in opam.
",13
swoole/swoole-src,C++,"English | 中文
Swoole








Swoole is an event-driven asynchronous & coroutine-based concurrency networking communication engine with high performance written in C and C++ for PHP.
✨Event-based
The network layer in Swoole is event-based and takes full advantage of the underlying epoll/kqueue implementation, making it really easy to serve millions of requests.
Swoole 4.x use a brand new engine kernel and now it has a full-time developer team, so we are entering an unprecedented period in PHP history which offers a unique possibility for rapid evolution in performance.
⚡️Coroutine
Swoole 4.x or later supports the built-in coroutine with high availability, and you can use fully synchronized code to implement asynchronous performance. PHP code without any additional keywords, the underlying automatic coroutine-scheduling.
Developers can understand coroutines as ultra-lightweight threads, and you can easily create thousands of coroutines in a single process.
MySQL
Concurrency 10K requests to read data from MySQL takes only 0.2s!
$s = microtime(true);
for ($c = 100; $c--;) {
    go(function () {
        $mysql = new Swoole\Coroutine\MySQL;
        $mysql->connect([
            'host' => '127.0.0.1',
            'user' => 'root',
            'password' => 'root',
            'database' => 'test'
        ]);
        $statement = $mysql->prepare('SELECT * FROM `user`');
        for ($n = 100; $n--;) {
            $result = $statement->execute();
            assert(count($result) > 0);
        }
    });
}
Swoole\Event::wait();
echo 'use ' . (microtime(true) - $s) . ' s';
Mixed server
You can create multiple services on the single event loop: TCP, HTTP, Websocket and HTTP2, and easily handle thousands of requests.
function tcp_pack(string $data): string
{
    return pack('n', strlen($data)) . $data;
}
function tcp_unpack(string $data): string
{
    return substr($data, 2, unpack('n', substr($data, 0, 2))[1]);
}
$tcp_options = [
    'open_length_check' => true,
    'package_length_type' => 'n',
    'package_length_offset' => 0,
    'package_body_offset' => 2
];
$server = new Swoole\WebSocket\Server('127.0.0.1', 9501, SWOOLE_BASE);
$server->set(['open_http2_protocol' => true]);
// http && http2
$server->on('request', function (Swoole\Http\Request $request, Swoole\Http\Response $response) {
    $response->end('Hello ' . $request->rawcontent());
});
// websocket
$server->on('message', function (Swoole\WebSocket\Server $server, Swoole\WebSocket\Frame $frame) {
    $server->push($frame->fd, 'Hello ' . $frame->data);
});
// tcp
$tcp_server = $server->listen('127.0.0.1', 9502, SWOOLE_TCP);
$tcp_server->set($tcp_options);
$tcp_server->on('receive', function (Swoole\Server $server, int $fd, int $reactor_id, string $data) {
    $server->send($fd, tcp_pack('Hello ' . tcp_unpack($data)));
});
$server->start();
Coroutine clients
Whether you DNS query or send requests or receive responses, all of these are scheduled by coroutine automatically.
go(function () {
    // http
    $http_client = new Swoole\Coroutine\Http\Client('127.0.0.1', 9501);
    assert($http_client->post('/', 'Swoole Http'));
    var_dump($http_client->body);
    // websocket
    $http_client->upgrade('/');
    $http_client->push('Swoole Websocket');
    var_dump($http_client->recv()->data);
});
go(function () {
    // http2
    $http2_client = new Swoole\Coroutine\Http2\Client('localhost', 9501);
    $http2_client->connect();
    $http2_request = new Swoole\Http2\Request;
    $http2_request->method = 'POST';
    $http2_request->data = 'Swoole Http2';
    $http2_client->send($http2_request);
    $http2_response = $http2_client->recv();
    var_dump($http2_response->data);
});
go(function () use ($tcp_options) {
    // tcp
    $tcp_client = new Swoole\Coroutine\Client(SWOOLE_TCP);
    $tcp_client->set($tcp_options);
    $tcp_client->connect('127.0.0.1', 9502);
    $tcp_client->send(tcp_pack('Swoole Tcp'));
    var_dump(tcp_unpack($tcp_client->recv()));
});
Channel
Channel is the only way for exchanging data between coroutines, the development combination of the Coroutine + Channel is the famous CSP programming model.
In Swoole development, Channel is usually used for implementing connection pool or scheduling coroutine concurrent.
The simplest example of a connection pool
In the following example, we have a thousand concurrently requests to redis. Normally, this has exceeded the maximum number of Redis connections setting and will throw a connection exception, but the connection pool based on Channel can perfectly schedule requests. We don't have to worry about connection overload.
class RedisPool
{
    /**@var \Swoole\Coroutine\Channel */
    protected $pool;

    /**
     * RedisPool constructor.
     * @param int $size max connections
     */
    public function __construct(int $size = 100)
    {
        $this->pool = new \Swoole\Coroutine\Channel($size);
        for ($i = 0; $i < $size; $i++) {
            $redis = new \Swoole\Coroutine\Redis();
            $res = $redis->connect('127.0.0.1', 6379);
            if ($res == false) {
                throw new \RuntimeException(""failed to connect redis server."");
            } else {
                $this->put($redis);
            }
        }
    }

    public function get(): \Swoole\Coroutine\Redis
    {
        return $this->pool->pop();
    }

    public function put(\Swoole\Coroutine\Redis $redis)
    {
        $this->pool->push($redis);
    }

    public function close(): void
    {
        $this->pool->close();
        $this->pool = null;
    }
}

go(function () {
    $pool = new RedisPool();
    // max concurrency num is more than max connections
    // but it's no problem, channel will help you with scheduling
    for ($c = 0; $c < 1000; $c++) {
        go(function () use ($pool, $c) {
            for ($n = 0; $n < 100; $n++) {
                $redis = $pool->get();
                assert($redis->set(""awesome-{$c}-{$n}"", 'swoole'));
                assert($redis->get(""awesome-{$c}-{$n}"") === 'swoole');
                assert($redis->delete(""awesome-{$c}-{$n}""));
                $pool->put($redis);
            }
        });
    }
});
Producer and consumers
Some Swoole's clients implement the defer mode for concurrency, but you can still implement it flexible with a combination of coroutines and channels.
go(function () {
    // User: I need you to bring me some information back.
    // Channel: OK! I will be responsible for scheduling.
    $channel = new Swoole\Coroutine\Channel;
    go(function () use ($channel) {
        // Coroutine A: Ok! I will show you the github addr info
        $addr_info = Co::getaddrinfo('github.com');
        $channel->push(['A', json_encode($addr_info, JSON_PRETTY_PRINT)]);
    });
    go(function () use ($channel) {
        // Coroutine B: Ok! I will show you what your code look like
        $mirror = Co::readFile(__FILE__);
        $channel->push(['B', $mirror]);
    });
    go(function () use ($channel) {
        // Coroutine C: Ok! I will show you the date
        $channel->push(['C', date(DATE_W3C)]);
    });
    for ($i = 3; $i--;) {
        list($id, $data) = $channel->pop();
        echo ""From {$id}:\n {$data}\n"";
    }
    // User: Amazing, I got every information at earliest time!
});
Timer
$id = Swoole\Timer::tick(100, function () {
    echo ""⚙️ Do something...\n"";
});
Swoole\Timer::after(500, function () use ($id) {
    Swoole\Timer::clear($id);
    echo ""⏰ Done\n"";
});
Swoole\Timer::after(1000, function () use ($id) {
    if (!Swoole\Timer::exists($id)) {
        echo ""✅ All right!\n"";
    }
});
The way of coroutine
go(function () {
    $i = 0;
    while (true) {
        Co::sleep(0.1);
        echo ""📝 Do something...\n"";
        if (++$i === 5) {
            echo ""🛎 Done\n"";
            break;
        }
    }
    echo ""🎉 All right!\n"";
});
🔥 Amazing runtime hooks
In the latest version of Swoole, we have added a new feature to make sync PHP network libraires to be coroutine libraires with only one line of code.
Just call Swoole\Runtime::enableCoroutine() method on the top of the script and use php-redis, 10K concurrent requests reading data from Redis takes only 0.1s!
Swoole\Runtime::enableCoroutine();
$s = microtime(true);
for ($c = 100; $c--;) {
    go(function () {
        ($redis = new Redis)->connect('127.0.0.1', 6379);
        for ($n = 100; $n--;) {
            assert($redis->get('awesome') === 'swoole');
        }
    });
}
Swoole\Event::wait();
echo 'use ' . (microtime(true) - $s) . ' s';
After you call it, the Swoole kernel will replace the function pointers of streams in ZendVM, if you use php_stream based extensions, all socket operations can be dynamically converted to be asynchronous IO scheduled by coroutine at runtime.
How many things you can do in 1s?
Sleep 10K times, read, write, check and delete files 10K times, use PDO and MySQLi to communicate with the database 10K times, create a TCP server and multiple clients to communicate with each other 10K times, create a UDP server and multiple clients to communicate with each other 10K times... Everything works well in one process!
Just see what the Swoole brings, just imagine...
Swoole\Runtime::enableCoroutine();
$s = microtime(true);

// i just want to sleep...
for ($c = 100; $c--;) {
    go(function () {
        for ($n = 100; $n--;) {
            usleep(1000);
        }
    });
}

// 10K file read and write
for ($c = 100; $c--;) {
    go(function () use ($c) {
        $tmp_filename = ""/tmp/test-{$c}.php"";
        for ($n = 100; $n--;) {
            $self = file_get_contents(__FILE__);
            file_put_contents($tmp_filename, $self);
            assert(file_get_contents($tmp_filename) === $self);
        }
        unlink($tmp_filename);
    });
}

// 10K pdo and mysqli read
for ($c = 50; $c--;) {
    go(function () {
        $pdo = new PDO('mysql:host=127.0.0.1;dbname=test;charset=utf8', 'root', 'root');
        $statement = $pdo->prepare('SELECT * FROM `user`');
        for ($n = 100; $n--;) {
            $statement->execute();
            assert(count($statement->fetchAll()) > 0);
        }
    });
}
for ($c = 50; $c--;) {
    go(function () {
        $mysqli = new Mysqli('127.0.0.1', 'root', 'root', 'test');
        $statement = $mysqli->prepare('SELECT `id` FROM `user`');
        for ($n = 100; $n--;) {
            $statement->bind_result($id);
            $statement->execute();
            $statement->fetch();
            assert($id > 0);
        }
    });
}

// php_stream tcp server & client with 12.8K requests in single process
function tcp_pack(string $data): string
{
    return pack('n', strlen($data)) . $data;
}

function tcp_length(string $head): int
{
    return unpack('n', $head)[1];
}

go(function () {
    $ctx = stream_context_create(['socket' => ['so_reuseaddr' => true, 'backlog' => 128]]);
    $socket = stream_socket_server(
        'tcp://0.0.0.0:9502',
        $errno, $errstr, STREAM_SERVER_BIND | STREAM_SERVER_LISTEN, $ctx
    );
    if (!$socket) {
        echo ""$errstr ($errno)\n"";
    } else {
        $i = 0;
        while ($conn = stream_socket_accept($socket, 1)) {
            stream_set_timeout($conn, 5);
            for ($n = 100; $n--;) {
                $data = fread($conn, tcp_length(fread($conn, 2)));
                assert($data === ""Hello Swoole Server #{$n}!"");
                fwrite($conn, tcp_pack(""Hello Swoole Client #{$n}!""));
            }
            if (++$i === 128) {
                fclose($socket);
                break;
            }
        }
    }
});
for ($c = 128; $c--;) {
    go(function () {
        $fp = stream_socket_client(""tcp://127.0.0.1:9502"", $errno, $errstr, 1);
        if (!$fp) {
            echo ""$errstr ($errno)\n"";
        } else {
            stream_set_timeout($fp, 5);
            for ($n = 100; $n--;) {
                fwrite($fp, tcp_pack(""Hello Swoole Server #{$n}!""));
                $data = fread($fp, tcp_length(fread($fp, 2)));
                assert($data === ""Hello Swoole Client #{$n}!"");
            }
            fclose($fp);
        }
    });
}

// udp server & client with 12.8K requests in single process
go(function () {
    $socket = new Swoole\Coroutine\Socket(AF_INET, SOCK_DGRAM, 0);
    $socket->bind('127.0.0.1', 9503);
    $client_map = [];
    for ($c = 128; $c--;) {
        for ($n = 0; $n < 100; $n++) {
            $recv = $socket->recvfrom($peer);
            $client_uid = ""{$peer['address']}:{$peer['port']}"";
            $id = $client_map[$client_uid] = ($client_map[$client_uid] ?? -1) + 1;
            assert($recv === ""Client: Hello #{$id}!"");
            $socket->sendto($peer['address'], $peer['port'], ""Server: Hello #{$id}!"");
        }
    }
    $socket->close();
});
for ($c = 128; $c--;) {
    go(function () {
        $fp = stream_socket_client(""udp://127.0.0.1:9503"", $errno, $errstr, 1);
        if (!$fp) {
            echo ""$errstr ($errno)\n"";
        } else {
            for ($n = 0; $n < 100; $n++) {
                fwrite($fp, ""Client: Hello #{$n}!"");
                $recv = fread($fp, 1024);
                list($address, $port) = explode(':', (stream_socket_get_name($fp, true)));
                assert($address === '127.0.0.1' && (int)$port === 9503);
                assert($recv === ""Server: Hello #{$n}!"");
            }
            fclose($fp);
        }
    });
}

Swoole\Event::wait();
echo 'use ' . (microtime(true) - $s) . ' s';
⌛️ Installation

As with any open source project, Swoole always provides the most reliable stability and the most powerful features in the latest released version. Please ensure as much as possible that you are using the latest version.

1. From binary package (beginners + dev-env)
See our download page
Compiling requirements

Linux, OS X or Cygwin, WSL
PHP 7.0.0 or later (The higher the version, the better the performance.)
GCC 4.8 or later

2. Install via PECL (beginners)
pecl install swoole
3. Install from source (recommended)
Please download the source packages from Releases or:
git clone https://github.com/swoole/swoole-src.git && \
cd swoole-src && \
git checkout v4.x.x
Compile and install at the source folder:
phpize && \
./configure && \
make && make install
Enable extension in PHP
After compiling and installing to the system successfully, you have to add a new line extension=swoole.so to php.ini to enable Swoole extension.
Extra compiler configurations

for example: ./configure --enable-openssl --enable-sockets


--enable-openssl or --with-openssl-dir=DIR
--enable-sockets
--enable-http2
--enable-mysqlnd (need mysqlnd, it just for supporting $mysql->escape method)

Upgrade

⚠️ If you upgrade from source, don't forget to make clean before you upgrade your swoole


pecl upgrade swoole
git pull && cd swoole-src && make clean && make && sudo make install
if you change your PHP version, please re-run phpize clean && phpize then try to compile

Major change since version 4.3.0
Async clients and API are moved to a separate PHP extension swoole_async since version 4.3.0, install swoole_async:
git clone https://github.com/swoole/async-ext.git
cd async-src
phpize
./configure
make -j 4
sudo make install
Enable it by adding a new line extension=swoole_async.so to php.ini.
💎 Frameworks & Components

Swoft is a modern, high-performance AOP and coroutine PHP framework.
Easyswoole EasySwoole is a distributed, persistent memory PHP framework based on the Swoole extension. It was created specifically for APIs to get rid of the performance penalties associated with process calls and file loading. EasySwoole highly encapsulates the Swoole Server and still maintains the original features of the Swoole server, supports simultaneous monitoring of HTTP, custom TCP, and UDP protocols, allowing developers to write multi-process, asynchronous, and highly available applications with minimal learning cost and effort.

Base on Swoole extension
Built-in HTTP, TCP, WebSocket,Udp Coroutine Server
Global dependency injection container
PSR-7 based HTTP message implementation
HTTP,TCP, WebSocket, Udp middleware support
Scalable high performance RPC
Database ORM
Mysql, Redis, RPC, HTTP Coroutine Clients
Coroutine and asynchronous task delivery
Custom user processes
RESTful supported
High performance router
Fast and flexible parameter validator
Powerful log component
Universal connection pools
Remote Console support
Crontab Rule Timer support


Saber Is a human-friendly, high-performance HTTP client component that has almost everything you can imagine.

🛠 Develop & Discussion

中文文档: http://wiki.swoole.com
Documentation: https://www.swoole.co.uk/docs
IDE Helper & API: https://github.com/swoole/ide-helper
中文社区: https://wiki.swoole.com/wiki/page/p-discussion.html
Twitter: https://twitter.com/php_swoole
Slack Group: https://swoole.slack.com

🍭 Benchmark

On the open source Techempower Web Framework benchmarks Swoole used MySQL database benchmark to rank first, and all performance tests ranked in the first echelon.
You can just run Benchmark Script to quickly test the maximum QPS of Swoole-HTTP-Server on your machine.

🖊️ Security issues
Security issues should be reported privately, via email, to the Swoole develop team team@swoole.com. You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message.
🖊️ Contribution
Your contribution to Swoole development is very welcome!
You may contribute in the following ways:

Report issues and feedback
Submit fixes, features via Pull Request
Write/polish documentation

❤️ Contributors
This project exists thanks to all the people who contribute. [Contributors].

🎙️ Official Evangelist
Demin has been playing with PHP since 2000, focusing on building high-performance, secure web services. He is an occasional conference speaker on PHP and Swoole, and has been working for companies in the states like eBay, Visa and Glu Mobile for years. You may find Demin on Twitter or GitHub.
📃 License
Apache License Version 2.0 see http://www.apache.org/licenses/LICENSE-2.0.html
",13286
jbfraser1/SailScores,C#,"SailScores
ASP.NET Core website and API for one-design sailboat scores. This is the code
for the site hosted at sailscores.com.
I've been keeping scores for our club for a few years and want to fix a bunch of
weaknesses with the options I've seen for scorekeeping. Much of the software I've seen is
dated, lacking support for mobile devices, expensive, or aimed at scoring a
single regatta as an event.
Sailscores aims to be:

easy to navigate and bookmark.
fast to use, particularly for entering new results.
web and mobile friendly.
Open REST API: I've got plans for some nicer client software but all using a common, straightforward API.
Crazy cheap for the foreseeable future. At some point I might ask for help covering expenses. (But I'm not quitting my day job any time soon.)
On the roadmap: analytics for race data.

This code is designed to be multitenant, and I'm very open to hosting other clubs. Site
is live in spring of 2019. I expect there will be plenty of fixes and enhancements in the next year.
Licensed with Mozilla Public License Version 2.0 : You may use this software, but
share the source for modifications that you distribute.
Contributions welcome. Feedback requested: There are many features I'm working
on, and it's nice to prioritize them with input from other users.
Sail fast...
jamie@widernets.com
",2
yapp-project/Toonie,Swift,"Toonie    
팀원 정보
iOS 클라이언트 :  박은비 양어진 이재은
서버 :  조경현
앱 소개
키워드를 통한 인스타툰 추천 서비스

사용자가 선택한 취향 태그를 기반으로 인스타툰을 추천합니다.
최근 본 작품 및 찜한 작품과 연관된 인스타툰도 추천합니다.
장르와 세부 키워드로 분류된 인스타툰을 찾아볼 수 있습니다.




버전 정보

version 1.0

앱 스토어
 

활용 기술

SwiftLint : 명확하고 깔끔한 코드 작성과 팀원 간의 코드 이해를 도움

# .swiftlint.yml
disabled_rules:
- leading_whitespace
- trailing_whitespace

included:

line_length:
warning: 99
error: 120

excluded:
- Toonie/Supporting Files/AppDelegate.swift
- Pods

xUnique : project.pbxproj 파일의 충돌을 최소화하고 해결을 도움
Travis CI : 자동 빌드 테스트를 통해 안정성 있는 코드 통합을 함
Kingfisher
Alamofire
Lottie
SnapKit : NSLayoutConstraint를 사용시 코드의 양이 많고 가독성이 떨어지기 때문에 SnapKit으로 유지보수성를 높이기 위해 사용하였습니다.
WebKit

",5
BayAreaMetro/travel-model-one,Java,"travel-model-one
The Metropolitan Transportation Commission (MTC) maintains a simulation model of typical weekday travel to assist in regional planning activities.  MTC makes the software and scripts necessary to implement the model as well as detailed model results available to the public.  Users of the model and/or the model's results are entirely responsible for the outcomes, interpretations, and conclusions they reach from the information.  Users of the MTC model or model results shall in no way imply MTC's support or review of their findings or analyses.
Model Versions
The following model versions are available in the repository:

Version 0.3 -- Maintained in branch v03.
Version 0.4 -- Maintained in branch v04.
Version 0.5 -- Maintained in branch v05.
Version 0.6 -- Maintained in branch v06.
Version 1.5 -- Maintained in branch master.

For additional details about the different versions, please see here
Any other branches are exploratory and not used in our planning work.
Please find a detailed User's Guide here.
",5
GDGKualaLumpur/ioxkl19,JavaScript,"I/O Extended 2019 Kuala Lumpur PWA
Progressive Web App for Google I/O Extended 2019 Kuala Lumpur.

Getting Started
Start development server:
npm run dev
Build:
npm run build
Set Google Cloud Project:
gcloud config set project <project-id>
Deploy:
gcloud app deploy io.yaml -v 1
Technology Stack

Preact
Firebase (Auth, Realtime Database, Hosting)
Service Worker

License
This PWA is published under the MIT license.
Feel free to clone and modify repo as you want, but don't forget to add reference to authors.
",3
wuxh123/iot_server,Python,"iot_server
利用epoll mqtt redis mysql mongodb 搭建的一个后台iot server的雏形。采用多进程的方式运行，提升cpu利用率。
开发工具
vs code
vscode 插件
pip install pylint
python库
pip install mysql-connector
pip install mysqlclient
pip isntall paho-mqtt
pip install protobuf
pip install pymongo
pip install redis
功能
1.设备通过socket连接到服务器
2.Iot server保存设备id
3.通过设备id订阅该设备的消息
4.中心通过该设备的定于发送指令
5.通过该server，将mqtt指令转发给具体的socket，实现数据透明传输。
特点
1.采用epoll+线程池处理指令，此处如果为cpu密集型操作，可以改成进程池
2.采用mqtt+threadpoll 处理指令。
其他
通过模拟多device操作，每毫秒发送数据，未发生数据丢失，粘包。cpu负荷在5%以下。
运行
python main.py
测试
python test.py
其他
1.如果觉得python性能不够好（我觉得不错的），可以改为pypy
2.数据粘包比较麻烦，我测试过，如果线程池是20，每毫秒3条数据会粘包。线程池50的话就没有发生粘包。
设备A通过epoll链接到iot server，server将其注册到mqtt broker

设备B通过epoll链接到iot server，server将其注册到mqtt broker

每毫秒设备A和设备2同时向iot server发送指令，指令转送给 mqttbroker。没有丢包，没有延时

设备A通过mqtt向设备B发送指令

",5
jbfraser1/SailScores,C#,"SailScores
ASP.NET Core website and API for one-design sailboat scores. This is the code
for the site hosted at sailscores.com.
I've been keeping scores for our club for a few years and want to fix a bunch of
weaknesses with the options I've seen for scorekeeping. Much of the software I've seen is
dated, lacking support for mobile devices, expensive, or aimed at scoring a
single regatta as an event.
Sailscores aims to be:

easy to navigate and bookmark.
fast to use, particularly for entering new results.
web and mobile friendly.
Open REST API: I've got plans for some nicer client software but all using a common, straightforward API.
Crazy cheap for the foreseeable future. At some point I might ask for help covering expenses. (But I'm not quitting my day job any time soon.)
On the roadmap: analytics for race data.

This code is designed to be multitenant, and I'm very open to hosting other clubs. Site
is live in spring of 2019. I expect there will be plenty of fixes and enhancements in the next year.
Licensed with Mozilla Public License Version 2.0 : You may use this software, but
share the source for modifications that you distribute.
Contributions welcome. Feedback requested: There are many features I'm working
on, and it's nice to prioritize them with input from other users.
Sail fast...
jamie@widernets.com
",2
yapp-project/Toonie,Swift,"Toonie    
팀원 정보
iOS 클라이언트 :  박은비 양어진 이재은
서버 :  조경현
앱 소개
키워드를 통한 인스타툰 추천 서비스

사용자가 선택한 취향 태그를 기반으로 인스타툰을 추천합니다.
최근 본 작품 및 찜한 작품과 연관된 인스타툰도 추천합니다.
장르와 세부 키워드로 분류된 인스타툰을 찾아볼 수 있습니다.




버전 정보

version 1.0

앱 스토어
 

활용 기술

SwiftLint : 명확하고 깔끔한 코드 작성과 팀원 간의 코드 이해를 도움

# .swiftlint.yml
disabled_rules:
- leading_whitespace
- trailing_whitespace

included:

line_length:
warning: 99
error: 120

excluded:
- Toonie/Supporting Files/AppDelegate.swift
- Pods

xUnique : project.pbxproj 파일의 충돌을 최소화하고 해결을 도움
Travis CI : 자동 빌드 테스트를 통해 안정성 있는 코드 통합을 함
Kingfisher
Alamofire
Lottie
SnapKit : NSLayoutConstraint를 사용시 코드의 양이 많고 가독성이 떨어지기 때문에 SnapKit으로 유지보수성를 높이기 위해 사용하였습니다.
WebKit

",5
BayAreaMetro/travel-model-one,Java,"travel-model-one
The Metropolitan Transportation Commission (MTC) maintains a simulation model of typical weekday travel to assist in regional planning activities.  MTC makes the software and scripts necessary to implement the model as well as detailed model results available to the public.  Users of the model and/or the model's results are entirely responsible for the outcomes, interpretations, and conclusions they reach from the information.  Users of the MTC model or model results shall in no way imply MTC's support or review of their findings or analyses.
Model Versions
The following model versions are available in the repository:

Version 0.3 -- Maintained in branch v03.
Version 0.4 -- Maintained in branch v04.
Version 0.5 -- Maintained in branch v05.
Version 0.6 -- Maintained in branch v06.
Version 1.5 -- Maintained in branch master.

For additional details about the different versions, please see here
Any other branches are exploratory and not used in our planning work.
Please find a detailed User's Guide here.
",5
GDGKualaLumpur/ioxkl19,JavaScript,"I/O Extended 2019 Kuala Lumpur PWA
Progressive Web App for Google I/O Extended 2019 Kuala Lumpur.

Getting Started
Start development server:
npm run dev
Build:
npm run build
Set Google Cloud Project:
gcloud config set project <project-id>
Deploy:
gcloud app deploy io.yaml -v 1
Technology Stack

Preact
Firebase (Auth, Realtime Database, Hosting)
Service Worker

License
This PWA is published under the MIT license.
Feel free to clone and modify repo as you want, but don't forget to add reference to authors.
",3
wuxh123/iot_server,Python,"iot_server
利用epoll mqtt redis mysql mongodb 搭建的一个后台iot server的雏形。采用多进程的方式运行，提升cpu利用率。
开发工具
vs code
vscode 插件
pip install pylint
python库
pip install mysql-connector
pip install mysqlclient
pip isntall paho-mqtt
pip install protobuf
pip install pymongo
pip install redis
功能
1.设备通过socket连接到服务器
2.Iot server保存设备id
3.通过设备id订阅该设备的消息
4.中心通过该设备的定于发送指令
5.通过该server，将mqtt指令转发给具体的socket，实现数据透明传输。
特点
1.采用epoll+线程池处理指令，此处如果为cpu密集型操作，可以改成进程池
2.采用mqtt+threadpoll 处理指令。
其他
通过模拟多device操作，每毫秒发送数据，未发生数据丢失，粘包。cpu负荷在5%以下。
运行
python main.py
测试
python test.py
其他
1.如果觉得python性能不够好（我觉得不错的），可以改为pypy
2.数据粘包比较麻烦，我测试过，如果线程池是20，每毫秒3条数据会粘包。线程池50的话就没有发生粘包。
设备A通过epoll链接到iot server，server将其注册到mqtt broker

设备B通过epoll链接到iot server，server将其注册到mqtt broker

每毫秒设备A和设备2同时向iot server发送指令，指令转送给 mqttbroker。没有丢包，没有延时

设备A通过mqtt向设备B发送指令

",5
nicohman/wyvern,Rust,"wyvern  
Wyvern is a command-line tool written in rust that is meant to make downloading GOG games and associated activities easier and faster on linux. It features:


Downloading games


Installing games without need for the graphical installers


One-command updating of games to their latest versions, while only updating files that have changed between versions.


GOG Connect functionality so you can scan for and claim games without leaving the terminal


Syncing save files to a filesystem backup(with integration with cloud services being worked on).


Optional(compile with the 'eidolonint' feature) integration with eidolon, so that it automatically registers installed games to eidolon.


The GitHub repo is a mirror of the main sr.ht repository.
See it working

Installation
Wyvern is available on crates.io, installable via cargo:
cargo install wyvern
You can download a binary that's automatically built from the latest git commit on my website. If you want to, you can also build it from source if you have cargo installed easily:

git clone https://git.sr.ht/~nicohman/wyvern && cd wyvern

cargo install --path . --force


Plus, it's available on the AUR as
wyvern, helpfully maintained by
@PinkCathodeCat@cathoderay.tube.
In addition, you can install it as a snap from snapcraft.io.
Dependencies
Wyvern has a few extra dependencies, but few are required:

rsync for save file syncing
innoextract for windows game installation
unzip for faster game installation

Usage
Run wyvern help for a list of commands:
wyvern 1.0.0
nicohman <nicohman@demenses.net>
A simple CLI tool for installing and maintaining linux GOG games

USAGE:
    wyvern <SUBCOMMAND>

FLAGS:
    -h, --help       Prints help information
    -V, --version    Prints version information

SUBCOMMANDS:
    connect    Operations associated with GOG Connect
    down       Download specific game
    help       Prints this message or the help of the given subcommand(s)
    install    Install a GOG game from an installer
    ls         List all games you own
    sync       Sync a game's saves to a specific location for backup
    update     Update a game if there is an update available

Contributing/Reporting bugs
Please file isues at the sr.ht issue tracker and patches/pull requests should be sent to the mailing list. However, I will still accept both on GitHub if need be.
Todo

Very happy to take feature requests!

",59
dotclear/dotclear,JavaScript,"README
WHAT IS DOTCLEAR ?
Dotclear is an open-source web publishing software.
Take control over your blog!
Dotclear project's purpose is to provide a user-friendly
tool allowing anyone to publish on the web, regardless of
their technical skills.
Features:

Easy publication
Fully customizable theme
User-friendly administration
Flexible template system
Media management
Choose from several editing syntax (wiki, markdown, textile or directly in wysiwyg)
Flexible comment system
Built-in antispam
Localization
Presentation widgets
Themes and plugins
Pages
Tags and categories
Automated installation
Support for several database types
Multiblog
Multi-user with permissions
Standards compliant
Accessible
Importing / exporting
Naturally optimized for search engines
Syndication feeds
Complete trackback support
Full Unicode support
XML/RPC client support
Extensible
Performance and scalability
Twice free

REQUIREMENTS
In order to run Dotclear you need:

A web server (Apache, Cherokee, Nginx, lighttpd, etc.)
PHP 5.6 with the following modules:

mbstring
iconv
simplexml
mysql, mysqli, postgresql or sqlite


A database server (MySQL or PostgreSQL) or SQLite.

INSTALLATION
Automatic installation
The easiest way to install the blog engine is automatic installation.
Download the one minute install file, upload it to your web space. Then open it in your favorite browser. You'll only have to follow the instructions on screen. See the documentation for more information.
Standard installation.
You need to download Dotclear archive, extract it then upload your files to your web space using an FTP client.
Then open your favorite browser and go to http://your.host.name/dotclear/admin/install/. A message alerts you that you haven't got a configuration file and offers to run the wizard. Click this link.
DOCUMENTATION
Still unsure if you want to move? A ""guided tour"" is what you need.
Dotclear is fully documented:

If you have moved in already, the User Manual is there for you.
The managers will turn to the Administration Guide.
Decorators and craftsmen will surely enjoy reading the Developer and designer resources.

Dotclear documentation uses a wiki. Feel free to contribute.
License
Copyright Olivier Meunier & Association Dotclear
GPL-2.0-only (https://www.gnu.org/licenses/gpl-2.0.html)
This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; version 2 of the License.
This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
CONTRIBUTING
Dotclear is an open source project. If you'd like to contribute, please read the CONTRIBUTING file.
You can submit a pull request, or feel free to use any other way you'd prefer.
Repositories:
https://hg.dotclear.org/dotclear/ (official)
https://bitbucket.org/dotclear/dotclear (Bitbucket)
https://github.com/dotclear/dotclear (Github, sync'ed with Bitbucket)
",11
mozilla-tw/FirefoxLite,Java,"Firefox Lite

Getting Involved
We encourage you to participate in this open source project. We love Pull Requests, Bug Reports, ideas, (security) code reviews or any kind of positive contribution. Please read the Community Participation Guidelines.

Issues: https://github.com/mozilla-tw/FirefoxLite/issues

Build instructions

Clone the repository:

git clone https://github.com/mozilla-tw/FirefoxLite

Since we're using submodule, run:

git submodule init
git submodule update


Open Android Studio and select File->Open and select FirefoxLite to open the project. Make sure to select the right build variant in Android Studio: focusWebkitDebug


Disable Instant Run in: Settings/Build, Execution, Deployment > Instant Run. (See note #1 for details.)


[1] We currently don't support instant run. An error message similar to:
/Users/tyu/Documents/zerda/FirefoxLite/app/src/main/java/org/mozilla/focus/utils/FirebaseHelper.java:29: error: cannot find symbol
final public class FirebaseHelper extends FirebaseWrapper {
                                          ^
  symbol: class FirebaseWrapper

will shown when building with instant run. We have an issue for this so feel free to help with this limitation. To disable instant run , press Cmd+Shit+a on Mac or Ctrl+Shift+a on Windows and enter ""instant run"" under ""Preference"" category. If it still doesn't work, try enter ./gradlew clean"" on mac or gradlew clean on Windows using command line in the project root.
Build instructions regarding Firebase
We're leveraging Firebase to offer some extra functionalities. However, Firebase is optional so normally you should be able to just develop on focusWebkitDebug.
Here are some Firebase build workarounds that you may need during normal development:

If you want to run UI test without setting up firebase tokens, remove testBuildType ""firebase"" from build.gradle or you will most likely see error message like this when you run connectedAndroidTest directly:

No tests found. This usually means that your test classes are not in the form that your test runner expects (e.g. don't inherit from TestCase or lack @Test annotations).

If you'd like to test the fully functional Firebase build, you should first setup the environment variables following these instructions and select focusWebkitFirebase
Pull request checks
To mimimize the chance you are blocked by our build checks, you can self check these locally:

(build) run ./gradlew clean checkstyle assembleFocusWebkitDebug lint findbugs assembleAndroidTest ktlint
(size check) run python tools/metrics/apk_size.py focus webkit
(Unit test) run ./gradlew testFocusWebkitDebugUnitTest
(UI test) run ./gradlew connectedAndroidTest

ktlint
Run ktlint --install-git-pre-commit-hook or ktlint --install-git-pre-push-hook for hooks
Run ./gradlew ktlint or ktlint to run check
Run ./gradlew ktlintformat -F or ktlint -F to fix format
If you want to go extreme,run ktlint -a -F. This will use Android rule and gives you a lot of complains about max length, but we are not using right now.
See https://ktlint.github.io/ for details.
Docs

Content blocking
Translations
Search
Telemetry

License
This Source Code Form is subject to the terms of the Mozilla Public
License, v. 2.0. If a copy of the MPL was not distributed with this
file, You can obtain one at http://mozilla.org/MPL/2.0/

",145
lutris/lutris,Python,"Lutris

Lutris is an open source gaming platform that makes gaming on Linux easier by
managing, installing and providing optimal settings for games.
Lutris does not sell games. For commercial games, you must own a copy to install
the game on Lutris.
The platform uses programs referered to as 'runners' to launch games,
Those runners (with the exception of Steam and web browsers) are provided and
managed by Lutris, so you don't need to install them with your package manager.
Scripts written by the community allow access to a library of games.
Using scripts, games can be played without manual setup.

Installer scripts
Lutris installations are fully automated through scripts, which can be written
in either JSON or YAML.
The scripting syntax is described in docs/installers.rst, and is also
available online at lutris.net.

Game library
Optional accounts can be created at lutris.net and linked with Lutris clients.
This enables your client to automatically sync fetch library from the website.
It is currently not possible to sync from the client to the cloud.
Via the website, it is also possible to sync your Steam library to your Lutris
library.
The Lutris client only stores a token when connected with the website, and your
login credentials are never saved.
This token is stored in ~/.cache/lutris/auth-token.

Configuration files

~/.config/lutris: The client, runners, and game configuration files

There is be no need to manually edit these files as everything should be done from the client.


lutris.conf: Preferences for the client's UI

system.yml: Default game configuration, which applies to every game

runners/*.yml: Runner-specific configurations

games/*.yml: Game-specific configurations


Game-specific configurations overwrite runner-specific configurations, which in
turn overwrite the system configuration.

Runners and the game database
~/.local/share/lutris: All data necessary to manage Lutris' library and games, including:

pga.db: An SQLite database tracking the game library, game installation status, various file locations, and some additional metadata
runners/*: Runners downloaded from lutris.net
icons/*.png and banners/*.jpg: Game banners and icons


Command line options
The following command line arguments are available:
-v, --version              Print the version of Lutris and exit
-d, --debug                Show debug messages
-i, --install              Install a game from a yml file
-e, --exec                 Execute a program with the lutris runtime
-l, --list-games           List all games in database
-o, --installed            Only list installed games
-s, --list-steam-games     List available Steam games
--list-steam-folders       List all known Steam library folders
-j, --json                 Display the list of games in JSON format
--reinstall                Reinstall game
--display=DISPLAY          X display to use

Additionally, you can pass a lutris: protocol link followed by a game
identifier on the command line such as:
lutris lutris:quake

This will install the game if it is not already installed, otherwise it will
launch the game. The game will always be installed if the --reinstall flag is passed.

Planned features
Lutris is far from complete, and some features have yet
to be implemented.
Here's what to expect from future versions of Lutris:

Humble Bundle integration
TOSEC database integration
Management of personal game data (i.e. syncing games across devices using private cloud storage)
Community features (friends list, chat, multiplayer game scheduling, etc.)
Controller configuration GUI (with xboxdrv support)


Support the project
Lutris is 100% community supported, to ensure a continuous developement on the
project, please consider donating to the project.
Our main platform for supporting Lutris is Patreon: https://www.patreon.com/lutris
but there are also other options available at https://lutris.net/donate

Come with us!
Want to make Lutris better? Help implement features, fix bugs, test
pre-releases, or simply chat with the developers?
You can always reach us on:

Discord: https://discordapp.com/invite/Pnt5CuY
IRC: #lutris on the Freenode servers
Github: https://github.com/lutris
Twitter: https://twitter.com/LutrisGaming

",2040
chef/chef-dk,Ruby,"Chef Development Kit





Umbrella Project: Workstation

Project State: Active
Issues Response Time Maximum: 14 days
Pull Request Response Time Maximum: 14 days

Chef Development Kit (ChefDK) brings Chef and the development tools developed by the Chef Community together and acts as the consistent interface to this awesomeness. This awesomeness is composed of:

Chef Infra Client
Berkshelf
Test Kitchen
ChefSpec
Foodcritic
Cookstyle
Delivery CLI
Push Jobs Client

This repository contains the code for the chef command. The full
package is built with omnibus. Project and component build definitions
are in the omnibus directory in this repository.
Installation
You can get the latest release of ChefDK from the downloads page.
On Mac OS X, you can also use homebrew-cask
to brew cask install chef/chef/chefdk.
Once you install the package, the chef-client suite, berks,
kitchen, and this application (chef) will be symlinked into your
system bin directory, ready to use.
Pre-release Candidates
The following commands will download the latest ChefDK package from the current channel.  The current channel holds builds that have passed testing and are candidates for release.
More information about flags supported by install.sh available here: https://docs.chef.io/api_omnitruck.html
Linux and OS/X:
In a terminal, run:
curl https://omnitruck.chef.io/install.sh | sudo bash -s -- -c current -P chefdk
To download a specific version, append the -v flag.  EG, -v 0.9.0.
Windows
Open up a Powershell command prompt as Administrator and run:
. { iwr -useb https://omnitruck.chef.io/install.ps1 } | iex; install -channel current -project chefdk
To download a specific version, append the -version flag.  EG, -version 0.9.0.
Usage
For help with Berkshelf, Test Kitchen, ChefSpec, Foodcritic, Delivery CLI or Push Jobs Client,
visit those projects' homepages for documentation and guides. For help with
chef-client and knife, visit the Chef documentation
and Learn Chef.
The chef Command
Our goal is for chef to become a workflow tool that builds on the
ideas of Berkshelf to provide an awesome experience that encourages
quick iteration and testing (and makes those things easy) and provides a
way to easily, reliably, and repeatably roll out new automation code to
your infrastructure.
While we've got a long way to go before we reach that goal we do have
some helpful bits of functionality already included in the chef
command:
chef generate
The generate subcommand generates skeleton Chef Infra code
layouts so you can skip repetitive boilerplate and get down to
automating your infrastructure quickly. Unlike other generators, it only
generates the minimum required files when creating a cookbook so you can
focus on the task at hand without getting overwhelmed by stuff you don't
need.
The following generators are built-in:

chef generate cookbook Creates a single cookbook.
chef generate recipe Creates a new recipe file in an existing
cookbook.
chef generate attribute Creates a new attributes file in an existing
cookbook.
chef generate template Creates a new template file in an existing
cookbook. Use the -s SOURCE option to copy a source file's content to
populate the template.
chef generate file Creates a new cookbook file in an existing
cookbook. Supports the -s SOURCE option similar to template.

The chef generate command also accepts additional --generator-arg key=value
pairs that can be used to supply ad-hoc data to a generator cookbook.
For example, you might specify --generator-arg database=mysql and then only
write a template for recipes/mysql.rb if context.database == 'mysql'.
chef gem
chef gem is a wrapper command that manages installation and updating
of rubygems for the Ruby installation embedded in the ChefDK package.
This allows you to install knife plugins, Test Kitchen drivers, and
other Ruby applications that are not packaged with ChefDK.
Gems are installed to a .chefdk directory in your home directory; any
executables included with a gem you install will be created in
~/.chefdk/gem/ruby/2.1.0/bin. You can run these executables with
chef exec, or use chef shell-init to add ChefDK's paths to your
environment. Those commands are documented below.
chef exec
chef exec <command> runs any arbitrary shell command with the PATH
environment variable and the ruby environment variables (GEM_HOME,
GEM_PATH, etc.) setup to point at the embedded ChefDK omnibus environment.
chef shell-init
chef shell-init SHELL_NAME emits shell commands that modify your
environment to make ChefDK your primary ruby. It supports bash, zsh,
fish and PowerShell (posh). For more information to help you decide if
this is desirable and instructions, see ""Using ChefDK as Your Primary
Development Environment"" below.
chef install
chef install reads a Policyfile.rb document, which contains a
run_list and optional cookbook version constraints, finds a set of
cookbooks that provide the desired recipes and meet dependency
constraints, and emits a Policyfile.lock.json describing the expanded
run list and locked cookbook set. The Policyfile.lock.json can be used
to install the cookbooks on another machine. The policy lock can be
uploaded to a Chef Infra Server (via the chef push command) to apply the
expanded run list and locked cookbook set to nodes in your
infrastructure. See the POLICYFILE_README.md for further details.
chef push
chef push POLICY_GROUP uploads a Policyfile.lock.json along with the cookbooks it
references to a Chef Infra Server. The policy lock is applied to a
POLICY_GROUP, which is a set of nodes that share the same run list and
cookbook set. This command operates in compatibility mode and has the
same caveats as chef install. See the POLICYFILE_README.md for further
details.
chef update
chef update updates a Policyfile.lock.json with the latest cookbooks
from upstream sources. It supports an --attributes flag which will
cause only attributes from the Policyfile.rb to be updated.
chef diff
chef diff shows an itemized diff between Policyfile locks. It can
compare Policyfile locks from local disk, git, and/or the Chef Infra Server,
based on the options given.
chef verify
chef verify tests the embedded applications. By default it runs a
quick ""smoke test"" to verify that the embedded applications are
installed correctly and can run basic commands. As an end user this is
probably all you'll ever need, but verify can also optionally run unit
and integration tests by supplying the --unit and --integration
flags, respectively.
You can also focus on a specific suite of tests by passing it as an argument.
For example chef verify git will only run the smoke tests for the git suite.
WARNING: The integration tests will do dangerous things like start
HTTP servers with access to your filesystem and even create users and
groups if run with sufficient privileges. The tests may also be
sensitive to your machine's configuration. If you choose to run these,
we recommend to only run them on dedicated, isolated hosts (we do this
in our build cluster to verify each build).
Using ChefDK as Your Primary Development Environment
By default, ChefDK only adds a few select applications to your PATH
and packages them in such a way that they are isolated from any other
Ruby development tools you have on your system. If you're happily using
your system ruby, rvm, rbenv, chruby or any other development
environment, you can continue to do so. Just ensure that the ChefDK
provided applications appear first in your PATH before any
gem-installed versions and you're good to go.
If you'd like to use ChefDK as your primary Ruby/Chef Infra development
environment, however, you can do so by initializing your shell with
ChefDK's environment.
To try it temporarily, in a new terminal session, run:
eval ""$(chef shell-init SHELL_NAME)""
where SHELL_NAME is the name of your shell (usually bash, but zsh is
also common). This modifies your PATH and GEM_* environment
variables to include ChefDK's paths (run without the eval to see the
generated code). Now your default ruby and associated tools will be
the ones from ChefDK:
which ruby
# => /opt/chefdk/embedded/bin/ruby
To add ChefDK to your shell's environment permanently, add the
initialization step to your shell's profile:
echo 'eval ""$(chef shell-init SHELL_NAME)""' >> ~/.YOUR_SHELL_PROFILE
Where YOUR_SHELL_PROFILE is ~/.bash_profile for most bash users,
~/.zshrc for zsh, and ~/.bashrc on Ubuntu.
Powershell
You can use chef shell-init with PowerShell on Windows.
To try it in your current session:
chef shell-init powershell | Invoke-Expression
To enable it permanently:
""chef shell-init powershell | Invoke-Expression"" >> $PROFILE
Fish
chef shell-init also supports fish.
To try it:
eval (chef shell-init fish)
To permanently enable:
echo 'eval (chef shell-init SHELL_NAME)' >> ~/.config/fish/config.fish
Uninstallation Instructions
Mac OS X
You can uninstall Chef Development Kit on Mac using the below commands.
First, remove the main package files:
# Remove the installed files
sudo rm -rf /opt/chefdk

# Remove the system installation entry
sudo pkgutil --forget com.getchef.pkg.chefdk
Next, remove the symlinks which the Chef Development Kit installs. The
location for these differs based on your OS X version.
Pre-El Capitan:
# Symlinks are in /usr/bin
ls -la /usr/bin | egrep '/opt/chefdk' | awk '{ print $9 }' | sudo xargs -I % rm -f /usr/bin/%
Post-El Capitan:
# Symlinks are in /usr/local/bin
ls -la /usr/local/bin | egrep '/opt/chefdk' | awk '{ print $9 }' | sudo xargs -I % rm -f /usr/local/bin/%
Windows
You can use Add / Remove Programs on Windows to remove the Chef Development
Kit from your system.
RHEL
You can use rpm to uninstall Chef Development Kit on RHEL based systems:
rpm -qa *chefdk*
yum remove <package>
rm -rf /opt/chefdk
rm -rf ~/.chefdk
Ubuntu
You can use dpkg to uninstall Chef Development Kit on Ubuntu based systems:
dpkg --list | grep chefdk # or dpkg --status chefdk

# Purge chefdk from the system.
# see man dkpg for details
dpkg -P chefdk
Contributing
For information on contributing to this project see https://github.com/chef/chef/blob/master/CONTRIBUTING.md
For ChefDK Developers
See the Development Guide for how to get started with
development on the ChefDK itself, as well as details on how dependencies,
packaging, and building works.

",386
